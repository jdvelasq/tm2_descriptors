Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
Efficient code assignment techniques for local memory on software managed multicores,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952909242&doi=10.1145%2f2738039&partnerID=40&md5=c8dc8758cddf539803d1ca6fe990d235,"Scaling the memory hierarchy is a major challenge when we scale the number of cores in a multicore processor. Software Managed Multicore (SMM) architectures come up as one of the promising solutions. In an SMM architecture, there are no caches, and each core has only a local scratchpad memory [Banakar et al. 2002]. As the local memory usually is small, large applications cannot be directly executed on it. Code and data of the task mapped to each core need to be managed between global memory and local memory. This article solves the problem of efficiently managing code on an SMM architecture. The primary requirement of generating efficient code assignments is a correct management cost model. In this article, we address this problem by proposing a cost calculation graph. In addition, we develop two heuristics CMSM (Code Mapping for Software Managed multicores) and CMSM-advanced that result in efficient code management execution on the local scratchpad memory. Experimental results collected after executing applications from the MiBench suite [Guthaus et al. 2001] demonstrate that merely by adopting the correct management cost calculation, even using previous code assignment schemes, we can improve performance by an average of 12%. Combining the correct management cost model and a more optimized code mapping algorithm together, our heuristics can reduce runtime in more than 80% of the cases, and by up to 20% on our set of benchmarks, compared to the state-of-the-art code assignment approach [Jung et al. 2010]. When compared with Instruction-level Parallelism (ILP) results, CMSM-advanced performs an average of 5% worse.We also simulate the benchmarks on a cache-based system, and find that the code management overhead on SMM core with our code management is much less than memory latency of a cache-based system. © 2015 ACM.",Code; Embedded systems; Instruction; Localmemory; Multicore processor; Scratchpad memory,Aluminum; Code converters; Codes (symbols); Computer architecture; Conformal mapping; Costs; Embedded systems; Mapping; Memory architecture; Program processors; Code; Instruction; Localmemory; Multi-core processor; Scratch pad memory; Cache memory
Cooperative data reduction in wireless sensor network,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952912314&doi=10.1145%2f2786755&partnerID=40&md5=302b4fe92c5c14cde7369aed54506568,"In wireless sensor networks, owing to the limited energy of the sensor node, it is very meaningful to propose a dynamic scheduling scheme with data management that reduces energy as soon as possible. However, traditional techniques treat data management as an isolated process on only selected individual nodes. In this article, we propose an aggressive data reduction architecture, which is based on error control within sensor segments and integrates three parallel dynamic control mechanisms.We demonstrate that this architecture not only achieves energy savings but also guarantees the data accuracy specified by the application. Furthermore, based on this architecture, we propose two implementations. The experimental results show that both implementations can raise the energy savings while keeping the error at an predefined and acceptable level. We observed that, compared with the basic implementation, the enhancement implementation achieves a relatively higher data accuracy. Moreover, the enhancement implementation is more suitable for the harsh environmental monitoring applications. Further, when both implementations achieve the same accuracy, the enhancement implementation saves more energy. Extensive experiments on realistic historical soil temperature data confirm the efficacy and efficiency of two implementations. © 2015 ACM.",Artificial neural network; Centroid sensor; Data reduction strategy; Energy saving; Wireless sensor networks,Energy conservation; Information management; Memory architecture; Network architecture; Neural networks; Scheduling; Sensor nodes; Wireless sensor networks; Dynamic scheduling; Environmental Monitoring; Error control; Limited energies; Parallel dynamics; Reduction strategy; Soil temperature data; Traditional techniques; Data reduction
Diagnosability under weak fairness,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952904385&doi=10.1145%2f2832910&partnerID=40&md5=486f065178f533969a7319cd0554b10a,"In partially observed Petri nets, diagnosis is the task of detecting whether the given sequence of observed labels indicates that some unobservable fault has occurred. Diagnosability is an associated property of the Petri net, stating that in any possible execution, an occurrence of a fault can eventually be diagnosed. In this article, we consider diagnosability under the weak fairness (WF) assumption, which intuitively states that no transition from a given set can stay enabled forever - it must eventually either fire or be disabled. We show that a previous approach to WF-diagnosability in the literature has a major flaw and present a corrected notion. Moreover, we present an efficient method for verifying WF-diagnosability based on a reduction to LTL-Xmodel checking. An important advantage of this method is that the LTL-X formula is fixed - in particular, the WF assumption does not have to be expressed as a part of it (which would make the formula length proportional to the size of the specification), but rather the ability of existing model checkers to handle weak fairness directly is exploited. © 2015 ACM.",Diagnosability; Formal verification; LTL-X; Model checking; Petri nets; Weak fairness,Fault detection; Formal verification; Petri nets; Diagnosability; LTL-X; Model checker; Unobservable; Weak fairness; Model checking
Perpetuu: A tiered solar-powered gis microserver,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952782897&doi=10.1145%2f2767128&partnerID=40&md5=1ab21b0a00fd6c63b8787d143ec18806,"The aftermath of a natural disaster is characterized by lack of a reliable medium for dissemination of information to survivors. The state-of-the-art emergency response systems rely on satellite radio-enabled devices, but survivors, unlike first responders, do not have access to such devices. To mitigate this problem, we present perpetuu, a solar-powered portable GIS microserver. The microserver node can be deployed in a disaster scene and can serve maps to survivors viewable on browsers of off-the-shelf mobile systems. The perpetuu nodes can form a wireless mesh to cover a large geographic region. A key innovation in the design of the perpetuu node is a tiered software and hardware architecture-the system combines a low-power micro-controller with a high-power micro-processor to provide a large spectrum of power states. perpetuu stays in its lowest power state most of the time, and it can in-vitro detect survivors using Wi-Fi sensing, and consequently wake up the higher-power tier to disseminate high-resolution maps on standard web browsers that provide directions to safe locations. The tiered design leverages hardware-assisted energy measurements and a wakeup controller to balance energy harvested from solar panels with energy consumed by the system. We evaluate perpetuu using measurements from our prototype and trace-based simulations, and show that it can function near-perpetually while serving maps to a large number of survivors. © 2015 ACM.",GIS micro server; Multi-tiered design; Solar powered,Computer hardware; Disasters; Geographic information systems; Hardware; Reconfigurable hardware; Web browsers; Emergency response systems; High resolution maps; Micro-servers; Multi-tiered; Natural disasters; Software and hardwares; Solar-powered; Trace-based simulation; Solar energy
Incremental analysis of cyclo-static synchronous dataflow graphs,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952766728&doi=10.1145%2f2792981&partnerID=40&md5=5ac6940ab6a4ef525ee762696fb2a378,"In this article, we present a mathematical characterisation of admissible schedules of cyclo-static dataflow (CSDF) graphs. We demonstrate how algebra ic manipulation of this characterization is related to unfolding CSDF actors and how this manipulation allows CSDF graphs to be transformed into MRSDF graphs that are equivalent, in the sense that they admit the same set of schedules. The presented transformation allows the rich set of existing analysis techniques for MRSDF graphs to be applied to CSDF graphs and generalizes the wellknown transformations from CSDF and MRSDF into HSDF. Moreover, it gives rise to an incremental approach to the analysis of CSDF graphs, where approximate analyses are combined with exact transformations. We show the applicability of this incremental approach by demonstrating its effectiveness on the problem of optimizing buffer sizes under a throughput constraint. © 2015 ACM.",Analysis; Cyclo-static synchronous; Dataflow graphs,Data flow graphs; Graphic methods; Analysis; Approximate analysis; Cyclo-static dataflow; Cyclo-static synchronous; Incremental analysis; Incremental approach; Synchronous dataflow graphs; Throughput constraints; Data flow analysis
Scalable global power management policy based on combinatorial optimization for multiprocessors,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952802228&doi=10.1145%2f2811404&partnerID=40&md5=04f056588159178dd4217f42f95087c0,"Multiprocessors have become the main architecture trend in modern systems due to the superior performance; nevertheless, the power consumption remains a critical challenge. Global power management (GPM) aims at dynamically finding the power state combination that satisfies the power budget constraint while maximizing the overall performance (or vice versa). Due to the increasing number of cores in amultiprocessor system, the scalability of GPM policies has become critical when searching satisfactory state combinations within acceptable time. This article proposes a highly scalable policy based on combinatorial optimization with theoretical proofs, whereas previous works take exhaustive search or heuristic methods. The proposed policy first applies an optimum algorithm to construct a state combination table in pseudo-polynomial time using dynamic programming. Then, the state combination is assigned to cores with minimum transition cost in linear time by mapping to the network flow problem. Simulation results show that the proposed policy achieves better system performance for any given power budget when compared to the state-of-the-art heuristic. Furthermore, the proposed policy demonstrates its prominent scalability with 125 times faster policy runtime for 512 cores. © 2015 ACM.",Combinatorial optimization; DVFS; Multiprocessor systems,Budget control; Combinatorial optimization; Energy management; Heuristic methods; Multiprocessing systems; Optimization; Polynomial approximation; Power management; Scalability; Critical challenges; DVFS; Management policy; Multi processor systems; Network flow problems; Polynomial-time; State of the art; Transition costs; Dynamic programming
"Editorial: Big data, internet of things, cybersecurity-a new trinity of embedded systems research",2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946138123&doi=10.1145%2f2820608&partnerID=40&md5=16d3c8c7b98b1b82d208ec8ef1e8a83e,[No abstract available],,
Runtime monitoring of cyber-physical systems under timing and memory constraints,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947231019&doi=10.1145%2f2744196&partnerID=40&md5=415e625af2707f80feae84adb8fb6995,"The goal of runtime monitoring is to inspect the well-being of a system by employing a monitor process that reads the state of the system during execution and evaluates a set of properties expressed in some specification language. The main challenge in runtime monitoring is dealing with the costs imposed in terms of resource utilization. In the context of cyber-physical systems, it is crucial for a software monitoring solution to be time predictable to improve scheduling, as well as support composition of monitoring solutions with an overall predictable behavior. Moreover, a small memory footprint is often required in components of cyberphysical systems, especially in deeply embedded systems. In this article, we propose a novel control-theoretic software monitoring solution for coordinating time predictability and memory utilization in runtime monitoring of systems that interact with the physical world. The controllers attempt to reduce monitoring jitter and maximize memory utilization while simultaneously ensuring the soundness of evaluation of properties. For systems where multiple properties are required to be monitored simultaneously, we construct a buffer sharing mechanism in which controllers dynamically share the memory space to negate the effect of bursts of environment actions, thus reducing jitter due to transient high loads. To validate our design choices, we present three case studies: (1) a Bluetooth mobile payment system, which shows a sporadic rate of events during peak hours; (2) a laser beam stabilizer for target tracking, and (3) a monitoring system for air/fuel ratio in a car engine exhaust and the CAM inlet position in the engine's cylinders. The experimental results of the case studies demonstrate up to 40% improvement in time predictability of the monitoring solution when compared to a basic event-triggered approach. Moreover, memory utilization reaches an average of 90% when using our dynamic buffer resizing mechanism. © 2015 ACM.",Cyber-physical systems; Resource efficiency; Runtime monitoring,Associative storage; Embedded systems; Engine cylinders; Engines; Exhaust systems (engine); Global system for mobile communications; Jitter; Laser beams; Memory architecture; Network components; Specification languages; Target tracking; Cyber physical systems (CPSs); Mobile payment system; Resource efficiencies; Resource utilizations; Runtime Monitoring; Small memory footprint; Software monitoring; Time predictabilities; Monitoring
Guest editorial for special issue application of concurrency to system design,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946102174&doi=10.1145%2f2809925&partnerID=40&md5=2e5920f20feedeb7756cc25b5ce70a15,[No abstract available],,
Resource synchronization and preemption thresholds within mixed-criticality scheduling,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947236195&doi=10.1145%2f2783440&partnerID=40&md5=8623efa32fde95dc950639769efd0388,"In a mixed-criticality system, multiple tasks with different levels of criticality may coexist on the same hardware platform. The scheduling algorithm EDF-VD (Earliest Deadline First with Virtual Deadlines) has been proposed for mixed-criticality systems, which assumes tasks do not share any common resources. We presentMC-SRP (Mixed-Criticality Stack Resource Policy), a resource synchronization protocol for EDF-VD, which allows resource sharing among tasks at the same criticality level and guarantees that each task is blocked at most once in each criticality mode. In addition, we present MC-SRPT (MC-SRP with Thresholds) for reducing the application stack size requirement in resource-constrained embedded systems. © 2015 ACM.",Mixed-criticality systems; Preemption threshold scheduling; Real-time scheduling; Resource synchronization protocol,Criticality (nuclear fission); Embedded systems; Real time systems; Response time (computer systems); Scheduling; Synchronization; Earliest deadline first; Mixed criticalities; Mixed-criticality systems; Preemption thresholds; Real - time scheduling; Resource-constrained embedded systems; Stack resource policies; Synchronization protocols; Scheduling algorithms
A storage device emulator for system performance evaluation,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947060017&doi=10.1145%2f2785969&partnerID=40&md5=dfc3765963d4b3e7ffb0769ae7a59ca9,"The performance and characteristics of the storage devices used in embedded systems can have a great influence on the overall end user experience. When building embedded systems or designing new storage device components, it is important for the designers to be able to evaluate how storage devices of different characteristics will affect the overall system performance. Storage device emulation enables a system's performance to be evaluated with simulated storage devices that are not yet available. In storage device emulation, the emulated storage device appears to the operating system (OS) as a real storage device and its service timings are determined by a disk model, which simulates the behavior of the target storage device. In the conventional storage device emulators, because the OS is running continuously in the real-time domain, the amount of time that the emulators can spend on processing each I/O request is limited by the service time of each corresponding I/O request. This timing constraint can make emulating high-speed storage devices a challenge for the conventional storage device emulators. In this article, we propose an OS state pausing approach to storage device emulation that can overcome the timing constraints faced by the conventional storage device emulators. By pausing the state of the OS while the storage device emulator is busy, the proposed emulator can spend as much time as it needs for processing each I/O request without affecting the performance of the emulated storage device as perceived by the OS. This allows the proposed storage device emulator to emulate storage devices that would otherwise be challenging or even impossible for the conventional storage device emulators. In addition, the main task of storage device emulation is offloaded to an external computer to minimize the impact of the emulation workload on the target machine. The proposed storage device emulator is implemented with the Linux OS1 on an embedded system development board. Experimental results show that the full-system performance benchmarks measured with the proposed storage device emulator are within 2% differences compared to the results of the reference system. ï¿½ 2015 ACM.",Storage emulation; Storage evaluation,Benchmarking; Computer operating systems; Embedded systems; Embedded system development; End-user experience; Real-time domains; Reference systems; System performance evaluation; System's performance; Target machines; Timing constraints; Virtual storage
Failure semantics for modal transition systems,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941954765&doi=10.1145%2f2746336&partnerID=40&md5=48b6642adb3fce1a95b8d8377ee2edbb,"With the aim to preserve deadlock freedom,we define a new refinement preorder for modal transition systems (MTSs), using an MTS-specific variant of testing inspired by De Nicola and Hennessy. We characterize this refinement with a kind of failure semantics and show that it ""supports itself,"" for example, in the sense of thoroughness-in contrast to standard modal refinements. We present a conjunction operator with respect to our new refinement, which is quite different from existing ones. It always returns an MTS-again in contrast to the case of modal refinement. Finally, we also consider De Nicola's and Hennessy's may- and must-testing, where the latter leads to a semantics that is also compositional for hiding. © 2015 ACM.",Conjunction; Divergence; Failure semantics; Hiding; May-testing; Modal transition systems; Testing; Thorough refinement,Software engineering; Testing; Conjunction; Divergence; Failure semantics; Hiding; May testing; Modal Transition Systems; Thorough refinement; Semantics
Maximizing the number of good dies for streaming applications in NoC-based MPSoCs under process variation,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942852437&doi=10.1145%2f2785968&partnerID=40&md5=e1ccb76618129b6f583a0828dc4811d8,"Scaling CMOS technology into nanometer feature-size nodes has made it practically impossible to precisely control the manufacturing process. This results in variation in the speed and power consumption of a circuit. As a solution to process-induced variations, circuits are conventionally implemented with conservative design margins to guarantee the target frequency of each hardware component in manufactured multiprocessor chips. This approach, referred to as worst-case design, results in a considerable circuit upsizing, in turn reducing the number of dies on a wafer. This work deals with the design of real-time systems for streaming applications (e.g., video decoders) constrained by a throughput requirement (e.g., frames per second) with reduced design margins, referred to as better-than-worst-case design. To this end, the first contribution of this work is a complete modeling framework that captures a streaming application mapped to an NoC-based multiprocessor system with voltage-frequency islands under process-induced die-to-die and within-die frequency variations. The framework is used to analyze the impact of variations in the frequency of hardware components on application throughput at the system level. The second contribution of this work is a methodology to use the proposed framework and estimate the impact of reducing circuit design margins on the number of good dies that satisfy the throughput requirement of a real-time streaming application. We show on both synthetic and real applications that the proposed better-than-worst-case design approach can increase the number of good dies by up to 9.6% and 18.8% for designs with and without fixed SRAM and IO blocks, respectively. © 2015 ACM.",Multiprocessor system; Process variation; Reduced design margins,CMOS integrated circuits; Design; Dies; Hardware; Integrated circuit manufacture; Interactive computer systems; Multiprocessing systems; Network-on-chip; Real time systems; System-on-chip; Throughput; Conservative designs; Design margin; Manufacturing process; Multi processor systems; Process Variation; Process-induced variation; Streaming applications; Voltage-frequency islands; Integrated circuit design
Parametrised modal interface automata,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941961350&doi=10.1145%2f2776892&partnerID=40&md5=a3c13499a43bbfde1960bf3a9d52d291,"Interface theories (ITs) enable us to analyse the compatibility interfaces and refine them while preserving their compatibility. However, most ITs are for finite state interfaces, whereas computing systems are often parametrised involving components, the number of which cannot be fixed.We present, to our knowledge, the first IT that allows us to specify a parametric number of interfaces. Moreover, we provide a fully algorithmic procedure, implemented in a tool, for checking the compatibility of and refinement between parametrised interfaces. Finally, we show that the restrictions of the technique are necessary; removing any of them renders the refinement checking problem undecidable. © 2015 ACM.",Compatibility; Decidability; Formal verification; Interface theories; Modal interface automata; Parameterized systems; Refinement,Automata theory; Computability and decidability; Formal verification; Robots; Algorithmic procedure; Compatibility; Computing system; Finite state; Interface automata; Parameterized system; Refinement; Refinement checking; Interface states
Memory-model-aware testing: A unified complexity analysis,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945179313&doi=10.1145%2f2753761&partnerID=40&md5=2f9a487f77360b62e56e92016e3a10c4,"To improve the performance of the memory system, multiprocessors implement weak memory consistency models. Weak memory models admit different views of the processes on their load and store instructions, thus allowing for computations that are not sequentially consistent. Program analyses have to take into account the memory model of the targeted hardware. This is challenging because numerous memory models have been developed, and every memory model requires its own analysis. In this article, we study a prominent approach to program analysis: testing. The testing problem takes as input sequences of operations, one for each process in the concurrent program. The task is to check whether these sequences can be interleaved to an execution of the entire program that respects the constraints of a memory model under consideration. We determine the complexity of the testing problem for most of the known memory models. Moreover, we study the impact on the complexity of parameters, such as the number of concurrent processes, the length of their executions, and the number of shared variables. What differentiates our contribution from related results is a uniform approach that avoids considering each memory model on its own. We build upon work of Steinke and Nutt. They showed that the existing memory models form a hierarchy where one model is called weaker than another one if it includes the latterâ€™s behavior. Using the Steinke-Nutt hierarchy, we develop three general concepts that allow us to quickly determine the complexity of a testing problem. First, we generalize the technique of problem reductions from complexity theory. So-called range reductions propagate hardness results between memory models, and we apply them to establish NP lower bounds for the stronger memory models. Second, for the weaker models, we present polynomial-time testing algorithms that are inspired by determinization algorithms for automata. Finally, we describe a single SAT encoding of the testing problem that works for all memory models in the Steinke-Nutt hierarchy to prove their membership in NP. Our results are general enough to carry over to future weak memory models. Moreover, they show that SAT solvers are adequate tools for testing. © 2015 ACM.",Complexity analysis; NP-completeness; SAT; Testing; Weak memory models,Polynomial approximation; Testing; Complexity analysis; Complexity theory; Concurrent process; Concurrent program; Memory consistency models; Np-completeness; SAT; Weak memory models; Software testing
OPLE: A heuristic custom instruction selection algorithm based on partitioning and local exploration of application dataflow graphs,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941950452&doi=10.1145%2f2764458&partnerID=40&md5=6f09ed5aeb391ce73203269a9c9645af,"In this article, a heuristic custom instruction (CI) selection algorithm is presented. The proposed algorithm, which is called OPLE for ""Optimization based on Partitioning and Local Exploration,"" uses a combination of greedy and optimal optimization methods. It searches for the near-optimal solution by reducing the search space based on partitioning the identified CI set. The partitioning of the identified set guarantees the success of the algorithm independent of the size of the identified set. First, the algorithm finds the near-optimal CIs from the candidate CIs for each part. Next, the suggested CIs from different parts are combined to determine the final selected CI set. To improve the set of the selected CIs, the solution is evolved by calling the algorithm iteratively. The efficacy of the algorithm is assessed by comparing its performance to those of optimal and nonoptimal methods. A comparative study is performed for a number of benchmarks under different area budgets and I/O constraints. The results reveal higher speedups for the OPLE algorithm, especially for larger identified candidate sets and/or small area budgets compared to those of the nonoptimal solutions. Compared to the nonoptimal techniques, the proposed algorithm provides 30% higher speedup improvement on average. The maximum improvement is 117%. The results also demonstrate that in many cases OPLE is able to find the optimal solution. © 2015 ACM.",ASIP; Custom instruction selection; Heuristic algorithm; Speedup,Budget control; Data flow analysis; Genetic algorithms; Heuristic algorithms; Iterative methods; Optimal systems; ASIP; Comparative studies; Custom instruction; Near-optimal solutions; Nonoptimal solutions; Optimization method; Selection algorithm; Speedup; Graph algorithms
Crenel-interval-based dynamic power management for periodic real-time systems,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942862684&doi=10.1145%2f2744197&partnerID=40&md5=fd87711026de3880903a74b19a3e0be6,"In order to save the energy consumption of real-time embedded systems, the integration of Dynamic Voltage and Frequency Scaling (DVFS) and Device Power Management (DPM) techniques has been well studied. In this article, we propose a new energy management scheme for periodic real-time tasks with implicit deadlines. We mainly focus on the DPM part by presenting a novel approach to the real-time DPM problem. Specifically, we first identify intervals for each device, which we refer to as Crenel Intervals, by partitioning the Earliest Deadline First (EDF) schedule of the tasks that need to access the device into successive intervals. The principle for identifying Crenel Intervals is that for each task, there is only one deadline located in each Crenel Interval. Next, targeting at a single device model and a multiple device model, respectively, we propose the CI-EDF and CI-EDFm algorithms to schedule task instances in each Crenel Interval, so as to form long and continuous slacks in each Crenel Interval but without jeopardizing any task deadlines. Then, the slack in the Crenel Intervals can be utilized to perform not only DPM, but also DVFS. The experimental results show that our approaches can achieve considerably more energy savings than existing techniques with comparable quality. © 2015 ACM.",Crenel Interval; Dynamic power management; Embedded real-time systems; Periodic tasks,Dynamic frequency scaling; Embedded systems; Energy conservation; Energy management; Energy utilization; Interactive computer systems; Power management; Response time (computer systems); Scheduling algorithms; Voltage scaling; Crenel Interval; Dynamic power management; Dynamic voltage and frequency scaling; Earliest deadline first; Embedded real time systems; Multiple devices; Periodic tasks; Real-time embedded systems; Real time systems
Analyzing event-based scheduling in concurrent reactive systems,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945197289&doi=10.1145%2f2783438&partnerID=40&md5=5163ac3b66d11d4d41c66f31ff054d6c,"The traditional research on scheduling focuses on task scheduling and schedulability analysis in concurrent reactive systems. In this article, we dedicate ourselves to event-based scheduling. We first formally define an event-based scheduling policy and propose the notion of the correctness of a scheduling policy in terms of weak termination. Then we investigate the correctness of the decomposition of scheduling controls and finally obtain a decentralized scheduling method. The method can automatically decompose the scheduling policies of a concurrent reactive system into atomic scheduling policies. Every atomic scheduling policy corresponds to one subsystem. Each of the subsystems is a completely independent system, which may be developed and deployed independently. An experiment demonstrates these results that may help engineers to design correct and efficient schedule policies for a concurrent reactive system. © 2015 ACM.",Concurrent reactive systems; Correctness; Priority; Scheduling,Hardware; Software engineering; Correctness; Decentralized scheduling; Independent systems; Priority; Reactive system; Schedulability analysis; Scheduling control; Scheduling policies; Scheduling
Anonymous split E-cash-toward mobile anonymous payments,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941959851&doi=10.1145%2f2783439&partnerID=40&md5=37f1196ccce1ffab79e9f7cb402792f4,"Anonymous E-Cash was first introduced in 1982 as a digital, privacy-preserving alternative to physical cash. A lot of research has since then been devoted to extend and improve its properties, leading to the appearance of multiple schemes. Despite this progress, the practical feasibility of E-Cash systems is still today an open question. Payment tokens are typically portable hardware devices in smart card form, resource constrained due to their size, and therefore not suited to support largely complex protocols such as E-Cash. Migrating to more powerful mobile platforms, for instance, smartphones, seems a natural alternative. However, this impliesmoving computations from trusted and dedicated execution environments to generic multiapplication platforms, which may result in security vulnerabilities. In this work, we propose a new anonymous E-Cash system to overcome this limitation. Motivated by existing payment schemes based on MTM (Mobile Trusted Module) architectures, we consider at design time a model in which user payment tokens are composed of two modules: an untrusted but powerful execution platform (e.g., smartphone) and a trusted but constrained platform (e.g., secure element). We show how the protocol's computational complexity can be relaxed by a secure split of computations: nonsensitive operations are delegated to the powerful platform, while sensitive computations are kept in a secure environment. We provide a full construction of our proposed Anonymous Split E-Cash scheme and show that it fully complies with the main properties of an ideal E-Cash system. Finally, we test its performance by implementing it on an Android smartphone equipped with a Java-Cardcompatible secure element. © 2015 ACM.",Android; Anonymity; E-Cash; Java Card; Payment systems; Privacy-enhancing technologies,Android (operating system); Electronic money; Java programming language; Privacy by design; Smart cards; Smartphones; Android; Anonymity; JAVA card; Payment systems; Privacy enhancing technologies; Mobile security
3D CV descriptor on parallel heterogeneous platforms,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942887739&doi=10.1145%2f2733377&partnerID=40&md5=b0847e49e7cc9181b1fdec1cdf9631fd,"Embedded three-dimensional (3D) Computer Vision (CV) is considered a technology enabler for future consumer applications, attracting a wide interest in academia and industry. However, 3D CV processing is a computation-intensive task. Its high computational cost is directly related to the processing of 3D point clouds, with the 3D descriptor computation representing one of the main bottlenecks. Understanding the main computational challenges of 3D CV applications, as well as the key characteristics, enabling features, and limitations of current computing platforms, is clearly strategic to identify the directions of evolution for future embedded processing systems targeting 3D CV. In this work, an innovative and complex 3D descriptor (called SHOT) has been ported on a high-end and an embedded computing platform. The high-end system is composed by a high-performance Intel CPU coupled with a Nvidia GPU. The embedded platform is, instead, composed by an ARM-based processor, coupled with the STHORM accelerator. STHORM is a many-core low-power accelerator developed by ST Microelectronics, featuring up to 64 computational units. The SHOT descriptor has been parallelized using the OpenCL programming model for both platforms. Finally, we have performed an in-depth performance comparison and analysis between general-purpose processors and accelerators in both high-end and embedded domains, discussing and highlighting the main differences in the Hardware/Software (HW/SW) design methodologies and approaches between high-end and embedded systems targeting 3D CV applications. © 2015 ACM.",3D descriptor; Computer vision; Embedded computing; GPU; HW accelerator; Low-power many-core; OpenCL; SHOT,ARM processors; Computer vision; General purpose computers; Microelectronics; Three dimensional computer graphics; 3d descriptor; Embedded computing; GPU; Many core; OpenCL; SHOT; Embedded systems
Parameter space representation of pareto front to explore hardware-software dependencies,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941956195&doi=10.1145%2f2764457&partnerID=40&md5=2f503ea88e2b45a692f735cdb7246be4,"Embedded systems design requires conflicting objectives to be optimized with an appropriate choice of hardware-software parameters. A simulation campaign can guide the design in finding the best tradeoffs, but due to the big number of possible configurations, it is often unfeasible to simulate them all. For these reasons, design space exploration algorithms aim at finding near-optimal system configurations by simulating only a subset of them. In this work, we present PS, a new multiobjective optimization algorithm, and evaluate it in the context of the embedded system design. The basic idea is to recognize interesting regions-that is, regions of the configuration space that provide better configurations with respect to other ones. PS evaluates more configurations in the interesting regions while less thoroughly exploring the rest of the configuration space. After a detailed formal description of the algorithm and the underlying concepts, we show a case study involving the hardware/software exploration of a VLIW architecture. Qualitative and quantitative comparisons of PS against a well-known multiobjective genetic approach demonstrate that while not outperforming it in terms of Pareto dominance, the proposed approach can balance the uniformity and granularity qualities of the solutions found, obtaining more extended Pareto fronts that provide a wider view of the potentiality of the designed device. Therefore, PS represents a further valid choice for the designer when objective constrains allow it. © 2015 ACM.",Design space exploration; Genetic algorithms; Multiobjective optimization,Embedded software; Embedded systems; Genetic algorithms; Systems analysis; Very long instruction word architecture; Wave functions; Configuration space; Conflicting objectives; Design space exploration; Formal Description; Hardware/software; Quantitative comparison; Software dependencies; Software parameters; Multiobjective optimization
Safety and progress for distributed cyber-physical systems with unreliable communication,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942945243&doi=10.1145%2f2739046&partnerID=40&md5=7381d42476f9e262a7ffa6dd08d9dd01,"Cyber-physical systems (CPSs) may interact and manipulate objects in the physical world, and therefore formal guarantees about their behavior are strongly desired. Static-time proofs of safety invariants, however, may be intractable for systems with distributed physical-world interactions. This is further complicated when realistic communication models are considered, for which there may not be bounds on message delays, or even when considering that messages will eventually reach their destination. In this work, we address the challenge of proving safety and progress in distributed CPSs communicating over an unreliable communication layer. We show that for this type of communication model, system safety is closely related to the results of a hybrid system's reachability computation, which can be computed at runtime. However, since computing reachability at runtime may be computationally intensive, we provide an approach that moves significant parts of the computation to design time. This approach is demonstrated with a case study of a simulation of multiple vehicles moving within a shared environment. © 2015 ACM.",Cyber-physical systems; Distributed system design; Hybrid automata; Reachability computation; Runtime verification,Embedded systems; Hybrid systems; Information theory; Cyber physical systems (CPSs); Distributed system designs; Hybrid automatons; Reachability; Run-time verification; Distributed computer systems
Architecture-aware real-time compression of execution traces,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941956764&doi=10.1145%2f2766449&partnerID=40&md5=c692b38e609acc8ec5ba00f151e20c2a,"In recent years, on-chip trace generation has been recognized as a solution to the debugging of increasingly complex software. An execution trace can be seen as the most fundamentally useful type of trace, allowing the execution path of software to be determined post hoc. However, the bandwidth required to output such a trace can be excessive. Our architecture-aware trace compression (AATC) scheme adds an on-chip branch predictor and branch target buffer to reduce the volume of execution trace data in real time through on-chip compression. Novel redundancy reduction strategies are employed, most notably in exploiting the widespread use of linked branches and the compiler-driven movement of return addresses between link register, stack, and program counter. In doing so, the volume of branch target addresses is reduced by 52%, whereas other algorithmic improvements further decrease trace volume. An analysis of spatial and temporal redundancy in the trace stream allows a comparison of encoding strategies to be made for systematically increasing compression performance. A combination of differential, Fibonacci, VarLen, and Move-to-Front encodings are chosen to produce two compressor variants: a performance-focused xAATC that encodes 56.5 instructions/bit using 24,133 gates and an area-efficient fAATC that encodes 48.1 instructions/bit using only 9,854 gates. © 2015 ACM.",Branch prediction; Software debugging; Trace compression; Trace generation,Bandwidth; Encoding (symbols); Program compilers; Redundancy; Branch prediction; Branch target buffers; Compression performance; Redundancy reductions; Software debugging; Spatial and temporal redundancies; Trace compression; Trace generation; Program debugging
STM-HRT: A robust and wait-free STM for hard real-time multicore embedded systems,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941946165&doi=10.1145%2f2786979&partnerID=40&md5=a64f23d672437ec46486a3cbacca28ec,"This article introduces STM-HRT, a nonblocking wait-free software transactional memory (STM) for hard real-time (HRT) multicore embedded systems. Resource access control in HRT systems is usually implemented with lock-based synchronization. However, these mechanisms may lead to deadlocks or starvations and do not scale well with the number of cores. Most existing nonblocking STM are not suitable for HRT systems, because it is not possible to find an upper bound of the execution time for each task. In this article, we show how STM-HRT can be a robust solution for resource sharing in HRT multicore systems. We provide a detailed description of STM-HRT architecture. We propose a set of arguments to establish the functional correctness of its concurrency control protocol. Finally, as part of a real-time analysis, we derive upper bounds on the computations required to access shared data under STM-HRT. © 2015 ACM.",Embedded systems; Hard real time; Nonblocking synchronization; Software transactional memory,Access control; Concurrency control; Data Sharing; Embedded systems; Storage allocation (computer); Concurrency control protocols; Functional correctness; Hard real-time; Lock-based synchronization; Multi-core embedded systems; Non-blocking synchronization; Resource access control; Software transactional memory; Real time systems
Action synthesis for branching time logic: Theory and applications,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941960184&doi=10.1145%2f2746337&partnerID=40&md5=fb4ad8729046f6a2b6e4be1551149041,"The article introduces a parametric extension of Action-Restricted Computation Tree Logic called pmARCTL. A symbolic fixed-point algorithm providing a solution to the exhaustive parameter synthesis problem is proposed. The parametric approach allows for an in-depth system analysis and synthesis of the correct parameter values. The time complexity of the problem and the algorithm is provided. An existential fragment of pmARCTL (pmEARCTL) is identified, in which all of the solutions can be generated from a minimal and unique base. A method for computing this base using symbolic methods is provided. The prototype tool SPATULA implementing the algorithm is applied to the analysis of three benchmarks: faulty Train-Gate-Controller, Peterson's mutual exclusion protocol, and a generic pipeline processing network. The experimental results show efficiency and scalability of our approach compared to the naive solution to the problem. © 2015 ACM.",Parameter synthesis; Parametric verification; Parametricmodel checking,Computer circuits; Analysis and synthesis; Computation tree logic; Fixed-point algorithms; Mutual exclusion protocol; Parameter synthesis; Parametric verification; Parametricmodel checking; Pipeline processing; Parameter estimation
A sliding window phase-only correlation method for side-channel alignment in a smartphone,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941954970&doi=10.1145%2f2783441&partnerID=40&md5=ae4e6901339908461852305966f610e7,"Future wireless embedded devices will be increasingly powerful, supporting many more applications including one of themost crucial, security. Although many embedded devices offer resistance to bus probing attacks due to their compact size and high levels of integration, susceptibility to attacks on their electromagnetic side channel must be analyzed. This side channel is often quite complex to analyze due to the complexities of the embedded device including operating system, interrupts, and so forth. This article presents a new methodology for analyzing a complex system's vulnerability to the EM side channel. The methodology proposes a sliding window phase-only correlation method for aligning electromagnetic emanations from a complex smartphone running native code utilizing an on-chip cache. Unlike previous research, experimental results demonstrate that data written to on-chip cache within an advanced 312MHz 0.13um processor executing AES can be attacked utilizing this new methodology. Furthermore, for the first time, it has been shown that the point of side-channel attack is not a spike of increased EM but an area of low EM amplitude, unlike what is noted in previous findings. This research is important for advancing side-channel analysis understanding in complex embedded processors and ensuring secure implementations in future embedded ubiquitous devices. © 2015 ACM.",Cache attack; EM attack; Side-channel analysis,Cache memory; Correlation methods; Mobile security; Smartphones; Cache attack; Electromagnetic emanation; EM attack; Embedded processors; Phase-only correlation; Secure implementation; Side-channel analysis; Ubiquitous devices; Side channel attack
Editorial: Oh security-where art thou?,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928499438&doi=10.1145%2f2742044&partnerID=40&md5=aead07fc10ab0d6d5c5c6198861b7aec,[No abstract available],,
Effective runtime resource management using linux control groups with the BarbequeRTRM framework,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928413119&doi=10.1145%2f2658990&partnerID=40&md5=8636ccf1493d09c9fc35cc70b69dab65,"The extremely high technology process reached by silicon manufacturing (smaller than 32nm) has led to production of computational platforms and SoC, featuring a considerable amount of resources. Whereas from one side such multi-and many-core platforms show growing performance capabilities, from the other side they are more and more affected by power, thermal, and reliability issues. Moreover, the increased computational capabilities allows congested usage scenarios with workloads subject to mixed and time-varying requirements. Effective usage of the resources should take into account both the application requirements and resources availability, with an arbiter, namely a resource manager in charge to solve the resource contention among demanding applications. Current operating systems (OS) have only a limited knowledge about application-specific behaviors and their time-varying requirements. Dedicated system interfaces to collect such inputs and forward them to the OS (e.g., its scheduler) are thus an interesting research area that aims at integrating the OS with an ad hoc resource manager. Such a component can exploit efficient low-level OS interfaces and mechanisms to extend its capabilities of controlling tasks and system resources. Because of the specific tasks and timings of a resource manager, this component can be easily and effectively developed as a user-space extension lying in between the OS and the controlled application. This article, which focuses on multicore Linux systems, shows a portable solution to enforce runtime resource management decisions based on the standard control groups framework. A burst and a mixed workload analysis, performed on a multicore-based NUMA platform, have reported some promising results both in terms of performance and power saving. © 2015 ACM.",Algorithm; Design; Management; Performance,Algorithms; Computer architecture; Computer operating systems; Design; Linux; Management; Managers; Natural resources management; Resource allocation; System-on-chip; Application requirements; Application specific; Computational capability; Computational platforms; Performance; Performance capability; Resource contention; Resource management; Decision support systems
Energy-efficient and high-performance lock speculation hardware for embedded multicore systems,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930673122&doi=10.1145%2f2700097&partnerID=40&md5=5fd93e52f4c944d47a6a867aadfaa099,"Embedded systems are becoming increasingly common in everyday life and like their general-purpose counterparts, they have shifted towards shared memory multicore architectures. However, they are much more resource constrained, and as they often run on batteries, energy efficiency becomes critically important. In such systems, achieving high concurrency is a key demand for delivering satisfactory performance at low energy cost. In order to achieve this high concurrency, consistency across the shared memory hierarchy must be accomplished in a cost-effective manner in terms of performance, energy, and implementation complexity. In this article, we propose EMBEDDED-SPEC, a hardware solution for supporting transparent lock speculation, without the requirement for special supporting instructions. Using this approach, we evaluate the energy consumption and performance of a suite of benchmarks, exploring a range of contention management and retry policies. We conclude that for resource-constrained platforms, lock speculation can provide real benefits in terms of improved concurrency and energy efficiency, as long as the underlying hardware support is carefully configured. © 2015.",energy-efficiency; lock elision; lock removal; low-power; Transactional memory,Benchmarking; Cost effectiveness; Embedded systems; Energy policy; Energy utilization; Locks (fasteners); Memory architecture; Software architecture; Contention managements; Embedded multicore; Hardware solutions; Implementation complexity; Lock elisions; Low Power; Multicore architectures; Transactional memory; Energy efficiency
Editorial: Schizoid design for critical embedded systems,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930623830&doi=10.1145%2f2761728&partnerID=40&md5=6663394d835d74aeb37cecb1b0ba63a2,[No abstract available],,
Communication optimizations for multithreaded code generation from Simulink models,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930616471&doi=10.1145%2f2644811&partnerID=40&md5=09ff11479a5da118cf2ed51e81b7e8de,"Communication frequency is increasing with the growing complexity of emerging embedded applications and the number of processors in the implemented multiprocessor SoC architectures. In this article, we consider the issue of communication cost reduction during multithreaded code generation from partitioned Simulink models to help designers in code optimization to improve system performance. We first propose a technique combining message aggregation and communication pipeline methods, which groups communications with the same destinations and sources and parallelizes communication and computation tasks. We also present a method to apply static analysis and dynamic emulation for efficient communication buffer allocation to further reduce synchronization cost and increase processor utilization. The existing cyclic dependency in the mapped model may hinder the effectiveness of the two techniques. We further propose a set of optimizations involving repartition with strongly connected threads to maximize the degree of communication reduction and preprocessing strategies with available delays in the model to reduce the number of communication channels that cannot be optimized. Experimental results demonstrate the advantages of the proposed optimizations with 11-143% throughput improvement. © 2015.",co-design; code generation; communication optimization; multiprocessor system-on-chip; Simulink; specification,Application specific integrated circuits; Codes (symbols); Cost benefit analysis; Microprocessor chips; Multiprocessing systems; Network components; Program compilers; Programmable logic controllers; Specifications; Static analysis; System-on-chip; Co-designs; Code Generation; Communication optimization; Multiprocessor system on chips; Simulink; Cost reduction
Temperature-aware data allocation for embedded systems with cache and scratchpad memory,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928411898&doi=10.1145%2f2629650&partnerID=40&md5=9d735ab958921f424f8f7a754d8fd219,"The hybrid memory architecture that contains both on-chip cache and scratchpad memory (SPM) has been widely used in embedded systems. In this article, we explore this hybrid memory architecture by jointly optimizing time performance and temperature for embedded systems with loops. Our basic idea is to adaptively adjust the workload distribution between cache and SPM based on the current temperature. For a problem in which the workload can be estimated a priori, we present a nonlinear programming formulation to optimally minimize the total execution time of a loop under the constraints of SPM size and temperature. To solve a problem in which the workload is not known a priori, we propose a temperature-aware adaptive loop scheduling algorithm called TALS to dynamically allocate data to cache and SPM at runtime. The experimental results show that our algorithms can effectively achieve both performance and temperature optimization for embedded systems with cache and SPM.",Algorithms; Design; Performance,Algorithms; Design; Embedded systems; Memory architecture; Nonlinear programming; Scheduling; Scheduling algorithms; Adaptive loops; Data allocation; On-chip cache; Performance; Scratch pad memory; Temperature aware; Temperature optimization; Work-load distribution; Cache memory
Virtual platform-based design space exploration of power-efficient distributed embedded applications,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929162108&doi=10.1145%2f2723161&partnerID=40&md5=bc9889f9792fdab0fca0e079f888877f,"Networked embedded systems are essential building blocks of a broad variety of distributed applications ranging from agriculture to industrial automation to healthcare and more. These often require specific energy optimizations to increase the battery lifetime or to operate using energy harvested from the environment. Since a dominant portion of power consumption is determined and managed by software, the software development process must have access to the sophisticated power management mechanisms provided by state-of-the-art hardware platforms to achieve the best tradeoff between system availability and reactivity. Furthermore, internode communications must be considered to properly assess the energy consumption. This article describes a design flow based on a SystemC virtual platform including both accurate power models of the hardware components and a fast abstract model of the wireless network. The platform allows both model-driven design of the application and the exploration of power and network management alternatives. These can be evaluated in different network scenarios, allowing one to exploit power optimization strategies without requiring expensive field trials. The effectiveness of the approach is demonstrated via experiments on a wireless body area network application. © 2015 ACM.",Model-based design; Network simulator; Power management techniques; Power/performance tradeoffs; Wireless sensor networks,Abstracting; Availability; Design; Embedded systems; Energy management; Energy utilization; Hardware; Logic design; Network management; Networks (circuits); Power management; Software design; Software engineering; Systems analysis; Wireless networks; Distributed applications; Model- based designs; Network simulators; Networked embedded systems; Power management techniques; Power/performance tradeoffs; Software development process; Wireless body area network; Wireless sensor networks
A probabilistic calculus for probabilistic real-time systems,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929157554&doi=10.1145%2f2717113&partnerID=40&md5=de0d9c2cb926c473aaf36c19e6494a36,"Challenges within real-time research are mostly in terms of modeling and analyzing the complexity of actual real-time embedded systems. Probabilities are effective in both modeling and analyzing embedded systems by increasing the amount of information for the description of elements composing the system. Elements are tasks and applications that need resources, schedulers that execute tasks, and resource provisioning that satisfies the resource demand. In this work, we present a model that considers component-based real-time systems with component interfaces able to abstract both the functional and nonfunctional requirements of components and the system. Our model faces probabilities and probabilistic real-time systems unifying in the same framework probabilistic scheduling techniques and compositional guarantees varying from soft to hard real time. We provide an algebra to work with the probabilistic notation developed and form an analysis in terms of sufficient probabilistic schedulability conditions for task systems with either preemptive fixed-priority or earliest deadline first scheduling paradigms. © 2015 ACM.",Probabilistic calculus; Probabilistic real-time modeling; Probabilistic real-time scheduling; Probabilistic real-time systems,Calculations; Embedded systems; Interactive computer systems; Scheduling; Scheduling algorithms; Component-based real-time systems; Earliest deadline first scheduling; Non-functional requirements; Probabilistic calculus; Probabilistic real-time systems; Real - time scheduling; Real time modeling; Real-time embedded systems; Real time systems
Optimized and scalable co-processor for McEliece with binary goppa codes,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928786943&doi=10.1145%2f2736284&partnerID=40&md5=0f53e0145e4646bb1245e63451c51752,"Asymmetric cryptographic primitives are essential to enable secure communications in public networks or public mediums. Such primitives can be deployed as software libraries or hardware co-processors, the latter beingmore commonly employed in systems on chip (SoC) scenarios, embedded devices, or application-specific servers. Unfortunately, the most commonly available solutions, based on RSA or elliptic curve cryptography (ECC), are highly processing intensive due to the underlying extended-precision modular arithmetic. Consequently, they are not available on highly constrained platforms. Aiming to tackle this issue, we here investigate an alternative asymmetric encryption scheme that relies on lightweight arithmetic: McEliece. This scheme is especially appealing because, being based on error correction codes, it displays a simpler arithmetic and leads to better performance when compared to RSA or ECC. To evaluate the implementation of this scheme in hardware, we propose and analyze a flexible architecture whose security level and time versus area usage characteristics can be reconfigured as desired. The proposed architecture is suitable to all usual security levels, ranging from 80 to 256 bits. It is also very efficient, being able to perform data decryption with binary Goppa codes in 56μs with 3,402 slices on a Xilinx Spartan-3AN FPGA, whereas the best-known result in the literature for the same FPGA is 115μs with 7,331 slices. Alternatively, the architecture can operate with quasi-dyadic Goppa (QD-Goppa) codes, which involves smaller keys than traditional binary Goppa codes. In the latter case, for an 80-bit security level, the decryption operation can take from 1.1ms with 1,129 slices to 68μs with 8,268 sices. By choosing a more hardware-friendly decoding algorithm, focusing hardware resources on most bottleneck operations and sharing hardware resource for two different algorithms, better results than the those in the literature were obtained. © 2015 ACM.",Code-based encryption; FPGA; Goppa codes; Hardware; Mceliece,Application programs; Computer hardware; Coprocessor; Curve fitting; Embedded systems; Error correction; Field programmable gate arrays (FPGA); Network architecture; Programmable logic controllers; Public key cryptography; System-on-chip; Cryptographic primitives; Decryption operations; Elliptic Curve Cryptography(ECC); Error correction codes; Flexible architectures; Goppa codes; McEliece; Proposed architectures; Codes (symbols)
"On the verification of concurrent, asynchronous programs with waiting queues",2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929149892&doi=10.1145%2f2700072&partnerID=40&md5=cef095916a9c0acfc55d274cf3704010,"Recently, new libraries, such as Grand Central Dispatch (GCD), have been proposed to directly harness the power of multicore platforms and to make the development of concurrent software more accessible to software engineers. When using such a library, the programmer writes so-called blocks, which are chunks of code, and dispatches them using synchronous or asynchronous calls to several types of waiting queues. A scheduler is then responsible for dispatching those blocks among the available cores. Blocks can synchronize via a global memory. In this article, we propose Queue-Dispatch Asynchronous Systems as a mathematical model that faithfully formalizes the synchronization mechanisms and behavior of the scheduler in those systems. We study in detail their relationships to classical formalisms such as pushdown systems, Petri nets, FIFO systems, and counter systems. Our main technical contributions are precise worst-case complexity results for the Parikh coverability problem and the termination problem for several subclasses of our model. We also consider an extension of QDAS with a fork-join mechanism. Adding fork-join to any of the subclasses that we have identified leads to undecidability of the coverability problem. This motivates the study of over-approximations. Finally, we consider handmade abstractions as a practical way of verifying programs that cannot be faithfully modeled by decidable subclasses of QDAS. © 2015 ACM.",Asynchronous programming; Waiting queues,Petri nets; Scheduling; Asynchronous programming; Coverability problem; Multi-core platforms; Synchronization mechanisms; Technical contribution; Termination problems; Waiting queues; Worst-case complexity; Queueing theory
Implementing QC-MDPC McEliece encryption,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928812245&doi=10.1145%2f2700102&partnerID=40&md5=01255fcf81e72cf04a1d18b5b483b7f4,"With respect to performance, asymmetric code-based cryptography based on binary Goppa codes has been reported as a highly interesting alternative to RSA and ECC. A major drawback is still the large keys in the range between 50 and 100KB that prevented real-world applications of code-based cryptosystems so far. A recent proposal by Misoczki et al. showed that quasi-cyclic moderate-density parity-check (QC-MDPC) codes can be used in McEliece encryption, reducing the public key to just 0.6KB to achieve an 80-bit security level. In this article, we provide optimized decoding techniques for MDPC codes and survey several efficient implementations of the QC-MDPC McEliece cryptosystem. This includes high-speed and lightweight architectures for reconfigurable hardware, efficient coding styles for ARM's Cortex-M4 microcontroller, and novel high-performance software implementations that fully employ vector instructions. Finally, we conclude that McEliece encryption in combination with QC-MDPC codes not only enables high-performance implementations but also allows for lightweight designs on a wide range of different platforms. © 2015 ACM.",Code-based cryptography; FPGA; Implementation; McEliece; MDPC codes; Microcontroller; Software,Computer software; Field programmable gate arrays (FPGA); Microcontrollers; Public key cryptography; Reconfigurable architectures; Reconfigurable hardware; Code-based cryptography; Efficient implementation; High performance implementations; Implementation; Lightweight architecture; McEliece; MDPC codes; Software implementation; Codes (symbols)
A novel memristor-based hardware security primitive,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928782283&doi=10.1145%2f2736285&partnerID=40&md5=db0a1ab95f36e43cb9797808c1144ce7,"Memristor is an exciting new addition to the repertoire of fundamental circuit elements. Alternatives tomany security protocols originally employing traditional mathematical cryptography involve novel hardware security primitives, such as Physically Unclonable Functions (PUFs). In this article, we propose a novel hybrid memristor-CMOS PUF circuit and demonstrate its suitability through extensive simulations of environmental and process variation effects. The proposed PUF circuit has substantially less hardware overhead than previously proposed memristor-based PUF circuits while being inherently resistant to machine learningbased modeling attacks because of challenge-dependent delays of the memristor stages. The proposed PUF can be conveniently used in many security applications and protocols based on hardware-intrinsic security. © 2015 ACM.",Hardware-intrinsic security; Memristor; Physically unclonable functions; Process variation; Security protocols,Cryptography; Delay circuits; Functions; Memristors; Network security; Extensive simulations; Fundamental circuits; Memristor; Physically unclonable functions; Process Variation; Security application; Security primitives; Security protocols; Hardware security
Design optimization of mixed-criticality real-time embedded systems,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929147058&doi=10.1145%2f2700103&partnerID=40&md5=4957ff6a39d6092df72b34516cd17060,"In this article, we are interested in implementing mixed-criticality real-time embedded applications on a given heterogeneous distributed architecture. Applications have different criticality levels, captured by their Safety-Integrity Level (SIL), and are scheduled using static-cyclic scheduling. According to certification standards, mixed-criticality tasks can be integrated onto the same architecture only if there is enough spatial and temporal separation among them. We consider that the separation is provided by partitioning, such that applications run in separate partitions, and each partition is allocated several time slots on a processor. Tasks of different SILs can share a partition only if they are all elevated to the highest SIL among them. Such elevation leads to increased development costs, which increase dramatically with each SIL. Tasks of higher SILs can be decomposed into redundant structures of lower SIL tasks. We are interested to determine (i) the mapping of tasks to processors, (ii) the assignment of tasks to partitions, (iii) the decomposition of tasks into redundant lower SIL tasks, (iv) the sequence and size of the partition time slots on each processor, and (v) the schedule tables, such that all the applications are schedulable and the development costs are minimized. We have proposed a Tabu Search-based approach to solve this optimization problem. The proposed algorithm has been evaluated using several synthetic and real-life benchmarks. © 2015 ACM.",Design optimization,Criticality (nuclear fission); Embedded systems; Integrated circuit design; Optimization; Safety engineering; Scheduling; Separation; Tabu search; Certification standards; Design optimization; Distributed architecture; Optimization problems; Real-time embedded systems; Safety integrity levels; Static cyclic scheduling; Tabu search based approaches; Real time systems
The future of real-time security: Latency-optimized lattice-based digital signatures,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929154385&doi=10.1145%2f2724714&partnerID=40&md5=77aae8a1fc916c95142f7dd043c1b031,"Advances in quantum computing have spurred a significant amount of research into public-key cryptographic algorithms that are resistant against postquantum cryptanalysis. Lattice-based cryptography is one of the important candidates because of its reasonable complexity combined with reasonable signature sizes. However, in a postquantum world, not only the cryptography will change but also the computing platforms. Large amounts of resource-constrained embedded systems will connect to a cloud of powerful server computers. We present an optimization technique for lattice-based signature generation on such embedded systems; our goal is to optimize latency rather than throughput. Indeed, on an embedded system, the latency of a single signature for user identification or message authentication is more important than the aggregate signature generation rate. We builda high-performance implementation using hardware/software codesign techniques. The key idea is to partition the signature generation scheme into offline and online phases. The signature scheme allows this separation because a large portion of the computation does not depend on the message to be signed and can be handled before the message is given. Then, we can map complex precomputation operations in software on a low-cost processor and utilize hardware resources to accelerate simpler online operations. To find the optimum hardware architecture for the target platform, we define and explore the design space and implement two design configurations. We realize our solutions on the Altera Cyclone-IV CGX150 FPGA. The implementation consists of a NIOS soft-core processor and a low-latency hash and polynomial multiplication engine. On average, the proposed low-latency architecture can generate a signature with a latency of 96 clock cycles at 40MHz, resulting in a response time of 2.4μs for a signing request. On equivalent platforms, this corresponds to a performance improvement of 33 and 105 times compared to previous hardware and software implementations, respectively. © 2015 ACM.",Digital signatures; FPGA; Hardware/software codesign; Lattice-based cryptography,Algorithms; Computer hardware; Cryptography; Electronic document identification systems; Embedded systems; Field programmable gate arrays (FPGA); Hardware; Hardware-software codesign; Integrated circuit design; Mobile security; Network security; Quantum computers; Social networking (online); Cryptographic algorithms; Hardware and software implementations; High performance implementations; Lattice-based cryptography; Message authentication; Optimization techniques; Polynomial multiplication; Resource-constrained embedded systems; Authentication
Practical lattice-based digital signature schemes,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928823050&doi=10.1145%2f2724713&partnerID=40&md5=230e516df160fc8bd7643fbdf5e4cbef,"Digital signatures are an important primitive for building secure systems and are used in most real-world security protocols. However, almost all popular signature schemes are either based on the factoring assumption (RSA) or the hardness of the discrete logarithm problem (DSA/ECDSA). In the case of classical cryptanalytic advances or progress on the development of quantum computers, the hardness of these closely related problems might be seriously weakened. A potential alternative approach is the construction of signature schemes based on the hardness of certain lattice problems that are assumed to be intractable by quantum computers. Due to significant research advancements in recent years, lattice-based schemes have now become practical and appear to be a very viable alternative to number-theoretic cryptography. In this article, we focus on recent developments and the current state of the art in lattice-based digital signatures and provide a comprehensive survey discussing signature schemes with respect to practicality. Additionally, we discuss future research areas that are essential for the continued development of lattice-based cryptography. © 2015 ACM.",Digital signatures; Lattices; Post-quantum cryptography,Crystal lattices; Electronic document identification systems; Hardness; Network security; Quantum cryptography; Qubits; Digital signature schemes; Discrete logarithm problems; Lattice problems; Lattice-based cryptography; Post quantum cryptography; Security protocols; Signature Scheme; State of the art; Authentication
Using network traffic to infer hardware state: A kernel-level investigation,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929151412&doi=10.1145%2f2700094&partnerID=40&md5=f427cbc8a6e77bf38405a84e625294be,"In this article, we illustrate that the boundary of a general-purpose node can be extended into the network by extracting information from network traffic generated by that general-purpose node to infer the state of its hardware components. This information is represented in a delay signature latent within the network traffic. In contrast, the traditional approach to determine the internal state of a node's resources meant that a software application with internal processes had to be resident on the node. The aforementioned delay signature is the keystone that provides a correlation between network traffic and the internal state of the source node. We characterize this delay signature by (1) identifying the different types of assembly language instructions that source this delay and (2) describing how architectural techniques, such as instruction pipelining and caching, give rise to this delay signature. In theory, highly utilized nodes (due to multiple threads) will contain excessive context switching and contention for shared resources. One important shared resource is main memory, and excessive use of this resource by applications and internal processes eventually leads to a decrease in cache efficiency that eventually stalls the instruction pipeline. Our results support this theory; specifically, we have observed that excessive context switching in active applications increases the effective memory access time and wastes precious CPU cycles, thus adding additional delay to the execution of load, store, and other instructions. Because the operating system (OS) kernel accesses memory to send network packets, the delay signature is induced into network traffic in situations where user-level utilization is high. We demonstrate this theory in two case studies: (1) resource discovery in cluster grids and (2) network-based detection of bitcoin mining on compromised nodes. © 2015 ACM.",Clusters assembly language instructions; Grid computing; LEON4 processor; Passive resource discovery,Application programs; Cache memory; Computation theory; Computational linguistics; Grid computing; Hardware; Assembly language; Extracting information; Hardware components; LEON4 processor; Memory access time; Resource discovery; Software applications; Traditional approaches; Network architecture
Global and partitioned multiprocessor fixed priority scheduling with deferred preemption,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928817867&doi=10.1145%2f2739954&partnerID=40&md5=ca79d26ef59c734d3e298af7f1f0df46,"This article introduces schedulability analysis for Global Fixed Priority Scheduling with Deferred Preemption (gFPDS) for homogeneous multiprocessor systems. gFPDS is a superset of Global Fixed Priority Preemptive Scheduling (gFPPS) and Global Fixed Priority Nonpreemptive Scheduling (gFPNS). We show how schedulability can be improved using gFPDS via appropriate choice of priority assignment and final nonpreemptive region lengths, and provide algorithms that optimize schedulability in this way. Via an experimental evaluation we compare the performance of multiprocessor scheduling using global approaches: gFPDS, gFPPS, and gFPNS, and also partitioned approaches employing FPDS, FPPS, and FPNS on each processor. © 2015 ACM.",Deferred preemption; Fixed priority; Global scheduling; Limited preemption; Multicore; Multiprocessor; Partitioned scheduling; Real-time,Multiprocessing systems; Deferred preemption; Fixed priorities; Global scheduling; Limited preemption; Multi core; Multiprocessor; Real time; Scheduling
Exploiting concurrency for the automated synthesis of MPSoC interconnects,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929168350&doi=10.1145%2f2700075&partnerID=40&md5=5fcb4e0b2ce921dd67267fc62f938479,"Multiprocessor Systems-on-Chip (MPSoC) applications can rely today on a very large spectrum of interconnection topologies potentially meeting given communication requirements, determining various trade-offs between cost and performance. Building interconnects that enable concurrent communication tasks introduces decisive opportunities for reducing the overall communication latency. This work identifies three levels of parallelism at the interconnect level: global parallelism across different independent domains; local or intradomain parallelism, relying on inherently concurrent interconnect components such as crossbars; and interdomain parallelism, where multiple concurrent paths across different local domains are exploited. We propose an automated methodology to search the design space, aimed at maximizing the exploitation of these forms of parallelism. The approach also takes into consideration possible dependencies between communication tasks, which further constrains the design space, making the identification of a feasible solution more challenging. By jointly solving a scheduling and interconnect synthesis problem, the methodology turns the description of the application communication requirements, including data dependencies, into an on-chip synthesizable interconnection structure along with a communication schedule satisfying given area constraints. The article thoroughly describes the formalisms and the methodology used to derive such optimized heterogeneous topologies. It also discusses some case studies emphasizing the impact of the proposed approach and highlighting the essential differences with a few other solutions presented in the technical literature. © 2015 ACM.",Communication scheduling; Design automation; Embedded systems; On-chip interconnects; Parallel architectures; Systems-on-chip,Automation; Computer aided design; Constraint satisfaction problems; Economic and social effects; Embedded systems; Fault tolerance; Grid computing; Integrated circuit design; Microprocessor chips; Multiprocessing systems; Parallel architectures; Scheduling; System-on-chip; Topology; Communication scheduling; Design automations; Heterogeneous topologies; Interconnection structure; Interconnection topologies; Multiprocessor systems on chips; On-chip interconnects; Systems on chips; Integrated circuit interconnects
Guaranteed computational resprinting via model-predictive control,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928800635&doi=10.1145%2f2724715&partnerID=40&md5=10dfb7a4b24424b9f70d75fa2b50a708,"Today and future many-core systems are facing the utilization wall and dark silicon problems, for which not all the processing engines can be powered at the same time as this will lead to a power consumption higher than the Total Design Power (TDP) budget. Recently, computational sprinting approaches addressed the problem by exploiting the intrinsic thermal capacitance of the chip and the properties of common applications, which require intense, but temporary, use of resources. The thermal capacitance, possibly augmented with phase change materials, enables the temporary activation of all the resources simultaneously, although they largely exceed the steady-state thermal design power. In this article, we present an innovative and lowoverhead hierarchical model-predictive controller for managing thermally safe sprinting with predictable resprinting rate, which ensures the correct execution of mixed-criticality tasks. Well-targeted simulations, also based on real workload benchmarks, show the applicability and the effectiveness of our solution. © 2015 ACM.",Computational sprinting; Dark silicon; MPSoC; Thermal control; Thermal model,Budget control; Capacitance; Hierarchical systems; Integrated circuit design; Phase change materials; System-on-chip; Computational sprinting; Dark silicons; MPSoC; Thermal control; Thermal model; Model predictive control
Introduction for embedded platforms for cryptography in the coming decade,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928822583&doi=10.1145%2f2745710&partnerID=40&md5=116a470bf6c28b4aeef984296dda8827,[No abstract available],,
Instruction-cache locking for improving embedded systems performance,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928814607&doi=10.1145%2f2700100&partnerID=40&md5=9dd1a8a1edaa08f9358f010245757c6e,"Cache memories in embedded systems play an important role in reducing the execution time of applications. Various kinds of extensions have been added to cache hardware to enable software involvement in replacement decisions, improving the runtime over a purely hardware-managed cache. Novel embedded systems, such as Intel's XScale and ARM Cortex processors, facilitate locking one or more lines in cache; this feature is called cache locking. We present a method in for instruction-cache locking that is able to reduce the average-case runtime of a program. We demonstrate that the optimal solution for instruction cache locking can be obtained in polynomial time. However, a fundamental lack of correlation between cache hardware and software program points renders such optimal solutions impractical. Instead, we propose two practical heuristics-based approaches to achieve cache locking. First, we present a static mechanism for locking the cache, in which the locked contents of the cache are kept fixed over the execution of the program. Next, we present a dynamic mechanism that accounts for changing program requirements at runtime. We devise a cost-benefit model to discover the memory addresses that should be locked in the cache. We implement our scheme inside a binary rewriter, widening the applicability of our scheme to binaries compiled using any compiler. Results obtained on a suite of MiBench benchmarks show that our static mechanism results in 20% improvement in the instruction-cache miss rate on average and up to 18% improvement in the execution time on average for applications having instruction accesses as a bottleneck, compared to no cache locking. The dynamic mechanism improves the cache miss rate by 35% on average and execution time by 32% on instruction-cache-constrained applications. © 2015 ACM.",Binary rewriting; Cache locking; Caches; Embedded systems,Benchmarking; Cost benefit analysis; Embedded systems; Locks (fasteners); Optimal systems; Optimization; Polynomial approximation; Binary rewriting; Cache locking; Caches; Cost-benefit models; Hardware and software; Instruction cache miss; Instruction caches; Systems performance; Cache memory
Modeling and analyzing dataflow applications on NoC-based many-core architectures,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928783055&doi=10.1145%2f2700081&partnerID=40&md5=9ab9d4e711da79fd40ee4dc3f88d85b0,"The advent of chip-level parallel architectures prompted a renewal of interest into dataflow process networks. The trend is to model an application independently from the architecture, then the model is morphed to best fit the target architecture. One downplayed aspect is the mapping of communications through the on-chip topology. The cost of such communications is often prevalent with regard to computations. This article establishes a dataflow process network called K-periodically Routed Graph (KRG), which serves the role of representing the various routing decisions during the transformation of a genuine application into a architecture-aware version for this application. © 2015 ACM.",Dataflow process network; Dequation algorithm architecture; Networkon-chip; Routing,Data flow analysis; Network architecture; Network routing; Parallel architectures; Algorithm architectures; Chip-level; Dataflow; Dataflow process networks; Many-core architecture; Routing; Routing decisions; Target architectures; Network-on-chip
On constrained implementation of lattice-based cryptographic primitives and schemes on smart cards,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928790374&doi=10.1145%2f2700078&partnerID=40&md5=892360713b5b6e67a6ae2f4dcc02067b,"Most lattice-based cryptographic schemes with a security proof suffer from large key sizes and heavy computations. This is also true for the simpler case of authentication protocols that are used on smart cards as a very-constrained computing environment. Recent progress on ideal lattices has significantly improved the efficiency and made it possible to implement practical lattice-based cryptography on constrained devices. However, to the best of our knowledge, no previous attempts have been made to implement lattice-based schemes on smart cards. In this article, we provide the results of our implementation of several state-of-the art lattice-based authentication protocols on smart cards and a microcontroller widely used in smart cards. Our results show that only a few of the proposed lattice-based authentication protocols can be implemented using limited resources of such constrained devices; however, cutting-edge ones are suitably efficient to be used practically on smart cards. Moreover, we have implemented fast Fourier transform (FFT) and discrete Gaussian sampling with different typical parameter sets, as well as versatile lattice-based public-key encryptions. These results have noticeable points that help to design or optimize lattice-based schemes for constrained devices. © 2015 ACM.",Authentication protocol; Constrained device; Constrained implementation; Lattice-based cryptography; Postquantum cryptography,Authentication; Fast Fourier transforms; Hardware security; Public key cryptography; Authentication protocols; Constrained devices; Constrained implementation; Lattice-based cryptography; Post quantum cryptography; Smart cards
Energy modeling of software for a hardware multithreaded embedded microprocessor,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929169023&doi=10.1145%2f2700104&partnerID=40&md5=e210d4346b7b6852eefce3e1ec34f082,"This article examines a hardware multithreaded microprocessor and discusses the impact such an architecture has on existing software energy modeling techniques. A framework is constructed for analyzing the energy behavior of the XMOS XS1-L multithreaded processor and a variation on existing software energy models is proposed, based on analysis of collected energy data. It is shown that by combining execution statistics with sufficient data on the processor's thread activity and instruction execution costs, a multithreaded software energy model used with Instruction Set Simulation can yield an average error margin of less than 7%. © 2015 ACM.",Computer architecture; Embedded systems; ISA-level energy modeling; Multithreading; Software energy modeling; XMOS XS1 xCORE,Computer hardware; Computer software; Embedded systems; Error statistics; Hardware; Microprocessor chips; Multitasking; Embedded microprocessors; Energy model; Execution costs; Instruction set simulations; Multi-threading; Multithreaded processors; Multithreaded softwares; XMOS XS1 xCORE; Computer architecture
Finite-state-machine overlay architectures for fast FPGA compilation and application portability,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928813525&doi=10.1145%2f2700082&partnerID=40&md5=1004d3251a7f65ef03a0c2cba3ab7a52,"Despite significant advantages, wider usage of field-programmable gate arrays (FPGAs) has been limited by lengthy compilation and a lack of portability. Virtual-architecture overlays have partially addressed these problems, but previous work focuses mainly on heavily pipelined applications with minimal control requirements.We expand previous work by enablingmore flexible control via overlay architectures for finitestate machines. Although not appropriate for control-intensive circuits, the presented architectures reduced compilation times of control changes in a convolution case study from 7 hours to less than 1 second, with no performance overhead and an area overhead of 0.2%. © 2015 ACM.",FPGA; Intermediate fabrics; Overlays; Virtual architecture,Pavement overlays; Software engineering; Application portability; Area overhead; Finite-state; Flexible control; Overlay architecture; Virtual architecture; Field programmable gate arrays (FPGA)
Stability of online resource managers for distributed systems under execution time variations,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926294771&doi=10.1145%2f2629495&partnerID=40&md5=79442739df9c784f1f64cdcff7e73621,"Today's embedded systems are exposed to variations in resource usage due to complex software applications, hardware platforms, and impact of the runtime environments. When these variations are large and efficiency is required, on-line resource managers may be deployed on the system to help it control its resource usage. An often neglected problem is whether these resource managers are stable, meaning that the resource usage is controlled under all possible scenarios. In distributed systems, this problem is particularly hard because applications distributed over many resources generate complex dependencies between their resources. Inthis article, we develop a mathematical model of the system, and derive conditions that, if satisfied, guarantee stability. © 2015 ACM.",Adaptive real-time systems; Control theory; Distributed systems; Stability criterion,Adaptive control systems; Application programs; Control theory; Embedded systems; Interactive computer systems; Managers; Online systems; Social networking (online); Stability criteria; System stability; Adaptive real-time systems; Complex software applications; Distributed systems; Hardware platform; Online resources; Resource managers; Runtime environments; Time variations; Real time systems
Multilevel phase analysis,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926351304&doi=10.1145%2f2629594&partnerID=40&md5=226866bc8e7e443548f7fb8c9cabc039,"Phase analysis, which classifies the set of execution intervals with similar execution behavior and resource requirements, has been widely used in a variety of systems, including dynamic cache reconfiguration, prefetching, race detection, and sampling simulation. Although phase granularity has been a major factor in the accuracy of phase analysis, it has not been well investigated, and most systems usually adopt a fine-grained scheme. However, such a scheme can only take account of recent local phase information and could be frequently interfered by temporary noise due to instant phase changes, which might notably limit the accuracy. In this article, we make the first investigation on the potential of multilevel phase analysis (MLPA), where different granularity phase analyses are combined together to improve the overall accuracy. The key observation is that the coarse-grained intervals belonging to the same phase usually consist of stably distributed fine-grained phases. Moreover, the phase of a coarse-grained interval can be accurately identified based on the fine-grained intervals at the beginning of its execution. Based on the observation, we design and implement an MLPA scheme. In such a scheme, a coarse-grained phase is first identified based on the finegrained intervals at the beginning of its execution. The following fine-grained phases in it are then predicted based on the sequence of fine-grained phases in the coarse-grained phase. Experimental results show that such a scheme can notably improve the prediction accuracy. Using a Markov fine-grained phase predictor as the baseline, MLPA can improve prediction accuracy by 20%, 39%, and 29% for next phase, phase change, and phase length prediction for SPEC2000, respectively, yet incur only about 2% time overhead and 40% space overhead (about 360 bytes in total). To demonstrate the effectiveness of MLPA, we apply it to a dynamic cache reconfiguration system that dynamically adjusts the cache size to reduce the power consumption and access time of the data cache. Experimental results show that MLPA can further reduce the average cache size by 15% compared to the fine-grained scheme. Moreover, for MLPA, we also observe that coarse-grained phases can better capture the overall program characteristics with fewer of phases and the last representative phase could be classified in a very early program position, leading to fewer execution internals being functionally simulated. Based on this observation, we also design a multilevel sampling simulation technique that combines both fine- and coarse-grained phase analysis for sampling simulation. Such a scheme uses fine-grained simulation points to represent only the selected coarse-grained simulation points instead of the entire program execution; thus, it could further reduce both the functional and detailed simulation time. Experimental results show that MLPA for sampling simulation can achieve a speedup in simulation time of about 8.3X with similar accuracy compared to 10M SimPoint. © 2015 ACM.",Dynamic prediction; Multilevel; Phase; Sampling; Simulation,Reboilers; Sampling; Design and implements; Different granularities; Dynamic prediction; Multilevel; Multilevel samplings; Phase; Resource requirements; Simulation; Forecasting
Towards write-activity-aware page table management for non-volatile main memories,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923641216&doi=10.1145%2f2697394&partnerID=40&md5=e3b27a9c0f98c547a9b085ed16b61d58,"Non-volatile memories such as phase change memory (PCM) and memristor are being actively studied as an alternative to DRAM-based main memory in embedded systems because of their properties, which include low power consumption and high density. Though PCM is one of the most promising candidates with commercial products available, its adoption has been greatly compromised by limited write endurance. As main memory is one of the most heavily accessed components, it is critical to prolong the lifetime of PCM. In this article, we present write-activity-aware page table management (WAPTM), a simple yet effective page table management scheme for reducing unnecessary writes, by redesigning system software and exploiting write-activity-aware features provided by the hardware. We implemented WAPTM in Google Android based on the ARM architecture and evaluated it with real Android applications. Experimental results show that WAPTM can significantly reduce writes in page tables, proving the feasibility and potential of prolonging the lifetime of PCM-based main memory through reducing writes at the OS level. © 2015 ACM",Memory management; Non-volatile memory; Operating systems; Phase change memory; Write activity aware,Android (operating system); Computer operating systems; Dynamic random access storage; Embedded systems; Android applications; Low-power consumption; Memory management; Non-volatile main memory; Non-volatile memory; Phase change memory (pcm); Re-designing systems; Write activity aware; Phase change memory
"ACDC: Small, predictable and high-performance data cache",2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923675974&doi=10.1145%2f2677093&partnerID=40&md5=14900394b974e1b2ec685666cafa3d57,"In multitasking real-time systems, the worst-case execution time (WCET) of each task and also the effects of interferences between tasks in the worst-case scenario need to be calculated. This is especially complex in the presence of data caches. In this article, we propose a small instruction-driven data cache (256 bytes) that effectively exploits locality. It works by preselecting a subset of memory instructions that will have data cache replacement permission. Selection of such instructions is based on data reuse theory. Since each selected memory instruction replaces its own data cache line, it prevents pollution and performance in tasks becomes independent of the size of the associated data structures. We have modeled several memory configurations using the Lock-MS WCET analysis method. Our results show that, on average, our data cache effectively services 88% of program data of the tested benchmarks. Such results double the worst-case performance of our tested multitasking experiments. In addition, in the worst case, they reach between 75% and 89% of the ideal case of always hitting in instruction and data caches. As well, we show that using partitioning on our proposed hardware only provides marginal benefits in worst-case performance, so using partitioning is discouraged. Finally, we study the viability of our proposal in the MiBench application suite by characterizing its data reuse, achieving hit ratios beyond 90% in most programs. © 2015 ACM Worst-case analysis.",,Application programs; Computer software reusability; Interactive computer systems; Multitasking; Real time systems; Software testing; Data caches; Marginal benefit; Memory configuration; Performance data; Wcet analysis; Worst case scenario; Worst-case execution time; Worst-case performance; Cache memory
Runtime optimization of system utility with variable hardware,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923697598&doi=10.1145%2f2656338&partnerID=40&md5=5c1a5d3f140a085ace3f77697d87d1e9,"Increasing hardware variability in newer integrated circuit fabrication technologies has caused corresponding power variations on a large scale. These variations are particularly exaggerated for idle power consumption, motivating the need to mitigate the effects of variability in systems whose operation is dominated by long idle states with periodic active states. In systems where computation is severely limited by anemic energy reserves and where a long overall system lifetime is desired, maximizing the quality of a given application subject to these constraints is both challenging and an important step toward achieving high-quality deployments. This work describes VaRTOS, an architecture and corresponding set of operating system abstractions that provide explicit treatment of both idle and active power variations for tasks running in real-time operating systems. Tasks in VaRTOS express elasticity by exposing individual knobs-shared variables that the operating system can tune to adjust task quality and, correspondingly, task power, maximizing application utility both on a per-task and on a system-wide basis. We provide results regarding online learning of instance-specific sleep power, active power, and task-level power expenditure on simulated hardware with demonstrated effects for several prototypical applications. Our results on networked sensing applications, which are representative of a broader category of applications that VaRTOS targets, show that VaRTOS can reduce variability-induced energy expenditure errors from over 70% in many cases to under 2% in most cases and under 5% in the worst case. © 2015 ACM",Embedded operating systems; Power consumption; Variability,Computer operating systems; Electric power utilization; Embedded systems; Embedded operating systems; Explicit treatments; Integrated circuit fabrication technology; Real time operating system; Runtime optimization; Sensing applications; System lifetimes; Variability; Real time systems
Libra: Software-controlled cell bit-density to balance wear in NAND flash,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923659040&doi=10.1145%2f2638552&partnerID=40&md5=98ae23eefbba4270fe895525e7d90832,"Hybrid flash storages combine a small Single-Level Cell (SLC) partition with a large Multilevel Cell (MLC) partition. Compared to MLC-only solutions, the SLC partition exploits fast and short local write updates, while the MLC part brings large capacity. On the whole, hybrid storage achieves a tangible performance improvement for a moderate extra cost. Yet, device lifetime is an important aspect often overlooked: in a hybrid system, a large ratio of writes may be directed to the small SLC partition, thus generating a local stress that could exhaust the SLC lifetime significantly sooner than the MLC partition's. To address this issue, we propose Libra, which builds on flash storage made solely of MLC flash and uses the memory devices in SLC mode when appropriate; that is, we exploit the fact that writing a single bit per cell in an MLC provides characteristics close to those of an ordinary SLC. In our scheme, the cell bit-density of a block can be decided dynamically by the flash controller, and the physical location of the SLC partition can now be moved around the whole device, balancing wear across it. This article provides a thorough analysis and characterization of the SLC mode forMLCs and gives evidence that the inherent flexibility provided by Libra simplifies considerably the stress balance on the device. Overall, our technique improves lifetime by up to one order of magnitude at no cost when compared to any hybrid storage that relies on a static SLC-MLC partitioning. © 2015 ACM",Endurance; Lifetime; MLC; NAND flash memory; SLC; Wear balancing; Wear leveling,Aspect ratio; Cells; Cytology; Durability; Hybrid systems; Memory architecture; NAND circuits; Wear of materials; Analysis and characterization; Inherent flexibility; Lifetime; Multi level cell (MLC); NAND flash memory; Physical locations; Single level cells; Wear leveling; Flash memory
A real-time multichannel memory controller and optimal mapping of memory clients to memory channels,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923668300&doi=10.1145%2f2661635&partnerID=40&md5=bc8a03ea26646ab64e1f8e4a8a3f2ee2,"Ever-increasing demands for main memory bandwidth and memory speed/power tradeoff led to the introduction of memories with multiple memory channels, such as Wide IO DRAM. Efficient utilization of a multichannel memory as a shared resource in multiprocessor real-time systems depends on mapping of the memory clients to the memory channels according to their requirements on latency, bandwidth, communication, and memory capacity. However, there is currently no real-time memory controller for multichannel memories, and there is no methodology to optimally configure multichannel memories in real-time systems. As a first work toward this direction, we present two main contributions in this article: (1) a configurable real-time multichannel memory controller architecture with a novel method for logical-to-physical address translation and (2) two design-time methods to map memory clients to the memory channels, one an optimal algorithm based on an integer programming formulation of the mapping problem, and the other a fast heuristic algorithm. We demonstrate the real-time guarantees on bandwidth and latency provided by our multichannel memory controller architecture by experimental evaluation. Furthermore, we compare the performance of the mapping problem formulation in a solver and the heuristic algorithm against two existing mapping algorithms in terms of computation time and mapping success ratio. We show that an optimal solution can be found in 2 hours using the solver and in less than 1 second with less than 7% mapping failure using the heuristic for realistically sized problems. Finally, we demonstrate configuring a Wide IO DRAM in a high-definition (HD) video and graphics processing system to emphasize the practical applicability and effectiveness of this work. © 2015 ACM",Heuristic algorithm; Memory controller; Multichannel memories; Optimal mapping,Bandwidth; Conformal mapping; Controllers; Digital television; Dynamic random access storage; Heuristic algorithms; Heuristic methods; Integer programming; Interactive computer systems; Physical addresses; Program translators; Experimental evaluation; Fast heuristic algorithms; High-definition videos; Integer programming formulations; Memory controller; Multichannel; Optimal mapping; Real time guarantees; Real time systems
Using a flexible fault-tolerant cache to improve reliability for ultra low voltage operation,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923659042&doi=10.1145%2f2629566&partnerID=40&md5=b6dde124bc5c339e106a3252b6a4c9e3,"Caches are known to consume a large part of total microprocessor power. Traditionally, voltage scaling has been used to reduce both dynamic and leakage power in caches. However, aggressive voltage reduction causes process-variation-induced failures in cache SRAM arrays, which compromise cache reliability. In this article, we propose FFT-Cache, a flexible fault-tolerant cache that uses a flexible defect map to configure its architecture to achieve significant reduction in energy consumption through aggressive voltage scaling while maintaining high error reliability. FFT-Cache uses a portion of faulty cache blocks as redundancy-using block-level or line-level replication within or between sets-to tolerate other faulty caches lines and blocks. Our configuration algorithm categorizes the cache lines based on degree of conflict between their blocks to reduce the granularity of redundancy replacement. FFT-Cache thereby sacrifices a minimal number of cache lines to avoid impacting performance while tolerating the maximum amount of defects. Our experimental results on a processor executing SPEC2K benchmarks demonstrate that the operational voltage of both L1/L2 caches can be reduced down to 375 mV, which achieves up to 80% reduction in the dynamic power and up to 48% reduction in the leakage power. This comes with only a small performance loss (<%5) and 13% area overhead. © 2015 ACM",Aggressive voltage scaling; Fault-tolerant cache; Low voltage operation; Remapping,Cache memory; Defects; Energy utilization; Facsimile; Fast Fourier transforms; Redundancy; Voltage scaling; Aggressive voltage scaling; Configuration algorithm; Fault-tolerant; Low voltage operation; Operational voltage; Process Variation; Reduction in energy consumption; Remapping; Fault tolerance
A Java processor IP design for embedded SoC,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923659045&doi=10.1145%2f2629649&partnerID=40&md5=ed1b36c4d714db4896d7aff151b27d7e,"In this article, we present a reusable Java processor IP for application processors of embedded systems. For the Java microarchitecture, we propose a low-cost stack memory design that supports a two-fold instruction folding pipeline and a low-complexity Java exception handling hardware.We also propose a mapping between the Java dynamic class loading model and the SoC platform-based design principle so that the Java core can be encapsulated as a reusable IP. To achieve this goal, a two-level method area with two on-chip circular buffers is proposed as an interface between the RISC core and the Java core. The proposed architecture is implemented on a Xilinx Virtex-5 FPGA device. Experimental results show that its performance has some advantages over other Java processors and a Java VM with JIT acceleration on a PowerPC platform. © 2015 ACM",Application processor soc; Dynamic class loading; Embedded systems; Java accelerator,Computer software reusability; Embedded systems; Internet protocols; Java programming language; Programmable logic controllers; System-on-chip; Application processors; Dynamic class loading; Java accelerators; Java exceptions; Java processors; Micro architectures; Proposed architectures; Two level methods; Integrated circuit design
Mobile computations with surrounding devices: Proximity sensing and multilayered work stealing,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923677679&doi=10.1145%2f2656214&partnerID=40&md5=fdbfc06240f0e238d93d60b0fde4f55a,"With the proliferation of mobile devices, and their increasingly powerful embedded processors and storage, vast resources increasingly surround users. We have been investigating the concept of on-demand ad hoc forming of groups of nearby mobile devices in the midst of crowds to cooperatively perform computationally intensive tasks as a service to local mobile users, or what we call mobile crowd computing. As devices can vary in processing power and some can leave a group unexpectedly or new devices join in, there is a need for algorithms that can distributework in a flexiblemanner and still work with different arrangements of devices that can arise in an ad hoc fashion. In this article, we first argue for the feasibility of such use of crowdembedded computations using theoretical justifications and reporting on our experiments on Bluetoothbased proximity sensing. We then present a multilayered work-stealing style algorithm for distributing work efficiently among mobile devices and compare speedups attainable for different topologies of devices networked with Bluetooth, justifying a topology-flexible opportunistic approach. While our experiments are with Bluetooth and mobile devices, the approach is applicable to ecosystems of various embedded devices with powerful processors, networking technologies, and storage that will increasingly surround users. © 2015 ACM",Crowd computing; Mobile computations; Mobile infrastructure; Work stealing,Bluetooth; Topology; Crowd computing; Embedded processors; Mobile computations; Mobile infrastructure; Networking technology; Processing power; Proximity sensing; Work stealing; Mobile telecommunication systems
Joint WCET and update activity minimization for cyber-physical systems,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988273235&doi=10.1145%2f2680539&partnerID=40&md5=b07c4b99aff3c59cbb0baff95e22bfa4,"A cyber-physical system (CPS) is a desirable computing platform for many industrial and scientific applications, such as industrial process monitoring, environmental monitoring, chemical processes, and battlefield surveillance. The application of CPSs has two challenges: First, CPSs often include a number of sensor nodes. Update of preloaded code on remote sensor nodes powered by batteries is extremely energy consuming. The code update issue in the energy-sensitive CPS must be carefully considered. Second, CPSs are often real-time embedded systems with real-time properties.Worst-case execution time (WCET) is one of themost important metrics in real-time system design. Whereas existing works only consider one of these two challenges at a time, in this article, a compiler optimization-joint WCET and update-conscious compilation, or WUCC-is proposed to jointly consider WCET and code update for CPSs. The novelty of the proposed approach is that the WCET problem and code update problem are considered concurrently such that a balanced solution with minimal WCET and minimal code difference can be achieved. The experimental results show that the proposed technique can minimize WCET and code difference effectively. © 2015 ACM 1539-9087/2015/01-ART2 $15.00.",Code similarity; Compilation; Cyber-physical system; Register allocation; WCET,Codes (symbols); Cyber Physical System; Interactive computer systems; Process monitoring; Real time systems; Sensor nodes; Code similarities; Compilation; Cyber-Physical System (CPS); Environmental Monitoring; Industrial process monitoring; Real-time embedded systems; Register allocation; WCET; Embedded systems
Automatic update of indoor location fingerprints with pedestrian dead reckoning,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923690527&doi=10.1145%2f2667226&partnerID=40&md5=58479cd6234d1175c975b5696c467955,"In this article, we propose a new method for automatically updating a Wi-Fi indoor positioning model on a cloud server by employing uploaded sensor data obtained from the smartphone sensors of a specific user who spends a lot of time in a given environment (e.g., a worker in the environment). In this work, we attempt to track the user with pedestrian dead reckoning techniques, and at the same time we obtain Wi-Fi scan data from a mobile device possessed by the user. With the scan data and the estimated coordinates uploaded to a cloud server, we can automatically create a pair consisting of a scan and its corresponding indoor coordinates during the user's daily life and update an indoor positioning model on the server by using the information. With this approach, we try to cope with the instability of Wi-Fi-based positioning methods caused by changing environmental dynamics, that is, layout changes and moving or removal of Wi-Fi access points. Therefore, ordinary users (e.g., customers) who do not have rich sensors can benefit from the continually updating positioning model. © 2015 ACM",Fingerprinting; Indoor positioning; Pedestrian dead reckoning; Sensors; Smartphones; Wi-Fi positioning,Cloud computing; Indoor positioning systems; Navigation; Sensors; Smartphones; Automatic updates; Environmental dynamics; Fingerprinting; Indoor positioning; Pedestrian dead reckonings; Positioning methods; Wi-fi access points; Wi-Fi Positioning; Wireless local area networks (WLAN)
A hardware-efficient architecture for accurate real-time disparity map estimation,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923675968&doi=10.1145%2f2629699&partnerID=40&md5=137876e6c39692f84e9c74a617c5190a,"Emerging embedded vision systems utilize disparity estimation as a means to perceive depth information to intelligently interact with their host environment and take appropriate actions. Such systems demand high processing performance and accurate depth perception while requiring low energy consumption, especially when dealing with mobile and embedded applications, such as robotics, navigation, and security. The majority of real-time dedicated hardware implementations of disparity estimation systems have adopted local algorithms relying on simple cost aggregation strategies with fixed and rectangular correlation windows. However, such algorithms generally suffer from significant ambiguity along depth borders and areas with low texture. To this end, this article presents the hardware architecture of a disparity estimation system that enables good performance in both accuracy and speed. The architecture implements an adaptive support weight stereo correspondence algorithm that integrates image segmentation information in an attempt to increase the robustness of the matching process. The article also presents hardware-oriented algorithmic modifications/optimization techniques that make the algorithm hardware-friendly and suitable for efficient dedicated hardware implementation. A comparison to the literature asserts that an FPGA implementation of the proposed architecture is among the fastest implementations in terms of million disparity estimations per second (MDE/s), and with an overall accuracy of 90.21%, it presents an effective processing speed/disparity map accuracy trade-off. © 2015 ACM",Accurate depth perception; Disparity map estimation; FPGA; Parallel architecture; Stereo vision,Depth perception; Economic and social effects; Embedded systems; Energy utilization; Field programmable gate arrays (FPGA); Image coding; Image segmentation; Parallel architectures; Real time systems; Robots; Stereo vision; Textures; Adaptive support weights; Disparity map; Efficient architecture; Low energy consumption; Processing performance; Proposed architectures; Segmentation informations; Stereo correspondences; Stereo image processing
Plugging versus logging: Adaptive buffer management for hybrid-mapping SSDs,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923651409&doi=10.1145%2f2629455&partnerID=40&md5=dd8d4e0afb12572b6ee95a9c1593cd54,"A promising technique to improve the write performance of solid-state disks (SSDs) is to use a disk write buffer. The goals of a write buffer is not only to reduce the write traffic to the flash chips but also to convert host write patterns into long and sequential write bursts. This study proposes a new buffer design consisting of a replacement policy and a write-back policy. The buffer monitors how the host workload stresses the flash translation layer upon garbage collection. This is used to dynamically adjust its replacement and writeback strategies for a good balance between write sequentiality and write randomness. When the garbage collection overhead is low, the write buffer favors high write sequentiality over low write randomness. When the flash translation layer observes a high overhead of garbage collection, the write buffer favors low write randomness over high write sequentiality. The proposed buffer design outperformed existing approaches by up to 20% under various workloads and flash translation algorithms, as will be shown in experiment results. © 2015 ACM",Flash memory; Solid-state disks; Write buffering,Random processes; Refuse collection; Adaptive buffer; Flash translation layer; Garbage collection; Replacement policy; Solid state disks; Translation algorithms; Write buffering; Write-back strategies; Flash memory
Heuristics on reachability trees for bicriteria scheduling of stream graphs on heterogeneous multiprocessor architectures,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923684261&doi=10.1145%2f2638553&partnerID=40&md5=6477c3d2afdf97bd6abedc8a2a6aa7ef,"In this article, we partition and schedule Synchronous Dataflow (SDF) graphs onto heterogeneous execution architectures in such a way as to minimize energy consumption and maximize throughput. Partitioning and scheduling SDF graphs onto homogeneous architectures is a well-known NP-hard problem. The heterogeneity of the execution architecture makes our problem exponentially challenging to solve. We model the problem as a weighted sum and solve it using novel state space exploration inspired from the theory of parallel automata. The resultant heuristic algorithm results in good scheduling when implemented in an existing stream framework. © 2015 ACM",Data parallelism; Optimization; Stream graphs; Task parallelism,Data flow analysis; Data streams; Energy utilization; Graphic methods; Heuristic algorithms; NP-hard; Optimization; Response time (computer systems); Scheduling; Space research; Bicriteria scheduling; Data parallelism; Heterogeneous multiprocessors; Maximize throughput; State space exploration; Stream graphs; Synchronous Dataflow; Task parallelism; Trees (mathematics)
Placement of linked dynamic data structures over heterogeneous memories in embedded systems,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923684267&doi=10.1145%2f2656208&partnerID=40&md5=345aaeeda2b8a2cffe03351f735e0381,"Software applications use dynamic memory (allocated and deallocated in the system's heap) to handle dynamism in their working conditions. Embedded systems tend to include complex memory organizations but most techniques for dynamic memory management do not deal with the placement of data objects in physical memory modules. Additionally, the performance of hardware-controlled cache memories may be severely hindered when used with linked data structures. We therefore present a methodology to map dynamic data on the multilevel memory subsystem of embedded systems, taking advantage of any available memories (e.g., on-chip SRAMs) and avoiding interference with the cache memories. The resulting data placement uses an exclusive memory model and is compatible with existing techniques for managing static data. Our methodology helps the designer achieve reductions in energy consumption and execution time that can be obtained by an expert in an automated way while keeping control over the process through multiple configuration knobs.",Design; Efficiency; Embedded; Memory management; Memory organization,Application programs; Data structures; Design; Efficiency; Embedded systems; Energy utilization; Information management; Memory architecture; Static random access storage; Dynamic data structure; Dynamic memory management; Embedded; Heterogeneous memory; Memory management; Memory organizations; Multiple configurations; Software applications; Cache memory
Enhancing design space exploration by extending CPU/GPU specifications onto FPGAs,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923684266&doi=10.1145%2f2656207&partnerID=40&md5=99091de67b4bb4995c9b350d27bf776f,"The design cycle for complex special-purpose computing systems is extremely costly and time-consuming. It involves a multiparametric design space exploration for optimization, followed by design verification. Designers of special purpose VLSI implementations often need to explore parameters, such as optimal bitwidth and data representation, through time-consuming Monte Carlo simulations. A prominent example of this simulation-based exploration process is the design of decoders for error correcting systems, such as the Low-Density Parity-Check (LDPC) codes adopted by modern communication standards, which involves thousands of Monte Carlo runs for each design point. Currently, high-performance computing offers a wide set of acceleration options that range from multicore CPUs to Graphics Processing Units (GPUs) and Field Programmable Gate Arrays (FPGAs). The exploitation of diverse target architectures is typically associated with developing multiple code versions, often using distinct programming paradigms. In this context, we evaluate the concept of retargeting a single OpenCL program to multiple platforms, thereby significantly reducing design time. A single OpenCL-based parallel kernel is used without modifications or code tuning on multicore CPUs, GPUs, and FPGAs. We use SOpenCL (Silicon to OpenCL), a tool that automatically converts OpenCL kernels to RTL in order to introduce FPGAs as a potential platform to efficiently execute simulations coded in OpenCL. We use LDPC decoding simulations as a case study. Experimental results were obtained by testing a variety of regular and irregular LDPC codes that range from short/medium (e.g., 8,000 bit) to long length (e.g., 64,800 bit) DVB-S2 codes. We observe that, depending on the design parameters to be simulated, on the dimension and phase of the design, the GPU or FPGA may suit different purposes more conveniently, thus providing different acceleration factors over conventional multicore CPUs. © 2015 ACM",,Computer graphics; Decoding; Field programmable gate arrays (FPGA); Forward error correction; Graphics processing unit; Intelligent systems; Monte Carlo methods; Multicore programming; Program processors; Satellite communication systems; Acceleration factors; Communication standards; Design space exploration; Field programmable gate array (FPGAs); High performance computing; Irregular LDPC codes; Low-density parity-check (LDPC) codes; Programming paradigms; Integrated circuit design
Factored planning: From automata to Petri nets,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923684265&doi=10.1145%2f2656215&partnerID=40&md5=917e96240dd9f4e46e606e6469185e6c,"Factored planning mitigates the state explosion problem by avoiding the construction of the state space of the whole system and instead working with the system's components. Traditionally, finite automata have been used to represent the components, with the overall system being represented as their product. In this article, we change the representation of components to safe Petri nets. This allows one to use cheap structural operations like transition contractions to reduce the size of the Petri net before its state space is generated, which often leads to substantial savings compared with automata. The proposed approach has been implemented and proved efficient on several factored planning benchmarks. This article is an extended version of our ACSD 2013 paper [Jezequel et al. 2013], with the addition of the proofs and the experimental results of Sections 6 and 7. © 2015 ACM",Factored planning; Weighted petri nets,Robots; Extended versions; State explosion problems; Petri nets
A hardware framework for yield and reliability enhancement in chip multiprocessors,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921927371&doi=10.1145%2f2629688&partnerID=40&md5=c40c172b83f6abebea9110b87064c472,"Device reliability and manufacturability have emerged as dominant concerns in end-of-road CMOS devices. An increasing number of hardware failures are attributed to manufacturability or reliability problems. Maintaining an acceptable manufacturing yield for chips containing tens of billions of transistors with wide variations in device parameters has been identified as a great challenge. Additionally, today's nanometer scale devices suffer from accelerated aging effects because of the extreme operating temperature and electric fields they are subjected to. Unless addressed in design, aging-related defects can significantly reduce the lifetime of a product. In this article, we investigate a micro-architectural scheme for improving yield and reliability of homogeneous chip multiprocessors (CMPs). The proposed solution involves a hardware framework that enables us to utilize the redundancies inherent in a multicore system to keep the system operational in the face of partial failures. A micro-architectural modification allows a faulty core in a CMP to use another core's resources to service any instruction that the former cannot execute correctly by itself. This service improves yield and reliability but may cause loss of performance. The target platform for quantitative evaluation of performance under degradation is a dual-core and a quad-core chip multiprocessor with one or more cores sustaining partial failure. Simulation studies indicate that when a large, high-latency, and sparingly used unit such as a floating-point unit fails in a core, correct execution may be sustained through outsourcing with at most a 16% impact on performance for a floating-point intensive application. For applications with moderate floating-point load, the degradation is insignificant. The performance impact may be mitigated even further by judicious selection of the cores to commandeer depending on the current load on each of the candidate cores. The area overhead is also negligible due to resource reuse. © 2015 ACM 1539-9087/2015/01-ART2 $15.00.",Chip multiprocessor; Fault tolerance through redundancy; Hardware reliability; Modeling and simulation of multicore systems,Adaptive systems; Adaptive systems; CMOS integrated circuits; CMOS integrated circuits; Digital arithmetic; Digital arithmetic; Electric fields; Electric fields; Fault tolerance; Fault tolerance; Hardware; Hardware; Microprocessor chips; Microprocessor chips; Multiprocessing systems; Multiprocessing systems; Product design; Product design; Reliability; Reliability; Architectural modification; Architectural modification; Chip multi-processors (CMPs); Chip multi-processors (CMPs); Chip Multiprocessor; Chip Multiprocessor; Hardware reliability; Hardware reliability; Multi-core systems; Multi-core systems; Nanometer scale devices; Nanometer scale devices; Quantitative evaluation; Quantitative evaluation; Reliability enhancement; Reliability enhancement; Redundancy; Redundancy
"Regular, special, and related issues",2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921928860&doi=10.1145%2f2698230&partnerID=40&md5=33b159c85155234fc69eda3fe83ea9d4,[No abstract available],,
Maximal synthesis for hennessy-milner logic,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921934000&doi=10.1145%2f2680540&partnerID=40&md5=da7f25aa1526239ad6d1ec6353b8b7af,"This article concerns the maximal synthesis for Hennessy-Milner Logic on Kripke structures with labeled transitions. We formally define, and prove the validity of, a theoretical framework that modifies a Kripke model to the least possible extent in order to satisfy a given HML formula. Applications of this work can be found in the field of controller synthesis and supervisory control for discrete-event systems. Synthesis is realized technically by first projecting the given Kripke model onto a bisimulation-equivalent partial tree representation, thereby unfolding up to the depth of the synthesized formula. Operational rules then define the required adaptations upon this structure in order to achieve validity of the synthesized formula. Synthesis might result in multiple valid adaptations, which are all related to the original model via simulation. Each simulant of the original Kripke model, which satisfies the synthesized formula, is also related to one of the synthesis results via simulation. This indicates maximality, or maximal permissiveness, in the context of supervisory control. In addition to the formal construction of synthesis as presented in this article, we present it in algorithmic form and analyze its computational complexity. Computer-verified proofs for two important theorems in this article have been created using the Coq proof assistant. © 2015 ACM 1539-9087/2015/01-ART2 $15.00.",Hennessy-Milner Logic; Maximal permissiveness; Supervisory control,Discrete event simulation; Discrete event simulation; Model checking; Model checking; Parallel processing systems; Parallel processing systems; Theorem proving; Theorem proving; Controller synthesis; Controller synthesis; Coq proof assistant; Coq proof assistant; Hennessy-Milner Logics; Hennessy-Milner Logics; Kripke structure; Kripke structure; Labeled transitions; Labeled transitions; Maximal permissiveness; Maximal permissiveness; Supervisory control; Supervisory control; Theoretical framework; Theoretical framework; Logic Synthesis; Logic Synthesis
The psi-calculi workbench: A generic tool for applied process calculi,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921930840&doi=10.1145%2f2682570&partnerID=40&md5=154e8d8cd33e033ea6cffac456c68e80,"Psi-calculi is a parametric framework for extensions of the pi-calculus with arbitrary data and logic. All instances of the framework inherit machine-checked proofs of the metatheory such as compositionality and bisimulation congruence. We present a generic analysis tool for psi-calculus instances, enabling symbolic execution and (bi)simulation checking for both unicast and broadcast communication. The tool also provides a library for implementing new psi-calculus instances.We provide examples from traditional communication protocols and wireless sensor networks. We also describe the theoretical foundations of the tool, including an improved symbolic operational semantics, with additional support for scoped broadcast communication. © 2015 ACM 1539-9087/2015/01-ART2 $15.00.",Process calculi; Symbolic semantics; Wireless sensor networks,Biomineralization; Biomineralization; Calculations; Calculations; Pathology; Pathology; Semantics; Semantics; Bisimulation congruences; Bisimulation congruences; Broadcast communication; Broadcast communication; Machine-checked proofs; Machine-checked proofs; Operational semantics; Operational semantics; Process calculi; Process calculi; Protocols and wireless; Protocols and wireless; Symbolic semantics; Symbolic semantics; Theoretical foundations; Theoretical foundations; Wireless sensor networks; Wireless sensor networks
Does the sharing of execution units improve performance/power of multicores?,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921916398&doi=10.1145%2f2680543&partnerID=40&md5=1451d09e98a96ab4e97fa2e6cab4c108,"Several studies and recent real-world designs have promoted sharing of underutilized resources between cores in a multicore processor to achieve better performance/power. It has been argued that when utilization of such resources is low, sharing has a negligible impact on performance while offering considerable area and power benefits. In this article, we investigate the performance and performance/watt implications of sharing large and underutilized resources between pairs of cores in a multicore. We first study sharing of the entire floating-point datapath (including reservation stations and execution units) by two cores, similar to AMD's Bulldozer.We find that while this architecture results in power savings for certain workload combinations, it also results in significant performance loss of up to 28%. Next, we study an alternative sharing architecture where only the floating-point execution units are shared, while the individual cores retain their reservation stations. This reduces the highest performance loss to 14%. We then extend the study to include sharing of other large execution units that are used infrequently, namely, the integer multiply and divide units. Subsequently, we analyze the impact of sharing hardware resources in SimultaneouslyMultithreaded (SMT) processors where multiple threads run concurrently on the same core. We observe that sharing improves performance/watt at a negligible performance cost only if the shared units have high throughput. Sharing low-throughput units reduces both performance and performance/watt. To increase the throughput of the shared units, we propose the use of Dynamic Voltage and Frequency Boosting (DVFB) of only the shared units that can be placed on a separate voltage island. Our results indicate that the use of DVFB improves both performance and performance/watt by as much as 22% and 10%, respectively. © 2015 ACM 1539-9087/2015/01-ART2 $15.00.",Multicore processors; Resource sharing; Shared floating-point execution,Digital arithmetic; Throughput; Floating points; Hardware resources; Improve performance; Multi-core processor; Real-world designs; Resource sharing; Sharing architectures; Simultaneously-multithreaded; Computer architecture
Interactive trace-based analysis toolset for manual parallelization of C programs,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921911404&doi=10.1145%2f2638556&partnerID=40&md5=81123e961e082c31870315703949903c,"Massive amounts of legacy sequential code need to be parallelized to make better use of modern multiprocessor architectures. Nevertheless, writing parallel programs is still a difficult task. Automated parallelization methods can be effective both at the statement and loop levels and, recently, at the task level, but they are still restricted to specific source code constructs or application domains. We present in this article an innovative toolset that supports developers when performing manual code analysis and parallelization decisions. It automatically collects and represents the program profile and data dependencies in an interactive graphical format that facilitates the analysis and discovery of manual parallelization opportunities. The toolset can be used for arbitrary sequential C programs and parallelization patterns. Also, its program-scope data dependency tracing at runtime can complement the tools based on static code analysis and can also benefit from it at the same time. We also tested the effectiveness of the toolset in terms of time to reach parallelization decisions and of their quality. We measured a significant improvement for several real-world representative applications. © 2015 ACM 1539-9087/2015/01-ART2 $15.00.",Data dependency analysis; Execution profiling; Graph abstraction; Graph analysis; Legacy C program parallelization; Source annotation,Codes (symbols); Codes (symbols); Parallel architectures; Parallel architectures; C programs; C programs; Data-dependency analysis; Data-dependency analysis; Execution profiling; Execution profiling; Graph abstraction; Graph abstraction; Graph analysis; Graph analysis; Source annotation; Source annotation; C (programming language); C (programming language)
A hybrid task mapping algorithm for heterogeneous MPSoCs,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921927543&doi=10.1145%2f2680542&partnerID=40&md5=aae42e098b0c1bfde154fda5f1f9768d,"The application workloads in modern MPSoC-based embedded systems are becoming increasingly dynamic. Different applications concurrently execute and contend for resources in such systems, which could cause serious changes in the intensity and nature of the workload demands over time. To cope with the dynamism of application workloads at runtime and improve the efficiency of the underlying system architecture, this article presents a hybrid task mapping algorithm that combines a static mapping exploration and a dynamic mapping optimization to achieve an overall improvement of system efficiency. We evaluate our algorithm using a heterogeneous MPSoC system with three real applications. Experimental results reveal the effectiveness of our proposed algorithm by comparing derived solutions to the ones obtained from several other runtime mapping algorithms. In test cases with three simultaneously active applications, the mapping solutions derived by our approach have average performance improvements ranging from 45.9% to 105.9% and average energy savings ranging from 14.6% to 23.5%. © 2015 ACM 1539-9087/2015/01-ART2 $15.00.",Embedded systems; KPN; MPSoC; Simulation; Task mapping,Conformal mapping; Energy conservation; Multiprocessing systems; System-on-chip; Active application; Heterogeneous mpsoc; MPSoC; Performance improvements; Simulation; System efficiency; Task mapping; Underlying systems; Embedded systems
Modeling and analysis of fault detection and fault tolerance in wireless sensor networks,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921930078&doi=10.1145%2f2680538&partnerID=40&md5=d22ecf97654fc5fe4f6c62341307c8a2,"Technological advancements in communications and embedded systems have led to the proliferation of Wireless Sensor Networks (WSNs) in a wide variety of application domains. These application domains include but are not limited to mission-critical (e.g., security, defense, space, satellite) or safety-related (e.g., health care, active volcano monitoring) systems. One commonality across all WSN application domains is the need to meet application requirements (e.g., lifetime, reliability). Many application domains require that sensor nodes be deployed in harsh environments, such as on the ocean floor or in an active volcano, making these nodes more prone to failures. Sensor node failures can be catastrophic for critical or safetyrelated systems. This article models and analyzes fault detection and fault tolerance in WSNs. To determine the effectiveness and accuracy of fault detection algorithms, we simulate these algorithms using ns-2. We investigate the synergy between fault detection and fault tolerance and use the fault detection algorithms' accuracies in our modeling of Fault-Tolerant (FT) WSNs.We develop Markov models for characterizingWSN reliability and Mean Time to Failure (MTTF) to facilitate WSN application-specific design. Results obtained from our FT modeling reveal that an FT WSN composed of duplex sensor nodes can result in as high as a 100% MTTF increase and approximately a 350% improvement in reliability over a Non-Fault-Tolerant (NFT) WSN. The article also highlights future research directions for the design and deployment of reliable and trustworthy WSNs. © 2015 ACM 1539-9087/2015/01-ART2 $15.00.",Fault detection; Fault-tolerance; Markov modeling; Reliability; Wireless sensor networks,Embedded systems; Embedded systems; Fault detection; Fault detection; Markov processes; Markov processes; Reliability; Reliability; Sensor nodes; Sensor nodes; Signal detection; Signal detection; Volcanoes; Volcanoes; Wireless sensor networks; Wireless sensor networks; Application requirements; Application requirements; Fault detection algorithm; Fault detection algorithm; Future research directions; Future research directions; Markov model; Markov model; Mean time to failure; Mean time to failure; Safety-related systems; Safety-related systems; Technological advancement; Technological advancement; Wireless sensor network (WSNs); Wireless sensor network (WSNs); Fault tolerance; Fault tolerance
System-level performance and power optimization for MPSoC: A memory access-aware approach,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921927687&doi=10.1145%2f2656339&partnerID=40&md5=f928c23d2a458005ba77555f21405736,"As the number of IPs in a multimedia Multi-Processor System-on-Chip (MPSoC) continues to increase, concurrent memory accesses from different IPs increasingly stress memory systems, which presents both opportunities and challenges for future MPSoC design. The impact of such requirements on the system-level design for MPSoC is twofold. First, contention among IPs prolongs memory access time, which exacerbates the persisting memory wall problem. Second, longer memory accesses lead to longer IP stall time, which results in unnecessary leakage waste. In this article, we propose two memory access-aware system-level design approaches for performance and leakage optimization. To alleviate the memory wall problem, we propose a Hierarchical Memory Scheduling (HMS) policy that schedules memory requests from the same IP and application consecutively to reduce interference among memory accesses from different IPs with a fairness guarantee. To reduce IP leakage waste due to long memory access, we propose a memory accessaware power-gating policy. A straightforward power-gating approach is to power gate an IP when it needs to fetch data from memory. However, due to the response time variation among memory accesses, aggressively power gating an IP whenever a memory request occurs may result in incorrect power-gating decisions. The proposed memory access-aware power-gating policy makes these decisions judiciously, based on the predicted memory latency of an individual IP and its energy breakeven time. The experimental results show that the proposed HMS memory scheduling policy improves system throughput by 42% compared to First-Come-First-Serve (FCFS) and by 21% compared to First-Ready First-Come-First-Serve (FR-FCFS) on an MPSoC for mobile phones. For the improvement of fairness, HMS improves fairness by 1.52× compared to FCFS and by 1.23× compared to FRFCFS. In the aspect of leakage optimization, our memory accessaware power-gating mechanism improves energy savings by 3.88× and reduces the performance penalty by 70% compared to conventional timeout-based power gating. We further demonstrate that our HMS memory scheduler can regulate memory access orders, thereby reducing memory response time variation. This leads to more accurate power-down decisions for both conventional timeout power gating and the proposed memory access- aware power gating. © 2015 ACM 1539-9087/2015/01-ART2 $15.00.",Memory scheduling; MPSoC; Power gating,Application specific integrated circuits; Application specific integrated circuits; Cellular telephone systems; Cellular telephone systems; Design; Design; Distributed computer systems; Distributed computer systems; Energy conservation; Energy conservation; Interference suppression; Interference suppression; Microprocessor chips; Microprocessor chips; Multiprocessing systems; Multiprocessing systems; Packet networks; Packet networks; Response time (computer systems); Response time (computer systems); System-on-chip; System-on-chip; Systems analysis; Systems analysis; First come first serves; First come first serves; Memory scheduling; Memory scheduling; MPSoC; MPSoC; Multi processor system on chips; Multi processor system on chips; Performance penalties; Performance penalties; Power gatings; Power gatings; Power-gating mechanisms; Power-gating mechanisms; System-level performance; System-level performance; Scheduling; Scheduling
"A brief comment on ""A complete self-testing and self-configuring NoC infrastructure for cost-effective MPSoCs"" [ACM Transactions on embedded computing systems 12 (2013) article 106]",2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921926031&doi=10.1145%2f2668121&partnerID=40&md5=3719d8458a83ce77442eea3a2182ed7d,"In the Ghiribaldi et al. [2013] paper, a complete self-testing and self configuring NoC infrastructure for cost-effective MPSoCs was presented in order to make NoC architecture tolerant to faults. To overcome the complexity involved during the complete reconfiguration of routing instances in the face of most of the usual failure patterns, Ghiribaldi et al. [2013] proposed a fast self-reconfiguration algorithm. The algorithm is based on segment-based routing implemented using Logic-Based Distributed Routing (LBDR) and claimed to have handled the most common NoC faults. The purpose of this comment is to demonstrate the inconsistency of the fast self-configuration method presented in Ghiribaldi et al. [2013]. To handle inconsistency, we present the correct set of LBDR bits and also argue that complete reconfiguration of the routing instance is mandatory to handle some fault combinations. New coverage results of the fast self-reconfiguration algorithm of Ghiribaldi et al. [2013] are also presented. © 2015 ACM 1539-9087/2015/01-ART2 $15.00.",Fault tolerance; Network-on-chip,Aluminum; Aluminum; Complex networks; Complex networks; Cost effectiveness; Cost effectiveness; Embedded systems; Embedded systems; Fault tolerance; Fault tolerance; Fault tolerant computer systems; Fault tolerant computer systems; Modular robots; Modular robots; Multiprocessing systems; Multiprocessing systems; System-on-chip; System-on-chip; VLSI circuits; VLSI circuits; Cost effective; Cost effective; Distributed routing; Distributed routing; Embedded computing system; Embedded computing system; Failure patterns; Failure patterns; NoC architectures; NoC architectures; Self configuration; Self configuration; Self reconfiguration; Self reconfiguration; Self-configuring; Self-configuring; Network-on-chip
GENESIS: Parallel application placement onto reconfigurable architectures (Invited for the special issue on runtime management),2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921912148&doi=10.1145%2f2629651&partnerID=40&md5=80aed4fce05fd4029b210300bc519a23,"Placement is though as the most time-consuming processes in physical implementation flows for reconfigurable architectures, while it highly affects the quality of derived application implementation, as it has impact on the maximum operating frequency. Throughout this article, we propose a novel placer, based on genetic algorithm, targeting to FPGAs. Rather than relevant approaches, which are executed sequentially, the new placer exhibits inherent parallelism, which can benefit from multicore processors. Experimental results prove the effectiveness of this solution, as it achieves average reduction of execution runtime and application's delay by 67× and 16%, respectively. © 2015 ACM 1539-9087/2015/01-ART2 $15.00.",CAD tool; Genetic algorithm; Placement; Reconfigurable architectures; Search space exploration,Computer aided design; Computer aided design; Genetic algorithms; Genetic algorithms; Space research; Space research; CAD tool; CAD tool; Inherent parallelism; Inherent parallelism; Maximum operating frequency; Maximum operating frequency; Multi-core processor; Multi-core processor; Parallel application; Parallel application; Placement; Placement; Runtime management; Runtime management; Search spaces; Search spaces; Reconfigurable architectures; Reconfigurable architectures
Energy-efficient thread assignment optimization for heterogeneous multicore systems,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921918243&doi=10.1145%2f2566618&partnerID=40&md5=2ea8ef1701dc3c2c05370535219293ac,"The current trend to move from homogeneous to heterogeneous multicore systems provides compelling opportunities for achieving performance and energy efficiency goals. Running multiple threads in multicore systems poses challenges on meeting limited shared resources, such as memory bandwidth. We propose an optimization approach that includes an Integer Linear Programming (ILP) optimization model and a scheme to dynamically determine thread-to-core assignment. We present simulation analysis that shows energy savings and performance gains for a variety of workloads compared to state-of-the-art schemes. We implemented and evaluated a prototype of our thread assignment approach at user level, leveraging Linux scheduling and performance-monitoring capabilities. © 2015 ACM 1539-9087/2015/01-ART2 $15.00.",Energy efficiency; Heterogeneous multicores; Memory bandwidth; Optimization; Real-time performance; Task scheduling,Bandwidth; Computer operating systems; Energy conservation; Integer programming; Optimization; Scheduling; Heterogeneous multi-core systems; Heterogeneous Multi-Cores; Integer Linear Programming; Memory bandwidths; Performance monitoring; Real time performance; State-of-the-art scheme; Task-scheduling; Energy efficiency
A study on parallelizing XML path filtering using accelerators,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928337182&doi=10.1145%2f2560040&partnerID=40&md5=1712735189f59907e4a4a7d6d0213930,"Publish-subscribe systems present the state of the art in information dissemination to multiple users. Such systems have evolved from simple topic-based to the current XML-based systems. XML-based pubsub systems provide users with more flexibility by allowing the formulation of complex queries on the content as well as the structure of the streaming messages. Messages that match a given user query are forwarded to the user. This article examines how to exploit the parallelism found in XPath filtering. Using an incoming XML stream, parsing and matching thousands of user profiles are performed simultaneously by matching engines. We show the benefits and trade-offs of mapping the proposed filtering approach onto FPGAs, processing streams of XML at wire speed, and GPUs, providing the flexibility of software. This is in contrast to conventional approaches bound by the sequential aspect of software computing, associated with a large memory footprint. By converting XPath expressions into custom stacks, our solution is the first to provide support for complex XPath structural constructs, such as parent-child and ancestor descendant relations, whilst allowing wildcarding and recursion. The measured speedups resulting from the GPU and FPGA accelerations versus single-core CPUs are up to 6.6X and 2.5 orders of magnitude, respectively. The FPGA approaches are up to 31X faster than software running on 12 CPU cores. © 2014 ACM.",Design; Experimentation; Performance,Economic and social effects; Field programmable gate arrays (FPGA); Information dissemination; Integrated circuit design; Message passing; Program processors; 'current; Complex queries; Experimentation; Multiple user; Parallelizing; Performance; Publish/Subscribe system; Simple++; State of the art; XML path; XML
Performance and reliability analysis of cross-layer optimizations of NAND flash controllers,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921903966&doi=10.1145%2f2629562&partnerID=40&md5=cfa5d1e7f7c319d627f9be7d1d031576,"NAND flash memories are becoming the predominant technology in the implementation of mass storage systems for both embedded and high-performance applications. However, when considering data and code storage in Non-Volatile Memories (NVMs), such as NAND flash memories, reliability and performance become a serious concern for systems designers. Designing NAND flash-based systems based on worst-case scenarios leads to waste of resources in terms of performance, power consumption, and storage capacity. This is clearly in contrast with the request for runtime reconfigurability, adaptivity, and resource optimization in modern computing systems. There is a clear trend toward supporting differentiated access modes in flash memory controllers, each one setting a differentiated tradeoff point in the performance-reliability optimization space. This is supported by the possibility of tuning the NAND flash memory performance, reliability, and power consumption through several tuning knobs such as the flash programming algorithm and the flash error correcting code. However, to successfully exploit these degrees of freedom, it is mandatory to clearly understand the effect that the combined tuning of these parameters has on the full NVM subsystem. This article performs a comprehensive quantitative analysis of the benefits provided by the runtime reconfigurability of an MLC NAND flash controller through the combined effect of an adaptable memory programming circuitry coupled with runtime adaptation of the ECC correction capability. The full NVM subsystem is taken into account, starting from a characterization of the low-level circuitry to the effect of the adaptation on a wide set of realistic benchmarks in order to provide readers a clear view of the benefit this combined adaptation may provide at the system level. © 2015 ACM 1539-9087/2015/01-ART2 $15.00.",Adaptable memory controllers; ECC; NAND flash memories,Controllers; Data storage equipment; Degrees of freedom (mechanics); Digital storage; Embedded systems; Monolithic microwave integrated circuits; NAND circuits; Reliability; Reliability analysis; Tuning; Cross layer optimization; ECC; High performance applications; Memory controller; NAND flash memory; Performance and reliabilities; Performance reliability; Runtime reconfigurability; Flash memory
Stubborn sets for time petri nets,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921897626&doi=10.1145%2f2680541&partnerID=40&md5=fc4bb578586e485eaad79588e4bdcdaa,"Themain limitation of the verification approaches based on state enumeration is the state explosion problem. The partial order reduction techniques aim at attenuating this problem by reducing the number of transitions to be fired from each state while preserving properties of interest. Among the reduction techniques proposed in the literature, this article considers the stubborn set method of Petri nets and investigates its extension to time Petri nets. It establishes some useful sufficient conditions for stubborn sets, which preserve deadlocks and k-boundedness of places. © 2015 ACM 1539-9087/2015/01-ART2 $15.00.",Deadlock property; Stubborn set method; Time Petri nets,State space methods; Boundedness; Deadlock property; Partial order reductions; Reduction techniques; State explosion problems; Stubborn set method; Stubborn sets; Time Petri nets; Petri nets
Static task partitioning for locked caches in multicore real-time systems,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921901561&doi=10.1145%2f2638557&partnerID=40&md5=91582f3b5754f30def96d83f70480b36,"Growing processing demand on multitasking real-time systems can be met by employing scalable multicore architectures. For such environments, locking cache lines for hard real-time systems ensures timing predictability of data references and may lower worst-case execution time. This work studies the benefits of cache locking on massive multicore architectures with private caches in the context of hard real-time systems. In shared cache architectures, the cache is a single resource shared among all of the tasks. However, in scalable cache architectures with private caches, conflicts exist only among the tasks scheduled on one core. This calls for a cache-aware allocation of tasks onto cores. The objective of this work is to increase the predictability of memory accesses resolved by caches while reducing the number of cores for a given task set. This allows designers to reduce the footprint of their subsystem of real-time tasks and thereby cost, either by choosing a product with fewer cores as a target or to allow more subsystems to be co-located on a given fixed number of cores. Our work proposes a novel variant of the cache-unaware First Fit Decreasing (FFD) algorithm called Naive locked First Fit Decreasing (NFFD) policy. We propose two cache-aware static scheduling schemes: (a) Greedy First Fit Decreasing (GFFD) and (b) Colored First Fit Decreasing (CoFFD) for task sets where tasks do not have intratask conflicts among locked regions (Scenario A). NFFD is capable of scheduling high utilization task sets that FFD cannot schedule. Experiments also show that CoFFD consistently outperforms GFFD, resulting in a lower number of cores and lower system utilization. CoFFD reduces the number of core requirements by 30% to 60% compared to NFFD. For a more generic case where tasks have intratask conflicts, we split the task partitioning between two phases: task selection and task allocation (Scenario B). Instead of resolving conflicts at a global level, these algorithms resolve conflicts among regions while allocating a task onto a core and unlocking at region level instead of task level. We show that a combination of dynamic ordering (task selection) with Chaitin's Coloring (task allocation) scheme reduces the number of cores required by up to 22% over a basic scheme (in a combination of monotone ordering and regional FFD). Regional unlocking allows this scheme to outperform CoFFD for medium utilization task sets from Scenario A. However, CoFFD performs better than any other scheme for high utilization task sets from Scenario A. Overall, this work is unique in considering the challenges of future multicore architectures for real-time systems and provides key insights into task partitioning and cache-locking mechanisms for architectures with private caches. © 2015 ACM 1539-9087/2015/01-ART2 $15.00.",Multicore architectures; Real-time systems; Timing analysis,Cache memory; Interactive computer systems; Locks (fasteners); Memory architecture; Scheduling; Software architecture; Cache architecture; Hard real-time systems; Medium utilization; Multicore architectures; Static scheduling; System utilization; Timing Analysis; Worst-case execution time; Real time systems
A software scheme for multithreading on CGRAs,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921906868&doi=10.1145%2f2638558&partnerID=40&md5=f0659e03f12ef9364e8277899cf5ce1a,"Recent industry trends show a drastic rise in the use of hand-held embedded devices, from everyday applications to medical (e.g., monitoring devices) and critical defense applications (e.g., sensor nodes). The two key requirements in the design of such devices are their processing capabilities and battery life. There is therefore an urgency to build high-performance and power-efficient embedded devices, inspiring researchers to develop novel system designs for the same. The use of a coprocessor (application-specific hardware) to offload power-hungry computations is gaining favor among system designers to suit their power budgets.We propose the use of CGRAs (Coarse-Grained Reconfigurable Arrays) as a power-efficient coprocessor. Though CGRAs have been widely used for streaming applications, the extensive compiler support required limits its applicability and use as a general purpose coprocessor. In addition, a CGRA structure can efficiently execute only one statically scheduled kernel at a time, which is a serious limitation when used as an accelerator to a multithreaded or multitasking processor. In this work, we envision a multithreaded CGRA where multiple schedules (or kernels) can be executed simultaneously on the CGRA (as a coprocessor). We propose a comprehensive software scheme that transforms the traditionally single-threaded CGRA into a multithreaded coprocessor to be used as a power-efficient accelerator for multithreaded embedded processors. Our software scheme includes (1) a compiler framework that integrates with existing CGRA mapping techniques to prepare kernels for execution on the multithreaded CGRA and (2) a runtime mechanism that dynamically schedules multiple kernels (offloaded from the processor) to execute simultaneously on the CGRA coprocessor. Our multithreaded CGRA coprocessor implementation thus makes it possible to achieve improved power-efficient computing in modern multithreaded embedded systems. © 2015 ACM 1539-9087/2015/01-ART2 $15.00.",CGRA; Compiler framework; Embedded system; Multithreading; Power efficiency; Runtime transformation; Scheduling,Budget control; Coprocessor; Embedded systems; Program compilers; Scheduling; Sensor nodes; Systems analysis; CGRA; Compiler framework; Multi-threading; Power efficiency; Runtimes; Multitasking
Management and optimization for nonvolatile memory-based hybrid scratchpad memory on multicore embedded processors,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908302156&doi=10.1145%2f2560019&partnerID=40&md5=2741cc6c6fedcf8a9637afe727360528,"The recent emergence of various Non-Volatile Memories (NVMs), with many attractive characteristics such as low leakage power and high-density, provides us with a new way of addressing the memory power consumption problem. In this article, we target embedded CMPs, and propose a novel Hybrid Scratch Pad Memory (HSPM) architecture which consists of SRAM and NVM to take advantage of the ultra-low leakage power, high density of NVM, and fast access of SRAM. A novel data allocation algorithm as well as an algorithm to determine the NVM/SRAM ratio for the novel HSPM architecture are proposed. The experimental results show that the data allocation algorithm can reduce the memory access time by 33.51% and the dynamic energy consumption by 16.81% on average for the HSPM architecture when compared with a greedy algorithm. The NVM/SRAM size determination algorithm can further reduce the memory access time by 14.7% and energy consumption by 20.1% on average. © 2014 ACM.",Data allocation; Energy; MRAM; Multicore processors; NVM; On-chip memory; PCM; SPM,Algorithms; Data storage equipment; Energy utilization; Microprocessor chips; MRAM devices; Pulse code modulation; Static random access storage; Data allocation; Energy; MRAM; Multi-core processor; NVM; On chip memory; SPM; Memory architecture
Exact safety verification of hybrid systems based on bilinear SOS representation,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921903023&doi=10.1145%2f2629424&partnerID=40&md5=b4f54824aa9f440e73c2e6855c33faee,"In this article, we address the problem of safety verification of nonlinear hybrid systems. A hybrid symbolicnumeric method is presented to compute exact inequality invariants of hybrid systems efficiently. Some numerical invariants of a hybrid system can be obtained by solving a bilinear SOS programming via the PENBMI solver or iterative method, then the modified Newton refinement and rational vector recovery techniques are applied to obtain exact polynomial invariants with rational coefficients, which exactly satisfy the conditions of invariants. Experiments on some benchmarks are given to illustrate the efficiency of our algorithm. © 2015 ACM 1539-9087/2015/01-ART2 $15.00.",Bilinear SOS programming; Invariant generation; Nonlinear hybrid system; Safety verification; Symbolic-numeric computation,Hybrid systems; Numerical methods; Systems engineering; Invariant generations; Nonlinear hybrid systems; Polynomial invariants; Rational coefficients; Recovery techniques; Safety verification; Symbolic-numeric computations; Symbolic-Numeric methods; Iterative methods
Resource-aware task scheduling,2015,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921909543&doi=10.1145%2f2638554&partnerID=40&md5=35291e0742867786f06e364ea2df94af,"Dependency-aware task-based parallel programming models have proven to be successful for developing efficient application software for multicore-based computer architectures. The programming model is amenable to programmers, thereby supporting productivity, whereas hardware performance is achieved through a runtime system that dynamically schedules tasks onto cores in such a way that all dependencies are respected. However, even if the scheduling is completely successful with respect to load balancing, the scaling with the number of cores may be suboptimal due to resource contention. Here we consider the problem of scheduling tasks not only with respect to their interdependencies but also with respect to their usage of resources, such as memory and bandwidth. At the software level, this is achieved by user annotations of the task resource consumption. In the runtime system, the annotations are translated into scheduling constraints. Experimental results for different hardware, demonstrating performance gains both for model examples and real applications, are presented. Furthermore, we provide a set of tools to detect resource sensitivity and predict the performance improvements that can be achieved by resource-aware scheduling. These tools are solely based on parallel execution traces and require no instrumentation or modification of the application code. © 2015 ACM 1539-9087/2015/01-ART2 $15.00.",Dependency aware; Dynamic scheduling; Resource contention; Task parallel,Application programs; Computer architecture; Computer hardware; Hardware; Multicore programming; Parallel programming; Dependency aware; Dynamic scheduling; Parallel programming model; Performance improvements; Resource contention; Resource-aware scheduling; Scheduling constraints; Task parallel; Scheduling
Task assignment algorithms for heterogeneous multiprocessors,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908162132&doi=10.1145%2f2660494&partnerID=40&md5=9a6461a7805110d26c87b6e6fae2c027,"Consider the problem of assigning implicit-deadline sporadic tasks on a heterogeneous multiprocessor platform comprising a constant number (denoted by t) of distinct types of processors-such a platform is referred to as a t-type platform. We present two algorithms, LPGIM and LPGNM, each providing the following guarantee. For a given t-type platform and a task set, if there exists a task assignment such that tasks can be scheduled to meet their deadlines by allowing them to migrate only between processors of the same type (intra-migrative), then: (i) LPGIM succeeds in finding such an assignment where the same restriction on task migration applies (intra-migrative) but given a platform in which only one processor of each type is 1 + α × t-1/t times faster and (ii) LPGNM succeeds in finding a task assignment where tasks are not allowed to migrate between processors (non-migrative) but given a platform in which every processor is 1 + α times faster. The parameter α is a property of the task set; it is the maximum of all the task utilizations that are no greater than one. To the best of our knowledge, for t-type heterogeneous multiprocessors: (i) for the problem of intra-migrative task assignment, no previous algorithm exists with a proven bound and hence our algorithm, LPGIM, is the first of its kind and (ii) for the problem of non-migrative task assignment, our algorithm, LPGNM, has superior performance compared to state-of-the-art. © 2014 Copyright held by the Owner/Author. Publication rights licensed to ACM.",Heterogeneous multiprocessors; Real-time scheduling,Software engineering; Heterogeneous multiprocessors; Real - time scheduling; State of the art; Task assignment; Task migration; Multiprocessing systems
Branch prediction-directed dynamic instruction cache locking for embedded systems,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908153700&doi=10.1145%2f2660492&partnerID=40&md5=e74fa2bbdbbe79e1619a00cffe1d7d92,"Cache locking is a cache management technique to preclude the replacement of locked cache contents. Cache locking is often adopted to improve cache access predictability inWorst-Case Execution Time (WCET) analysis. Static cache lockingmethods have been proposed recently to improve Average-Case Execution Time (ACET) performance. This article presents an approach, Branch Prediction-directed Dynamic Cache Locking (BPDCL), to improve system performance through cache conflict miss reduction. In the proposed approach, the control flow graph of a program is first partitioned into disjoint execution regions, then memory blocks worth locking are determined by calculating the locking profit for each region. These two steps are conducted during compilation time. At runtime, directed by branch predictions, locking routines are prefetched into a small high-speed buffer. The predetermined cache locking contents are loaded and locked at specific execution points during program execution. Experimental results show that the proposed BPDCL method exhibits an average improvement of 25.9%, 13.8%, and 8.0% on cache miss rate reduction in comparison to cases with no cache locking, the static locking method, and the dynamic locking method, respectively. © 2014 Copyright held by the Owner/Author. Publication rights licensed to ACM.",Branch prediction; Dynamic cache locking; Execution region partitioning; Instruction cache; System performance,Cache memory; Data flow analysis; Embedded systems; Flow graphs; Forecasting; Branch prediction; Dynamic cache; Execution region partitioning; Instruction caches; System performance; Locks (fasteners)
Provably good task assignment for two-type heterogeneous multiprocessors using cutting planes,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908170562&doi=10.1145%2f2660495&partnerID=40&md5=6172db3dbe1c24c2045e58b5888977b5,"Consider scheduling of real-time tasks on a multiprocessor where migration is forbidden. Specifically, consider the problem of determining a task-to-processor assignment for a given collection of implicit-deadline sporadic tasks upon a multiprocessor platform in which there are two distinct types of processors. For this problem, we propose a new algorithm, LPC (task assignment based on solving a Linear Program with Cutting planes). The algorithm offers the following guarantee: for a given task set and a platform, if there exists a feasible task-to-processor assignment, then LPC succeeds in finding such a feasible task-to-processor assignment as well but on a platform in which each processor is 1.5× faster and has three additional processors. For systems with a large number of processors, LPC has a better approximation ratio than state-of-the-art algorithms. To the best of our knowledge, this is the first work that develops a provably good real-time task assignment algorithm using cutting planes. © 2014 Copyright held by the Owner/Author. Publication rights licensed to ACM.",Cutting planes; Heterogeneous multiprocessors; Linear programming; Real-time scheduling,Linear programming; Multiprocessing systems; Scheduling; Approximation ratios; Cutting planes; Heterogeneous multiprocessors; Linear programs; Multi-processor platforms; Processor assignments; Real - time scheduling; State-of-the-art algorithms; Approximation algorithms
A hybrid storage access framework for high-performance virtual machines,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908160907&doi=10.1145%2f2660493&partnerID=40&md5=aea59d3c06ecd514bdd3fbc9aaa89e4d,"In recent years, advances in virtualization technology have enabled multiple virtual machines to run on a physical machine, such that each virtual machine can perform independently with its own operating system. The IT industry has adopted virtualization technology because of its ability to improve hardware resource utilization, achieve low-power consumption, support concurrent applications, simplify device management, and reduce maintenance costs. However, because of the hardware limitation of storage devices, the I/O capacity could cause performance bottlenecks. To address the problem, we propose a hybrid storage access framework that exploits solid-state drives (SSDs) to improve the I/O performance in a virtualization environment. © 2014 Copyright held by the Owner/Author. Publication rights licensed to ACM.",Hybrid storage systems; Solid-state drives; Virtual machines,Network security; Virtual storage; Virtualization; Device management; Hardware resource utilization; Hybrid storage systems; Low-power consumption; Maintenance cost; Performance bottlenecks; Solid state drives; Virtualization technologies; Virtual machine
Introduction to the special issue on virtual prototyping of parallel and embedded systems (ViPES),2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919675690&doi=10.1145%2f2675739&partnerID=40&md5=35471f760fea2e200873a483dc21eb76,[No abstract available],,
Energy efficiency analysis for the Single Frequency Approximation (SFA) Scheme,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908208283&doi=10.1145%2f2660490&partnerID=40&md5=1f107d15c1f3d9312743288104850f43,"Energy-efficient designs are important issues in computing systems. This article studies the energy efficiency of a simple and linear-time strategy, called the Single Frequency Approximation (SFA) scheme, for periodic real-time tasks on multicore systems with a shared supply voltage in a voltage island. The strategy executes all the cores at a single frequency to just meet the timing constraints. SFA has been adopted in the literature after task partitioning, but the worst-case performance of SFA in terms of energy consumption incurred is an open problem. We provide comprehensive analysis for SFA to derive the cycle utilization distribution for its worst-case behaviour for energy minimization. Our analysis shows that the energy consumption incurred by using SFA for task execution is at most 1.53 (1.74, 2.10, 2.69, respectively), compared to the energy consumption of the optimal voltage/frequency scaling, when the dynamic power consumption is a cubic function of the frequency and the voltage island has up to 4 (8, 16, 32, respectively) cores. The analysis shows that SFA is indeed an effective scheme under practical settings, even though it is not optimal. Furthermore, since all the cores run at a single frequency and no frequency alignment for Dynamic Voltage and Frequency Scaling (DVFS) between cores is needed, any unicore dynamic power management technique for reducing the energy consumption for idling can be easily incorporated individually on each core in the voltage island. This article also provides an analysis of energy consumption for SFA combined with procrastination for Dynamic Power Management (DPM), resulting in an increment of 1 from the previous results for task execution. Furthermore, we also extend our analysis for deriving the approximation factor of SFA for a multicore system with multiple voltage islands. © 2014 Copyright held by the Owner/Author. Publication rights licensed to ACM.",Energy efficiency; Multicore; Power management; SFA; Single frequency approximation; Voltage island,Dynamic frequency scaling; Energy utilization; Power management; Real time systems; Voltage scaling; Dynamic power consumption; Dynamic power management; Dynamic voltage and frequency scaling; Energy efficiency analysis; Energy-efficient design; Multi core; Single frequency; Voltage island; Energy efficiency
Host-compiled multicore system simulation for early real-time performance evaluation,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919423987&doi=10.1145%2f2678020&partnerID=40&md5=0e275f19f43858229b71d1d4df6e6ddd,"With increasing complexity and software content, modern embedded platforms employ a heterogeneous mix of multicore processors along with hardware accelerators in order to provide high performance in limited power budgets. To evaluate real-time performance and other constraints, full system simulations are essential. With traditional approaches being either slow or inaccurate, so-called source-level or hostcompiled simulators have recently emerged as a solution for rapid evaluation of the complete system at early design stages. In such approaches, a faster simulation is achieved by abstracting execution behavior and increasing simulation granularity. However, existing source-level simulators often focus on application behavior only while neglecting the effects of hardware/software interactions and their associated speed and accuracy trade-offs. In this article, we present a host-compiled simulator that emulates software execution in a full-system context. Our simulator incorporates abstract models of both real-time operating systems (RTOSs) and multicore processors to replicate timing-accurate hardware/software interactions and to enable full system cosimulation. An integrated approach for automatic timing granularity adjustment (ATGA) uses observations of the system state to automatically control the timing model and optimally navigate speed versus accuracy conditions. Results as applied to industrial-strength platforms confirm that OS- and system-level effects can significantly contribute to overall accuracy and simulation overhead. By providing careful abstractions, our models can achieve full system simulations at equivalent speeds of more than a thousand MIPS with less than 3% timing error. Coupled with the capability to easily adjust simulation parameters and configurations, this demonstrates the benefits of our simulator for early application development and design space exploration. © 2014 ACM.",Abstract modeling; Native simulation; System-level design,Abstracting; Air navigation; Budget control; Computer operating systems; Economic and social effects; Integrated circuit design; Real time systems; Simulators; Abstract modeling; Application development; Design space exploration; Full-system simulation; Native simulation; Real time operating system; Real time performance evaluation; System level design; Computer aided software engineering
STEAM: A smart temperature and energy aware multicore controller,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908221896&doi=10.1145%2f2661430&partnerID=40&md5=e0e03dc1b80653b8ec2d474abce2f8ec,"Recent empirical studies have shown thatmulticore scaling is fast becoming power limited, and consequently, an increasing fraction of a multicore processor has to be under clocked or powered off. Therefore, in addition to fundamental innovations in architecture, compilers and parallelization of application programs, there is a need to develop practical and effective dynamic energy management (DEM) techniques for multicore processors. ExistingDEMtechniques mainly target reducing processor power consumption and temperature, and only few of them have addressed improving energy efficiency for multicore systems.With energy efficiency taking a center stage in all aspects of computing, the focus of the DEM needs to be on finding practical methods to maximize processor efficiency. Towards this, this article presents STEAM - an optimal closed-loop DEM controller designed for multicore processors. The objective is to maximize energy efficiency by dynamic voltage and frequency scaling (DVFS). Energy efficiency is defined as the ratio of performance to power consumption or performance-per-watt (PPW). This is the same as the number of instructions executed per Joule. The PPW metric is actually replaced by PαPW (performanceα-per-Watt), which allows for controlling the importance of performance versus power consumption by varying α. The proposed controller was implemented on a Linux system and tested with the Intel Sandy Bridge processor. There are three power management schemes called governors, available with Intel platforms. They are referred to as (1) Powersave (lowest power consumption), (2) Performance (achieves highest performance), and (3) Ondemand. Our simple and lightweight controller when executing SPEC CPU2006, PARSEC, and MiBench benchmarks have achieved an average of 18% improvement in energy efficiency (MIPS/Watt) over these ACPI policies. Moreover, STEAM also demonstrated an excellent prediction of core temperatures and power consumption, and the ability to control the core temperatures wihin 3°C of the specified maximum. Finally, the overhead of the STEAM implementation (in terms of CPU resources) is less than 0.25%. The entire implementation is self-contained and can be installed on any processor with very little prior knowledge of the processor. © 2014 Copyright held by the Owner/Author. Publication rights licensed to ACM.",Closed-loop controller; Dynamic energy management; Dynamic thermal management; Dynamic voltage and frequency scaling; Energy efficiency; Kalman filter; Leakage power dependence on temperature; Least-squares estimation; Multicore; Performance optimal; Power and temperature modeling,Application programs; Computer operating systems; Controllers; Dynamic frequency scaling; Electric power utilization; Frequency estimation; Green computing; Kalman filters; Multicore programming; Program compilers; Steam; Temperature control; Voltage scaling; Closed loop controllers; Dynamic energy managements; Dynamic thermal management; Dynamic voltage and frequency scaling; Leakage power; Least squares estimation; Multi core; Performance optimal; Temperature modeling; Energy efficiency
Utility-based resource overbooking for cyber-physical systems,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908207025&doi=10.1145%2f2660497&partnerID=40&md5=e7162e7231cf55a49ffb940653e84bf2,"Traditional hard real-time scheduling algorithms require the use of the worst-case execution times to guarantee that deadlines will be met. Unfortunately, many algorithms with parameters derived from sensing the physical world suffer large variations in execution time, leading to pessimistic overall utilization, such as visual recognition tasks. In this article, we present ZS-QRAM, a scheduling approach that enables the use of flexible execution times and application-derived utility to tasks in order to maximize total system utility. In particular, we provide a detailed description of the algorithm, the formal proofs for its temporal protection, and a detailed, evaluation. Our evaluation uses the Utility Degradation Resilience (UDR) showing that ZS-QRAM is able to obtain 4× as much UDR as ZSRM, a previous overbooking approach, and almost 2× as much UDR as Rate-Monotonic with Period Transformation (RM/TP). We then evaluate a Linux kernel module implementation of our scheduler on an Unmanned Air Vehicle (UAV) platform. We show that, by using our approach, we are able to keep the tasks that render the most utility by degrading lower-utility ones even in the presence of highly dynamic execution times. © 2014 Copyright held by the Owner/Author. Publication rights licensed to ACM.",Mixed-criticality systems; Quality of service; Real-time scheduling; Unmanned aerial vehicles; Utility functions,Computer operating systems; Cyber Physical System; Embedded systems; Quality of service; Scheduling; Unmanned aerial vehicles (UAV); Mixed-criticality systems; Over-booking approaches; Real - time scheduling; Resource overbooking; Unmanned air vehicles; Utility functions; Visual recognition; Worst-case execution time; Real time systems
"Editorial: Embedded, cyber-physical, hybrid..",2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919622985&doi=10.1145%2f2678027&partnerID=40&md5=99698d86258dcf6f15c8632d96b4b132,[No abstract available],,
Cache-related preemption delay analysis for multilevel noninclusive caches,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908216350&doi=10.1145%2f2632156&partnerID=40&md5=04b3f31beb73a92a6735adf08411b6e2,"With the rapid growth of complex hardware features, timing analysis has become an increasingly difficult problem. The key to solving this problem lies in the precise and scalable modeling of performance-enhancing processor features (e.g., cache). Moreover, real-time systems are often multitasking and use preemptive scheduling, with fixed or dynamic priority assignment. For such systems, cache related preemption delay (CRPD) may increase the execution time of a task. Therefore, CRPD may affect the overall schedulability analysis. Existingworks propose to bound the value of CRPD in a single-level cache. In this article,we propose a CRPD analysis framework that can be used for a two-level, noninclusive cache hierarchy. In addition, our proposed framework is also applicable in the presence of shared caches. We first show that CRPD analysis faces several new challenges in the presence of a multilevel, noninclusive cache hierarchy. Our proposed framework overcomes all such challenges and we can formally prove he correctness of our framework. We have performed experiments with several subject programs, including an unmanned aerial vehicle (UAV) controller and an in-situ space debris monitoring instrument. Our experimental results suggest that we can provide sound and precise CRPD estimates using our framework. © 2014 Copyright held by the Owner/Author. Publication rights licensed to ACM.",Cache-related preemption delay; Multicore; multilevel caches; Shared caches; WCET,Antennas; Interactive computer systems; Patient monitoring; Space debris; Unmanned aerial vehicles (UAV); Cache-related preemption delay; Multi core; Multi-level cache; Shared cache; WCET; Real time systems
Multicopy cache: A highly energy-efficient cache architecture,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908176758&doi=10.1145%2f2632162&partnerID=40&md5=365cbb60975279d18c9c794c74183960,"Caches are known to consume a large part of total microprocessor energy. Traditionally, voltage scaling has been used to reduce both dynamic and leakage power in caches.However, aggressive voltage reduction causes process-variation-induced failures in cache SRAM arrays, thus compromising cache reliability. We present MultiCopy Cache (MC2), a new cache architecture that achieves significant reduction in energy consumption through aggressive voltage scaling whilemaintaining high error resilience (reliability) by exploiting multiple copies of each data item in the cache. Unlike many previous approaches,MC2 does not require any error map characterization and therefore is responsive to changing operating conditions (e.g., Vdd noise, temperature, and leakage) of the cache. MC2 also incurs significantly lower overheads compared to other ECC-based caches. Our experimental results on embedded benchmarks demonstrate that MC2 achieves up to 60% reduction in energy and energy-delay product (EDP) with only 3.5% reduction in IPC and no appreciable area overhead. © 2014 ACM. © 2014 Copyright held by the Owner/Author. Publication rights licensed to ACM.",Fault tolerance; Low-power cache; Low-power design; Low-power memory organization; Variation-aware cache,Buffer storage; Electric power supplies to apparatus; Energy efficiency; Energy utilization; Fault tolerance; Static random access storage; Voltage scaling; Aggressive voltage scaling; Changing operating conditions; Energy-efficient caches; Low power cache; Low-power design; Low-power memory; Reduction in energy consumption; Variation aware cache; Memory architecture
Minimizing stack and communication memory usage in real-time embedded applications,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908156765&doi=10.1145%2f2632160&partnerID=40&md5=979a23103db06f137c493c16cc2ec467,"In the development of real-time embedded applications, especially those on systems-on-chip, an efficient use of RAM memory is as important as the effective scheduling of the computation resources. The protection of communication and state variables accessed by concurrent tasks must provide real-time schedulability guarantees while using the least amount of memory. Several schemes, including preemption thresholds, have been developed to improve schedulability and save stack space by selectively disabling preemption. However, the design synthesis problem is still open. In this article, we target the assignment of the scheduling parameters to minimize memory usage for systems of practical interest, including designs compliant with automotive standards. We propose algorithms either proven optimal or shown to improve on randomized optimization methods like simulated annealing. © 2014 Copyright held by the Owner/Author. Publication rights licensed to ACM.",Data synchronization mechanism; Memory usage; Preemption threshold scheduling; Stack requirement,Embedded systems; Random access storage; Real time systems; Scheduling; Simulated annealing; System-on-chip; Computation resources; Data synchronization; Embedded application; Memory usage; Preemption thresholds; Randomized optimizations; Scheduling parameters; Stack requirement; Multitasking
Optimal priority assignment to control tasks,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908212097&doi=10.1145%2f2660496&partnerID=40&md5=33af12bfc13876a741125a352f0b5f84,"In embedded real-time systems, task priorities are often assigned to meet deadlines. However, in control tasks, a late completion of a task has no catastrophic consequence; rather, it has a quantifiable impact in the control performance achieved by the task. In this article, we address the problem of determining the optimal assignment of priorities and periods of sampled-data control tasks that run over a shared computation unit. We show that the minimization of the overall cost can be performed efficiently using a branch and bound algorithm that can be further speeded up by allowing for a small degree of suboptimality. Detailed numerical simulations are presented to show the advantages of various branching alternatives, the overall algorithm effectiveness, and its scalability with the number of tasks. © 2014 Copyright held by the Owner/Author. Publication rights licensed to ACM.",Branch-and-bound optimization; Control systems; Optimal design; Priority assignment; Real-time systems,Branch and bound method; Control systems; Embedded systems; Interactive computer systems; Branch-and-bound algorithms; Branch-and-bound optimization; Catastrophic consequences; Control performance; Embedded real time systems; Optimal design; Priority assignment; Sampled-data control; Real time systems
LegaSCi: Legacy systemc model integration into parallel simulators,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919428387&doi=10.1145%2f2678018&partnerID=40&md5=c31efccd28aa11bbf224f28bd7e06496,"Architects and developers use virtual prototypes of computer systems to receive early feedback on hardware design decisions as well as to develop and debug system software. This is facilitated by the comprehensive inspection capabilities virtual prototypes offer. For virtual prototypes, execution speed is crucial to support the users' productivity. Parallel simulation techniques are employed to offset the speed impact of the increasing number of cores that need to be simulated in virtual prototypes of parallel and embedded systems. SystemC is the de facto industry standard library for virtual platform modeling. Since currently no parallel SystemC library is commonly available, typical SystemC models are coded for execution in sequential simulation environments. Simply putting such models into parallel simulators may lead to thread-safety issues and may additionally cause nondeterministic simulator behavior. This article proposes a methodology to support simulation creators to face the challenge of integrating such legacy models into parallel SystemC environments. The feasibility of the proposed method is evaluated by parallelizing the latest instance of the EU FP7 project EURETILE embedded platform simulator. Using legaSCi, on four host processor cores a speedup of 2.13 is demonstrated, without having to change the individual models of the simulator. © 2014 ACM.",Determinism; Integration; Legacy code; Parallel simulation; Programming model; SystemC; Thread safety,Computer hardware; Embedded systems; Integration; Program debugging; Simulators; Software prototyping; Virtual prototyping; Determinism; Legacy code; Parallel simulations; Programming models; SystemC; Thread safeties; Digital libraries
"Introduction to the special issue on real-time, embedded and cyber-physical systems",2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908213740&doi=10.1145%2f2660488&partnerID=40&md5=fda8db6cc53e6f5bedabd97dc5ddf7dc,[No abstract available],,
Plug & Chip: A framework for supporting rapid prototyping of 3D hybrid virtual SoCs,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919455221&doi=10.1145%2f2661634&partnerID=40&md5=6b13b6f8e0daf27ac424e0e2585bb45d,"In the embedded system domain there is a continuous demand towards providing higher flexibility for application development. This trend strives for virtual prototyping solutions capable of performing fast system simulation. Among other benefits, such a solution supports concurrent hardware/software system design by enabling to start developing, testing, and validating the embedded software substantially earlier than has been possible in the past. Towards this direction, throughout this article we introduce a new framework, named Plug & Chip, targeting to support rapid prototyping of 2D and 3D digital systems. In contrast to other relevant approaches, our solution provides higher flexibility by enabling incremental system design, while also handling platforms developed with the usage of 3D integration technology. © 2014 ACM.",Embedded system; FPGA; Virtualization,Embedded systems; Field programmable gate arrays (FPGA); Rapid prototyping; Software testing; Systems analysis; Virtualization; 3-D integration; Application development; Continuous demand; Digital system; Fast systems; Hardware/software systems; Incremental system design; Virtual reality
A design and analysis framework for thermal-resilient hard real-time systems,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908211329&doi=10.1145%2f2632154&partnerID=40&md5=37a926248f90582144f80ee97a15e72f,"We address the challenge of designing predictable real-time systems in an unpredictable thermal environment where environmental temperature may dynamically change (e.g., implantable medical devices). Towards this challenge, we propose a control-theoretic design methodology that permits a system designer to specify a set of hard real-time performance modes under which the system may operate. The system automatically adjusts the real-time performance mode based on the external thermal stress. We show (via analysis, simulations, and a hardware testbed implementation) that our control design framework is stable and control performance is equivalent to previous real-time thermal approaches, even under dynamic temperature changes. A crucial and novel advantage of our framework over previous real-time control is the ability to guarantee hard deadlines even under transitions between modes. Furthermore, our system design permits the calculation of a new metric called thermal resiliency that characterizes the maxium external thermal stress that any hard real-time performance mode can withstand. Thus, our design framework and analysis may be classified as a thermal stress analysis for real-time systems. © 2014 Copyright held by the Owner/Author. Publication rights licensed to ACM.",Controltheoretic systems; EDF; Multimode system; Multimode systems; Reactive systems; Real-time systems; Schedulability; Thermal resiliency; Thermal-aware periodic resource; Thermal-aware systems; Thermalaware system,Interactive computer systems; Real time control; Real time systems; Response time (computer systems); Stress analysis; Systems analysis; Thermal stress; Multimode system; Reactive system; Schedulability; Thermal resiliency; Thermal-Aware; Design
A framework for supporting adaptive fault-tolerant solutions,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919420623&doi=10.1145%2f2629473&partnerID=40&md5=d408cb0a63eba85c188b46b51cccb1d6,"For decades, computer architects pursued one primary goal: performance. The ever-faster transistors provided by Moore's law were translated into remarkable gains in operation frequency and power consumption. However, the device-level size and architecture complexity impose several new challenges, including a decrease in dependability level due to physical failures. In this article we propose a software-supported methodology based on game theory for adapting the aggressiveness of fault tolerance at runtime. Experimental results prove the efficiency of our solution since it achieves comparable fault masking to relevant solutions, but with significantly lower mitigation cost. More specifically, our framework speeds up the identification of suspicious failure resources on average by 76% as compared to the HotSpot tool. Similarly, the introduced solution leads to average Power×Delay (PDP) savings against an existing TMR approach by 53%. © 2014 ACM.",Adaptive fault tolerance; CAD tool; FPGA; Reliability,Computer aided design; Field programmable gate arrays (FPGA); Game theory; Reliability; Adaptive fault tolerances; Adaptive fault tolerant; Average power; CAD tool; Computer architects; Fault masking; Mitigation costs; Operation frequency; Fault tolerance
Dynamically instrumenting the QEMU emulator for linux process trace generation with the GDB debugger,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919420181&doi=10.1145%2f2678022&partnerID=40&md5=6f0274d0afb39ff25b25e1ab16a4c19c,"In software debugging, trace generation techniques are used to resolve highly complex bugs. However, the emulators increasingly used for embedded software development do not yet offer the types of trace generation infrastructure available in hardware. In this article, we make changes to the ARM ISA emulation of the QEMU emulator to allow for continuous instruction-level trace generation. Using a standard GDB client, tracepoints can be inserted to dynamically log registers and memory addresses without altering executing code. The ability to run trace experiments in five different modes allows the scope of trace generation to be narrowed as needed, down to the level of a single Linux process. Our scheme collects the execution traces of a Linux process on average between 9.6x-0.7x the speed of existing QEMU trace capabilities, with 96.7% less trace data volume. Compared to a software-instrumented tracing scheme, our method is both unobtrusive and performs on average between 3-4 orders of magnitude faster.",Binary translation; Emulation; GDB; Instrumentation; Linux; QEMU; Software debugging; Trace generation,Linux; Software design; Binary translation; Emulation; Instrumentation; QEMU; Software debugging; Trace generation; Program debugging
Designing trusted embedded systems from finite state machines,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908210287&doi=10.1145%2f2638555&partnerID=40&md5=b447eb0b921cd377e010f13c794c1ae1,"Sequential components are crucial for a real-time embedded system as they control the system based on the system's current state and real life input. In this article, we explore the security and trust issues of sequential system design from the perspective of a finite state machine (FSM), which is the most popular model used to describe sequential systems. Specifically, we find that the traditional FSM synthesis procedure will introduce security risks and cannot guarantee trustworthiness in the implemented circuits. Indeed, we show that not only do there exist simple and effective ways to attack a sequential system, it is also possible to insert a hardware Trojan Horse into the design without introducing any significant design overhead. We then formally define the notion of trust in FSM and propose a novel approach to designing trusted circuits from the FSM specification. We demonstrate both our findings on the security threats and the effectiveness of our proposed method on Microelectronics Center of North Carolina (MCNC) sequential circuit benchmarks. © 2014 Copyright held by the Owner/Author. Publication rights licensed to ACM.",EDA; Finite state machine; Flip-flop; Trust; Unauthorized access,Finite automata; Flip flop circuits; Malware; Microelectronics; Network security; North Carolina; Real-time embedded systems; Security and trusts; Security threats; Sequential systems; Synthesis procedure; Trust; Unauthorized access; Embedded systems
Scheduling temporal data with dynamic snapshot consistency requirement in vehicular cyber-physical systems,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908218609&doi=10.1145%2f2629546&partnerID=40&md5=6bf1091345863f58adad8827a9f5e49e,"Timely and efficient data dissemination is one of the fundamental requirements to enable innovative applications in vehicular cyber-physical systems (VCPS). In this work, we intensively analyze the characteristics of temporal data dissemination in VCPS. On this basis, we formulate the static and dynamic snapshot consistency requirements on serving real-time requests for temporal data items. Two online algorithms are proposed to enhance the system performance with different requirements. In particular, a reschedule mechanism is developed to make the scheduling adaptable to the dynamic snapshot consistency requirement. A comprehensive performance evaluation demonstrates the superiority of the proposed algorithms. © 2014 Copyright held by the Owner/Author. Publication rights licensed to ACM.",Real-time scheduling; Temporal data dissemination; Vehicular cyberphysical systems,Cyber Physical System; Embedded systems; Real time systems; Scheduling; Comprehensive performance evaluation; Consistency requirements; Cyber physical systems (CPSs); Efficient data disseminations; On-line algorithms; Real - time scheduling; Real time; Temporal Data; Dynamics
Combating software and sybil attacks to data integrity in crowd-sourced embedded systems,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908163256&doi=10.1145%2f2629338&partnerID=40&md5=fe0bd07dafb06efa3d12e1d0cc54fe7d,"Crowd-sourced mobile embedded systems allow people to contribute sensor data, for critical applications, including transportation, emergency response and eHealth. Data integrity becomes imperative as malicious participants can launch software and Sybil attacks modifying the sensing platform and data. To address these attacks, we develop (1) a Trusted Sensing Peripheral (TSP) enabling collection of high-integrity raw or aggregated data, and participation in applications requiring additional modalities; and (2) a Secure Tasking and Aggregation Protocol (STAP) enabling aggregation of TSP trusted readings by untrusted intermediaries, while efficiently detecting fabricators. Evaluations demonstrate that TSP and STAP are practical and energyefficient. © 2014 Copyright held by the Owner/Author. Publication rights licensed to ACM.",Critical systems; Crowd-sourced sensing; Data integrity; Embedded systems; Mobile computing; Security; Trust,Computer crime; Crowdsourcing; Mobile computing; Space time adaptive processing; Critical systems; Crowd-sourced sensing; Data integrity; Security; Trust; Embedded systems
Introduction to special issue on risk and trust in embedded critical systems,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908219829&doi=10.1145%2f2659008&partnerID=40&md5=782a88b5afbf3d1455503403cdc0aaa5,[No abstract available],,
Real-time power management for embedded m2m using intelligent learning methods,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908211309&doi=10.1145%2f2632158&partnerID=40&md5=55e078ed47db19933aa2a4743cf3e387,"In this work, an embedded system working model is designed with one server that receives requests by a requester by a service queue that is monitored by a Power Manager (PM). A novel approach is presented based on reinforcement learning to predict the best policy amidst existing DPM policies and deterministic markovian nonstationary policies (DMNSP). We apply reinforcement learning, namely a computational approach to understanding and automating goal-directed learning that supports different devices according to their DPM. Reinforcement learning uses a formal framework defining the interaction between agent and environment in terms of states, response action, and reward points. The capability of this approach is demonstrated by an event-driven simulator designed using Java with a power-manageable machine-tomachine device. Our experiment result shows that the proposed dynamic power management with timeout policy gives average power saving from 4% to 21% and the novel dynamic power management with DMNSP gives average power saving from 10% to 28% more than already proposed DPM policies. © 2014 Copyright held by the Owner/Author. Publication rights licensed to ACM.",Dynamic power management; Intelligent reinforcement and indexing,Computer software; Learning systems; Markov processes; Reinforcement learning; Computational approach; Dynamic power management; Event-driven simulators; Formal framework; Intelligent learning; Real-time power managements; Service queues; Working models; Power management
"Editorial: Diversity galore & a call for resilient, sustainable and secure system design",2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995630345&doi=10.1145%2f2632152&partnerID=40&md5=939ac8f24b52fbc1fff314c89a083d9e,[No abstract available],,
Algebra of parameterised graphs,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995578224&doi=10.1145%2f2627351&partnerID=40&md5=00f4adad59488d7efb345f121555423b,"One of the difficulties in designing modern hardware systems is the necessity for comprehending and dealing with a very large number of system configurations, operational modes, and behavioural scenarios. It is often infeasible to consider and specify each individual mode explicitly, and one needs methodologies and tools to exploit similarities between the individual modes and work with groups of modes rather than individual ones. The modes and groups of modes have to be managed in a compositional way: the specification of the system should be composed from specifications of its blocks. This includes both structural and behavioural composition. Furthermore, one should be able to transform and optimise the specifications in a formal way. In this article, we propose a new formalism, called parameterised graphs. It extends the existing conditional partial order graphs (CPOGs) formalism in several ways. First, it deals with general graphs rather than just partial orders. Moreover, it is fully compositional. To achieve this, we introduce an algebra of parameterised graphs by specifying the equivalence relation by a set of axioms, which is proved to be sound, minimal, and complete. This allows one to manipulate the specifications as algebraic expressions using the rules of this algebra. We demonstrate the usefulness of the developed formalism on several case studies coming from the area of microelectronics design. 2014 Copyright held by the Owner/Author.",Conditional partial-order graphs; Design; G.2.2 [discrete mathematics]: graph theory; Instruction set architecture; Microelectronics; Parameterised graphs; Switching networks; Synthesis; Theory; Transistor networks,Algebra; Computer architecture; Design; Graph theory; Microelectronics; Parameterization; Specifications; Switching networks; Synthesis (chemical); Trees (mathematics); G.2.2 [discrete mathematics]: graph theories; Instruction set architecture; Parameterised graphs; Partial order; Theory; Algebraic expression; Case-studies; Equivalence relations; General graph; Hardware system; Operational modes; System configurations; Graphic methods
Incremental bisimulation abstraction refinement,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995595396&doi=10.1145%2f2627352&partnerID=40&md5=f577bf17cfebd7d9fd44a8f1975c1c85,"Abstraction refinement techniques in probabilistic model checking are prominent approaches for verification of very large or infinite-state probabilistic concurrent systems. At the core of the refinement step lies the implicit or explicit analysis of a counterexample. This article proposes an abstraction refinement approach for the probabilistic computation tree logic (PCTL), which is based on incrementally computing a sequence of may- and must-quotient automata. These are induced by depth-bounded bisimulation equivalences of increasing depth. The approach is both sound and complete, since the equivalences converge to the genuine PCTL equivalence. Experimental results with a prototype implementation show the effectiveness of the approach. © 2014 ACM.",Algorithms; Bisimulation; CEGAR; D2.4 [software engineering]: software/program verification-model checking; Experimentation; Probabilistic automata; Verification,Abstracting; Algorithms; Automata theory; Computation theory; Software engineering; Verification; Bisimulations; CEGAR; Experimentation; Probabilistic automata; Software/program verification; Abstraction refinement; Bisimulation equivalences; Concurrent systems; Explicit analysis; Probabilistic computation tree logic (PCTL); Probabilistic model checking; Prototype implementations; Sound and complete; Model checking
Sequentially constructive concurrency - A conservative extension of the synchronous model of computation,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995602188&doi=10.1145%2f2627350&partnerID=40&md5=24339575b1d0da0b66b202d45f8d1f31,"Synchronous languages ensure determinate concurrency but at the price of restrictions on what programs are considered valid, or constructive. Meanwhile, sequential languages such as C and Java offer an intuitive, familiar programming paradigm but provide no guarantees with regard to determinate concurrency. The sequentially constructive (SC) model of computation (MoC) presented here harnesses the synchronous execution model to achieve determinate concurrency while taking advantage of familiar, convenient programming paradigms from sequential languages. In essence, the SC MoC extends the classical synchronous MoC by allowing variables to be read and written in any order and multiple times, as long as the sequentiality expressed in the program provides sufficient scheduling information to rule out race conditions. This allows to use programming patterns familiar from sequential programming, such as testing and later setting the value of a variable, which are forbidden in the standard synchronous MoC. The SC MoC is a conservative extension in that programs considered constructive in the common synchronous MoC are also SC and retain the same semantics. In this article, we investigate classes of shared variable accesses, define SC-admissible scheduling as a restriction of ""free scheduling,"" derive the concept of sequential constructiveness, and present a priority-based scheduling algorithm for analyzing and compiling SC programs efficiently. 2014 Copyright held by the Owner/Author.",Algorithms; Concurrency; Constructiveness; D.1.3 [programming techniques]: concurrent programming; D.3.1 [programming languages]: formal definitions and theory-semantics; Determinacy; Determinism; Embedded systems; Esterel; Languages; Reactive systems; Synchronous languages; Theory,Algorithms; Computation theory; Embedded systems; Query languages; Scheduling; Scheduling algorithms; Semantics; Concurrency; Concurrent programming; Constructiveness; Determinacy; Determinism; Esterel; Formal definition; Reactive system; Synchronous languages; Theory; Conservative extensions; Priority-based scheduling; Programming paradigms; Scheduling information; Sequential languages; Sequential programming; Shared variable access; C (programming language); Scheduling
Introduction to special issue on Application of Concurrency to System Design (ACSD'13),2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995577329&doi=10.1145%2f2627347&partnerID=40&md5=095b46b1571fda9d81abf9b1ba632a18,[No abstract available],,
Polynomial sufficient conditions of well-behavedness and home markings in subclasses of weighted petri nets,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995610089&doi=10.1145%2f2627349&partnerID=40&md5=dd32cbd3f58a7ac851fbea9ea418c9eb,"Join-Free Petri nets, whose transitions have at most one input place, model systems without synchronizations, while Choice-Free Petri nets, whose places have at most one output transition, model systems without conflicts. These classes respectively encompass the state machines (S-systems) and the marked graphs (T-systems). Whereas a structurally bounded and structurally live Petri net is said to be ""well-formed"", a bounded and live Petri net is said to be ""well-behaved"". Necessary and sufficient conditions for the well-formedness of Join-Free and Choice-Free nets have been known for some time, yet the behavioral properties of these classes are still not well understood. In particular polynomial sufficient conditions for liveness, that is, polynomial in time and with a polynomial initial number of tokens, have not been found until now. Besides, home markings, which can be reached from every reachable marking thus allowing for the construction of systems that can return to their initial data distribution, are not well apprehended either for these subclasses. We extend results on weighted T-systems to the class of weighted Petri nets and present transformations which preserve the language of the system and reduce the initial marking.We introduce a notion of balancing that makes possible the transformation of conservative systems into so-called ""token-conservative"" systems, whose number of tokens is invariant, while retaining the feasible transition sequences. This transformation is pertinent for all well-formed Petri nets and leads to polynomial sufficient conditions of liveness for well-formed Join-Free and Choice-Free nets. Finally, we also provide polynomial live and home markings for Fork-Attribution systems. © 2014 ACM.",Algorithms; Balancing; Boundedness; C.3 [special-purpose and application-based systems]: real-time and embedded systems; Choice-free; D.2.2 [software engineering]: design tools and techniques-petri nets; Design; Event graph; Fork-attribution; Home marking; Join-free; Liveness; Performance; Polynomial sufficient condition; Reversible; S-system; State machine; Synchronous dataflow; T-system; Token-conservative; Transformation; Weighted Petri net; Well-behavedness; Well-formedness,Algorithms; Application programs; Balancing; Data flow analysis; Design; Embedded systems; Petri nets; Polynomials; Software engineering; Boundedness; Choice-free; Design tools and techniques; Event graphs; Fork-attribution; Home marking; Liveness; Performance; Real-time and embedded systems; Reversible; S-systems; State machine; Synchronous Dataflow; T-Systems; Token-conservative; Transformation; Behavioral properties; Conservative systems; Data distribution; Initial marking; Join-free petri nets; Output transition; Transition sequences; Weighted T-system; Real time systems; Petri nets
Tomahawk: Parallelism and heterogeneity in communications signal processing MPSoCs,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995633298&doi=10.1145%2f2517087&partnerID=40&md5=46f6a346a17f0829be2a5d2d30b3343d,"Heterogeneity and parallelism in MPSoCs for 4G (and beyond) communications signal processing are inevitable in order to meet stringent power constraints and performance requirements. The question arises on how to cope with the problem of system programmability and runtime management incurred by the statically or even dynamically varying number and type of processing elements. This work addresses this challenge by proposing the concept of a heterogeneous many-core platform called Tomahawk. Apart from the definition of the system architecture, in this approach a unified framework including a model of computation, a programming interface and a dedicated runtime management unit called CoreManager is proposed. The increase of system complexity in terms of application parallelism and number of resources may lead to a dramatic increase of the management costs, hence causing performance degradation. For this reason, the efficient implementation of the CoreManager becomes a major issue in system design. This work compares the performance and capabilities of various CoreManager HW/SW solutions, based on ASIC, RISC and ASIP paradigms. The results demonstrate that the proposed ASIP-based solution approaches the performance of the ASIC realization, while preserving the full flexibility of the software (RISC-based) implementation. © 2014 ACM.",C.3 [special-purpose and application-based systems]: signal processing systems; CoreManager; Data dependency checking; Design; Experimentation; Hardware scheduler; Heterogeneous MPSoCS; Instruction set extensions; Measurement; Network-on-chip; Out-of-order scheduling; Performance; Real-time; Runtime management; Software defined radio; Software scheduler; Task scheduling; taskC; Tomahawk,Application programs; Complex networks; Computer architecture; Design; Information management; Measurements; Multiprocessing systems; Network-on-chip; Real time systems; Reconfigurable hardware; Scheduling; Software radio; System-on-chip; Systems analysis; 4G mobile communication systems; CoreManager; Data dependencies; Experimentation; Hardware schedulers; Heterogeneous MPSoCS; Instruction set extension; Out-of-order scheduling; Performance; Real time; Runtime management; Signal processing systems; Software-defined radios; Task-scheduling; taskC; Tomahawk; Efficient implementation; Heterogeneous many cores; Model of computation; Performance degradation; Performance requirements; Processing elements; Programming interface; System architectures; Signal processing
"UWB microwave imaging for breast cancer detection: Many-core, GPU, or FPGA?",2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912139521&doi=10.1145%2f2530534&partnerID=40&md5=32f5a2e31cabeff10c901952bf85d841,"An UWB microwave imaging system for breast cancer detection consists of antennas, transceivers, and a high-performance embedded system for elaborating the received signals and reconstructing breast images. In this article we focus on this embedded system. To accelerate the image reconstruction, the Beamforming phase has to be implemented in a parallel fashion.We assess its implementation in three currently available high-end platforms based on a multicore CPU, a GPU, and an FPGA, respectively. We then project the results applying technology scaling rules to future many-core CPUs, many-thread GPUs, and advanced FPGAs. We consider an optimistic case in which available resources increase according to Moore's law only, and a pessimistic case in which only a fraction of those resources are available due to a limited power budget. In both scenarios, an implementation that includes a high-end FPGA outperforms the other alternatives.Since the number of effectively usable cores in future many-cores will be power-limited, and there is a trend toward the integration of power-efficient accelerators, we conjecture that a chip consisting of a many-core section and a reconfigurable logic section will be the perfect platform for this application. © 2014 ACM.",Breast cancer detection; FPGA; GPU; Many-core; Microwave imaging; Ultra-wideband,Budget control; Diseases; Embedded systems; Field programmable gate arrays (FPGA); Image processing; Image reconstruction; Imaging systems; Medical imaging; Program processors; Radio transceivers; Reconfigurable hardware; Graphics processing unit; Ultra-wideband (UWB); Breast cancer detection; GPU; Many core; Microwave imaging; Microwave imaging systems; Received signals; Reconfigurable logic; Technology scaling; Multi-core cpus; Power efficient; Ultra-wideband (UWB); Medical imaging
Editorial: Special issue on design challenges for many-core processors,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995584876&doi=10.1145%2f2567941&partnerID=40&md5=56ae310fe0345926bb36419a8cc18003,[No abstract available],,
Editorial: Embedded systems - More than methodology,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995686662&doi=10.1145%2f2587894&partnerID=40&md5=2d2484cdaf356e35aa01704984a46004,[No abstract available],,
HARP: Harnessing inactive threads in many-core processors,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995553506&doi=10.1145%2f2567938&partnerID=40&md5=3b813fc8a2b7e0555ad64b3b7c2328d6,"SIMT accelerators are equipped with thousands of computational resources. Conventional accelerators, however, fail to fully utilize available resources due to branch and memory divergences. This underutilization is manifested in two underlying inefficiencies: pipeline width underutilization and pipeline depth underutilization. Width underutilization occurs when SIMD execution units are not entirely utilized due to branch divergences. This affects lane activity and results in SIMD inefficiency. Depth underutilization takes place when the pipeline runs out of active threads and is forced to leave pipeline stages idle. This work addresses both inefficiencies by harnessing inactive threads available to the pipeline.We introduce Harnessing inActive thReads in many-core Processors (or simply HARP) to improve width and depth utilization in accelerators. We show how using inactive yet ready threads can enhance performance. Moreover, we investigate implementation details and study microarchitectural changes needed to build a HARP-enhanced accelerator. Furthermore, we evaluate HARP under a variety of microarchitectural design points. We measure the area overhead associated with HARP and compare to conventional alternatives. Under Fermi-like GPUs, we show that HARP provides 10% speedup on average (maximum of 1.6X) at the cost of 3.5% area overhead. Our analysis shows that HARP performs better under narrower SIMD and shorter pipelines. © 2014 ACM.",Accelerator; Branch divergence; C.1.2 [processor architectures]: multiple data stream architectures (multiprocessors); Design; Many-core; Measurement; Memory divergence; Multithreading; Performance,Acceleration; Design; Measurements; Memory architecture; Particle accelerators; Pipelines; Program processors; Branch divergence; Many core; Memory divergences; Multi-threading; Multiple data stream architectures (multiprocessors); Performance; Area overhead; Computational resources; Design points; Execution units; Many-core processors; Pipeline depth; Pipeline stages; Multitasking; Pipelines
Bluetooth aided mobile phone localization: A nonlinear neural circuit approach,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927749319&doi=10.1145%2f2560018&partnerID=40&md5=b51878dfb49050c426b4a220fe7bf317,"It is meaningful to design a strategy to roughly localize mobile phones without a GPS by exploiting existing conditions and devices especially in environments without GPS availability (e.g., tunnels, subway stations, etc.). The availability of Bluetooth devices for most phones and the existence of a number of GPS equipped phones in a crowd of phone users enable us to design a Bluetooth aided mobile phone localization strategy. With the position of GPS equipped phones as beacons, and with the Bluetooth connection between neighbor phones as proximity constraints, we formulate the problem into an inequality problem defined on the Bluetooth network. A recurrent neural network is developed to solve the problem distributively in real time. The convergence of the neural network and the solution feasibility to the defined problem are both theoretically proven. The hardware implementation architecture of the proposed neural network is also given in this article. As applications, rough localizations of drivers in a tunnel and localization of customers in a supermarket are explored and simulated. Simulations demonstrate the effectiveness of the proposed method. © 2014 ACM.",Bluetooth; Localization; Mobile phone; Neural circuit; Recurrent neural network,Bluetooth; Cellular telephones; Global system for mobile communications; Hardware; Mobile phones; Problem solving; Recurrent neural networks; Subway stations; Telephone sets; Bluetooth connections; Bluetooth device; Bluetooth networks; Hardware implementations; Inequality problem; Localization; Mobile phone localizations; Neural circuits; Telephone circuits
NoC-based fault-tolerant cache design in chip multiprocessors,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995584189&doi=10.1145%2f2567939&partnerID=40&md5=117cebbba89525addd04e27d5480f52b,"Advances in technology scaling increasingly make emerging Chip MultiProcessor (CMP) platforms more susceptible to failures that cause various reliability challenges. In such platforms, error-prone on-chip memories (caches) continue to dominate the chip area. Also, Network-on-Chip (NoC) fabrics are increasingly used to manage the scalability of these architectures. We present a novel solution for efficient implementation of fault-tolerant design of Last-Level Cache (LLC) in CMP architectures. The proposed approach leverages the interconnection network fabric to protect the LLC cache banks against permanent faults in an efficient and scalable way. During an LLC access to a faulty block, the network detects and corrects the faults, returning the fault-free data to the requesting core. Leveraging the NoC interconnection fabric, designers can implement any cache fault-tolerant scheme in an efficient, modular, and scalable manner for emerging multicore/manycore platforms. We propose four different policies for implementing a remapping-based fault-tolerant scheme leveraging the NoC fabric in different settings. The proposed policies enable design trade-offs between NoC traffic (packets sent through the network) and the intrinsic parallelism of these communication mechanisms, allowing designers to tune the system based on design constraints. We perform an extensive design space exploration on NoC benchmarks to demonstrate the usability and efficacy of our approach. In addition, we perform sensitivity analysis to observe the behavior of various policies in reaction to improvements in the NoC architecture. The overheads of leveraging the NoC fabric are minimal: on an 8-core, 16-cache-bank CMP we demonstrate reliable access to LLCs with additional overheads of less than 3% in area and less than 7% in power. © 2014 ACM.","Algorithms; B.3.2 [design styles]: cache memories; B.3.4 [reliability, testing, and fault-tolerance]: error checking, redundant design; C.1.2 [multiple data stream architectures (multiprocessors)]: interconnection architectures; Chip multiprocessor; Design; Fault-tolerant design; Network-on-chip; Reliability; Remapping",Adaptive systems; Algorithms; Cache memory; Computer architecture; Design; Distributed computer systems; Economic and social effects; Fault tolerance; Fault tolerant computer systems; Integrated circuit interconnects; Interconnection networks (circuit switching); Multiprocessing systems; Network architecture; Network-on-chip; Reliability; Sensitivity analysis; Servers; Systems analysis; Chip Multiprocessor; Design styles; Error-checking; Fault tolerant design; Interconnection architecture; Remapping; Communication mechanisms; Efficient implementation; Fault tolerant schemes; Interconnection fabrics; Intrinsic parallelisms; Lastlevel caches (LLC); Network-on-chip(NoC); Integrated circuit design
Ultra-low-power adder stage design for exascale floating point units,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995563195&doi=10.1145%2f2567932&partnerID=40&md5=8ce819e0f02e2e1ffd3bcd595f24c6d7,"Currently, the most powerful supercomputers can provide tens of petaflops. Future many-core systems are estimated to provide an exaflop. However, the power budget limitation makes these machines still unfeasible and unaffordable. Floating Point Units (FPUs) are critical from both the power consumption and performance points of view of today's microprocessors and supercomputers. Literature offers very different designs. Some of them are focused on increasing performance no matter the penalty, and others on decreasing power at the expense of lower performance. In this article, we propose a novel approach for reducing the power of the FPU without degrading the rest of parameters. Concretely, this power reduction is also accompanied by an area reduction and a performance improvement. Hence, an overall energy gain will be produced. According to our experiments, our proposed unit consumes 17.5%, 23% and 16.5% less energy for single, double and quadruple precision, with an additional 15%, 21.5% and 14.5% delay reduction, respectively. Furthermore, area is also diminished by 4%, 4.5 and 5%. © 2014 ACM.",Algorithms; B.2.4 [arithmetic and logic structures]: high-speed arithmetic - cost/performance; Design; Floating point unit; Multiply accumulation; Multispeculation; Performance,Adders; Algorithms; Budget control; Design; Supercomputers; Floating point units; High-speed arithmetic; Multiply accumulations; Multispeculation; Performance; Area reduction; Delay reduction; Performance points; Power budgets; Power reductions; Quadruple precision; Ultra low power; Digital arithmetic
Yield-enhancement schemes for multicore processor and memory stacked 3D ICs,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995680451&doi=10.1145%2f2567933&partnerID=40&md5=b5724b52eb5d5c73eef8e966d3b51696,"A three-dimensional (3D) integrated circuit (IC) with multiple dies vertically connected by through-silicon-via (TSV) offers many benefits over current 2D ICs. Multicore logic-memory die stacking has been considered as one candidate for 3D ICs by utilizing the TSV to provide high data bandwidth between logic and memory. However, 3D ICs suffer from the low-yield issue. This article proposes effective yield-enhancement techniques for multicore die-stacked 3D ICs. Two reconfiguration schemes are proposed to logically swap the positions of cores in the dies of 3D ICs such that the yield of 3D ICs is increased. Two algorithms also are proposed to determine the reconfiguration effectively. Simulation results show that the proposed reconfiguration schemes can achieve a yield gain ranging from 1% to 11%. © 2014 ACM.","Algorithms; B.8.1 [performance and reliability]: reliability, testing, and fault toelerance; Design; Reliability",Algorithms; Design; Dies; Electronics packaging; Integrated circuit manufacture; Reliability; Timing circuits; Data bandwidth; Multi-core processor; Over current; Performance and reliabilities; Reconfiguration schemes; Three-dimensional (3D) integrated circuits; Through-Silicon-Via (TSV); Yield enhancement; Die stacking; Three dimensional integrated circuits
An asymmetric dual-processor architecture for low-power information appliances,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995543643&doi=10.1145%2f2560538&partnerID=40&md5=d7cd76363cf94e2fd14a815dcc835731,"As users become increasingly conscious of their energy footprint - either to improve battery life or to respect the environment - improved energy efficiency of systems has gained in importance. This is especially important in the context of information appliances such as e-book readers that are meant to replace books, since their energy efficiency impacts how long the appliance can be used on a single charge of the battery. In this article, we present a new software and hardware architecture for information appliances that provides significant advantages in terms of device lifetime. The architecture combines a low-power microcontroller with a high-performance application processor, where the low-power microcontroller is used to handle simple user interactions (e.g., turning pages, inking, entering text) without waking up the main application processor.We demonstrate how this architecture is easily adapted to the traditional way of building user interfaces using a user interface markup language. We report on our initial measurements using an E Ink-based prototype. When comparing our hybrid architecture to a simpler solution we found that we can increase the battery life by a factor of 1.72 for a reading task and by a factor of 3.23 for a writing task. We conclude by presenting design guidelines aimed at optimizing the overall energy signature of information appliances. © 2014 ACM.",C. [computer systems organization]; C.3 [computer systems organization]: special-purpose and application-based systems - real-time and embedded systems; Design; Energy consumption; H.5.2 [information interfaces and presentation]: user interfaces; Human computer interface; Human factors; Interface design; Measurement; Performance,Computer architecture; Design; Electric batteries; Embedded systems; Energy efficiency; Energy utilization; Human computer interaction; Human engineering; Interfaces (computer); Markup languages; Measurements; Microcontrollers; Real time systems; Computer systems organization; H.5.2 [Information Interfaces and Presentation]: User Interfaces; Human computer interfaces; Interface designs; Performance; Real-time and embedded systems; User interfaces
Energy optimization for real-time multiprocessor system-on-chip with optimal DVFS and DPM combination,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995662249&doi=10.1145%2f2567935&partnerID=40&md5=08926c317262f5c8f8ecc597806b4431,"Energy optimization is a critical design concern for embedded systems. Combining DVFS+DPM is considered as one preferable technique to reduce energy consumption. There have been optimal DVFS+DPM algorithms for periodic independent tasks running on uniprocessor in the literature. Optimal combination of DVFS and DPM for periodic dependent tasks on multicore systems is however not yet reported. The challenge of this problem is that the idle intervals of cores are not easy tomodel. In this article, a novel technique is proposed to directly model the idle intervals of individual cores such that both DVFS and DPM can be optimized at the same time. Based on this technique, the energy optimization problem is formulated by means of mixed integrated linear programming. We also present techniques to prune the exploration space of the formulation. Experimental results using real-world benchmarks demonstrate the effectiveness of our approach compared to existing approaches. © 2014 ACM.",C.3 [special-purpose and application-based system]: real-time and embedded systems; Design; DPM; DVFS; Energy optimization; Real-time MPSoCs; Scheduling,Application specific integrated circuits; Design; Embedded systems; Energy utilization; Linear programming; Multiprocessing systems; Scheduling; System-on-chip; DVFS; Energy optimization; Multi-core systems; Multiprocessor system on chips; Optimal combination; Real time; Real-time and embedded systems; Reduce energy consumption; Dependent tasks; Independent tasks; Novel techniques; Real time systems; System-on-chip
HARS: A Hardware-Assisted Runtime Software for embedded many-core architectures,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995676266&doi=10.1145%2f2517311&partnerID=40&md5=03a57a86c6a2269a3a39b22d540a8fbc,"The current trend in embedded computing consists in increasing the number of processing resources on a chip. Following this paradigm, cluster-based many-core accelerators with a shared hierarchical memory have emerged. Handling synchronizations on these architectures is critical since parallel implementations speed-ups of embedded applications strongly depend on the ability to exploit the largest possible number of cores while limiting task management overhead. This article presents the combination of a low-overhead complete runtime software and a flexible hardware accelerator for synchronizations called HARS (Hardware-Assisted Runtime Software). Experiments on a multicore test chip showed that the hardware accelerator for synchronizations has less than 1% area overhead compared to a cluster of the chip while reducing synchronization latencies (up to 2.8 times compared to a test-and-set implementation) and contentions. The runtime software part offers basic features like memory management but also optimized execution engines to allow the easy and efficient extraction of the parallelism in applications with multiple programming models. By using the hardware acceleration as well as a very low overhead task scheduling software technique, we show that HARS outperforms an optimized state-of-the-art task scheduler by 13% for the execution of a parallel application. © 2014 ACM.",B.5.0 [register-transfer-level implementation]: general; C.1.4 [processor architectures]: parallel architectures; D.4.1 [operating systems]: process management; D.4.7 [operating systems]: organization and design - Real-time systems and embedded systems; Design; Embedded runtime software; Hardware acceleration; Many-core architectures; Multicore architectures; Performance,Acceleration; Application programs; Design; Embedded systems; Hardware; Interactive computer systems; Memory architecture; Multicore programming; Multiprocessing systems; Multitasking; Parallel architectures; Real time systems; Reconfigurable hardware; Software architecture; Synchronization; Hardware acceleration; Many-core architecture; Multicore architectures; Performance; Process management; Processor architectures; Real-time systems and embedded systems; Register transfer level; Run-time software; Embedded application; Hardware accelerators; Many-core accelerators; Parallel application; Parallel implementations; Processing resources; Computer architecture; Application programs
GPU-like on-chip system for decoding LDPC codes,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995579366&doi=10.1145%2f2538668&partnerID=40&md5=5fff3052dcdb1dc8daf619f5ffcd0392,"Rapid prototyping is an important step in the development and the verification of computationally demanding tasks of digital communication systems, such as Forward Error Correction (FEC) decoding. The goal is to replace time-consuming simulations based on abstract models of the system with real-time experiments under real-world conditions. GPU-like architecture is a promising approach to fully exploit the potential of FPGA-based acceleration platforms. In this article, an application-specific GPU-like architecture and a complete compilation framework for decoding LDPC codes are proposed. The interest in an application-specific GPU in comparison with current GPUs is detailed. Finally, real-time experimentations demonstrate the potential of the GPU-like decoder to investigate both algorithmic and architectural issues. © 2014 ACM.",Alogrithms; C.3 [computer systems organization]: special-purpose and application-based systems; Design; FEC techniques; FPGA implementation; GPU-like architecture; LDPC codes; MIPS processor; Signal processing systems; SIMD matrix,Abstracting; Decoding; Design; Digital communication systems; Error correction; Field programmable gate arrays (FPGA); Forward error correction; Program processors; Reconfigurable hardware; Signal processing; Alogrithms; Computer systems organization; FEC techniques; FPGA implementations; LDPC codes; MIPS processor; Signal processing systems; Codes (symbols)
Elon: Enabling efficient and long-term reprogramming for wireless sensor networks,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995666059&doi=10.1145%2f2560017&partnerID=40&md5=bb9236d6e4c58198f2960416a4238265,"We present a new mechanism called Elon for enabling efficient and long-term reprogramming in wireless sensor networks. Elon reduces the transferred code size significantly by introducing the concept of replaceable component. It avoids the cost of hardware reboot with a novel software reboot mechanism. Moreover, it significantly prolongs the reprogrammable lifetime (i.e., the time period during which the sensor nodes can be reprogrammed) by avoiding flash writes for TelosB nodes. Experimental results show that Elon transfers up to 120-389 times less information than Deluge, and 18-42 times less information than Stream. The software reboot mechanism that Elon applies reduces the rebooting cost by 50.4%-53.87% in terms of beacon packets, and 56.83% in terms of unsynchronized nodes. In addition, Elon prolongs the reprogrammable lifetime by a factor of 3.3. © 2014 ACM.",C.2.1 [computer communication networks]: network architecture and design - Distributed networks; Component; D.4.7 [operating systems]: organization and design; Design; Experimentation; Performance; Reboot; Reprogramming; Wireless sensor network,Computer operating systems; Design; Distributed computer systems; Network architecture; Sensor nodes; Component; Distributed networks; Experimentation; Performance; Reboot; Reprogramming; Wireless sensor networks
Editorial: Special Section on ESTIMedia'13,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995686740&doi=10.1145%2f2567942&partnerID=40&md5=ed22b390355ac802c87ef473c922fe89,[No abstract available],,
NoC contention analysis using a branch-and-prune algorithm,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995678317&doi=10.1145%2f2567937&partnerID=40&md5=e48e0a78e937cbbe1b9e9a60fdcdf757,"""Many-core"" systems based on a Network-on-Chip (NoC) architecture offer various opportunities in terms of performance and computing capabilities, but at the same time they pose many challenges for the deployment of real-time systems, which must fulfill specific timing requirements at runtime. It is therefore essential to identify, at design time, the parameters that have an impact on the execution time of the tasks deployed on these systems and the upper bounds on the other key parameters. The focus of this work is to determine an upper bound on the traversal time of a packet when it is transmitted over the NoC infrastructure. Towards this aim, we first identify and explore some limitations in the existing recursive-calculus-based approaches to compute the Worst-Case Traversal Time (WCTT) of a packet. Then, we extend the existing model by integrating the characteristics of the tasks that generate the packets. For this extended model, we propose an algorithm called ""Branch and Prune"" (BP). Our proposedmethod provides tighter and safe estimates than the existing recursive-calculus-based approaches. Finally, we introduce a more general approach, namely ""Branch, Prune and Collapse"" (BPC) which offers a configurable parameter that provides a flexible trade-off between the computational complexity and the tightness of the computed estimate. The recursive-calculus methods and BP present two special cases of BPC when a trade-off parameter is 1 or ∞, respectively. Through simulations, we analyze this trade-off, reason about the implications of certain choices, and also provide some case studies to observe the impact of task parameters on the WCTT estimates. © 2014 ACM.",Algorithms; B.4 [input/output and data communications]: performance analysis and design aids - worst-case analysis; C.3 [computer systems organization]: multiple data stream architectures - interconnection architectures; C.3 [computer systems organization]: special purpose and application based systems - real-time and embedded systems; Design; Many-core systems; Network-on-chip; Performance; Real-time systems; Wormhole routing,Algorithms; Calculations; Complex networks; Computer architecture; Computer hardware description languages; Design; Distributed computer systems; Economic and social effects; Embedded systems; Fault tolerant computer systems; Interactive computer systems; Network architecture; Network-on-chip; Routers; Servers; Data-communication; Many core; Multiple data streams; Performance; Real-time and embedded systems; Wormhole routing; Branch and prunes; Branch-and-prune algorithms; Computing capability; Configurable parameter; Extended model; Network-on-chip architectures; Recursive calculus; Timing requirements; Real time systems
ForEVeR: A complementary formal and runtime verification approach to correct NoC functionality,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995624306&doi=10.1145%2f2514871&partnerID=40&md5=fa3df47b28df08c4a5b0d6ff29e65407,"As silicon technology scales, modern processor and embedded systems are rapidly shifting towards complex chip multi-processor (CMP) and system-on-chip (SoC) designs. As a side effect of complexity of these designs, ensuring their correctness has become increasingly problematic. Within these domains, Network-on-Chips (NoCs) are a de-facto choice to implement on-chip interconnect; their design is quickly becoming extremely complex in order to keep up with communication performance demands. As a result, design errors in the NoC may go undetected and escape into the final silicon. In this work, we propose ForEVeR, a solution that complements the use of formal methods and runtime verification to ensure functional correctness in NoCs. Formal verification, due to its scalability limitations, is used to verify smaller modules, such as individual router components. To deliver correctness guarantees for the complete network, we propose a network-level detection and recovery solution that monitors the traffic in the NoC and protects it against escaped functional bugs. To this end, ForEVeR augments the baseline NoC with a lightweight checker network that alerts destination nodes of incoming packets ahead of time. If a bug is detected, flagged by missed packet arrivals, our recovery mechanism delivers the in-flight data safely to the intended destination via the checker network. ForEVeR's experimental evaluation shows that it can recover from NoC design errors at only 4.9% area cost for an 8x8 mesh interconnect, over a time interval ranging from 0.5K to 30K cycles per recovery event, and it incurs no performance overhead in the absence of errors. ForEVeR can also protect NoC operations against soft-errors: a growing concern with the scaling of silicon. ForEVeR leverages the same monitoring hardware to detect soft-error manifestations, in addition to design-errors. Recovery of the soft-error affected packets is guaranteed by building resiliency features into our checker network. ForEVeR incurs minimal performance penalty up to a flit error rate of 0.01% in lightly loaded networks. © 2014 ACM.",C.1.2 [processor architectures]: multiprocessors; Formal verification; Functional correctness; Network-on-chip; NoC; Reliability; Runtime verification; Verification,Complex networks; Computer system recovery; Distributed computer systems; Embedded systems; Error correction; Errors; Fault tolerant computer systems; Formal methods; Formal verification; Integrated circuit interconnects; Multiprocessing systems; Network-on-chip; Programmable logic controllers; Radiation hardening; Recovery; Reliability; Routers; Silicon; System-on-chip; Verification; Chip multiprocessors; Communication performance; Experimental evaluation; Functional correctness; Performance penalties; Processor architectures; Run-time verification; System on chip design; On chip interconnect; Integrated circuit design
PAIS: Parallelism-aware interconnect scheduling in multicores,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995543628&doi=10.1145%2f2567934&partnerID=40&md5=2cb5da3d9262b31844da873dad9894c1,"Multicore processors have the potential to deliver scalable performance by distributing computation across multiple cores. However, the communication cost of parallel application thread execution may significantly limit the performance achievable due to latency and contention on shared resources in the on-chip network of multicores experienced by packets from critical threads. We present PAIS, Parallelism-Aware Interconnect Scheduling, that bolsters performance and energy efficiency of parallel applications. PAIS dynamically detects thread execution progress based on communication latency and scheduling, and it accelerates communication for slowly executing threads by prioritizing packets from those threads with flow control and priority-based arbitration. © 2014 ACM.","C.1.2 [processor architectures]: multiple data stream architectures - multiprocessors, interconnection architectures; Design; On-chip network; Parallelism; Performance; Scheduling",Design; Energy efficiency; Integrated circuit interconnects; Network architecture; Scheduling; Scheduling algorithms; Communication latency; Multi-core processor; Multiple data streams; On-chip networks; Parallel application; Parallelism; Performance; Scalable performance; Communication cost; Priority-based; Shared resources; Parallel processing systems; Integrated circuit interconnects
Bandwidth allocation for fixed-priority-scheduled compositional real-time systems,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995563221&doi=10.1145%2f2560038&partnerID=40&md5=76a20f8c73148b96c25ea03954de4e32,"Recent research in compositional real-time systems has focused on determination of a component's real-time interface parameters. An important objective in interface-parameter determination is minimizing the bandwidth allocated to each component of the system while simultaneously guaranteeing component schedulability. With this goal in mind, in this article, we explore fixed-priority schedulability in compositional setting. First we derive an efficient exact test based on iterative convergence for sporadic task systems scheduled by fixed-priority (e.g., deadline monotonic, rate monotonic) upon an explicit-deadline periodic (EDP) resource. Then we address the time complexity of the exact test by developing a fully-polynomial-time approximation scheme (FPTAS) for allocating bandwidth to components. Our parametric algorithm takes the task system and an accuracy parameter ε > 0 as input and returns a bandwidth which is guaranteed to be at most a factor (1 + ε) times the optimal minimum bandwidth required to successfully schedule the task system. We perform thorough simulation over synthetically generated task systems to compare the performance of our proposed efficient-exact and the approximate algorithm and observe a significant decrease in runtime and a very small relative error when comparing the approximate algorithm with the exact algorithm and the sufficient algorithm. © 2014 ACM.",Algorithms; Bandwidth allocation; C.3 [computer systems organization]: special-purpose and application-based systems - real-time and embedded systems; Compositional real-time systems; D.4.7 [operating systems]: organization and design - real-time and embedded systems; Design; Fixed-priority scheduling; FPTAS; Theory,Algorithms; Bandwidth; Computer operating systems; Design; Embedded systems; Frequency allocation; Interactive computer systems; Iterative methods; Polynomial approximation; Approximate algorithms; Fixed priority scheduling; FPTAS; Fully polynomial time approximation schemes; Parametric algorithms; Real-time and embedded systems; Sporadic task systems; Theory; Real time systems
METEOR: Hybrid photonic ring-mesh network-on-chip for multicore architectures,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995562500&doi=10.1145%2f2567940&partnerID=40&md5=8734a5079db0e0590667e5e8cdb0babb,"With increasing application complexity and improvements in process technology, Chip MultiProcessors (CMPs) with tens to hundreds of cores on a chip are becoming a reality. Networks-on-Chip (NoCs) have emerged as scalable communication fabrics that can support high bandwidths for these massively parallel multicore systems. However, traditional electrical NoC implementations still need to overcome the challenges of high data transfer latencies and large power consumption. On-chip photonic interconnects with high performance-per-watt characteristics have recently been proposed as an alternative to address these challenges for intra-chip communication. In this article, we explore using low-cost photonic interconnects on a chip to enhance traditional electrical NoCs. Our proposed hybrid photonic ring-mesh NoC (METEOR) utilizes a configurable photonic ring waveguide coupled to a traditional 2D electrical mesh NoC. Experimental results indicate a strong motivation to consider the proposed architecture for future CMPs, as it can provide about 5x reduction in power consumption and improved throughput and access latencies, compared to traditional electrical 2D mesh and torus NoC architectures. Compared to other previously proposed hybrid photonic NoC fabrics such as the hybrid photonic torus, Corona, and Firefly, our proposed fabric is also shown to have lower photonic area overhead, power consumption, and energy-delay product, while maintaining competitive throughput and latency. © 2014 ACM.","B.71 [integrated circuits]: types and design styles VLSI (very large scale integration); C.1.2 [interconnection architectures]: common bus, multiport memory, crossbar switch; C.1.2 [processor architectures]: multiple data stream architectures (multiprocessors); Chip multiprocessor; Design; Experimentation; Network-on-chip; Performance; Photonic interconnect",Adaptive systems; Complex networks; Computer architecture; Data transfer; Design; Distributed computer systems; Electric power utilization; Fault tolerant computer systems; Integrated circuit interconnects; Mesh generation; Multiprocessing systems; Network architecture; Servers; Software architecture; VLSI circuits; Chip Multiprocessor; Design styles; Experimentation; Multi-port memory; Multiple data stream architectures (multiprocessors); Performance; Application complexity; Chip multi-processors (CMPs); In-process technology; Intra-chip communications; Multicore architectures; Photonic interconnects; Proposed architectures; Scalable communication; Network-on-chip
"Towards a scalable, low-power all-optical architecture for networks-on-chip",2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995704511&doi=10.1145%2f2567930&partnerID=40&md5=abfc29a58b41d7e0955e68f615f84cd2,"This article proposes a scalable wavelength-routed optical Network on Chip (NoC) based on the Spidergon topology, named Power-efficient Scalable Wavelength-routed Network-on-chip (PeSWaN). The key idea of the proposed all-optical architecture is the utilization of per-receiver wavelengths in the data network to prevent network contention and the adoption of per-sender wavelengths in the control network to avoid end-point contention. By performing a series of simulations, we study the efficiency of the proposed architecture, its power and energy consumption, and the data transmission delay. Moreover, we compare the proposed architecture with electrical NoCs and alternative ONoC architectures under various traffic patterns. © 2014 ACM.",Algorithms; B.4.3 [input/output and data communications]: interconnections - optics; B.4.4 [input/output and data communications]: performance analysis and design aids - simulation; B.7.1 [integrated circuits]: types and design styles - advanced technologies; Design; Network-on-chip; Optics; Performance; Power consumption; Scalability; Wavelength routing,Algorithms; Convolutional codes; Design; Electric power utilization; Energy utilization; Low power electronics; Network architecture; Optical communication; Optics; Scalability; Servers; Data transmission delay; Data-communication; Design styles; Performance; Proposed architectures; Wavelength routed optical networks; Wavelength routing; Wavelength-routed networks; Control network; Network contention; Networks on chips; Traffic pattern; Network-on-chip
On-chip traffic regulation to reduce coherence protocol cost on a microthreaded many-core architecture with distributed caches,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995673161&doi=10.1145%2f2567931&partnerID=40&md5=052e91d28328057a379097f8244efdc7,"When hardware cache coherence scales to many cores on chip, over saturated traffic of the shared memory system may offset the benefit from massive hardware concurrency. In this article, we investigate the cost of a write-update protocol in terms of on-chip memory network traffic and its adverse effects on the system performance based on a multithreaded many-core architecture with distributed caches. We discuss possible software and hardware solutions to alleviate the network pressure. We find that in the context of massive concurrency, by introducing a write-merging buffer with 0.46% area overhead to each core, applications with good locality and concurrency are boosted up by 18.74% in performance on average. Other applications also benefit from this addition and even achieve a throughput increase of 5.93%. In addition, this improvement indicates that higher levels of concurrency per core can be exploited without impacting performance, thus tolerating latency better and giving higher processor efficiencies compared to other solutions. © 2014 ACM.",C.4.0 [performance of systems]: design studies; Design; Distributed cache; Experimentation; Hardware coherence; Many-core system; Massive parallelism; On-chip memory network; Performance; Write combination,Cache memory; Design; Hardware; Internet protocols; Memory architecture; Multiprocessing systems; Network architecture; Reconfigurable hardware; Design studies; Distributed cache; Experimentation; Many core; Massive parallelism; On chip memory; Performance; Write combination; Coherence protocol; Many-core architecture; On-chip traffic; Performance based; Shared memory system; Software and hardwares; Write update protocols; Computer architecture; Network architecture
Post-silicon platform for the functional diagnosis and debug of networks-on-chip,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995572386&doi=10.1145%2f2567936&partnerID=40&md5=c6224c93613b2640a6d6b9534fd189f7,"The increasing number of units in today's systems-on-chip andmulticore processors has led to complex intrachip communication solutions. Specifically, Networks-on-Chip (NoCs) have emerged as a favorable fabric to provide high bandwidth and low latency in connecting many units in a same chip. To achieve these goals, the NoC often includes complex components and advanced features, leading to the development of large and highly complex interconnect subsystems. One of the biggest challenges in these designs is to ensure the correct functionality of this communication infrastructure. To support this goal, an increasing fraction of the validation effort has shifted to post-silicon validation, because it permits exercising network activities that are too complex to be validated in pre-silicon. However, post-silicon validation is hindered by the lack of observability of the network's internal operations and thus, diagnosing functional errors during this phase is very difficult. In this work, we propose a post-silicon validation platform that improves observability of network operations by taking periodic snapshots of the traffic traversing the network. Each node's local cache is configured to temporarily store the snapshot logs in a designated area reserved for post-silicon validation and relinquished after product release. Each snapshot log is analyzed locally by a software algorithm running on its corresponding core, in order to detect functional errors. Upon error detection, all snapshot logs are aggregated at a central location to extract additional debug data, including an overview of network traffic surrounding the error event, as well as a partial reconstruction of the routes followed by packets in flight at the time. In our experiments, we found that this approach allows us to detect several types of functional errors, as well as observe, on average, over 50% of the network's traffic and reconstruct at least half of each of their routes through the network. © 2014 ACM.",C.1.2 [processor architectures]: multiprocessors; Functional correctness; Networks-on-chip; Performance monitoring; Post-silicon validation; Verification,Complex networks; Errors; Observability; Silicon; System-on-chip; Verification; Functional correctness; Networks on chips; Performance monitoring; Post-silicon validations; Processor architectures; Communication infrastructure; Internal operations; Intra-chip communications; Multi-core processor; Network activities; Network operations; Software algorithms; Network-on-chip
Transport triggered architecture to perform carrier synchronization for LTE,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995543047&doi=10.1145%2f2560036&partnerID=40&md5=72f05916a269b91d20860fc74684d294,"In this article implementation of carrier frequency offset estimate for 20MHz LTE baseband processing is discussed. LTE (Long Term Evolution) is a wireless communication standard that makes use of some innovative techniques to gain very high data rates (>100Mbps). This goal for such a high throughput also imposes design challenges for the industry and academia such as in the case of handheld mobile devices where the power budget is very limited. Implicitly high throughput means we need more computation power and more energy. On the other hand industry is also struggling for a flexible hardware solution, or software defined a radio (SDR), to amortize the huge cost of required hardware changes as the wireless standards have kept evolving. Design innovations are now needed to confront those challenges of low power and flexible design without changing the hardware. The implementation is made on Transport Triggered Architecture (TTA), which is a unique concept in computer architecture design, based on the single instruction, ""MOVE"". The power consumption of the architecture when synthesized on 180nm technology at 180MHz and 1.8V is 18.39mW. The total area occupied excluding memory is 0.6mm2. The proposed TTA solution has been compared with, a more ASIC (application specific integrated circuits), like ASIP (application specific instruction processor) solution and a coprocessor accelerator-based solution. The proposed solution is more flexible: easily programmable due to high level language support, easily scalable, and still efficient in energy consumption needed to complete the CFO (carrier frequency offset) estimation task. Because of these attractive characteristics, TTA is also a potential candidate for SDR platforms. © 2014 ACM.",3gpp LTE(Long Term Evolution); ASIP; C.3 [special-purpose and application-based systems]: real-time and embedded systems; Carrier synchronization; Design; Performance; TTA(Transport Triggered Architecture); VLIW,Budget control; Computer architecture; Computer hardware; Computer programming languages; Design; Embedded systems; Energy utilization; Frequency allocation; Frequency estimation; Hardware; High level languages; Integrated circuit design; Mobile devices; Mobile telecommunication systems; Real time systems; Reconfigurable hardware; Signal encoding; Throughput; Very long instruction word architecture; Wireless telecommunication systems; 3gpp lte; ASIP; Carrier synchronization; Performance; Real-time and embedded systems; Transport triggered architecture; VLIW; Long Term Evolution (LTE)
Simulation-based functional verification of dynamically reconfigurable systems,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930373303&doi=10.1145%2f2560042&partnerID=40&md5=dfc0d876c9da1d3da66cd8219876d2b8,"Dynamically reconfigurable systems (DRS) implemented using field-programmable gate arrays (FPGAs) allow hardware logic to be partially reconfigured while the rest of the design continues to operate. By mapping multiple reconfigurable hardware modules to the same physical region of an FPGA, such systems are able to time-multiplex their modules at runtime and adapt themselves to changing execution requirements. This architectural flexibility introduces challenges for verifying system functionality. New simulation approaches are required to extend traditional simulation techniques to assist designers in testing and debugging the time-varying behavior of DRS. This article summarizes our previous work on ReSim, the first tool to allow cycle-accurate yet physically independent simulation of a DRS reconfiguring both its logic and state. Furthermore, ReSim-based simulation does not require changing the design for simulation purposes and thereby verifies the implementation-ready design instead of a variation of the design. We discuss the conflicting requirements of simulation accuracy and verification productivity in verifying DRS designs and describe our approach to resolve this challenge. Through a range of case studies, we demonstrate that ReSim assists designers in detecting fabric-independent bugs of DRS designs and helps to achieve verification closure of DRS design projects. © 2014 ACM.",Dynamically reconfigurable systems; FPGA; Verification,Computer hardware; Design; Embedded systems; Field programmable gate arrays (FPGA); Hardware; Integrated circuit design; Microprocessor chips; Structural design; Telecommunication networks; Verification; Dynamically reconfigurable systems; Functional verification; Simulation accuracy; Simulation approach; Simulation technique; System functionality; Testing and debugging; Time varying behavior; Reconfigurable hardware
Online learning of timeout policies for dynamic power management,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930327129&doi=10.1145%2f2529992&partnerID=40&md5=e16654008eb59887c6f71c381e78d609,"Dynamic power management (DPM) refers to strategies which selectively change the operational states of a device during runtime to reduce the power consumption based on the past usage pattern, the current workload, and the given performance constraint. The powermanagement problem becomesmore challenging when the workload exhibits nonstationary behavior which may degrade the performance of any single or static DPM policy. This article presents a reinforcement learning (RL)-based DPM technique for optimal selection of timeout values in the different device states. Each timeout period determines how long the device will remain in a particular state before the transition decision is taken. The timeout selection is based on workload estimates derived from a Multilayer Artificial Neural Network (ML-ANN) and an objective function given by weighted performance and power parameters. Our DPM approach is further able to adapt the powerperformance weights online to meet user-specified power and performance constraints, respectively.We have completely implemented our DPM algorithm on our embedded traffic surveillance platform and performed long-term experiments using real traffic data to demonstrate the effectiveness of the DPM. Our results show that the proposed learning algorithm not only adequately explores the power-performance trade-off with nonstationary workload but can also successfully perform online adjustment of the trade-off parameter in order to meet the user-specified constraint. © 2014 ACM.",Dynamic powermanagement; Online learning; Reinforcement learning; Timeout policies; Trafficmonitoring,Economic and social effects; Energy management; Learning algorithms; Neural networks; Parameter estimation; Power management; Reinforcement learning; Social networking (online); Multilayer artificial neural networks; Non-stationary behaviors; Online learning; Power-management problem; Power-performance trade-offs; Timeout policy; Trafficmonitoring; User-specified constraints; E-learning
Message blinding method requiring no multiplicative inversion for RSA,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930345111&doi=10.1145%2f2560020&partnerID=40&md5=eb36d0092b5e60fa6733a7122792e168,"This article proposes a new message blinding methods requiring no multiplicative inversion for RSA. Most existing message blinding methods for RSA additionally require the multiplicative inversion, even though computational complexity of this operation is O(n3) which is equal to that of the exponentiation. Thus, this additional operation is known to be the main drawback of the existing message blinding methods for RSA. In addition to requiring no additional multiplicative inversion, our new countermeasure provides the security against various power analysis attacks as well as general differential power analysis. © 2014 ACM.",Message blinding method; Power analysis; Side channel attacks,Hardware security; Differential power Analysis; Exponentiations; Message blinding method; Multiplicative inversion; Power analysis; Power analysis attack; Side channel attack
Python to accelerate embedded SoC design: A case study for systems biology,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930358823&doi=10.1145%2f2560032&partnerID=40&md5=ab6052e3002a168600b174f89e323a16,"We present SysPy (System Python) a tool which exploits the strengths of the popular Python scripting language to boost design productivity of embedded System on Chips for FPGAs. SysPy acts as a ""glue"" software between mature HDLs, ready-to-use VHDL components and programmable processor soft IP cores. SysPy can be used to: (i) automatically translate hardware components described in Python into synthesizable VHDL, (ii) capture top-level structural descriptions of processor-centric SoCs in Python, (iii) implement all the steps necessary to compile the user's C code for an instruction set processor core and generate processor specific Tcl scripts that import to the design project all the necessary HDL files of the processor's description and instantiate/connect the core to other blocks in a synthesizable top-level Python description. Moreover, we have developed a Hardware Abstraction Layer (HAL) in Python which allows user applications running in a host PC to utilize effortlessly the SoC's resources in the FPGA. SysPy's design capabilities, when complemented with the developed HAL software API, provide all the necessary tools for hw/sw partitioning and iterative design for efficient SoC's performance tuning. We demonstrate how SysPy's design flow and functionalities can be used by building a processor-centric embedded SoC for computational systems biology. The designed SoC, implemented using a Xilinx Virtex-5 FPGA, combines the flexibility of a programmable soft processor core (Leon3) with the high performance of an application specific core to simulate flexibly and efficiently the stochastic behavior of large size biomolecular reaction networks. Such networks are essential for studying the dynamics of complex biological systems consisting of multiple interacting pathways. © 2014 ACM.",Biomolecular reaction networks; FPGA; Gillespie's Stochastic Simulation Algorithm; Hw/sw co-design; Python; Scripting languages; SoC; SysPy; Systems Biology; VHDL,Abstracting; Biology; C (programming language); Complex networks; Computational linguistics; Computer hardware description languages; Computer simulation languages; Computer software; Design; Embedded systems; Field programmable gate arrays (FPGA); Hardware; Hardware-software codesign; High level languages; Iterative methods; Microprocessor chips; Stochastic models; Stochastic systems; System-on-chip; Biomolecular reactions; HW/SW Codesign; Python; Scripting languages; SoC; Stochastic simulation algorithms; SysPy; Systems biology; VHDL; Integrated circuit design
Rapid evaluation of custom instruction selection approaches with FPGA estimation,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930354010&doi=10.1145%2f2560014&partnerID=40&md5=b41f9e3395f223f82fe1f79fb315ff37,"The main aim of this article is to demonstrate that a fast and accurate FPGA estimation engine is indispensable in design flows for custom instruction (template) selection. The need for a FPGA estimation engine stems from the difficulty in predicting the FPGA performance measures of selected custom instructions. We will present a FPGA estimation technique that partitions the high-level representation of custom instructions into clusters based on the structural organization of the target FPGA, while taking into account general logic synthesis principles adopted by FPGA tools. In this work, we have evaluated a widely used graph covering algorithm with various heuristics for custom instruction selection. In addition, we present an algorithm called Refined Largest Fit First (RLFF) that relies on a graph covering heuristic to select nonoverlapping superset templates, which typically incorporate frequently used basic templates. The initial solution is further refined by considering overlapping templates that were ignored previously to see if their introduction could lead to higher performance. While RLFF provides the most efficient cover compared to the ILP method and other graph covering heuristics, FPGA estimation results reveals that RLFF leads to the worst performance in certain applications. It is therefore a worthy proposition to equip design flows with accurate FPGA estimation in order to rapidly determine the most profitable custom instruction approach for a given application. © 2014 ACM.",Approximation algorithms; Customizable processors; FPGA; ISA extension,Algorithms; Approximation algorithms; Engines; Field programmable gate arrays (FPGA); Heuristic methods; Inductive logic programming (ILP); Logic Synthesis; Custom instruction; Customizable processors; Estimation results; Estimation techniques; Initial solution; ISA extension; Performance measure; Structural organization; Integrated circuit design
Towards scalable arithmetic units with graceful degradation,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930371540&doi=10.1145%2f2499367&partnerID=40&md5=9b867d94ce132283a6b24ccd2bfcea38,"This article presents a new family of scalable arithmetic units (ScAUs) targeting resource-constrained, embedded devices. We, first, study the performance, power, area and scalability properties of general adders. Next, suitable error-detection schemes for low-power embedded systems are discussed. As a result, our ScAUs are enhanced with a suitable error-detection scheme, resulting in a Parity-Checked ScAU (PCScAU) design. The PCScAU strikes a flexible trade-off between space and time redundancy, offering dependability similar to high-end techniques for the area and power cost of low-end approaches. An alternative design, the Precision-Scalable Arithmetic Unit (PScAU) maintains throughput with degraded precision in case of hardware failures. The PScAU is targeting dependable applications where latency rather than numerical accuracy is more important. The PScAU's downscaled mode is also interesting for runtime thermal management due to its advantageous power consumption.We implemented and synthesized the PCScAU, PScAUand a few important reference designs (double-, triple- and quadruple-modular-redundancy adders with/without input gating) in 90-nm UMC technology. Overall, the PC-ScAU ranks first in 9 out of 10 power-delay-area (PDA)-product variants. It exhibits 16% area savings and 12% performance speedup for 7% increase in total power consumption, compared to the cheapest form of conventional hardware replication with the same fault coverage. The PDA product of the PCScAU is, thus, reduced by 21%. It is interesting that, while total power slightly increases, the PCScAU static power in fact decreases by 14%. Therefore, for newer technology nodes where the static power component is significant, the PCScAU can also achieve-next to performance and area - significant power improvements. © 2014 ACM.",Computer arithmetic; Embedded systems; Error correction; Error detection; Fault tolerance; Graceful degradation; Low power consumption; Scalable design,Adders; Computer hardware; Design; Economic and social effects; Error correction; Error detection; Errors; Fault detection; Fault tolerance; Hardware; Program processors; Redundancy; Alternative designs; Computer arithmetic; Graceful degradation; Low power embedded systems; Low-power consumption; Power improvements; Scalable design; Total power consumption; Embedded systems
A multiple-FPGA parallel computing architecture for real-time simulation of soft-object deformation,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930362956&doi=10.1145%2f2560031&partnerID=40&md5=dde82d5a16cccb3ecf6503ae7269c401,Hardware-based parallel computing is proposed for acceleration of finite-element (FE) analysis of linear elastic deformation models. An implementation of the Preconditioned Conjugate Gradient algorithm on N Field Programmable Gate Array (FPGA) devices solves the large linear system of equations arising from the FE discretization. The system employs a large number of customized fixed-point computing units with a high-throughput memory architecture. An implementation of this scalable architecture on four Altera EP3SE110 FPGA devices yields a peak performance of 604 Giga Operations per second. This enables haptic simulation of a 3-dimensional deformable object of 21000 elements at an update rate of 400Hz. © 2014 ACM.,Conjugate gradient method; Deformation modeling; Field-programmable gate array; Finite-element method; Hardware acceleration; High-performance computing; Parallel computing; Real-time simulation; Soft-tissue modeling,Computer architecture; Conjugate gradient method; Finite element method; Hardware; Linear systems; Logic gates; Memory architecture; Parallel architectures; Parallel processing systems; Transients; Deformation modeling; Hardware acceleration; High performance computing; Real time simulations; Soft tissue modeling; Field programmable gate arrays (FPGA)
Building timing predictable embedded systems,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961291367&doi=10.1145%2f2560033&partnerID=40&md5=7f03351c9228f92bdbf2843674e07b8c,"A large class of embedded systems is distinguished from general-purpose computing systems by the need to satisfy strict requirements on timing, often under constraints on available resources. Predictable system design is concerned with the challenge of building systems for which timing requirements can be guaranteed a priori. Perhaps paradoxically, this problem has become more difficult by the introduction of performanceenhancing architectural elements, such as caches, pipelines, and multithreading, which introduce a large degree of uncertainty and make guarantees harder to provide. The intention of this article is to summarize the current state of the art in research concerning how to build predictable yet performant systems. We suggest precise definitions for the concept of ""predictability"", and present predictability concerns at different abstraction levels in embedded system design. First, we consider timing predictability of processor instruction sets. Thereafter, we consider how programming languages can be equipped with predictable timing semantics, covering both a language-based approach using the synchronous programming paradigm, as well as an environment that provides timing semantics for a mainstream programming language (in this case C). We present techniques for achieving timing predictability on multicores. Finally, we discuss how to handle predictability at the level of networked embedded systems where randomly occurring errors must be considered. © 2014 ACM.",Embedded systems; Predictability; Resource sharing; Safety-critical systems; Timing analysis,C (programming language); Computational linguistics; Embedded software; Semantics; Systems analysis; Architectural element; General-purpose computing; Networked embedded systems; Predictability; Resource sharing; Safety critical systems; Synchronous programming; Timing Analysis; Embedded systems
Editorial: Embedded everywhere for everyone,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930319281&doi=10.1145%2f2559122&partnerID=40&md5=feeb994097f3ed8679d004d14d8bb81c,[No abstract available],,
Simultaneous hardware and time redundancy with online task scheduling for low energy highly reliable standby-sparing system,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930326526&doi=10.1145%2f2523781%2f2560035&partnerID=40&md5=ec958436dba061436bf2e5f4f7a1c531,"Standby-sparing is one of the common techniques in order to design fault-tolerant safety-critical systems where the high level of reliability is needed. Recently, the minimization of energy consumption in embedded systems has attracted a lot of concerns. Simultaneous considering of high reliability and low energy consumption by DVS is a challenging problem in designing such a system, since using DVS has been shown to reduce the reliability profoundly. In this article, we have studied different schemes of standby-sparing systems from the energy consumption and reliability point of view.Moreover, we propose a new standby-sparing scheme which addresses both reliability and energy consumption jointly together. This scheme uses a simple energy management coupled with an online task scheduler which tries to dispatch those ready tasks which are expected to lead to high reliability and low energy consumption in the system. The effectiveness of the proposed scheme has been shown on TGFF under stochastic workloads. The results show 52% improvement on energy saving compared to the conventional hot standby-sparing system. Moreover, two orders of magnitude higher reliability is obtained on average, while preserving the same level of energy saving as compared to the state-of-the-art low-energy standby-sparing system (LESS). © 2014 ACM.",Energy consumption; Hard real-time systems; Reliability; Scheduling,Embedded systems; Energy conservation; Energy utilization; Interactive computer systems; Multitasking; Reliability; Scheduling; Social networking (online); Stochastic systems; Voltage scaling; Hard real-time systems; High reliability; Low energy consumption; Orders of magnitude; Safety critical systems; State of the art; Task-scheduling; Time redundancy; Real time systems
Extended instruction exploration for multiple-issue architectures,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930339673&doi=10.1145%2f2560039&partnerID=40&md5=8b93678e180e668746aa34dd040bcc48,"In order to satisfy the growing demand for high-performance computing in modern embedded devices, several architectural and microarchitectural enhancements have been implemented in processor architectures. Extended instruction (EI) is often used for architectural enhancement, while issuing multiple instructions is a common approach for microarchitectural enhancement. The impact of combining both of these approaches in the same design is not well understood.While previous studies have shown that EI can potentially improve performance in some applications on certain multiple-issue architectures, the algorithms used to identify EI for multiple-issue architectures yield only limited performance improvement. This is because not all arithmetic operations are suited for EI for multiple-issue architectures. To explore the full potential of EI for multiple-issue architectures, two important factors need to be considered: (1) the execution performance of an application is dominated by critical (located on the critical path) and highly resource-contentious (i.e., having a high probability of being delayed during execution due to hardware resource limitations) operations, and (2) an operation may become critical and/or highly resource contentious after some operations are added to the EI. This article presents an EI exploration algorithm for multiple-issue architectures that focuses on these two factors. Simulation results show that the proposed algorithm outperforms previously published algorithms. © 2014 ACM.",Application-specific instruction-set processor (ASIP); Customizable processor; Extended instruction (EI); Instruction set extension (ISE); Multiple-issue architecture,Algorithms; Application specific integrated circuits; Microprocessor chips; Program processors; Application specific instruction set processor; Customizable processors; Extended instruction (EI); Instruction set extension; Multiple issue; Computer architecture
Adaptive wear-leveling algorithm for PRAM main memory with a DRAM buffer,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908288850&doi=10.1145%2f2558427&partnerID=40&md5=94d29d5c5fb1a84e45f9b9e25f66316b,"Phase Change RAM (PRAM) is a candidate to replace DRAM main memory due to its low idle power consumption and high scalability. However, its latency and endurance have generated problems in fulfilling its main memory role. The latency can be treated with a DRAM buffer, but the endurance problem remains, with three critical points that need to be improved despite the use of, existing wear-leveling algorithms. First, existing DRAM buffering schemes do not consider write count distribution. Second, swapping and shifting operations are performed statically. Finally, swapping and shifting operations are loosely coupled with a DRAM buffer. As a remedy to these drawbacks, we propose an adaptive wear-leveling algorithm that consists of three novel schemes for PRAM main memory with a DRAM buffer. The PRAM-aware DRAM buffering scheme reduces the write count and prevents skewed writing by considering the write count and clean data based on the least recently used (LRU) scheme. The adaptive multiple swapping and shifting scheme makes the write count even with the dynamic operation timing, the number of swapping pages being based on the workload pattern. Our DRAM buffer-aware swapping and shifting scheme reduces overhead by curbing additional swapping and shifting operations, thus reducing unnecessary write operations. To evaluate the wear-leveling effect, we have implemented a PIN-based wear-leveling simulator. The evaluation confirms that the PRAM lifetime increases from 0.68 years with the previous wear-leveling algorithm to 5.32 years with the adaptive wear-leveling algorithm. © 2014 ACM.",Adaptivewear-leveling; DRAMbuffering; PIN-based simulator; PRAMmain memory; Swapping and shifting,Algorithms; Phase change memory; Random access storage; DRAMbuffering; Dynamic operations; High scalabilities; Least recently used; Phase change rams; Swapping and shifting; Wear-leveling algorithms; Workload patterns; Dynamic random access storage
Embedded RAIDs-on-chip for bus-based chip-multiprocessors,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930335407&doi=10.1145%2f2533316&partnerID=40&md5=6c3f7c8f1c3c9c1477e45bc8e28b2eff,"The dual effects of larger die sizes and technology scaling, combined with aggressive voltage scaling for power reduction, increase the error rates for on-chip memories. Traditional on-chip memory reliability techniques (e.g., ECC) incur significant power and performance overheads. In this article, we propose a low-power-andperformance- overhead Embedded RAID (E-RAID) strategy and present Embedded RAIDs-on-Chip (E-RoC), a distributed dynamically managed reliable memory subsystem for bus-based Chip-Multiprocessors. E-RoC achieves reliability through redundancy by optimizing RAID-like policies tuned for on-chip distributed memories. We achieve on-chip reliability of memories through the use of Distributed Dynamic ScratchPad Allocatable Memories (DSPAMs) and their allocation policies. We exploit aggressive voltage scaling to reduce power consumption overheads due to parallel DSPAM accesses, and rely on the E-RoC Manager to automatically handle any resulting voltage-scaling-induced errors. We demonstrate how E-RAIDs can further enhance the fault tolerance of traditional memory reliability approaches by designing E-RAID levels that exploit ECC. Finally, we show the power and flexibility of the E-RoC concept by showing the benefits of having a heterogeneous E-RAID levels that fit each application's needs (fault tolerance, power/energy, performance). Our experimental results on CHStone/Mediabench II benchmarks show that our E-RAID levels converge to 100% error-free data rates much faster than traditional ECC approaches. Moreover, E-RAID levels that exploit ECC can guarantee 99.9% error-free data rates at ultra low Vdd on average, where as traditional ECC approaches were able to attain at most 99.1% error-free data rates. We observe an average of 22% dynamic power consumption increase by using traditional ECC approaches with respect to the baseline (non-voltage scaled SPMs), whereas our E-RAID levels are able to save dynamic power consumption by an average of 27% (w.r.t. the same non-voltage scaled SPMs baseline), while incurring worst-case 2% higher performance overheads than traditional ECC approaches. By voltage scaling the memories, we see that traditional ECC approaches are able to save static energy by 6.4% (average), where as our E-RAID approaches achieve 23.4% static energy savings (average). Finally, we observe that mixing E-RAID levels allows us to further reduce the dynamic power consumption by up to 55.5% at the cost of an average 5.6% increase in execution time over traditional approaches. © 2014 ACM.",Chip-multiprocessors; Embedded systems; Information assurance; Policy; Scratchpad memory; Security; Virtualization,Cost reduction; Embedded systems; Energy conservation; Errors; Memory architecture; Microprocessor chips; Multiprocessing systems; Public policy; Redundancy; Reliability; Voltage scaling; Chip Multiprocessor; Information assurance; Scratch pad memory; Security; Virtualizations; Fault tolerance
Task scheduling: A control-theoretical viewpoint for a general and flexible solution,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930331574&doi=10.1145%2f2560015&partnerID=40&md5=4d0b10ca4e95692cc128484edca186ef,"This article presents a new approach to the design of task scheduling algorithms, where system-theoretical methodologies are used throughout. The proposal implies a significant perspective shift with respect to mainstream design practices, but yields large payoffs in terms of simplicity, flexibility, solution uniformity for different problems, and possibility to formally assess the results also in the presence of unpredictable run-time situations. A complete implementation example is illustrated, together with various comparative tests, and a methodological treatise of the matter. © 2014 ACM.",Control-based system design; Discrete-time dynamic systems; Feedback control; Formal assessment; Task scheduling,Algorithms; Feedback control; Multitasking; Scheduling; Scheduling algorithms; Comparative tests; Control-based systems; Design practice; Discrete-time dynamic system; Formal assessment; Run-time situation; Task-scheduling; Task-scheduling algorithms; Discrete time control systems
A low-power instruction replay mechanism for design of resilient microprocessors,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930326572&doi=10.1145%2f2560034&partnerID=40&md5=769a2b5857bfb910109ff2a1b3e8abde,"There is a growing concern about the increasing rate of defects in computing substrates. Traditional redundancy solutions prove to be too expensive for commodity microprocessor systems. Modern microprocessors feature multiple execution units to take advantage of instruction level parallelism. However, most workloads do not exhibit the level of instruction level parallelism that a typical microprocessor is resourced for. This offers an opportunity to reexecute instructions using idle execution units. But, relying solely on idle resources will not provide full instruction coverage and there is a need to explore other alternatives. To that end, we propose and evaluate two instruction replay schemes within the same core for online testing of the execution units. One scheme (RER) reexecutes only the retired instructions, while the other (REI) reexecutes all the issued instructions. The complete proposed solution requires a comparator and minor modifications to control logic, resulting in negligible hardware overhead. Both soft and hard error detection are considered and the performance and energy impact of both schemes are evaluated and compared against previously proposed redundant execution schemes. Results show that even though the proposed schemes result in a small performance penalty when compared to previous work, the energy overhead is significantly reduced. © 2014 ACM.",Dual use of superscalar datapath (DUAL); Energy and performance efficient test; Functional unit test; Online error detection; Out of Order Reliable Superscalar (O3RS); Out-of-order (OOO); Re-execute on Issue (REI); Re-execute on Retire (RER),Computation theory; Error detection; Integrated circuit design; Microprocessor chips; Social networking (online); Functional units; Instruction level parallelism; Microprocessor systems; Modern microprocessor; On-line error detection; Out of order; Performance penalties; Superscalar; Parallel processing systems
Providing reliable and real-time delivery in the presence of body shadowing in breadcrumb systems,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930365986&doi=10.1145%2f2557633&partnerID=40&md5=3baba76f912825e01b79f8f16791baf4,"The primary goal of breadcrumb trail sensor networks is to transmit in real-time users' physiological parameters that measure life-critical functions to an incident commander through reliablemultihop communication. In applications using breadcrumb solutions, there are often many users working together, and this creates a well-known body shadowing effect (BSE). In this article, we first measure the characteristics of body shadowing for 2.4GHz sensor nodes. Our empirical results show that the body shadowing effect leads to severe packet loss and consequently very poor real-time performance. Then we develop a novel Intentional Forwarding solution. This solution accurately detects the shadowing mode and enables selected neighbors to forward data packets. Experimental results from a fully implemented testbed demonstrate that Intentional Forwarding is able to improve the end-to-end average packet delivery ratio (PDR) from 58% to 93% and worst-case PDR from 45% to 85%, and is able to meet soft real-time requirements even under severe body shadowing problems. © 2014 ACM.",Body shadowing effects; Breadcrumb systems; Packet delivery ratio; Real-time performance,Physiological models; Sensor networks; Sensor nodes; Body shadowing; Body-shadowing effect; Critical functions; Incident commander; Packet delivery ratio; Physiological parameters; Real time performance; Soft real time; Real time systems
An approach to manage reconfigurations and reduce area cost in hard real-time reconfigurable systems,2014,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921061467&doi=10.1145%2f2560037&partnerID=40&md5=e6aa17403890527b1e6ff1876ddb9179,"This article presents a methodology for building real-time reconfigurable systems that ensures that all the temporal constraints of a set of applications are met while optimizing the utilization of the available reconfigurable resources. Starting from a static platform thatmeets all the real-time deadlines, our approach takes advantage of runtime reconfiguration in order to reduce the area needed while guaranteeing that all the deadlines are still met. This goal is achieved by identifying which tasks must be always ready for execution in order to meet the deadlines and by means of a methodology that also allows reducing the area requirements. © 2014 ACM.",Embedded systems design; FPGAs; Real-time task scheduling; Reconfigurable architectures,Embedded software; Embedded systems; Field programmable gate arrays (FPGA); Reconfigurable architectures; Reconfigurable hardware; Structural design; Area cost; Area requirement; Hard real-time; Real-time tasks; Reconfigurable resources; Reconfigurable systems; Run time reconfiguration; Temporal constraints; Real time systems
Article 33: A Novel Low-Power Embedded Object Recognition System Working at Multi-Frames per Second,2013,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025385622&doi=10.1145%2f2485984.2499550&partnerID=40&md5=5d72ed22e2b094f38f2da62d99ead599,[No abstract available],,
Call for Papers,2010,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024293090&doi=10.1145%2f1880050.1880065&partnerID=40&md5=9d3d90c91fcabc32be6f80addd5d387e,[No abstract available],,
Efficient Software Implementation of Embedded Communication Protocol Controllers Using Asynchronous Software Thread Integration with Time- and Space-Efficient Procedure Calls,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025405506&doi=10.1145%2f1210268.1210270&partnerID=40&md5=479420c18f5af4b59fcff5930650c705,"The overhead of context switching limits efficient scheduling of multiple concurrent threads on a uniprocessor when real-time requirements exist. A software-implemented protocol controller may be crippled by this problem. The available idle time may be too short to recover through context switching, so only the primary thread can execute during message activity, slowing the secondary threads and potentially missing deadlines. Asynchronous software thread integration (ASTI) uses coroutine calls and integration, letting threads make independent progress efficiently, and reducing the needed context switches. We demonstrate the methods with a software implementation of an automotive communication protocol (J1850) and several secondary threads. © 2007, ACM. All rights reserved.",Asynchronous software thread integration; Design; Experimentation; fine-grain concurrency; hardware to software migration; J1850; software-implemented communication protocol controllers,
An Efficient B-Tree Layer Implementation for Flash-Memory Storage Systems,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021210434&doi=10.1145%2f1275986.1275991&partnerID=40&md5=b988a94479303ef5bd191fbb025bc001,"With the significant growth of the markets for consumer electronics and various embedded systems, flash memory is now an economic solution for storage systems design. Because index structures require intensively fine-grained updates/modifications, block-oriented access over flash memory could introduce a significant number of redundant writes. This might not only severely degrade the overall performance, but also damage the reliability of flash memory. In this paper, we propose a very different approach, which can efficiently handle fine-grained updates/modifications caused by B-tree index access over flash memory. The implementation is done directly over the flash translation layer (FTL); hence, no modifications to existing application systems are needed. We demonstrate that when index structures are adopted over flash memory, the proposed methodology can significantly improve the system performance and, at the same time, reduce both the overhead of flash-memory management and the energy dissipation. The average response time of record insertions and deletions was also significantly reduced. © 2007, ACM. All rights reserved.",Algorithm; B-tree; database systems; Design; embedded systems; Flash memory; Performance; storage systems,
A Log Buffer-Based Flash Translation Layer Using Fully-Associative Sector Translation,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025155936&doi=10.1145%2f1275986.1275990&partnerID=40&md5=27c39f5ea910e86d16974286115e12f8,"Flash memory is being rapidly deployed as data storage for mobile devices such as PDAs, MP3 players, mobile phones, and digital cameras, mainly because of its low electronic power, nonvolatile storage, high performance, physical stability, and portability. One disadvantage of flash memory is that prewritten data cannot be dynamically overwritten. Before overwriting prewritten data, a time-consuming erase operation on the used blocks must precede, which significantly degrades the overall write performance of flash memory. In order to solve this “erase-before-write” problem, the flash memory controller can be integrated with a software module, called “flash translation layer (FTL).” Among many FTL schemes available, the log block buffer scheme is considered to be optimum. With this scheme, a small number of log blocks, a kind of write buffer, can improve the performance of write operations by reducing the number of erase operations. However, this scheme can suffer from low space utilization of log blocks. In this paper, we show that there is much room for performance improvement in the log buffer block scheme, and propose an enhanced log block buffer scheme, called FAST (full associative sector translation). Our FAST scheme improves the space utilization of log blocks using fully-associative sector translations for the log block sectors. We also show empirically that our FAST scheme outperforms the pure log block buffer scheme. © 2007, ACM. All rights reserved.",address translation; Algorithms; associative mapping; Flash memory; FTL; log blocks; Performance,
The XTREM Power and Performance Simulator for the Intel XScale Core: Design and Experiences,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015272442&doi=10.1145%2f1210268.1210272&partnerID=40&md5=5abceaa8358de42e3be85207c9e3687b,"Managing power concerns in microprocessors has become a pressing research problem across the domains of computer architecture, CAD, and compilers. As a result, several parameterized cycle-level power simulators have been introduced. While these simulators can be quite useful for microarchitectural studies, their generality limits how accurate they can be for any one chip family. Furthermore, their hardware focus means that they do not explicitly enable studying the interaction of different software layers, such as Java applications and their underlying runtime system software. This paper describes and evaluates XTREM, a power-simulation tool tailored for the Intel XScale microarchitecture. In building XTREM, our goals were to develop a microarchitecture simulator that, while still offering size parameterizations for cache and other structures, more accurately reflected a realistic processor pipeline. We present a detailed set of validations based on multimeter power measurements and hardware performance counter sampling. XTREM exhibits an average performance error of only 6.5% and an even smaller average power error: 4%. The paper goes on to present an application study enabled by the simulator. Namely, we use XTREM to produce an energy consumption breakdown for Java CDC and CLDC applications. Our simulator measurements indicate that a large percentage of the total energy consumption (up to 35%) is devoted to the virtual machine's support functions. © 2007, ACM. All rights reserved.",Experimentation; Intel XScale technology; Java; Languages; Measurements; Performance; power measurements; Power modeling,
Power Management in Energy Harvesting Sensor Networks,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016294846&doi=10.1145%2f1274858.1274870&partnerID=40&md5=867dedfb8dc18823fc86565ffd785339,"Power management is an important concern in sensor networks, because a tethered energy infrastructure is usually not available and an obvious concern is to use the available battery energy efficiently. However, in some of the sensor networking applications, an additional facility is available to ameliorate the energy problem: harvesting energy from the environment. Certain considerations in using an energy harvesting source are fundamentally different from that in using a battery, because, rather than a limit on the maximum energy, it has a limit on the maximum rate at which the energy can be used. Further, the harvested energy availability typically varies with time in a nondeterministic manner.While a deterministic metric, such as residual battery, suffices to characterize the energy availability in the case of batteries, a more sophisticated characterization may be required for a harvesting source. Another issue that becomes important in networked systems with multiple harvesting nodes is that different nodes may have different harvesting opportunity. In a distributed application, the same end-user performance may be achieved using different workload allocations, and resultant energy consumptions at multiple nodes. In this case, it is important to align the workload allocation with the energy availability at the harvesting nodes.We consider the above issues in power management for energy-harvesting sensor networks. We develop abstractions to characterize the complex time varying nature of such sources with analytically tractable models and use them to address key design issues. We also develop distributed methods to efficiently use harvested energy and test these both in simulation and experimentally on an energy-harvesting sensor network, prototyped for this work. © 2007, ACM. All rights reserved.",Adaptive Duty Cycling; Algorithms; Design; Energy Neutrality; Experimentation; Heliomote; Lifetime; Measurement; Performance; Power Management; Theory,
Guest Editorial,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025380187&doi=10.1145%2f1210268.1216577&partnerID=40&md5=3b94b5fc983cddc878c9eaf01b8a37fc,[No abstract available],,
Guest Editorial: Introduction to the Special Issue on Software and Compilers for Embedded Systems,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025379269&doi=10.1145%2f1234675.1234676&partnerID=40&md5=b832a4017464e01764177cb8fca13061,[No abstract available],,
Energy Efficient DVS Schedule for Fixed-Priority Real-Time Systems,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014846112&doi=10.1145%2f1274858.1274867&partnerID=40&md5=bef620c008ccf034fcbe45c33267a10a,"Energy consumption has become an increasingly important consideration in designing many realtime embedded systems. Variable voltage processors, if used properly, can dramatically reduce such system energy consumption. In this paper, we present a technique to determine voltage settings for a variable voltage processor that utilizes a fixed-priority assignment to schedule jobs. By exploiting more efficiently the processor slack time, our approach can be more effective in reducing the execution speed for real-time tasks when necessary. Our approach also produces the minimum constant voltage needed to feasibly schedule the entire job set. With both randomly generated and practical examples, our heuristic approach can achieve the dynamic energy reduction very close to the theoretically optimal one (within 2%) with much less computation cost. © 2007, ACM. All rights reserved.",Algorithms; Dynamic voltage scaling; fixed-priority scheduling; low power; Performance; real time,
Energy Optimal Speed Control of a Producer—Consumer Device Pair,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025397843&doi=10.1145%2f1274858.1274868&partnerID=40&md5=832fedec1376289e2506c85d1037f37b,"We propose a modular approach for minimizing the total energy consumed by a pair of generic communicating devices (producer–consumer scenario) by jointly controlling their speed profiles. Each device (like a CPU, or disk drive) is assumed to have a controllable variable called its speed (e.g., a CPU's clock frequency, a disk drive's spindle motor speed) that affects its power consumption and performance (e.g., throughput, data transfer rate). The device and task models we analyzed were inspired by applications like CD recording (hard drive to CD drive data transfer) and data processing (disk drive to CPU data transfer). The proposed solution can be used for any pair of devices with convex (for continuous speed sets) orW-convex (a discrete version of a convex function for discrete speed sets) power–speed relationships. For discrete speed sets, the method operates directly on the power–speed values and does not require an analytical relationship between power and speed. The key to solving the two-device optimization problem was the observation that it could be split into two single device parametric optimization problems, where the parameters correspond to the common task that both the devices must execute. The following divide-and-conquer approach is proposed: [divide] the optimal speed policy and energy consumption of each device is derived as an analytical function of its task parameters; [conquer] the optimal values of these parameters are found by minimizing the sum of the parameterized energy functions and plugged back into the parameterized speed profiles. The main advantage of this approach is that each device can be characterized independently and this allows system designers to mix and match manufacturer-supplied device energy curves to evaluate and optimize different application scenarios. We demonstrate our approach using three device characterization examples (for a CD drive, hard drive, and a CPU) and two application scenarios (CD recording, MD5 checksum computation). © 2007, ACM. All rights reserved.",disk drive; Energy optimization; Experimentation; joint optimization; Performance; processor; speed control; Theory,
Safety Verification of Hybrid Systems by Constraint Propagation-Based Abstraction Refinement,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016685426&doi=10.1145%2f1210268.1210276&partnerID=40&md5=852bced894bb16462fbb98e3df1ccf5c,"This paper deals with the problem of safety verification of nonlinear hybrid systems.We start from a classical method that uses interval arithmetic to check whether trajectories can move over the boundaries in a rectangular grid. We put this method into an abstraction refinement framework and improve it by developing an additional refinement step that employs interval-constraint propagation to add information to the abstraction without introducing new grid elements. Moreover, the resulting method allows switching conditions, initial states, and unsafe states to be described by complex constraints, instead of sets that correspond to grid elements. Nevertheless, the method can be easily implemented, since it is based on a well-defined set of constraints, on which one can run any constraint propagation-based solver. Tests of such an implementation are promising. © 2007, ACM. All rights reserved.",Algorithms; constraint propagation; Hybrid systems; intervals; Reliability; Verification,
Introduction to the Special LCTES'05 Issue,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024243014&doi=10.1145%2f1274858.1274859&partnerID=40&md5=a87f4b8aa5f829e001ffb0ea9c8e3abe,[No abstract available],,
Probabilistic Design of Multimedia Embedded Systems,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018195746&doi=10.1145%2f1275986.1275987&partnerID=40&md5=9025c5f322eb2131209880f97df34de0,"In this paper, we propose the novel concept of probabilistic design for multimedia embedded systems, which is motivated by the challenge of how to design, but not overdesign, such systems while systematically incorporating performance requirements of multimedia application, uncertainties in execution time, and tolerance for reasonable execution failures. Unlike most present techniques that are based on either worst- or average-case execution times of application tasks, where the former guarantees the completion of each execution, but often leads to overdesigned systems, and the latter fails to provide any completion guarantees, the proposed probabilistic design method takes advantage of unique features mentioned above of multimedia systems to relax the rigid hardware requirements for software implementation and avoid overdesigning the system. In essence, this relaxation expands the design space and we further develop an off-line online minimum effort algorithm for quick exploration of the enlarged design space at early design stages. This is the first step toward our goal of bridging the gap between real-time analysis and embedded software implementation for rapid and economic multimedia system design. It is our belief that the proposed method has great potential in reducing system resource while meeting performance requirements. The experimental results confirm this as we achieve significant saving in system's energy consumption to provide a statistical completion ratio guarantee (i.e., the expected number of completions over a large number of iterations is greater than a given value). © 2007, ACM. All rights reserved.",Algorithms; Design; Performance,
Accurate and Fast System-Level Power Modeling: An Xscale-Based Case Study,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952914775&doi=10.1145%2f1274858.1274864&partnerID=40&md5=0f85ff0785020422f1790761006b1402,"Accurate and fast system modeling is central to the rapid design space exploration needed for embedded-system design. With fast, complex SoCs playing a central role in such systems, system designers have come to require MIPS-range simulation speeds and near-cycle accuracy. The sophisticated simulation frameworks that have been developed for high-speed system performance modeling do not address power consumption, although it is a key design constraint. In this paper, we define a simulation-based methodology for extending system performance-modeling frameworks to also include power modeling. We demonstrate the use of this methodology with a case study of a real, complex embedded system, comprising the Intel XScale® embedded microprocessor, its WMMXTM SIMD coprocessor, L1 caches, SDRAM and the on-board address and data buses. We describe detailed power models for each of these components and validate them against physical measurements from hardware, demonstrating that such frameworks enable designers to model both power and performance at high speeds without sacrificing accuracy. Our results indicate that the power estimates obtained are accurate within 5% of physical measurements from hardware, while simulation speeds consistently exceed a million instructions per second (MIPS). © 2007, ACM. All rights reserved.",Design; embedded systems; Experimentation; Measurement; Performance; Power modeling; SystemC,
Improving Security for Periodic Tasks in Embedded Systems through Scheduling,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36348941936&doi=10.1145%2f1275986.1275992&partnerID=40&md5=67a1ab449cab7e883b73553830319486,"While many scheduling algorithms for periodic tasks ignore security requirements posed by sensitive applications and are, consequently, unable to perform properly in embedded systems with security constraints, in this paper, we present an approach to scheduling periodic tasks in embedded systems subject to security and timing constraints. We design a necessary and sufficient feasibility check for a set of periodic tasks with security requirements. With the feasibility test in place, we propose a scheduling algorithm, or SASES (security-aware scheduling for embedded systems), which accounts for both security and timing requirements. SASES judiciously distributes slack times among a variety of security services for a set of periodic tasks, thereby optimizing security for embedded systems without sacrificing schedulability. To demonstrate the effectiveness of SASES, we apply the proposed SASES to real-world embedded systems such as an automated flight control system. We show, through extensive simulations, that SASES is able to maximize security for embedded systems while guaranteeing timeliness. In particular, SASES significantly improves security over three baseline algorithms by up to 107%. © 2007, ACM. All rights reserved.",Algorithms; embedded systems; Performance; periodic tasks; Real-time systems; scheduling; security-sensitive applications,
Software Design Patterns for Tinyos,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-74549207607&doi=10.1145%2f1274858.1274860&partnerID=40&md5=1188c3ae478f0e1cc1cab929dca69569,"We present design patterns used by software components in the TinyOS sensor network operating system. They differ significantly from traditional software design patterns because of the constraints of sensor networks and to TinyOS's focus on static allocation and whole-program composition. We describe how nesC has evolved to support these design patterns by including a few simple language primitives and optimizations. © 2007, ACM. All rights reserved.",Design; Design Patterns; embedded systems; Languages; nesC; TinyOS,
Reducing Branch Predictor Leakage Energy by Exploiting Loops,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883892597&doi=10.1145%2f1234675.1234678&partnerID=40&md5=ccd56085e378bf0320998a90349be43c,"With the scaling of technology, leakage energy will become the dominant source of energy consumption. Besides cache memories, branch predictors are among the largest on-chip array structures and consume nontrivial leakage energy. This paper proposes two cost-effective loop-based strategies to reduce the branch predictor leakage without impacting prediction accuracy or performance. The loop-based approaches exploit the fact that loops usually only contain a small number of instructions and, hence, even fewer branch instructions while taking a significant fraction of the execution time. Consequently, all the nonactive entries of branch predictors can be placed into the low leakage mode during the loop execution in order to reduce leakage energy. Compiler and circuit supports are discussed to implement the proposed leakage-reduction strategies. Compared to the recently proposed decay-based approach, our experimental results show that the loop-based approach can extract 16.2% more dead time of the branch predictor, on average, leading to more leakage energy savings without impacting the branch prediction accuracy and performance. © 2007, ACM. All rights reserved.",Branch prediction; compiler; Design; Experimentation; leakage energy; Performance,
Automated Reduction of the Memory Footprint of the Linux Kernel,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921296627&doi=10.1145%2f1274858.1274861&partnerID=40&md5=e9ae96c05723c0871ebd5df1644b41a7,"The limited built-in configurability of Linux can lead to expensive code size overhead when it is used in the embedded market. To overcome this problem, we propose the application of link-time compaction and specialization techniques that exploit the a priori known, fixed runtime environment of many embedded systems. In experimental setups based on the ARM XScale and i386 platforms, the proposed techniques are able to reduce the kernel memory footprint with over 16%. We also show how relatively simple additions to existing binary rewriters can implement the proposed techniques for a complex, very unconventional program, such as the Linux kernel. We note that even after specialization, a lot of seemingly unnecessary code remains in the kernel and propose to reduce the footprint of this code by applying code-compression techniques. This technique, combined with the previous ones, reduces the memory footprint with over 23% for the i386 platform and 28% for the ARM platform. Finally, we pinpoint an important code size growth problem when compaction and compression techniques are combined on the ARM platform. © 2007, ACM. All rights reserved.",compaction; compression; Experimentation; Linux kernel; operating system; Performance; specialization; system calls,
Link-Time Compaction and Optimization of ARM Executables,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862975675&doi=10.1145%2f1210268.1210273&partnerID=40&md5=5409303b58cead71baadf20873586b28,"The overhead in terms of code size, power consumption, and execution time caused by the use of precompiled libraries and separate compilation is often unacceptable in the embedded world, where real-time constraints, battery life-time, and production costs are of critical importance. In this paper, we present our link-time optimizer for the ARM architecture. We discuss how we can deal with the peculiarities of the ARM architecture related to its visible program counter and how the introduced overhead can to a large extent be eliminated. Our link-time optimizer is evaluated with four tool chains, two proprietary ones from ARM and two open ones based on GNU GCC. When used with proprietary tool chains from ARM Ltd., our link-time optimizer achieved average code size reductions of 16.0 and 18.5%, while the programs have become 12.8 and 12.3% faster, and 10.7 to 10.1% more energy efficient. Finally, we show how the incorporation of link-time optimization in tool chains may influence library interface design. © 2007, ACM. All rights reserved.",compaction; Experimentation; linker; optimization; Performance; Performance,
Static Strands: Safely Exposing Dependence Chains for Increasing Embedded Power Efficiency,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80455126717&doi=10.1145%2f1274858.1274862&partnerID=40&md5=4a2091aa598d99f20b79db40d3a3eff7,"Modern embedded processors are designed to maximize execution efficiency—the amount of performance achieved per unit of energy dissipated while meeting minimum performance levels. To increase this efficiency, we propose utilizing static strands, dependence chains without fan-out, which are exposed by a compiler pass. These dependent instructions are resequenced to be sequential and annotated to communicate their location to the hardware. Importantly, this modified application is binary compatible and functionally identical to the original, allowing transparent execution on a baseline processor. However, these static strands can be easily collapsed and optimized by simple processor modifications, significantly reducing the workload energy. Results show that over 30% of MediaBench and Spec2000int dynamic instructions can be collapsed, reducing issue logic energy by 20%, bypass energy 19%, and register file energy 14%. In addition, by increasing the effective capactity of pipeline resources by almost a third, average IPC can be improved up to 15%. This performance gain can then be traded in for a lower clock frequency to maintain a basline level of performance, further reducing energy. © 2007, ACM. All rights reserved.",Architecture; dependency collapsing; Design; energy; Performance; sequentiality,
Power-Efficient Prefetching for Embedded Processors,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053415214&doi=10.1145%2f1210268.1210271&partnerID=40&md5=2e81a0baa24ef6e1a3c9f876453d68ae,"Because of stringent power constraints, aggressive latency-hiding approaches, such as prefetching, are absent in the state-of-the-art embedded processors. There are two main reasons that make prefetching power inefficient. First, compiler-inserted prefetch instructions increase code size and, therefore, could increase I-cache power. Second, inaccurate prefetching (especially for hardware prefetching) leads to high D-cache power consumption because of useless accesses. In this work, we show that it is possible to support power-efficient prefetching through bit-differential offset assignment. We target the prefetching of relocatable stack variables with a high degree of precision. By assigning the offsets of stack variables in such a way that most consecutive addresses differ by 1 bit, we can prefetch them with compact prefetch instructions to save I-cache power. The compiler first generates an access graph of consecutive memory references and then attempts a layout of the memory locations in the smallest hypercube. Each dimension of the hypercube represents a 1-bit differential addressing. The embedding is carried out in as compact a hypercube as possible in order to save memory space. Each load/store instruction carries a hint regarding prefetching the next memory reference by encoding its differential address with respect to the current one. To reduce D-cache power cost, we further attempt to assign offsets so that most of the consecutive accesses map to the same cache line. Our prefetching is done using a one entry line buffer [Wilson et al. 1996]. Consequently, many look-ups in D-cache reduce to incremental ones. This results in D-cache activity reduction and power savings. Our prefetcher requires both compiler and hardware support. In this paper, we provide implementation on the processor model close to ARM with small modification to the ISA.We tackle issues such as out-of-order commit, predication, and speculation through simple modifications to the processor pipeline on noncritical paths. Our goal in this work is to boost performance while maintaining/lowering power consumption. Our results show 12% speedup and slight power reduction. The runtime virtual space loss for stack and static data is about 11.8%. © 2007, ACM. All rights reserved.",Algorithms; bit-differential addressing; Data prefetching; Design; embedded processors; offset assignment; Performance,
Beyond Single-Appearance Schedules: Efficient DSP Software Synthesis Using Nested Procedure Calls,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878482747&doi=10.1145%2f1234675.1234681&partnerID=40&md5=ebead58f634ec9935f26dd701345ba6c,"Synthesis of digital signal-processing (DSP) software from dataflow-based formal models is an effective approach for tackling the complexity of modern DSP applications. In this paper, an efficient method is proposed for applying subroutine call instantiation of module functionality when synthesizing embedded software from a dataflow specification. The technique is based on a novel recursive decomposition of subgraphs in a cluster hierarchy that is optimized for low buffer size. Applying this technique, one can achieve significantly lower buffer sizes than what is available for minimum code size inlined schedules, which have been the emphasis of prior work on software synthesis. Furthermore, it is guaranteed that the number of procedure calls in the synthesized program is polynomially bounded in the size of the input dataflow graph, even though the number of module invocations may increase exponentially. This recursive decomposition approach provides an efficient means for integrating subroutine-based module instantiation into the design space of DSP software synthesis. The experimental results demonstrate a significant improvement in buffer cost, especially for more irregular multirate DSP applications, with moderate code and execution time overhead. © 2007, ACM. All rights reserved.",Algorithms; block diagram compiler; Design; design methodology; embedded systems; hierarchical graph decomposition; Languages; memory optimization; procedural implementation; Synchronous dataflow,
ASIP Architecture Exploration for Efficient IP Sec Encryption: A Case Study,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38949175611&doi=10.1145%2f1234675.1234679&partnerID=40&md5=016fcab2bf5bf66cbc7f19ee09b6dcc8,"Application-Specific Instruction-Set Processors (ASIPs) are becoming increasingly popular in the world of customized, application-driven System-on-Chip (SoC) designs. Efficient ASIP design requires an iterative architecture exploration loop—gradual refinement of the processor architecture starting from an initial template. To accomplish this task, design automation tools are used to detect bottlenecks in embedded applications, to implement application-specific processor instructions, and to automatically generate the required software tools (such as instruction-set simulator, C-compiler, assembler, and profiler), as well as to synthesize the hardware. This paper describes an architecture exploration loop for an ASIP coprocessor that implements common encryption functionality used in symmetric block cipher algorithms for internet protocol security (IPSec). The coprocessor is accessed via shared memory and, as a consequence, our approach is easily adaptable to arbitrary main processor architectures. This paper presents the extended version of our case study that has been already published on the SCOPES conference in 2004. In both papers, a MIPS architecture is used as the main processor and Blowfish as encryption algorithm. © 2007, ACM. All rights reserved.",ADL; ASIP; computer-aided design; Design; IPSec; Security,
Power Macromodeling of MPSoC Message Passing Primitives,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976220073&doi=10.1145%2f1274858.1274869&partnerID=40&md5=aeb6a65ede2710a2e7a73864ee7df3a1,"Estimating the energy consumption of software in multiprocessor systems-on-chip (MPSoCs) is crucial for enabling quick evaluations of both software and hardware optimizations. However, high-level estimations should be applicable at software level, possibly constructing effective power models depending on parameters that can be extracted directly from the application characteristics. We propose a methodology for accurate analysis of power consumption of message-passing primitives in a MPSoC, and, in particular, an energy model which, in spite of its simplicity, allows to model the traffic-dependent nature of energy consumption through the use of a single, abstract parameter, namely, the size of the message exchanged. © 2007, ACM. All rights reserved.",Communication primitives; macromodeling; Measurement Performance; multiprocessor; system-on-chip,
Robust Implicit EDF: A Wireless Mac Protocol for Collaborative Real-Time Systems,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924686247&doi=10.1145%2f1274858.1274866&partnerID=40&md5=b8d1635eb0bd30e8f5d08326e8a2d490,"Advances in wireless technology have brought us closer to extensive deployment of distributed realtime embedded systems connected through a wireless channel. The medium-access control (MAC) layer protocol is critical in providing a real-time guarantee. We have devised a real-time wireless MAC protocol, robust implicit earliest deadline first, or RI-EDF. Packets are transmitted according to EDF scheduling rules, offering a protocol that implicitly avoids contention. In the event of a packet loss or a node failure, every node has the opportunity to recover the schedule based on a static recovery priority, offering a protocol that is robust with no central point of failure. We demonstrate in simulations that RI-EDF provides better goodput and lower packet loss than existing protocols like 802.11 PCF and EDCF. In our implementation and distributed control test-bed, we show that RI-EDF provides better throughput than the TinyOS MAC-layer protocol. Overall, RI-EDF provides predictable temporal behavior with minimal impact on node failures, packet losses, and noise in the channel. © 2007, ACM. All rights reserved.",Earliest deadline first; medium-access control; real time; Reliability; wireless,
Classifying Interprocess Communication in Process Network Representation of Nested-Loop Programs,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857211140&doi=10.1145%2f1234675.1234680&partnerID=40&md5=29156110f27782a76b000fd5cba1331b,"New embedded signal-processing architectures are emerging that are composed of loosely coupled heterogeneous components like CPUs or DSPs, specialized IP cores, reconfigurable units, or memories. We believe that these architectures should be programmed using the process network model of computation. To ease the mapping of applications, we are developing the Compaan compiler that automatically derives a process network (PN) description from an application written in Matlab or C. In this paper, we investigate a particular problem in nested loop programs, which is about classifying the interprocess communication in the PN representation of the nested loop program. The global memory arrays present in the code have to be replaced by a distributed communication structure used for communicating data between the network processes. We show that four types of communication exist, each exhibiting different requirements when realizing them in hardware or software. We first present two compile time tests that are based on integer linear programming to decide the type of the communication. In the second part of this paper, we present alternative classification techniques that have polynomial complexity. However, in some cases, those techniques do not give a definitive answer and the ILP tests have to be applied. All present tests are combined in a hybrid classification scheme that correctly classifies the interprocess communication. In only 5% of the cases to classify, we have to rely on integer linear programming while, in the remaining 95%, the alternative techniques presented in this paper are able to correctly classify each case. The hybrid classification scheme has become an important part of our Compaan compiler. © 2007, ACM. All rights reserved.",Classification of Interprocess Communication; hybrid classification approach; integer linear programming; matrix manipulations; Nested Loop Programs; Process Networks; Static analysis,
Scalable Precision Cache Analysis for Real-Time Software,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76749165854&doi=10.1145%2f1274858.1274863&partnerID=40&md5=2716c96122ed34f5e3a2927618c0fd52,"Caches are needed to increase the processor performance, but the temporal behavior is difficult to predict, especially in embedded systems with preemptive scheduling. Current approaches use simplified assumptions or propose complex analysis algorithms to bound the cache-related preemption delay. In this paper, a scalable preemption delay analysis for associative instruction caches to control the analysis precision and the time-complexity is proposed. An accurate preemption delay calculation is integrated into a cache-aware schedulability analysis. The framework is evaluated in several experiments. © 2007, ACM. All rights reserved.",Algorithms; cache; embedded systems; Measurement; Performance; preemptive scheduling; Worst-case execution time analysis,
Selective Code Transformation for Dual Instruction Set Processors,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953991073&doi=10.1145%2f1234675.1234677&partnerID=40&md5=ee6b695136c2768462ebba2c62e00479,"Embedded systems are often constrained in terms of both code size and execution time, because of a limited amount of available memory and real-time nature of applications. A dual instruction set processor, which supports a reduced instruction set (16 bits/instruction), in addition to a full instruction set (32 bits/instruction), allows an opportunity for a tradeoff between these two design criteria. Specifically, while the reduced instruction set can be used to reduce code size by providing smaller instructions, a program compiled into the reduced instruction set typically runs slower than the same program compiled into the full instruction set. Motivated by this observation, we propose a code generation technique that exploits this tradeoff relationship by selectively using the two instruction sets for different sections in the program. The proposed technique, called selective code transformation, not only provides a mechanism to enable a flexible tradeoff between a program's code size and its execution time, but also facilitates program optimization toward enhancing its worst case performance. The results from our experiments show that our proposed technique can be effectively used to fine-tune an application program on a spectrum of code size and execution performance, which, in turn, enables a system-wide optimization on memory space and execution speed involving multiple applications. © 2007, ACM. All rights reserved.",Design; Dual instruction set processors; mixed-width instruction set architecture; Performance; reduced bid-width instruction set architecture,
Timing Analysis for Preemptive Multitasking Real-Time Systems with Caches,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349518439&doi=10.1145%2f1210268.1210275&partnerID=40&md5=b5d80c6670dbf72944d2fb9722f9a09f,"In this paper, we propose an approach to estimate the worst-case response time (WCRT) of each task in a preemptive multitasking single-processor real-time system utilizing an L1 cache. The approach combines intertask cache-eviction analysis and intratask cache-access analysis to estimate the number of cache lines that can possibly be evicted by the preempting task and also be accessed again by the preempted task after preemptions (thus requiring the preempted task to reload the cache line(s)). This cache-reload delay caused by preempting task(s) is then incorporated intoWCRT analysis. Three sets of applications with up to six concurrent tasks running are used to test our approach. The experimental results show that our approach can tighten the WCRT estimate by up to 32% (1.4X) over prior state-of-the-art. © 2007, ACM. All rights reserved.",Algorithms; Performance; Real-time; worst-case response time,
Techniques for Maintaining Connectivity in Wireless Ad-hoc Networks Under Energy Constraints,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856855039&doi=10.1145%2f1275986.1275988&partnerID=40&md5=5aebf0adba9b06203b460aa575d871e6,"Distributed wireless systems (DWSs) are emerging as the enabler for next-generation wireless applications. There is a consensus that DWS-based applications, such as pervasive computing, sensor networks, wireless information networks, and speech and data communication networks, will form the backbone of the next technological revolution. Simultaneously, with great economic, industrial, consumer, and scientific potential, DWSs pose numerous technical challenges. Among them, two are widely considered as crucial: autonomous localized operation and minimization of energy consumption. We address the fundamental problem of how to maximize the lifetime of the network using only local information, while preserving network connectivity. We start by introducing the care-free sleep (CS) Theorem that provides provably optimal conditions for a node to go into sleep mode while ensuring that global connectivity is not affected. The CS theorem is the basis for an efficient localized algorithm that decides which nodes will go to into sleep mode and for how long. We have also developed mechanisms for collecting neighborhood information and for the coordination of distributed energy minimization protocols. The effectiveness of the approach is demonstrated using a comprehensive study of the performance of the algorithm over a wide range of network parameters. Another important highlight is the first mathematical and Monte Carlo analysis that establishes the importance of considering nodes within a small number of hops in order to preserve energy. © 2007, ACM. All rights reserved.",ad-hoc networks; connectivity; energy management; Low power; Performance; power management; Reliability; sleeping coordination,
Hardware/Software IP Integration Using the ROSES Design Environment,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-63149183850&doi=10.1145%2f1275986.1275989&partnerID=40&md5=c5b4670ae7d4908073afb954a961ad57,"Considering current time-to-market pressures, IP reuse is mandatory for the design of complex embedded systems-on-chip (SoC). The integration of IP components into a given design is the most complex task in the whole reuse process. This paper describes the IP integration approach implemented in the ROSES design environment, which presents a unique combination of features that enhance IP reuse: automatic assembly of interfaces between heterogeneous software and hardware IP components; easy adaptation to different on-chip communication structures and bus and core standards; generation of customized and minimal OSs for programmable components; and an architecture-independent high-level API embedded into SystemC that makes application software independent from system implementation. Application code is written by using communication functions available in this API. ROSES automatically assembles wrappers that implement these functions, such that the application code does not need to be modified in order to run in the final synthesized system. © 2007, ACM. All rights reserved.",Design; IP integration; Systems-on-chip,
The Molen Compiler for Reconfigurable Processors,2007,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012229875&doi=10.1145%2f1210268.1210274&partnerID=40&md5=4104d3dd3aa65603226eae0a076731fd,"In this paper, we describe the compiler developed to target the Molen reconfigurable processor and programming paradigm. The compiler automatically generates optimized binary code for C applications, based on pragma annotation of the code executed on the reconfigurable hardware. For the IBM PowerPC 405 processor included in the Virtex II Pro platform FPGA, we implemented code generation, register, and stack frame allocation following the PowerPC EABI (embedded application binary interface). The PowerPC backend has been extended to generate the appropriate instructions for the reconfigurable hardware and data transfer, taking into account the information of the specific hardware implementations and system. Starting with an annotated C application, a complete design flow has been integrated to generate the executable bitstream for the reconfigurable processor. The flexible design of the proposed infrastructure allows to consider the special features of the reconfigurable architectures. In order to hide the reconfiguration latencies, we implemented an instructionscheduling algorithm for the dynamic hardware configuration instructions. The algorithm schedules, in advance, the hardware configuration instructions, taking into account the conflicts for the reconfigurable hardware resources (FPGA area) between the hardware operations. To verify the Molen compiler, we used the multimedia video frame M-JPEG encoder of which the extended discrete cosine transform (DCT*) function was mapped on the FPGA. We obtained an overall speedup of 2.5 (about 84% efficiency over the maximal theoretical speedup of 2.96). The performance efficiency is achieved using automatically generated nonoptimized DCT* hardware implementation. The instruction-scheduling algorithm has been tested for DCT, quantization, and VLC operations. Based on simulation results, we determine that, while a simple scheduling produces a significant performance decrease, our proposed scheduling contributes for up to 16x M-JPEG encoder speedup. © 2007, ACM. All rights reserved.",Algorithms; FPGA; Instruction scheduling; Languages; Performance; reconfigurable computing,
Automatic Rate Desynchronization of Embedded Reactive Programs,2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-49749153182&doi=10.1145%2f1165780.1165786&partnerID=40&md5=a31a3ea605527768f990954f702b3257,"Many embedded reactive programs perform computations at different rates, while still requiring the overall application to satisfy very tight temporal constraints. We propose a method to automatically distribute programs such that the obtained parts can be run at different rates, which we call rate desynchronization. We consider general programs whose control structure is a finite state automaton and with a DAG of actions in each state. The motivation is to take into account long-duration tasks inside the programs: these are tasks whose execution time is long compared to the other computations in the application, and whose maximal execution rate is known and bounded. Merely scheduling such a long duration task at a slow rate would not work since the whole program would be slowed down if compiled into sequential code. It would thus be impossible to meet the temporal constraints, unless such long duration tasks could be desynchronized from the remaining computations. This is precisely what our method achieves: it distributes the initial program into several parts, so that the parts performing the slow computations can be run at an appropriate rate, therefore not impairing the global reaction time of the program. We present in detail our method, all the involved algorithms, and a small running example. We also compare our method with the related work. © 2006, ACM. All rights reserved.",Algorithms; automatic distribution; Design; desynchronization; Embedded programs; long-duration tasks; parallelization algorithm; reactive systems,
"Energy-Efficient, Utility Accrual Scheduling under Resource Constraints for Mobile Embedded Systems",2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997552443&doi=10.1145%2f1165780.1165781&partnerID=40&md5=21e28f5e83fc3f746930d2fbedff713a,"We present an energy-efficient, utility accrual, real-time scheduling algorithm called ReUA. ReUA considers an application model where activities are subject to time/utility function time constraints, mutual exclusion constraints on shared non-CPU resources, and statistical performance requirements on individual activity timeliness behavior. The algorithm targets mobile embedded systems where system-level energy consumption is also a major concern. For such a model, we consider the scheduling objectives of (1) satisfying the statistical performance requirements and (2) maximizing the system-level energy efficiency, while respecting resource constraints. Since the problem is NP-hard, ReUA allocates CPU cycles using statistical properties of application cycle demands, and heuristically computes schedules with a polynomial time cost. We analytically establish several timeliness and nontimeliness properties of the algorithm. Further, our simulation experiments illustrate ReUA's effectiveness and superiority. © 2006, ACM. All rights reserved.",,
Parallelizing Load/Stores on Dual-Bank Memory Embedded Processors,2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866041541&doi=10.1145%2f1165780.1165784&partnerID=40&md5=eea8b59511687644e508d88e1cf65b55,"Many modern embedded processors such as DSPs support partitioned memory banks (also called X-Y memory or dual-bank memory) along with parallel load/store instructions to achieve higher code density and performance. In order to effectively utilize the parallel load/store instructions, the compiler must partition the memory-resident values and assign them to X or Y bank. This paper gives a postregister allocation solution to merge the generated load/store instructions into their parallel counterparts. Simultaneously, our framework performs allocation of values to X or Y memory banks. We first remove as many load/stores and register-register moves as possible through an excellent iterated coalescing based register allocator by Appel and George [1996]. We then attempt to parallelize the generated load/stores using a multipass approach. The basic phase of our approach attempts the merger of load/stores without duplication and web splitting. We model this problem as a graph-coloring problem in which each value is colored as either X or Y. We then construct a motion scheduling graph (MSG), based on the range of motion for each load/store instruction. MSG reflects potential instructions that could be merged. We propose a notion of pseudofixed boundaries so that the load/store movement is less affected by register dependencies. We prove that the coloring problem for MSG is NP-complete and solve it with two different heuristic algorithms with different complexity. We then propose a two-level iterative process to attempt instruction duplication, variable duplication, web splitting, and local conflict elimination to effectively merge the remaining load/stores. Finally, we clean up some multiple-aliased load/stores. To improve the performance, we combine profiling information with each stage coupled with some modifications to the algorithm. We show that our framework results in parallelization of a large number of load/stores without much growth in data and code segments. The average speedup for our optimization pass reaches roughly 13% if no profile information is available and 17% with profile information. The average code and data segment growth is controlled within 13%. © 2006, ACM. All rights reserved.",Design; DSP architectures; memory bank allocation; parallel load/stores; Performance; profile driven optimization,
A Split-Mask Countermeasure for Low-Energy Secure Embedded Systems,2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994544941&doi=10.1145%2f1165780.1165783&partnerID=40&md5=042821d59fc22834af56b06f2ba07f2f,"Future wireless embedded devices will be increasingly powerful, supporting many more applications, including one of the most crucial—security. Although many embedded devices offer more resistance to bus—probing attacks because of their compact size, susceptibility to power or electromagnetic analysis attacks must be analyzed. This paper presents a new split-mask countermeasure to thwart low-order differential power analysis (DPA) and differential EM analysis (DEMA). For the first time, real-power and EM measurements are used to analyze the difficulty of launching new third-order DPA and DEMA attacks on a popular low-energy 32-bit embedded ARM processor. Results show that the new split-mask countermeasure provides increased security without large overheads of energy dissipation, compared to previous research. With the emergence of security applications in PDAs, cell phones, and other embedded devices, low-energy countermeasures for resistance to low-order DPA/DEMA is crucial for supporting future enabled wireless internet. © 2006, ACM. All rights reserved.",Algorithms; countermeasures; Design; EM analysis; Experimentation; Measurement; power attacks; Security; Side channel analysis,
EnviroSuite: An Environmentally Immersive Programming Framework for Sensor Networks,2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009732121&doi=10.1145%2f1165780.1165782&partnerID=40&md5=73e8be4c24385c79368a169455e7bb48,"Sensor networks open a new frontier for embedded-distributed computing. Paradigms for sensor network programming-in-the-large have been identified as a significant challenge toward developing large-scale applications. Classical programming languages are too low-level. This paper presents the design, implementation, and evaluation of EnviroSuite, a programming framework that introduces a new paradigm, called environmentally immersive programming, to abstract distributed interactions with the environment. Environmentally immersive programming refers to an object-based programming model in which individual objects represent physical elements in the external environment. It allows the programmer to think directly in terms of environmental abstractions. EnviroSuite provides language primitives for environmentally immersive programming that map transparently into a support library of distributed algorithms for tracking and environmental monitoring. We show how nesC code of realistic applications is significantly simplified using EnviroSuite and demonstrate the resulting system performance on Mica2 and XSM platforms. © 2006, ACM. All rights reserved.",Abstractions; Design; embedded systems; Experimentation; Languages; middleware; Performance; programming models; sensor networks; tracking,
Reducing Power while Increasing Performance with SuperCISC,2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350602204&doi=10.1145%2f1165780.1165785&partnerID=40&md5=d8a5aa747f4f2a3de14ed40c2188cfa0,"Multiprocessor Systems on Chips (MPSoCs) have become a popular architectural technique to increase performance. However, MPSoCs may lead to undesirable power consumption characteristics for computing systems that have strict power budgets, such as PDAs, mobile phones, and notebook computers. This paper presents the super-complex instruction-set computing (SuperCISC) Embedded Processor Architecture and, in particular, investigates performance and power consumption of this device compared to traditional processor architecture-based execution. SuperCISC is a heterogeneous, multicore processor architecture designed to exceed performance of traditional embedded processors while maintaining a reduced power budget compared to low-power embedded processors. At the heart of the SuperCISC processor is a multicore VLIW (Very Large Instruction Word) containing several homogeneous execution cores/functional units. In addition, complex and heterogeneous combinational hardware function cores are tightly integrated to the core VLIW engine providing an opportunity for improved performance and reduced energy consumption. Our SuperCISC processor core has been synthesized for both a 90-nm Stratix II Field Programmable Gate Aray (FPGA) and a 160-nm standard cell Application-Specific Integrated Circuit (ASIC) fabrication process from OKI, each operating at approximately 167 MHz for the VLIW core. We examine several reasons for speedup and power improvement through the SuperCISC architecture, including predicated control flow, cycle compression, and a reduction in arithmetic power consumption, which we call power compression. Finally, testing our SuperCISC processor with multimedia and signal-processing benchmarks, we show how the SuperCISC processor can provide performance improvements ranging from 7X to 160X with an average of 60X, while also providing orders of magnitude of power improvements for the computational kernels. The power improvements for our benchmark kernels range from just over 40X to over 400X, with an average savings exceeding 130X. By combining these power and performance improvements, our total energy improvements all exceed 1000X. As these savings are limited to the computational kernels of the applications, which often consume approximately 90% of the execution time, we expect our savings to approach the ideal application improvement of 10X. © 2006, ACM. All rights reserved.",Design; Low-power; multicore architectures; Performance; predication; synthesis; VLIW,
Design Space Exploration Using Arithmetic-Level Hardware-Software Cosimulation for Configurable Multiprocessor Platforms,2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-51549094298&doi=10.1145%2f1151074.1151080&partnerID=40&md5=2d1331f451efa1adca4196befbf04446,"Configurable multiprocessor platforms consist of multiple soft processors configured on FPGA devices. They have become an attractive choice for implementing many computing applications. In addition to the various ways of distributing software execution among the multiple soft processors, the application designer can customize soft processors and the connections between them in order to improve the performance of the applications running on the multiprocessor platform. State-of-the-art design tools rely on low-level simulation to explore the various design trade-offs offered by configurable multiprocessor platforms. These low-level simulation based exploration techniques are too time-consuming and can be a major bottleneck to efficient design space exploration on these platforms. We propose a design space exploration technique for configurable multiprocessor platforms using arithmetic-level cycle-accurate hardware-software cosimulation. Arithmetic-level abstractions of the hardware and software execution platforms are created within the proposed cosimulation environment. The configurable multiprocessor platforms are described using these arithmetic-level abstractions. Hardware and software simulators are tightly integrated to concurrently simulate the arithmetic behavior of the multiprocessor platform. The simulation within the integrated simulators are synchronized to provide cycle-accurate simulation results for the complete multiprocessor platform. By doing so, we significantly speed up the cosimulation process for configurable multiprocessor platforms. Exploration of the various hardware-software design trade-offs provided by configurable multiprocessor platforms can be performed within the proposed cycle-accurate cosimulation environment. After the final designs are identified, the corresponding low-level implementations with the desired cycle-accurate arithmetic behavior are generated automatically. For illustrative purposes, we provide an implementation of our approach based on MATLAB/Simulink. We show the cosimulation of two numerical computation applications and one image-processing application on a popular configurable multiprocessor platform within the MATLAB/Simulink-based cosimulation environment. For these three applications, our arithmeticlevel cosimulation approach leads to speed-ups in simulation time of up tomore than 800x compared with the low-level simulation approaches. The designs of these applications identified using our arithmetic-level cosimulation approach achieve execution time speed-ups up to 5.6x, compared with other designs considered in our experiments. © Configurable multiprocessor platforms consist of multiple soft processors configured on FPGA devices. They have become an attractive choice for implementing many computing applications. In addition to the various ways of distributing software execution among the multiple soft processors, the application designer can customize soft processors and the connections between them in order to improve the performance of the applications running on the multiprocessor platform. State-of-the-art design tools rely on low-level simulation to explore the various design trade-offs offered by configurable multiprocessor platforms. These low-level simulation based exploration techniques are too time-consuming and can be a major bottleneck to efficient design space exploration on these platforms. We propose a design space exploration technique for configurable multiprocessor platforms using arithmetic-level cycle-accurate hardware-software cosimulation. Arithmetic-level abstractions of the hardware and software execution platforms are created within the proposed cosimulation environment. The configurable multiprocessor platforms are described using these arithmetic-level abstractions. Hardware and software simulators are tightly integrated to concurrently simulate the arithmetic behavior of the multiprocessor platform. The simulation within the integrated simulators are synchronized to provide cycle-accurate simulation results for the complete multiprocessor platform. By doing so, we significantly speed up the cosimulation process for configurable multiprocessor platforms. Exploration of the various hardware-software design trade-offs provided by configurable multiprocessor platforms can be performed within the proposed cycle-accurate cosimulation environment. After the final designs are identified, the corresponding low-level implementations with the desired cycle-accurate arithmetic behavior are generated automatically. For illustrative purposes, we provide an implementation of our approach based on MATLAB/Simulink. We show the cosimulation of two numerical computation applications and one image-processing application on a popular configurable multiprocessor platform within the MATLAB/Simulink-based cosimulation environment. For these three applications, our arithmeticlevel cosimulation approach leads to speed-ups in simulation time of up tomore than 800x compared with the low-level simulation approaches. The designs of these applications identified using our arithmetic-level cosimulation approach achieve execution time speed-ups up to 5.6x, compared with other designs considered in our experiments. © 2006, ACM. All rights reserved.",cosimulation; Design; design space exploration; FPGA; Performance; processor,
UML-Based Multiprocessor SoC Design Framework,2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013838708&doi=10.1145%2f1151074.1151077&partnerID=40&md5=800d180d87b8356cf2c12033a0c9afed,"This paper describes a complete design flow for multiprocessor systems-on-chips (SoCs) covering the design phases from system-level modeling to FPGA prototyping. The design of complex heterogeneous systems is enabled by raising the abstraction level and providing several system-level design automation tools. The system is modeled in a UML design environment following a new UML profile that specifies the practices for orthogonal application and architecture modeling. The design flow tools are governed in a single framework that combines the subtools into a seamless flow and visualizes the design process. Novel features also include an automated architecture exploration based on the system models in UML, as well as the automatic back and forward annotation of information in the design flow. The architecture exploration is based on the global optimization of systems that are composed of subsystems, which are then locally optimized for their particular purposes. As a result, the design flow produces an optimized component allocation, task mapping, and scheduling for the described application. In addition, it implements the entire system for FPGA prototyping board. As a case study, the design flow is utilized in the integration of state-of-the-art technology approaches, including a wireless terminal architecture, a network-on-chip, and multiprocessing utilizing RTOS in a SoC. In this study, a central part of a WLAN terminal is modeled, verified, optimized, and prototyped with the presented framework. © This paper describes a complete design flow for multiprocessor systems-on-chips (SoCs) covering the design phases from system-level modeling to FPGA prototyping. The design of complex heterogeneous systems is enabled by raising the abstraction level and providing several system-level design automation tools. The system is modeled in a UML design environment following a new UML profile that specifies the practices for orthogonal application and architecture modeling. The design flow tools are governed in a single framework that combines the subtools into a seamless flow and visualizes the design process. Novel features also include an automated architecture exploration based on the system models in UML, as well as the automatic back and forward annotation of information in the design flow. The architecture exploration is based on the global optimization of systems that are composed of subsystems, which are then locally optimized for their particular purposes. As a result, the design flow produces an optimized component allocation, task mapping, and scheduling for the described application. In addition, it implements the entire system for FPGA prototyping board. As a case study, the design flow is utilized in the integration of state-of-the-art technology approaches, including a wireless terminal architecture, a network-on-chip, and multiprocessing utilizing RTOS in a SoC. In this study, a central part of a WLAN terminal is modeled, verified, optimized, and prototyped with the presented framework. © 2006, ACM. All rights reserved.",architecture exploration; Design; design flow; Performance; UML 2.0; Verification,
Guest Editorial,2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958461304&doi=10.1145%2f1151074.1151075&partnerID=40&md5=778b3c0622cca61cf0ffad31122ed4e4,[No abstract available],,
Collaborative Operating System and Compiler Power Management for Real-Time Applications,2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003870832&doi=10.1145%2f1132357.1132361&partnerID=40&md5=c83d851cdf45d5021a3f93cff45be61b,"Managing energy consumption has become vitally important to battery-operated portable and embedded systems. Dynamic voltage scaling (DVS) reduces the processor's dynamic power consumption quadratically at the expense of linearly decreasing the performance. When reducing energy with DVS for real-time systems, one must consider the performance penalty to ensure that deadlines can be met. In this paper, we introduce a novel collaborative approach between the compiler and the operating system (OS) to reduce energy consumption. We use the compiler to annotate an application's source code with path-dependent information called power-management hints (PMHs). This fine-grained information captures the temporal behavior of the application, which varies by executing different paths. During program execution, the OS periodically changes the processor's frequency and voltage based on the temporal information provided by the PMHs. These speed adaptation points are called power-management points (PMPs). We evaluate our scheme using three embedded applications: a video decoder, automatic target recognition, and a sub-band tuner. Our scheme shows an energy reduction of up to 57% over no power-management and up to 32% over a static power-management scheme.We compare our scheme to other schemes that solely utilize PMPs for power-management and show experimentally that our scheme achieves more energy savings. We also analyze the advantages and disadvantages of our approach relative to another compiler-directed scheme. © Managing energy consumption has become vitally important to battery-operated portable and embedded systems. Dynamic voltage scaling (DVS) reduces the processor's dynamic power consumption quadratically at the expense of linearly decreasing the performance. When reducing energy with DVS for real-time systems, one must consider the performance penalty to ensure that deadlines can be met. In this paper, we introduce a novel collaborative approach between the compiler and the operating system (OS) to reduce energy consumption. We use the compiler to annotate an application's source code with path-dependent information called power-management hints (PMHs). This fine-grained information captures the temporal behavior of the application, which varies by executing different paths. During program execution, the OS periodically changes the processor's frequency and voltage based on the temporal information provided by the PMHs. These speed adaptation points are called power-management points (PMPs). We evaluate our scheme using three embedded applications: a video decoder, automatic target recognition, and a sub-band tuner. Our scheme shows an energy reduction of up to 57% over no power-management and up to 32% over a static power-management scheme. We compare our scheme to other schemes that solely utilize PMPs for power-management and show experimentally that our scheme achieves more energy savings. We also analyze the advantages and disadvantages of our approach relative to another compiler-directed scheme. © 2006, ACM. All rights reserved.",Algorithms; collaborative OS and compiler; dynamic voltage scaling; Experimentation; Management; power-management; Real-time; voltage scaling points placement,
A Design Methodology for Application-Specific Networks-on-Chip,2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015586212&doi=10.1145%2f1151074.1151076&partnerID=40&md5=c8024ac23c0291be9c6367521f4f060e,"With the help of HW/SW codesign, system-on-chip (SoC) can effectively reduce cost, improve reliability, and produce versatile products. The growing complexity of SoC designs makes on-chip communication subsystem design as important as computation subsystem design. While a number of codesign methodologies have been proposed for on-chip computation subsystems, many works are needed for on-chip communication subsystems. This paper proposes application-specific networkson-chip (ASNoC) and its design methodology. ASNoC is used for two high-performance SoC applications. The methodology (1) can automatically generate optimized ASNoC for different applications, (2) can generate a corresponding distributed shared memory along with an ASNoC, (3) can use both recorded and statistical communication traces for cycle-accurate performance analysis, (4) is based on standardized network component library and floorplan to estimate power and area, (5) adapts an industrial-grade network modeling and simulation environment, OPNET, which makes the methodology ready to use, and (6) can be easily integrated into current HW/SW codesign flow. Using the methodology, ASNoC is generated for a H.264HDTVdecoder SoC and Smart Camera SoC. ASNoC and 2D mesh networks-on-chip are compared in performance, power, and area in detail. The comparison results show that ASNoC provide substantial improvements in power, performance, and cost compared to 2D mesh networks-on-chip. In the H.264 HDTV decoder SoC, ASNoC uses 39% less power, 59% less silicon area, 74% less metal area, 63% less switch capacity, and 69% less interconnection capacity to achieve 2X performance compared to 2D mesh networks-on-chip. © With the help of HW/SW codesign, system-on-chip (SoC) can effectively reduce cost, improve reliability, and produce versatile products. The growing complexity of SoC designs makes on-chip communication subsystem design as important as computation subsystem design. While a number of codesign methodologies have been proposed for on-chip computation subsystems, many works are needed for on-chip communication subsystems. This paper proposes application-specific networkson-chip (ASNoC) and its design methodology. ASNoC is used for two high-performance SoC applications. The methodology (1) can automatically generate optimized ASNoC for different applications, (2) can generate a corresponding distributed shared memory along with an ASNoC, (3) can use both recorded and statistical communication traces for cycle-accurate performance analysis, (4) is based on standardized network component library and floorplan to estimate power and area, (5) adapts an industrial-grade network modeling and simulation environment, OPNET, which makes the methodology ready to use, and (6) can be easily integrated into current HW/SW codesign flow. Using the methodology, ASNoC is generated for a H.264HDTVdecoder SoC and Smart Camera SoC. ASNoC and 2D mesh networks-on-chip are compared in performance, power, and area in detail. The comparison results show that ASNoC provide substantial improvements in power, performance, and cost compared to 2D mesh networks-on-chip. In the H.264 HDTV decoder SoC, ASNoC uses 39% less power, 59% less silicon area, 74% less metal area, 63% less switch capacity, and 69% less interconnection capacity to achieve 2X performance compared to 2D mesh networks-on-chip. © 2006, ACM. All rights reserved.",Algorithms; Application-specific; architecture; Design; methodology; networks-on-chip; Performance; regular topology; Theory,
Energy-Efficient Embedded Software Implementation on Multiprocessor System-on-Chip with Multiple Voltages,2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41249103553&doi=10.1145%2f1151074.1151078&partnerID=40&md5=8cc657c0ec180228cb5c1e63323a0f70,"This paper develops energy-driven completion ratio guaranteed scheduling techniques for the implementation of embedded software on multiprocessor systems with multiple supply voltages. We leverage application's performance requirements, uncertainties in execution time, and tolerance for reasonable execution failures to scale each processor's supply voltage at run-time to reduce the multiprocessor system's total energy consumption. Specifically, we study how to trade the difference between the system's highest achievable completion ratio Qmaxand the required completion ratio Q0for energy saving. First, we propose a best-effort energy minimization algorithm (BEEM1) that achieves Qmaxwith the provably minimum energy consumption. We then relax its unrealistic assumption on the application's real execution time and develop algorithm BEEM2 that only requires the application's best- and worst-case execution times. Finally, we propose a hybrid offline on-line completion ratio guaranteed energy minimization algorithm (QGEM) that provides the required Q0with further energy reduction based on the probabilistic distribution of the application's execution time. We implement the proposed algorithms and verify their energy efficiency on real-life DSP applications and the TGFF random benchmark suite. BEEM1, BEEM2, and QGEM all provide the required completion ratio with average energy reduction of 28.7, 26.4, and 35.8%, respectively. © This paper develops energy-driven completion ratio guaranteed scheduling techniques for the implementation of embedded software on multiprocessor systems with multiple supply voltages. We leverage application's performance requirements, uncertainties in execution time, and tolerance for reasonable execution failures to scale each processor's supply voltage at run-time to reduce the multiprocessor system's total energy consumption. Specifically, we study how to trade the difference between the system's highest achievable completion ratio Qmaxand the required completion ratio Q0for energy saving. First, we propose a best-effort energy minimization algorithm (BEEM1) that achieves Qmaxwith the provably minimum energy consumption. We then relax its unrealistic assumption on the application's real execution time and develop algorithm BEEM2 that only requires the application's best- and worst-case execution times. Finally, we propose a hybrid offline on-line completion ratio guaranteed energy minimization algorithm (QGEM) that provides the required Q0with further energy reduction based on the probabilistic distribution of the application's execution time. We implement the proposed algorithms and verify their energy efficiency on real-life DSP applications and the TGFF random benchmark suite. BEEM1, BEEM2, and QGEM all provide the required completion ratio with average energy reduction of 28.7, 26.4, and 35.8%, respectively. © 2006, ACM. All rights reserved.",Algorithms; completion ratio; Design; Energy minimization; hardware/software co-design; multiple voltage; multiprocessor; Performance,
A Retargetable Framework for Instruction-Set Architecture Simulation,2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015320005&doi=10.1145%2f1151074.1151083&partnerID=40&md5=e35d40ceb0113c4be074bd5d6c168d6a,"Instruction-set architecture (ISA) simulators are an integral part of today's processor and software design process. While increasing complexity of the architectures demands high-performance simulation, the increasing variety of available architectures makes retargetability a critical feature of an instruction-set simulator. Retargetability requires generic models while high-performance demands target specific customizations. To address these contradictory requirements, we have developed a generic instruction model and a generic decode algorithm that facilitates easy and efficient retargetability of the ISA-simulator for a wide range of processor architectures, such as RISC, CISC, VLIW, and variable length instruction-set processors. The instruction model is used to generate compact and easy to debug instruction descriptions that are very similar to that of architecture manual. These descriptions are used to generate high-performance simulators. Our retargetable framework combines the flexibility of interpretive simulation with the speed of compiled simulation. The generation of the simulator is completely separate from the simulation engine. Hence, we can incorporate any fast simulation technique in our retargetable framework without introducing any performance penalty. To demonstrate this, we have incorporated fast IS-CS simulation engine in our retargetable framework which has generated 70% performance improvement over the best known simulators in this category. We illustrate the retargetability of our approach using two popular, yet different, realistic architectures: the SPARC and the ARM. © Instruction-set architecture (ISA) simulators are an integral part of today's processor and software design process. While increasing complexity of the architectures demands high-performance simulation, the increasing variety of available architectures makes retargetability a critical feature of an instruction-set simulator. Retargetability requires generic models while high-performance demands target specific customizations. To address these contradictory requirements, we have developed a generic instruction model and a generic decode algorithm that facilitates easy and efficient retargetability of the ISA-simulator for a wide range of processor architectures, such as RISC, CISC, VLIW, and variable length instruction-set processors. The instruction model is used to generate compact and easy to debug instruction descriptions that are very similar to that of architecture manual. These descriptions are used to generate high-performance simulators. Our retargetable framework combines the flexibility of interpretive simulation with the speed of compiled simulation. The generation of the simulator is completely separate from the simulation engine. Hence, we can incorporate any fast simulation technique in our retargetable framework without introducing any performance penalty. To demonstrate this, we have incorporated fast IS-CS simulation engine in our retargetable framework which has generated 70% performance improvement over the best known simulators in this category. We illustrate the retargetability of our approach using two popular, yet different, realistic architectures: the SPARC and the ARM. © 2006, ACM. All rights reserved.",architecture description language; decode algorithm; Design; generic instruction model; instruction binary encoding; Language; Performance; Retargetable instruction-set simulation,
FAST: Frequency-Aware Static Timing Analysis,2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024290387&doi=10.1145%2f1132357.1132364&partnerID=40&md5=235ce00fd4755f882bb57a3edaeb57a4,"Energy is a valuable resource in embedded systems as the lifetime of many such systems is constrained by their battery capacity. Recent advances in processor design have added support for dynamic frequency/voltage scaling (DVS) for saving energy. Recent work on real-time scheduling focuses on saving energy in static as well as dynamic scheduling environments by exploiting idle time and slack because of early task completion forDVS of subsequent tasks. These scheduling algorithms rely on a priori knowledge of worst-case execution times (WCET) for each task. They assume that DVS has no effect on the worst-case execution cycles (WCEC) of a task and scale the WCET according to the processor frequency. However, for systems with memory hierarchies, the WCEC typically does change under DVS because of requency modulation. Hence, current assumptions used by DVS schemes result in a highly exaggerated WCET. This paper contributes novel techniques for tight and flexible static timing analysis, particularly well-suited for dynamic scheduling schemes. The technical contributions are as follows: (1) We assess the problem of changing execution cycles owing to scaling techniques. (2) We propose a parametric approach toward bounding the WCET statically with respect to the frequency. Using a parametric model, we can capture the effect of changes in frequency on the WCEC and, thus, accurately model the WCET over any frequency range. (3) We discuss design and implementation of the frequency-aware static timing analysis (FAST) tool based on our prior experience with static timing analysis. (4)We demonstrate in experiments that our FAST tool provides safe upper bounds on the WCET, which are tight. The FAST tool allows us to capture the WCET of six benchmarks using equations that overestimate the WCET by less than 1%. FAST equations can also be used to improve existing DVS scheduling schemes to ensure that the effect of frequency scaling on WCET is considered and that the WCET used is not exaggerated. (5) We leverage three DVS scheduling schemes by incorporating FAST into them and by showing that the energy consumption further decreases. (6) We compare experimental results using two different energy models to demonstrate or verify the validity of simulation methods. To the best of our knowledge, this study of DVS effects on timing analysis is unprecedented. © Energy is a valuable resource in embedded systems as the lifetime of many such systems is constrained by their battery capacity. Recent advances in processor design have added support for dynamic frequency/voltage scaling (DVS) for saving energy. Recent work on real-time scheduling focuses on saving energy in static as well as dynamic scheduling environments by exploiting idle time and slack because of early task completion forDVS of subsequent tasks. These scheduling algorithms rely on a priori knowledge of worst-case execution times (WCET) for each task. They assume that DVS has no effect on the worst-case execution cycles (WCEC) of a task and scale the WCET according to the processor frequency. However, for systems with memory hierarchies, the WCEC typically does change under DVS because of requency modulation. Hence, current assumptions used by DVS schemes result in a highly exaggerated WCET. This paper contributes novel techniques for tight and flexible static timing analysis, particularly well-suited for dynamic scheduling schemes. The technical contributions are as follows: (1) We assess the problem of changing execution cycles owing to scaling techniques. (2) We propose a parametric approach toward bounding the WCET statically with respect to the frequency. Using a parametric model, we can capture the effect of changes in frequency on the WCEC and, thus, accurately model the WCET over any frequency range. (3) We discuss design and implementation of the frequency-aware static timing analysis (FAST) tool based on our prior experience with static timing analysis. (4)We demonstrate in experiments that our FAST tool provides safe upper bounds on the WCET, which are tight. The FAST tool allows us to capture the WCET of six benchmarks using equations that overestimate the WCET by less than 1%. FAST equations can also be used to improve existing DVS scheduling schemes to ensure that the effect of frequency scaling on WCET is considered and that the WCET used is not exaggerated. (5) We leverage three DVS scheduling schemes by incorporating FAST into them and by showing that the energy consumption further decreases. (6) We compare experimental results using two different energy models to demonstrate or verify the validity of simulation methods. To the best of our knowledge, this study of DVS effects on timing analysis is unprecedented. © 2006, ACM. All rights reserved.",Algorithms; dynamic voltage scaling; Experimentation; Real-time systems; scheduling; worst-case execution time analysis,
A New Efficient EDA Tool Design Methodology,2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016670587&doi=10.1145%2f1151074.1151082&partnerID=40&md5=1814994bdb1d19692b17decff03bc078,"New sophisticated EDA tools and methodologies will be needed to make products viable in the future marketplace by simplifying the various design stages. These tools will permit system design at a high abstraction level and enable automatic refinement through several abstraction levels to obtain a final prototype. They will have to be based on representations that are clean, complete, and easy to manipulate. In order to develop these new EDA tools, key features such as standardization, metadata programming, reflectivity, and introspection are needed. This work proposes a. Net Framework-based methodology, which possesses all these required key features. This methodology simplifies specification, synthesis, and validation of systems and enables the efficient creation/customization of EDA tools at low cost and development time. We show the effectiveness of this methodology by presenting its application for the design of a new EDA tool called ESys.Net (Embedded System design with.Net). We emphasize the specification and simulation aspects of this tool. © New sophisticated EDA tools and methodologies will be needed to make products viable in the future marketplace by simplifying the various design stages. These tools will permit system design at a high abstraction level and enable automatic refinement through several abstraction levels to obtain a final prototype. They will have to be based on representations that are clean, complete, and easy to manipulate. In order to develop these new EDA tools, key features such as standardization, metadata programming, reflectivity, and introspection are needed. This work proposes a.Net Framework-based methodology, which possesses all these required key features. This methodology simplifies specification, synthesis, and validation of systems and enables the efficient creation/customization of EDA tools at low cost and development time. We show the effectiveness of this methodology by presenting its application for the design of a new EDA tool called ESys.Net (Embedded System design with.Net). We emphasize the specification and simulation aspects of this tool. © 2006, ACM. All rights reserved.",.Net Framework; Algorithms; attribute programming; C#; CoDesign; Design; Documentation; embedded systems; ESys.Net; Experimentation; Languages; modeling and simulation; Performance; Reliability; SoC; Standardization; SystemC; Verification; VHDL,
A Formal Method for Hardware IP Design and Integration under I/O and Timing Constraints,2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024285994&doi=10.1145%2f1132357.1132359&partnerID=40&md5=c78555c68e08dce63976c7537c652c98,"IP integration, which is one of the most important SoC design steps, requires taking into account communication and timing constraints. In that context, design and reuse can be improved using IP cores described at a high abstraction level. In this paper, we present an IP design approach that relies on three main phases: (1) constraint modeling, (2) IP constraint analysis steps for feasibility checking, and (3) synthesis.We propose a set of techniques dedicated to the digital signal processing domain that lead to an optimized IP core integration. Based on a generic architecture of components, the method we propose provides automatic generation of IP cores designed under integration constraints. We show the effectiveness of our approach with a DCT core design case study. © IP integration, which is one of the most important SoC design steps, requires taking into account communication and timing constraints. In that context, design and reuse can be improved using IP cores described at a high abstraction level. In this paper, we present an IP design approach that relies on three main phases: (1) constraint modeling, (2) IP constraint analysis steps for feasibility checking, and (3) synthesis.We propose a set of techniques dedicated to the digital signal processing domain that lead to an optimized IP core integration. Based on a generic architecture of components, the method we propose provides automatic generation of IP cores designed under integration constraints. We show the effectiveness of our approach with a DCT core design case study. © 2006, ACM. All rights reserved.",communication interface unit; constrained synthesis; Design; design and integration; digital signal processing and multimedia applications; Performance; SoC,
Scheduling Refinement in Abstract RTOS Models,2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48649086921&doi=10.1145%2f1151074.1151079&partnerID=40&md5=943413d2879aac229e098e6f82342db0,"Scheduling decision for real-time embedded software applications has a great impact on system performance and, therefore, is an important issue in RTOS design. Moreover, it is highly desirable to have the system designer able to evaluate and select the right scheduling policy at high abstraction levels, in order to allow faster exploration of the design space. In this paper, we address this problem by introducing an abstract RTOS model, as well as a new approach to refine an unscheduled high-level model to a high-level model with RTOS scheduling. This approach is based on SystemC language and enables the system designer to quickly evaluate different dynamic scheduling policies and make the optimal choice in early design stages. Furthermore, we present a case of study where our model is used to simulate and analyze a telecom system. © Scheduling decision for real-time embedded software applications has a great impact on system performance and, therefore, is an important issue in RTOS design. Moreover, it is highly desirable to have the system designer able to evaluate and select the right scheduling policy at high abstraction levels, in order to allow faster exploration of the design space. In this paper, we address this problem by introducing an abstract RTOS model, as well as a new approach to refine an unscheduled high-level model to a high-level model with RTOS scheduling. This approach is based on SystemC language and enables the system designer to quickly evaluate different dynamic scheduling policies and make the optimal choice in early design stages. Furthermore, we present a case of study where our model is used to simulate and analyze a telecom system. © 2006, ACM. All rights reserved.",Design; Real-time operating systems; RTOS scheduling; transaction level Modeling,
Dynamic Allocation for Scratch-Pad Memory Using Compile-Time Decisions,2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47649086892&doi=10.1145%2f1151074.1151085&partnerID=40&md5=ac156e5e4604e857db50f0751bc827ce,"In this research, we propose a highly predictable, low overhead, and, yet, dynamic, memoryallocation strategy for embedded systems with scratch pad memory. A scratch pad is a fast compilermanaged SRAM memory that replaces the hardware-managed cache. It is motivated by its better real-time guarantees versus cache and by its significantly lower overheads in energy consumption, area, and overall runtime, even with a simple allocation scheme. Primarily scratch pad allocation methods are of two types. First, software-caching schemes emulate the workings of a hardware cache in software. Instructions are inserted before each load/store to check the software-maintained cache tags. Such methods incur large overheads in runtime, code size, energy consumption, and SRAM space for tags and deliver poor real-time guarantees just like hardware caches. A second category of algorithms partitions variables at compile-time into the two banks. However, a drawback of such static allocation schemes is that they do not account for dynamic program behavior. It is easy to see why a data allocation that never changes at runtime cannot achieve the full locality benefits of a cache. We propose a dynamic allocation methodology for global and stack data and program code that; (i) accounts for changing program requirements at runtime, (ii) has no software-caching tags, (iii) requires no runtime checks, (iv) has extremely low overheads, and (v) yields 100% predictable memory access times. In this method, data that is about to be accessed frequently is copied into the scratch pad using compiler-inserted code at fixed and infrequent points in the program. Earlier data is evicted if necessary. When compared to a provably optimal static allocation, results show that our scheme reduces runtime by up to 39.8% and energy by up to 31.3%, on average, for our benchmarks, depending on the SRAM size used. The actual gain depends on the SRAM size, but our results show that close to the maximum benefit in runtime and energy is achieved for a substantial range of small SRAM sizes commonly found in embedded systems. Our comparison with a direct mapped cache shows that our method performs roughly as well as a cached architecture. © In this research, we propose a highly predictable, low overhead, and, yet, dynamic, memoryallocation strategy for embedded systems with scratch pad memory. A scratch pad is a fast compilermanaged SRAM memory that replaces the hardware-managed cache. It is motivated by its better real-time guarantees versus cache and by its significantly lower overheads in energy consumption, area, and overall runtime, even with a simple allocation scheme. Primarily scratch pad allocation methods are of two types. First, software-caching schemes emulate the workings of a hardware cache in software. Instructions are inserted before each load/store to check the software-maintained cache tags. Such methods incur large overheads in runtime, code size, energy consumption, and SRAM space for tags and deliver poor real-time guarantees just like hardware caches. A second category of algorithms partitions variables at compile-time into the two banks. However, a drawback of such static allocation schemes is that they do not account for dynamic program behavior. It is easy to see why a data allocation that never changes at runtime cannot achieve the full locality benefits of a cache. We propose a dynamic allocation methodology for global and stack data and program code that; (i) accounts for changing program requirements at runtime, (ii) has no software-caching tags, (iii) requires no runtime checks, (iv) has extremely low overheads, and (v) yields 100% predictable memory access times. In this method, data that is about to be accessed frequently is copied into the scratch pad using compiler-inserted code at fixed and infrequent points in the program. Earlier data is evicted if necessary. When compared to a provably optimal static allocation, results show that our scheme reduces runtime by up to 39.8% and energy by up to 31.3%, on average, for our benchmarks, depending on the SRAM size used. The actual gain depends on the SRAM size, but our results show that close to the maximum benefit in runtime and energy is achieved for a substantial range of small SRAM sizes commonly found in embedded systems. Our comparison with a direct mapped cache shows that our method performs roughly as well as a cached architecture. © 2006, ACM. All rights reserved.",compiler; embedded systems; Memory allocation; Performance; scratch pad; software caching; software-managed cache,
Cache Coherence Tradeoffs in Shared-Memory MPSoCs,2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015307763&doi=10.1145%2f1151074.1151081&partnerID=40&md5=8f7d23f2a09568f24c60a8b5aecb365d,"Shared memory is a common interprocessor communication paradigm for single-chip multiprocessor platforms. Snoop-based cache coherence is a very successful technique that provides a clean shared-memory programming abstraction in general-purpose chip multiprocessors, but there is no consensus on its usage in resource-constrained multiprocessor systems on chips (MPSoCs) for embedded applications. This work aims at providing a comparative energy and performance analysis of cache-coherence support schemes in MPSoCs. Thanks to the use of a complete multiprocessor simulation platform, which relies on accurate technology-homogeneous power models, we were able to explore different cache-coherent shared-memory communication schemes for a number of cache configurations and workloads. © Shared memory is a common interprocessor communication paradigm for single-chip multiprocessor platforms. Snoop-based cache coherence is a very successful technique that provides a clean shared-memory programming abstraction in general-purpose chip multiprocessors, but there is no consensus on its usage in resource-constrained multiprocessor systems on chips (MPSoCs) for embedded applications. This work aims at providing a comparative energy and performance analysis of cache-coherence support schemes in MPSoCs. Thanks to the use of a complete multiprocessor simulation platform, which relies on accurate technology-homogeneous power models, we were able to explore different cache-coherent shared-memory communication schemes for a number of cache configurations and workloads. © 2006, ACM. All rights reserved.",Cache coherence; low power; Measurement Performance; multiprocessor; system-on-chip,
Evaluating Network Processors Using NetBench,2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949581735&doi=10.1145%2f1151074.1151084&partnerID=40&md5=cb910634548c7a4d45fd1c5f47505226,"The Network Processor market is one of the fastest growing segments of the microprocessor industry today. In spite of this increasing market importance, there does not exist a common framework to compare the performance of different Network Processor designs. Our primary goal in this study is to fill this gap by creating the NetBench benchmarking suite. NetBench is designed to represent Network Processor workloads. It contains 11 programs that form 18 different applications. The programs are selected from all levels of packet processing: Small, low-level code fragments as well as large application-level programs are included in the suite. These applications are representative of the Network Processor applications in the market. Using the SimpleScalar simulator to model an ARM processor, we study these programs in detail and compare key characteristics, such as instructions per cycle, instruction distribution, cache behavior, and branch prediction accuracy with the programs from MediaBench. Using statistical analysis, we show that the simulation results for the programs in NetBench have significantly different characteristics than programs in MediaBench. Finally, we present performance measurements from Intel IXP1200 Network Processor to show how NetBench can be utilized. © The Network Processor market is one of the fastest growing segments of the microprocessor industry today. In spite of this increasing market importance, there does not exist a common framework to compare the performance of different Network Processor designs. Our primary goal in this study is to fill this gap by creating the NetBench benchmarking suite. NetBench is designed to represent Network Processor workloads. It contains 11 programs that form 18 different applications. The programs are selected from all levels of packet processing: Small, low-level code fragments as well as large application-level programs are included in the suite. These applications are representative of the Network Processor applications in the market. Using the SimpleScalar simulator to model an ARM processor, we study these programs in detail and compare key characteristics, such as instructions per cycle, instruction distribution, cache behavior, and branch prediction accuracy with the programs from MediaBench. Using statistical analysis, we show that the simulation results for the programs in NetBench have significantly different characteristics than programs in MediaBench. Finally, we present performance measurements from Intel IXP1200 Network Processor to show how NetBench can be utilized. © 2006, ACM. All rights reserved.",benchmarking; Design; Embedded systems; Measurement; network processors; Performance; Standardization,
A Transformational Perspective into the Core of an Abstract Class Loader for the SSP,2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890301511&doi=10.1145%2f1196636.1196639&partnerID=40&md5=ddf5d113f86af8772df482ccbaa43ece,"The SSP is a hardware implementation of a subset of the JVM for use in high-consequence embedded applications. In this context, a majority of the activities belonging to class loading, as it is defined in the specification of the JVM, can be performed statically. Static class loading has the net result of dramatically simplifying the design of the SSP, as well as increasing its performance. Because of the high consequence nature of its applications, strong evidence must be provided that all aspects of the SSP have been implemented correctly. This includes the class loader. This article explores the possibility of formally verifying a class loader for the SSP implemented in the strategic programming language TL. Specifically, an implementation of the core activities of an abstract class loader is presented and its verification in ACL2 is considered. © 2006, ACM. All rights reserved.",ACL2 Verification; HATS; higher-order rewriting; Java Class Loader Strategic programming; Program Transformation; SSP; TL,
Reducing Code Size Through Address Register Assignment,2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856460584&doi=10.1145%2f1132357.1132365&partnerID=40&md5=67e4265bea561924fceac7a6019d36a7,"In DSP processors, minimizing the amount of address calculations is critical for reducing code size and improving performance, since studies of programs have shown that instructions that manipulate address registers constitute a significant portion of the overall instruction count (up to 55%). This work presents a compiler-based optimization strategy to “reduce the code size in embedded systems.” Our strategy maximizes the use of indirect addressing modes with postincrement/decrement capabilities available in DSP processors. These modes can be exploited by ensuring that successive references to variables access consecutive memory locations. To achieve this spatial locality, our approach uses both access pattern modification (program code restructuring) and memory storage reordering (data layout restructuring). Experimental results on a set of benchmark codes show the effectiveness of our solution and indicate that our approach outperforms the previous approaches to the problem. In addition to resulting in significant reductions in instruction memory (storage) requirements, the proposed technique improves execution time. © In DSP processors, minimizing the amount of address calculations is critical for reducing code size and improving performance, since studies of programs have shown that instructions that manipulate address registers constitute a significant portion of the overall instruction count (up to 55%). This work presents a compiler-based optimization strategy to “reduce the code size in embedded systems.” Our strategy maximizes the use of indirect addressing modes with postincrement/decrement capabilities available in DSP processors. These modes can be exploited by ensuring that successive references to variables access consecutive memory locations. To achieve this spatial locality, our approach uses both access pattern modification (program code restructuring) and memory storage reordering (data layout restructuring). Experimental results on a set of benchmark codes show the effectiveness of our solution and indicate that our approach outperforms the previous approaches to the problem. In addition to resulting in significant reductions in instruction memory (storage) requirements, the proposed technique improves execution time. © 2006, ACM. All rights reserved.",address registers; Design; DSP; Performance; register assignment; Software compilation,
Dual Flow Nets: Modeling the Control/Data-Flow Relation in Embedded Systems,2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878493053&doi=10.1145%2f1132357.1132360&partnerID=40&md5=6584db5be6a03fc4208024aa557d572e,"This paper addresses the interrelation between control and data flow in embedded system models through a new design representation, called Dual Flow Net (DFN). A modeling formalism with a very close-fitting control and data flow is achieved by this representation, as a consequence of enhancing its underlying Petri net structure. The work presented in this paper does not only tackle the modeling side in embedded systems design, but also the validation of embedded system models through formal methods. Various introductory examples illustrate the applicability of the DFN principles, whereas the capability of the model to with complex designs is demonstrated through the design and verification of a real-life Ethernet coprocessor. © This paper addresses the interrelation between control and data flow in embedded system models through a new design representation, called Dual Flow Net (DFN). A modeling formalism with a very close-fitting control and data flow is achieved by this representation, as a consequence of enhancing its underlying Petri net structure. The work presented in this paper does not only tackle the modeling side in embedded systems design, but also the validation of embedded system models through formal methods. Various introductory examples illustrate the applicability of the DFN principles, whereas the capability of the model to with complex designs is demonstrated through the design and verification of a real-life Ethernet coprocessor. © 2006, ACM. All rights reserved.",CTL; Design; Dual flow nets; embedded systems; formal verification; LTL; modeling; Petri nets; symbolic model checking; Theory; tripartite graph; Verification,
Software Thread Integration for Embedded System Display Applications,2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953859494&doi=10.1145%2f1132357.1132362&partnerID=40&md5=cb1edcca541c6acb9559c36bde9c2061,"Embedded systems require control of many concurrent real-time activities, leading to system designs that feature a variety of hardware peripherals, with each providing a specific, dedicated service. These peripherals increase system size, cost, weight, and design time. Software thread integration (STI) provides low-cost thread concurrency on general-purpose processors by automatically interleaving multiple threads of control into one. This simplifies hardware to software migration (which eliminates dedicated hardware) and can help embedded system designers meet design constraints, such as size, weight and cost. We have developed concepts for performing STI and have implemented many in our automated postpass compiler Thrint. Here we present the transformations and examine how well the compiler integrates threads for two display applications. We examine the integration procedure, the processor load, and code memory expansion. Integration allows reclamation of CPU idle time, allowing run-time speedups of 1.6x to 3.6x. © Embedded systems require control of many concurrent real-time activities, leading to system designs that feature a variety of hardware peripherals, with each providing a specific, dedicated service. These peripherals increase system size, cost, weight, and design time. Software thread integration (STI) provides low-cost thread concurrency on general-purpose processors by automatically interleaving multiple threads of control into one. This simplifies hardware to software migration (which eliminates dedicated hardware) and can help embedded system designers meet design constraints, such as size, weight and cost. We have developed concepts for performing STI and have implemented many in our automated postpass compiler Thrint. Here we present the transformations and examine how well the compiler integrates threads for two display applications. We examine the integration procedure, the processor load, and code memory expansion. Integration allows reclamation of CPU idle time, allowing run-time speedups of 1.6x to 3.6x. © 2006, ACM. All rights reserved.",Algorithms; Design; fine-grain concurrency; hardware to software migration; software thread integration,
Reducing Dynamic and Leakage Energy in VLIW Architectures,2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952957956&doi=10.1145%2f1132357.1132358&partnerID=40&md5=ac73e8cc0bf9841bc65e924d0ff79982,"The mobile computing device market has been growing rapidly. This brings the technologies that optimize system energy to the forefront. As circuits continue to scale in the future, it would be important to optimize both leakage and dynamic energy. Effective optimization of leakage and dynamic energy consumption requires a vertical integration of techniques spanning from circuit to software levels. Schedule slacks in codes executing in VLIW architectures present an opportunity for such an integration. In this paper, we present three compiler-directed techniques that take advantage of schedule slacks to optimize leakage and dynamic energy consumption. Integer ALU (IALU) components operating with multiple supply voltages are designed to provide different lowenergy versions that possess different operational latencies. The goal of the first technique explored is to maximize the number of operations mapped to IALU components with the lowest energy consumption without extending the schedule length. We also consider a variant of this technique that saves more energy at the cost of some performance loss. The second technique uses two leakagecontrol mechanisms to reduce leakage energy consumption when no operations are scheduled in the component. Our evaluation of these two approaches, using fifteen benchmarks, shows that based on the number and duration of slacks, the availability of low-energy functional units and the relative magnitude of leakage and dynamic energy, either leakage or dynamic energy consumption, will provide more energy gains. Finally, we provide a unified energy-optimization strategy that integrates both dynamic and leakage energy-reduction schemes. The proposed techniques have been incorporated into a cycle accurate simulator using parameters extracted from circuit-level simulation. Our results show that the unified scheme generates better results than using either of dynamic and leakage energy-reduction techniques independently. © The mobile computing device market has been growing rapidly. This brings the technologies that optimize system energy to the forefront. As circuits continue to scale in the future, it would be important to optimize both leakage and dynamic energy. Effective optimization of leakage and dynamic energy consumption requires a vertical integration of techniques spanning from circuit to software levels. Schedule slacks in codes executing in VLIW architectures present an opportunity for such an integration. In this paper, we present three compiler-directed techniques that take advantage of schedule slacks to optimize leakage and dynamic energy consumption. Integer ALU (IALU) components operating with multiple supply voltages are designed to provide different lowenergy versions that possess different operational latencies. The goal of the first technique explored is to maximize the number of operations mapped to IALU components with the lowest energy consumption without extending the schedule length. We also consider a variant of this technique that saves more energy at the cost of some performance loss. The second technique uses two leakagecontrol mechanisms to reduce leakage energy consumption when no operations are scheduled in the component. Our evaluation of these two approaches, using fifteen benchmarks, shows that based on the number and duration of slacks, the availability of low-energy functional units and the relative magnitude of leakage and dynamic energy, either leakage or dynamic energy consumption, will provide more energy gains. Finally, we provide a unified energy-optimization strategy that integrates both dynamic and leakage energy-reduction schemes. The proposed techniques have been incorporated into a cycle accurate simulator using parameters extracted from circuit-level simulation. Our results show that the unified scheme generates better results than using either of dynamic and leakage energy-reduction techniques independently. © 2006, ACM. All rights reserved.",Algorithms; design; measurement; performance,
Offset Assignment Using Simultaneous Variable Coalescing,2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67249112878&doi=10.1145%2f1196636.1196641&partnerID=40&md5=128b4404284132fea4d8b9e2f9cb3191,"The generation of efficient addressing code is a central problem in compiling for processors with restricted addressing modes, like digital signal processors (DSPs). Offset assignment (OA) is the problem of allocating scalar variables to memory, so as to minimize the need of addressing instructions. This problem is called simple offset assignment (SOA) when a single address register is available, and general offset assignment (GOA) when more address registers are used. This paper shows how variables' liveness information can be used to dramatically reduce the addressing instructions required to access local variables on the program stack. Two techniques that make effective use of variable coalescing to solve SOA and GOA are described, namely coalescing SOA (CSOA) and coalescing GOA (CGOA). In addition, a thorough comparison between these algorithms and others described in the literature is presented. The experimental results, when compiling MediaBench benchmark programs with the LANCE compiler, reveal a very significant improvement of the proposed techniques over the other available solutions to the problem. © 2006, ACM. All rights reserved.",address registers; Algorithms; autoincrement addressing modes; DSPs; Languages; Performance; register allocation; Stack offset assignment; variable coalescing,
"Memory Overflow Protection for Embedded Systems Using Run-Time Checks, Reuse, and Compression",2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963800954&doi=10.1145%2f1196636.1196637&partnerID=40&md5=77205e0f4c90a0627337ca6fe7ee286f,"Embedded systems usually lack virtual memory and are vulnerable to memory overflow since they lack a mechanism to detect overflow or use swap space thereafter. We present a method to detect memory overflows using compiler-inserted software run-time checks. Its overheads in run-time and energy are 1.35 and 1.12%, respectively. Detection of overflow allows system-specific remedial action. We also present techniques to grow the stack or heap segment after they overflow, into previously unutilized space, such as dead variables, free holes in the heap, and space freed by compressing live variables. These may avoid the out-of-memory error if the space recovered is enough to complete execution. The reuse methods are able to grow the stack or heap beyond its overflow by an amount that varies widely by application—the amount of recovered space ranges from 0.7 to 93.5% of the combined stack and heap size. © 2006, ACM. All rights reserved.",data compression; heap overflow; Languages; Out-of-memory errors; Reliability; reliability; reuse; run-time checks; stack overflow,
Predicate Abstraction for Reachability Analysis of Hybrid Systems,2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34948903134&doi=10.1145%2f1132357.1132363&partnerID=40&md5=a3cf2e6751b3d6662ecb13ed5bc85ad3,"Embedded systems are increasingly finding their way into a growing range of physical devices. These embedded systems often consist of a collection of software threads interacting concurrently with each other and with a physical, continuous environment. While continuous dynamics have been well studied in control theory, and discrete and distributed systems have been investigated in computer science, the combination of the two complexities leads us to the recent research on hybrid systems. This paper addresses the formal analysis of such hybrid systems. Predicate abstraction has emerged to be a powerful technique for extracting finite-state models from infinite-state discrete programs. This paper presents algorithms and tools for reachability analysis of hybrid systems by combining the notion of predicate abstraction with recent techniques for approximating the set of reachable states of linear systems using polyhedra. Given a hybrid system and a set of predicates, we consider the finite discrete quotient whose states correspond to all possible truth assignments to the input predicates. The tool performs an on-the-fly exploration of the abstract system. We present the basic techniques for guided search in the abstract state-space, optimizations of these techniques, implementation of these in our verifier, and case studies demonstrating the promise of the approach. We also address the completeness of our abstraction-based verification strategy by showing that predicate abstraction of hybrid systems can be used to prove bounded safety. © Embedded systems are increasingly finding their way into a growing range of physical devices. These embedded systems often consist of a collection of software threads interacting concurrently with each other and with a physical, continuous environment. While continuous dynamics have been well studied in control theory, and discrete and distributed systems have been investigated in computer science, the combination of the two complexities leads us to the recent research on hybrid systems. This paper addresses the formal analysis of such hybrid systems. Predicate abstraction has emerged to be a powerful technique for extracting finite-state models from infinite-state discrete programs. This paper presents algorithms and tools for reachability analysis of hybrid systems by combining the notion of predicate abstraction with recent techniques for approximating the set of reachable states of linear systems using polyhedra. Given a hybrid system and a set of predicates, we consider the finite discrete quotient whose states correspond to all possible truth assignments to the input predicates. The tool performs an on-the-fly exploration of the abstract system. We present the basic techniques for guided search in the abstract state-space, optimizations of these techniques, implementation of these in our verifier, and case studies demonstrating the promise of the approach. We also address the completeness of our abstraction-based verification strategy by showing that predicate abstraction of hybrid systems can be used to prove bounded safety. © 2006, ACM. All rights reserved.",Algorithms; hybrid systems; predicate abstraction; Reachability analysis; Reliability; Verification,
Vista: VPO Interactive System for Tuning Applications,2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877721016&doi=10.1145%2f1196636.1196640&partnerID=40&md5=5e8238450d6ed3c7f470a8f568dabe9d,"Software designers face many challenges when developing applications for embedded systems. One major challenge is meeting the conflicting constraints of speed, code size, and power consumption. Embedded application developers often resort to hand-coded assembly language to meet these. © 2006, ACM. All rights reserved.",Algorithms; Experimentation; genetic algorithms; interactive compilation; Measurement; Performance; phase ordering; User-directed code improvement,
Hardware Support for Detecting Illegal References in a Multiapplication Real-Time Java Environment,2006,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956380836&doi=10.1145%2f1196636.1196638&partnerID=40&md5=15f402f365e0f48267ff954144b72885,"Our objective is to adapt the Java memory management to an embedded system, e.g., a wireless PDA executing concurrent multimedia applications within a single JVM. This paper provides software, and hardware-based solutions detecting both illegal references across the application memory spaces and dangling pointers within an application space. We give an approach to divide/share the memory among the applications executing concurrently in the system. We introduce and define application-specific memory, building upon the real-time specification for Java (RTSJ) from the real-time Java expert group. The memory model used in RTSJ imposes strict rules for assignment between memory areas, preventing the creation of dangling pointers, and thus maintaining the pointer safety of Java. Our implementation solution to ensure the checking of these rules before each assignment inserts write barriers that use a stack-based algorithm. This solution adversely affects both the performance and predictability of the RTSJ applications, which can be improved by using an existing hardware support. © 2006, ACM. All rights reserved.",algorithms; design; garbage collection; Languages; memory management; performance; Write barriers,
Optimizing Instruction Cache Performance of Embedded Systems,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149077823&doi=10.1145%2f1113830.1113839&partnerID=40&md5=e6dc18ff280578718990f3cc16feb683,"In the embedded domain, the gap between memory and processor performance and the increase in application complexity need to be supported without wasting precious system resources: die size, power, etc. For these reasons, effective exploitation of small and simple cache memories is of the utmost importance. However, programs running on such caches can experience serious inefficiencies due to cache conflicts. We present a new Cache-Aware Code Allocation Technique (CAT), which transforms the structure of programs so that their behavior toward memory can meet the locality features the cache is able to exploit. The proposed approach uses detailed information of program execution to place program areas into memory and employs the new idea of “look-forward estimation” that helps to seek better global layouts during the placement of each area. CAT-optimized programs outperform the original ones achieving the same miss rate on two times, and sometimes four times, smaller caches. Moreover, CAT improves the instruction miss rate by more than 40% if compared to the best procedure-reordering algorithm. CAT performances derive from the increased number of cache lines that support the execution of optimized applications and from a more balanced load on them. © 2005, ACM. All rights reserved.",cache performance; code generation; code reordering; conflict miss; Design; Embedded systems; Optimization; Performance,
Compiler-Directed High-Level Energy Estimation and Optimization,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994181779&doi=10.1145%2f1113830.1113835&partnerID=40&md5=0f3fbc73485f84d579605d560a5406d0,"The demand for high-performance architectures and powerful battery-operated mobile devices has accentuated the need for power optimization. While many power-oriented hardware optimization techniques have been proposed and incorporated in current systems, the increasingly critical power constraints have made it essential to look for software-level optimizations as well. The compiler can play a pivotal role in addressing the power constraints of a system as it wields a significant influence on the application's runtime behavior. This paper presents a novel Energy-Aware Compilation (EAC) framework that estimates and optimizes energy consumption of a given code, taking as input the architectural and technological parameters, energy models, and energy/performance/code size constraints. The framework has been validated using a cycle-accurate architectural-level energy simulator and found to be within 6% error margin while providing significant estimation speedup. The estimation speed of EAC is the key to the number of optimization alternatives that can be explored within a reasonable compilation time. As shown in this paper, EAC allows compiler writers and system designers to investigate power-performance tradeoffs of traditional compiler optimizations and to develop energy-conscious high-level code transformations. © 2005, ACM. All rights reserved.",Energy-Aware Compilation (EAC); Languages; mobile devices,
Analyzing Data Reuse for Cache Reconfiguration,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013366564&doi=10.1145%2f1113830.1113836&partnerID=40&md5=49295f42eefea5b01c8e1b9787141b6a,"Classical compiler optimizations assume a fixed cache architecture and modify the program to take best advantage of it. In some cases, this may not be the best strategy because each nest might work best with a different cache configuration and transforming a nest for a given fixed cache configuration may not be possible due to data and control dependences. Working with a fixed cache configuration can also increase energy consumption in loops where the best required configuration is smaller than the default (fixed) one. In this paper, we take an alternate approach and modify the cache configuration for each nest, depending on the access pattern exhibited by the nest. We call this technique compiler-directed cache polymorphism (CDCP). More specifically, in this paper, we make the following contributions. First, we present an approach for analyzing data reuse properties of loop nests. Second, we give algorithms to simulate the footprints of array references in their reuse space. Third, based on our reuse analysis, we present an optimization algorithm to compute the cache configurations for each loop nest. Our experimental results show that CDCP is very effective in finding the near-optimal data cache configurations for different nests in array-intensive applications. © 2005, ACM. All rights reserved.",Algorithms; cache locality; cache polymorphism; compilers; data reuse; Design; Embedded software; energy consumption; Measurement; Performance,
Loops in Esterel,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449476657&doi=10.1145%2f1113830.1113832&partnerID=40&md5=bbad702566dfcab10562992b77f59c4c,"ESTEREL is a synchronous design language for the specification of reactive systems. Thanks to its compact formal semantics, code generation for ESTEREL is essentially provably correct. In practice, due to the many intricacies of an optimizing compiler, an actual proof would be in order. To begin with, we need a precise description of an efficient translation scheme, into some lower-level formalism. We tackle this issue on a specific part of the compilation process: the translation of loop constructs. First, because of instantaneous loops, programs may generate runtime errors, which cannot be tolerated for embedded systems, and have to be predicted and prevented at compile time. Second, because of schizophrenia, loops must be partly unfolded, making C code generation, as well as logic synthesis, nonlinear in general. Clever expansion strategies are required to minimize the unfolding. We first characterize these two difficulties w.r.t. the formal semantics of ESTEREL. We then derive very efficient, correct-by-construction algorithms to verify and transform loops at compile time, using static analysis and program rewriting techniques. With this aim in view, we extend the language with a new gotopause construct, which we use to encode loops. It behaves as a noninstantaneous jump instruction compatible with concurrency. © 2005, ACM. All rights reserved.",Algorithms; code generation; Languages; static analysis; Synchronous languages; Theory,
Shortest-Path Algorithms for Real-Time Scheduling of FIFO Tasks with Minimal Energy Use,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987951328&doi=10.1145%2f1113830.1113838&partnerID=40&md5=c8b70721ee55bd847fca16ebf22ec23a,"We present an algorithm for scheduling a set of nonrecurrent tasks (or jobs) with FIFO realtime constraints so as to minimize the total energy consumed when the tasks are performed on a dynamically variable voltage processor. Our algorithm runs in linear time and thus, in this case, is an improvement over the classical algorithm of Yao et al. It was inspired by considering the problem as a shortest-path problem.We also propose an algorithm to deal with the case where the processor has only a limited number of clock frequencies. This algorithm gives the optimum schedule with the minimum number of speed changes, which is important when the speed switching overhead cannot be neglected. All our algorithms are linear in the number of tasks if the arrivals and deadlines are sorted and otherwise need O(N log N) time. These complexities are shown to be the best possible. Finally, we extend our results to fluid tasks and to nonconvex cost functions. © 2005, ACM. All rights reserved.",Algorithms; low-power design; Performance; Real-time systems; scheduling; voltage,
Translating Discrete-Time Simulink to Lustre,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547418979&doi=10.1145%2f1113830.1113834&partnerID=40&md5=bdfae67b5d9b20b78507324f9077b2ee,"We present amethod of translating discrete-time Simulink models to Lustre programs. Our method consists of three steps: type inference, clock inference, and hierarchical bottom-up translation. In the process, we explain and formalize the typing and timing mechanisms of Simulink. The method has been implemented in a prototype tool called S2L, which has been used in the context of a European research project to translate two automotive controller models provided by Audi. © 2005, ACM. All rights reserved.",Code generation; Design; embedded software; Languages; Lustre; Reliability; Simulink; Verification,
Range-Free Localization and Its Impact on Large Scale Sensor Networks,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855367697&doi=10.1145%2f1113830.1113837&partnerID=40&md5=30c645cf47c8403b96e2855236d116f6,"With the proliferation of location dependent applications in sensor networks, location awareness becomes an essential capability of sensor nodes. Because coarse accuracy is sufficient for most sensor network applications, solutions in range-free localization are being pursued as a cost-effective alternative to more expensive range-based approaches. In this paper, we present APIT, a novel localization algorithm that is range-free.We show that our APIT scheme performs best when an irregular radio pattern and random node placement are considered, and low communication overhead is desired. We compare our work, via extensive simulation, with three state-of-the-art range-free localization schemes to identify the preferable system configurations of each. In addition, we provide insight into the impact of localization accuracy on various location dependent applications and suggestions on improving their performance in the presence of such inaccuracy. © 2005, ACM. All rights reserved.",Algorithms; Design; Localization; location discovery; Performance; positioning; sensor network,
Eliminating Stack Overflow by Abstract Interpretation,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014871817&doi=10.1145%2f1113830.1113833&partnerID=40&md5=6778664019121b9f5ad985d0be1f31a6,"An important correctness criterion for software running on embedded microcontrollers is stack safety: a guarantee that the call stack does not overflow. Our first contribution is a method for statically guaranteeing stack safety of interrupt-driven embedded software using an approach based on context-sensitive dataflow analysis of object code. We have implemented a prototype stack analysis tool that targets software for Atmel AVR microcontrollers and tested it on embedded applications compiled from up to 30,000 lines of C. We experimentally validate the accuracy of the tool, which runs in under 10 sec on the largest programs that we tested. The second contribution of this paper is the development of two novel ways to reduce stackmemory requirements of embedded software. © 2005, ACM. All rights reserved.",abstract interpretation; call stack; context sensitive; dataflow analysis; interrupt-driven; Languages; Microcontroller; Reliability; sensor network; Verification,
A Curriculum for Embedded System Engineering,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44049086142&doi=10.1145%2f1086519.1086525&partnerID=40&md5=232bfd610581b7f4184cc6c89384fdf7,"The paper presents a curriculum for a 4-year undergraduate program in Embedded System Engineering (ESE). The curriculum was developed using a two-step approach. First, a body of education knowledge for Embedded System Engineering was defined. The body consists of sixteen knowledge areas. Each area is composed of several knowledge units, some designated as core and others as electives. The minimum lecture time for the core of each knowledge area is identified. The Body of Knowledge for Computer Engineering, developed by the IEEE-CS/ACM task force for Computing Curricula, was used as a reference. The education knowledge for ESE then served as the base for the development of the program curriculum. The curriculum has a strong mathematics and basic science base, an in-depth exposure to engineering science and design of systems implemented with digital hardware and software, and coverage of two prominent application areas of embedded systems. The curriculum core takes approximately 3 years of the program; the remaining part is elective. © 2005, ACM. All rights reserved.",Design; Embedded system engineering; Embedded system engineering curriculum; Undergraduate engineering curriculum,
What Is Embedded Systems and How Should It Be Taught?-Results from a Didactic Analysis,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008048951&doi=10.1145%2f1086519.1086528&partnerID=40&md5=cbce75ae70a1040e8b4b2b664b13411c,"This paper provides an analysis of embedded systems education using a didactic approach. Didactics is a field of educational studies mostly referring to research aimed at investigating what's unique with a particular subject and how this subject ought to be taught. From the analysis we conclude that embedded systems has a thematic identity and a functional legitimacy. This implies that the subject would benefit from being taught with an exemplifying selection and using an interactive communication, meaning that the education should move from teaching “something of everything” toward “everything of something.” The interactive communication aims at adapting the education toward the individual student, which is feasible if using educational methods inspired by projectorganized and problem-based learning. This educational setting is also advantageous as it prepares the students for a future career as embedded system engineers. The conclusions drawn from the analysis correlate with our own experiences from education in mechatronics as well as with a recently published study of 21 companies in Sweden dealing with industrial software engineering. © 2005, ACM. All rights reserved.",Didactic analysis; education; Human Factors; Theory,
Skiing the Embedded Systems Mountain,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-48749094360&doi=10.1145%2f1086519.1086523&partnerID=40&md5=af209c7aa8a7dfdde4df70636901605b,"UCLA teaches students how to master the steep slopes of the embedded systems mountain. The EE201A graduate course connects high-level design specification to embedded implementation. There is a long-standing and wide culture gap between system designers that create those abstract specifications and the system architects that need to implement them. In industry, the culture gap has separated software from hardware teams, and platform creators from platform users. In an embedded context, where these are very tightly connected, this leads to large inefficiencies both in design time and design results. Our course takes students to both sides of the gap and lets them look at this problem from different perspectives. For a given application, it teaches how to select target architectures, tools, and design methods. The course covers a stepwise systematic design process. It includes specification, transformation, and refinement of an application. Specifications enable systematic and structured expression of an application. Transformations rework specifications into ones that are a better match for a given target architecture. Refinements lower the abstraction level toward the target architecture. The embedded systems mountain is traversed in two directions. A vertical refinement axis covers elements such as power-memory-reduction methods or fixedpoint refinement. A horizontal exploration axis covers various architecture alternatives including application-specific integrated circuits (ASIC), domain-specific processors, digital signal processors (DSP), embedded cores, programmable processors, and system-on-chip (SOC). During the course, the students also go through an extensive design project to apply the methods learned in this course. A typical embedded application is used to drive the project. In this paper it is illustrated using an embedded version of an image encoder, more specifically a JPEG encoder. Several commercial tools, design environments, and platforms have been used as alternative implementation targets for this application. © 2005, ACM. All rights reserved.",cosimulation; Design; design space exploration; Education,
Guidelines for a Graduate Curriculum on Embedded Software and Systems,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011697035&doi=10.1145%2f1086519.1086526&partnerID=40&md5=b496d530f7bf6258cb58b84c0c363730,"The design of embedded real-time systems requires skills from multiple specific disciplines, including, but not limited to, control, computer science, and electronics. This often involves experts from differing backgrounds, who do not recognize that they address similar, if not identical, issues from complementary angles. Design methodologies are lacking in rigor and discipline so that demonstrating correctness of an embedded design, if at all possible, is a very expensive proposition that may delay significantly the introduction of a critical product. While the economic importance of embedded systems is widely acknowledged, academia has not paid enough attention to the education of a community of high-quality embedded system designers, an obvious difficulty being the need of interdisciplinarity in a period where specialization has been the target of most education systems. This paper presents the reflections that took place in the European Network of Excellence Artist leading us to propose principles and structured contents for building curricula on embedded software and systems. © 2005, ACM. All rights reserved.",architecture and design; Computer Science Education; control; Curriculum; distributed systems; embedded systems; extrafunctional properties; Graduate curriculum; Information Systems Education; labs; real-time,
Reducing Data Cache Leakage Energy Using a Compiler-Based Approach,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56749127516&doi=10.1145%2f1086519.1086529&partnerID=40&md5=7a3f2440d47db95a774da1462cebb341,"Silicon technology advances have made it possible to pack millions of transistors-switching at high clock speeds-on a single chip. While these advances bring unprecedented performance to electronic products, they also pose difficult power/energy consumption problems. For example, large number of transistors in dense on-chip cache memories consume significant static (leakage) power even if the cache is not used by the current computation. While previous compiler research studied code and data restructuring for improving data cache performance, to our knowledge, there exists no compiler-based study that targets data cache leakage power consumption. In this paper, we present code restructuring techniques for array-based and pointer-intensive applications for reducing data cache leakage energy consumption. The idea is to let the compiler analyze the application code and insert instructions that turn off cache lines that keep variables not used by the current computation. This turning-off does not destroy contents of a cache line and waking up the cache line (when it is accessed later) does not incur much overhead. Due to inherent data locality in applications, we find that, at a given time, only a small portion of the data cache needs to be active; the remaining part can be placed into a leakage-saving mode (state); i.e., they can be turned off. Our experimental results indicate that the proposed compiler-based strategy reduces the cache energy consumption significantly. We also demonstrate how different compiler optimizations can increase the effectiveness of our strategy. © 2005, ACM. All rights reserved.",Algorithms; arrayintensive applications; Compiler analysis; data caches; energy optimization; Experimentation; pointer-intensive applications,
Dynamic Delay-Constrained Minimum-Energy Dissemination in Wireless Sensor Networks,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547732703&doi=10.1145%2f1086519.1086530&partnerID=40&md5=08ae27c5c11f6c9e296d7544a45e0aae,"Disseminating data generated by sensors to users is one of useful functions of sensor networks. In probable real-time applications of sensor networks, multiple mobile users should receive data within their end-to-end delay constraint. In this paper, we propose a dynamic DElay-constrained minimum-Energy Dissemination (DEED) scheme. A dissemination tree (d-tree) is updated in a distributed way without regenerating the tree from scratch, such that energy consumption of the tree is minimized while satisfying end-to-end delay constraints. The d-tree is adjusted using delay estimation based on geometric distance. DEED increases the probability that packets arrive at users within an upper-bound end-to-end delay (UBED) and minimizes energy consumption in both building the d-tree and disseminating data to mobile sinks. Evaluation results show that DEED makes each node consume small energy resources and maintains fewer UBED misses when compared to Directed Diffusion and other baselines for sensor networks. © 2005, ACM. All rights reserved.",Algorithms; Design; energy; multicast; Sensor network,
Editorial,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-64149119790&doi=10.1145%2f1086519.1086520&partnerID=40&md5=44634e132d281aadfe0ee8726bc6e4e3,[No abstract available],,
The Embedded Software Consortium of Taiwan,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548770792&doi=10.1145%2f1086519.1086527&partnerID=40&md5=82a76a5e469046980d97ff4062b4e89a,"The advancement of semiconductor manufacturing technology makes it practical to place a traditional board-level embedded system on a single chip. The evolvement of system-on-chip (SoC) techniques presents new challenges for integrated circuit designs as well as embedded software and systems. To address these challenges, the Ministry of Education (MOE) of Taiwan has been running the VLSI Circuits and Systems Education Program since 1996. This program adopts a topdown approach by forming six domain-specific, intercollegiate consortia. The Embedded Software (ESW) consortium addresses the challenges of embedded software for SoC systems. This paper first introduces the six-consortium architecture and the organization and programs of ESW. We next describe the embedded software curriculum developed by ESW. This curriculum will later be implemented in most universities and colleges in Taiwan to promote the capabilities of embedded software design and implementations. Finally, we present an execution summary of ESW 2004. © 2005, ACM. All rights reserved.",Documentation; educational curricula; Embedded software; integrated circuit design; Management,
An Overview of Embedded System Design Education at Berkeley,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008051269&doi=10.1145%2f1086519.1086521&partnerID=40&md5=0106ab401802d390fe829d0dcc6081b1,"Embedded systems have been a traditional area of strength in the research agenda of the University of California at Berkeley. In parallel to this effort, a pattern of graduate and undergraduate classes has emerged that is the result of a distillation process of the research results. In this paper, we present the considerations that are driving our curriculum development and we review our undergraduate and graduate program. In particular, we describe in detail a graduate class (EECS249: Design of Embedded Systems: Modeling, Validation and Synthesis) that has been taught for six years. A common feature of our education agenda is the search for fundamentals of embedded system science rather than embedded system design techniques, an approach that today is rather unique. © 2005, ACM. All rights reserved.",architectural design; Design; embedded software; embedded systems; functional design; Graduate and undergraduate education; sourcework; Standardization; Theory; Verification,
Undergraduate Embedded System Education at Carnegie Mellon,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014656529&doi=10.1145%2f1086519.1086522&partnerID=40&md5=47a050768b048f681586774c16aae682,"Embedded systems encompass a wide range of applications, technologies, and disciplines, necessitating a broad approach to education. We describe embedded system coursework during the first 4 years of university education (the U.S. undergraduate level). Embedded application curriculum areas include: small and single-microcontroller applications, control systems, distributed embedded control, system-on-chip, networking, embedded PCs, critical systems, robotics, computer peripherals, wireless data systems, signal processing, and command and control. Additional cross-cutting skills that are important to embedded system designers include: security, dependability, energyaware computing, software/systems engineering, real-time computing, and human-computer interaction. We describe lessons learned from teaching courses in many of these areas, as well as general skills taught and approaches used, including a heavy emphasis on course projects to teach system skills. © 2005, ACM. All rights reserved.",curriculum; Design; Embedded systems education; Experimentation; Human Factors; Performance; Reliability; Security,
Introducing Embedded Software and Systems Education and Advanced Learning Technology in an Engineering Curriculum,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994271594&doi=10.1145%2f1086519.1086524&partnerID=40&md5=442a268d56da432a4e5356ccca705279,"Embedded software and systems are at the intersection of electrical engineering, computer engineering, and computer science, with, increasing importance, in mechanical engineering. Despite the clear need for knowledge of systems modeling and analysis (covered in electrical and other engineering disciplines) and analysis of computational processes (covered in computer science), few academic programs have integrated the two disciplines into a cohesive program of study. This paper describes the efforts conducted at Vanderbilt University to establish a curriculum that addresses the needs of embedded software and systems. Given the compartmentalized nature of traditional engineering schools, where each discipline has an independent program of study, we have had to devise innovative ways to bring together the two disciplines. The paper also describes our current efforts in using learning technology to construct, manage, and deliver sophisticated computer-aided learning modules that can supplement the traditional course structure in the individual disciplines through out-of-class and in-class use. © 2005, ACM. All rights reserved.",Computer-aided learning; Embedded Systems Education,
Converging CSP Specifications and C++ Programming via Selective Formalism,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961747531&doi=10.1145%2f1067915.1067919&partnerID=40&md5=6f7c307dd06910aa653862dde0bc8f72,"CSP (communicating sequential processes) is a useful algebraic notation for creating a hierarchical behavioral specification for concurrent systems, due to its formal interprocess synchronization and communication semantics. CSP specifications are amenable to simulation and formal verification by model-checking tools. A translator has been created to synthesize C++ code from CSP for execution with an object-oriented framework called CSP++, thereby making CSP specifications directly executable. To overcome the drawback that CSP is neither a full-featured nor popular programming language, an approach called “selective formalism” allows the use of CSP to be limited to specifying the control portion of a system, while the rest of its functionality is supplied in the form of C++ modules. These are activated through association with abstract events in the CSP specification. This is a new means of bringing convergence between a formal method and a popular programming language. It is believed that this methodology can be extended to hardware/software codesign for embedded systems. © 2005, ACM. All rights reserved.",Design; Executable specifications; hardware/software codesign; Languages; object-oriented application frameworks; Verification,
PLTL-Partitioned Model Checking for Reactive Systems under Fairness Assumptions,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977546439&doi=10.1145%2f1067915.1067918&partnerID=40&md5=d6c3eb5acad16ce36fe509d230c52dac,"We are interested in verifying dynamic properties of finite state reactive systems under fairness assumptions by model checking. The systems we want to verify are specified through a top-down refinement process. In order to deal with the state explosion problem, we have proposed in previous works to partition the reachability graph and to perform the verification on each part separately. Moreover, we have defined a class, called Bmod, of dynamic properties that are verifiable by parts, whatever the partition. We decide if a property P belongs to Bmodby looking at the form of the Büchi automaton that accepts -P. However, when a property P belongs to Bmod, the property f ⟹ P, where f is a fairness assumption, does not necessarily belong to Bmod. In this paper, we propose to use the refinement process in order to build the parts on which the verification has to be performed. We then show that with such a partition, if a property P is verifiable by parts and if f is the expression of the fairness assumptions on a system, then the property f ⟹ P is still verifiable by parts. This approach is illustrated by its application to the chip card protocol T=1 using the B engineering design language. © 2005, ACM. All rights reserved.",Design; fairness assumptions; out-of-core model checking; PLTL model checking; Refinement design; Verification,
Instantaneous Current Modeling in a Complex VLIW Processor Core,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247248990&doi=10.1145%2f1067915.1067923&partnerID=40&md5=d08f0778043d778c7120305d672fd002,"Measuring and modeling instantaneous current consumption or current dynamics of a processor is important in embedded system designs, wireless communications, low-energy mobile computing, security of communications, and reliability. In this paper,we introduce a new instruction-level based macromodeling approach for instantaneous current consumption in a complex processor core along with new instantaneous current measurement techniques at the instruction and program level. Current consumption and voltage supplywaveforms of a processor core were acquired by a sampling oscilloscope through an external interrupt-based setup. Accurate measurements of current, power and energy consumption at the instruction, block, or program level were obtained from analyzing the stored current and voltage waveforms. The current simulation methodology uses elementary functions called atomic functions to approximate the instantaneous current consumption at the instruction level. Based on these atomic functions, a simulated instantaneous current waveform at the program level was built. First, a base waveform of the current simulation was generated by the use of four basic current superposition principles. Secondly, a final waveform of the simulated current was generated from the base waveform by applying a factorial adjustment as a function of the instruction parallelism and sequencing. Step-by-step modeling procedures with numerical examples are presented. The model captured 98% of the variation of the instantaneous current for six complex applications, with an average RMS error of less than 2.2% of the average measured mean. Energy estimates obtained by the use of the simulated current waveforms were within 1.4% of the measured values. This research is important, since for the first time highly accurate instruction-based models of instantaneous current and power for complex processor cores have been developed. © 2005, ACM. All rights reserved.",current and power measurement in a processor; Experimentation; instantaneous current model; Instruction-level current model; Measurement; power and energy model; Theory; Verification,
A Reprogrammable Customization Framework for Efficient Branch Resolution in Embedded Processors,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-52349100827&doi=10.1145%2f1067915.1067924&partnerID=40&md5=2bad0973a2653fb0f71f5825fe588a97,"We present a customization framework for embedded processors which employs the utilization of application-specific information, thus specializing the processor's microarchitecture to the application needs. The increased processor utilization leads to a low-cost system implementation with no sacrifice in performance requirements and to reduced custom hardware in a typical SOC. We illustrate these ideas through the branch resolution problem, known to impose severe performance degradation on control-dominated embedded applications. A customization approach for early branch resolution and subsequent folding is presented. The application-specific information is captured by the microarchitecture through a low-cost reprogrammable hardware, thus attaining the twin benefits of processor standardization and application-specific customization. Experimental results show that for a representative set of control-dominated applications a reduction in the range of 3-22% in processor cycles can be achieved, thus extending the scope of low-cost embedded processors in complex codesigns for control intensive systems. © 2005, ACM. All rights reserved.",Branch resolution; Design; Performance; pipeline organization,
Combining Supervisor Synthesis and Model Checking,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847206936&doi=10.1145%2f1067915.1067920&partnerID=40&md5=30deddc1ac37a610ab3b4fbfa9af55cc,"Model checking and supervisor synthesis have been successful in solving different design problems related to discrete systems in the last decades. In this paper, we analyze some advantages and drawbacks of these approaches and combine them for mutual improvement. We achieve this through a generalization of the supervisory control problem proposed by Ramadge and Wonham. The objective of that problem is to synthesize a supervisor which constrains a system's behavior according to a given specification, ensuring controllability and coaccessibility. By introducing a new representation of the solution using systems of μ-calculus equations, we are able to handle these two conditions separately and thus to exchange the coaccessibility requirement by any condition that could be used in model checking. Well-known results on μ-calculus model checking allow us to easily assess the computational complexity of any generalization. Moreover, the model checking approach also delivers algorithms to solve the generalized synthesis problem. We include an example in which the coaccessibility requirement is replaced by fairness constraints. The paper also contains an analysis of related work by several authors. © 2005, ACM. All rights reserved.",Design and Verification; model checking; Ramadge-Wonham; supervisor synthesis,
A Highly Configurable Cache for Low Energy Embedded Systems,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56749142443&doi=10.1145%2f1067915.1067921&partnerID=40&md5=8ee7b33873308ce9c735b7ea2f019c5c,"Energy consumption is a major concern in many embedded computing systems. Several studies have shown that cache memories account for about 50% of the total energy consumed in these systems. The performance of a given cache architecture is determined, to a large degree, by the behavior of the application executing on the architecture. Desktop systems have to accommodate a very wide range of applications and therefore the cache architecture is usually set by the manufacturer as a best compromise given current applications, technology, and cost. Unlike desktop systems, embedded systems are designed to run a small range of well-defined applications. In this context, a cache architecture that is tuned for that narrow range of applications can have both increased performance as well as lower energy consumption. We introduce a novel cache architecture intended for embedded microprocessor platforms. The cache has three software-configurable parameters that can be tuned to particular applications. First, the cache's associativity can be configured to be direct-mapped, two-way, or four-way set-associative, using a novel technique we call way concatenation. Second, the cache's total size can be configured by shutting down ways. Finally, the cache's line size can be configured to have 16, 32, or 64 bytes. A study of 23 programs drawn from Powerstone, MediaBench, and Spec2000 benchmark suites shows that the configurable cache tuned to each program saved energy for every program compared to a conventional four-way set-associative cache as well as compared to a conventional direct-mapped cache, with an average savings of energy related to memory access of over 40%. © 2005, ACM. All rights reserved.",architecture tuning; Cache; configurable; Design; embedded systems; low energy; low power; memory hierarchy; microprocessor; Performance,
Data Space-Oriented Tiling for Enhancing Locality,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015260319&doi=10.1145%2f1067915.1067922&partnerID=40&md5=2123608bf97df8473e6dbc8975d3f89c,"Improving locality of data references is becoming increasingly important due to increasing gap between processor cycle times and off-chip memory access latencies. Improving data locality not only improves effective memory access time but also reduces memory system energy consumption due to data references. An optimizing compiler can play an important role in enhancing data locality in array-intensive embedded media applications with regular data access patterns. This paper presents a compiler-based data space-oriented tiling approach (DST). In this strategy, the data space (e.g., an array of signals) is logically divided into chunks (called data tiles) and each data tile is processed in turn. In processing a data tile, our approach traverses the entire iteration space of all nests in the code and executes all iterations (potentially coming from different nests) that access the data tile being processed. In doing so, it also takes data dependences into account. Since a data space is common across all nests that access it, DST can potentially achieve better results than traditional iteration space (loop) tiling by exploiting internest data locality. We also present an example application of DST for improving the effectiveness of a scratch pad memory (SPM) for data accesses. SPMs are alternatives to conventional cache memories in embedded computing world. These small on-chip memories, like caches, provide fast and low-power access to data; but, they differ from conventional data caches in that their contents are managed by compiler instead of hardware. We have implemented DST in a source-to-source translator and quantified its benefits using a simulator. Our preliminary results with several array-intensive applications and varying input sizes show that our approach outperforms classical iteration spaceoriented tiling as well as a data-oriented approach that considers each nest in isolation. © 2005, ACM. All rights reserved.",arrayintensive applications; data locality; Design; iteration space tiling; Performance; scratch pad memory; Software compilation,
Verification of Safety Properties for Parameterized Regular Systems,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646436616&doi=10.1145%2f1067915.1067917&partnerID=40&md5=94d567b22182ab71c53ed11e3551855a,"We propose a combination of heuristic methods to prove properties of control signals for regular systems defined by means of affine recurrence equations (AREs). We benefit from the intrinsic regularity of the underlying polyhedral model to handle parameterized systems in a symbolic way. Our techniques apply to safety properties. The general proof process consists in an iteration that alternates two heuristics. We are able to identify the cases when this iteration will stop in a finite number of steps. These techniques have been implemented in a high level synthesis environment based on the polyhedral model. © 2005, ACM. All rights reserved.",codesign; Design; embedded systems; parameterized systems; Polyhedral model; regular systems; Verification,
Guest Editorial: Special Issue on Models and Methodologies for Co-Design of Embedded Systems,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889097687&doi=10.1145%2f1067915.1067916&partnerID=40&md5=d5387f6a24eea94bd2152144b2b1d42d,"This special issue is based on innovative ideas presented and discussed during the first ACM/IEEE Conference on Formal Methods and Models for Co-Design(MEMOCODE)held at Mont Saint Michel in France during the summer of 2003. Selected papers from the conference were invited for this special issue together with an open call for papers soliciting novel contributions on the topics of this conference. Rigorous reviews of 12 submissions led to the selection of four papers for this special issue. In this editorial statement, we outline the premise and the context of this special issue, and give a short introduction to the theme under consideration and briefly introduce the papers selected.We also thank the authors who submitted their contributions to this special issue, and all the reviewers without whose dedication and hard work toward ensuring the quality of the selections, editing this special issue would have been impossible. © 2005, ACM. All rights reserved.",,
Synthesis of Application-Specific Highly Efficient Multi-Mode Cores for Embedded Systems,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979977948&doi=10.1145%2f1053271.1053278&partnerID=40&md5=e854db31b4a395a97a56a98f63aef11e,"In this paper, we present a novel design methodology for synthesizing multiple configurations (or modes) into a single programmable core that can be used in embedded systems. Recent portable applications require reconfigurability of a system along with efficiency in terms of power, performance, and area. The field programmable gate arrays (FPGAs) provide a reconfigurable platform; however, they are slower in speed with significantly higher power and area than achievable by a customized application-specific integrated circuits (ASIC). Implementation of a system in either FPGA or ASIC represents a trade-off between programmability and design efficiency. In this work, we have developed techniques to realize efficient reconfigurable cores for a set of user-specified applications. The resultant system, named as multimode system, can easily switch configurations throughout the set of configurations it is designed for. A data flow graph transformation method coupled with efficient scheduling and allocation is used to automatically synthesize a Multi-Mode system from its behavior-level specifications. Experimental results on several applications demonstrate that our implementations can achieve about 60X power reduction on average and run 3.5X faster over corresponding FPGA implementations. © 2005, ACM. All rights reserved.",application specific integrated circuits (ASIC); Design; Digital signal processing (DSP); embedded systems; Experimentation; high level synthesis; Performance; reconfigurable system; Reliability; synthesis,
Dynamic Coalescing for 16-Bit Instructions,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-56849113035&doi=10.1145%2f1053271.1053273&partnerID=40&md5=022fe2b3c3d1b10fa49b9e901b5af0fd,"In the embedded domain, memory usage and energy consumption are critical constraints. Embedded processors such as the ARM and MIPS provide a 16-bit instruction set, (called Thumb in the case of the ARM family of processors), in addition to the 32-bit instruction set to address these concerns. Using 16-bit instructions one can achieve code size reduction and instruction cache energy savings at the cost of performance. This paper presents a novel approach that enhances the performance of 16-bit Thumb code. We have observed that throughout Thumb code there exist Thumb instruction pairs that are equivalent to a single ARM instruction. We have developed enhancements to the processor microarchitecture and the Thumb instruction set to exploit this property. We enhance the Thumb instruction set by incorporating Augmenting eXtensions (AX). A Thumb instruction pair that can be combined into a single ARM instruction is replaced by an AXThumb instruction pair by the compiler. The AX instruction is coalesced with the immediately following Thumb instruction to generate a single ARM instruction at decode time. The enhanced microarchitecture ensures that coalescing does not introduce pipeline delays or increase cycle time thereby resulting in reduction of both instruction counts and cycle counts. Using AX instructions and coalescing hardware we are also able to support efficient predicated execution in 16-bit mode. © 2005, ACM. All rights reserved.",16-bit Thumb ISA; 32-bit ARM ISA; Algorithms; AX instructions; code size; Embedded processor; energy; instruction coalescing; Measurement; Performance; performance,
SAFE-OPS: An Approach to Embedded Software Security,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015850662&doi=10.1145%2f1053271.1053279&partnerID=40&md5=17ea86cea86ea806bb25b5ca9dab82b8,"The new-found ubiquity of embedded processors in consumer and industrial applications brings with it an intensified focus on security, as a strong level of trust in the system software is crucial to their widespread deployment. The growing area of software protection attempts to address the key steps used by hackers in attacking a software system. In this paper, we introduce a unique approach to embedded software protection that utilizes a hardware/software codesign methodology. Results demonstrate that this framework can be the successful basis for the development of embedded applications that meet a wide range of security and performance requirements. © 2005, ACM. All rights reserved.",Design; HW/SW codesign; Measurement; Performance; Security; Software protection,
Schedulability-Driven Frame Packing for Multicluster Distributed Embedded Systems,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995274455&doi=10.1145%2f1053271.1053276&partnerID=40&md5=8943008718458b5fe7ccfb91a7c83378,"We present an approach to frame packing for multicluster distributed embedded systems consisting of time-triggered and event-triggered clusters, interconnected via gateways. In our approach, the application messages are packed into frames such that the application is schedulable, thus the end-to-end message communication constraints are satisfied. We have proposed a schedulability analysis for applications consisting of mixed event-triggered and time-triggered processes and messages, and a worst-case queuing delay analysis for the gateways, responsible for routing intercluster traffic. Optimization heuristics for frame packing aiming at producing a schedulable system have been proposed. Extensive experiments and a real-life example show the efficiency of our frame-packing approach. © 2005, ACM. All rights reserved.",Algorithms; Design; distributed embedded systems; Frame packing; Performance; schedulability analysis; Theory,
Memory Safety Without Garbage Collection for Embedded Applications,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149150286&doi=10.1145%2f1053271.1053275&partnerID=40&md5=ae54fefd0354afeebab4a71c4e4175d7,"Traditional approaches to enforcing memory safety of programs rely heavily on run-time checks of memory accesses and on garbage collection, both of which are unattractive for embedded applications. The goal of our work is to develop advanced compiler techniques for enforcing memory safety with minimal run-time overheads. In this paper, we describe a set of compiler techniques that, together with minor semantic restrictions on C programs and no new syntax, ensure memory safety and provide most of the error-detection capabilities of type-safe languages, without using garbage collection, and with no run-time software checks, (on systems with standard hardware support for memory management). The language permits arbitrary pointer-based data structures, explicit deallocation of dynamically allocated memory, and restricted array operations. One of the key results of this paper is a compiler technique that ensures that dereferencing dangling pointers to freed memory does not violate memory safety, without annotations, run-time checks, or garbage collection, and works for arbitrary type-safe C programs. Furthermore, we present a new interprocedural analysis for static array bounds checking under certain assumptions. For a diverse set of embedded C programs, we show that we are able to ensure memory safety of pointer and dynamic memory usage in all these programs with no run-time software checks (on systems with standard hardware memory protection), requiring only minor restructuring to conform to simple type restrictions. Static array bounds checking fails for roughly half the programs we study due to complex array references, and these are the only cases where explicit run-time software checks would be needed under our language and system assumptions. © 2005, ACM. All rights reserved.",automatic pool allocation; compilers; Embedded systems; Languages; programming languages; region management; Security; security; static analysis,
Optimal Voltage Allocation Techniques for Dynamically Variable Voltage Processors,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645222076&doi=10.1145%2f1053271.1053280&partnerID=40&md5=667ad65f13c641b61ddbcedf466cc099,"This paper presents important, new results of a study on the problem of task scheduling and voltage allocation in dynamically variable voltage processors, the purpose of which was minimization of processor energy consumption. The contributions are twofold: (1) For given multiple discrete supply voltages and tasks with arbitrary arrival-time/deadline constraints, we propose a voltage allocation technique that produces a feasible task schedule with optimal processor energy consumption. (2) We then extend the problem to include the case in which tasks have nonuniform loads (i.e.; switched) capacitances and solve it optimally. The proposed technique, called Alloc-vt, in (1) is based on the prior results in [Yao, Demers and Shenker. 1995. In Proceedings of IEEE Symposium on Foundations of Computer Science. 374-382] (which is optimal for dynamically continuously variable voltages, but not for discrete ones) and [Ishihara and Yasuura. 1998. In Proceedings of International Symposium on Low Power Electronics and Design. 197-202] (which is optimal for a single task, but not for multiple tasks), whereas the proposed technique, called Alloc-vtcap, in (2) is based on an efficient linear programming (LP) formulation. Both techniques solve the allocation problems optimally in polynomial time. © 2005, ACM. All rights reserved.",Algorithms; Design; Dynamic voltage scaling; low power design; Performance; scheduling; variable voltage processor,
Introduction to the Special Issue,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024249931&doi=10.1145%2f1053271.1053272&partnerID=40&md5=c6f7865773ea15bd6efbfce4ce6c8e4c,[No abstract available],,
The Implementation and Evaluation of Dynamic Code Decompression Using DISE,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651534522&doi=10.1145%2f1053271.1053274&partnerID=40&md5=5e2ac64aa9fdc0c23b44ffc452c2e037,"Code compression coupled with dynamic decompression is an important technique for both embedded and general-purpose microprocessors. Postfetch decompression, in which decompression is performed after the compressed instructions have been fetched, allows the instruction cache to store compressed code but requires a highly efficient decompression implementation. We propose implementing postfetch decompression using a new hardware facility called dynamic instruction stream editing (DISE). DISE provides a programmable decoder-similar in structure to those in many IA-32 processors-that is used to add functionality to an application by injecting custom code snippets into its fetched instruction stream.We present a DISE-based implementation of postfetch decompression and show that it naturally supports customized program-specific decompression dictionaries, enables parameterized decompression allowing similar-but-not-identical instruction sequences to share dictionary entries, and uses no decompression-specific hardware. We present extensive experimental results showing the virtue of this approach and evaluating the factors that impact its efficacy. We also present implementation-neutral results that give insight into the characteristics of any postfetch decompression technique. Our experiments not only demonstrate significant reduction in code size (up to 35%) but also significant improvements in performance (up to 20%) and energy (up to 10%). © 2005, ACM. All rights reserved.",Code compression; code decompression; Design; DISE; dynamic instruction stream editing; dynamic instrumentation; Experimentation; Performance,
"Pruning-Based, Energy-Optimal, Deterministic I/O Device Scheduling for Hard Real-Time Systems",2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-29344452797&doi=10.1145%2f1053271.1053277&partnerID=40&md5=af98fee544ee6c9387fe437930f14053,"Software-controlled (or dynamic) power management (DPM) in embedded systems has emerged as an attractive alternative to inflexible hardware solutions. However, DPM via I/O device scheduling for hard real-time systems has received relatively little attention. In this paper,we present an offline I/O device scheduling algorithm called energy-optimal device scheduler (EDS). For a given set of jobs, it determines the start time of each job such that the energy consumption of the I/O devices is minimized. EDS also ensures that no real-time constraint is violated. The device schedules are provably energy optimal under hard real-time job deadlines. Temporal and energy-based pruning are used to reduce the search space significantly. Since the I/O device scheduling problem is NPcomplete, we also describe a heuristic called maximum device overlap (MDO) to generate nearoptimal solutions in polynomial time.We present experimental results to show that EDS and MDO reduce the energy consumption of I/O devices significantly for hard real-time systems. © 2005, ACM. All rights reserved.",device scheduling; hard real-time systems; I/O devices; Performance; Schedulability analysis; Theory,
Energy Macromodeling of Embedded Operating Systems,2005,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011833865&doi=10.1145%2f1053271.1053281&partnerID=40&md5=26e792f7becc75c0ab962179c9c738f1,"As embedded systems get more complex, deployment of embedded operating systems (OSs) as software run-time engines has become common. In particular, this trend is true even for batterypowered embedded systems, where maximizing battery life is a primary concern. In such OS-driven embedded software, the overall energy consumption depends very much on which OS is used and how the OS is used. Therefore, the energy effects of the OS need to be studied in order to design low-energy systems effectively. In this paper, we discuss the motivation for performing OS energy characterization and propose a methodology to perform the characterization systematically. The methodology consists of two parts. The first part is analysis, which is concerned with identifying a set of components that can be used to characterize the OS energy consumption, called energy characteristics. The second part is macromodeling, which is concerned with obtaining quantitative macromodels for the energy characteristics. It involves the process of experiment design, data collection, and macromodel fitting. The OS energy macromodels can be used conveniently as OS energy estimators in high-level or architectural optimization of embedded systems for low-energy consumption. As far as we know, this work is the first attempt to systematically tackle energy macromodeling of an embedded OS. To demonstrate our approach, we present experimental results for two wellknown embedded OSs, namely, iC/OS and embedded Linux OS. © 2005, ACM. All rights reserved.",characterization; energy consumption; Linux; Measurement; Performance,
Towards Fault-Tolerant Cryptographic Computations over Finite Fields,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006520295&doi=10.1145%2f1015047.1015054&partnerID=40&md5=88dfe3981d4ed5c5a97496b8859009db,"Cryptographic schemes, such as authentication, confidentiality, and integrity, rely on computations in very large finite fields, whose hardware realization may require millions of logic gates. In a straightforward design, even a single fault in such a complex circuit is likely to yield an incorrect result and may be exploited by an attacker to break the cryptosystem. In this regard, we consider computing over finite fields in presence of certain faults in multiplier circuits. Our work reported here deals with errors caused by such faults in polynomial basis multipliers over finite fields of characteristic two and presents a scheme to correct single errors. Towards this, pertinent theoretical results are derived, and both bit-parallel and bit-serial fault tolerant multipliers are proposed. © 2004, ACM. All rights reserved.",Algorithms; Error correction; fault-tolerant computing; finite fields; polynomial basis multiplier; Security; security,
Statistics and Secret Leakage,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-24044554454&doi=10.1145%2f1015047.1015050&partnerID=40&md5=6c48d5032c1727f605a895d48d45e6bf,"In addition to its usual complexity assumptions, cryptography silently assumes that information can be physically protected in a single location. As one can easily imagine, real-life devices are not ideal and information may leak through different physical channels. This paper gives a rigorous definition of leakage immunity and presents several leakage detection tests. In these tests, failure confirms the probable existence of secret-correlated emanations and indicates how likely the leakage is. Success does not refute the existence of emanations but indicates that significant emanations were not detected on the strength of the evidence presented, which of course, leaves the door open to reconsider the situation if further evidence comes to hand at a later date. © 2004, ACM. All rights reserved.",Algorithms; Cryptography; Measurement; Security; side-channel analysis,
Efficient Digit-Serial Normal Basis Multipliers over Binary Extension Fields,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017122508&doi=10.1145%2f1015047.1015053&partnerID=40&md5=180403643618f3a65fedf8dd5fafd207,"In this article, two digit-serial architectures for normal basis multipliers over GF(2m) are presented. These two structures have the same gate count and gate delay. We also consider two special cases of optimal normal bases for the two digit-serial architectures. A straightforward implementation leaves gate redundancy in both of them. An algorithm that can considerably reduce the redundancy is also developed. The proposed architectures are compared with the existing ones in terms of gate and time complexities. © 2004, ACM. All rights reserved.",Algorithms; Digit-serial multiplier; finite field; normal basis; Security; security,
LiSP: A Lightweight Security Protocol for Wireless Sensor Networks,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016596484&doi=10.1145%2f1015047.1015056&partnerID=40&md5=10f66cc52370dcdb5d82d3c7079db176,"Small low-cost sensor devices with limited resources are being used widely to build a self-organizing wireless network for various applications, such as situation monitoring and asset surveillance. Making such a sensor network secure is crucial to their intended applications, yet challenging due to the severe resource constraints in each sensor device. We present a lightweight security protocol (LiSP) that makes a tradeoff between security and resource consumption via efficient rekeying. The heart of the protocol is the novel rekeying mechanism that offers (1) efficient key broadcast without requiring retransmission/ACKs, (2) authentication for each key-disclosure without incurring additional overhead, (3) the ability of detecting/recovering lost keys, (4) seamless key refreshment without disrupting ongoing data encryption/decryption, and (5) robustness to inter-node clock skews. Furthermore, these benefits are preserved in conventional contention-based medium access control protocols that do not support reliable broadcast. Our performance evaluation shows that LiSP reduces resource consumption significantly, while requiring only three hash computations, on average, and a storage space for eight keys. © 2004, ACM. All rights reserved.",Authentication; Design; key management; lightweight security; Security; sensor networks,
Security on FPGAs: State-of-the-Art Implementations and Attacks,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84999466301&doi=10.1145%2f1015047.1015052&partnerID=40&md5=a51f37053a81d9343b2e644662c40016,"In the last decade, it has become apparent that embedded systems are integral parts of our every day lives. The wireless nature of many embedded applications as well as their omnipresence has made the need for security and privacy preserving mechanisms particularly important. Thus, as field programmable gate arrays (FPGAs) become integral parts of embedded systems, it is imperative to consider their security as a whole. This contribution provides a state-of-the-art description of security issues on FPGAs, both from the system and implementation perspectives. We discuss the advantages of reconfigurable hardware for cryptographic applications, show potential security problems of FPGAs, and provide a list of open research problems. Moreover, we summarize both public and symmetric-key algorithm implementations on FPGAs. © 2004, ACM. All rights reserved.",Algorithms; attacks; cryptographic applications; Cryptography; Design; FPGA; Performance; reconfigurable hardware; reverse engineering; Security; security,
Security in Embedded Systems: Design Challenges,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014332650&doi=10.1145%2f1015047.1015049&partnerID=40&md5=6e1a09e5ec34719b0c432a59e45efcd2,"Many modern electronic systems—including personal computers, PDAs, cell phones, network routers, smart cards, and networked sensors to name a few—need to access, store, manipulate, or communicate sensitive information, making security a serious concern in their design. Embedded systems, which account for a wide range of products from the electronics, semiconductor, telecommunications, and networking industries, face some of the most demanding security concerns—on the one hand, they are often highly resource constrained, while on the other hand, they frequently need to operate in physically insecure environments. Security has been the subject of intensive research in the context of general-purpose computing and communications systems. However, security is often misconstrued by embedded system designers as the addition of features, such as specific cryptographic algorithms and security protocols, to the system. In reality, it is a new dimension that designers should consider throughout the design process, along with other metrics such as cost, performance, and power. The challenges unique to embedded systems require new approaches to security covering all aspects of embedded system design from architecture to implementation. Security processing, which refers to the computations that must be performed in a system for the purpose of security, can easily overwhelm the computational capabilities of processors in both low-and high-end embedded systems. This challenge, which we refer to as the “security processing gap,” is compounded by increases in the amounts of data manipulated and the data rates that need to be achieved. Equally daunting is the “battery gap” in battery-powered embedded systems, which is caused by the disparity between rapidly increasing energy requirements for secure operation and slow improvements in battery technology. The final challenge is the “assurance gap,” which relates to the gap between functional security measures (e.g., security services, protocols, and their constituent cryptographic algorithms) and actual secure implementations. This paper provides an introduction to the challenges involved in secure embedded system design, discusses recent advances in addressing them, and identifies opportunities for future research. © 2004, ACM. All rights reserved.",architecture; authentication; battery life; cryptographic algorithms; decryption; Design; Embedded systems; encryption; hardware design; processing requirements; Security; security; security attacks; security protocols; tamper resistance,
A Fast String-Matching Algorithm for Network Processor-Based Intrusion Detection System,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79551630961&doi=10.1145%2f1015047.1015055&partnerID=40&md5=3c66c831f6a87a6e73ccfdef8438c6ba,"Network intrusion detection systems (NIDSs) are one of the latest developments in security. The matching of packet strings against collected signatures dominates signature-based NIDS performance. Network processors are also one of the fastest growing segments of the semiconductor market, because they are designed to provide scalable and flexible solutions that can accommodate change quickly and economically. This work presents a fast string-matching algorithm (called FNP) over the network processor platform that conducts matching sets of patterns in parallel. This design also supports numerous practical features such as case-sensitive string matching, signature prioritization, and multiple-content signatures. This efficient multiple-pattern matching algorithm utilizes the hardware facilities provided by typical network processors instead of employing the external lookup co-processors. To verify the efficiency and practicability of the proposed algorithm, it was implemented on the Vitesse IQ2000 network processor platform. The searching patterns used in the present experiments are derived from the well-known Snort ruleset cited by most opensource and commercial NIDSs. This work shows that combining our string-matching methodology, hashing engine supported by most network processors, and characteristics of current Snort signatures frequently improves performance and reduces number of memory accesses compared to conventional string-matching algorithms. Another contribution of this work is to highlight that, besides total number of searching patterns, shortest pattern length is also a major influence on NIDS multipattern matching algorithm performance. © 2004, ACM. All rights reserved.",Algorithms; Design; Intrusion detection; network processor; pattern matching; Performance,
Elliptic and Hyperelliptic Curves on Embedded μP,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015880452&doi=10.1145%2f1015047.1015051&partnerID=40&md5=c813d810c99bf0b48862d7aada82ba4b,"It is widely recognized that data security will play a central role in future IT systems. Providing public-key cryptographic primitives, which are the core tools for security, is often difficult on embedded processor due to computational, memory, and power constraints. This contribution appears to be the first thorough comparison of two public-key families, namely elliptic curve (ECC) and hyperelliptic curve cryptosystems on a wide range of embedded processor types (ARM, ColdFire, PowerPC). We investigated the influence of the processor type, resources, and architecture regarding throughput. Further, we improved previously known HECC algorithms resulting in a more efficient arithmetic. © 2004, ACM. All rights reserved.",Algorithms; Design; Elliptic curves cryptosystem; hyperelliptic curve cryptosystem; implementation; Performance; Security,
Blocking-Aware Processor Voltage Scheduling for Real-Time Tasks,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997196860&doi=10.1145%2f993396.993401&partnerID=40&md5=13878dc343baab80f2443b2261b647eb,"As mobile computing is getting popular, there is a growing need for techniques that minimize energy consumption on battery-powered mobile devices. Processor voltage scheduling can effectively reduce processor energy consumption by lowering the processor speed. In this paper, we study voltage scheduling for real-time periodic tasks with non-preemptible sections. Three schemes are proposed: The static speed algorithm derives the minimum static feasible speed based on the stack resource policy. Due to blocking, this static speed is usually higher than the speed required for scheduling fully preemptible tasks (called the utilization speed). Two dynamic speed algorithms are then introduced to further reduce energy consumption. The novel dual speed algorithm operates the processor at the utilization speed whenever possible and switches to the higher static speed only when blocking occurs. The dual speed dynamic reclaiming algorithm reserves time budget for each job, reclaims the unused time budget from completed jobs and redistributes it to subsequent jobs so they can run at a lower speed whenever possible. Feasibility conditions for real-time task sets have been derived and proved mathematically. Simulation results show that the proposed voltage scheduling algorithms dramatically reduce processor energy consumption over non-power-aware scheduling algorithms. Furthermore, the two dynamic speed algorithms consistently outperform the static speed scheme in a wide range of system and workload conditions. © 2004, ACM. All rights reserved.",Algorithms; Design; Dynamic power management; non-preemptible sections; poweraware scheduling; real-time systems,
The Design of Dynamically Reconfigurable Datapath Coprocessors,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008021361&doi=10.1145%2f993396.993403&partnerID=40&md5=8e821d5eed741394aa3a68659bedf167,"Increasing nonrecurring engineering and mask costs are making it harder to turn to hardwired application specific integrated circuit (ASIC) solutions for high-performance applications. The volume required to amortize these high costs has been increasing, making it increasingly expensive to afford ASIC solutions for medium-volume products. This has led to designers seeking programmable solutions of varying sorts using these so-called programmable platforms. These programmable platforms span a large range from bit-level programmable field programmable gate arrays to word-level programmable application-specific, and in some cases even general-purpose processors. The programmability comes with a power and performance overhead. Attempts to reduce this overhead typically involve making some core hardwired ASIC like logic blocks accessible to the programmable elements. This paper presents one such hybrid solution in this space-a relatively simple processor with a dynamically reconfigurable datapath acting as an accelerating coprocessor. This datapath consists of hardwired function units and reconfigurable interconnect.We present a methodology for the design of these solutions and illustrate it with two complete case studies: an MPEG2 coder, and a GSM coder, to show how significant speedups can be obtained using relatively little hardware. This work is part of the MESCAL project, which is geared towards developing design environments for the development of application-specific platforms. © 2004, ACM. All rights reserved.",Algorithm; coarse-grain reconfigurable fabric; datapath synthesis; Design; interconnection design; Loop pipelining; Performance; reconfigurable datapath,
Editorial Special Issue on Dynamically Adaptable Embedded Systems,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024282206&doi=10.1145%2f993396.993397&partnerID=40&md5=70afac09a4721a2105e26e460bf78fd2,[No abstract available],,
Adaptive Scheduling Server for Power-Aware Real-Time Tasks,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019535523&doi=10.1145%2f993396.993400&partnerID=40&md5=7e75a748f34018ecfb6a2b84f6f80dae,"In this paper, we propose a novel scheduling framework for a dynamic real-time environment with energy constraints. This framework dynamically adjusts the CPU voltage/frequency so that no task in the system misses its deadline and the total energy savings of the system are maximized. In this paper, we consider only realistic, discrete-level speeds. Each task in the system consumes a certain amount of energy, which depends on a speed chosen for execution. The process of selecting speeds for execution while maximizing the energy savings of the system requires the exploration of a large number of combinations, which is too time consuming to be computed online. Thus, we propose an integrated heuristic methodology, which executes an optimization procedure in a low computation time. This scheme allows the scheduler to handle power-aware real-time tasks with low cost while maximizing the use of the available resources and without jeopardizing the temporal constraints of the system. Simulation results show that our heuristic methodology is able to generate power-aware scheduling solutions with near-optimal performance. © 2004, ACM. All rights reserved.",Algorithms; Heuristics; real-time scheduling; variable voltage scheduling,
A Design Flow for Partially Reconfigurable Hardware,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-26444544394&doi=10.1145%2f993396.993399&partnerID=40&md5=2d952bad8d83d293c5928c6fe3476763,"This paper presents a top-down designer-driven design flow for creating hardware that exploits partial run-time reconfiguration. Computer-aided design (CAD) tools are presented, which complement conventional FPGA design environments to enable the specification, simulation (both functional and timing), synthesis, automatic placement and routing, partial configuration generation and control of partially reconfigurable designs. Collectively these tools constitute the dynamic circuit switching CAD framework. A partially reconfigurable Viterbi decoder design is presented to demonstrate the design flow and illustrate possible power consumption reductions and performance improvements through the exploitation of partial reconfiguration. © 2004, ACM. All rights reserved.",configuration control; Design; dynamically reconfigurable logic (DRL); FPGA; Performance; power estimation; run-time reconfiguration (RTR); Verification; Viterbi decoder,
AIDA: Adaptive Application-Independent Data Aggregation in Wireless Sensor Networks,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862131680&doi=10.1145%2f993396.993406&partnerID=40&md5=214f769a395256468ef6746605354750,"Sensor networks, a novel paradigm in distributed wireless communication technology, have been proposed for various applications including military surveillance and environmental monitoring. These systems deploy heterogeneous collections of sensors capable of observing and reporting on various dynamic properties of their surroundings in a time sensitive manner. Such systems suffer bandwidth, energy, and throughput constraints that limit the quantity of information transferred from end-to-end. These factors coupled with unpredictable traffic patterns and dynamic network topologies make the task of designing optimal protocols for such networks difficult. Mechanisms to perform data-centric aggregation utilizing application-specific knowledge provide a means to augmenting throughput, but have limitations due to their lack of adaptation and reliance on application-specific decisions. We, therefore, propose a novel aggregation scheme that adaptively performs application-independent data aggregation in a time sensitive manner. Our work isolates aggregation decisions into a module that resides between the network and the data-link layer and does not require any modifications to the currently existing MAC and network layer protocols. We take advantage of queuing delay and the broadcast nature of wireless communication to concatenate network units into an aggregate using a novel adaptive feedback scheme to schedule the delivery of this aggregate to the MAC layer for transmission. In our evaluation we show that endto-end transmission delay is reduced by as much as 80% under heavy traffic loads. Additionally, we show as much as a 50% reduction in transmission energy consumption with an overall reduction in header overhead. Theoretical analysis, simulation, and a test-bed implementation on Berkeley's MICA motes are provided to validate our claims. © 2004, ACM. All rights reserved.",adaptive algorithms; Algorithms; congestion control; Data aggregation; Design; energy conservation; feedback control; Performance; sensor networks,
A Self-Tuning Cache Architecture for Embedded Systems,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024279046&doi=10.1145%2f993396.993405&partnerID=40&md5=d7632b06d951fd3252bb7a188cf6e54f,"Memory accesses often account for about half of a microprocessor system's power consumption. Customizing a microprocessor cache's total size, line size, and associativity to a particular program is well known to have tremendous benefits for performance and power. Customizing caches has until recently been restricted to core-based flows, in which a new chip will be fabricated. However, several configurable cache architectures have been proposed recently for use in prefabricated microprocessor platforms. Tuning those caches to a program is still, however, a cumbersome task left for designers, assisted in part by recent computer-aided design (CAD) tuning aids. We propose to move that CAD on-chip, which can greatly increase the acceptance of tunable caches.We introduce on-chip hardware implementing an efficient cache tuning heuristic that can automatically, transparently, and dynamically tune the cache to an executing program. Our heuristic seeks not only to reduce the number of configurations that must be examined, but also traverses the search space in a way that minimizes costly cache flushes. By simulating numerous Powerstone and MediaBench benchmarks, we show that such a dynamic self-tuning cache saves on average 40% of total memory access energy over a standard nontuned reference cache. © 2004, ACM. All rights reserved.",architecture tuning; Cache; configurable; Design; dynamic optimization; embedded systems; Experimentation; low energy; low power; on-chip CAD; Performance,
An Optimal Algorithm for Minimizing Run-Time Reconfiguration Delay,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-26444563534&doi=10.1145%2f993396.993398&partnerID=40&md5=575932d7ec50362bb2e953c468e6b5b5,"Reconfiguration delay is one of the major barriers in the way of dynamically adapting a system to its application requirements. The run-time reconfiguration delay is quite comparable to the application latency for many classes of applications and might even dominate the application run-time. In this paper, we present an efficient optimal algorithm for minimizing the run-time reconfiguration (context switching) delay of executing an application on a dynamically adaptable system. The system is composed of a number of cameras with embedded reconfigurable resources collaborating in order to track an object. The operations required to execute in order to track the object are revealed to the system at run-time and can change according to a number of parameters, such as the target shape and proximity. Similarly, we can assume that the applications comprising tasks are already scheduled and each of them has to be realized on the reconfigurable fabric in order to be executed. The modeling and the algorithm are both applicable to partially reconfigurable platforms as well as multi-FPGA systems. The algorithm can be directly applied to minimize the application runtime for the typical classes of applications, where the actual execution delay of the basic operations is negligible compared to the reconfiguration delay. We prove the optimality and the efficiency of our algorithm. We report the experimental results, which demonstrate a 2.5-40% improvement on the total run-time reconfiguration delay as compared to other heuristics. © 2004, ACM. All rights reserved.",Algorithms; instantiation ordering; Performance; Reconfigurable computing; reconfiguration delay,
Dynamic Adaptation for Fault Tolerance and Power Management in Embedded Real-Time Systems,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013138265&doi=10.1145%2f993396.993402&partnerID=40&md5=044877926b72189994b2997a06893c74,"Safety-critical embedded systems often operate in harsh environmental conditions that necessitate fault-tolerant computing techniques. In addition, many safety-critical systems execute real-time applications that require strict adherence to task deadlines. These embedded systems are also energy-constrained, since system lifetime is determined largely by the battery lifetime. In this paper, we investigate dynamic adaptation techniques based on checkpointing and dynamic voltage scaling (DVS) for fault tolerance and power management. We first present schedulability tests that provide the criteria under which checkpointing can provide fault tolerance and real-time guarantees.We then present an adaptive checkpointing scheme in which the checkpointing interval for a task is dynamically adjusted during execution, and checkpoints are inserted based not only on the available slack, but also on the occurrences of faults. Next, we combine adaptive checkpointing with DVS to achieve power reduction. Finally, we develop an adaptive checkpointing scheme for a set of multiple tasks in real-time systems. An offline preprocessing based on linear programming is used to determine the parameters that are provided as inputs to the online adaptive checkpointing procedure. Simulation results show that compared to previous methods, the proposed adaptive checkpointing approach increases the likelihood of timely task completion in the presence of faults. When combined with DVS, adaptive checkpointing also leads to considerable energy savings. © 2004, ACM. All rights reserved.",Algorithm; Checkpointing; dynamic voltage scaling; Performance,
Multitasking on Reconfigurable Architectures: Microarchitecture Support and Dynamic Scheduling,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749549866&doi=10.1145%2f993396.993404&partnerID=40&md5=d3d673326c29f4be95038c6fe2ede4de,"Dynamic scheduling for system-on-chip (SoC) platforms has become an important field of research due to the emerging range of applications with dynamic behavior (e.g., MPEG-4). Dynamically reconfigurable architectures are an interesting solution for this type of applications. Scheduling for dynamically reconfigurable architectures might be classified in two major broad categories: (1) static scheduling techniques or (2) use of an operating system (OS) for reconfigurable computing. However, research efforts demonstrate a trend to move tasks traditionally assigned to the OS into hardware (thus increasing performance and reducing power). In this paper, we introduce a methodology for dynamically reconfigurable architectures. The dynamic scheduling of tasks to several reconfigurable units is performed by a hardware-based multitasking support unit. Two different versions of the microarchitecture are possible (with or without a hardware configuration prefetch unit). The dynamic scheduling algorithms are also explained. Both algorithms try to minimize the reconfiguration overhead by overlapping the execution of tasks with device reconfigurations. An exhaustive study (using the developed simulation and performance analysis framework) of this novel proposal is presented, and the effect of the microarchitecture parameters has been studied. Results demonstrate the benefits of our approach (achieving similar performance to a static configuration solution but using half of the resources). The hardware configuration prefetch unit is useful (i.e., minimize the execution time) in applications with low level of parallelism. © 2004, ACM. All rights reserved.",Adaptable architectures and microarchitectures; Algorithms; Design; dynamic scheduling; Performance; runtime support for dynamic reconfiguration,
Design of Secure Cryptography Against the Threat of Power-Attacks in DSP-Embedded Processors,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-27944472807&doi=10.1145%2f972627.972632&partnerID=40&md5=f1ca2bfa82d9c388420f0e64ee339edf,"Embedded wireless devices require secure high-performance cryptography in addition to low-cost and low-energy dissipation. This paper presents for the first time a design methodology for security on a VLIW complex DSP-embedded processor core. Elliptic curve cryptography is used to demonstrate the design for security methodology. Results are verified with real dynamic power measurements and show that compared to previous research a 79% improvement in performance is achieved. Modification of power traces are performed to resist simple power analysis attack with up to 39% overhead in performance, up to 49% overheads in energy dissipation, and up to 11% overhead in code size. Simple power analysis on the VLIW DSP core is shown to be more correlated to routine ordering than individual instructions. For the first time, differential power analysis results on a VLIW using real power measurements are presented. Results show that the processor instruction level parallelism and large bus size contribute in making differential power analysis attacks extremely difficult. This research is important for industry since efficient yet secure cryptography is crucial for wireless communication devices. © 2004, ACM. All rights reserved.",Design; Microprocessor/Microcomputer Application; Performance; Security; Smartcards; VLIW,
Sensor Deployment and Target Localization in Distributed Sensor Networks,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925153243&doi=10.1145%2f972627.972631&partnerID=40&md5=fb27fdf3fd088f73578f43a0491290b7,"The effectiveness of cluster-based distributed sensor networks depends to a large extent on the coverage provided by the sensor deployment. We propose a virtual force algorithm (VFA) as a sensor deployment strategy to enhance the coverage after an initial random placement of sensors. For a given number of sensors, the VFA algorithm attempts to maximize the sensor field coverage. A judicious combination of attractive and repulsive forces is used to determine the new sensor locations that improve the coverage. Once the effective sensor positions are identified, a one-time movement with energy consideration incorporated is carried out, that is, the sensors are redeployed, to these positions. We also propose a novel probabilistic target localization algorithm that is executed by the cluster head. The localization results are used by the cluster head to query only a few sensors (out of those that report the presence of a target) for more detailed information. Simulation results are presented to demonstrate the effectiveness of the proposed approach. © 2004, ACM. All rights reserved.",Algorithms; cluster head; Cluster-based sensor networks; Management; Performance; sensor field coverage; sensor placement; virtual force,
Compiling with Code-Size Constraints,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-28344436739&doi=10.1145%2f972627.972635&partnerID=40&md5=edd09cd0c44032ec2503ba13ffd9fc3d,"Most compilers ignore the problems of limited code space in embedded systems. Designers of embedded software often have no better alternative than to manually reduce the size of the source code or even the compiled code. Besides being tedious and error prone, such optimization results in obfuscated code that is difficult to maintain and reuse. In this paper, we present a step towards code-size-aware compilation. We phrase register allocation and code generation as an integer linear programming problem where the upper bound on the code size can simply be expressed as an additional constraint. The resulting compiler, when applied to six commercial microcontroller programs, generates code nearly as compact as carefully crafted code. © 2004, ACM. All rights reserved.",Algorithms; Banked architecture; integer linear programming; Measurement; Performance; register allocation; space optimization,
Energy Efficient Wireless Packet Scheduling and Fair Queuing,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010359614&doi=10.1145%2f972627.972629&partnerID=40&md5=fb7f8dfb9ee8526143d44c0635141dc6,"As embedded systems are being networked, often wirelessly, an increasingly larger share of their total energy budget is due to the communication. This necessitates the development of power management techniques that address communication subsystems, such as radios, as opposed to computation subsystems, such as embedded processors, to which most of the research effort thus far has been devoted. In this paper, we present techniques for energy efficient packet scheduling and fair queuing in wireless communication systems. Our techniques are based on an extensive slack management approach that dynamically adapts the output rate of the system in accordance with the input packet arrival rate. We use a recently proposed radio power management technique, dynamic modulation scaling (DMS), as a control knob to enable energy-latency trade-offs during wireless packet transmission. We first analyze a single input stream scenario, and describe a rate adaptation technique that results in significantly lower energy consumption (reductions of up to 10×), while still bounding the resulting packet delays. By appropriately setting the various parameters of our algorithm, the system can be made to traverse the energy-latencyfidelity trade-off space. We extend our techniques to a multiple input stream scenario, and present E2WFQ, an energy efficient version of the weighted fair queuing (WFQ) algorithm for fair packet scheduling. Simulation results show that large energy savings can be obtained through the use of E2WFQ, with only a small, bounded increase in worst case packet latency. Further, our results demonstrate that E2WFQ does not adversely affect the throughput allocation (and hence, fairness) of WFQ. © 2004, ACM. All rights reserved.",Algorithms; Design; Energy efficient design; fair scheduling; Performance; power management; wireless communication,
Dynamic Voltage Scheduling with Buffers in Low-Power Multimedia Applications,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994656775&doi=10.1145%2f1027794.1027796&partnerID=40&md5=62837f6b146f9a0b35c9f4f6c2ce15f6,"Power-efficient design of multimedia applications becomes more important as they are used increasingly in many embedded systems. We propose a simple dynamic voltage scheduling (DVS) technique, which suits multimedia applications well and, in case of soft real-time applications, allows all idle intervals of the processor to be fully exploited by using buffers. Our main theme is to determine the minimum buffer size to maximize energy saving in three cases: (i) single task, (ii) multiple subtask, and (iii) multitask. We also present a technique of adjusting task deadlines for further reducing energy consumption in the multiple-subtask and multitask cases. Unlike other DVS techniques using buffers, we guarantee to meet the real-time latency constraint. Experimental results show that the proposed technique does indeed achieve significant power reduction in real-world multimedia applications. © 2004, ACM. All rights reserved.",Algorithms; Buffer requirement estimation; dynamic voltage scheduling; low-power systems; multimedia applications; Performance,
Schedulability Analysis of Applications with Stochastic Task Execution Times,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862915857&doi=10.1145%2f1027794.1027797&partnerID=40&md5=8a692665285a783cec28153da59f4fdf,"In the past decade, the limitations of models considering fixed (worst-case) task execution times have been acknowledged for large application classes within soft real-time systems. A more realistic model considers the tasks having varying execution times with given probability distributions. Considering such a model with specified task execution time probability distribution functions, an important performance indicator of the system is the expected deadline miss ratio of the tasks and of the task graphs. This article presents an approach for obtaining this indicator in an analytic way. Our goal is to keep the analysis cost low, in terms of required analysis time and memory, while considering as general classes of target application models as possible. The following main assumptions have been made on the applications that are modeled as sets of task graphs: the tasks are periodic, the task execution times have given generalized probability distribution functions, the task execution deadlines are given and arbitrary, the scheduling policy can belong to practically any class of non-preemptive scheduling policies, and a designer supplied maximum number of concurrent instantiations of the same task graph is tolerated in the system. Experiments show the efficiency of the proposed technique for monoprocessor systems. © 2004, ACM. All rights reserved.",Performance; Schedulability analysis; soft real-time systems; stochastic task execution times; Theory,
Self-Configuring Localization Systems: Design and Experimental Evaluation,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012100118&doi=10.1145%2f972627.972630&partnerID=40&md5=cb05a393613121862190cbc83418746b,"Embedded networked sensors promise to revolutionize the way we interact with our physical environment and require scalable, ad hoc deployable and energy-efficient node localization/positioning. This paper describes the motivation, design, implementation, and experimental evaluation (on sharply resource-constrained devices) of a self-configuring localization system using radio beacons. We identify beacon density as an important parameter in determining localization quality, which saturates at a transition density. We develop algorithms to improve localization quality by (i) automating placement of new beacons at low densities (HEAP) and (ii) rotating functionality among redundant beacons while increasing system lifetime at high densities (STROBE). © 2004, ACM. All rights reserved.",Algorithms; Design; Experimentation; localization; Location; Measurement; Performance; Reliability; self-configuration; sensor networks,
Real-Time Garbage Collection for Flash-Memory Storage Systems of Real-Time Embedded Systems,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905495271&doi=10.1145%2f1027794.1027801&partnerID=40&md5=6473746486814525b63cb537fa15d8f0,"Flash-memory technology is becoming critical in building embedded systems applications because of its shock-resistant, power economic, and nonvolatile nature. With the recent technology breakthroughs in both capacity and reliability, flash-memory storage systems are now very popular in many types of embedded systems. However, because flash memory is a write-once and bulk-erase medium, we need a translation layer and a garbage-collection mechanism to provide applications a transparent storage service. In the past work, various techniques were introduced to improve the garbage-collection mechanism. These techniques aimed at both performance and endurance issues, but they all failed in providing applications a guaranteed performance. In this paper, we propose a real-time garbage-collection mechanism, which provides a guaranteed performance, for hard real-time systems. On the other hand, the proposed mechanism supports non-real-time tasks so that the potential bandwidth of the storage system can be fully utilized. A wear-leveling method, which is executed as a non-real-time service, is presented to resolve the endurance problem of flash memory. The capability of the proposed mechanism is demonstrated by a series of experiments over our system prototype. © 2004, ACM. All rights reserved.",Algorithms; Design; Embedded systems; flash memory; garbage collection; real-time system; storage systems,
Iterative Schedule Optimization for Voltage Scalable Distributed Embedded Systems,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013625412&doi=10.1145%2f972627.972636&partnerID=40&md5=587fc45835c7d846dffc6af7d80cd174,"We present an iterative schedule optimization for multirate system specifications, mapped onto heterogeneous distributed architectures containing dynamic voltage scalable processing elements (DVS-PEs). To achieve a high degree of energy reduction, we formulate a generalized DVS problem, taking into account the power variations among the executing tasks. An efficient heuristic is presented that identifies optimized supply voltages by not only “simply” exploiting slack time, but under the additional consideration of the power profiles. Thereby, this algorithm minimizes the energy dissipation of heterogeneous architectures, including power-managed processing elements, effectively. Further, we address the simultaneous schedule optimization toward timing behavior and DVS utilization by integrating the proposed DVS heuristic into a genetic list scheduling approach. We investigate and analyze the possible energy reduction at both steps of the co-synthesis (voltage scaling and scheduling), including the power variations effects. Extensive experiments indicate that the presented work produces solutions with high quality. © 2004, ACM. All rights reserved.",Algorithms; Design; Dynamic voltage scaling; embedded systems; energy minimization; heterogeneous distributed systems; Optimization; scheduling; system synthesis,
Information Flow in Hybrid Systems,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992374077&doi=10.1145%2f1027794.1027799&partnerID=40&md5=69ce67f7a012c3cba2b00c4b4304179f,"Our aim is to study the information flow problem in hybrid systems, namely systems consisting of a discrete program with an analog environment. Information flows compromise the security of a system because they cause leakage of protected information. In order to tackle information flow in real-life systems, we introduce new classes of hybrid systems that extend the known ones while preserving their properties. Then, we define a logic to specify information flow. By means of some examples, we show that, by this logic, we are able to express information flows in hybrid systems and to certify that some suspect behaviors of these systems do not give rise to any information flow. We give a model checking procedure for our logic, and we prove that it gives a correct answer whenever it terminates. Moreover, for a particular class of hybrid systems, we give a version of the procedure that always terminates. © 2004, ACM. All rights reserved.",Design; Hybrid systems; information flow; Security; Verification,
Processor-Memory Coexploration Using an Architecture Description Language,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013460366&doi=10.1145%2f972627.972634&partnerID=40&md5=1b8fa8bcb209e839a5f1da440ba2f62d,"Memory represents a major bottleneck in modern embedded systems in terms of cost, power, and performance. Traditionally, memory organizations for programmable embedded systems assume a fixed cache hierarchy. With the widening processor-memory gap, more aggressive memory technologies and organizations have appeared, allowing customization of a heterogeneous memory architecture tuned for specific target applications. However, such a processor-memory coexploration approach critically needs the ability to explicitly capture heterogeneous memory architectures. We present in this paper a language-based approach to explicitly capture the memory subsystem configuration, generate a memory-aware software toolkit, and perform coexploration of the processor-memory architectures. We present a set of experiments using our memory-aware architectural description language (ADL) to drive the exploration of the memory subsystem for the TI C6211 processor architecture, demonstrating cost, performance, and energy trade-offs. © 2004, ACM. All rights reserved.",architecture description language; Design; design space exploration; Experimentation; Languages; memory exploration; Processor-memory codesign,
Guest Editorial: Special Issue on Networked Embedded Systems,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024282267&doi=10.1145%2f972627.972628&partnerID=40&md5=6ef508b39439cf3a3a4da5281e0af238,[No abstract available],,
Energy Savings and Speedups from Partitioning Critical Software Loops toHardware in Embedded Systems,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013441915&doi=10.1145%2f972627.972637&partnerID=40&md5=4e89e4ffb55ebe2b31625d917acf0754,"We present results of extensive hardware/software partitioning experiments on numerous benchmarks. We describe our loop-oriented partitioning methodology for moving critical code from hardware to software. Our benchmarks included programs from PowerStone, MediaBench, and Net-Bench. Our experiments included estimated results for partitioning using an 8051 8-bit microcontroller or a 32-bit MIPS microprocessor for the software, and using on-chip configurable logic or custom application-specific integrated circuit hardware for the hardware. Additional experiments involved actual measurements taken from several physical implementations of hardware/software partitionings on real single-chip microprocessor/configurable-logic devices. We also estimated results assuming voltage scalable processors. We provide performance, energy, and size data for all of the experiments.We found that the benchmarks spent an average of 80% of their execution time in only 3% of their code, amounting to only about 200 bytes of critical code. For various experiments, we found that moving critical code to hardware resulted in average speedups of 3 to 5 and average energy savings of 35% to 70%, with average hardware requirements of only 5000 to 10,000 gates. To our knowledge, these experiments represent the most comprehensive hardware/software partitioning study published to date. © 2004, ACM. All rights reserved.",Design; embedded systems; FPGA; Hardware/software partitioning; low energy; Performance; platforms; speedup; synthesis,
Multilevel μTESLA: Broadcast Authentication for Distributed Sensor Networks,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000276942&doi=10.1145%2f1027794.1027800&partnerID=40&md5=a288bd4e1992ef5755438b3e8b64350e,"Broadcast authentication is a fundamental security service in distributed sensor networks. This paper presents the development of a scalable broadcast authentication scheme named multilevel μTESLA based on μTESLA, a broadcast authentication protocol whose scalability is limited by its unicast-based initial parameter distribution. Multilevel μTESLA satisfies several nice properties, including low overhead, tolerance of message loss, scalability to large networks, and resistance to replay attacks as well as denial-of-service attacks. This paper also presents the experimental results obtained through simulation, which demonstrate the performance of the proposed scheme under severe denial-of-service attacks and poor channel quality. © 2004, ACM. All rights reserved.",Broadcast authentication; Design; Security; sensor networks; TESLA,
Modeling and Validation of Pipeline Specifications,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013378522&doi=10.1145%2f972627.972633&partnerID=40&md5=b0771e668f5c508a8b0c915cd0d4ba45,"Verification is one of the most complex and expensive tasks in the current Systems-on-Chip design process. Many existing approaches employ a bottom-up approach to pipeline validation, where the functionality of an existing pipelined processor is, in essence, reverse-engineered from its RT-level implementation. Our validation technique is complementary to these bottom-up approaches. Our approach leverages the system architect's knowledge about the behavior of the pipelined architecture, through architecture description language (ADL) constructs, and thus allows a powerful top-down approach to pipeline validation. The most important requirement in top-down validation process is to ensure that the specification (reference model) is golden. This paper addresses automatic validation of processor, memory, and coprocessor pipelines described in an ADL. We present a graph-based modeling that captures both structure and behavior of the architecture. Based on this model, we present algorithms to ensure that the static behavior of the pipeline is well formed by analyzing the structural aspects of the specification.We applied our methodology to verify specification of several realistic architectures from different architectural domains to demonstrate the usefulness of our approach. © 2004, ACM. All rights reserved.",Algorithms; architecture description language; Design; Languages; Modeling of processor pipeline; pipeline validation; pipelined processor specification; Verification,
Code Size Reduction Technique and Implementation for Software-Pipelined DSP Applications,2003,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008028251&doi=10.1145%2f950162.950168&partnerID=40&md5=a3f0a5163668263d7de1ff768da66036,"Software pipelining technique is extensively used to exploit instruction-level parallelism of loops, but also significantly expands the code size. For embedded systems with very limited on-chip memory resources, code size becomes one of the most important optimization concerns. This paper presents the theoretical foundation of code size reduction for software-pipelined loops based on retiming concept. We propose a general Code-size REDuction technique (CRED) for various kinds of processors. Our CRED algorithms integrate the code size reduction with software pipelining. The experimental results show the effectiveness of the CRED technique on both code size reduction and code size/performance trade-off space exploration. © 2003, ACM. All rights reserved.",Algorithms; Design; DSP processors; Retiming; scheduling; software pipelining,
Speculating to Reduce Unnecessary Power Consumption,2003,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024266150&doi=10.1145%2f950162.950165&partnerID=40&md5=10789d62734b77524054b9130401c7ea,"The power consumption of current processors keeps increasing in spite of aggressive circuit design techniques and process shrinks. One of the reasons for this increase is the complexity of the microarchitecture required to achieve the performance that each processor generation demands. These techniques, such as branch prediction and on-chip level two caches, increase not only the power consumption of the committed instructions, but also the useless power associated with those block accesses that generate results that are not needed for the correct execution and commit of the instructions. In this work, the different accesses that a particular block receives are classified into four different components, based on whether the accesses are performed by instructions of the correct path or the wrong (mispredicted) path, and also based on whether the results of the accesses are needed or not for the correct execution of the instructions. Out of the four components, only one accounts for the useful accesses to the block, that is, accesses performed to correctly execute instructions that will be committed. The other three components account for the useless activity on the block. The simulations performed indicate that, if the useless power dissipation of a high-performance processor could be totally removed with no performance degradation, the overall processor power consumption would be reduced by as much as 65% compared to the same processor in which all the blocks are accessed every cycle. This work then proposes a microarchitectural technique that targets the reduction of the useless power dissipation. The technique consists of predicting whether the result of a particular block of logic will be useful in order to execute the instructions (no matter whether the instructions will be eventually committed or not). If it is predicted useless, then the block is disabled. A case example is presented where two blocks are predicted for low power: the on-chip L2 cache for instruction fetches and the branch target buffer (BTB). The IPC versus power-consumption design space is explored for a particular microprocessor architecture. Both the average and the peak power consumption are targeted. High-level estimations are done to show that it is plausible that the ideas described might produce a significant reduction in useless block accesses. As an example, 65% accesses to the L2 cache can be eliminated at a 0.2% IPC degradation, and about 5% accesses to the BTB can be saved at the penalty of 0.7% IPC reduction. © 2003, ACM. All rights reserved.",Design; Low-power design; low-power microarchitectures,
Evaluation of Hardware and Software Schedulers for Embedded Switches,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-27644505870&doi=10.1145%2f1027794.1027798&partnerID=40&md5=ad923f1926261dcc70f1fa429a95f05e,"High-speed packet switches become increasingly important to embedded systems because they provide multiple parallel data paths necessary in emerging systems such as embedded multiprocessors, multiprotocol communication processors, and so on. The most promising architecture for embedded switches is the one that uses multiple input queues, due to its low-cost integration in conventional embedded systems, which include memory management subsystems. Such switches require highspeed schedulers, in order to resolve conflicts among packet destinations and to achieve low latency, high bandwidth communication, while providing fairness guarantees. In general, these schedulers are categorized as centralized or distributed, depending on their operation. In this paper, we evaluate hardware and software implementations of two schedulers: 2-dimensional round-robin and FIRM, which are centralized and distributed, respectively. The evaluation is performed for embedded system implementation, on a system that includes an FPGA and an embedded processor on-chip. The performance results show that, in contrast to expectations, centralized schedulers provide better performance than distributed ones in hardware implementations. In software implementations for embedded processors, surprisingly, distributed schedulers achieve better performance, due to better management of the processor's limited resources and simpler code; our experiments have shown that compilers for embedded systems are quite limited and require significant improvement. Finally, we evaluate the scalability of the schedulers, in terms of throughput, circuit complexity, and power consumption, based on implementation technology, considering the dramatic improvements expected in the availability of high-speed programmable logic and embedded processors on the same chip. © 2004, ACM. All rights reserved.",Algorithms; ATM switches; Design; embedded switches; embedded systems; packet switches; Performance; scheduling,
Compressing MIPS Code by Multiple Operand Dependencies,2003,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024261797&doi=10.1145%2f950162.950164&partnerID=40&md5=d723ee770bf806803231a5662b2150cc,"Intuitively, destination registers of some instructions have great possibilities to be used as the source registers of the immediately subsequent instructions. Such destination register/source register pairs have been exploited previously to improve code compression ratio [compression ratioD(Dictionary SizeCEncoded Program Size)/Original Program Size]. This paper further examines the exploitation of both register and immediate operand dependencies to improve the compression ratio. A mapping tag is used to flag dependency relationships so that dependent operands can be omitted during compression. The compression ratio is enhanced by both the removal of dependent operands and the sharing of mapping tags between different types of dependencies and between different instructions. Simulation results show that the proposed method results in the best compression ratio achieved so far, giving average compression ratios of 33.8% for MediaBench benchmarks and 33.6% for SPEC95 benchmarks, both compiled for a MIPS R2000 processor. © 2003, ACM. All rights reserved.",Algorithms; benchmarks; Code compression; data compression; Design; Experimentation; instruction set architecture; Languages,
Modeling and Optimizing Run-Time Reconfiguration Using Evolutionary Computation,2004,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33744959714&doi=10.1145%2f1027794.1027795&partnerID=40&md5=33df81aa2880ba7899fb7d4bf58984af,"The hardware-software (HW-SW) partitioning of applications to dynamically reconfigurable embedded systems allows for customization of their hardware resources during run-time to meet the demands of executing applications. The run-time reconfiguration (RTR) of such systems can have an impact on the HW-SW partitioning strategy and the system performance. It is therefore important to consider approaches to optimally reduce the RTR overhead during the HW-SW partitioning stage. In order to examine potential benefits in performance, it is necessary to develop a method to model and evaluate the RTR. In this paper, a novel method of modeling and evaluating such RTR-reducedHW-SW partitions is presented. The techniques of computation-reconfiguration overlap and the retention of circuitry between reconfigurations are used within this model to explore the possibilities of RTR reduction. The integration of this model into the authors' current genetic-algorithm-driven HW-SW partitioner is also presented, with two applications used to illustrate the benefits of RTR-reduced exploration during HW-SW partitioning. © 2004, ACM. All rights reserved.",Algorithms; Design; Embedded performance; Evolutionary computing; FPGAs; Modeling; Optimization; partitioning; run-time reconfiguration,
Tiny Instruction Caches For Low Power Embedded Systems,2003,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966499492&doi=10.1145%2f950162.950163&partnerID=40&md5=7656571782a4e2f529dedd819a588846,"Instruction caches have traditionally been used to improve software performance. Recently, several tiny instruction cache designs, including filter caches and dynamic loop caches, have been proposed to instead reduce software power. We propose several new tiny instruction cache designs, including preloaded loop caches, and one-level and two-level hybrid dynamic/preloaded loop caches. We evaluate the existing and proposed designs on embedded system software benchmarks from both the Powerstone and MediaBench suites, on two different processor architectures, for a variety of different technologies. We show on average that filter caching achieves the best instruction fetch energy reductions of 60-80%, but at the cost of about 20% performance degradation, which could also affect overall energy savings.We show that dynamic loop caching gives good instruction fetch energy savings of about 30%, but that if a designer is able to profile a program, preloaded loop caching can more than double the savings. We describe automated methods for quickly determining the best loop cache configuration, methods useful in a core-based design flow. © 2003, ACM. All rights reserved.",architecture tuning; Design; embedded systems; filter cache; fixed program; instruction cache; Loop cache; low energy; low power,
Adaptive Mode Control: A Static-Power-Efficient Cache Design,2003,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0344841297&doi=10.1145%2f860176.860181&partnerID=40&md5=dbc6a0e9db27143bfb2accf337fb8cde,"Lower threshold voltages in deep submicron technologies cause more leakage current, increasing static power dissipation. This trend, combined with the trend of larger/more cache memories dominating die area, has prompted circuit designers to develop SRAM cells with low-leakage operating modes (e.g., sleep mode). Sleep mode reduces static power dissipation, but data stored in a sleeping cell is unreliable or lost. So, at the architecture level, there is interest in exploiting sleep mode to reduce static power dissipation while maintaining high performance. Current approaches dynamically control the operating mode of large groups of cache lines or even individual cache lines. However, the performance monitoring mechanism that controls the percentage of sleep-mode lines, and identifies particular lines for sleep mode, is somewhat arbitrary. There is no way to know what the performance could be with all cache lines active, so arbitrary miss rate targets are set (perhaps on a per-benchmark basis using profile information), and the control mechanism tracks these targets.We propose applying sleep mode only to the data store and not the tag store. By keeping the entire tag store active the hardware knows what the hypothetical miss rate would be if all data lines were active, and the actual miss rate can be made to precisely track it. Simulations show that an average of 73% of I-cache lines and 54% of D-cache lines are put in sleep mode with an average IPC impact of only 1.7%, for 64 KB caches. © 2003, ACM. All rights reserved.",adaptive mode control; Cache; Design; Experimentation; Performance; static power,
Maximizing Rewards for Real-Time Applications with Energy Constraints,2003,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955937293&doi=10.1145%2f950162.950166&partnerID=40&md5=d5e7379fc8e9916b7b3f17f6295e6d9f,"New technologies have brought about a proliferation of embedded systems, which vary from control systems to sensor networks to personal digital assistants. Many of the portable embedded devices run several applications, which typically have three constraints that need to be addressed: energy, deadline, and reward. However, many of these portable devices do not have powerful enough CPUs and batteries to run all applications within their deadlines. An optimal scheme would allow the device to run the most applications, each using the most amount of CPU cycles possible, without depleting the energy source while still meeting all deadlines. In this paper we propose a solution to this problem; to our knowledge, this is the first solution that combines the three constraints mentioned above. We devise two algorithms, an optimal algorithm for homogeneous applications (with respect to power consumption) and a heuristic iterative algorithm that can also accommodate heterogeneous applications (i.e., those with different power consumption functions). We show by simulation that our iterative algorithm is fast and within 1% of the optimal. © 2003, ACM. All rights reserved.",Algorithms; operating systems; Power management; real-time; reward-based; scheduling,
A Case Study of a System-Level Approach to Power-Aware Computing,2003,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84999817120&doi=10.1145%2f860176.860178&partnerID=40&md5=18d9832713c5b08e48e40d05c83cb3aa,"This paper introduces a systematic approach to power awareness in mobile, handheld computers. It describes experimental evaluations of several techniques for improving the energy efficiency of a system, ranging from the network level down to the physical level of the battery. At the network level, a new routing method based upon the power consumed by the network subsystem is shown to improve power consumption by 15% on average and to reduce latency by 75% over methods that consider only the transmitted power. At the boundary between the network and the processor levels, the paper presents the problem of local versus remote processing and derives a figure of merit for determining whether a computation should be completed locally or remotely, one that involves the relative performance of the local and remote system, the transmission bandwidth and power consumption, and the network congestion. At the processor level, the main memory bandwidth is shown to have a significant effect on the relationship between performance and CPU frequency, which in turn determines the energy savings of dynamic CPU speed-setting. The results show that accounting for the main memory bandwidth using Amdahl's law permits the performance speed-up and peak power versus the CPU frequency to be estimated to within 5%. The paper concludes with a technique for mitigating the loss of battery energy capacity with large peak currents, showing an improvement of up to 10% in battery life, albeit at some cost to the size and weight of the system. © 2003, ACM. All rights reserved.",battery properties; Design; dynamic power management; energy-aware; Experimentation; handheld computers; Measurement; multihop wireless network; Performance; Power-aware,
Special Issue on Power-Aware Embedded Computing,2003,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34047115157&doi=10.1145%2f860176.860177&partnerID=40&md5=4e723daf16dff6e7909530f63bfdd1e7,[No abstract available],,
On Energy-Optimal Voltage Scheduling for Fixed-Priority Hard Real-Time Systems,2003,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052451482&doi=10.1145%2f860176.860183&partnerID=40&md5=29b2d93970d8bea95a870a1a803a1efb,"We address the problem of energy-optimal voltage scheduling for fixed-priority hard real-time systems, on which we present a complete treatment both theoretically and practically. Although most practical real-time systems are based on fixed-priority scheduling, there have been few research results known on the energy-optimal fixed-priority scheduling problem. First, we prove that the problem is NP-hard. Then, we present a fully polynomial time approximation scheme (FPTAS) for the problem. For any ɛ>0, the proposed approximation scheme computes a voltage schedule whose energy consumption is at most (1+ɛ) times that of the optimal voltage schedule. Furthermore, the running time of the proposed approximation scheme is bounded by a polynomial function of the number of input jobs and 1/ɛ. Given the NP-hardness of the problem, the proposed approximation scheme is practically the best solution because it can compute a near-optimal voltage schedule (i.e., provably arbitrarily close to the optimal schedule) in polynomial time. Experimental results show that the approximation scheme finds more efficient (almost optimal) voltage schedules faster than the best existing heuristic. © 2003, ACM. All rights reserved.",Algorithms; approximation algorithms; dynamic voltage scaling; Fixed-priority scheduling; fully polynomial time approximation scheme; real-time systems; variable voltage processor,
Energy Management for Battery-Powered Embedded Systems,2003,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017378082&doi=10.1145%2f860176.860179&partnerID=40&md5=2a572716d4c62b42d373d6a623bb77b2,"Portable embedded computing systems require energy autonomy. This is achieved by batteries serving as a dedicated energy source. The requirement of portability places severe restrictions on size and weight, which in turn limits the amount of energy that is continuously available to maintain system operability. For these reasons, efficient energy utilization has become one of the key challenges to the designer of battery-powered embedded computing systems. In this paper, we first present a novel analytical battery model, which can be used for the battery lifetime estimation. The high quality of the proposed model is demonstrated with measurements and simulations. Using this battery model, we introduce a new “battery-aware” cost function, which will be used for optimizing the lifetime of the battery. This cost function generalizes the traditional minimization metric, namely the energy consumption of the system. We formulate the problem of battery-aware task scheduling on a single processor with multiple voltages. Then, we prove several important mathematical properties of the cost function. Based on these properties, we propose several algorithms for task ordering and voltage assignment, including optimal idle period insertion to exercise charge recovery. This paper presents the first effort toward a formal treatment of battery-aware task scheduling and voltage scaling, based on an accurate analytical model of the battery behavior. © 2003, ACM. All rights reserved.",Algorithms; Battery; low-power design; modeling; Performance; scheduling; voltage scaling,
Online Strategies for Dynamic Power Management in Systems with Multiple Power-Saving States,2003,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011339072&doi=10.1145%2f860176.860180&partnerID=40&md5=55eacd92187a3345bfb42fca20f0181f,"Online dynamic power management (DPM) strategies refer to strategies that attempt to make power-mode-related decisions based on information available at runtime. In making such decisions, these strategies do not depend upon information of future behavior of the system, or any a priori knowledge of the input characteristics. In this paper, we present online strategies, and evaluate them based on a measure called the competitive ratio that enables a quantitative analysis of the performance of online strategies. All earlier approaches (online or predictive) have been limited to systems with two power-saving states (e.g., idle and shutdown). The only earlier approaches that handled multiple power-saving states were based on stochastic optimization. This paper provides a theoretical basis for the analysis of DPM strategies for systems with multiple power-down states, without resorting to such complex approaches. We show how a relatively simple “online learning” scheme can be used to improve the competitive ratio over deterministic strategies using the notion of “probability-based” online DPM strategies. Experimental results show that the algorithm presented here attains the best competitive ratio in comparison with other known predictive DPM algorithms. The other algorithms that come close to matching its performance in power suffer at least an additional 40% wake-up latency on average. Meanwhile, the algorithms that have comparable latency to our methods use at least 25% more power on average. © 2003, ACM. All rights reserved.",Algorithms; Dynamic; online algorithms; Performance; power management,
Power Management for Energy-Aware Communication Systems,2003,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0347116788&doi=10.1145%2f860176.860184&partnerID=40&md5=24e1c1e875dc46ec81b74ca0cd21fd4b,"System-level power management has become a key technique to render modern wireless communication devices economically viable. Despite their relatively large impact on the system energy consumption, power management for radios has been limited to shutdown-based schemes, while processors have benefited from superior techniques based on dynamic voltage scaling (DVS). However, similar scaling approaches that trade-off energy versus performance are also available for radios. To utilize these in radio power management, existing packet scheduling policies have to be thoroughly rethought to make them energy-aware, essentially opening a whole new set of challenges the same way the introduction of DVS did to CPU task scheduling. We use one specific scaling technique, dynamic modulation scaling (DMS), as a vehicle to outline these challenges, and to introduce the intricacies caused by the nonpreemptive nature of packet scheduling and the time-varying wireless channel. © 2003, ACM. All rights reserved.",adaptive; Algorithms; Design; Energy-efficient; Performance; scaling; wireless communications,
Automatic Compilation to a Coarse-Grained Reconfigurable System-opn-Chip,2003,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988637860&doi=10.1145%2f950162.950167&partnerID=40&md5=0fe390807ff3fdc7510539dd20eca20d,"The rapid growth of device densities on silicon has made it feasible to deploy reconfigurable hardware as a highly parallel computing platform. However, one of the obstacles to the wider acceptance of this technology is its programmability. The application needs to be programmed in hardware description languages or an assembly equivalent, whereas most application programmers are used to the algorithmic programming paradigm. SA-C has been proposed as an expression-oriented language designed to implicitly express data parallel operations. The Morphosys project proposes an SoC architecture consisting of reconfigurable hardware that supports a data-parallel, SIMD computational model. This paper describes a compiler framework to analyze SA-C programs, perform optimizations, and automatically map the application onto the Morphosys architecture. The mapping process is static and it involves operation scheduling, processor allocation and binding, and register allocation in the context of the Morphosys architecture. The compiler also handles issues concerning data streaming and caching in order to minimize data transfer overhead. We have compiled some important image-processing kernels, and the generated schedules reflect an average speedup in execution times of up to 6£ compared to the execution on 800 MHz Pentium III machines. © 2003, ACM. All rights reserved.",Algorithms; compilers; Design; Experimentation; Reconfigurable computing; SIMD,
Cool-Cache: A Compiler-Enabled Energy Efficient Data Caching Framework for Embedded/Multimedia Processors,2003,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015443357&doi=10.1145%2f860176.860182&partnerID=40&md5=98652ed94c97a3f3ae44106d0cb6375f,"The unique characteristics of multimedia/embedded applications dictate media-sensitive architectural and compiler approaches to reduce the power consumption of the data cache. Our goal is exploring energy savings for embedded/multimedia workloads without sacrificing performance. Here, we present two complementary media-sensitive energy-saving techniques that leverage static information. While our first technique is applicable to existing architectures, in our second technique we adopt a more radical approach and propose a new tagless caching architecture by reevaluating the architecture—compiler interface. Our experiments show that substantial energy savings are possible in the data cache. Across a wide range of cache and architectural configurations, we obtain up to 77% energy savings, while the performance varies from 14% improvement to 4% degradation depending on the application. © 2003, ACM. All rights reserved.",cache partitioning; compiler-architecture interaction; Design; Experimentation; Low-power design; Performance; tagless caching,
Static Resource Models for Code-Size Efficient Embedded Processors,2003,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024260835&doi=10.1145%2f643470.643475&partnerID=40&md5=017bde415247f28a96155d82a1566dbc,"Due to an increasing need for flexibility, embedded systems embody more and more programmable processors as their core components. Due to silicon area and power considerations, the corresponding instruction sets are often highly encoded to minimize code size for given performance requirements. This has hampered the development of robust optimizing compilers because the resulting irregular instruction set architectures are far from convenient compiler targets. Among other considerations, they introduce an interdependence between the tasks of instruction selection and scheduling. This so-called phase coupling is so strong that, in practice, instruction selection rather than scheduling is responsible for the quality of the schedule, which tends to disappoint. The lack of efficient compilation tools has also severely hampered the design space exploration of code-size efficient instruction sets, and correspondingly, their tuning to the application domain. In this article, we present an approach that reduces the need for explicit instruction selection by transferring constraints implied by the instruction set to static resource constraints. All resulting schedules are then guaranteed to correspond to a valid implementation with given instructions. We also demonstrate the suitability of this model to enable instruction set design (-space exploration) with a simple, well-understood and proven method long used in high-level synthesis (HLS) of ASICs. Experimental results show the efficacy of our approach. © 2003, ACM. All rights reserved.",,
"Special Issue on Compilers, Architecture, and Synthesis for Embedded Systems",2003,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024276586&doi=10.1145%2f643470.643471&partnerID=40&md5=73f252bc4c7816aab431b4c253fa81de,[No abstract available],,
Array Recovery and High-Level Transformations for DSP Applications,2003,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1142271338&doi=10.1145%2f643470.643472&partnerID=40&md5=011812a6b3302ad8a0414c694e465980,"Efficient implementation of DSP applications is critical for many embedded systems. Optimizing compilers for application programs, written in C, largely focus on code generation and scheduling, which, with their growing maturity, are providing diminishing returns. As DSP applications typically make extensive use of pointer arithmetic, the alternative use of high-level, source-to-source, transformations has been largely ignored. This article develops an array recovery technique that automatically converts pointers to arrays, enabling the empirical evaluation of high-level transformations. High-level techniques were applied to the DSPstone benchmarks on three platforms: TriMedia TM-1000, Texas Instruments TMS320C6201, and the Analog Devices SHARC ADSP- 21160. On average, the best transformation gave a factor of 2.43 improvement across the platforms. In certain cases, a speedup of 5.48 was found for the SHARC, 7.38 for the TM-1, and 2.3 for the C6201. These preliminary results justify pointer to array conversion and further investigation into the use of high-level techniques for embedded compilers. Copyright © 2003, ACM. All rights reserved.",Algorithms; dataflow graphs; embedded processors; Experimentation; high-level transformations; Measurement; Pointer conversion,
Partitioned Instruction Cache Architecture for Energy Efficiency,2003,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-1542750720&doi=10.1145%2f643470.643473&partnerID=40&md5=6669518a7bce052a4002015eedc322e0,"The demand for high-performance architectures and powerful battery-operated mobile devices has accentuated the need for low-power systems. In many media and embedded applications, the memory system can consume more than 50% of the overall system energy, making it a ripe candidate for optimization. To address this increasingly important problem, this article studies energy-efficient cache architectures in the memory hierarchy that can have a significant impact on the overall system energy consumption. Existing cache optimization approaches have looked at partitioning the caches at the circuit level and enabling/disabling these cache partitions (subbanks) at the architectural level for both performance and energy. In contrast, this article focuses on partitioning the cache resources architecturally for energy and energy-delay optimizations. Specifically, we investigate ways of splitting the cache into several smaller units, each of which is a cache by itself (called a subcache). Subcache architectures not only reduce the per-access energy costs, but can potentially improve the locality behavior as well. The proposed subcache architecture employs a page-based placement strategy, a dynamic page remapping policy, and a subcache prediction policy in order to improve the memory system energy behavior, especially on-chip cache energy. Using applications from the SPECjvm98 and SPEC CPU2000 benchmarks, the proposed subcache architecture is shown to be very effective in improving both the energy and energy-delay metrics. It is more beneficial in larger caches as well. Copyright © 2003, ACM. All rights reserved.",Caches; Design; energy; Experimentation; memory system,
Data Remapping for Design Space Optimization of Embedded Memory Systems,2003,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-26444518564&doi=10.1145%2f643470.643474&partnerID=40&md5=b72e197f07e12ad08af29b8f076416d6,"In this article, we present a novel linear time algorithm for data remapping, that is, (i) lightweight; (ii) fully automated; and (iii) applicable in the context of pointer-centric programming languages with dynamic memory allocation support. All previous work in this area lacks one or more of these features.We proceed to demonstrate a novel application of this algorithm as a key step in optimizing the design of an embedded memory system. Specifically, we show that by virtue of locality enhancements via data remapping, we may reduce the memory subsystem needs of an application by 50%, and hence concomitantly reduce the associated costs in terms of size, power, and dollar-investment (61%). Such a reduction overcomes key hurdles in designing high-performance embedded computing solutions. Namely, memory subsystems are very desirable from a performance standpoint, but their costs have often limited their use in embedded systems. Thus, our innovative approach offers the intriguing possibility of compilers playing a significant role in exploring and optimizing the design space of a memory subsystem for an embedded design. To this end and in order to properly leverage the improvements afforded by a compiler optimization, we identify a range of measures for quantifying the cost-impact of popular notions of locality, prefetching, regularity of memory access, and others. The proposed methodology will become increasingly important, especially as the needs for application specific embedded architectures become prevalent. In addition, we demonstrate the wide applicability of data remapping using several existing microprocessors, such as the Pentium and UltraSparc. Namely, we show that remapping can achieve a performance improvement of 20% on the average. Similarly, for a parametric research HPL-PD microprocessor, which characterizes the new Itanium machines, we achieve a performance improvement of 28% on average. All of our results are achieved using applications from the DIS, Olden and SPEC2000 suites of integer and floating point benchmarks. Copyright © 2003, ACM. All rights reserved.",Algorithms; caches; compiler optimization; data remapping; Design; Design space exploration; embedded systems; memory hierarchy; memory subsystem; Performance,
Introduction to the Two Special Issues on Memory,2003,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025420332&doi=10.1145%2f605459.605460&partnerID=40&md5=10b4c3a728e777b7e15f68c980e1b33e,[No abstract available],,
Access Pattern-Based Memory and Connectivity Architecture Exploration,2003,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-27644546282&doi=10.1145%2f605459.605462&partnerID=40&md5=f5c1aebcf7c01980bf4a1e653d0f939a,"Memory accesses represent a major bottleneck in embedded systems power and performance. Traditionally, designers tried to alleviate this problem by relying on a simple cache hierarchy, or a limited use of special purpose memory modules such as stream buffers. Although real-life applications contain a large number of memory references to a diverse set of data structures, a significant percentage of all memory accesses in the application are generated from a few memory instructions that exhibit predictable, well-known access patterns; this creates an opportunity for memory customization, targeting the needs of these access patterns. We present APEX, an approach that extracts, analyzes and clusters the most active access patterns in the application, and aggressively customizes the memory architecture to match the needs of the application. Moreover, though the memory modules are important, the rate at which the memory system can produce the data for the CPU is significantly impacted by the connectivity architecture between the memory subsystem and the CPU. Thus, it is critical to consider the connectivity architecture early in the design flow, in conjunction with the memory architecture. We couple the exploration of memory modules together with their connectivity, to evaluate a wide range of cost, performance, and energy connectivity architectures. We use a heuristic to prune the design space, guiding the exploration towards the most promising designs. We present experiments on a set of large real-life benchmarks, showing significant performance improvements for varied cost and power characteristics, allowing the designer to evaluate customized memory and connectivity configurations for embedded systems. © 2003, ACM. All rights reserved.",access patterns; architecture exploration; Design; Experimentation; Memory; Performance,
System Synthesis of Synchronous Multimedia Applications,2003,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746920889&doi=10.1145%2f605459.605463&partnerID=40&md5=fa68de0df0f382ff999cae2b6adff6aa,"Modern system design is being increasingly driven by applications such as multimedia and wireless sensing and communications, which have intrinsic quality of service (QoS) requirements, such as throughput, error-rate, and resolution. One of the most crucial QoS guarantees that the system has to provide is the timing constraint among the interacting media (synchronization) and within each media (latency). We have developed the first framework for system design with timing QoS guarantees. In particular, we address how to design system-on-chip with minimum silicon area to meet both latency and synchronization constraints. The proposed design methodology consists of two phases: hardware configuration selection and on-chip memory/storage minimization. In the first phase, we use silicon area and system performance as criteria to identify all the competitive hardware configurations (i.e., Pareto points) that facilitate the needs of synchronous applications. In the second phase, we determine the minimum on-chip memory requirement to meet the timing constraints for each Pareto point. An overall system evaluation is conducted to select the best system configuration. We have developed optimal algorithms that schedule a priori specified applications to meet their synchronization requirements with the minimum size of memory. We have also implemented on-line heuristics for real-time applications. The effectiveness of our algorithms has been demonstrated on a set of simulated MPEG streams from popular movies. © 2003, ACM. All rights reserved.",Algorithms; Design; high-level embedded systems synthesis; on-chip memory minimization; synchronization,
"Energy-Aware Design of Embedded Memories: A Survey of Technologies, Architectures, and Optimization Techniques",2003,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015962264&doi=10.1145%2f605459.605461&partnerID=40&md5=c7468a10f4f72aad7c266fa4edf8e3f5,"Embedded systems are often designed under stringent energy consumption budgets, to limit heat generation and battery size. Since memory systems consume a significant amount of energy to store and to forward data, it is then imperative to balance power consumption and performance in memory system design. Contemporary system design focuses on the trade-off between performance and energy consumption in processing and storage units, as well as in their interconnections. Although memory design is as important as processor design in achieving the desired design objectives, the former topic has received less attention than the latter in the literature. This article centers on one of the most outstanding problems in chip design for embedded applications. It guides the reader through different memory technologies and architectures, and it reviews the most successful strategies for optimizing them in the power/performance plane. © 2003, ACM. All rights reserved.",Design; embedded memories; Embedded systems; integration; memories; nonvolatile; Performance; system-on-a-chip; volatile,
Low-Energy Off-Chip SDRAM Memory Systems for Embedded Applications,2003,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006108082&doi=10.1145%2f605459.605464&partnerID=40&md5=b4fd1cf45c15a4bb9b878568a9263a1e,"Memory systems are dominant energy consumers, and thus many energy reduction techniques for memory buses and devices have been proposed. For practical energy reduction practices, we have to take into account the interaction between a processor and cache memories together with application programs. Furthermore, energy characterization of memory systems must be accurate enough to justify various techniques. In this article, we build an in-house energy simulator for memory systems that is accelerated by special hardware support while maintaining accuracy. We explore energy behavior of memory systems for various values of the processor and memory clock frequencies and cache configuration. Each experiment is performed with 24M instruction steps of real application programs to guarantee accuracy. The simulator is based on precise energy characterization of memory systems including buses, bus drivers, and memory devices by a cycle-accurate energy measurement technique.We characterize energy consumption of each component by an energy state machine whose states and transitions are associated with the dynamic and static energy costs, respectively. Our approach easily characterizes the energy consumption of complex SDRAMs. We divide and quantify energy components of main memory systems for high-level reduction. The energy simulator enables us to devise practical energy reduction schemes by providing the actual amount of reduction out of the total energy consumption in main memory systems.We introduce several practical energy reduction techniques for SDRAM memory systems and demonstrate energy reduction ratio over the SDRAM memory systems with commercial SDRAM controller chipsets. We classify the SDRAM memory systems into high-performance and mid-performance classes and achieve suitable system configurations for each class. For instance, a typical high-performance 32-bit, 64 MB SDRAM memory system consumes 19.6 mJ, 33.8 mJ, 35.4 mJ, and 37.0 mJ for 24M instructions of an MP3 decoder, a JPEG compressor, a JPEG decompressor, and an MPEG4 decoder, respectively. Our reduction scheme saves 12.7 mJ, 15.1 mJ, 15.5 mJ, and 14.8 mJ, and the reduction ratios are 64.8%, 44.6%, 43.8%, and 40.1%, respectively, without compromising execution speed. © 2003, ACM. All rights reserved.",Design; Experimentation; Low power; Measurement; memory system; Performance; SDRAM,
Application-Adaptive Intelligent Cache Memory System,2002,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-55849135721&doi=10.1145%2f581888.581893&partnerID=40&md5=01e518ab92134dfe779a00edf42f5089,"This article presents the design of a simple hardware-controlled, high performance cache system. The design supports fast access time, optimal utilization of temporal and spatial localities adaptive to given applications, and a simple dynamic fetching mechanism with different fetch sizes. Support for dynamically varying the fetch size makes the cache equally effective for general-purpose as well as multimedia applications. Our cache organization and operational mechanism are especially designed to maximize temporal locality and spatial locality, selectively and adaptively. Simulation shows that the average memory access time of the proposed cache is equal to that of a conventional direct-mapped cache with eight times as much space. In addition, the simulations show that our cache achieves better performance than a 2-way or 4-way set associative cache with twice as much space. The average miss ratio, compared with the victim cache with 32-byte block size, is improved by about 41% or 60% for general applications and multimedia applications, respectively. It is also shown that power consumption of the proposed cache is around 10% to 60% lower than other cache systems that we examine. Our cache system thus offers high performance with low power consumption and low hardware cost. © 2002, ACM. All rights reserved.",Design; dynamic block fetching and cache memory; Experimentation; general application; media application; Memory hierarchy; Performance; spatial locality; temporal locality,
System-Level Exploration of Association Table Implementations in Telecom Network Applications,2002,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-25444515522&doi=10.1145%2f581888.581895&partnerID=40&md5=683bfe6cebb51e0dcf567b13b7e85606,"We present a new exploration and optimization method at the system level to select customized implementations for dynamic data sets, as encountered in telecom network, database, and multimedia applications. Our method fits in the context of embedded system synthesis for such applications, and enables to further raise the abstraction level of the initial specification, where dynamic data sets can be specified without low-level details. Our method is suited for hardware and software implementations. In this paper, it mainly aims at minimizing the average memory power, although it can also be driven by other cost functions such as memory size and performance. Compared with existing methods, for large dynamic data sets, it can save up to 90% of the average memory power, while still saving up to 80% of the average memory size. © 2002, ACM. All rights reserved.",Memory; memory management; Performance; System-level exploration,
An Optimal Memory Allocation Scheme for Scratch-Pad-Based Embedded Systems,2002,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872094294&doi=10.1145%2f581888.581891&partnerID=40&md5=98ff79bee340d024d5ae056fd5d3935b,"This article presents a technique for the efficient compiler management of software-exposed heterogeneous memory. In many lower-end embedded chips, often used in microcontrollers and DSP processors, heterogeneous memory units such as scratch-pad SRAM, internal DRAM, external DRAM, and ROM are visible directly to the software, without automatic management by a hardware caching mechanism. Instead, the memory units are mapped to different portions of the address space. Caches are avoided due to their cost and power consumption, and because they make it difficult to guarantee real-time performance. For this important class of embedded chips, the allocation of data to different memory units to maximize performance is the responsibility of the software. Current practice typically leaves it to the programmer to partition the data among different memory units. We present a compiler strategy that automatically partitions the data among the memory units. We show that this strategy is optimal, relative to the profile run, among all static partitions for global and stack data. For the first time, our allocation scheme for stacks distributes the stack among multiple memory units. For global and stack data, the scheme is provably equal to or better than any other compiler scheme or set of programmer annotations. Results from our benchmarks show a 44.2% reduction in runtime from using our distributed stack strategy vs. using a unified stack, and a further 11.8% reduction in runtime from using a linear optimization strategy for allocation vs. a simpler greedy strategy; both in the case of the SRAM size being 20% of the total data size. For some programs, less than 5% of data in SRAM achieves a similar speedup. © 2002, ACM. All rights reserved.",allocation; Design; embedded; heterogeneous; Measurement; Memory; Performance; storage,
Introduction to the Inaugural Issue,2002,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025382010&doi=10.1145%2f581888.581889&partnerID=40&md5=4a0fc7e4f27656fb56ef941e5ded3a30,[No abstract available],,
Frequent Value Locality and its Applications,2002,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038364440&doi=10.1145%2f581888.581894&partnerID=40&md5=69705951245ab1e53ccd5a532e4dc785,"By analyzing the behavior of a set of benchmarks, we demonstrate that a small number of distinct values tend to occur very frequently in memory. On an average, only eight of these frequent values were found to occupy 48% of memory locations for the benchmarks studied. In addition, we demonstrate that the identity of frequent values remains stable over the entire execution of the program and these values are scattered fairly uniformly across the allocated memory. We present three different algorithms for finding frequent values and experimentally demonstrate their effectiveness. Each of these algorithms is designed to suit a different application scenario. Since the contents of memory exhibit frequent value locality, it is expected that frequent values will be observed in data streams that flow across different points in the memory hierarchy. We exploit this observation for developing two low-power designs: a low-power level-one data cache and a low-power external data bus. In each of these applications a different form of encoding of frequent values is employed to obtain a low-power design. We also experimentally demonstrate the effectiveness of these designs. © 2002, ACM. All rights reserved.",Design; encoding techniques; Frequently occurring values; low power data bus; low power data cache; Measurement; Performance; value profiling,
Tuning Garbage Collection for Reducing Memory System Energy in an Embedded Java Environment,2002,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-14244256378&doi=10.1145%2f581888.581892&partnerID=40&md5=1fff2d30173865fca3292685ac5958e1,"Java has been widely adopted as one of the software platforms for the seamless integration of diverse computing devices. Over the last year, there has been great momentum in adopting Java technology in devices such as cellphones, PDAs, and pagers where optimizing energy consumption is critical. Since, traditionally, the Java virtual machine (JVM), the cornerstone of Java technology, is tuned for performance, taking into account energy consumption requires reevaluation, and possibly redesign of the virtual machine. This motivates us to tune specific components of the virtual machine for a battery-operated architecture. As embedded JVMs are designed to run for long periods of time on limited-memory embedded systems, creating and managing Java objects is of critical importance. The garbage collector (GC) is an important part of the JVM responsible for the automatic reclamation of unused memory. This article shows that the GC is not only important for limited-memory systems but also for energy-constrained architectures. This article focuses on tuning the GC to reduce energy consumption in a multibanked memory architecture. Tuning the GC is important not because it consumes a sizeable portion of overall energy during execution, but because it influences the energy consumed in the memory during application execution. In particular, we present a GC-controlled leakage energy optimization technique that shuts off memory banks that do not hold live data. Using two different commercial GCs and a suite of thirteen mobile applications, we evaluate the effectiveness of the GC-controlled energy optimization technique and study its sensitivity to different parameters such as bank size, the garbage collection frequency, object allocation style, compaction style, and compaction frequency. We observe that the energy consumption of an embedded Java application can be significantly more if the GC parameters are not tuned appropriately. Further, we notice that the object allocation pattern and the number of memory banks available in the underlying architecture are limiting factors on how effectively GC parameters can be used to optimize the memory energy consumption. © 2002, ACM. All rights reserved.",Design; Garbage collector; Java Virtual Machine (JVM); K Virtual Machine (KVM); low power computing; Measurement,
Dataflow Driven Partitioning of Machine Learning Applications for Optimal Energy Use in Batteryless Systems,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139164415&doi=10.1145%2f3520135&partnerID=40&md5=fdfd0939fdfb5e92c75afb6deee65039,"Sensing systems powered by energy harvesting have traditionally been designed to tolerate long periods without energy. As the Internet of Things (IoT) evolves toward a more transient and opportunistic execution paradigm, reducing energy storage costs will be key for its economic and ecologic viability. However, decreasing energy storage in harvesting systems introduces reliability issues. Transducers only produce intermittent energy at low voltage and current levels, making guaranteed task completion a challenge. Existing ad hoc methods overcome this by buffering enough energy either for single tasks, incurring large data-retention overheads, or for one full application cycle, requiring a large energy buffer. We present Julienning: an automated method for optimizing the total energy cost of batteryless applications. Using a custom specification model, developers can describe transient applications as a set of atomically executed kernels with explicit data dependencies. Our optimization flow can partition data- and energy-intensive applications into multiple execution cycles with bounded energy consumption. By leveraging interkernel data dependencies, these energy-bounded execution cycles minimize the number of system activations and nonvolatile data transfers, and thus the total energy overhead. We validate our methodology with two batteryless cameras running energy-intensive machine learning applications. Using a solar testbed, we replay real-world illuminance traces to experimentally demonstrate optimized batteryless execution with a transducer-to-application energy efficiency of 74.5%. Partitioning results demonstrate that compared to ad hoc solutions, our method can reduce the required energy storage by over 94% while only incurring a 0.12% energy overhead.  © 2022 Association for Computing Machinery.",Energy efficiency; energy harvesting; low-power design,Cost reduction; Data transfer; Digital storage; Electric power supplies to apparatus; Energy harvesting; Energy storage; Energy utilization; Internet of things; Machine learning; Transducers; Battery-less; Data dependencies; Dataflow; Energy; Energy overheads; Execution cycles; Low-power design; Machine learning applications; Optimal energy; Total energy; Energy efficiency
MHDeep: Mental Health Disorder Detection System Based on Wearable Sensors and Artificial Neural Networks,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146423324&doi=10.1145%2f3527170&partnerID=40&md5=57e8747be9f326ca5a169bc18c9e934d,"Mental health problems impact the quality of life of millions of people around the world. However, diagnosis of mental health disorders is a challenging problem that often relies on self-reporting by patients about their behavioral patterns and social interactions. Therefore, there is a need for new strategies for diagnosis and daily monitoring of mental health conditions. The recent introduction of body-area networks consisting of a plethora of accurate sensors embedded in smartwatches and smartphones and edge-compatible deep neural networks (DNNs) points toward a possible solution. Such wearable medical sensors (WMSs) enable continuous monitoring of physiological signals in a passive and non-invasive manner. However, disease diagnosis based on WMSs and DNNs, and their deployment on edge devices, such as smartphones, remains a challenging problem. These challenges stem from the difficulty of feature engineering and knowledge distillation from the raw sensor data, as well as the computational and memory constraints of battery-operated edge devices. To this end, we propose a framework called MHDeep that utilizes commercially available WMSs and efficient DNN models to diagnose three important mental health disorders: schizoaffective, major depressive, and bipolar. MHDeep uses eight different categories of data obtained from sensors integrated in a smartwatch and smartphone. These categories include various physiological signals and additional information on motion patterns and environmental variables related to the wearer. MHDeep eliminates the need for manual feature engineering by directly operating on the data streams obtained from participants. Because the amount of data is limited, MHDeep uses a synthetic data generation module to augment real data with synthetic data drawn from the same probability distribution. We use the synthetic dataset to pre-train the weights of the DNN models, thus imposing a prior on the weights. We use a grow-and-prune DNN synthesis approach to learn both architecture and weights during the training process. We use three different data partitions to evaluate the MHDeep models trained with data collected from 74 individuals. We conduct two types of evaluations: at the data instance level and at the patient level. MHDeep achieves an average test accuracy, across the three data partitions, of 90.4%, 87.3%, and 82.4%, respectively, for classifications between healthy and schizoaffective disorder instances, healthy and major depressive disorder instances, and healthy and bipolar disorder instances. At the patient level, MHDeep DNN models achieve an accuracy of 100%, 100%, and 90.0% for the three mental health disorders, respectively, based on inference that uses 40, 16, and 22 minutes of sensor data collection from each patient. © 2022 Association for Computing Machinery.",Disease diagnosis; health monitoring; mental health disorders; neural networks; synthetic data generation; wearable medical sensors,Data handling; Diagnosis; mHealth; Neural network models; Physiology; Probability distributions; Smartphones; Wearable computers; Wearable sensors; Disease diagnosis; Health disorders; Health monitoring; Medical sensors; Mental health; Mental health disorder; Neural-networks; Smart phones; Synthetic data generations; Wearable medical sensor; Deep neural networks
Mobile or FPGA? A Comprehensive Evaluation on Energy Efficiency and a Unified Optimization Framework,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146371266&doi=10.1145%2f3528578&partnerID=40&md5=fec3d9af4bb5a825d57b21f14f3ae583,"Efficient deployment of Deep Neural Networks (DNNs) on edge devices (i.e., FPGAs and mobile platforms) is very challenging, especially under a recent witness of the increasing DNN model size and complexity. Model compression strategies, including weight quantization and pruning, are widely recognized as effective approaches to significantly reduce computation and memory intensities, and have been implemented in many DNNs on edge devices. However, most state-of-the-art works focus on ad hoc optimizations, and there lacks a thorough study to comprehensively reveal the potentials and constraints of different edge devices when considering different compression strategies. In this article, we qualitatively and quantitatively compare the energy efficiency of FPGA-based and mobile-based DNN executions using mobile GPU and provide a detailed analysis. Based on the observations obtained from the analysis, we propose a unified optimization framework using block-based pruning to reduce the weight storage and accelerate the inference speed on mobile devices and FPGAs, achieving high hardware performance and energy-efficiency gain while maintaining accuracy.  © 2022 Association for Computing Machinery.",DNN model compression; edge device; efficient deep learning,Computer aided design; Deep neural networks; Energy efficiency; Field programmable gate arrays (FPGA); Comprehensive evaluation; Compression strategies; Deep neural network model compression; Edge device; Efficient deep learning; Mobile platform; Model compression; Model size; Neural network model; Unified optimization framework; Neural network models
Winograd Convolution for Deep Neural Networks: Efficient Point Selection,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145004114&doi=10.1145%2f3524069&partnerID=40&md5=6633a7a4ea82819014154307b2cc63ff,"Convolutional neural networks (CNNs) have dramatically improved the accuracy of image, video, and audio processing for tasks such as object recognition, image segmentation, and interactive speech systems. CNNs require large amounts of computing resources for both training and inference, primarily because the convolution layers are computationally intensive. Fast convolution algorithms such as Winograd convolution can greatly reduce the computational cost of these layers. However, Winograd convolution has poor numeric properties, such that greater savings in computation cause exponentially increasing floating point errors.A defining feature of each Winograd convolution algorithm is a set of real-value points where polynomials are sampled. The choice of points impacts the numeric accuracy of the algorithm, but the optimal set of points for small convolutions remains unknown. Existing work considers only small integers and simple fractions as candidate points. In this work, we propose a novel approach to point selection using points of the form using the full range of real-valued numbers for c. We show that groups of this form cause cancellations in the Winograd transform matrices that reduce numeric error. We find empirically that the error for different values of c forms a rough curve across the range of real-value numbers. It is therefore possible to localize the values of c that lead to lower error. We show that it is not necessary to choose integers or simple fractions as evaluation points, and that lower errors can be achieved with non-obvious real-valued points. We study a range of sizes for small convolutions and achieve reduction in error ranging from 2% to around 59% for both 1D and 2D convolution, when compared to state of the art. Furthermore, we identify patterns in cases when we select a subset of our proposed points that will always lead to a lower error. Finally, we implement a complete Winograd convolution layer and use it to run state-of-the-art deep convolution neural networks on real datasets and show that our proposed points achieve reduction in error, ranging from 22% to 63%, while also showing how an increased Winograd output size can result in execution speed-up for some cases. © 2022 Association for Computing Machinery.",deep neural networks; Toom-Cook algorithm; Winograd convolution,Audio systems; Convolution; Digital arithmetic; Errors; Image enhancement; Image segmentation; Multilayer neural networks; Object recognition; Speech recognition; % reductions; Convolution algorithm; Convolutional neural network; Point selection; Real values; Simple++; Toom-Cook; Toom-cook algorithm; Winograd; Winograd convolution; Deep neural networks
Toward Adversary-aware Non-iterative Model Pruning through Dynamic Network Rewiring of DNNs,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138887784&doi=10.1145%2f3510833&partnerID=40&md5=0ad0fc538daa46d11bbf809c3bd20063,"We present a dynamic network rewiring (DNR) method to generate pruned deep neural network (DNN) models that both are robust against adversarially generated images and maintain high accuracy on clean images. In particular, the disclosed DNR training method is based on a unified constrained optimization formulation using a novel hybrid loss function that merges sparse learning with robust adversarial training. This training strategy dynamically adjusts inter-layer connectivity based on per-layer normalized momentum computed from the hybrid loss function. To further improve the robustness of the pruned models, we propose DNR++, an extension of the DNR method where we introduce the idea of sparse parametric Gaussian noise tensor that is added to the weight tensors to yield robust regularization. In contrast to existing robust pruning frameworks that require multiple training iterations, the proposed DNR and DNR++ achieve an overall target pruning ratio with only a single training iteration and can be tuned to support both irregular and structured channel pruning. To demonstrate the efficacy of the proposed method under the no-increased-training-time ""free""adversarial training scenario, we finally present FDNR++, a simple yet effective training modification that can yield robust yet compressed models requiring training time comparable to that of an unpruned non-adversarial training. To evaluate the merits of our disclosed training methods, experiments were performed with two widely accepted models, namely VGG16 and ResNet18, on CIFAR-10 and CIFAR-100 as well as with VGG16 on Tiny-ImageNet. Compared to the baseline uncompressed models, our methods provide over 20× compression on all the datasets without any significant drop of either clean or adversarial classification performance. Moreover, extensive experiments show that our methods consistently find compressed models with better clean and adversarial image classification performance than what is achievable through state-of-the-art alternatives. We provide insightful observations to help make various model, parameter density, and prune-type selection choices and have open-sourced our saved models and test codes to ensure reproducibility of our results.  © 2022 Copyright held by the owner/author(s).",adversarial robustness; model pruning; Pruning,Classification (of information); Constrained optimization; Deep neural networks; Gaussian noise (electronic); Iterative methods; Adversarial robustness; Classification performance; Dynamic network; Loss functions; Model pruning; Network rewiring; Non-iterative; Pruning; Training methods; Training time; Tensors
PhiNets: A Scalable Backbone for Low-power AI at the Edge,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146368006&doi=10.1145%2f3510832&partnerID=40&md5=f14be217d08204fdd044efebf2337ab1,"In the Internet of Things era, where we see many interconnected and heterogeneous mobile and fixed smart devices, distributing the intelligence from the cloud to the edge has become a necessity. Due to limited computational and communication capabilities, low memory and limited energy budget, bringing artificial intelligence algorithms to peripheral devices, such as end-nodes of a sensor network, is a challenging task and requires the design of innovative solutions. In this work, we present PhiNets, a new scalable backbone optimized for deep-learning-based image processing on resource-constrained platforms. PhiNets are based on inverted residual blocks specifically designed to decouple the computational cost, working memory, and parameter memory, thus exploiting all available resources for a given platform. With a YoloV2 detection head and Simple Online and Realtime Tracking (SORT), the proposed architecture achieves state-of-the-art results in (i) detection on the COCO and VOC2012 benchmarks, and (ii) tracking on the MOT15 benchmark. PhiNets obtain a reduction in parameter count of around 90% with respect to previous state-of-the-art models (EfficientNetv1, MobileNetv2) and achieve better performance with lower computational cost. Moreover, we demonstrate our approach on a prototype node based on an STM32H743 microcontroller (MCU) with 2 MB of internal Flash and 1MB of RAM and achieve power requirements in the order of 10 mW. The code for the PhiNets is publicly available on GitHub.1  © 2022 Association for Computing Machinery.",edge AI; Multi-object tracking; neural networks; Tiny ML,Budget control; Deep learning; Flash memory; Image processing; Random access storage; Sensor nodes; Tracking (position); Communication capabilities; Computational capability; Computational costs; Edge AI; Low Power; Multi-object tracking; Neural-networks; Smart devices; State of the art; Tiny ML; Constrained optimization
MaPHeA: A Framework for Lightweight Memory Hierarchy-aware Profile-guided Heap Allocation,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146439407&doi=10.1145%2f3527853&partnerID=40&md5=b2c9429775a276c1f47bcb06122321cf,"Hardware performance monitoring units (PMUs) are a standard feature in modern microprocessors, providing a rich set of microarchitectural event samplers. Recently, numerous profile-guided optimization (PGO) frameworks have exploited them to feature much lower profiling overhead compared to conventional instrumentation-based frameworks. However, existing PGO frameworks mainly focus on optimizing the layout of binaries; they overlook rich information provided by the PMU about data access behaviors over the memory hierarchy. Thus, we propose MaPHeA, a lightweight Memory hierarchy-aware Profile-guided Heap Allocation framework applicable to both HPC and embedded systems. MaPHeA guides and applies the optimized allocation of dynamically allocated heap objects with very low profiling overhead and without additional user intervention to improve application performance. To demonstrate the effectiveness of MaPHeA, we apply it to optimizing heap object allocation in an emerging DRAM-NVM heterogeneous memory system (HMS), selective huge-page utilization, and controlling the cacheability of the objects with the low temporal locality. In an HMS, by identifying and placing frequently accessed heap objects to the fast DRAM region, MaPHeA improves the performance of memory-intensive graph-processing and Redis workloads by 56.0% on average over the default configuration that uses DRAM as a hardware-managed cache of slow NVM. By identifying large heap objects that cause frequent TLB misses and allocating them to huge pages, MaPHeA increases the performance of the read and update operations of Redis by 10.6% over the transparent huge-page implementation of Linux. Also, by distinguishing the objects that cause cache pollution due to their low temporal locality and applying write-combining to them, MaPHeA improves the performance of STREAM and RADIX workloads by 20.0% on average over the system without cacheability control. © 2022 Association for Computing Machinery.",heap allocation; heterogeneous memory system; huge page; Profile-guided optimization,Computer operating systems; Embedded systems; Memory architecture; Heap allocation; Heterogeneous memory; Heterogeneous memory system; Huge page; Memory hierarchy; Memory systems; Optimisations; Performance; Performance-monitoring; Profile-guided optimization; Dynamic random access storage
Online Learning for Orchestration of Inference in Multi-user End-edge-cloud Networks,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136253055&doi=10.1145%2f3520129&partnerID=40&md5=22b125ecb1b90de716f547b7febd362b,"Deep-learning-based intelligent services have become prevalent in cyber-physical applications, including smart cities and health-care. Deploying deep-learning-based intelligence near the end-user enhances privacy protection, responsiveness, and reliability. Resource-constrained end-devices must be carefully managed to meet the latency and energy requirements of computationally intensive deep learning services. Collaborative end-edge-cloud computing for deep learning provides a range of performance and efficiency that can address application requirements through computation offloading. The decision to offload computation is a communication-computation co-optimization problem that varies with both system parameters (e.g., network condition) and workload characteristics (e.g., inputs). However, deep learning model optimization provides another source of tradeoff between latency and model accuracy. An end-to-end decision-making solution that considers such computation-communication problem is required to synergistically find the optimal offloading policy and model for deep learning services. To this end, we propose a reinforcement-learning-based computation offloading solution that learns optimal offloading policy considering deep learning model selection techniques to minimize response time while providing sufficient accuracy. We demonstrate the effectiveness of our solution for edge devices in an end-edge-cloud system and evaluate with a real-setup implementation using multiple AWS and ARM core configurations. Our solution provides 35% speedup in the average response time compared to the state-of-the-art with less than 0.9% accuracy reduction, demonstrating the promise of our online learning framework for orchestrating DL inference in end-edge-cloud systems. © 2022 Association for Computing Machinery.",computation offloading; Edge computing; neural network; online learning,Decision making; Deep learning; E-learning; Learning systems; Online systems; Reinforcement learning; Response time (computer systems); Cloud networks; Cloud systems; Computation offloading; Edge clouds; Edge computing; Learning models; Learning services; Multiusers; Neural-networks; Online learning; computation offloading
Brain-inspired Cognition in Next-generation Racetrack Memories,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146428368&doi=10.1145%2f3524071&partnerID=40&md5=1a2aa0dfa05b1e702365fdf5ae4a5330,"Hyperdimensional computing (HDC) is an emerging computational framework inspired by the brain that operates on vectors with thousands of dimensions to emulate cognition. Unlike conventional computational frameworks that operate on numbers, HDC, like the brain, uses high-dimensional random vectors and is capable of one-shot learning. HDC is based on a well-defined set of arithmetic operations and is highly error resilient. The core operations of HDC manipulate HD vectors in bulk bit-wise fashion, offering many opportunities to leverage parallelism. Unfortunately, on conventional von Neumann architectures, the continuous movement of HD vectors among the processor and the memory can make the cognition task prohibitively slow and energy intensive. Hardware accelerators only marginally improve related metrics. In contrast, even partial implementations of an HDC framework inside memory can provide considerable performance/energy gains as demonstrated in prior work using memristors. This article presents an architecture based on racetrack memory (RTM) to conduct and accelerate the entire HDC framework within memory. The proposed solution requires minimal additional CMOS circuitry by leveraging a read operation across multiple domains in RTMs called transverse read (TR) to realize exclusive-or (XOR) and addition operations. To minimize the CMOS circuitry overhead, an RTM nanowire-based counting mechanism is proposed. Using language recognition as the example workload, the proposed RTM HDC system reduces the energy consumption by 8.6× compared to the state-of-the-art in-memory implementation. Compared to dedicated hardware design realized with an FPGA, RTM-based HDC processing demonstrates 7.8× and 5.3× improvements in the overall runtime and energy consumption, respectively. © 2022 Association for Computing Machinery.",domain wall memory; embedded systems; High-dimensional computing; hyperdimensional computing; in-memory computing; language recognition; processing-in-memory; racetrack memory,Brain; CMOS integrated circuits; Embedded systems; Energy utilization; Memory architecture; Vectors; Domain wall memory; Embedded-system; High-dimensional; High-dimensional computing; Higher-dimensional; Hyperdimensional computing; In-memory computing; Language recognition; Processing-in-memory; Racetrack memory; Domain walls
CAP'NN: A Class-aware Framework for Personalized Neural Network Inference,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146371202&doi=10.1145%2f3520126&partnerID=40&md5=bb6ff241d960e3c070d9fcdbbe27526d,"We propose a framework for Class-aware Personalized Neural Network Inference (CAP'NN), which prunes an already-trained neural network model based on the preferences of individual users. Specifically, by adapting to the subset of output classes that each user is expected to encounter, CAP'NN is able to prune not only ineffectual neurons but also miseffectual neurons that confuse classification, without the need to retrain the network. CAP'NN also exploits the similarities among pruning requests from different users to minimize the timing overheads of pruning the network. To achieve this, we propose a clustering algorithm that groups similar classes in the network based on the firing rates of neurons for each class and then implement a lightweight cache architecture to store and reuse information from previously pruned networks. In our experiments with VGG-16, AlexNet, and ResNet-152 networks, CAP'NN achieves, on average, up to 47% model size reduction while actually improving the top-1(5) classification accuracy by up to 3.9%(3.4%) when the user only encounters a subset of the trained classes in these networks.  © 2022 Association for Computing Machinery.",Class-aware pruning; energy-efficient inference; personalized inference,Clustering algorithms; Energy efficiency; Neurons; Class-aware pruning; Energy efficient; Energy-efficient inference; Model-based OPC; Network inference; Network-based; Neural network model; Neural-networks; Personalized inference; Trained neural networks; Neural networks
Evaluating Controlled Memory Request Injection for Efficient Bandwidth Utilization and Predictable Execution in Heterogeneous SoCs,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146417376&doi=10.1145%2f3548773&partnerID=40&md5=e054b80ad60a7a1908801ebdd2565673,"High-performance embedded platforms are increasingly adopting heterogeneous systems-on-chip (HeSoC) that couple multi-core CPUs with accelerators such as GPU, FPGA, or AI engines. Adopting HeSoCs in the context of real-time workloads is not immediately possible, though, as contention on shared resources like the memory hierarchy - and in particular the main memory (DRAM) - causes unpredictable latency increase. To tackle this problem, both the research community and certification authorities mandate (i) that accesses from parallel threads to the shared system resources (typically, main memory) happen in a mutually exclusive manner by design, or (ii) that per-thread bandwidth regulation is enforced. Such arbitration schemes provide timing guarantees, but make poor use of the memory bandwidth available in a modern HeSoC. Controlled Memory Request Injection (CMRI) is a recently-proposed bandwidth limitation concept that builds on top of a mutually-exclusive schedule but still allows the threads currently not entitled to access memory to use as much of the unused bandwidth as possible without losing the timing guarantee. CMRI has been discussed in the context of a multi-core CPU, but the same principle applies also to a more complex system such as an HeSoC. In this article, we introduce two CMRI schemes suitable for HeSoCs: Voluntary Throttling via code refactoring and Bandwidth Regulation via dynamic throttling. We extensively characterize a proof-of-concept incarnation of both schemes on two HeSoCs: an NVIDIA Tegra TX2 and a Xilinx UltraScale+, highlighting the benefits and the costs of CMRI for synthetic workloads that model worst-case DRAM access. We also test the effectiveness of CMRI with real benchmarks, studying the effect of interference among the host CPU and the accelerators. © 2022 Association for Computing Machinery.",Heterogeneous systems-on-chip; memory interference; Predictable Execution,Bandwidth; Dynamic random access storage; Embedded systems; Memory architecture; Program processors; Band-width utilization; Efficient bandwidth; Heterogeneous system-on-chip; Heterogeneous systems; Main-memory; Memory interferences; Multi-cores; Performance; Predictable execution; Systems-on-Chip; System-on-chip
Introduction to the Special Issue on Accelerating AI on the Edge - Part 1,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146395940&doi=10.1145%2f3558078&partnerID=40&md5=c64dc275d542bc6f3ed5f6483eb89081,[No abstract available],,
"DyCo: Dynamic, Contextualized AI Models",2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146429916&doi=10.1145%2f3520131&partnerID=40&md5=038af5ab2b919cf01a158b06d9be0a1e,"Devices with limited computing resources use smaller AI models to achieve low-latency inferencing. However, model accuracy is typically much lower than the accuracy of a bigger model that is trained and deployed in places where the computing resources are relatively abundant. We describe DyCo, a novel system that ensures privacy of stream data and dynamically improves the accuracy of small models used in devices. Unlike knowledge distillation or federated learning, DyCo treats AI models as black boxes. DyCo uses a semi-supervised approach to leverage existing training frameworks and network model architectures to periodically train contextualized, smaller models for resource-constrained devices. DyCo uses a bigger, highly accurate model in the edge-cloud to auto-label data received from each sensor stream. Training in the edge-cloud (as opposed to the public cloud) ensures data privacy, and bespoke models for thousands of live data streams can be designed in parallel by using multiple edge-clouds. DyCo uses the auto-labeled data to periodically re-train, stream-specific, bespoke small models. To reduce the periodic training costs, DyCo uses different policies that are based on stride, accuracy, and confidence information.We evaluate our system, and the contextualized models, by using two object detection models for vehicles and people, and two datasets (a public benchmark and another real-world proprietary dataset). Our results show that DyCo increases the mAP accuracy measure of small models by an average of 16.3% (and up to 20%) for the public benchmark and an average of 19.0% (and up to 64.9%) for the real-world dataset. DyCo also decreases the training costs for contextualized models by more than an order of magnitude. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",contextualized; deep learning; edge cloud; edge computing; Object detector; semi-supervised learning,Data privacy; Deep learning; Distillation; Object detection; Object recognition; Computing resource; Contextualized; Deep learning; Edge clouds; Edge computing; Low latency; Object detectors; Resource use; Semi-supervised learning; Training costs; Edge computing
Design-Technology Co-Optimization for NVM-Based Neuromorphic Processing Elements,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131527397&doi=10.1145%2f3524068&partnerID=40&md5=9db73b6e6e665359db67787a3e7b481f,"An emerging use case of machine learning (ML) is to train a model on a high-performance system and deploy the trained model on energy-constrained embedded systems. Neuromorphic hardware platforms, which operate on principles of the biological brain, can significantly lower the energy overhead of an ML inference task, making these platforms an attractive solution for embedded ML systems. We present a design-technology tradeoff analysis to implement such inference tasks on the processing elements (PEs) of a non-volatile memory (NVM)-based neuromorphic hardware. Through detailed circuit-level simulations at scaled process technology nodes, we show the negative impact of technology scaling on the information-processing latency, which impacts the quality of service of an embedded ML system. At a finer granularity, the latency inside a PE depends on (1) the delay introduced by parasitic components on its current paths, and (2) the varying delay to sense different resistance states of its NVM cells. Based on these two observations, we make the following three contributions. First, on the technology front, we propose an optimization scheme where the NVM resistance state that takes the longest time to sense is set on current paths having the least delay, and vice versa, reducing the average PE latency, which improves the quality of service. Second, on the architecture front, we introduce isolation transistors within each PE to partition it into regions that can be individually power-gated, reducing both latency and energy. Finally, on the system-software front, we propose a mechanism to leverage the proposed technological and architectural enhancements when implementing an ML inference task on neuromorphic PEs of the hardware. Evaluations with a recent neuromorphic hardware architecture show that our proposed design-technology co-optimization approach improves both performance and energy efficiency of ML inference tasks without incurring high cost-per-bit. © 2022 Association for Computing Machinery.",design-technology co-optimization (DTCO); Neuromorphic computing; non-volatile memory (NVM); oxide-based resistive random access memory (OxRRAM),Computer aided design; Embedded systems; Energy efficiency; Memory architecture; Nonvolatile storage; Co-optimization; Design technologies; Design-technology co-optimization; Machine-learning; Neuromorphic computing; Non-volatile memory; Oxide-based resistive random access memory; Processing elements; Random access memory; Quality of service
Formally Verified Loop-Invariant Code Motion and Assorted Optimizations,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134950245&doi=10.1145%2f3529507&partnerID=40&md5=a061768cd1e2ab0a2f8c8950e6a2712c,"We present an approach for implementing a formally certified loop-invariant code motion optimization by composing an unrolling pass and a formally certified yet efficient global subexpression elimination. This approach is lightweight: each pass comes with a simple and independent proof of correctness. Experiments show the approach significantly narrows the performance gap between the CompCert certified compiler and state-of-the-art optimizing compilers. Our static analysis employs an efficient yet verified hashed set structure, resulting in the fast compilation. © 2022 Copyright held by the owner/author(s).",common subexpression elimination; CompCert; Verified compilation,Codes (symbols); Program compilers; Common subexpression elimination; Compcert; Loop-invariant code motion; Motion optimization; Optimisations; Performance gaps; Proof of correctness; Simple++; Sub-expressions; Verified compilation; Static analysis
Synaptic Activity and Hardware Footprint of Spiking Neural Networks in Digital Neuromorphic Systems,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131045134&doi=10.1145%2f3520133&partnerID=40&md5=9ac2af19fa08fda0b3376f7a776a2884,"Spiking neural networks are expected to bring high resources, power, and energy efficiency to machine learning hardware implementations. In this regard, they could facilitate the integration of Artificial Intelligence in highly constrained embedded systems, such as image classification in drones or satellites. If their logic resource efficiency is widely accepted in the literature, their energy efficiency still remains debated. In this article, a novel high-level metric is used to characterize the expected energy efficiency gain when using Spiking Neural Networks (SNN) instead of Formal Neural Networks (FNN) for hardware implementation: Synaptic Activity Ratio (SAR). This metric is applied to a selection of classification tasks including images and 1D signals. Moreover, a high-level estimator for logic resources, power usage, execution time, and energy is introduced for neural network hardware implementations on FPGA, based on four existing accelerator architectures covering both sequential and parallel implementation paradigms for both spiking and formal coding domains. This estimator is used to evaluate the reliability of the Synaptic Activity Ratio metric to characterize spiking neural network energy efficiency gain on the proposed dataset benchmark. This study led to the conclusion that spiking domain offers significant power and energy savings in sequential implementations. This study also shows that synaptic activity is a critical factor that must be taken into account when addressing low-energy systems. © 2022 Association for Computing Machinery.",energy modeling; high-level estimations; Spiking neural networks,Computer circuits; Computing power; Embedded systems; Image classification; Low power electronics; Neural networks; Activity ratios; Efficiency gain; Energy model; Hardware implementations; High-level estimation; Logic resources; Neural-networks; Neuromorphic systems; Sequential implementation; Spiking neural network; Energy efficiency
Energy-efficient and Reliable Inference in Nonvolatile Memory under Extreme Operating Conditions,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144124951&doi=10.1145%2f3520130&partnerID=40&md5=7e90e78058cb4d1b34e23a516b403a50,"Beyond-edge devices can operate outside the reach of the power grid and without batteries. Such devices can be deployed in large numbers in regions that are difficult to access. Using machine learning, these devices can solve complex problems and relay valuable information back to a host. Many such devices deployed in low Earth orbit can even be used as nanosatellites. Due to the harsh and unpredictable nature of the environment, these devices must be highly energy-efficient, be capable of operating intermittently over a wide temperature range, and be tolerant of radiation. Here, we propose a non-volatile processing-in-memory architecture that is extremely energy-efficient, supports minimal overhead checkpointing for intermittent computing, can operate in a wide range of temperatures, and has a natural resilience to radiation.  © 2022 Association for Computing Machinery.",beyond-edge computing; Processing in memory,Energy efficiency; Memory architecture; Nanosatellites; Orbits; Beyond-edge computing; Complex problems; Edge computing; Energy efficient; Machine-learning; Non-volatile memory; Nonvolatile memory; Operating condition; Power grids; Processing-in-memory; Edge computing
Cache Abstraction for Data Race Detection in Heterogeneous Systems with Non-coherent Accelerators,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146440281&doi=10.1145%2f3535457&partnerID=40&md5=54aad37d14d38756046603725febd689,"Embedded systems are becoming increasingly complex and heterogeneous, featuring multiple processor cores (which might themselves be heterogeneous) as well as specialized hardware accelerators, all accessing shared memory. Many accelerators are non-coherent (i.e., do not support hardware cache coherence) because it reduces hardware complexity, cost, and power consumption, while potentially offering superior performance. However, the disadvantage of non-coherence is that the software must explicitly synchronize between accelerators and processors, and this synchronization is notoriously error-prone. We propose an analysis technique to find data races in software for heterogeneous systems that include non-coherent accelerators. Our approach builds on classical results for data race detection, but the challenge turns out to be analyzing cache behavior rather than the behavior of the non-coherent accelerators. Accordingly, our central contribution is a novel, sound (data-race-preserving) abstraction of cache behavior. We prove our abstraction sound, and then to demonstrate the precision of our abstraction, we implement it in a simple dynamic race detector for a system with a processor and a massively parallel accelerator provided by a commercial FPGA-based accelerator vendor. On eleven software examples provided by the vendor, the tool had zero false positives and was able to detect previously unknown data races in two of the 11 examples. © 2022 Copyright held by the owner/author(s).",caching; Data race; hardware accelerator; memory coherence,Cache memory; Embedded systems; Memory architecture; Cache behavior; Caching; Data race detection; Data races; Embedded-system; Hardware accelerators; Heterogeneous systems; Memory coherence; Multiple processors; Processor cores; Abstracting
A Fall Detection Network by 2D/3D Spatio-temporal Joint Models with Tensor Compression on Edge,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146433229&doi=10.1145%2f3531004&partnerID=40&md5=c04c47816335bad7683304af2f6f22af,"Falling is ranked highly among the threats in elderly healthcare, which promotes the development of automatic fall detection systems with extensive concern. With the fast development of the Internet of Things (IoT) and Artificial Intelligence (AI), camera vision-based solutions have drawn much attention for single-frame prediction and video understanding on fall detection in the elderly by using Convolutional Neural Network (CNN) and 3D-CNN, respectively. However, these methods hardly supervise the intermediate features with good accurate and efficient performance on edge devices, which makes the system difficult to be applied in practice. This work introduces a fast and lightweight video fall detection network based on a spatio-temporal joint-point model to overcome these hurdles. Instead of detecting fall motion by the traditional CNNs, we propose a Long Short-Term Memory (LSTM) model based on time-series joint-point features extracted from a pose extractor. We also introduce the increasingly mature RGB-D camera and propose 3D pose estimation network to further improve the accuracy of the system. We propose to apply tensor train decomposition on the model to reduce storage and computational consumption so the deployment on edge devices can to realized. Experiments are conducted to verify the proposed framework. For fall detection task, the proposed video fall detection framework achieves a high sensitivity of 98.46% on Multiple Cameras Fall, 100% on UR Fall, and 98.01% on NTU RGB-D 120. For pose estimation task, our 2D model attains 73.3 mAP in the COCO keypoint challenge, which outperforms the OpenPose by 8%. Our 3D model attains 78.6% mAP on NTU RGB-D dataset with 3.6× faster speed than OpenPose. © 2022 Association for Computing Machinery.",2D/3D pose estimation; Fall detection system on board; spatio-temporal joint-point model; tensorized compression,3D modeling; Cameras; Convolutional neural networks; Internet of things; Long short-term memory; Tensors; 2D-3D pose estimation; Convolutional neural network; Detection networks; Detection system; Fall detection; Fall detection system on board; Point models; Spatio-temporal; Spatio-temporal joint-point model; Tensorized compression; Fall detection
Introduction to the Special Issue on Accelerating AI on the Edge - Part 2,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146441089&doi=10.1145%2f3563127&partnerID=40&md5=31ac38cde9984f7afbe5846e964d90d2,[No abstract available],,
WasmAndroid: A Cross-Platform Runtime for Native Programming Languages on Android,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146425349&doi=10.1145%2f3530286&partnerID=40&md5=dce0d32b0189cec596cf498ce93387ed,"Open source hardware such as RISC-V has been gaining substantial momentum. Recently, they have begun to embrace Google's Android operating system to leverage its software ecosystem. Despite the encouraging progress, a challenging issue arises: a majority of Android applications are written in native languages and need to be recompiled to target new hardware platforms. Unfortunately, this recompilation process is not scalable because of the explosion of new hardware platforms. To address this issue, we present WasmAndroid, a high-performance cross-platform runtime for native Android applications. With WasmAndroid, developers can compile their source code to WebAssembly, an efficient and portable bytecode format that can be executed everywhere without additional reconfiguration. Developers can also transpile existing application binaries to WebAssembly when source code is not available. WebAssembly's language model is very different from other common languages. This mismatch leads to many unique implementation challenges. In this article, we provide workable solutions and conduct a thorough system evaluation. We show that WasmAndroid provides acceptable performance to execute native applications in a cross-platform manner. © 2022 Association for Computing Machinery.",Android; cross-platform; open source hardware; WebAssembly,Open source software; Open systems; Android; Android applications; Cross-platform; Google+; Hardware platform; Native programming; Open-source hardwares; Runtimes; Source codes; Webassembly; Android (operating system)
A Write-Related and Read-Related DRAM Allocation Strategy Inside Solid-State Drives (SSDs),2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146422643&doi=10.1145%2f3561301&partnerID=40&md5=005a939df8bc7b365e9fc73d2767a676,"Although NAND flash memory has the advantages of small size, low-power consumption, shock resistance, and fast access speed, NAND flash memory still faces the problems of ""out-of-place updates,""""garbage collection,""and ""unbalanced execution time""due to its hardware limitations. Usually, a flash translation layer (FTL) can maintain the mapping cache (in limited DRAM space) to store the frequently accessed address mapping for ""out-of-place updates""and maintain the read/write buffer (in limited DRAM space) to store the frequently accessed data for ""garbage collection""and ""unbalanced execution time"". In this article, we will propose a write-related and read-related DRAM allocation strategy inside solid-state drives (SSDs). The design idea behind the write-related DRAM allocation method is to calculate the suitable DRAM allocation for the write buffer and the write mapping cache by building a statistical model with a minimum expected value of writes for NAND flash memory. To further reduce reads in NAND flash memory, the design idea behind the read-related DRAM allocation method is to adopt a cost-benefit policy to reallocate the proper DRAM space from the write buffer and the write mapping cache to the read buffer and the read mapping cache, respectively. According to the experimental results, we can demonstrate that the proposed write-related and read-related DRAM allocation strategy can reduce more reads/writes in NAND flash memory than other methods to improve the response time. © 2022 Association for Computing Machinery.",dynamic DRAM allocation; flash translation layer; NAND flash memory,Cache memory; Cost benefit analysis; Flash-based SSDs; Integrated circuit design; Mapping; Memory architecture; NAND circuits; Refuse collection; Allocation methods; Allocation strategy; Design ideas; Dynamic DRAM allocation; Flash translation layer; Garbage collection; Low-power consumption; Lower-power consumption; Mapping caches; NAND flash memory; Dynamic random access storage
LanCeX: A Versatile and Lightweight Defense Method against Condensed Adversarial Attacks in Image and Audio Recognition,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146423456&doi=10.1145%2f3555375&partnerID=40&md5=7f7854f04bbfc4d779e63e15694832c7,"Convolutional Neural Networks (CNNs) are widely deployed in various embedded recognition applications. However, they demonstrate a considerable vulnerability to adversarial attacks, which leverage the well-designed perturbations to mislead the recognition results. Recently, for easier perturbation injection and higher attack effectiveness, the adversarial perturbations have been concentrated into a small area with various types and different data modalities. When defending such condensed adversarial attacks on the embedded recognition scenarios, most of the existing defense works highlight two critical issues. First, they are particularly designed for each individual condensed attack scenario, lacking enough versatility to accommodate attacks with different data modalities. Second, they rely on computation-intensive preprocessing techniques, which is impractical for time-sensitive embedded recognition scenarios. In this article, we propose LanCeX-a versatile and lightweight CNN defense solution against condensed adversarial attacks. By examining the CNN's intrinsic vulnerability, we first identify the common attacking mechanism behind condensed adversarial attacks across different data modalities. Based on this mechanism, LanCeX can defend against various condensed attacks with the optimal computation workload in different recognition scenarios. Experiments show that LanCeX can achieve an average 91%, 85%, and 90% detection success rate and optimal adversarial mitigation performance in three recognition scenarios, respectively: image classification, object detection, and audio recognition. Moreover, LanCeX is at most 3× faster compared with the state-of-the-art defense methods, making it feasible to use with resource-constrained embedded systems. © 2022 Association for Computing Machinery.",Convolutional Neural Networks (CNNs); image classification; object detection; physical adversarial attack; voice recognition,Convolution; Convolutional neural networks; Embedded systems; Image classification; Network security; Object recognition; Attacks scenarios; Audio-recognition; Computation intensives; Convolutional neural network; Critical issues; Images classification; Objects detection; Physical adversarial attack; Small area; Object detection
An Efficient and Flexible Stochastic CGRA Mapping Approach,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146421970&doi=10.1145%2f3550071&partnerID=40&md5=784c4ec0ada94fbb688a661e9bed74d8,"Coarse-Grained Reconfigurable Array (CGRA) architectures are promising high-performance and power-efficient platforms. However, mapping applications efficiently on CGRA is a challenging task. This is known to be an NP complete problem. Hence, finding good mapping solutions for a given CGRA architecture within a reasonable time is complex. Additionally, finding scalability in compilation time and memory footprint for large heterogeneous CGRAs is also a well known problem. In this article, we present a stochastic mapping approach that can efficiently explore the architecture space and allows finding best of solutions while having limited and steady use of memory footprint. Experimental results show that our compilation flow allows to reach performances with low-complexity CGRA architectures that are as good as those obtained with more complex ones thanks to the better exploration of the mapping solution space. Parameters considered in our experiments are number of tiles, Register File (RF) size, number of load/store (LS) units, network topologies, and so on. Our results demonstrate that high-quality compilation for a wide range of applications is possible within reasonable run-times. Experiments with several DSP benchmarks show that the best CGRA configuration from the architectural exploration surpasses an ultra low-power DSP optimized RISC-V CPU to achieve up to 15.28× (with an average of 6× and minimum of 3.4×) performance gain and 29.7× (with an average of 13.5× and minimum of 6.3×) energy gain with an area overhead of 1.5× only. © 2022 Association for Computing Machinery.",CGRA; mapping,Benchmarking; Complex networks; Computational complexity; Memory architecture; Network architecture; Network topology; Reconfigurable architectures; Stochastic systems; Array architecture; Array mapping; Coarse-grained reconfigurable arrays; High power; Mapping applications; Memory footprint; Performance; Power efficient; Stochastic mapping; Stochastics; Mapping
PISCOT: A Pipelined Split-Transaction COTS-Coherent Bus for Multi-Core Real-Time Systems,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146437817&doi=10.1145%2f3556975&partnerID=40&md5=1cd4dfe3296f4667821e458c6020effb,"Tasks in modern embedded systems such as automotive and avionics communicate among each other using shared data towards achieving the desired functionality of the whole system. In commodity platforms, cores communicate data through the shared memory hierarchy and correctness is maintained by a cache coherence protocol. Recent works investigated the deployment of coherence protocols in real-time systems and showed significant performance improvements. Nonetheless, we find these works to require modifications to commodity coherence protocols, assume simple in-order pipelines, and most importantly suffer from significant latency delays due to coherence interference along with average performance degradation. In this work, we propose PISCOT: a predictable and coherent bus architecture that (i) provides a considerably tighter bound compared to the state-of-the-art predictable coherent solutions (4× tighter bounds in a quad-core system). (ii) It does so with a negligible performance loss compared to conventional high-performance architecture coherence delays (less than 4% for SPLASH-3 benchmarks). This improves average performance by up to 5× (2.8× on average) compared to its predictable coherence counterpart. Finally, (iii) it achieves that without requiring any modifications to conventional coherence protocols. We show this by integrating PISCOT on top of two protocols with a detailed implementation with complete transient states: MSI and MESI. © 2022 Association for Computing Machinery.",Datasets; gaze detection; neural networks; text tagging,Benchmarking; Cache memory; Embedded systems; Interactive computer systems; Memory architecture; Network architecture; Pipelines; Coherence protocol; Coherent bus; Dataset; Gaze detection; Multi-cores; Neural-networks; Performance; Real - Time system; Text tagging; Tight bound; Real time systems
Formally Verified Next-generation Airborne Collision Avoidance Games in ACAS X,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146432473&doi=10.1145%2f3544970&partnerID=40&md5=ee9b11a89cab9608cac342884f80a328,"The design of aircraft collision avoidance algorithms is a subtle but important challenge that merits the need for provable safety guarantees. Obtaining such guarantees is nontrivial given the unpredictability of the interplay of the intruder aircraft decisions, the ownship pilot reactions, and the subtlety of the continuous motion dynamics of aircraft. Existing collision avoidance systems, such as TCAS and the Next-Generation Airborne Collision Avoidance System ACAS X, have been analyzed assuming severe restrictions on the intruder's flight maneuvers, limiting their safety guarantees in real-world scenarios where the intruder may change its course. This work takes a conceptually significant and practically relevant departure from existing ACAS X models by generalizing them to hybrid games with first-class representations of the ownship and intruder decisions coming from two independent players, enabling significantly advanced predictive power. By proving the existence of winning strategies for the resulting Adversarial ACAS X in differential game logic, collision-freedom is established for the rich encounters of ownship and intruder aircraft with independent decisions along differential equations for flight paths with evolving vertical/horizontal velocities. We present three classes of models of increasing complexity: single-advisory infinite-time models, bounded time models, and infinite time, multi-advisory models. Within each class of models, we identify symbolic conditions and prove that there then always is a possible ownship maneuver that will prevent a collision between the two aircraft. © 2022 Copyright held by the owner/author(s).",ACAS X; Airborne Collision Avoidance; differential game logic; hybrid games; theorem proving,Aircraft; Aircraft accidents; Computer circuits; Differential equations; Free flight; Game theory; ACAS X; Airborne collision avoidance; Airborne collisions; Collisions avoidance; Differential game logic; Differential games; Hybrid game; Infinite time; Safety guarantees; Time modeling; Collision avoidance
A Framework for Neural Network Architecture and Compile Co-optimization,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146423376&doi=10.1145%2f3533251&partnerID=40&md5=a2eac00add841b0a19e007d147d541fa,"The efficiency of deep neural network (DNN) solutions on real hardware devices are mainly decided by the DNN architecture and the compiler-level scheduling strategy on the hardware. When we try to fully exploit the underlying hardware and obtain the optimal tradeoff between DNN accuracy and runtime performance, we discovered that the two optimization goals of DNN architecture and scheduling policy are intimately related to each other. However, current hardware-aware Neural Architecture Search (NAS) methods primarily focus on the DNN architecture search process, ignoring the effects of various compiler-level scheduling strategies (e.g., graph-level optimization, loop transformations, parallelization, etc.) on network candidates being evaluated in the search process. As a result, they may overlook the true-optimal DNN implementations on hardware, which can only be discovered by trying-out different combinations of scheduling strategies and DNN architectures. This work proposes a NAS framework (CHaNAS) that searches for not only the network architecture but also the dedicated compiler-level scheduling policy, as the optimal co-design solution on the target hardware. We propose to use a block-based pre-scheduling methodology to reduce the co-design search space and enable the automatic generation of the optimal co-design, including the network architecture and the tensor programs that practice the scheduling policy. Further, we introduce a new search objective function based on the generalization gap to prevent the selection of architectures that are prone to overfitting. We evaluate CHaNAS on Imagenet on different hardware back-ends against the state-of-the-art hardware-aware search method based on the MobileNet-v3 search space. Experimental results show that the co-design solutions obtained by ChaNAS show up to 1.6×, 1.9×, and 1.7×, 24 performance boost on NVIDIA P100 GPU, Intel Xeon 8163 CPU, and Samsung Note 10 Mobile, respectively, over the baselines of the same-level accuracy. © 2022 Association for Computing Machinery.",compiler optimization; DNN-scheduling Co-design; hardware-aware neural architecture search,Network architecture; Program compilers; Co-designs; Compiler optimizations; Deep neural network-scheduling co-design; Hardware-aware neural architecture search; Network scheduling; Neural architectures; Neural network architecture; Scheduling policies; Scheduling strategies; Search method; Deep neural networks
Deep Ensemble Learning for Human Activity Recognition Using Wearable Sensors via Filter Activation,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146436065&doi=10.1145%2f3551486&partnerID=40&md5=2c78b20ad7ed3f0adc631aee35c30ec1,"During the past decade, human activity recognition (HAR) using wearable sensors has become a new research hot spot due to its extensive use in various application domains such as healthcare, fitness, smart homes, and eldercare. Deep neural networks, especially convolutional neural networks (CNNs), have gained a lot of attention in HAR scenario. Despite exceptional performance, CNNs with heavy overhead is not the best option for HAR task due to the limitation of computing resource on embedded devices. As far as we know, there are many invalid filters in CNN that contribute very little to output. Simply pruning these invalid filters could effectively accelerate CNNs, but it inevitably hurts performance. In this article, we first propose a novel CNN for HAR that uses filter activation. In comparison with filter pruning that is motivated for efficient consideration, filter activation aims to activate these invalid filters from an accuracy boosting perspective. We perform extensive experiments on several public HAR datasets, namely, UCI-HAR (UCI), OPPORTUNITY (OPPO), UniMiB-SHAR (Uni), PAMAP2 (PAM2), WISDM (WIS), and USC-HAD (USC), which show the superiority of the proposed method against existing state-of-the-art (SOTA) approaches. Ablation studies are conducted to analyze its internal mechanism. Finally, the inference speed and power consumption are evaluated on an embedded Raspberry Pi Model 3 B plus platform. © 2022 Association for Computing Machinery.",convolutional neural network; deep learning; filter activation; human activity recognition; Sensor,Automation; Convolution; Convolutional neural networks; Deep neural networks; Intelligent buildings; Pattern recognition; Wearable sensors; Applications domains; Convolutional neural network; Deep learning; Eldercare; Ensemble learning; Filter activation; Hotspots; Human activity recognition; Performance; Smart homes; Chemical activation
Resource-demand Estimation for Edge Tensor Processing Units,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143832511&doi=10.1145%2f3520132&partnerID=40&md5=6910b19a8c4485de2c653b0b39e7e49f,"Machine learning has shown tremendous success in a large variety of applications. The evolution of machine-learning applications from cloud-based systems to mobile and embedded devices has shifted the focus from only quality-related aspects towards the resource demand of machine learning. For embedded systems, dedicated accelerator hardware promises the energy-efficient execution of neural network inferences. Their precise resource demand in terms of execution time and power demand, however, is undocumented. Developers, therefore, face the challenge to fine-tune their neural networks such that their resource demand matches the available budgets. This article presents Precious, a comprehensive approach to estimate the resource demand of an embedded neural network accelerator. We generate randomised neural networks, analyse them statically, execute them on an embedded accelerator while measuring their actual power draw and execution time, and train estimators that map the statically analysed neural network properties to the measured resource demand. In addition, this article provides an in-depth analysis of the neural networks' resource demands and the responsible network properties. We demonstrate that the estimation error of Precious can be below 1.5% for both power draw and execution time. Furthermore, we discuss what estimator accuracy is practically achievable and how much effort is required to achieve sufficient accuracy.  © 2022 Copyright held by the owner/author(s).",Neural network accelerator; resource awareness,Budget control; Embedded systems; Energy efficiency; Demand estimation; Machine learning applications; Machine-learning; Network properties; Neural network accelerator; Neural-networks; Power draw; Processing units; Resource awareness; Resource demands; Machine learning
"TAB: Unified and Optimized Ternary, Binary, and Mixed-precision Neural Network Inference on the Edge",2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140122344&doi=10.1145%2f3508390&partnerID=40&md5=439697d79744e26f25ab2f8d7e1fc8d4,"Ternary Neural Networks (TNNs) and mixed-precision Ternary Binary Networks (TBNs) have demonstrated higher accuracy compared to Binary Neural Networks (BNNs) while providing fast, low-power, and memory-efficient inference. Related works have improved the accuracy of TNNs and TBNs, but overlooked their optimizations on CPU and GPU platforms. First, there is no unified encoding for the binary and ternary values in TNNs and TBNs. Second, existing works store the 2-bit quantized data sequentially in 32/64-bit integers, resulting in bit-extraction overhead. Last, adopting standard 2-bit multiplications for ternary values leads to a complex computation pipeline, and efficient mixed-precision multiplication between ternary and binary values is unavailable. In this article, we propose TAB as a unified and optimized inference method for ternary, binary, and mixed-precision neural networks. TAB includes unified value representation, efficient data storage scheme and novel bitwise dot product pipelines on CPU/GPU platforms. We adopt signed integers for consistent value representation across binary and ternary values. We introduce a bitwidth-last data format that stores the first and second bits of the ternary values separately to remove the bit extraction overhead. We design the ternary and binary bitwise dot product pipelines based on Gated-XOR using up to 40% fewer operations than State-Of-The-Art (SOTA) methods. Theoretical speedup analysis shows that our proposed TAB-TNN is 2.3× fast as the SOTA ternary method RTN, 9.8× fast as 8-bit integer quantization (INT8), and 39.4× fast as 32-bit full-precision convolution (FP32). Experiment results on CPU and GPU platforms show that our TAB-TNN has achieved up to 34.6× speedup and 16× storage size reduction compared with FP32 layers. TBN, Binary-activation Ternary-weight Network (BTN), and BNN in TAB are up to 40.7×, 56.2×, and 72.2× as fast as FP32. TAB-TNN is up to 70.1% faster and 12.8% more power-efficient than RTN on Darknet-19 while keeping the same accuracy. TAB is open source as a PyTorch Extension1 for easy integration with existing CNN models. © 2022 Association for Computing Machinery.",binary neural networks; edge computing; Ternary neural networks,Digital storage; Extraction; Graphics processing unit; Low power electronics; Pipelines; Product design; Binary neural networks; Binary precision; Bit extraction; Bit integers; Edge computing; Mixed precision; Neural-networks; Power efficient; Products pipeline; Ternary neural network; Edge computing
OnSRAM: Efficient Inter-Node On-Chip Scratchpad Management in Deep Learning Accelerators,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146425880&doi=10.1145%2f3530909&partnerID=40&md5=cda1143c47b10ca5c5b776ad32b1e62d,"Hardware acceleration of Artificial Intelligence (AI) workloads has gained widespread popularity with its potential to deliver unprecedented performance and efficiency. An important challenge remains in how AI accelerators are programmed to sustain high utilization without impacting end-user productivity. Prior software optimizations start with an input graph and focus on node-level optimizations, viz. dataflows and hierarchical tiling, and graph-level optimizations such as operation fusion. However, little effort has been devoted to inter-node on-chip scratchpad memory (SPM) management in Deep Learning (DL) accelerators, whose significance is bolstered by the recent trends in complex network topologies and the emergence of eager execution in DL frameworks.We characterize and show that there exists up to a 5.2× performance gap in DL inference to be bridged using SPM management and propose OnSRAM, a novel SPM management framework integrated with the compiler runtime of a DL accelerator. We develop two variants, viz. OnSRAM-Static, which works on static graphs to identify data structures that can be lucratively held on-chip based on their size, liveness and significance, and OnSRAM-Eager, which targets an eager execution model (no graph) and uses a history-based speculative scheme to hold/discard data structures. We integrate OnSRAM with TensorFlow and analyze it on multiple accelerator configurations. Across a suite of 12 images, objects, and language networks, on a 3 TFLOP system with a 2 MB SPM and 32 GBps external memory bandwidth, OnSRAM-Static and OnSRAM-Eager achieve 1.02-4.8× and 1.02-3.1× reduction in inference latency (batch size of 1), over a baseline with no SPM management. In terms of energy savings, we observe average reductions of 1.51× (up to 4.1×) and 1.23× (up to 2.9×) for the static and eager execution scenarios, respectively. © 2022 Association for Computing Machinery.",Artificial intelligence; compiler; hardware acceleration; memory management; neural networks,Complex networks; Data flow analysis; Data structures; Deep learning; Energy conservation; Graph theory; Memory architecture; Neural networks; % reductions; Compiler; Hardware acceleration; Memory-management; Neural-networks; On chips; Optimisations; Performance; Scratch-pad memory managements; Scratchpad; Program compilers
K-Periodic Scheduling for Throughput-Buffering Trade-Off Exploration of CSDF,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146417579&doi=10.1145%2f3559760&partnerID=40&md5=3c4fbde20c235a8f52d87dd9e99e8b69,"The design of time-critical embedded systems often requires static models of computation such as cyclo-static dataflow. These models enable performance guarantees, execution correctness, and optimized memory usage. Nonetheless, determining optimal buffer sizing of dataflow applications remains difficult: existing methods offer either approximate solutions or fail to provide solutions for complex instances. We propose a throughput-buffering trade-off exploration that uses K-periodic scheduling to direct a design-space exploration - providing optimal solutions while significantly reducing the search space compared to existing methodologies. We compare this strategy against previous approaches and demonstrate search-space reductions over two benchmark suites, resulting in significant improvements in computation times while retaining optimal results. © 2022 Copyright held by the owner/author(s).",Dataflow; static analysis,Data flow analysis; Embedded systems; Static analysis; Cyclo-static dataflow; Dataflow; Embedded-system; Execution correctness; Model of computation; Performance guarantees; Periodic scheduling; Static modelling; Time-critical; Trade off; Economic and social effects
Edge-SLAM: Edge-Assisted Visual Simultaneous Localization and Mapping,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146416247&doi=10.1145%2f3561972&partnerID=40&md5=cb45e1377bcccd561f6dc1218d99c258,"Localization in urban environments is becoming increasingly important and used in tools such as ARCore [18], ARKit [34] and others. One popular mechanism to achieve accurate indoor localization and a map of the space is using Visual Simultaneous Localization and Mapping (Visual-SLAM). However, Visual-SLAM is known to be resource-intensive in memory and processing time. Furthermore, some of the operations grow in complexity over time, making it challenging to run on mobile devices continuously. Edge computing provides additional compute and memory resources to mobile devices to allow offloading tasks without the large latencies seen when offloading to the cloud. In this article, we present Edge-SLAM, a system that uses edge computing resources to offload parts of Visual-SLAM. We use ORB-SLAM2 [50] as a prototypical Visual-SLAM system and modify it to a split architecture between the edge and the mobile device. We keep the tracking computation on the mobile device and move the rest of the computation, i.e., local mapping and loop closing, to the edge. We describe the design choices in this effort and implement them in our prototype. Our results show that our split architecture can allow the functioning of the Visual-SLAM system long-term with limited resources without affecting the accuracy of operation. It also keeps the computation and memory cost on the mobile device constant, which would allow for the deployment of other end applications that use Visual-SLAM. We perform a detailed performance and resources use (CPU, memory, network, and power) analysis to fully understand the effect of our proposed split architecture. © 2022 Association for Computing Machinery.",concurrency; edge computing; localization; mapping; mobile systems; split architecture; Visual simultaneous localization and mapping,computation offloading; Deep learning; Indoor positioning systems; Memory architecture; Mobile edge computing; Network architecture; Robotics; Concurrency; Edge computing; Indoor localization; Localisation; Localisation Systems; Mapping systems; Mobile systems; Split architectures; Urban environments; Visual simultaneous localization and mappings; Mapping
Hardware Trojan Detection using Transition Probability with Minimal Test Vectors,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146418821&doi=10.1145%2f3545000&partnerID=40&md5=83f4fc7f14b631975f5d7e0bfaf4840e,"Hardware Trojans (HTs) are malicious manipulations of the standard functionality of an integrated circuit (IC). Sophisticated defense against HT attacks has become the utmost current research endeavor. In particular, the HTs whose operations depend on the rare activation condition are the most critical ones. Among other techniques, logic test by rare net excitation is advocated as one of the viable detection methods due to no extra hardware requirement. However, logic test faces a tremendous challenge of the overhead of testing configuration. This work presents a methodology based on the primary input's impact over rare nets using transition probability to select the useful test vectors. To generate a test vector, each input's toggle probability is calculated, which drastically minimizes the search space. The capability of rare-signal generation selects the final list of test vectors. Simulations performed in the presence of different HT triggers on different benchmark circuits, like ISCAS '85, ISCAS '89, and ITC '99, show that the proposed methodology is capable of producing test vectors with significantly improved rare net coverage. Furthermore, compared to an existing technique, the proposed methodology produces average higher rare switching (around 72%) inside a netlist. © 2022 Association for Computing Machinery.",logic test; N-detect Test; Transition probability,Computer circuits; Integrated circuits; Malware; Vector spaces; Vectors; 'current; Activation conditions; Detection methods; Hardware Trojan detection; Logic test; N-detect test; Primary inputs; Test vectors; Testing configurations; Transition probabilities; Hardware security
Contention Grading and Adaptive Model Selection for Machine Vision in Embedded Systems,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138493737&doi=10.1145%2f3520134&partnerID=40&md5=356baa8770f56b7d32f52ad69c0abf8c,"Real-time machine vision applications running on resource-constrained embedded systems face challenges for maintaining performance. An especially challenging scenario arises when multiple applications execute at the same time, creating contention for the computational resources of the system. This contention results in increase in inference delay of the machine vision applications, which can be unacceptable for time-critical tasks. To address this challenge, we propose an adaptive model selection framework that mitigates the impact of system contention and prevents unexpected increases in inference delay by trading off the application accuracy minimally. The framework has two parts, which are performed pre-deployment and at runtime. The pre-deployment part profiles the system for contention in a black-box manner and produces a model set that is specifically optimized for the contention levels observed in the system. The runtime part predicts the inference delays of each model considering the system contention and selects the best model according to the predictions for each frame. Compared to a fixed individual model with similar accuracy, our framework improves the performance by significantly reducing the inference delay violations against a specified threshold. We implement our framework on the Nvidia Jetson TX2 platform and show that our approach achieves greater than 20% reductions in delay violations over the individual baseline models.  © 2022 Association for Computing Machinery.",Adaptive computing; DNN model selection; resource contention,Embedded systems; Grading; Real time systems; Adaptive computing; Adaptive model selection; Delay violation; DNN model selection; Machine-vision; Model Selection; Performance; Resource contention; Runtimes; Vision applications; Computer vision
"Edge Intelligence: Concepts, Architectures, Applications, and Future Directions",2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134883880&doi=10.1145%2f3486674&partnerID=40&md5=bb0d78db839085f00ef8cfc60f987b9a,"The name edge intelligence, also known as Edge AI, is a recent term used in the past few years to refer to the confluence of machine learning, or broadly speaking artificial intelligence, with edge computing. In this article, we revise the concepts regarding edge intelligence, such as cloud, edge, and fog computing, the motivation to use edge intelligence, and compare current approaches and analyze application scenarios. To provide a complete review of this technology, previous frameworks and platforms for edge computing have been discussed in this work to provide the general view of the basis for Edge AI. Similarly, the emerging techniques to deploy deep learning models at the network edge, as well as specialized platforms and frameworks to do so, are review in this article. These devices, techniques, and frameworks are analyzed based on relevant criteria at the network edge, such as latency, energy consumption, and accuracy of the models, to determine the current state of the art as well as current limitations of the proposed technologies. Because of this, it is possible to understand the current possibilities to efficiently deploy state-of-the-art deep learning models at the network edge based on technologies such as artificial intelligence accelerators, tensor processing units, and techniques that include federated learning and gossip training. Finally, the challenges of Edge AI are discussed in the work, as well as the future directions that can be extracted from the evolution of the edge computing and Internet of Things approaches. © 2022 Copyright held by the owner/author(s).",artificial intelligence; deep learning; Edge AI; edge computing; Edge intelligence; machine learning,Deep learning; Energy utilization; Fog computing; Learning systems; 'current; Application scenario; Deep learning; Edge AI; Edge computing; Edge intelligence; Learning models; Machine-learning; Network edges; State of the art; Edge computing
Federated Self-training for Semi-supervised Audio Recognition,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141754632&doi=10.1145%2f3520128&partnerID=40&md5=7399ec028c4a748cc1c966d64b8f9e08,"Federated Learning is a distributed machine learning paradigm dealing with decentralized and personal datasets. Since data reside on devices such as smartphones and virtual assistants, labeling is entrusted to the clients or labels are extracted in an automated way. Specifically, in the case of audio data, acquiring semantic annotations can be prohibitively expensive and time-consuming. As a result, an abundance of audio data remains unlabeled and unexploited on users' devices. Most existing federated learning approaches focus on supervised learning without harnessing the unlabeled data. In this work, we study the problem of semi-supervised learning of audio models via self-training in conjunction with federated learning. We propose FedSTAR to exploit large-scale on-device unlabeled data to improve the generalization of audio recognition models. We further demonstrate that self-supervised pre-trained models can accelerate the training of on-device models, significantly improving convergence within fewer training rounds. We conduct experiments on diverse public audio classification datasets and investigate the performance of our models under varying percentages of labeled and unlabeled data. Notably, we show that with as little as 3% labeled data available, FedSTAR on average can improve the recognition rate by 13.28% compared to the fully supervised federated model. © 2022 Copyright held by the owner/author(s).",audio classification; deep learning; Federated learning; self-supervised learning; semi-supervised learning; sound recognition,Audio acoustics; Classification (of information); Deep learning; Learning systems; Music; Semantics; Audio classification; Audio data; Audio-recognition; Deep learning; Federated learning; Self-supervised learning; Self-training; Semi-supervised learning; Sound recognition; Unlabeled data; Supervised learning
TensorRT-Based Framework and Optimization Methodology for Deep Learning Inference on Jetson Boards,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135243098&doi=10.1145%2f3508391&partnerID=40&md5=84891d796c7df7b86ffd1e58f7f4e5d7,"As deep learning inference applications are increasing in embedded devices, an embedded device tends to equip neural processing units (NPUs) in addition to a multi-core CPU and a GPU. NVIDIA Jetson AGX Xavier is an example. For fast and efficient development of deep learning applications, TensorRT is provided as the SDK for high-performance inference, including an optimizer and runtime that delivers low latency and high throughput for deep learning inference applications. Like most deep learning frameworks, TensorRT assumes that the inference is executed on a single processing element, GPU or NPU, not both. In this article, we present a TensorRT-based framework supporting various optimization parameters to accelerate a deep learning application targeted on an NVIDIA Jetson embedded platform with heterogeneous processors, including multi-threading, pipelining, buffer assignment, and network duplication. Since the design space of allocating layers to diverse processing elements and optimizing other parameters is huge, we devise a parameter optimization methodology that consists of a heuristic for balancing pipeline stages among heterogeneous processors and fine-tuning the process for optimizing parameters. With nine real-life benchmarks, we could achieve 101%∼680% performance improvement and up to 55% energy reduction over the baseline inference using a GPU only. © 2022 Association for Computing Machinery.",acceleration; Deep learning; framework; optimization,Benchmarking; Deep learning; Heuristic methods; Learning systems; Optimization; Pipeline processing systems; Deep learning; Embedded device; Framework; Heterogeneous processors; Neural-processing; Optimisations; Optimization methodology; Performance; Processing elements; Processing units; Graphics processing unit
Leveraging Computational Storage for Power-Efficient Distributed Data Analytics,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146439655&doi=10.1145%2f3528577&partnerID=40&md5=cca372f4ebea3651ca7bda41f340aa5d,"This article presents a family of computational storage drives (CSDs) and demonstrates their performance and power improvements due to in-storage processing (ISP) when running big data analytics applications. CSDs are an emerging class of solid state drives that are capable of running user code while minimizing data transfer time and energy. Applications that can benefit from in situ processing include distributed training, distributed inferencing, and databases. To achieve the full advantage of the proposed ISP architecture, we propose software solutions for workload balancing before and at runtime for training and inferencing applications. Other applications such as sharding-based databases can readily take advantage of our ISP structure without additional tooling. Experimental results on different capacity and form factors of CSDs show up to 3.1× speedup in processing while reducing the energy consumption and data transfer by up to 67% and 68%, respectively, compared to regular enterprise solid state drives. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Computational storage drives; data analytics; distributed processing; in-storage processing; near-data processing; NLP; solid state drives,Application programs; Data handling; Data transfer; Digital storage; Energy utilization; Computational storage drive; Data analytics; Distributed data analytics; Distributed processing; In-storage processing; Near-data processing; Performance; Power efficient; Solid state drive; Storage drives; Data Analytics
EdgeWise: Energy-efficient CNN Computation on Edge Devices under Stochastic Communication Delays,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146369615&doi=10.1145%2f3530908&partnerID=40&md5=ae4cbc4c493acb6d833d8fc67fa831e8,"This article presents a framework to enable the energy-efficient execution of convolutional neural networks (CNNs) on edge devices. The framework consists of a pair of edge devices connected via a wireless network: a performance and energy-constrained device D as the first recipient of data and an energy-unconstrained device N as an accelerator for D. Device D decides on-the-fly how to distribute the workload with the objective of minimizing its energy consumption while accounting for the inherent uncertainty in network delay and the overheads involved in data transfer. These challenges are tackled by adopting the data-driven modeling framework of Markov Decision Processes, whereby an optimal policy is consulted by D in O(1) time to make layer-by-layer assignment decisions. As a special case, a linear-time dynamic programming algorithm is also presented for finding optimal layer assignment at once, under the assumption that the network delay is constant throughout the execution of the application. The proposed framework is demonstrated on a platform comprised of a Raspberry PI 3 as D and an NVIDIA Jetson TX2 as N. An average improvement of 31% and 23% in energy consumption is achieved compared to the alternatives of executing the CNNs entirely on D and N. Two state-of-the-art methods were also implemented and compared with the proposed methods.  © 2022 Association for Computing Machinery.",edge computing; energy-efficient; Internet of Things (IoT),Convolutional neural networks; Data transfer; Dynamic programming; Energy efficiency; Energy utilization; Green computing; Internet of things; Markov processes; Stochastic systems; Communication delays; Convolutional neural network; Edge computing; Energy efficient; Energy-consumption; Internet of thing; Layer assignment; Network computations; Network-delay; Stochastics; Edge computing
A Unified Programmable Edge Matrix Processor for Deep Neural Networks and Matrix Algebra,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146369594&doi=10.1145%2f3524453&partnerID=40&md5=126260aff075805cd0296947acf28d75,"Matrix Algebra and Deep Neural Networks represent foundational classes of computational algorithms across multiple emerging applications like Augmented Reality or Virtual Reality, autonomous navigation (cars, drones, robots), data science, and various artificial intelligence-driven solutions. An accelerator-based architecture can provide performance and energy efficiency supporting fixed functions through customized data paths. However, constrained Edge systems requiring multiple applications and diverse matrix operations to be efficiently supported, cannot afford numerous custom accelerators. In this article, we present MxCore, a unified architecture that comprises tightly coupled vector and programmable cores sharing data through highly optimized interconnects along with a configurable hardware scheduler managing the co-execution. We submit MxCore as the generalized approach to facilitate the flexible acceleration of multiple Matrix Algebra and Deep-learning applications across a range of sparsity levels. Unified compute resources improve overall resource utilization and performance per unit area. Aggressive and novel microarchitecture techniques along with block-level sparsity support optimize compute and data-reuse to minimize bandwidth and power requirements enabling ultra-low latency applications for low-power and cost-sensitive Edge deployments. MxCore requires a small silicon footprint of 0.2068 mm2, in a modern 7-nm process at 1 GHz and achieves (0.15 FP32 and 0.62 INT8) TMAC/mm2, dissipating only 11.66 μW of leakage power. At iso-technology and iso-frequency, MxCore provides an energy efficiency of 651.4×, 159.9×, 104.8×, and 124.2× as compared to the 128-core Nvidia's Maxwell GPU for dense General Matrix Multiply, sparse Deep Neural Network, Cholesky decomposition, and triangular matrix solve respectively.  © 2022 Copyright held by the owner/author(s).",algorithm-hardware co-design; ASIC; convolution neural network; Deep neural network learning; edge computing; hardware acceleration; matrix factorization; matrix solve,Application specific integrated circuits; Augmented reality; Closed loop control systems; Computer architecture; Computing power; Deep neural networks; Edge computing; Energy efficiency; Factorization; Integrated circuit design; Network architecture; Algorithm-hardware co-design; Co-designs; Convolution neural network; Deep neural network learning; Edge computing; Hardware acceleration; matrix; Matrix factorizations; Matrix solve; Neural network learning; Matrix algebra
HyDREA: Utilizing Hyperdimensional Computing for a More Robust and Efficient Machine Learning System,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146482356&doi=10.1145%2f3524067&partnerID=40&md5=0017fa38ff195f6b14d8f0e7ef2d9487,"Today's systems rely on sending all the data to the cloud and then using complex algorithms, such as Deep Neural Networks, which require billions of parameters and many hours to train a model. In contrast, the human brain can do much of this learning effortlessly. Hyperdimensional (HD) Computing aims to mimic the behavior of the human brain by utilizing high-dimensional representations. This leads to various desirable properties that other Machine Learning (ML) algorithms lack, such as robustness to noise in the system and simple, highly parallel operations. In this article, we propose , a HyperDimensional Computing system that is Robust, Efficient, and Accurate. We propose a Processing-in-Memory (PIM) architecture that works in a federated learning environment with challenging communication scenarios that cause errors in the transmitted data. adaptively changes the bitwidth of the model based on the signal-to-noise ratio (SNR) of the incoming sample to maintain the accuracy of the HD model while achieving significant speedup and energy efficiency. Our PIM architecture is able to achieve a speedup of 28× and 255× better energy efficiency compared to the baseline PIM architecture for Classification and achieves 32 × speed up and 289 × higher energy efficiency than the baseline architecture for Clustering. is able to achieve this by relaxing hardware parameters to gain energy efficiency and speedup while introducing computational errors. We show experimentally, HD Computing is able to handle the errors without a significant drop in accuracy due to its unique robustness property. For wireless noise, we found that is 48 × more robust to noise than other comparable ML algorithms. Our results indicate that our proposed system loses less than 1% Classification accuracy, even in scenarios with an SNR of 6.64. We additionally test the robustness of using HD Computing for Clustering applications and found that our proposed system also looses less than 1% in the mutual information score, even in scenarios with an SNR under 7 dB, which is 57 × more robust to noise than K-means.  © 2022 Copyright held by the owner/author(s).",brain-insipred hyperdimensional computing; Machine Learning; processing-in-memory,Behavioral research; Brain; Computer aided instruction; Deep neural networks; Energy efficiency; Errors; K-means clustering; Learning systems; Network architecture; Signal to noise ratio; Brain-insipred hyperdimensional computing; Complex algorithms; Dimensional representation; High-dimensional; Higher-dimensional; Human brain; Machine learning algorithms; Machine learning systems; Machine-learning; Processing-in-memory; Memory architecture
Resource-Efficient Continual Learning for Sensor-Based Human Activity Recognition,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146434782&doi=10.1145%2f3530910&partnerID=40&md5=a8d461b4345535c3e67ddabb75ca9beb,"Recent advances in deep learning have granted unrivaled performance to sensor-based human activity recognition (HAR). However, in a real-world scenario, the HAR solution is subject to diverse changes over time such as the need to learn new activity classes or variations in the data distribution of the already-included activities. To solve these issues, previous studies have tried to apply directly the continual learning methods borrowed from the computer vision domain, where it is vastly explored. Unfortunately, these methods either lead to surprisingly poor results or demand copious amounts of computational resources, which is infeasible for the low-cost resource-constrained devices utilized in HAR. In this paper, we provide a resource-efficient and high-performance continual learning solution for HAR. It consists of an expandable neural network trained with a replay-based method that utilizes a highly-compressed replay memory whose samples are selected to maximize data variability. Experiments with four open datasets, which were conducted on two distinct microcontrollers, show that our method is capable of achieving substantial accuracy improvements over baselines in continual learning such as Gradient Episodic Memory, while utilizing only one-third of the memory and being up to 3× faster. © 2022 Association for Computing Machinery.",Continual learning; deep learning; human activity recognition,Learning systems; Pattern recognition; Change-over time; Continual learning; Data distribution; Deep learning; Human activity recognition; Learn+; Learning methods; Performance; Real-world scenario; Resource-efficient; Deep learning
Design and Scaffolded Training of an Efficient DNN Operator for Computer Vision on the Edge,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146432889&doi=10.1145%2f3511212&partnerID=40&md5=dddacd45014d841b14ab6f9750ad1300,"Massively parallel systolic arrays and resource-efficient depthwise separable convolutions are two promising hardware and software techniques to accelerate DNN inference on the edge. Interestingly, their combination is inefficient: Computational patterns of depthwise separable convolutions do not exhibit a rhythmic systolic flow and lack sufficient data reuse to saturate systolic arrays. In this article, we formally analyse this inefficiency and propose an efficient operator, an optimal hardware dataflow, and a superior training methodology towards alleviating this. The efficient operator, called Fully-Separable Convolutions (FuSeConv),1 is a drop-in replacement for depthwise-separable convolutions. FuSeConv generalizes factorization of convolution fully along their spatial and depth dimensions. The resultant computation is systolic and efficiently maps to systolic arrays. The optimal hardware dataflow, called Spatial-Tiled Output Stationary(ST-OS), maximizes the efficiency of FuSeConv on systolic arrays. It maps independent convolutions to rows of the systolic array to maximise resource-utilization with negligible VLSI overheads. Neural Operator Scaffolding (NOS) scaffolds the training of FuSeConv operators by distilling knowledge from the more expensive depthwise separable convolution operation. This bridges the accuracy gap between FuSeConv networks and networks with depthwise-separable convolutions. Additionally, NOS can be combined with Neural Architecture Search (NAS) to trade off latency and accuracy.The hardware-software co-design of FuSeConv with ST-OS achieves a significant speedup of 4.1-9.25× with state-of-the-art efficient networks for the ImageNet dataset. The parameter efficiency of FuSeConv and its significant superiority over depthwise-separable convolutions on systolic arrays illustrates their promise as a strong solution on the edge. Training FuSeConv networks with NOS achieves accuracy comparable to the depthwise-separable convolution baselines. Further, by combining NOS with NAS, we design networks that define state-of-the-art models improving on both accuracy and latency for computer vision on systolic arrays. © 2022 Association for Computing Machinery.",computer vision; Deep neural networks; edge computing; efficient hardware; efficient networks; hardware-software co-design; systems for deep learning,Computer hardware; Computer vision; Convolution; Economic and social effects; Edge computing; Efficiency; Embedded systems; Hardware-software codesign; Personnel training; Scaffolds; Software design; Dataflow; Edge computing; Efficient hardware; Efficient network; Hardware/software codesign; Massively parallels; Neural architectures; Resource-efficient; State of the art; System for deep learning; Deep neural networks
Block Walsh-Hadamard Transform-based Binary Layers in Deep Neural Networks,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137846093&doi=10.1145%2f3510026&partnerID=40&md5=1c511c7fa95c1ffe989082b422ca59d1,"Convolution has been the core operation of modern deep neural networks. It is well known that convolutions can be implemented in the Fourier Transform domain. In this article, we propose to use binary block Walsh-Hadamard transform (WHT) instead of the Fourier transform. We use WHT-based binary layers to replace some of the regular convolution layers in deep neural networks. We utilize both one-dimensional (1D) and 2D binary WHTs in this article. In both 1D and 2D layers, we compute the binary WHT of the input feature map and denoise the WHT domain coefficients using a nonlinearity that is obtained by combining soft-thresholding with the tanh function. After denoising, we compute the inverse WHT. We use 1D-WHT to replace the 1 × 1 convolutional layers, and 2D-WHT layers can replace the 3 × 3 convolution layers and Squeeze-and-Excite layers. 2D-WHT layers with trainable weights can be also inserted before the Global Average Pooling layers to assist the dense layers. In this way, we can reduce the number of trainable parameters significantly with a slight decrease in trainable parameters. In this article, we implement the WHT layers into MobileNet-V2, MobileNet-V3-Large, and ResNet to reduce the number of parameters significantly with negligible accuracy loss. Moreover, according to our speed test, the 2D-FWHT layer runs about 24 times as fast as the regular 3 × 3 convolution with 19.51% less RAM usage in an NVIDIA Jetson Nano experiment.  © 2022 Association for Computing Machinery.",block division; Fast Walsh-Hadamard transform; image classification; smooth-thresholding,Deep neural networks; Hadamard transforms; Image classification; Multilayer neural networks; Network layers; 2D layer; Block division; Fast Walsh-Hadamard transforms; Images classification; Input features; One-dimensional; Smooth-thresholding; Thresholding; Transform domain; Walsh Hadamard Transforms; Convolution
Hardware-friendly User-specific Machine Learning for Edge Devices,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146368941&doi=10.1145%2f3524125&partnerID=40&md5=b9b2f72d7b286193fad2d33f2d4812a1,"Machine learning (ML) on resource-constrained edge devices is expensive and often requires offloading computation to the cloud, which may compromise the privacy of user data. In contrast, the type of data processed at edge devices is user-specific and limited to a few inference classes. In this work, we explore building smaller, user-specific machine learning models, rather than utilizing a generic, compute-intensive machine learning model that caters to a diverse range of users. We first present a hardware-friendly, lightweight pruning technique to create user-specific models directly on mobile platforms, while simultaneously executing inferences. The proposed technique leverages compute sharing between pruning and inference, customizes the backward pass of training, and chooses a pruning granularity for efficient processing on edge. We then propose architectural support to prune user-specific models on a systolic edge ML inference accelerator. We demonstrate that user-specific models provide a speedup of 2.9× and 2.3× on the mobile CPUs for the ResNet-50 and Inception-V3 models.  © 2022 Association for Computing Machinery.",Datasets; image classification; inference; neural networks; personalized ML; pruning,Classification (of information); computation offloading; Image classification; Program processors; Dataset; Images classification; Inference; Machine learning models; Machine-learning; Neural-networks; Offloading computations; Personalized machine learning; Pruning; User data; Machine learning
Quantized Sparse Training: A Unified Trainable Framework for Joint Pruning and Quantization in DNNs,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144276415&doi=10.1145%2f3524066&partnerID=40&md5=9ce85666ec984ed1ee6942ce89ab2406,"Deep neural networks typically have extensive parameters and computational operations. Pruning and quantization techniques have been widely used to reduce the complexity of deep models. Both techniques can be jointly used for realizing significantly higher compression ratios. However, separate optimization processes and difficulties in choosing the hyperparameters limit the application of both the techniques simultaneously. In this study, we propose a novel compression framework, termed as quantized sparse training, that prunes and quantizes networks jointly in a unified training process. We integrate pruning and quantization into a gradient-based optimization process based on the straight-through estimator. Quantized sparse training enables us to simultaneously train, prune, and quantize a network from scratch. The empirical results validate the superiority of the proposed methodology over the recent state-of-the-art baselines with respect to both the model size and accuracy. Specifically, quantized sparse training achieves a 135 KB model size in the case of VGG16, without any accuracy degradation, which is 40% of the model size feasible based on the state-of-the-art pruning and quantization approach.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep learning; joint pruning and quantization; model compression; neural network,Computational operations; Deep learning; High compression ratio; Hyper-parameter; Joint pruning and quantization; Model compression; Model size; Neural-networks; Quantisation; State of the art; Deep neural networks
A Passive Online Technique for Learning Hybrid Automata from Input/Output Traces,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146375590&doi=10.1145%2f3556543&partnerID=40&md5=928d1ff3002089550fd5a59570bfe59a,"Specification synthesis is the process of deriving a model from the input-output traces of a system. It is used extensively in test design, reverse engineering, and system identification. One type of the resulting artifact of this process for cyber-physical systems is hybrid automata. They are intuitive, precise, tool independent, and at a high level of abstraction, and can model systems with both discrete and continuous variables. In this article, we propose a new technique for synthesizing hybrid automaton from the input-output traces of a non-linear cyber-physical system. Similarity detection in non-linear behaviors is the main challenge for extracting such models. We address this problem by utilizing the Dynamic Time Warping technique. Our approach is passive, meaning that it does not need interaction with the system during automata synthesis from the logged traces; and online, which means that each input/output trace is used only once in the procedure. In other words, each new trace can be used to improve the already synthesized automaton. We evaluated our algorithm in one industrial and two simulated case studies. The accuracy of the derived automata shows promising results. © 2022 Association for Computing Machinery.",Automata learning; hybrid automata; learning hybrid automata; passive learning,Automata theory; E-learning; Embedded systems; Automaton learning; Cybe-physical systems; Cyber-physical systems; Hybrid automatons; Input-output; Learning hybrid automaton; Online technique; Passive learning; System-identification; Test designs; Cyber Physical System
Efficient Realization of Decision Trees for Real-Time Inference,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138605351&doi=10.1145%2f3508019&partnerID=40&md5=e3faaa002cd3ae55c20ed63a8e506574,"For timing-sensitive edge applications, the demand for efficient lightweight machine learning solutions has increased recently. Tree ensembles are among the state-of-the-art in many machine learning applications. While single decision trees are comparably small, an ensemble of trees can have a significant memory footprint leading to cache locality issues, which are crucial to performance in terms of execution time. In this work, we analyze memory-locality issues of the two most common realizations of decision trees, i.e., native and if-else trees. We highlight that both realizations demand a more careful memory layout to improve caching behavior and maximize performance. We adopt a probabilistic model of decision tree inference to find the best memory layout for each tree at the application layer. Further, we present an efficient heuristic to take architecture-dependent information into account thereby optimizing the given ensemble for a target computer architecture. Our code-generation framework, which is freely available on an open-source repository, produces optimized code sessions while preserving the structure and accuracy of the trees. With several real-world data sets, we evaluate the elapsed time of various tree realizations on server hardware as well as embedded systems for Intel and ARM processors. Our optimized memory layout achieves a reduction in execution time up to 75 % execution for server-class systems, and up to 70 % for embedded systems, respectively.  © 2022 Copyright held by the owner/author(s).",Architecture-Aware realization; cache-aware optimization; optimized memory layout; random forest; real-time inference,Cache memory; Embedded systems; Machine learning; Memory architecture; Open source software; Open systems; Optimization; Real time systems; Architecture-aware realization; Cache-aware optimization; Embedded-system; Lightweight machines; Memory layout; Optimisations; Optimized memory layout; Performance; Random forests; Real-time inference; Decision trees
DynO: Dynamic Onloading of Deep Neural Networks from Cloud to Device,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146365727&doi=10.1145%2f3510831&partnerID=40&md5=df6988e689631d34dd5e7b93a2ef8e0e,"Recently, there has been an explosive growth of mobile and embedded applications using convolutional neural networks (CNNs). To alleviate their excessive computational demands, developers have traditionally resorted to cloud offloading, inducing high infrastructure costs and a strong dependence on networking conditions. On the other end, the emergence of powerful SoCs is gradually enabling on-device execution. Nonetheless, low- and mid-tier platforms still struggle to run state-of-the-art CNNs sufficiently. In this article, we present DynO, a distributed inference framework that combines the best of both worlds to address several challenges, such as device heterogeneity, varying bandwidth, and multi-objective requirements. Key components that enable this are its novel CNN-specific data packing method, which exploits the variability of precision needs in different parts of the CNN when onloading computation, and its novel scheduler, which jointly tunes the partition point and transferred data precision at runtime to adapt inference to its execution environment. Quantitative evaluation shows that DynO outperforms the current state of the art, improving throughput by over an order of magnitude over device-only execution and up to 7.9× over competing CNN offloading systems, with up to 60× less data transferred.  © 2022 Association for Computing Machinery.",Deep neural networks; distributed systems; offloading,computation offloading; Convolutional neural networks; Computational demands; Convolutional neural network; Distributed systems; Embedded application; Explosive growth; Infrastructure costs; Mobile applications; Offloading; State of the art; Strong dependences; Deep neural networks
Accelerated Fire Detection and Localization at Edge,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146368675&doi=10.1145%2f3510027&partnerID=40&md5=fb6a31e63ab5f4f01fe04fc518ab49c0,"Fire-related incidents continue to be reported as a leading cause of life and property destruction. Automated fire detection and localization (AFDL) systems have grown in importance with the evolution of applied robotics, especially because use of robots in disaster situations can lead to avoidance of human fatality. The importance of AFDL on resource-constrained devices has further grown, as most unmanned vehicles (drones or ground vehicles) are battery operated with limited computational capacity, the disaster situations cannot guarantee uninterrupted communication with high-end resources in the cloud, and yet faster response time is a prime necessity. Traditional computer vision-based techniques require hand-engineered features on a case-by-case basis. Deep Learning-based classifiers perform well for fire/no-fire classification due to the availability of large datasets for training; however, a dearth of good fire localization datasets renders the localization performance below par. We have tried to address both problems with a multi-task learned cascaded model that triggers localization workflow only if the presence of fire is detected, through a strong classifier trained on available large fire datasets. This presents only fire images to a relatively weaker localization model, reducing false positives, false negatives, and thereby improving overall AFDL accuracy. The multi-task learning (MTL) approach for end-to-end training of a stitched classifier and object localizer model on diverse datasets enabled us to build a strong fire classifier and feature extractor. It also resulted in a single unified model, capable of running on ""on-board""compute infrastructure without compromising on accuracy.To achieve the target inference rate for the AFDL deployment, we have investigated the effect of quantization and compression due to hardware acceleration on an MTL model. This article presents an approach to automate the hardware-software co-design to find the optimum parameter partitioning for a given MTL problem, especially when some parts of the model are hardware accelerated. We present combined evaluation results showing that our methodology and the corresponding AFDL model strikes a balance between the frames inferred per second and several accuracy metrics. We report fire localization accuracy in terms of mean average precision (object detection), which has not been done earlier for embedded AFDL systems.  © 2022 Association for Computing Machinery.",drone; Fire detection; localization; neural networks; on-board processing; UAV,Classification (of information); Computer vision; Deep learning; Drones; Embedded systems; Fires; Image enhancement; Large dataset; Learning systems; Object detection; Object recognition; Support vector machines; Detection and localization; Disaster situations; Fire detection; Fire detection systems; Localisation; Localisation Systems; Localization modeling; Multitask learning; Neural-networks; On-board processing; Aircraft detection
More Is Less: Model Augmentation for Intermittent Deep Inference,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144136468&doi=10.1145%2f3506732&partnerID=40&md5=ee43962d0f1c6998eefb0364b637b96a,"Energy harvesting creates an emerging intermittent computing paradigm but poses new challenges for sophisticated applications such as intermittent deep neural network (DNN) inference. Although model compression has adapted DNNs to resource-constrained devices, under intermittent power, compressed models will still experience multiple power failures during a single inference. Footprint-based approaches enable hardware-accelerated intermittent DNN inference by tracking footprints, independent of model computations, to indicate accelerator progress across power cycles. However, we observe that the extra overhead required to preserve progress indicators can severely offset the computation progress accumulated by intermittent DNN inference. This work proposes the concept of model augmentation to adapt DNNs to intermittent devices. Our middleware stack, JAPARI, appends extra neural network components into a given DNN, to enable the accelerator to intrinsically integrate progress indicators into the inference process, without affecting model accuracy. Their specific positions allow progress indicator preservation to be piggybacked onto output feature preservation to amortize the extra overhead, and their assigned values ensure uniquely distinguishable progress indicators for correct inference recovery upon power resumption. Evaluations on a Texas Instruments device under various DNN models, capacitor sizes, and progress preservation granularities show that JAPARI can speed up intermittent DNN inference by 3× over the state of the art, for common convolutional neural architectures that require heavy acceleration. © 2022 Association for Computing Machinery.",Deep neural networks; edge computing; energy harvesting; intermittent systems; model adaptation,Computing power; Edge computing; Energy harvesting; Internet of things; Middleware; Computing paradigm; Deep inference; Edge computing; Intermittent systems; Model Adaptation; Model compression; Network inference; Power; Power failure; Resourceconstrained devices; Deep neural networks
FELIX: A Ferroelectric FET Based Low Power Mixed-Signal In-Memory Architecture for DNN Acceleration,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135193775&doi=10.1145%2f3529760&partnerID=40&md5=cb4932317bdb97e55fd42b20ab29b22b,"Today, a large number of applications depend on deep neural networks (DNN) to process data and perform complicated tasks at restricted power and latency specifications. Therefore, processing-in-memory (PIM) platforms are actively explored as a promising approach to improve the throughput and the energy efficiency of DNN computing systems. Several PIM architectures adopt resistive non-volatile memories as their main unit to build crossbar-based accelerators for DNN inference. However, these structures suffer from several drawbacks such as reliability, low accuracy, large ADCs/DACs power consumption and area, high write energy, and so on. In this article, we present a new mixed-signal in-memory architecture based on the bit-decomposition of the multiply and accumulate (MAC) operations. Our in-memory inference architecture uses a single FeFET as a non-volatile memory cell. Compared to the prior work, this system architecture provides a high level of parallelism while using only 3-bit ADCs. Also, it eliminates the need for any DAC. In addition, we provide flexibility and a very high utilization efficiency even for varying tasks and loads. Simulations demonstrate that we outperform state-of-the-art efficiencies with 36.5 TOPS/W and can pack 2.05 TOPS with 8-bit activation and 4-bit weight precision in an area of 4.9 mm2 using 22 nm FDSOI technology. Employing binary operation, we obtain 1169 TOPS/W and over 261 TOPS/W/mm2 on system level. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bit-decomposition; convolution neural networks; current-mode ADC; FeFET crossbar array; in-memory computation; mixed-signal processing,Acceleration; Computing power; Digital storage; Energy efficiency; Low power electronics; Memory architecture; Network architecture; Signal processing; Bit-decomposition; Convolution neural network; Crossbar arrays; Current-mode ADC; Current-modes; FeFET crossbar array; In-memory computation; Memory computations; Mixed signal; Mixed-signal processing; Deep neural networks
RDF: A Reconfigurable Dataflow Model of Computation,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146420305&doi=10.1145%2f3544972&partnerID=40&md5=cca09f72fa538e619d0834158ce37e69,"Dataflow Models of Computation (MoCs) are widely used in embedded systems, including multimedia processing, digital signal processing, telecommunications, and automatic control. In a dataflow MoC, an application is specified as a graph of actors connected by FIFO channels. One of the first and most popular dataflow MoCs, Synchronous Dataflow (SDF), provides static analyses to guarantee boundedness and liveness, which are key properties for embedded systems. However, SDF and most of its variants lack the capability to express the dynamism needed by modern streaming applications. In particular, the applications mentioned above have a strong need for reconfigurability to accommodate changes in the input data, the control objectives, or the environment. We address this need by proposing a new MoC called Reconfigurable Dataflow (RDF). RDF extends SDF with transformation rules that specify how and when the topology and actors of the graph may be reconfigured. Starting from an initial RDF graph and a set of transformation rules, an arbitrary number of new RDF graphs can be generated at runtime. A key feature of RDF is that it can be statically analyzed to guarantee that all possible graphs generated at runtime will be consistent and live. We introduce the RDF MoC, describe its associated static analyses, and present its implementation and some experimental results. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",boundedness; graph rewriting; liveness; Models of computation; reconfigurable systems; static analyses; synchronous dataflow,Automation; Computation theory; Data handling; Digital signal processing; Embedded systems; Resource Description Framework (RDF); Response time (computer systems); Static analysis; Boundedness; Data flow modeling; Dataflow; Graph rewriting; Liveness; Model of computation; Reconfigurable; Reconfigurable-systems; Static analyze; Synchronous Dataflow; Data flow analysis
ATCN: Resource-efficient Processing of Time Series on Edge,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146371502&doi=10.1145%2f3524070&partnerID=40&md5=8fd63d1f36fc98b0811695f647eec7c7,"This article presents a scalable deep learning model called Agile Temporal Convolutional Network (ATCN) for highly accurate fast classification and time series prediction in resource-constrained embedded systems. ATCN is a family of compact networks with formalized hyperparameters that enable application-specific adjustments to be made to the model architecture. It is primarily designed for embedded edge devices with very limited performance and memory, such as wearable biomedical devices and real-time reliability monitoring systems. ATCN makes fundamental improvements over the mainstream temporal convolutional neural networks, including residual connections to increase the network depth and accuracy and the incorporation of separable depth-wise convolution to reduce the computational complexity of the model. As part of the present work, two ATCN families, namely T0 and T1, are also presented and evaluated on different ranges of embedded processors: Cortex-M7 and Cortex-A57 processors. An evaluation of the ATCN models against the best-in-class InceptionTime and MiniRocket shows that ATCN almost maintains accuracy while improving the execution time on a broad range of embedded and cyber-physical applications with demand for real-time processing on the embedded edge. At the same time, in contrast to existing solutions, ATCN is the first time series classifier based on deep learning that can be run bare-metal on embedded microcontrollers (Cortex-M7) with limited computational performance and memory capacity while delivering state-of-the-art accuracy.  © 2022 Association for Computing Machinery.",real-time edge computing; Recurrent Neural Networks (RNN); Temporal Convolutional Networks (TCN),Convolution; Convolutional neural networks; Deep neural networks; Edge computing; Embedded systems; Real time systems; Recurrent neural networks; Convolutional networks; Cortexes; Edge computing; Network resource; Real- time; Real-time edge computing; Recurrent neural network; Resource-efficient; Temporal convolutional network; Times series; Time series
Temporal Robustness of Temporal Logic Specifications: Analysis and Control Design,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142507142&doi=10.1145%2f3550072&partnerID=40&md5=573894018fdc32e8efd8cf3fbb194f38,"We study the temporal robustness of temporal logic specifications and show how to design temporally robust control laws for time-critical control systems. This topic is of particular interest in connected systems and interleaving processes such as multi-robot and human-robot systems where uncertainty in the behavior of individual agents and humans can induce timing uncertainty. Despite the importance of time-critical systems, temporal robustness of temporal logic specifications has not been studied, especially from a control design point of view. We define synchronous and asynchronous temporal robustness and show that these notions quantify the robustness with respect to synchronous and asynchronous time shifts in the predicates of the temporal logic specification. It is further shown that the synchronous temporal robustness upper bounds the asynchronous temporal robustness. We then study the control design problem in which we aim to design a control law that maximizes the temporal robustness of a dynamical system. Our solution consists of a Mixed-Integer Linear Programming (MILP) encoding that can be used to obtain a sequence of optimal control inputs. While asynchronous temporal robustness is arguably more nuanced than synchronous temporal robustness, we show that control design using synchronous temporal robustness is computationally more efficient. This tradeoff can be exploited by the designer depending on the particular application at hand. We conclude the article with a variety of case studies. © 2022 Association for Computing Machinery.",control design; signal temporal logic; temporal robustness; Time-critical systems,Computer circuits; Dynamical systems; Integer programming; Machine design; Robustness (control systems); Specifications; Temporal logic; Analysis and controls; Analysis/design; Control design; Control laws; Signal temporal logic; Temporal logic specifications; Temporal robustness; Time-critical; Time-critical systems; Uncertainty; Robust control
Performance Modeling of Computer Vision-based CNN on Edge GPUs,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146368446&doi=10.1145%2f3527169&partnerID=40&md5=5903470501585530aba6f2a419835271,"Convolutional Neural Networks (CNNs) are currently widely used in various fields, particularly for computer vision applications. Edge platforms have drawn tremendous attention from academia and industry due to their ability to improve execution time and preserve privacy. However, edge platforms struggle to satisfy CNNs' needs due to their computation and energy constraints. Thus, it is challenging to find the most efficient CNN that respects accuracy, time, energy, and memory footprint constraints for a target edge platform. Furthermore, given the size of the design space of CNNs and hardware platforms, performance evaluation of CNNs entails several efforts. Consequently, designers need tools to quickly explore large design space and select the CNN that offers the best performance trade-off for a set of hardware platforms. This article proposes a Machine Learning (ML)-based modeling approach for CNN performances on edge GPU-based platforms for vision applications. We implement and compare five of the most successful ML algorithms for accurate and rapid CNN performance predictions on three different edge GPUs in image classification. Experimental results demonstrate the robustness and usefulness of our proposed methodology. For three of the five ML algorithms - XGBoost, Random Forest, and Ridge Polynomial regression - average errors of 11%, 6%, and 8% have been obtained for CNN inference execution time, power consumption, and memory usage, respectively.  © 2022 Association for Computing Machinery.",CNN; edge GPU; execution time; machine learning; memory usage; Performance modeling; power consumption; regression analysis,Computer hardware; Computer vision; Convolutional neural networks; Economic and social effects; Graphics processing unit; Inference engines; Machine learning; Program processors; Random errors; Regression analysis; Convolutional neural network; Design spaces; Edge GPU; Execution time; Hardware platform; Machine learning algorithms; Machine-learning; Memory usage; Performance Modeling; Vision based; Electric power utilization
A Construction Kit for Efficient Low Power Neural Network Accelerator Designs,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146369377&doi=10.1145%2f3520127&partnerID=40&md5=b304e13edf43f68d726342ee6ec96b33,"Implementing embedded neural network processing at the edge requires efficient hardware acceleration that combines high computational throughput with low power consumption. Driven by the rapid evolution of network architectures and their algorithmic features, accelerator designs are constantly being adapted to support the improved functionalities. Hardware designers can refer to a myriad of accelerator implementations in the literature to evaluate and compare hardware design choices. However, the sheer number of publications and their diverse optimization directions hinder an effective assessment. Existing surveys provide an overview of these works but are often limited to system-level and benchmark-specific performance metrics, making it difficult to quantitatively compare the individual effects of each utilized optimization technique. This complicates the evaluation of optimizations for new accelerator designs, slowing-down the research progress.In contrast to previous surveys, this work provides a quantitative overview of neural network accelerator optimization approaches that have been used in recent works and reports their individual effects on edge processing performance. The list of optimizations and their quantitative effects are presented as a construction kit, allowing to assess the design choices for each building block individually. Reported optimizations range from up to 10,000× memory savings to 33× energy reductions, providing chip designers with an overview of design choices for implementing efficient low power neural network accelerators.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",design optimization; Edge processing; hardware accelerator,Benchmarking; Computer aided design; Energy efficiency; Low power electronics; Network architecture; Accelerator design; Construction kit; Design optimization; Edge processing; Embedded neural networks; Hardware accelerators; Low Power; Neural-network processing; Neural-networks; Optimisations; Neural networks
Optimus: An Operator Fusion Framework for Deep Neural Networks,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144759356&doi=10.1145%2f3520142&partnerID=40&md5=19fa714abefc71ea95f315985731cd8a,"The reduction of neural parameters and operations for the applications on embedded and IoT platforms in current deep neural network (DNN) architectures has received increasing attention. Relatively, the intermediate feature maps of such lightweight neural networks begin to grow and usually outsize the on-chip memory as the new bottleneck, which introduces considerable power-consuming off-chip memory accesses. To reduce the feature-induced memory accesses, operator fusion has been proposed to parallelize the execution of multiple convolutional layers and shown significant reduction of off-chip memory accesses. However, how to fuse the neural operators is still a challenging issue that heavily depends on both the neural network (NN) topology and the specific DNN accelerator configuration. In this work, we observed prior operator fusion approaches fail to guarantee memory-level optimality as they search in the constrained operator fusion design space. Considering the complexity of the NN topologies and the constrained resources of the DNN accelerators, we develop a novel operator fusion framework, Optimus. Optimus includes an accurate memory cost model dedicated to the scheduler to evaluate the potential operator-fusion schemes and a directed acyclic graph-based operator fusion algorithm for both off-line and on-line workload deployment scenarios, which altogether generates high-efficiency operator-fusion solutions for arbitrary network models running on DNN accelerators. The experimental results show that Optimus reduces 17-75% off-chip memory accesses and obtains 1.86×-3.66× energy efficiency on state-of-the-art DNN workloads when compared to the baselines and brings significant power-efficiency boost to the DNN accelerators of different architectures and dataflows.  © 2022 Association for Computing Machinery.",embedded processor; layer fusion; memory; Neural network,Constrained optimization; Directed graphs; Energy efficiency; Graphic methods; Memory architecture; Multilayer neural networks; Network architecture; % reductions; 'current; Embedded processors; Layer fusion; Memory access; Neural network architecture; Neural network topology; Neural-networks; Off-chip memory; Optimus; Deep neural networks
Survey of Control-flow Integrity Techniques for Real-time Embedded Systems,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142421269&doi=10.1145%2f3538275&partnerID=40&md5=ac146ddb7285454882bee3603f659da3,"Computing systems, including real-time embedded systems, are becoming increasingly connected to allow for more advanced and safer operation. Such embedded systems are also often resource-constrained, for example, with lower processing capabilities compared to general-purpose computing systems like desktops or servers. With the advent of paradigms such as internet-of-things (IoT), embedded systems in both commercial and industrial contexts are being increasingly interconnected and exposed to the external networks to improve automation and efficiency of operation. However, allowing external interfaces to such embedded systems increases their exposure to attackers. With an increase in attacks against embedded systems ranging from home appliances to industrial control systems operating critical equipment that have real-time requirements, it is imperative that defense mechanisms be created that explicitly consider such resource and real-time constraints. Control-flow integrity (CFI) is a family of defense mechanisms that prevent attackers from modifying the flow of execution. We survey CFI techniques, ranging from the basic to state of the art, that are built for embedded systems and real-time embedded systems and find that there is a dearth, especially for real-time embedded systems, of CFI mechanisms. We then present open challenges to the community to help drive future research in this domain. © 2022 Association for Computing Machinery.",control-flow integrity; embedded systems; real-time systems; Survey,Domestic appliances; Embedded systems; Interactive computer systems; Internet of things; Network security; Real time systems; Computing system; Control-flow integrities; Defence mechanisms; Embedded-system; General-purpose computing; Industrial context; Processing capability; Real - Time system; Real-time embedded systems; Safe operation; Surveys
Benchmarking and Configuring Security Levels in Intermittent Computing,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142458227&doi=10.1145%2f3522748&partnerID=40&md5=b1e95852da83459a0b715d3ee60c2b77,"Intermittent computing derives its name from the intermittent character of the power source used to drive the computing, typically an energy harvester of ambient energy sources. Intermittent computing is characterized by frequent transitions between the powered and the non-powered state. To enable the processor to quickly recover from unexpected power loss, regular checkpoints store the run-time state of the program, including variables, control information, and machine state. In sensitive applications such as logged measurements, checkpoints must be secured against tamper and replay. We investigate the overhead of creating, securing, and restoring checkpoints with respect to the application. We propose a configurable checkpoint security setting that leverages application properties to reduce overhead of checkpoint security and implement the same using a secure checkpointing protocol. We discuss a prototype implementation for a FRAM-based micro-controller, and we characterize the cost of adding and configuring security to traditional checkpointing using a suite of embedded benchmark applications. © 2022 Copyright held by the owner/author(s).",AEAD; benchmark; checkpoint security; embedded systems; Intermittent computing; non-volatile memory,Benchmarking; Ferroelectric RAM; Program processors; AEAD; Ambients; Benchmark; Checkpoint security; Embedded-system; Energy Harvester; Intermittent computing; Non-volatile memory; Power sources; Security level; Embedded systems
A Segmented Adaptive Router for Near Energy-Proportional Networks-on-Chip,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142391057&doi=10.1145%2f3529106&partnerID=40&md5=3195bbdfe02f6c6755822f27eb0657f4,"A Network-on-Chip (NoC) is an essential component of a chip multiprocessor (CMP) which however contributes to a large fraction of system energy. The unpredictability of traffic across a NoC frequently involves an expensive over-sizing of NoC resources which in turn leads to a significant contribution to the CMP power consumption. There exists a body of work addressing this issue, however so far solutions fall short when aiming for power reduction whilst maintaining high NoC performance. This paper proposes to combine router architecture optimizations with smart resource management to overcome this limitation. Based on a fully segmented architecture, we present an online adaptive router adjusting its active routing resources to meet the current traffic demand. This enhanced power-gating strategy significantly decreases both static and dynamic power consumption of the NoC, up to 70% for synthetic traffic patterns and up to 58% for real traffic workloads, while preserving NoC latency and throughput. Thanks to these adaptive power-saving mechanisms the proposed segmented NoC router provides near energy-proportional operation across the range of used benchmarks. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",dynamic power management; energy efficiency; Network-on-chip,Electric power utilization; Energy efficiency; Memory architecture; Network architecture; Power management; Routers; Servers; Adaptive routers; Chip Multiprocessor; Chip performance; Dynamic power management; Energy; Higher networks; Networks on chips; Over-sizing; Power reductions; System energy; Network-on-chip
QUAREM: Maximising QoE Through Adaptive Resource Management in Mobile MPSoC Platforms,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138713242&doi=10.1145%2f3526116&partnerID=40&md5=71c409a28fbb114ea46e6a3233a7d138,"Heterogeneous multi-processor system-on-chip (MPSoC) smartphones are required to offer increasing performance and user quality-of-experience (QoE), despite comparatively slow advances in battery technology. Approaches to balance instantaneous power consumption, performance and QoE have been reported, but little research has considered how to perform longer-term budgeting of resources across a complete battery discharge cycle. Approaches that have considered this are oblivious to the daily variability in the user's desired charging time-of-day (plug-in time), resulting in a failure to meet the user's battery life expectations, or else an unnecessarily over-constrained QoE. This paper proposes QUAREM, an adaptive resource management approach in mobile MPSoC platforms that maximises QoE while meeting battery life expectations. The proposed approach utilises a model that learns and then predicts the dynamics of the energy usage pattern and plug-in times. Unlike state-of-the-art approaches, we maximise the QoE through the adaptive balancing of the battery life and the quality of service (QoS) for the duration of the battery discharge. Our model achieves a good degree of accuracy with a mean absolute percentage error of 3.47% and 2.48% for the energy demand and plug-in times, respectively. Experimental evaluation on an off-the-shelf commercial smartphone shows that QUAREM achieves the expected battery life of the user within 20-25% energy demand variation with little or no QoE degradation. © 2022 Association for Computing Machinery.",adaptive resource management; Battery budgeting; heterogeneous MPSoC; maximising user experience; QoE-aware resource management; quality of experience,Energy management; Multiprocessing systems; Natural resources management; Quality of service; Resource allocation; Secondary batteries; Smartphones; System-on-chip; Adaptive Resource Management; Battery budgeting; Battery life; Heterogeneous multi-processor system-on-chip; Maximizing user experience; Multi processor system on chips; Quality of experience; Quality-of-experience-aware resource management; Resource management; Users' experiences; Budget control
An Efficient CNN Accelerator for Low-Cost Edge Systems,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142427881&doi=10.1145%2f3539224&partnerID=40&md5=8ba3da2e0b474e74d2a80ed89290d9dd,"Customized hardware based convolutional neural network (CNN or ConvNet) accelerators have attracted significant attention for applications in a low-cost, edge computing system. However, there is a lack of research that seeks to optimize at both the algorithm and hardware levels simultaneously in resource-constrained FPGA systems. In this paper, we first analyze ConvNet models to find one that is most suitable for a low-cost FPGA implementation. Based on the analysis, we select MobileNetV2 as the backbone of our research due to its hardware-friendly structure. We use a quantized implementation with 4-bit precision and optimize further with a smaller input resolution of 192 × 192 to obtain a 68.8% detection accuracy on ImageNet, which represents only a 3.2% accuracy loss compared to a floating-point model that uses the full input size. We then develop a hardware implementation that uses a low-cost FPGA. To accelerate the depth-wise separable ConvNet and utilize DRAM resources efficiently with parallel processing, we propose a novel scoreboard architecture to dynamically schedule DRAM data requests in order to maintain a high hardware utilization. The number of DSP blocks used is about six times smaller than in prior work. In addition, internal block RAM utilization is approximately nine times more efficient than in prior work. Our proposed design achieves 3.07 frames per second (FPS) on the low-cost and resource constrained FPGA system. © 2022 Association for Computing Machinery.",Convolutional neural network (CNN); EfficientNet; embedded system; FPGA; hardware accelerator; MobileNet,Constrained optimization; Convolution; Convolutional neural networks; Costs; Digital arithmetic; Embedded systems; Computing system; Convnet; Convolutional neural network; Edge computing; Efficientnet; Embedded-system; Hardware accelerators; Low-costs; Mobilenet; Field programmable gate arrays (FPGA)
Attack-resilient Fusion of Sensor Data with Uncertain Delays,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142433762&doi=10.1145%2f3532181&partnerID=40&md5=cc4ff1f8489cb4af368f19d328451aa9,"Malicious attackers may disrupt the safety of autonomous systems through compromising sensors to feed wrong measurements to the controller. This article proposes attack-resilient sensor fusion that combines local sensor readings and shared sensing information from multiple sources. The method results in higher resilience against sensor attacks through jointly considering sensing noise and uncertain communication delay. To be specific, we first identify the considerable impact of the delay on determining attacked sensors. Second, we present a novel two-dimensional abstract sensor model, where each measurement is augmented as a probabilistic interval based on the convolution of the noise and delay. Third, we propose a fusion algorithm that admits the fused value with highest joint probability distribution of the intervals to tolerate corrupted measurements. Finally, we demonstrate the effectiveness of our method in a vehicle-platoon case study using extensive simulations and testbed experiments. © 2022 Association for Computing Machinery.",attack-resilience; Autonomous systems; security; sensor fusion; uncertain delay,Abstracting; Sensor data fusion; Attack resiliences; Autonomous system; Local sensors; Multiple source; Security; Sensing information; Sensor fusion; Sensor readings; Sensors data; Uncertain delay; Probability distributions
Human Activity Recognition on Microcontrollers with Quantized and Adaptive Deep Neural Networks,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142448197&doi=10.1145%2f3542819&partnerID=40&md5=9e5bdc282b9d347d9a3cd57fbbd50512,"Human Activity Recognition (HAR) based on inertial data is an increasingly diffused task on embedded devices, from smartphones to ultra low-power sensors. Due to the high computational complexity of deep learning models, most embedded HAR systems are based on simple and not-so-accurate classic machine learning algorithms. This work bridges the gap between on-device HAR and deep learning, proposing a set of efficient one-dimensional Convolutional Neural Networks (CNNs) that can be deployed on general purpose microcontrollers (MCUs). Our CNNs are obtained combining hyper-parameters optimization with sub-byte and mixed-precision quantization, to find good trade-offs between classification results and memory occupation. Moreover, we also leverage adaptive inference as an orthogonal optimization to tune the inference complexity at runtime based on the processed input, hence producing a more flexible HAR system.With experiments on four datasets, and targeting an ultra-low-power RISC-V MCU, we show that (i) we are able to obtain a rich set of Pareto-optimal CNNs for HAR, spanning more than 1 order of magnitude in terms of memory, latency, and energy consumption; (ii) thanks to adaptive inference, we can derive >20 runtime operating modes starting from a single CNN, differing by up to 10% in classification scores and by more than 3× in inference complexity, with a limited memory overhead; (iii) on three of the four benchmarks, we outperform all previous deep learning methods, while reducing the memory occupation by more than 100×. The few methods that obtain better performance (both shallow and deep) are not compatible with MCU deployment; (iv) all our CNNs are compatible with real-time on-device HAR, achieving an inference latency that ranges between 9 μs and 16 ms. Their memory occupation varies in 0.05-23.17 kB, and their energy consumption in 0.05 and 61.59 μJ, allowing years of continuous operation on a small battery supply.  © 2022 Association for Computing Machinery.",adaptive neural networks; edge computing; energy efficiency; human activity recognition; mixed precision; Quantized neural networks,Classification (of information); Complex networks; Computing power; Convolutional neural networks; Deep neural networks; Economic and social effects; Embedded systems; Energy utilization; Green computing; Learning systems; Microcontrollers; Pareto principle; Adaptive neural networks; Convolutional neural network; Edge computing; Energy-consumption; Human activity recognition; Human activity recognition systems; Mixed precision; Neural-networks; Quantized neural network; Runtimes; Energy efficiency
Prediction Modeling for Application-Specific Communication Architecture Design of Optical NoC,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138427889&doi=10.1145%2f3520241&partnerID=40&md5=074b106453fd0726c40155010f060290,"Multi-core systems-on-chip are becoming state-of-the-art. Therefore, there is a need for a fast and energy-efficient interconnect to take full advantage of the computational capabilities. Integration of silicon photonics with a traditional electrical interconnect in a Network-on-Chip (NoC) proposes a promising solution for overcoming the scalability issues of electrical interconnect. In this article, we derive and evaluate prediction modeling techniques for the design space exploration (DSE) of application-specific communication architectures for an Optical Network-on-Chip (ONoC). Our proposed model accurately predicts network packet latency, contention delay, and the static and dynamic energy consumption of the network. This work specifically addresses the challenge of accurately estimating performance metrics of the entire design space without having to perform time-consuming and computationally intensive exhaustive simulations. The proposed technique, based on machine learning (ML), can build accurate prediction models using only 10% to 50% (best case and worst case) of the entire design space. The accuracy, expressed as R2 (Coefficient of Determination) is 0.99901, 0.99967, 0.99996, and 0.99999 for network packet latency, contention delay, static energy consumption, and dynamic energy consumption, respectively, in six different benchmarks from the Splash-2 benchmark suite, chosen among 6 different machine learning prediction models. © 2022 Association for Computing Machinery.",application-specific communication architecture design; design space exploration; machine learning modeling; Optical Network-on-Chip (NoC); prediction modeling; simulation,Energy efficiency; Energy utilization; Fiber optic networks; Forecasting; Integrated circuit design; Integrated circuit interconnects; Machine learning; Memory architecture; Network architecture; Optical communication; Servers; Silicon photonics; Application specific; Application-specific communication architecture design; Architecture designs; Communication architectures; Design space exploration; Machine learning models; Optical network-on-chip; Optical networks on chips; Prediction modelling; Simulation; Network-on-chip
Rethinking the Interactivity of OS and Device Layers in Memory Management,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142415482&doi=10.1145%2f3530876&partnerID=40&md5=a6ab7c5954d2f32b8a42a90f679150a6,"In the big data era, a huge number of services has placed a fast-growing demand on the capacity of DRAM-based main memory. However, due to the high hardware cost and serious leakage power/energy consumption, the growth rate of DRAM capacity cannot meet the increased rate of the required main memory space when the energy or hardware cost is a critical concern. To tackle this issue, hybrid main-memory devices/modules have been proposed to replace the pure DRAM main memory with a hybrid main memory module that provides a large main memory space by integrating a small-sized DRAM and a large-sized non-volatile memory (NVM) into the same memory module. Although NVMs have high-density and low-cost features, they suffer from the low read/write performance and low endurance issue, compared to DRAM. Thus, inside the hybrid main-memory module, it also includes a memory management design to use DRAM as the cache of NVMs to enhance its performance and lifetime. However, it also introduces new design challenges in both the OS and the memory module. In this work, we rethink the interactivity of OS and hybrid main-memory module, and propose a cross-layer cache design that (1) utilizes the information from the operating system to optimize the hit ratio of the DRAM cache inside the memory module, and (2) takes advantage of the bulk-size (or block-based) read/write feature of NVM to minimize the time overhead on the data movement between DRAM and NVM. At the same time, this cross-layer cache design is very lightweight and only introduces limited runtime management overheads. A series of experiments was conducted to evaluate the effectiveness of the proposed cross-layer cache design. The results show that the proposed design could improve access performance for up to 88%, compared to the investigated well-known page replacement algorithms. © 2022 Association for Computing Machinery.",cache design; Hybrid main-memory; non-volatile memory (NVM); performance,Cache memory; Growth rate; Integrated circuit design; Nonvolatile storage; Cache design; Cross layer; Hybrid main memory; Interactivity; Main-memory; Memory modules; Memory-management; Non-volatile memory; Performance; Dynamic random access storage
CAN Bus Intrusion Detection Based on Auxiliary Classifier GAN and Out-of-distribution Detection,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142213778&doi=10.1145%2f3540198&partnerID=40&md5=05cc31941464333f28bb9bd968aaf0b8,"The Controller Area Network (CAN) is a ubiquitous bus protocol present in the Electrical/Electronic (E/E) systems of almost all vehicles. It is vulnerable to a range of attacks once the attacker gains access to the bus through the vehicle's attack surface. We address the problem of Intrusion Detection on the CAN bus and present a series of methods based on two classifiers trained with Auxiliary Classifier Generative Adversarial Network (ACGAN) to detect and assign fine-grained labels to Known Attacks and also detect the Unknown Attack class in a dataset containing a mixture of (Normal + Known Attacks + Unknown Attack) messages. The most effective method is a cascaded two-stage classification architecture, with the multi-class Auxiliary Classifier in the first stage for classification of Normal and Known Attacks, passing Out-of-Distribution (OOD) samples to the binary Real-Fake Classifier in the second stage for detection of the Unknown Attack class. Performance evaluation demonstrates that our method achieves both high classification accuracy and low runtime overhead, making it suitable for deployment in the resource-constrained in-vehicle environment. © 2022 Association for Computing Machinery.",Automotive security; controller area network; deep learning; GAN; intrusion detection,Classification (of information); Controllers; Deep learning; Generative adversarial networks; Intrusion detection; Process control; Vehicles; Automotive security; Automotives; Bus protocol; Controller-area network; Controller-area-network bus; Deep learning; Electrical/Electronics; GAN; Intrusion-Detection; Unknown attacks; Control system synthesis
wfspan: Wait-free Dynamic Memory Management,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134296476&doi=10.1145%2f3533724&partnerID=40&md5=cecbe4df290a2dac9f3c9115f2792b24,"Dynamic memory allocation plays a vital role in modern application programs. Modern lock-free memory allocators based on hardware atomic primitives usually provide good performance. However, threads may starve in these lock-free implementations, leading to unbounded worst-case execution time that is not allowed in real-time embedded systems. This article presents decentralized dynamic memory management, wfspan, based on non-linearizable wait-free lists. It employs a helping mechanism to ensure no starvation in the lock-free implementation. From the perspective of design tradeoff, wfspan guarantees bounded execution steps in both allocation and deallocation procedure, at the cost of increasing bounded worst-case memory footprint. The results of running benchmarks on an x86/64 and an aarch64 machine illustrate that wfspan achieves competitive performance and memory footprint compared to lock-based and lock-free practical memory allocators while showing superior to other allocators in terms of worst-case execution time. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",concurrent algorithms; lock-free; Memory allocator; real-time systems; wait-free,Application programs; Benchmarking; Embedded systems; Interactive computer systems; Locks (fasteners); Memory architecture; Storage allocation (computer); Concurrent algorithms; Dynamic memory allocation; Dynamic memory management; Free dynamics; Lock-free; Memory allocators; Memory footprint; Real - Time system; Wait free; Worst-case execution time; Real time systems
How Flexible is Your Computing System?,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133603694&doi=10.1145%2f3524861&partnerID=40&md5=40f62b36c88b9a19a0575db6fd7c0d4d,"In literature, computer architectures are frequently claimed to be highly flexible, typically implying the existence of trade-offs between flexibility and performance or energy efficiency. Processor flexibility, however, is not very sharply defined, and consequently these claims cannot be validated, nor can such hypothetical relations be fully understood and exploited in the design of computing systems. This paper is an attempt to introduce scientific rigour to the notion of flexibility in computing systems. A survey is conducted to provide an overview of references to flexibility in literature, both in the computer architecture domain, as well as related fields. A classification is introduced to categorize different views on flexibility, which ultimately form the foundation for a qualitative definition of flexibility. Departing from the qualitative definition of flexibility, a generic quantifiable metric is proposed, enabling valid quantitative comparison of the flexibility of various architectures. To validate the proposed method, and evaluate the relation between the proposed metric and the general notion of flexibility, the flexibility metric is measured for 25 computing systems, including CPUs, GPUs, DSPs, and FPGAs, and 40 ASIPs taken from literature. The obtained results provide insights into some of the speculative trade-offs between flexibility and properties such as energy efficiency and area efficiency. Overall the proposed quantitative flexibility metric shows to be commensurate with some generally accepted qualitative notions of flexibility collected in the survey, although some surprising discrepancies can also be observed. The proposed metric and the obtained results are placed into context of the state of the art on compute flexibility, and extensive reflection provides not only a complete overview of the field, but also discusses possible alternative approaches and open issues. Note that this work does not aim to provide a final answer to the definition of flexibility, but rather provides a framework to initiate a broader discussion in the computer architecture society on defining, understanding, and ultimately taking advantage of flexibility. © 2022 Copyright held by the owner/author(s).",Flexibility; metric; versatility,Commerce; Economic and social effects; Energy efficiency; Program processors; Surveys; Area efficiency; Computing system; Flexibility; Metric; Performance; Property; Quantitative comparison; State of the art; Trade off; Versatility; Computer architecture
Scheduling in Real-Time Mobile Systems,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134599489&doi=10.1145%2f3517747&partnerID=40&md5=9b472eae45f93fc7e2c2ba0f21edb312,"To guarantee the safety and security of a real-time mobile system such as an intelligent transportation system, it is necessary to model and analyze its behaviors prior to actual development. In particular, the mobile objects in such systems must be isolated from each other so that they do not collide with each other. Since isolation means two or more mobile objects must not be located in the same place at the same time, a scheduling policy is required to control and coordinate the movement of such objects. However, traditional scheduling theories are based on task scheduling which is coarse-grained and cannot be directly used for fine-grained isolation controls. In this article, we first propose a fine-grained event-based formal model called a time dependency structure and use it to model and analyze real-time mobile systems. Next, an event-based schedule is defined and the composition of schedules is discussed. Then, we investigate the schedulability of isolation - that is, checking whether a given schedule ensures the isolation relationship among mobile objects or not. After that, we present an automation approach for scheduling generation to guarantee isolation controls in real-time mobile systems. Finally, case studies and simulation experiments demonstrate the usability and effectiveness of our approach.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",ambient; isolation; Mobility; scheduling policy,Intelligent systems; Real time systems; Ambients; Event-based; Fine grained; Isolation; Mobile objects; Mobile systems; Mobility; Modelling and analysis; Real- time; Scheduling policies; Scheduling
Camaroptera: A Long-range Image Sensor with Local Inference for Remote Sensing Applications,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134589956&doi=10.1145%2f3510850&partnerID=40&md5=17b7074be56fce199480831c6f3785af,"Batteryless image sensors present an opportunity for long-life, long-range sensor deployments that require zero maintenance, and have low cost. Such deployments are critical for enabling remote sensing applications, e.g., instrumenting national highways, where individual devices are deployed far (kms away) from supporting infrastructure. In this work, we develop and characterize Camaroptera, the first batteryless image-sensing platform to combine energy-harvesting with active, long-range (LoRa) communication. We also equip Camaroptera with a Machine Learning-based processing pipeline to mitigate costly, long-distance communication of image data. This processing pipeline filters out uninteresting images and only transmits the images interesting to the application. We show that compared to running a traditional Sense-and-Send workload, Camaroptera's Local Inference pipeline captures and sends upto more images of interest to an application. By performing Local Inference, Camaroptera also sends upto fewer uninteresting images, instead using that energy to capture upto more new images, increasing its sensing effectiveness and availability. We fully prototype the Camaroptera hardware platform in a compact, 2 cm 3 cm 5 cm volume. Our evaluation demonstrates the viability of a batteryless, remote, visual-sensing platform in a small package that collects and usefully processes acquired data and transmits it over long distances (kms), while being deployed for multiple decades with zero maintenance.  © 2022 Association for Computing Machinery.",batteryless computing; edge computing; Energy-harvesting sensor; long-lifetime sensing; long-range sensing,Costs; Data handling; Edge computing; Energy harvesting; Image processing; Image sensors; Pipeline processing systems; Remote sensing; Battery-less; Batteryless computing; Edge computing; Energy harvesting sensors; Long lifetime; Long-lifetime sensing; Long-range sensing; Range sensing; Remote sensing applications; Sensing platforms; Pipelines
Store-n-Learn: Classification and Clustering with Hyperdimensional Computing across Flash Hierarchy,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134626832&doi=10.1145%2f3503541&partnerID=40&md5=0510ed19788a102fb7f788c0e19afb58,"Processing large amounts of data, especially in learning algorithms, poses a challenge for current embedded computing systems. Hyperdimensional (HD) computing (HDC) is a brain-inspired computing paradigm that works with high-dimensional vectors called hypervectors. HDC replaces several complex learning computations with bitwise and simpler arithmetic operations at the expense of an increased amount of data due to mapping the data into high-dimensional space. These hypervectors, more often than not, cannot be stored in memory, resulting in long data transfers from storage. In this article, we propose Store-n-Learn, an in-storage computing solution that performs HDC classification and clustering by implementing encoding, training, retraining, and inference across the flash hierarchy. To hide the latency of training and enable efficient computation, we introduce the concept of batching in HDC. We also present on-chip acceleration for HDC encoding in flash planes. This enables us to exploit the high parallelism provided by the flash hierarchy and encode multiple data points in parallel in both batched and non-batched fashion. Store-n-Learn also implements a single top-level FPGA accelerator with novel implementations for HDC classification training, retraining, inference, and clustering on the encoded data. Our evaluation over 10 popular datasets shows that Store-n-Learn is on average 222× (543×) faster than CPU and 10.6× (7.3×) faster than the state-of-the-art in-storage computing solution, INSIDER for HDC classification (clustering).  © 2022 Copyright held by the owner/author(s).",classification; clustering; Hyperdimensional computing; in-storage computing,Data transfer; Digital storage; Embedded systems; Encoding (symbols); Learning algorithms; Signal encoding; 'current; Classification and clustering; Clusterings; Computing solutions; Embedded computing system; Encodings; Hyperdimensional computing; In-storage computing; Large amounts of data; Learn+; Classification (of information)
AppAxO: Designing Application-specific Approximate Operators for FPGA-based Embedded Systems,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134627009&doi=10.1145%2f3513262&partnerID=40&md5=b11d37adcfd249a2f8bd1ddbaea34f10,"Approximate arithmetic operators, such as adders and multipliers, are increasingly used to satisfy the energy and performance requirements of resource-constrained embedded systems. However, most of the available approximate operators have an application-agnostic design methodology, and the efficacy of these operators can only be evaluated by employing them in the applications. Furthermore, the various available libraries of approximate operators do not share any standard approximation-induction policy to design new operators according to an application's accuracy and performance constraints. These limitations also hinder the utilization of machine learning models to explore and determine approximate operators according to an application's requirements. In this work, we present a generic design methodology for implementing FPGA-based application-specific approximate arithmetic operators. Our proposed technique utilizes lookup tables and carry-chains of FPGAs to implement approximate operators according to the input configurations. For instance, for an accurate multiplier utilizing K lookup tables, our methodology utilizes K-bit configurations to design approximate multipliers. We then utilize various machine learning models to evaluate and select configurations satisfying application accuracy and performance constraints. We have evaluated our proposed methodology for three benchmark applications, i.e., biomedical signal processing, image processing, and ANNs. We report more non-dominated approximate multipliers with better hypervolume contribution than state-of-the-art designs for these benchmark applications with the proposed design methodology.  © 2022 Association for Computing Machinery.",Approximate computing; arithmetic circuits; energy efficient computing; FPGA; high-level synthesis; machine learning models; multipliers,Benchmarking; Embedded systems; Energy efficiency; High level synthesis; Image processing; Integrated circuit design; Learning systems; Machine learning; Table lookup; Timing circuits; Application specific; Approximate computing; Arithmetic circuit; Benchmark applications; Design Methodology; Energy-efficient computing; High-level synthesis; Machine learning models; Multiplier; Performance constraints; Field programmable gate arrays (FPGA)
A Framework for Calculating WCET Based on Execution Decision Diagrams,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134604935&doi=10.1145%2f3476879&partnerID=40&md5=94f0e862f72627e89af7fb9ecb8b3859,"Due to the dynamic behaviour of acceleration mechanisms such as caches and branch predictors, static Worst-case Execution Time (WCET) analysis methods tend to scale poorly to modern hardware architectures. As a result, a trade-off must be found between the duration and the precision of the analysis, leading to an overestimation of the WCET bounds. In turn, this reduces the schedulability and resource usage of the system. In this article, we present a new data structure to speed up the analysis: the eXecution Decision Diagram (XDD), which is an ad hoc extension of Binary Decision Diagrams tailored for WCET analysis problems. We show how XDDs can be used to represent efficiently execution states in a modern hardware platform. Moreover, we propose a new process to build the Integer Linear Programming system of the Implicit Path Enumeration Technique using XDD. We use benchmark applications to demonstrate how the use of an XDD substantially increases the scalability of WCET analysis and the precision of the obtained WCET.  © 2022 Association for Computing Machinery.",pipeline analysis; Static WCET analysis; timing anomalies; variable latencies,Benchmarking; Computer aided design; Economic and social effects; Embedded systems; Integer programming; Timing circuits; Acceleration mechanisms; Decision diagram; Dynamic behaviors; Pipeline analyse; Static bad-case execution time analyse; Time based; Timing anomalies; Variable latencies; Worst-case execution time; Worst-case execution-time analysis; Binary decision diagrams
How to Enable Index Scheme for Reducing the Writing Cost of DNA Storage on Insertion and Deletion,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134588329&doi=10.1145%2f3516482&partnerID=40&md5=a0f8214ae00d7c7e0a480a0ca76d22d6,"Recently, the requirement of storing digital data has been growing rapidly; however, the conventional storage medium cannot satisfy these huge demands. Fortunately, thanks to biological technology development, storing digital data into deoxyribonucleic acid (DNA) has become possible in recent years. Furthermore, because of the attractive features (e.g., high storing density, long-term durability, and stability), DNA storage has been regarded as a potential alternative storage medium to store massive digital data in the future. Nevertheless, reading and writing digital data over DNA requires a series of extremely time-consuming processes (i.e., DNA sequencing and DNA synthesis). More specifically, among the two costs, the writing cost is the predominant cost of a DNA data storage system. Therefore, to enable efficient DNA storage, this article proposes an index management scheme for reducing the number of accesses to DNA storage. Additionally, this article introduces a new DNA data encoding format with VERA (Version Editing Recovery Approach) to reduce the total writing bits while inserting and deleting the data. To the best of our knowledge, this work is the first work to provide a total data management solution for DNA storage. According to the experimental results, the proposed design with VERA can reduce the cost by 77% and improve the performance by 71% compared to the append-only methods.  © 2022 Association for Computing Machinery.",data storage; data update; indexing scheme; Molecular DNA,Bioinformatics; Cost reduction; Digital storage; DNA sequences; Gene encoding; Information management; Biological technology; Data storage; Data update; Digital datas; Indexing scheme; Insertions and deletions; Long term durability; Molecular deoxyribonucleic acid; Storage medium; Technology development; DNA
Introduction to the Special Issue on Memory and Storage Systems for Embedded and IoT Applications: Part 2,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134612531&doi=10.1145%2f3531707&partnerID=40&md5=c28af11915ff73375d19334349418774,[No abstract available],,
Physics-Driven Page Fault Handling for Customized Deception against CPS Malware,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134559215&doi=10.1145%2f3502742&partnerID=40&md5=9ef4ce78c04f1bb31e3a3bb9b5e9eed9,"Malware crafted to attack cyber-physical systems such as the electrical power grid have a physics-centric nucleus. Cyber-physical systems malware understand physics and hence use their knowledge to guide how they initiate physical damage on a compromised industrial computer. We develop a physics-driven page fault handler in the seL4 microkernel, which, in addition to reducing the page fault rate, differentiates active physics in main memory from passive physics in the backing store. We aid the identification of active physics via a CPU scheduler that tracks the evolution of active physics over time. We exploit the concept of active physics to develop deception that is customized to attack the physics-centric nucleus of malware. We evaluated this research against a variety of malware samples and techniques, including both numerous samples from publicly available repositories and custom-made academic code, and present our findings in the article. The physics data of reference pertain to an electrical substation, with a higher focus on a power transformer and related industrial computer algorithms.  © 2022 Association for Computing Machinery.",Cyber-physical systems; defensive cyber deception; industrial malware; machine learning; operating systems,Electric power transmission networks; Embedded systems; Machine learning; Malware; Power transformers; Cybe-physical systems; Cyber-physical systems; Defensive cybe deception; Electrical power; Fault handling; Industrial computers; Industrial malware; Machine-learning; Malwares; Operating system; Cyber Physical System
Toward Efficient and Adaptive Design of Video Detection System with Deep Neural Networks,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134621486&doi=10.1145%2f3484946&partnerID=40&md5=37796f8c57fffc2308c650d0bde7efea,"In the past decade, Deep Neural Networks (DNNs), e.g., Convolutional Neural Networks, achieved human-level performance in vision tasks such as object classification and detection. However, DNNs are known to be computationally expensive and thus hard to be deployed in real-time and edge applications. Many previous works have focused on DNN model compression to obtain smaller parameter sizes and consequently, less computational cost. Such methods, however, often introduce noticeable accuracy degradation. In this work, we optimize a state-of-the-art DNN-based video detection framework - Deep Feature Flow (DFF) from the cloud end using three proposed ideas. First, we propose Asynchronous DFF (ADFF) to asynchronously execute the neural networks. Second, we propose a Video-based Dynamic Scheduling (VDS) method that decides the detection frequency based on the magnitude of movement between video frames. Last, we propose Spatial Sparsity Inference, which only performs the inference on part of the video frame and thus reduces the computation cost. According to our experimental results, ADFF can reduce the bottleneck latency from 89 to 19 ms. VDS increases the detection accuracy by 0.6% mAP without increasing computation cost. And SSI further saves 0.2 ms with a 0.6% mAP degradation of detection accuracy.  © 2022 Association for Computing Machinery.",embedded software; mobile computing; model pruning; neural networks; parallel computing; real-time system; Video detection,Convolutional neural networks; Embedded systems; Interactive computer systems; Object detection; Real time systems; Computation costs; Detection accuracy; Efficient designs; Mobile-computing; Model pruning; Neural-networks; Parallel com- puting; Real - Time system; Video detection; Video frame; Deep neural networks
DFSynthesizer: Dataflow-based Synthesis of Spiking Neural Networks to Neuromorphic Hardware,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134587100&doi=10.1145%2f3479156&partnerID=40&md5=d03eb520878e90ad10a56512b6e9ffdf,"Spiking Neural Networks (SNNs) are an emerging computation model that uses event-driven activation and bio-inspired learning algorithms. SNN-based machine learning programs are typically executed on tile-based neuromorphic hardware platforms, where each tile consists of a computation unit called a crossbar, which maps neurons and synapses of the program. However, synthesizing such programs on an off-the-shelf neuromorphic hardware is challenging. This is because of the inherent resource and latency limitations of the hardware, which impact both model performance, e.g., accuracy, and hardware performance, e.g., throughput. We propose DFSynthesizer, an end-to-end framework for synthesizing SNN-based machine learning programs to neuromorphic hardware. The proposed framework works in four steps. First, it analyzes a machine learning program and generates SNN workload using representative data. Second, it partitions the SNN workload and generates clusters that fit on crossbars of the target neuromorphic hardware. Third, it exploits the rich semantics of the Synchronous Dataflow Graph (SDFG) to represent a clustered SNN program, allowing for performance analysis in terms of key hardware constraints such as number of crossbars, dimension of each crossbar, buffer space on tiles, and tile communication bandwidth. Finally, it uses a novel scheduling algorithm to execute clusters on crossbars of the hardware, guaranteeing hardware performance. We evaluate DFSynthesizer with 10 commonly used machine learning programs. Our results demonstrate that DFSynthesizer provides a much tighter performance guarantee compared to current mapping approaches.  © 2022 Association for Computing Machinery.",compiler; machine learning; mapping; Neuromorphic computing; Spiking Neural Networks (SNN); Synchronous Dataflow Graph (SDFG),Biomimetics; Clustering algorithms; Data flow analysis; Learning algorithms; Machine learning; Neural networks; Program compilers; Scheduling algorithms; Semantics; Compiler; Learning projects; Machine-learning; Network-based; Neural-networks; Neuromorphic computing; Neuromorphic hardwares; Spiking neural network; Synchronoi dataflow graph; Synchronous dataflow graphs; Mapping
Reduced Memory Viterbi Decoding for Hardware-accelerated Speech Recognition,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134574231&doi=10.1145%2f3510028&partnerID=40&md5=47ed9a9884a8d3295798769e4c5ec5e2,"Large Vocabulary Continuous Speech Recognition systems require Viterbi searching through a large state space to find the most probable sequence of phonemes that led to a given sound sample. This needs storing and updating of a large Active State List (ASL) in the on-chip memory (OCM) at regular intervals (called frames), which poses a major performance bottleneck for speech decoding. Most works use hash tables for OCM storage while beam-width pruning to restrict the ASL size. To achieve a decent accuracy and performance, a large OCM, numerous acoustic probability computations, and DRAM accesses are incurred.We propose to use a binary search tree for ASL storage and a max heap data structure to track the worst cost state and efficiently replace it when a better state is found. With this approach, the ASL size can be reduced from over 32K to 512 with minimal impact on recognition accuracy for a 7,000-word vocabulary model. This, combined with a caching technique for acoustic scores, reduced the DRAM data accessed by 31 and the acoustic probability computations by 26.The approach has also been implemented in hardware on a Xilinx Zynq FPGA at 200 MHz using the Vivado SDS compiler. We study the tradeoffs among the amount of OCM used, word error rate, and decoding speed to show the effectiveness of the approach. The resulting implementation is capable of running faster than real time with 91% lesser block-RAMs.  © 2022 Association for Computing Machinery.",binary search trees; On-chip memory; Viterbi decoding,Binary trees; Dynamic random access storage; Speech recognition; Trees (mathematics); Viterbi algorithm; Active state; Binary search trees; Hardware-accelerated; Large vocabulary continuous speech recognition; On-chip-memory; Probability computations; Reduced memory; Speech recognition systems; Viterbi; Viterbi decoding; Decoding
A Survey of Blockchain Data Management Systems,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134611409&doi=10.1145%2f3502741&partnerID=40&md5=26b5bd464360fdfb3c7f5c616ad6b107,"Blockchain has been widely deployed in various fields, such as finance, education, and public services. Blockchain has decentralized mechanisms with persistency and auditability and runs as an immutable distributed ledger, where transactions are jointly performed through cryptocurrency-based consensus algorithms by worldwide distributed nodes. There have been many survey papers reviewing the blockchain technologies from different perspectives, e.g., digital currencies, consensus algorithms, and smart contracts. However, none of them have focused on the blockchain data management systems. To fill in this gap, we have conducted a comprehensive survey on the data management systems, based on three typical types of blockchain, i.e., standard blockchain, hybrid blockchain, and DAG (Directed Acyclic Graph)-based blockchain. We categorize their data management mechanisms into three layers: blockchain architecture, blockchain data structure, and blockchain storage engine, where block architecture indicates how to record transactions on a distributed ledger, blockchain data structure refers to the internal structure of each block, and blockchain storage engine specifies the storage form of data on the blockchain system. For each layer, the works advancing the state-of-the-art are discussed together with technical challenges. Furthermore, we lay out several possible future research directions for the blockchain data management systems.  © 2022 Association for Computing Machinery.",blockchain architecture; blockchain data structure; blockchain storage engine; Data management,Blockchain; Data structures; Directed graphs; Distributed ledger; Engines; Surveys; Virtual storage; Block-chain; Blockchain architecture; Blockchain data structure; Blockchain storage engine; Consensus algorithms; Data management system; Public services; Storage engines; Information management
Cache Interference-aware Task Partitioning for Non-preemptive Real-time Multi-core Systems,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134618840&doi=10.1145%2f3487581&partnerID=40&md5=f418667aa3689ee89e2fdc48c72f06c5,"Shared caches in multi-core processors introduce serious difficulties in providing guarantees on the real-time properties of embedded software due to the interaction and the resulting contention in the shared caches. Prior work has studied the schedulability analysis of global scheduling for real-time multi-core systems with shared caches. This article considers another common scheduling paradigm: partitioned scheduling in the presence of shared cache interference. To achieve this, we propose CITTA, a cache interference-aware task partitioning algorithm. We first analyze the shared cache interference between two programs for set-associative instruction and data caches. Then, an integer programming formulation is constructed to calculate the upper bound on cache interference exhibited by a task, which is required by CITTA. We conduct schedulability analysis of CITTA and formally prove its correctness. A set of experiments is performed to evaluate the schedulability performance of CITTA against global EDF scheduling and other greedy partition approaches such as First-fit and Worst-fit over randomly generated tasksets and realistic workloads in embedded systems. Our empirical evaluations show that CITTA outperforms global EDF scheduling and greedy partition approaches in terms of task sets deemed schedulable.  © 2022 Association for Computing Machinery.",partitioned scheduling; real-time systems; schedulability analysis; Shared caches,Embedded systems; Integer programming; Interactive computer systems; Real time systems; EDF scheduling; Interference-aware; Multi-core systems; Non-preemptive; Partitioned scheduling; Real - Time system; Real- time; Schedulability analysis; Shared cache; Task partitioning; Scheduling
DL-RSIM: A Reliability and Deployment Strategy Simulation Framework for ReRAM-based CNN Accelerators,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134577471&doi=10.1145%2f3507639&partnerID=40&md5=bfb89042b0336797ba5e4f5b3b11ba79,"Memristor-based deep learning accelerators provide a promising solution to improve the energy efficiency of neuromorphic computing systems. However, the electrical properties and crossbar structure of memristors make these accelerators error-prone. In addition, due to the hardware constraints, the way to deploy neural network models on memristor crossbar arrays affects the computation parallelism and communication overheads. To enable reliable and energy-efficient memristor-based accelerators, a simulation platform is needed to precisely analyze the impact of non-ideal circuit/device properties on the inference accuracy and the influence of different deployment strategies on performance and energy consumption. In this paper, we propose a flexible simulation framework, DL-RSIM, to tackle this challenge. A rich set of reliability impact factors and deployment strategies are explored by DL-RSIM, and it can be incorporated with any deep learning neural networks implemented by TensorFlow. Using several representative convolutional neural networks as case studies, we show that DL-RSIM can guide chip designers to choose a reliability-friendly design option and energy-efficient deployment strategies and develop optimization techniques accordingly.  © 2022 Association for Computing Machinery.",deep learning accelerator; energy efficiency; processing-in-memory; reliability; resistive random access memory; Simulation framework,Acceleration; Convolutional neural networks; Deep learning; Energy utilization; Green computing; Memristors; Reliability; RRAM; Simulation platform; Deep learning accelerator; Deployment strategy; Energy efficient; Memristor; Processing-in-memory; Random access memory; Reliability strategy; Resistive random access memory; Simulation framework; Strategy simulation; Energy efficiency
Minimizing Stack Memory for Partitioned Mixed-criticality Scheduling on Multiprocessor Platforms,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127455313&doi=10.1145%2f3506703&partnerID=40&md5=9fbd4b045bf7a253cd4fe0c6c02bc9da,"A Mixed-Criticality System (MCS) features the integration of multiple subsystems that are subject to different levels of safety certification on a shared hardware platform. In cost-sensitive application domains such as automotive E/E systems, it is important to reduce application memory footprint, since such a reduction may enable the adoption of a cheaper microprocessor in the family. Preemption Threshold Scheduling (PTS) is a well-known technique for reducing system stack usage. We consider partitioned multiprocessor scheduling, with Preemption Threshold Adaptive Mixed-Criticality (PT-AMC) as the task scheduling algorithm on each processor and address the optimization problem of finding a feasible task-To-processor mapping with minimum total system stack usage on a resource-constrained multi-processor. We present the Extended Maximal Preemption Threshold Assignment Algorithm (EMPTAA), with dual purposes of improving the taskset's schedulability if it is not already schedulable, and minimizing system stack usage of the schedulable taskset. We present efficient heuristic algorithms for finding sub-optimal yet high-quality solutions, including Maximum Utilization Difference based Partitioning (MUDP) and MUDP with Backtrack Mapping (MUDP-BM), as well as a Branch-And-Bound (BnB) algorithm for finding the optimal solution. Performance evaluation with synthetic task sets demonstrates the effectiveness and efficiency of the proposed algorithms.  © 2022 Association for Computing Machinery.",Mixed-criticality systems; multiprocessor scheduling; partitioned scheduling; preemption threshold scheduling,Constrained optimization; Criticality (nuclear fission); Heuristic algorithms; Mapping; Multiprocessing systems; Scheduling algorithms; Mixed criticalities; Mixed-criticality systems; Multi processor scheduling; Multi-processor platforms; Partitioned scheduling; Preemption threshold scheduling; Preemption thresholds; Stack memory; System features; System stacks; Scheduling
Protecting Network-on-Chip Intellectual Property Using Timing Channel Fingerprinting,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127432374&doi=10.1145%2f3495565&partnerID=40&md5=d8c681ea4cea99b76b4831e474e8005f,"The theft of Intellectual property (IP) is a serious security threat for all businesses that are involved in the creation of IP. In this article, we consider such attacks against IP for Network-on-Chip (NoC) that are commonly used as a popular on-chip scalable communication medium for Multiprocessor System-on-Chip. As a protection mechanism, we propose a timing channel fingerprinting method and show its effectiveness by implementing five different solutions using this method. We also provide a formal proof of security of the proposed method. We show that the proposed technique provides better security and requires much lower hardware overhead (64%-74% less) compared to an existing NoC IP security solution without affecting the normal packet latency or degrading the NoC performance.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",fingerprinting technique; Intellectual property protection; NoC IP protection; timing channel,Intellectual property core; Internet protocols; Network-on-chip; Servers; Timing circuits; Communication media; Fingerprinting techniques; Intellectual property protection; Network-on-chip intellectual property protection; Networks on chips; On chips; Scalable communication; Security threats; Timing channels; Intellectual property
Efficient-Grad: Efficient Training Deep Convolutional Neural Networks on Edge Devices with Gradient Optimizations,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127438068&doi=10.1145%2f3504034&partnerID=40&md5=b2dadf664f2b9581a6e1544fa7313444,"With the prospering of mobile devices, the distributed learning approach, enabling model training with decentralized data, has attracted great interest from researchers. However, the lack of training capability for edge devices significantly limits the energy efficiency of distributed learning in real life. This article describes Efficient-Grad, an algorithm-hardware co-design approach for training deep convolutional neural networks, which improves both throughput and energy saving during model training, with negligible validation accuracy loss. The key to Efficient-Grad is its exploitation of two observations. Firstly, the sparsity has potential for not only activation and weight, but gradients and the asymmetry residing in the gradients for the conventional back propagation (BP). Secondly, a dedicated hardware architecture for sparsity utilization and efficient data movement can be optimized to support the Efficient-Grad algorithm in a scalable manner. To the best of our knowledge, Efficient-Grad is the first approach that successfully adopts a feedback-Alignment (FA)-based gradient optimization scheme for deep convolutional neural network training, which leads to its superiority in terms of energy efficiency. We present case studies to demonstrate that the Efficient-Grad design outperforms the prior arts by 3.72x in terms of energy efficiency.  © 2022 Association for Computing Machinery.",Deep neural networks; edge computing; gradient pruning; hardware acceleration; model training,Arts computing; Backpropagation; Computer hardware; Convolution; Convolutional neural networks; Edge computing; Energy efficiency; Multilayer neural networks; Co-design approach; Decentralised; Distributed learning; Edge computing; Energy-savings; Gradient optimization; Gradient pruning; Hardware acceleration; Learning approach; Model training; Deep neural networks
Scenario Based Run-Time Switching for Adaptive CNN-Based Applications at the Edge,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127427264&doi=10.1145%2f3488718&partnerID=40&md5=a7114d46b7b5943e26b252243a5c3763,"Convolutional Neural Networks (CNNs) are biologically inspired computational models that are at the heart of many modern computer vision and natural language processing applications. Some of the CNN-based applications are executed on mobile and embedded devices. Execution of CNNs on such devices places numerous demands on the CNNs, such as high accuracy, high throughput, low memory cost, and low energy consumption. These requirements are very difficult to satisfy at the same time, so CNN execution at the edge typically involves trade-offs (e.g., high CNN throughput is achieved at the cost of decreased CNN accuracy). In existing methodologies, such trade-offs are either chosen once and remain unchanged during a CNN-based application execution, or are adapted to the properties of the CNN input data. However, the application needs can also be significantly affected by the changes in the application environment, such as a change of the battery level in the edge device. Thus, CNN-based applications need a mechanism that allows to dynamically adapt their characteristics to the changes in the application environment at run-Time. Therefore, in this article, we propose a scenario-based run-Time switching (SBRS) methodology, that implements such a mechanism.  © 2022 Association for Computing Machinery.",Convolutional neural networks; execution at the edge; run-Time adaptation,Biomimetics; Commerce; Convolutional neural networks; Economic and social effects; Energy utilization; Natural language processing systems; Application environment; Biologically-inspired; Convolutional neural network; Execution at the edge; Network based applications; Runtime adaptation; Runtimes; Scenario-based; Time switching; Trade off; Convolution
EC-ECC: Accelerating Elliptic Curve Cryptography for Edge Computing on Embedded GPU TX2,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125914916&doi=10.1145%2f3492734&partnerID=40&md5=193a6c3f7c2011ae19aab48d34cd1761,"Driven by artificial intelligence and computer vision industries, Graphics Processing Units (GPUs) are now rapidly achieving extraordinary computing power. In particular, the NVIDIA Tegra K1/X1/X2 embedded GPU platforms, which are also treated as edge computing devices, are now widely used in embedded environments such as mobile phones, game consoles, and vehicle-mounted systems to support high-dimension display, auto-pilot, and so on. Meanwhile, with the rise of the Internet of Things (IoT), the demand for cryptographic operations for secure communications and authentications between edge computing nodes and IoT devices is also expanding. In this contribution, instead of the conventional implementations based on FPGA, ASIC, and ARM CPUs, we provide an alternative solution for cryptographic implementation on embedded GPU devices. Targeting the new cipher suite added in TLS 1.3, we implement Edwards25519/448 and Curve25519/448 on an edge computing platform, embedded GPU NVIDIA Tegra X2, where various performance optimizations are customized for the target platform, including a novel parallel method for the register-limited embedded GPUs. With about 15 W of power consumption, it can provide 210k/31k ops/s of Curve25519/448 scalar multiplication, 834k/123k ops/s of fixed-point Edwards25519/448 scalar multiplication, and 150k/22k ops/s of unknown-point one, which are respectively the primitives and main workloads of key agreement, signature generation, and verification of the TLS 1.3 protocol. Our implementations achieve 8 to 26 times speedup of OpenSSL running in the very powerful ARM CPU of the same platform and outperform the state-of-The-Art implementations in FPGA by a wide margin with better power efficiency.  © 2022 Association for Computing Machinery.",ECC; edge computing; embedded graphics processing units,Computer games; Computer graphics; Display devices; Embedded systems; Field programmable gate arrays (FPGA); Graphics processing unit; Internet of things; Program processors; Public key cryptography; Computing devices; Computing power; ECC; Edge computing; Embedded environment; Embedded graphic processing unit; Graphics processing; Mobile phone games; Processing units; Scalar multiplication; Edge computing
Read Refresh Scheduling and Data Reallocation against Read Disturb in SSDs,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127446462&doi=10.1145%2f3495254&partnerID=40&md5=98b17ab4f8ffd8c6f9d7ea490e5d9b80,"Read disturb is a circuit-level noise in flash-based Solid-State Drives (SSDs), induced by intensive read requests, which may result in unexpected read errors. The approach of read refresh (RR) is commonly adopted to mitigate its negative effects by unconditionally migrating all valid data pages in the RR block to another new block. However, routine RR operations greatly impact the I/O responsiveness of SSDs, because the processing on normal I/O requests must be blocked at the same time. To further reduce the negative effects of read refresh, this article proposes a read refresh scheduling and data reallocation method to deal with two primary issues with respect to an RR operation, including where to place data pages and when to trigger page migrations. Specifically, we first construct a data reallocation model to match the data pages in the RR block and the destination blocks for addressing the issue of where to place the data. The model considers not only the read hotness of pages in the RR block, but also the accumulated read counts of the destination blocks. Moreover, for addressing the issue of when to trigger data migrations, we build a timing decision model to determine the time points for completing page migrations by considering the factors of the intensity of I/Os and the disturb situation on the RR block. Through a series of simulation experiments based on several realistic disk traces, we illustrate that the proposed RR scheduling and data reallocation mechanism can noticeably reduce the read errors by more than 10.3%, on average, and the long-Tail latency by between 43.9% and 64.0%at the 99.99th percentile, in contrast to state-of-The-Art methods.  © 2022 Association for Computing Machinery.",modeling; read disturb; read refresh; reliability; Solid-state drivers,Circuit levels; Data pages; Data-migration; Decision modeling; Modeling; Page migration; Read disturb; Read refresh; Solid-state driver; Timing decisions; Scheduling
Probabilistic Risk-Aware Scheduling with Deadline Constraint for Heterogeneous SoCs,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127427953&doi=10.1145%2f3489409&partnerID=40&md5=dc368299d4bd8bc8a8a380ed1f2cfd6a,"Hardware Trojans can compromise System-on-Chip (SoC) performance. Protection schemes implemented to combat these threats cannot guarantee 100% detection rate and may also introduce performance overhead. This paper defines the risk of running a job on an SoC as a function of the misdetection rate of the hardware Trojan detection methods implemented on the cores in the SoC. Given the user-defined deadlines of each job, our goal is to minimize the job-level risk as well as the deadline violation rate for both static and dynamic scheduling scenarios. We assume that there is no relationship between the execution time and risk of a task executed on a core. Our risk-Aware scheduling algorithm first calculates the probability of possible task allocations and then uses it to derive the task-level deadlines. Each task is then allocated to the core with minimum risk that satisfies the task-level deadline. In addition, in dynamic scheduling, where multiple jobs are injected randomly, we propose to explicitly operate with a reduced virtual deadline to avoid possible future deadline violations. Simulations on randomly generated graphs show that our static scheduler has no deadline violations and achieves 5.1%-17.2% lower job-level risk than the popular Earliest Time First (ETF) algorithm when the deadline constraint is 1.2×-3.0× the makespan of ETF. In the dynamic case, the proposed algorithm achieves a violation rate comparable to that of Earliest Deadline First (EDF), an algorithm optimized for dynamic scenarios. Even when the injection rate is high, it outperforms EDF with 8.4%-10% lower risk when the deadline is 1.5×-3.0× the makespan of ETF.  © 2022 Association for Computing Machinery.",dynamic scheduling; heterogeneous SoC; scheduling algorithm; static scheduling; System on Chip(SoC),Malware; Programmable logic controllers; Response time (computer systems); Risk perception; Scheduling algorithms; Silica; System-on-chip; Deadline constraint; Dynamic scheduling; Heterogeneous system on chip; Heterogeneous systems; Risk aware; Static scheduling; Systems-on-Chip; Task levels; Violation rates; Scheduling
DirectNVM: Hardware-accelerated NVMe SSDs for High-performance Embedded Computing,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124790720&doi=10.1145%2f3463911&partnerID=40&md5=a6edd71febf75b238bb18aacf66cbb3e,"With data-intensive artificial intelligence (AI) and machine learning (ML) applications rapidly surging, modern high-performance embedded systems, with heterogeneous computing resources, critically demand low-latency and high-bandwidth data communication. As such, the newly emerging NVMe (Non-Volatile Memory Express) protocol, with parallel queuing, access prioritization, and optimized I/O arbitration, starts to be widely adopted as a de facto fast I/O communication interface. However, effectively leveraging the potential of modern NVMe storage proves to be nontrivial and demands fine-grained control, high processing concurrency, and application-specific optimization. Fortunately, modern FPGA devices, capable of efficient parallel processing and application-specific programmability, readily meet the underlying physical layer requirements of the NVMe protocol, therefore providing unprecedented opportunities to implementing a rich-featured NVMe middleware to benefit modern high-performance embedded computing.In this article, we present how to rethink existing accessing mechanisms of NVMe storage and devise innovative hardware-assisted solutions to accelerating NVMe data access performance for the high-performance embedded computing system. Our key idea is to exploit the massively parallel I/O queuing capability, provided by the NVMe storage system, through leveraging FPGAs' reconfigurability and native hardware computing power to operate transparently to the main processor. Specifically, our DirectNVM system aims at providing effective hardware constructs for facilitating high-performance and scalable userspace storage applications through (1) hardening all the essential NVMe driver functionalities, therefore avoiding expensive OS syscalls and enabling zero-copy data access from the application, (2) relying on hardware for the I/O communication control instead of relying on OS-level interrupts that can significantly reduce both total I/O latency and its variance, and (3) exposing cutting-edge and application-specific weighted-round-robin I/O traffic scheduling to the userspace.To validate our design methodology, we developed a complete DirectNVM system utilizing the Xilinx Zynq MPSoC architecture that incorporates a high-performance application processor (APU) equipped with DDR4 system memory and a hardened configurable PCIe Gen3 block in its programmable logic part. We then measured the storage bandwidth and I/O latency of both our DirectNVM system and a conventional OS-based system when executing the standard FIO benchmark suite [2]. Specifically, compared against the PetaLinux built-in kernel driver code running on a Zynq MPSoC, our DirectNVM has shown to achieve up to 18.4× higher throughput and up to 4.5× lower latency. To ensure the fairness of our performance comparison, we also measured our DirectNVM system against the Intel SPDK [26], a highly optimized userspace asynchronous NVMe I/O framework running on a X86 PC system. Our experiment results have shown that our DirectNVM, even running on a considerably less powerful embedded ARM processor than a full-scale AMD processor, achieved up to 2.2× higher throughput and 1.3× lower latency. Furthermore, by experimenting with a multi-threading test case, we have demonstrated that our DirectNVM's weighted-round-robin scheduling can significantly optimize the bandwidth allocation between latency-constraint frontend applications and other backend applications in real-time systems. Finally, we have developed a theoretical framework of performance modeling with classic queuing theory that can quantitatively define the relationship between a system's I/O performance and its I/O implementation. © 2022 Association for Computing Machinery.",FPGA; high-Throughput high-Performance computing; NVMe; SSDs,Bandwidth; Computation theory; Digital storage; Embedded systems; Middleware; Network layers; Application specific; Embedded computing; High-throughput; High-throughput high-performance computing; Low latency; Modern high performance; Non-volatile memory express; Performance; Performance computing; SSD; Field programmable gate arrays (FPGA)
An Energy-Efficient DRAM Cache Architecture for Mobile Platforms with PCM-Based Main Memory,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124794922&doi=10.1145%2f3451995&partnerID=40&md5=2baa15eeb502d1f1b988fbb248e6604a,"A long battery life is a first-class design objective for mobile devices, and main memory accounts for a major portion of total energy consumption. Moreover, the energy consumption from memory is expected to increase further with ever-growing demands for bandwidth and capacity. A hybrid memory system with both DRAM and PCM can be an attractive solution to provide additional capacity and reduce standby energy. Although providing much greater density than DRAM, PCM has longer access latency and limited write endurance to make it challenging to architect it for main memory. To address this challenge, this article introduces CAMP, a novel DRAM cache architecture for mobile platforms with PCM-based main memory. A DRAM cache in this environment is required to filter most of the writes to PCM to increase its lifetime, and deliver highest efficiency even for a relatively small-sized DRAM cache that mobile platforms can afford. To address this CAMP divides DRAM space into two regions: a page cache for exploiting spatial locality in a bandwidth-efficient manner and a dirty block buffer for maximally filtering writes. CAMP improves the performance and energy-delay-product by 29.2% and 45.2%, respectively, over the baseline PCM-oblivious DRAM cache, while increasing PCM lifetime by 2.7×. And CAMP also improves the performance and energy-delay-product by 29.3% and 41.5%, respectively, over the state-of-the-art design with dirty block buffer, while increasing PCM lifetime by 2.5×. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",DRAM cache; hybrid memory system; phase change memory (PCM),Bandwidth; Cache memory; Energy efficiency; Energy utilization; Integrated circuit design; Memory architecture; Phase change memory; Cache architecture; DRAM cache; Energy-consumption; Hybrid memory; Hybrid memory system; Main-memory; Memory systems; Mobile platform; Phase change memory; Phase-change memory; Dynamic random access storage
CORIDOR: Using COherence and TempoRal LocalIty to Mitigate Read Disurbance ErrOR in STT-RAM Caches,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124794364&doi=10.1145%2f3484493&partnerID=40&md5=401860175439dd4f4064d83b9cd1149e,"In the deep sub-micron region, ""spin-transfer torque RAM""(STT-RAM) suffers from ""read-disturbance error""(RDE), whereby a read operation disturbs the stored data. Mitigation of RDE requires restore operations, which imposes latency and energy penalties. Hence, RDE presents a crucial threat to the scaling of STT-RAM. In this paper, we offer three techniques to reduce the restore overhead. First, we avoid the restore operations for those reads, where the block will get updated at a higher level cache in the near future. Second, we identify read-intensive blocks using a lightweight mechanism and then migrate these blocks to a small SRAM buffer. On a future read to these blocks, the restore operation is avoided. Third, for data blocks having zero value, a write operation is avoided, and only a flag is set. Based on this flag, both read and restore operations to this block are avoided. We combine these three techniques to design our final policy, named CORIDOR. Compared to a baseline policy, which performs restore operation after each read, CORIDOR achieves a 31.6% reduction in total energy and brings the relative CPI (cycle-per-instruction) to 0.64×. By contrast, an ideal RDE-free STT-RAM saves 42.7% energy and brings the relative CPI to 0.62×. Thus, our CORIDOR policy achieves nearly the same performance as an ideal RDE-free STT-RAM cache. Also, it reaches three-fourths of the energy-saving achieved by the ideal RDE-free cache. We also compare CORIDOR with four previous techniques and show that CORIDOR provides higher restore energy savings than these techniques. © 2022 Association for Computing Machinery.",non-volatile memory; on-chip cache; Read-disturbance; reliability,Computer aided design; Energy conservation; Restoration; Static random access storage; Cycles per instructions; Deep sub-micron; Deep-sub microns; Energy  savings; Energy-savings; On-chip cache; Read operation; Read-disturbance; Spin transfer torque; Temporal locality; Cache memory
L2C: Combining Lossy and Lossless Compression on Memory and I/O,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124791025&doi=10.1145%2f3481641&partnerID=40&md5=91582201dcf5293829349872030a4dd2,"In this article, we introduce L2C, a hybrid lossy/lossless compression scheme applicable both to the memory subsystem and I/O traffic of a processor chip. L2C employs general-purpose lossless compression and combines it with state-of-the-art lossy compression to achieve compression ratios up to 16:1 and to improve the utilization of chip's bandwidth resources. Compressing memory traffic yields lower memory access time, improving system performance, and energy efficiency. Compressing I/O traffic offers several benefits for resource-constrained systems, including more efficient storage and networking. We evaluate L2C as a memory compressor in simulation with a set of approximation-tolerant applications. L2C improves baseline execution time by an average of 50% and total system energy consumption by 16%. Compared to the lossy and lossless current state-of-the-art memory compression approaches, L2C improves execution time by 9% and 26%, respectively, and reduces system energy costs by 3% and 5%, respectively. I/O compression efficacy is evaluated using a set of real-life datasets. L2C achieves compression ratios of up to 10.4:1 for a single dataset and on average about 4:1, while introducing no more than 0.4% error. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",approximate computing; lossy compression; Memory compression,Bandwidth compression; Data compression; Energy utilization; Green computing; Memory architecture; Approximate computing; Bandwidth resource; Compression scheme; Lossless compression; Lossy compressions; Memory compression; Memory subsystems; Processor chips; State of the art; System energy; Energy efficiency
Software Hint-Driven Data Management for Hybrid Memory in Mobile Systems,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124792516&doi=10.1145%2f3494536&partnerID=40&md5=8d5fdb4b907489a3126fc86039e8a716,"Hybrid memory systems, comprised of emerging non-volatile memory (NVM) and DRAM, have been proposed to address the growing memory demand of current mobile applications. Recently emerging NVM technologies, such as phase-change memories (PCM), memristor, and 3D XPoint, have higher capacity density, minimal static power consumption and lower cost per GB. However, NVM has longer access latency and limited write endurance as opposed to DRAM. The different characteristics of distinct memory classes render a new challenge for memory system design.Ideally, pages should be placed or migrated between the two types of memories according to the data objects' access properties. Prior system software approaches exploit the program information from OS but at the cost of high software latency incurred by related kernel processes. Hardware approaches can avoid these latencies, however, hardware's vision is constrained to a short time window of recent memory requests, due to the limited on-chip resources.In this work, we propose OpenMem: a hardware-software cooperative approach that combines the execution time advantages of pure hardware approaches with the data object properties in a global scope. First, we built a hardware-based memory manager unit (HMMU) that can learn the short-term access patterns by online profiling, and execute data migration efficiently. Then, we built a heap memory manager for the heterogeneous memory systems that allows the programmer to directly customize each data object's allocation to a favorable memory device within the presumed object life cycle. With the programmer's hints guiding the data placement at allocation time, data objects with similar properties will be congregated to reduce unnecessary page migrations.We implemented the whole system on the FPGA board with embedded ARM processors. In testing under a set of benchmark applications from SPEC 2017 and PARSEC, experimental results show that OpenMem reduces 44.6% energy consumption with only a 16% performance degradation compared to the all-DRAM memory system. The amount of writes to the NVM is reduced by 14% versus the HMMU-only, extending the NVM device lifetime. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",hardware/software co-design; heterogeneous system; Hybrid memory; mobile computing,Benchmarking; Computer software; Energy utilization; Information management; Integrated circuit design; Life cycle; Managers; Mobile computing; Phase change memory; 'current; Data objects; Hardware/software codesign; Heterogeneous systems; Hybrid memory; Memory manager; Memory systems; Mobile systems; Mobile-computing; Property; Dynamic random access storage
ARES: Persistently Secure Non-Volatile Memory with Processor-transparent and Hardware-friendly Integrity Verification and Metadata Recovery,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124801648&doi=10.1145%2f3492735&partnerID=40&md5=97ff237842e4fcfab7e3d8ef26190180,"Emerging byte-addressable Non-Volatile Memory (NVM) technology, although promising superior memory density and ultra-low energy consumption, poses unique challenges to achieving persistent data privacy and computing security, both of which are critically important to the embedded and IoT applications. Specifically, to successfully restore NVMs to their working states after unexpected system crashes or power failure, maintaining and recovering all the necessary security-related metadata can severely increase memory traffic, degrade runtime performance, exacerbate write endurance problem, and demand costly hardware changes to off-the-shelf processors.In this article, we designed and implemented ARES, a new FPGA-assisted processor-transparent security mechanism that aims at efficiently and effectively achieving all three aspects of a security triad - confidentiality, integrity, and recoverability - in modern embedded computing. Given the growing prominence of CPU-FPGA heterogeneous computing architectures, ARES leverages FPGA's hardware reconfigurability to offload performance-critical and security-related functions to the programmable hardware without microprocessors' involvement. In particular, recognizing that the traditional Merkle tree caching scheme cannot fully exploit FPGA's parallelism due to its sequential and recursive function calls, we (1) proposed a Merkle tree cache architecture that partitions a unified cache into multiple levels with parallel accesses and (2) further designed a novel Merkle tree scheme that flattened and reorganized the computation in the traditional Merkle tree verification and update processes to fully exploit the parallel cache ports and to fully pipeline time-consuming hashing operations. Beyond that, to accelerate the metadata recovery process, multiple parallel recovery units are instantiated to recover counter metadata and multiple Merkle sub-trees.Our hardware prototype of the ARES system on a Xilinx U200 platform shows that ARES achieved up to 1.4× lower latency and 2.6× higher throughput against the baseline implementation, while metadata recovery time was shortened by 1.8 times. When integrated with an embedded processor, neither hardware changes nor software changes are required. We also developed a theoretical framework to analytically model and explain experimental results. © 2022 Association for Computing Machinery.",,Cryptography; Data privacy; Energy utilization; Field programmable gate arrays (FPGA); Forestry; Metadata; Computing security; Integrity verifications; Low energy consumption; Memory density; Merkle trees; Power failure; Runtime performance; Ultra low energy; Working state; Write endurances; Recovery
Introduction to the Special Issue on Memory and Storage Systems for Embedded and IoT Applications,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124844133&doi=10.1145%2f3505283&partnerID=40&md5=6473378502f88c6482699eb6459b7095,[No abstract available],,
Performance and Power Estimation of STT-MRAM Main Memory with Reliable System-level Simulation,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124792361&doi=10.1145%2f3476838&partnerID=40&md5=edc716bcda177eeea29958d8e743be5e,"It is questionable whether DRAM will continue to scale and will meet the needs of next-generation systems. Therefore, significant effort is invested in research and development of novel memory technologies. One of the candidates for next-generation memory is Spin-Transfer Torque Magnetic Random Access Memory (STT-MRAM). STT-MRAM is an emerging non-volatile memory with a lot of potential that could be exploited for various requirements of different computing systems. Being a novel technology, STT-MRAM devices are already approaching DRAM in terms of capacity, frequency, and device size. Although STT-MRAM technology got significant attention of various major memory manufacturers, academic research of STT-MRAM main memory remains marginal. This is mainly due to the unavailability of publicly available detailed timing and current parameters of this novel technology, which are required to perform a reliable main memory simulation on performance and power estimation. This study demonstrates an approach to perform a cycle accurate simulation of STT-MRAM main memory, being the first to release detailed timing and current parameters of this technology from academia - essentially enabling researchers to conduct reliable system-level simulation of STT-MRAM using widely accepted existing simulation infrastructure. The results show a fairly narrow overall performance deviation in response to significant variations in key timing parameters, and the power consumption experiments identify the key power component that is mostly affected with STT-MRAM. © 2022 Association for Computing Machinery.",high-performance computing; main memory; STT-MRAM,Dynamic random access storage; Magnetic recording; High-performance computing; Magnetic random access memory; Main-memory; Performance computing; Performance estimation; Power estimations; Reliable systems; Spin transfer torque; Spin-transfer torque magnetic random access memory; Timing parameters; MRAM devices
Microarchitectural Exploration of STT-MRAM Last-level Cache Parameters for Energy-efficient Devices,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124795481&doi=10.1145%2f3490391&partnerID=40&md5=d03c6ac4d9044545f20a1900a7a15420,"As the technology scaling advances, limitations of traditional memories in terms of density and energy become more evident. Modern caches occupy a large part of a CPU physical size and high static leakage poses a limit to the overall efficiency of the systems, including IoT/edge devices. Several alternatives to CMOS SRAM memories have been studied during the past few decades, some of which already represent a viable replacement for different levels of the cache hierarchy. One of the most promising technologies is the spin-transfer torque magnetic RAM (STT-MRAM), due to its small basic cell design, almost absent static current and non-volatility as an added value. However, nothing comes for free, and designers will have to deal with other limitations, such as the higher latencies and dynamic energy consumption for write operations compared to reads. The goal of this work is to explore several microarchitectural parameters that may overcome some of those drawbacks when using STT-MRAM as last-level cache (LLC) in embedded devices. Such parameters include: number of cache banks, number of miss status handling registers (MSHRs) and write buffer entries, presence of hardware prefetchers. We show that an effective tuning of those parameters may virtually remove any performance loss while saving more than 60% of the LLC energy on average. The analysis is then extended comparing the energy results from calibrated technology models with data obtained with freely available tools, highlighting the importance of using accurate models for architectural exploration. © 2022 Copyright held by the owner/author(s).",Energy efficiency; gem5; last-level cache; nvsim; spec cpu2017; stt-mram,Cache memory; Energy utilization; Magnetic recording; MRAM devices; Static random access storage; Cache parameters; Energy; Energy efficient; Gem5; Last-level caches; Nvsim; Spec cpu2017; Spin transfer torque; STT-MRAM; Technology scaling; Energy efficiency
Telomere: Real-Time NAND Flash Storage,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124805697&doi=10.1145%2f3479157&partnerID=40&md5=f3f4044ac32a0af0c699b88a7f1c8d32,"Modern solid-state disks achieve high data transfer rates due to their massive internal parallelism. However, out-of-place updates for flash memory incur garbage collection costs when valid data needs to be copied during space reclamation. The root cause of this extra cost is that solid-state disks are not always able to accurately determine data lifetime and group together data that expires before the space needs to be reclaimed. Real-time systems found in autonomous vehicles, industrial control systems, and assembly-line robots store data from hundreds of sensors and often have predictable data lifetimes. These systems require guaranteed high storage bandwidth for read and write operations by mission-critical real-time tasks. In this article, we depart from the traditional block device interface to guarantee the high throughput needed to process large volumes of data. Using data lifetime information from the application layer, our proposed real-time design, called Telomere, is able to intelligently lay out data in NAND flash memory and eliminate valid page copies during garbage collection. Telomere's real-time admission control is able to guarantee tasks their required read and write operations within their periods. Under randomly generated tasksets containing 500 tasks, Telomere achieves 30% higher throughput with a 5% storage cost compared to pre-existing techniques. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",flash translation layer; Real-time storage; SSD,Chromosomes; Flash memory; Interactive computer systems; Memory architecture; NAND circuits; Real time systems; Refuse collection; Flash translation layer; Garbage collection; High-throughput; Read operation; Real- time; Real-time storage; Solid state disks; SSD; Telomeres; Write operations; Data transfer
Accurate Estimation of Service Rates in Interleaved Scratchpad Memory Systems,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124801039&doi=10.1145%2f3457171&partnerID=40&md5=5938f1ed010d9bf8ae695644400a4b7f,"The prototyping of embedded platforms demands rapid exploration of multi-dimensional parameter sets. Especially the design of the memory system is essential to guarantee high utilization while reducing conflicts at the same time. To aid the design process, several probabilistic models to estimate the throughput of interleaved memory systems have been proposed. While accurately estimating the average throughput of the system, these models fail to determine the impact on individual processing elements. To mitigate this divergence, we extend three known models to include non-uniform access probabilities and priorities. © 2022 Association for Computing Machinery.",Scratchpad memory; throughput estimation,Computer aided design; Accurate estimation; Design-process; Embedded platforms; High utilizations; Memory systems; Multi-dimensional parameters; Parameter set; Scratch-pad memory; Service rates; Throughput estimation; Memory architecture
Holistic Resource Allocation under Federated Scheduling for Parallel Real-time Tasks,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124790636&doi=10.1145%2f3489467&partnerID=40&md5=8267840c65311cf87ed3c02394611b72,"With the technology trend of hardware and workload consolidation for embedded systems and the rapid development of edge computing, there has been increasing interest in supporting parallel real-time tasks to better utilize the multi-core platforms while meeting the stringent real-time constraints. For parallel real-time tasks, the federated scheduling paradigm, which assigns each parallel task a set of dedicated cores, achieves good theoretical bounds by ensuring exclusive use of processing resources to reduce interferences. However, because cores share the last-level cache and memory bandwidth resources, in practice tasks may still interfere with each other despite executing on dedicated cores. Such resource interferences due to concurrent accesses can be even more severe for embedded platforms or edge servers, where the computing power and cache/memory space are limited. To tackle this issue, in this work, we present a holistic resource allocation framework for parallel real-time tasks under federated scheduling. Under our proposed framework, in addition to dedicated cores, each parallel task is also assigned with dedicated cache and memory bandwidth resources. Further, we propose a holistic resource allocation algorithm that well balances the allocation between different resources to achieve good schedulability. Additionally, we provide a full implementation of our framework by extending the federated scheduling system with Intel's Cache Allocation Technology and MemGuard. Finally, we demonstrate the practicality of our proposed framework via extensive numerical evaluations and empirical experiments using real benchmark programs. © 2022 Association for Computing Machinery.",federated scheduling; Parallel real-time systems; resource partitioning,Bandwidth; Cache memory; Embedded systems; Interactive computer systems; Resource allocation; Scheduling; Space platforms; Cache bandwidth; Dedicated cores; Federated scheduling; Memory bandwidths; Parallel real-time system; Parallel task; Real - Time system; Real-time tasks; Resource partitioning; Resources allocation; Real time systems
Software-Managed Read and Write Wear-Leveling for Non-Volatile Main Memory,2022,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124799219&doi=10.1145%2f3483839&partnerID=40&md5=d274810938149befd315d520ec9983e6,"In-memory wear-leveling has become an important research field for emerging non-volatile main memories over the past years. Many approaches in the literature perform wear-leveling by making use of special hardware. Since most non-volatile memories only wear out from write accesses, the proposed approaches in the literature also usually try to spread write accesses widely over the entire memory space. Some non-volatile memories, however, also wear out from read accesses, because every read causes a consecutive write access. Software-based solutions only operate from the application or kernel level, where read and write accesses are realized with different instructions and semantics. Therefore different mechanisms are required to handle reads and writes on the software level. First, we design a method to approximate read and write accesses to the memory to allow aging aware coarse-grained wear-leveling in the absence of special hardware, providing the age information. Second, we provide specific solutions to resolve access hot-spots within the compiled program code (text segment) and on the application stack. In our evaluation, we estimate the cell age by counting the total amount of accesses per cell. The results show that employing all our methods improves the memory lifetime by up to a factor of 955×. © 2022 Copyright held by the owner/author(s).",age approximation; Non-volatile memory; read-destructive; wear-leveling,Application programs; Nonvolatile storage; Semantics; Age approximation; Coarse-grained; Different mechanisms; Memory space; Non-volatile main memory; Read-destructive; Research fields; Software-based solutions; Special hardware; Wear-Leveling; Wear of materials
Improving Variational Autoencoder based Out-of-Distribution Detection for Embedded Real-time Applications,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115845056&doi=10.1145%2f3477026&partnerID=40&md5=2b845cc78e708a76e8e9ea905e0419ae,"Uncertainties in machine learning are a significant roadblock for its application in safety-critical cyber-physical systems (CPS). One source of uncertainty arises from distribution shifts in the input data between training and test scenarios. Detecting such distribution shifts in real-time is an emerging approach to address the challenge. The high dimensional input space in CPS applications involving imaging adds extra difficulty to the task. Generative learning models are widely adopted for the task, namely out-of-distribution (OoD) detection. To improve the state-of-the-art, we studied existing proposals from both machine learning and CPS fields. In the latter, safety monitoring in real-time for autonomous driving agents has been a focus. Exploiting the spatiotemporal correlation of motion in videos, we can robustly detect hazardous motion around autonomous driving agents. Inspired by the latest advances in the Variational Autoencoder (VAE) theory and practice, we tapped into the prior knowledge in data to further boost OoD detection's robustness. Comparison studies over nuScenes and Synthia data sets show our methods significantly improve detection capabilities of OoD factors unique to driving scenarios, 42% better than state-of-the-art approaches. Our model also generalized near-perfectly, 97% better than the state-of-the-art across the real-world and simulation driving data sets experimented. Finally, we customized one proposed method into a twin-encoder model that can be deployed to resource limited embedded devices for real-time OoD detection. Its execution time was reduced over four times in low-precision 8-bit integer inference, while detection capability is comparable to its corresponding floating-point model. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",domain shift; Out-of-distribution detection,Digital arithmetic; Machine learning; Safety engineering; Auto encoders; Autonomous driving agents; Data set; Detection capability; Domain shift; Embedded real-time applications; Out-of-distribution detection; Real- time; State of the art; Uncertainty; Embedded systems
Guaranteeing Timely Response to Changes of Monitored Objects by Assigning Deadlines and Periods to Tasks,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115856508&doi=10.1145%2f3477027&partnerID=40&md5=3da4687dbcb85f581999247550a39ea3,"Timely response to changes of monitored objects is the key to ensuring the safety and reliability of cyber-physical systems (CPSs). There are two kinds of tasks in CPSs: update tasks and control tasks. Update tasks are responsible for updating the data in the system based on the state of the objects they monitor. Control tasks are responsible for making decisions based on the data in the system. The response time of the system to the change of a monitored object consists of two parts: the time taken by update tasks to reflect the change to the system, and the time taken by control tasks to make decisions according to the data in the system. Deadlines and periods of update tasks and control tasks directly affect the response time. Reasonable deadline and period assignment is the key to ensuring timely response to the changes of monitored objects. In this paper, we study the deadline and period assignment in CPSs. To the best of our knowledge, all existing work only focuses on the deadline and period assignment for update tasks with the goal of ensuring the freshness of the data in CPSs, and this is the first study focusing on the deadline and period assignment for both update tasks and control tasks with the goal of ensuring timely response to the changes of monitored objects. A new problem about response time control and system workload control is defined in this paper. Two deadline and period assignment methods are proposed to solve the defined problem. All the proposed methods can be used in the CPSs adopting the earliest deadline first (EDF) scheduling method. Experiments with randomly generated tasks are conducted to evaluate the performance of the proposed methods in terms of acceptance ratio and execution efficiency. © 2021 Association for Computing Machinery.",CPSs; Deadline and period assignment; EDF scheduling; hybrid tasks; timely response,Response time (computer systems); Scheduling; Control task; Deadline and period assignment; Decision-based; Earliest deadline first scheduling; Hybrid tasks; Making decision; System workloads; Time control; Time systems; Timely response; Embedded systems
Algorithm-hardware Co-design of Attention Mechanism on FPGA Devices,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115831568&doi=10.1145%2f347702&partnerID=40&md5=fec8b43d8e06e6db59d2f7fc7d45fc1d,"Multi-head self-attention (attention mechanism) has been employed in a variety of fields such as machine translation, language modeling, and image processing due to its superiority in feature extraction and sequential data analysis. This is benefited from a large number of parameters and sophisticated model architecture behind the attention mechanism. To efficiently deploy attention mechanism on resource-constrained devices, existing works propose to reduce the model size by building a customized smaller model or compressing a big standard model. A customized smaller model is usually optimized for the specific task and needs effort in model parameters exploration. Model compression reduces model size without hurting the model architecture robustness, which can be efficiently applied to different tasks. The compressed weights in the model are usually regularly shaped (e.g. rectangle) but the dimension sizes vary (e.g. differs in rectangle height and width). Such compressed attention mechanism can be efficiently deployed on CPU/GPU platforms as their memory and computing resources can be flexibly assigned with demand. However, for Field Programmable Gate Arrays (FPGAs), the data buffer allocation and computing kernel are fixed at run time to achieve maximum energy efficiency. After compression, weights are much smaller and different in size, which leads to inefficient utilization of FPGA on-chip buffer. Moreover, the different weight heights and widths may lead to inefficient FPGA computing kernel execution. Due to the large number of weights in the attention mechanism, building a unique buffer and computing kernel for each compressed weight on FPGA is not feasible. In this work, we jointly consider the compression impact on buffer allocation and the required computing kernel during the attention mechanism compressing. A novel structural pruning method with memory footprint awareness is proposed and the associated accelerator on FPGA is designed. The experimental results show that our work can compress Transformer (an attention mechanism based model) by 95x. The developed accelerator can fully utilize the FPGA resource, processing the sparse attention mechanism with the run-time throughput performance of 1.87 Tops in ZCU102 FPGA. © 2021 Association for Computing Machinery.",algorithm; attention; Co-design; FPGA; hardware; tops; transformer,Computer aided design; Computer hardware description languages; Energy efficiency; Image processing; Integrated circuit design; Memory architecture; Modeling languages; Attention; Attention mechanisms; Buffer allocation; Co-designs; Hardware; Model size; Modeling architecture; Runtimes; Top; Transformer; Field programmable gate arrays (FPGA)
HW-FlowQ: A Multi-Abstraction Level HW-CNN Co-design Quantization Methodology,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115837277&doi=10.1145%2f3476997&partnerID=40&md5=01eb4f06f24c4403cf44f016b880c130,"Model compression through quantization is commonly applied to convolutional neural networks (CNNs) deployed on compute and memory-constrained embedded platforms. Different layers of the CNN can have varying degrees of numerical precision for both weights and activations, resulting in a large search space. Together with the hardware (HW) design space, the challenge of finding the globally optimal HW-CNN combination for a given application becomes daunting. To this end, we propose HW-FlowQ, a systematic approach that enables the co-design of the target hardware platform and the compressed CNN model through quantization. The search space is viewed at three levels of abstraction, allowing for an iterative approach for narrowing down the solution space before reaching a high-fidelity CNN hardware modeling tool, capable of capturing the effects of mixed-precision quantization strategies on different hardware architectures (processing unit counts, memory levels, cost models, dataflows) and two types of computation engines (bit-parallel vectorized, bit-serial). To combine both worlds, a multi-objective non-dominated sorting genetic algorithm (NSGA-II) is leveraged to establish a Pareto-optimal set of quantization strategies for the target HW-metrics at each abstraction level. HW-FlowQ detects optima in a discrete search space and maximizes the task-related accuracy of the underlying CNN while minimizing hardware-related costs. The Pareto-front approach keeps the design space open to a range of non-dominated solutions before refining the design to a more detailed level of abstraction. With equivalent prediction accuracy, we improve the energy and latency by 20% and 45% respectively for ResNet56 compared to existing mixed-precision search methods. © 2021 Association for Computing Machinery.",Convolutional neural networks; genetic algorithms; hardware modeling; multi-objective optimization; quantization,Abstracting; Convolution; Data flow analysis; Genetic algorithms; Iterative methods; Memory architecture; Neural networks; Pareto principle; Abstraction level; Co-designs; Convolutional neural network; Design spaces; Hardware models; Mixed precision; Multi-objectives optimization; Quantisation; Search spaces; Target hardware; Multiobjective optimization
Real-time Attack-recovery for Cyber-physical Systems Using Linear-quadratic Regulator,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115831321&doi=10.1145%2f3477010&partnerID=40&md5=0ced7ac1020654ce5f3bfc7efe3bf972,"The increasing autonomy and connectivity in cyber-physical systems (CPS) come with new security vulnerabilities that are easily exploitable by malicious attackers to spoof a system to perform dangerous actions. While the vast majority of existing works focus on attack prevention and detection, the key question is ""what to do after detecting an attack?"". This problem attracts fairly rare attention though its significance is emphasized by the need to mitigate or even eliminate attack impacts on a system. In this article, we study this attack response problem and propose novel real-time recovery for securing CPS. First, this work's core component is a recovery control calculator using a Linear-Quadratic Regulator (LQR) with timing and safety constraints. This component can smoothly steer back a physical system under control to a target state set before a safe deadline and maintain the system state in the set once it is driven to it. We further propose an Alternating Direction Method of Multipliers (ADMM) based algorithm that can fast solve the LQR-based recovery problem. Second, supporting components for the attack recovery computation include a checkpointer, a state reconstructor, and a deadline estimator. To realize these components respectively, we propose (i) a sliding-window-based checkpointing protocol that governs sufficient trustworthy data, (ii) a state reconstruction approach that uses the checkpointed data to estimate the current system state, and (iii) a reachability-based approach to conservatively estimate a safe deadline. Finally, we implement our approach and demonstrate its effectiveness in dealing with totally 15 experimental scenarios which are designed based on 5 CPS simulators and 3 types of sensor attacks. © 2021 Association for Computing Machinery.",Cyber-physical system; linear-quadratic regulator; real-time; recovery; security; sensor attack,Cyber Physical System; Real time systems; Recovery; Attack detection; Attack prevention; Linear quadratic; Linear-quadratic regulator; Quadratic regulators; Real- time; Security; Security vulnerabilities; Sensor attack; System state; Embedded systems
Prepare: Power-Aware Approximate Real-time Task Scheduling for Energy-Adaptive QoS Maximization,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115833110&doi=10.1145%2f3476993&partnerID=40&md5=fb3db0ab93e409e30f2b4f8470ee105e,"Achieving high result-accuracy in approximate computing (AC) based real-time applications without violating power constraints of the underlying hardware is a challenging problem. Execution of such AC real-time tasks can be divided into the execution of the mandatory part to obtain a result of acceptable quality, followed by a partial/complete execution of the optional part to improve accuracy of the initially obtained result within the given time-limit. However, enhancing result-accuracy at the cost of increased execution length might lead to deadline violations with higher energy usage. We propose Prepare, a novel hybrid offline-online approximate real-time task-scheduling approach, that first schedules AC-based tasks and determines operational processing speeds for each individual task constrained by system-wide power limit, deadline, and task-dependency. At runtime, by employing fine-grained DVFS, the energy-adaptive processing speed governing mechanism of Prepare reduces processing speed during each last level cache miss induced stall and scales up the processing speed once the stall finishes to a higher value than the predetermined one. To ensure on-chip thermal safety, this higher processing speed is maintained only for a short time-span after each stall, however, this reduces execution times of the individual task and generates slacks. Prepare exploits the slacks either to enhance result-accuracy of the tasks, or to improve thermal and energy efficiency of the underlying hardware, or both. With a 70-80% workload, Prepare offers 75% result-accuracy with its constrained scheduling, which is enhanced by 5.3% for our benchmark based evaluation of the online energy-adaptive mechanism on a 4-core based homogeneous chip multi-processor, while meeting the deadline constraint. Overall, while maintaining runtime thermal safety, Prepare reduces peak temperature by up to 8.6 °C for our baseline system. Our empirical evaluation shows that constrained scheduling of Prepare outperforms a state-of-the-art scheduling policy, whereas our runtime energy-adaptive mechanism surpasses two current DVFS based thermal management techniques. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",approximate computing; cache Miss; DVFS; energy and thermal efficiency; multi-core systems; Real-time scheduling,Energy efficiency; Multitasking; Power management; Scheduling; Scheduling algorithms; Speed; Approximate computing; Cache Miss; DVFS; Energy; Energy adaptive; Energy and thermal efficiency; Multi-core systems; Processing speed; Real time scheduling; Thermal-efficiency; Real time systems
Structured Proofs for Adversarial Cyber-Physical Systems,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115860323&doi=10.1145%2f3477024&partnerID=40&md5=81d7640665d8449a406efa27f4337c70,"Many cyber-physical systems (CPS) are safety-critical, so it is important to formally verify them, e.g. in formal logics that show a model's correctness specification always holds. Constructive Differential Game Logic (CdGL) is such a logic for (constructive) hybrid games, including hybrid systems. To overcome undecidability, the user first writes a proof, for which we present a proof-checking tool.We introduce Kaisar, the first language and tool for CdGL proofs, which until now could only be written by hand with a low-level proof calculus. Kaisar's structured proofs simplify challenging CPS proof tasks, especially by using programming language principles and high-level stateful reasoning. Kaisar exploits CdGL's constructivity and refinement relations to build proofs around models of game strategies. The evaluation reproduces and extends existing case studies on 1D and 2D driving. Proof metrics are compared and reported experiences are discussed for the original studies and their reproductions. © 2021 Association for Computing Machinery.",Cyber-physical systems; formal proof; hybrid games; structured proofs,Calculations; Cyber Physical System; Formal methods; High level languages; Hybrid systems; Safety engineering; Case-studies; Constructivity; Differential games; Formal proofs; Hybrid game; Logic proofs; Proof calculus; Proof checking; Structured proof; Undecidability; Embedded systems
Exploiting Activation Sparsity for Fast CNN Inference on Mobile GPUs,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115836730&doi=10.1145%2f347708&partnerID=40&md5=9d30480b5b70099ef769ce8e7df2acf2,"Over the past several years, the need for on-device deep learning has been rapidly increasing, and efficient CNN inference on mobile platforms has been actively researched. Sparsity exploitation has been one of the most active research themes, but the studies mostly focus on weight sparsity by weight pruning. Activation sparsity, on the contrary, requires compression at runtime for every input tensor. Hence, the research on activation sparsity mainly targets NPUs that can efficiently process this with their own hardware logic. In this paper, we observe that it is difficult to accelerate CNN inference on mobile GPUs with natural activation sparsity and that the widely used CSR-based sparse convolution is not sufficiently effective due to the compression overhead. We propose several novel sparsification methods that can boost activation sparsity without harming accuracy. In particular, we selectively sparsify some layers with an extremely high sparsity and adopt sparse convolution or dense convolution depending on the layers. Further, we present an efficient sparse convolution method without compression and demonstrate that it can be faster than the CSR implementation. With ResNet-50, we achieved 1.88 speedup compared to TFLite on a Mali-G76 GPU. © 2021 Association for Computing Machinery.",convolutional neural network; On-device deep learning; sparsity,Chemical activation; Computer hardware; Convolutional neural networks; Deep neural networks; Program processors; Convolution methods; Convolutional neural network; Mobile platform; Natural activation; On-device deep learning; Runtimes; Sparsification; Sparsity; Sparsity exploitations; Convolution
Learning Nondeterministic Real-Time Automata,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115886034&doi=10.1145%2f3477030&partnerID=40&md5=ea7f80311bc81227c534b5dd83f5628a,"We present an active learning algorithm named NRTALearning for nondeterministic real-time automata (NRTAs). Real-time automata (RTAs) are a subclass of timed automata with only one clock which resets at each transition. First, we prove the corresponding Myhill-Nerode theorem for real-time languages. Then we show that there exists a unique minimal deterministic real-time automaton (DRTA) recognizing a given real-time language, but the same does not hold for NRTAs. We thus define a special kind of NRTAs, named residual real-time automata (RRTAs), and prove that there exists a minimal RRTA to recognize any given real-time language. This transforms the learning problem of NRTAs to the learning problem of RRTAs. After describing the learning algorithm in detail, we prove its correctness and polynomial complexity. In addition, based on the corresponding Myhill-Nerode theorem, we extend the existing active learning algorithm NL∗for nondeterministic finite automata to learn RRTAs. We evaluate and compare the two algorithms on two benchmarks consisting of randomly generated NRTAs and rational regular expressions. The results show that NRTALearning generally performs fewer membership queries and more equivalence queries than the extended NL∗algorithm, and the learnt NRTAs have much fewer locations than the corresponding minimal DRTAs. We also conduct a case study using a model of scheduling of final testing of integrated circuits. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Active learning; model learning; nondeterministic real-time automata; real-time languages,Artificial intelligence; Learning algorithms; Real time systems; Active Learning; Active-learning algorithm; Learn+; Learning problem; Model learning; Myhill-nerode theorem; Nondeterministic real-time automaton; Real-time automaton; Real-time languages; Timed Automata; Learning systems
Data Pattern Aware Reliability Enhancement Scheme for 3D Solid-State Drives,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115867993&doi=10.1145%2f347700&partnerID=40&md5=b642a4e7045cae28a566431d404bdc7c,"3D charge-trap (CT) NAND flash-based SSD has been used widely for its large capacity, low cost per bit, and high endurance. One-shot program (OSP) scheme, as a variation of incremental step pulse programming (ISPP) scheme, has been employed to program data for CT flash, whose program unit is the Word-Line (WL) instead of the page. The existing program optimization schemes either make trade-offs among program latency and reliability by adjusting the program step voltage on demand; or remap the most error-prone cell states to others by re-encoding programmed data. However, the data pattern, which represents the ratio of 1s in data values, has not been thoroughly studied. In this paper, we observe that most small files do not contain uniform 1s and 0s among these common file types (i.e., image, audio, text, executable file), leading to programming WL cells in different states unevenly. Some cell states dominate over the WL, while others are not. Based on this observation, we propose a flexible reliability enhancement scheme based on the OSP scheme. This scheme programs the cells into different states with varied, i.e., these cells in one state, whose number is the largest in one WL, are programmed with a fine-grained (namely slow write). In contrast, the minority are programmed with a coarse-grained (namely fast write). So the reliability is improved due to averaging the major enhanced cells with the minor degraded cells without program latency overhead. A series of experiments have been conducted, and the results indicate that the proposed scheme achieves 34% read performance improvement and 16% lifetime elongation on average. © 2021 Association for Computing Machinery.",3D NAND flash; one-shot program; program step voltage; reliability,Cells; Computerized tomography; Cytology; Memory architecture; NAND circuits; Reliability; 3d NAND flash; Cell state; Charge trap; Data patterns; NAND Flash; One-shot program; Program schemes; Program step voltage; Reliability enhancement; Step voltages; Economic and social effects
SAGE: A Split-Architecture Methodology for Efficient End-to-End Autonomous Vehicle Control,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115866024&doi=10.1145%2f347706&partnerID=40&md5=ce22257a02351423732043d118deb92e,"Autonomous vehicles (AV) are expected to revolutionize transportation and improve road safety significantly. However, these benefits do not come without cost; AVs require large Deep-Learning (DL) models and powerful hardware platforms to operate reliably in real-time, requiring between several hundred watts to one kilowatt of power. This power consumption can dramatically reduce vehicles' driving range and affect emissions. To address this problem, we propose SAGE: a methodology for selectively offloading the key energy-consuming modules of DL architectures to the cloud to optimize edge, energy usage while meeting real-time latency constraints. Furthermore, we leverage Head Network Distillation (HND) to introduce efficient bottlenecks within the DL architecture in order to minimize the network overhead costs of offloading with almost no degradation in the model's performance. We evaluate SAGE using an Nvidia Jetson TX2 and an industry-standard Nvidia Drive PX2 as the AV edge, devices and demonstrate that our offloading strategy is practical for a wide range of DL models and internet connection bandwidths on 3G, 4G LTE, and WiFi technologies. Compared to edge-only computation, SAGE reduces energy consumption by an average of 36.13%, 47.07%, and 55.66% for an AV with one low-resolution camera, one high-resolution camera, and three high-resolution cameras, respectively. SAGE also reduces upload data size by up to 98.40% compared to direct camera offloading. © 2021 Association for Computing Machinery.",autonomous vehicles; computation offloading; deep learning; edge computing; Energy optimization,Cameras; Computer architecture; Control system synthesis; Deep learning; Digital storage; Distillation; Edge computing; Energy utilization; Inverse problems; Network architecture; Autonomous Vehicles; Computation offloading; Deep learning; Edge computing; Energy optimization; High resolution camera; Learning architectures; Learning models; Real- time; Split architectures; Autonomous vehicles
Hardware Performance Counters: Ready-Made vs Tailor-Made,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115821491&doi=10.1145%2f3476996&partnerID=40&md5=4b639f4f2253f250439b6011de784234,"Micro-architectural footprints can be used to distinguish one application from another. Most modern processors feature hardware performance counters to monitor the various micro-architectural events when an application is executing. These ready-made hardware performance counters can be used to create program fingerprints and have been shown to successfully differentiate between individual applications. In this paper, we demonstrate how ready-made hardware performance counters, due to their coarse-grain nature (low sampling rate and bundling of similar events, e.g., number of instructions instead of number of add instructions), are insufficient to this end. This observation motivates exploration of tailor-made hardware performance counters to capture fine-grain characteristics of the programs. As a case study, we evaluate both ready-made and tailor-made hardware performance counters using post-quantum cryptographic key encapsulation mechanism implementations. Machine learning models trained on tailor-made hardwareperformance counter streams demonstrate that they can uniquely identify the behavior of every post-quantum cryptographic key encapsulation mechanism algorithm with at least 98.99% accuracy. © 2021 Association for Computing Machinery.",Hardware performance counters; machine learning; post quantum cryptographic algorithms,Computer hardware description languages; Machine learning; Public key cryptography; Coarse grains; Cryptographic algorithms; Cryptographic key; Finer grains; Hardware performance counters; Key encapsulation mechanisms; Modern processors; Post quantum; Post quantum cryptographic algorithm; Sampling rates; Application programs
Specification Guided Automated Synthesis of Feedback Controllers,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115862740&doi=10.1145%2f3477011&partnerID=40&md5=37863c66d0a0868a7eabf5447402a860,"The growing use of complex Cyber-Physical Systems (CPSs) in safety-critical applications has led to the demand for the automatic synthesis of robust feedback controllers that satisfy a given set of formal specifications. Controller synthesis from the high-level specification is an NP-Hard problem. We propose a heuristic-based automated technique that synthesizes feedback controllers guided by Signal Temporal Logic (STL) specifications. Our technique involves rigorous analysis of the traces generated by the closed-loop system, matrix decomposition, and an incremental multi-parameter tuning procedure. In case a controller cannot be found to satisfy all the specifications, we propose a technique for modifying the unsatisfiable specifications so that the controller synthesized for the satisfiable subset of specifications now also satisfies the modified specifications. We demonstrate our technique on eleven controllers used as standard closed-loop control system benchmarks, including complex controllers having multiple independent or nested control loops. Our experimental results establish that the proposed algorithm can automatically solve complex feedback controller synthesis problems within a few minutes. © 2021 Association for Computing Machinery.",controller synthesis; falsification; Feedback control; multi-parameter tuning; parametric signal temporal logic,Closed loop systems; Computer circuits; Embedded systems; Feedback control; Formal specification; Robust control; Safety engineering; Temporal logic; Automated synthesis; Automatic synthesis; Controller synthesis; Feedback controller; Multi-parameter tuning; Multiparameters; Parameters tuning; Parametric signal temporal logic; Robust feedback controllers; Safety critical applications; Controllers
Verified Lustre Normalization with Node Subsampling,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115873755&doi=10.1145%2f3477041&partnerID=40&md5=dcf0d7b80679e1d184ca6e957dd5fe04,"Dataflow languages allow the specification of reactive systems by mutually recursive stream equations, functions, and boolean activation conditions called clocks. Lustre and Scade are dataflow languages for programming embedded systems. Dataflow programs are compiled by a succession of passes. This article focuses on the normalization pass which rewrites programs into the simpler form required for code generation.Vélus is a compiler from a normalized form of Lustre to CompCert's Clight language. Its specification in the Coq interactive theorem prover includes an end-to-end correctness proof that the values prescribed by the dataflow semantics of source programs are produced by executions of generated assembly code. We describe how to extend Vélus with a normalization pass and to allow subsampled node inputs and outputs. We propose semantic definitions for the unrestricted language, divide normalization into three steps to facilitate proofs, adapt the clock type system to handle richer node definitions, and extend the end-to-end correctness theorem to incorporate the new features. The proofs require reasoning about the relation between static clock annotations and the presence and absence of values in the dynamic semantics. The generalization of node inputs requires adding a compiler pass to ensure the initialization of variables passed in function calls. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",interactive theorem proving; Stream languages; verified compilation,Boolean functions; Embedded systems; Program compilers; Semantics; Specifications; Theorem proving; Activation conditions; Data flow language; Dataflow; Embedded-system; End-to-end correctness; Interactive theorem proving; Normalisation; Reactive system; Stream languages; Verified compilation; Clocks
Two Birds with One Stone: Boosting Both Search and Write Performance for Tree Indices on Persistent Memory,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115849823&doi=10.1145%2f3476981&partnerID=40&md5=06ba6f169de0a13469556402e003c0a3,"The advance of byte-addressable persistent memory (PM) makes it a hot topic to revisit traditional tree indices such as B+-tree and radix tree, and a few new persistent memory-friendly tree indices have been proposed. However, due to the special features of persistent memory compared to DRAM and the limitations of B+-tree-like indices, it is much harder to optimize both search and write performance for tree indices on persistent memory. As a result, most existing indices for persistent memory, e.g., WB-tree, proposed to improve write performance while sacrificing search performance. Aiming to optimize both write and search performance for tree indices on persistent memory, in this paper, we first propose a novel Two-Layer Architecture (TLA) for constructing tree indices on persistent memory. The key idea, of TLA is to organize the index with a search-optimized top layer and a write-optimized bottom layer, letting the top layer optimize search performance and the bottom layer improve write performance. By adopting efficient structures for the two layers, TLA can boost both write and search performance for tree indices on persistent memory. Following the TLA architecture, we present a new index called TLBtree (Two-Layer B+-tree) offering high search and write performance for persistent memory. Moreover, we develop a concurrent TLBtree to support non-blocking read operations in multi-core environment. We evaluate our proposals under a server equipped with real Intel Optane persistent memory. The results show that TLBtree outperforms the state-of-the-art tree indices, including WB-tree, Fast&Fair, and FPTree, in both search and write performance. Also, the concurrent TLBtree can achieve up to 3.7x speedup than its competitors under the multi-core environment. © 2021 Association for Computing Machinery.",B+-tree; hybrid index; persistent memory; read/write optimization,Forestry; Memory architecture; B trees; Hybrid index; Layer architectures; Optimisations; Performance; Persistent memory; Read/write optimization; Search performance; Tree indices; Two-layer; Dynamic random access storage
Predictive Monitoring with Logic-Calibrated Uncertainty for Cyber-Physical Systems,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115854610&doi=10.1145%2f3477032&partnerID=40&md5=6b03c59ad31aa0405cfbdf010b360667,"Predictive monitoring-making predictions about future states and monitoring if the predicted states satisfy requirements-offers a promising paradigm in supporting the decision making of Cyber-Physical Systems (CPS). Existing works of predictive monitoring mostly focus on monitoring individual predictions rather than sequential predictions. We develop a novel approach for monitoring sequential predictions generated from Bayesian Recurrent Neural Networks (RNNs) that can capture the inherent uncertainty in CPS, drawing on insights from our study of real-world CPS datasets. We propose a new logic named Signal Temporal Logic with Uncertainty (STL-U) to monitor a flowpipe containing an infinite set of uncertain sequences predicted by Bayesian RNNs. We define STL-U strong and weak satisfaction semantics based on whether all or some sequences contained in a flowpipe satisfy the requirement. We also develop methods to compute the range of confidence levels under which a flowpipe is guaranteed to strongly (weakly) satisfy an STL-U formula. Furthermore, we develop novel criteria that leverage STL-U monitoring results to calibrate the uncertainty estimation in Bayesian RNNs. Finally, we evaluate the proposed approach via experiments with real-world CPS datasets and a simulated smart city case study, which show very encouraging results of STL-U based predictive monitoring approach outperforming baselines. © 2021 Association for Computing Machinery.",Predictive monitoring; uncertainty,Bayesian networks; Computer circuits; Cyber Physical System; Decision making; Forecasting; Monitoring; Semantics; Bayesian; Confidence levels; Decisions makings; Individual prediction; Monitoring results; Novel criterion; Predictive monitoring; Real-world; Sequential prediction; Uncertainty; Embedded systems
Regime Inference for Sound Floating-Point Optimizations,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115863608&doi=10.1145%2f3477012&partnerID=40&md5=f975c5c8efccd21b1163f36f888051f3,"Efficient numerical programs are required for proper functioning of many systems. Today's tools offer a variety of optimizations to generate efficient floating-point implementations that are specific to a program's input domain. However, sound optimizations are of an ""all or nothing""fashion with respect to this input domain-if an optimizer cannot improve a program on the specified input domain, it will conclude that no optimization is possible. In general, though, different parts of the input domain exhibit different rounding errors and thus have different optimization potential. We present the first regime inference technique for sound optimizations that automatically infers an effective subdivision of a program's input domain such that individual sub-domains can be optimized more aggressively. Our algorithm is general; we have instantiated it with mixed-precision tuning and rewriting optimizations to improve performance and accuracy, respectively. Our evaluation on a standard benchmark set shows that with our inferred regimes, we can, on average, improve performance by 65% and accuracy by 54% with respect to whole-domain optimizations. © 2021 Association for Computing Machinery.",Floating-point arithmetic; regime inference,Benchmarking; All or nothings; Floating point implementation; Floating points; Floating-point arithmetic; Improve performance; IMPROVE-A; Numerical programs; Optimisations; Optimizers; Regime inference; Digital arithmetic
Exploring Efficient Architectures on Remote In-Memory NVM over RDMA,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115857332&doi=10.1145%2f347704&partnerID=40&md5=c4c64251a1c2a3b46e6012582a131ffc,"Efficiently accessing remote file data remains a challenging problem for data processing systems. Development of technologies in non-volatile dual in-line memory modules (NVDIMMs), in-memory file systems, and RDMA networks provide new opportunities towards solving the problem of remote data access. A general understanding about NVDIMMs, such as Intel Optane DC Persistent Memory (DCPM), is that they expand main memory capacity with a cost of multiple times lower performance than DRAM. With an in-depth exploration presented in this paper, however, we show an interesting finding that the potential of NVDIMMs for high-performance, remote in-memory accesses can be revealed through careful design. We explore multiple architectural structures for accessing remote NVDIMMs in a real system using Optane DCPM, and compare the performance of various structures. Experiments are conducted to show significant performance gaps among different ways of using NVDIMMs as memory address space accessible through RDMA interface. Furthermore, we design and implement a prototype of user-level, in-memory file system, RIMFS, in the device DAX mode on Optane DCPM. By comparing against the DAX-supported Linux file system, Ext4-DAX, we show that the performance of remote reads on RIMFS over RDMA is 11.44 higher than that on a remote Ext4-DAX on average. The experimental results also show that the performance of remote accesses on RIMFS is maintained on a heavily loaded data server with CPU utilization as high as 90%, while the performance of remote reads on Ext4-DAX is significantly reduced by 49.3%, and the performance of local reads on Ext4-DAX is even more significantly reduced by 90.1%. The performance comparisons of writes exhibit the same trends. © 2021 Association for Computing Machinery.",Non-volatile memory; RDMA; remote access,Computer operating systems; Data handling; File organization; Integrated circuit design; Memory architecture; Data-processing system; Dual-in line memory modules; Efficient architecture; Filesystem; Nonvolatile; Performance; Persistent memory; RDMA; Remote access; Remote files; Dynamic random access storage
MARS: MmWave-based Assistive Rehabilitation System for Smart Healthcare,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115885492&doi=10.1145%2f347703&partnerID=40&md5=079390b00089ed5a876048c189ffbcc0,"Rehabilitation is a crucial process for patients suffering from motor disorders. The current practice is performing rehabilitation exercises under clinical expert supervision. New approaches are needed to allow patients to perform prescribed exercises at their homes and alleviate commuting requirements, expert shortages, and healthcare costs. Human joint estimation is a substantial component of these programs since it offers valuable visualization and feedback based on body movements. Camera-based systems have been popular for capturing joint motion. However, they have high-cost, raise serious privacy concerns, and require strict lighting and placement settings. We propose a millimeter-wave (mmWave)-based assistive rehabilitation system (MARS) for motor disorders to address these challenges. MARS provides a low-cost solution with a competitive object localization and detection accuracy. It first maps the 5D time-series point cloud from mmWave to a lower dimension. Then, it uses a convolution neural network (CNN) to estimate the accurate location of human joints. MARS can reconstruct 19 human joints and their skeleton from the point cloud generated by mmWave radar. We evaluate MARS using ten specific rehabilitation movements performed by four human subjects involving all body parts and obtain an average mean absolute error of 5.87 cm for all joint positions. To the best of our knowledge, this is the first rehabilitation movements dataset using mmWave point cloud. MARS is evaluated on the Nvidia Jetson Xavier-NX board. Model inference takes only 64 s and consumes 442 J energy. These results demonstrate the practicality of MARS on low-power edge devices. © 2021 Association for Computing Machinery.",Human pose estimation; millimeter wave; point cloud; smart healthcare,Millimeter waves; Object detection; Patient rehabilitation; Assistive rehabilitations; Current practices; Human joints; Human pose estimations; Mm waves; Patient's suffering; Point-clouds; Rehabilitation exercise; Rehabilitation System; Smart healthcare; Health care
Domain-specific Hybrid Mapping for Energy-efficient Baseband Processing in Wireless Networks,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115823528&doi=10.1145%2f3476991&partnerID=40&md5=c72eb90c4928c0a2fbf55295c49e6687,"Advancing telecommunication standards continuously push for larger bandwidths, lower latencies, and faster data rates. The receiver baseband unit not only has to deal with a huge number of users expecting connectivity but also with a high workload heterogeneity. As a consequence of the required flexibility, baseband processing has seen a trend towards software implementations in cloud Radio Access Networks (cRANs). The flexibility gained from software implementation comes at the price of impoverished energy efficiency. This paper addresses the trade-off between flexibility and efficiency by proposing a domain-specific hybrid mapping algorithm. Hybrid mapping is an established approach from the model-based design of embedded systems that allows us to retain flexibility while targeting heterogeneous hardware. Depending on the current workload, the runtime system selects the most energy-efficient mapping configuration without violating timing constraints. We leverage the structure of baseband processing, and refine the scheduling methodology, to enable efficient mapping of 100s of tasks at the millisecond granularity, improving upon state-of-the-art hybrid approaches. We validate our approach on an Odroid XU4 and virtual platforms with application-specific accelerators on an open-source prototype. On different LTE workloads, our hybrid approach shows significant improvements both at design time and at runtime. At design-time, mappings of similar quality to those obtained by state-of-the-art methods are generated around four orders of magnitude faster. At runtime, multi-application schedules are computed 37.7% faster than the state-of-the-art without compromising on the quality. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",5g; baseband processing; energy-efficiency; hybrid mapping,5G mobile communication systems; Conformal mapping; Economic and social effects; Embedded systems; Open source software; Wireless networks; 5g; Base-band processing; Design time; Domain specific; Energy efficient; Hybrid approach; Hybrid mapping; Runtimes; Software implementation; State of the art; Energy efficiency
MSYNC: A Generalized Formal Design Pattern for Virtually Synchronous Multirate Cyber-physical Systems,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115858389&doi=10.1145%2f3477036&partnerID=40&md5=bc9d588c22feaccab4b3410b343e6d0d,"TTA and PALS are two prominent formal design patterns-with different strengths and weaknesses-for virtually synchronous distributed cyber-physical systems (CPSs). They greatly simplify the design and verification of such systems by allowing us to design and verify their underlying synchronous designs. In this paper we introduce and verify MSYNC as a formal design (and verification) pattern/synchronizer for hierarchical multirate CPSs that generalizes, and combines the advantages of, both TTA and (single-rate and multirate) PALS. We also define an extension of TTA to multirate CPSs as a special case. We show that MSYNC outperforms both TTA and PALS in terms of allowing shorter periods, and illustrate the MSYNC design and verification approach with a case study on a fault-tolerant distributed control system for turning an airplane. © 2021 Association for Computing Machinery.",PALS; synchronizers; time-triggered architecture; Virtual synchrony,Closed loop control systems; Cyber Physical System; Distributed parameter control systems; Design Patterns; Formal design; Multi rate; PALS; Short periods; Single rate; Synchronous designs; Time-triggered architectures; Verification patterns; Virtual synchrony; Embedded systems
ROBIN: A Robust Optical Binary Neural Network Accelerator,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115853257&doi=10.1145%2f3476988&partnerID=40&md5=ab1858b4959ffbf5b73a65e010e47ec0,"Domain specific neural network accelerators have garnered attention because of their improved energy efficiency and inference performance compared to CPUs and GPUs. Such accelerators are thus well suited for resource-constrained embedded systems. However, mapping sophisticated neural network models on these accelerators still entails significant energy and memory consumption, along with high inference time overhead. Binarized neural networks (BNNs), which utilize single-bit weights, represent an efficient way to implement and deploy neural network models on accelerators. In this paper, we present a novel optical-domain BNN accelerator, named ROBIN, which intelligently integrates heterogeneous microring resonator optical devices with complementary capabilities to efficiently implement the key functionalities in BNNs. We perform detailed fabrication-process variation analyses at the optical device level, explore efficient corrective tuning for these devices, and integrate circuit-level optimization to counter thermal variations. As a result, our proposed ROBIN architecture possesses the desirable traits of being robust, energy-efficient, low latency, and high throughput, when executing BNN models. Our analysis shows that ROBIN can outperform the best-known optical BNN accelerators and many electronic accelerators. Specifically, our energy-efficient ROBIN design exhibits energy-per-bit values that are ∼4 × lower than electronic BNN accelerators and ∼933 × lower than a recently proposed photonic BNN accelerator, while a performance-efficient ROBIN design shows ∼3 × and ∼25 × better performance than electronic and photonic BNN accelerators, respectively. © 2021 Association for Computing Machinery.",binarized neural networks; design optimization; inference acceleration; Silicon photonics,Acceleration; Embedded systems; Program processors; Binarized neural network; Binary neural networks; Design optimization; Domain specific; Energy efficient; Inference acceleration; Neural network model; Neural-networks; Optical-; Performance; Energy efficiency
Federated Scheduling of Sporadic DAGs on Unrelated Multiprocessors,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115859400&doi=10.1145%2f3477018&partnerID=40&md5=73a99c2bcd60c5d108bb421fa805904a,"This paper presents a federated scheduling algorithm for implicit-deadline sporadic DAGs that execute on an unrelated heterogeneous multiprocessor platform. We consider a global work-conserving scheduler to execute a single DAG exclusively on a subset of the unrelated processors. Formal schedulability analysis to find the makespan of a DAG on its dedicated subset of the processors is proposed. The problem of determining each subset of dedicated unrelated processors for each DAG such that the DAG meets its deadline (i.e., designing the federated scheduling algorithm) is tackled by proposing a novel processors-to-task assignment heuristic using a new concept called processor value. Empirical evaluation is presented to show the effectiveness of our approach. © 2021 Association for Computing Machinery.",DAGs; Federated; heterogeneous; multiprocessor; scheduling; sporadic; unrelated; work-conserving,Multiprocessing systems; Scheduling algorithms; DAG; Federated; Heterogeneous; Heterogeneous multiprocessors; Makespan; Multi-processor platforms; Schedulability analysis; Sporadics; Unrelated; Work-conserving; Scheduling
Schedulability Analysis for Timed Automata with Tasks,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115867819&doi=10.1145%2f3477020&partnerID=40&md5=1a3581420b96078b930734bcda45c09f,"Research on modeling and analysis of real-time computing systems has been done in two areas, model checking and real-time scheduling theory. In model checking, an expressive modeling formalism such as timed automata (TA) is used to model complex systems, but the analysis is typically very expensive due to state-space explosion. In real-time scheduling theory, the analysis techniques are highly efficient, but the models are often restrictive. In this paper, we aim to exploit the possibility of applying efficient analysis techniques rooted in real-time scheduling theory to analysis of real-time task systems modeled by timed automata with tasks (TAT). More specifically, we develop efficient techniques to analyze the feasibility of TAT-based task models (i.e., whether all tasks can meet their deadlines on single-processor) using demand bound functions (DBF), a widely used workload abstraction in real-time scheduling theory. Our proposed analysis method has a pseudo-polynomial time complexity if the number of clocks used to model each task is bounded by a constant, which is much lower than the exponential complexity of the traditional model-checking based analysis approach (also assuming the number of clocks is bounded by a constant). We apply dynamic programming techniques to implement the DBF-based analysis framework, and propose state space pruning techniques to accelerate the analysis process. Experimental results show that our DBF-based method can analyze a TAT system with 50 tasks within a few minutes, which significantly outperforms the state-of-the-art TAT-based schedulability analysis tool TIMES. © 2021 Association for Computing Machinery.",demand bound function; schedulability test; Timed automata,Automata theory; Clocks; Dynamic programming; Polynomial approximation; Real time systems; Scheduling; Analysis techniques; Bound function; Demand bound function; Models checking; Real time scheduling; Schedulability analysis; Schedulability test; Scheduling theory; Task-based; Timed Automata; Model checking
Precise Correlation Extraction for IoT Fault Detection with Concurrent Activities,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115882251&doi=10.1145%2f3477025&partnerID=40&md5=f4211523ea9344979bab3dba3d2be51a,"In the Internet of Things (IoT) environment, detecting a faulty device is crucial to guarantee the reliable execution of IoT services. To detect a faulty device, existing schemes trace a series of events among IoT devices within a certain time window, extract correlations among them, and find a faulty device that violates the correlations. However, if a few users share the same IoT environment, since their concurrent activities make non-correlated devices react together in the same time window, the existing schemes fail to detect a faulty device without differentiating the concurrent activities. To correctly detect a faulty device in the multiple concurrent activities, this work proposes a new precise correlation extraction scheme, called PCoExtractor. Instead of using a time window, PCoExtractor continuously traces the events, removes unrelated device statuses that inconsistently react for the same activity, and constructs fine-grained correlations. Moreover, to increase the detection precision, this work newly defines a fine-grained correlation representation that reflects not only sensor values and functionalities of actuators but also their transitions and program states such as contexts. Compared to existing schemes, PCoExtractor detects and identifies 40.06% more faults for 4 IoT services with concurrent activities of 12 users while reducing 80.3% of detection and identification times. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",anomaly detection; compiler; Internet of Things,Anomaly detection; Extraction; Fault detection; Anomaly detection; Concurrent activities; Correlation extraction; Environment detecting; Faults detection; Faulty devices; Fine grained; Precise correlations; Reliable execution; Time windows; Internet of things
HMDS: A Makespan Minimizing DAG Scheduler for Heterogeneous Distributed Systems,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115846197&doi=10.1145%2f3477037&partnerID=40&md5=04374a860b32719da1d4c9cbc526006b,"The problem of scheduling Directed Acyclic Graphs in order to minimize makespan (schedule length), is known to be a challenging and computationally hard problem. Therefore, researchers have endeavored towards the design of various heuristic solution generation techniques both for homogeneous as well as heterogeneous computing platforms. This work first presents HMDS-Bl, a list-based heuristic makespan minimization algorithm for task graphs on fully connected heterogeneous platforms. Subsequently, HMDS-Bl has been enhanced by empowering it with a low-overhead depth-first branch and bound based search approach, resulting in a new algorithm called HMDS. HMDS has been equipped with a set of novel tunable pruning mechanisms, which allow the designer to obtain a judicious balance between performance (makespan) and solution generation times, depending on the specific scenario at hand. Experimental analyses using randomly generated DAGs as well as benchmark task graphs, have shown that HMDS is able to comprehensively outperform state-of-the-art algorithms such as HEFT, PEFT, PPTS, etc., in terms of archived makespans while incurring bounded additional computation time overhead. © 2021 Association for Computing Machinery.",DAG scheduling; depth-first branch and bound; distributed systems; heterogeneous platforms; heuristic search; list scheduling; state-space search,Distributed computer systems; Heuristic algorithms; Scheduling; Space platforms; Branch and bounds; DAG scheduling; Depth first; Depth-first branch and bound; Heterogeneous platforms; Heuristic search; List-scheduling; Makespan; State space search; Tasks graph; Directed graphs
Compositional Learning and Verification of Neural Network Controllers,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115851367&doi=10.1145%2f3477023&partnerID=40&md5=00c9bb4cd690853feed455adaf98bc0d,"Recent advances in deep learning have enabled data-driven controller design for autonomous systems. However, verifying safety of such controllers, which are often hard-to-analyze neural networks, remains a challenge. Inspired by compositional strategies for program verification, we propose a framework for compositional learning and verification of neural network controllers. Our approach is to decompose the task (e.g., car navigation) into a sequence of subtasks (e.g., segments of the track), each corresponding to a different mode of the system (e.g., go straight or turn). Then, we learn a separate controller for each mode, and verify correctness by proving that (i) each controller is correct within its mode, and (ii) transitions between modes are correct. This compositional strategy not only improves scalability of both learning and verification, but also enables our approach to verify correctness for arbitrary compositions of the subtasks. To handle partial observability (e.g., LiDAR), we additionally learn and verify a mode predictor that predicts which controller to use. Finally, our framework also incorporates an algorithm that, given a set of controllers, automatically synthesizes the pre-and postconditions required by our verification procedure. We validate our approach in a case study on a simulation model of the F1/10 autonomous car, a system that poses challenges for existing verification tools due to both its reliance on LiDAR observations, as well as the need to prove safety for complex track geometries. We leverage our framework to learn and verify a controller that safely completes any track consisting of an arbitrary sequence of five kinds of track segments. © 2021 Association for Computing Machinery.",compositional reasoning; Neural networks; verification,Air navigation; Deep learning; Neural networks; Optical radar; Arbitrary compositions; Car navigation; Compositional reasoning; Compositional strategies; Controller designs; Learn+; Neural network controllers; Neural-networks; Program Verification; Subtask; Controllers
LATTE: LSTM Self-Attention based Anomaly Detection in Embedded Automotive Platforms,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115852058&doi=10.1145%2f3476998&partnerID=40&md5=8bf6c5f8a6323f12f6e39b6cfbff0970,"Modern vehicles can be thought of as complex distributed embedded systems that run a variety of automotive applications with real-time constraints. Recent advances in the automotive industry towards greater autonomy are driving vehicles to be increasingly connected with various external systems (e.g., roadside beacons, other vehicles), which makes emerging vehicles highly vulnerable to cyber-attacks. Additionally, the increased complexity of automotive applications and the in-vehicle networks results in poor attack visibility, which makes detecting such attacks particularly challenging in automotive systems. In this work, we present a novel anomaly detection framework called LATTE to detect cyber-attacks in Controller Area Network (CAN) based networks within automotive platforms. Our proposed LATTE framework uses a stacked Long Short Term Memory (LSTM) predictor network with novel attention mechanisms to learn the normal operating behavior at design time. Subsequently, a novel detection scheme (also trained at design time) is used to detect various cyber-attacks (as anomalies) at runtime. We evaluate our proposed LATTE framework under different automotive attack scenarios and present a detailed comparison with the best-known prior works in this area, to demonstrate the potential of our approach. © 2021 Association for Computing Machinery.",Anomaly detection; automotive networks; cyber-physical systems; machine learning; recurrent neural networks,Anomaly detection; Automotive industry; Complex networks; Computer crime; Control system synthesis; Crime; Cyber Physical System; Long short-term memory; Network security; Real time systems; Anomaly detection; Attack visibilities; Automotive applications; Automotive networks; Automotives; Design time; Distributed embedded system; External systems; In-vehicle networks; Real time constraints; Embedded systems
HEART: Hybrid Memory and Energy-Aware Real-Time Scheduling for Multi-Processor Systems,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115881868&doi=10.1145%2f3477019&partnerID=40&md5=51104264e74680e1eebd64f23cb4ded9,"Dynamic power management (DPM) reduces the power consumption of a computing system when it idles, by switching the system into a low power state for hibernation. When all processors in the system share the same component, e.g., a shared memory, powering off this component during hibernation is only possible when all processors idle at the same time. For a real-time system, the schedulability property has to be guaranteed on every processor, especially if idle intervals are considered to be actively introduced.In this work, we consider real-time systems with hybrid shared-memory architectures, which consist of shared volatile memory (VM) and non-volatile memory (NVM). Energy-efficient execution is achieved by applying DPM to turn off all memories during the hibernation mode. Towards this, we first explore the hybrid memory architectures and suggest a task model, which features configurable hibernation overheads. We propose a multi-processor procrastination algorithm (HEART), based on partitioned earliest-deadline-first (pEDF) scheduling. Our algorithm facilitates reducing the energy consumption by actively enlarging the hibernation time. It enforces all processors to idle simultaneously without violating the schedulability condition, such that the system can enter the hibernation state, where shared memories are turned off. Throughout extensive evaluation of HEART, we demonstrate (1) the increase in potential hibernation time, respectively the decrease in energy consumption, and (2) that our algorithm is not only more general but also has better performance than the state of the art with respect to energy efficiency in most cases. © 2021 Association for Computing Machinery.",dynamic power management; hybrid memory architecture; Multi-processor; non-volatile memory; normally-off computing,Energy efficiency; Energy utilization; Green computing; Interactive computer systems; Multiprocessing systems; Nonvolatile storage; Power management; Real time systems; Response time (computer systems); Scheduling; Scheduling algorithms; Dynamic power management; Energy-consumption; Hybrid energy; Hybrid memory; Hybrid memory architecture; Multi-processors; Normally off; Normally-off computing; Real - Time system; Shared memory; Memory architecture
Intermittent-Aware Neural Architecture Search,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115832853&doi=10.1145%2f3476995&partnerID=40&md5=154a7c3a72d20d48fc22b9db6860277d,"The increasing paradigm shift towards intermittent computing has made it possible to intermittently execute deep neural network (DNN) inference on edge devices powered by ambient energy. Recently, neural architecture search (NAS) techniques have achieved great success in automatically finding DNNs with high accuracy and low inference latency on the deployed hardware. We make a key observation, where NAS attempts to improve inference latency by primarily maximizing data reuse, but the derived solutions when deployed on intermittently-powered systems may be inefficient, such that the inference may not satisfy an end-to-end latency requirement and, more seriously, they may be unsafe given an insufficient energy budget. This work proposes iNAS, which introduces intermittent execution behavior into NAS to find accurate network architectures with corresponding execution designs, which can safely and efficiently execute under intermittent power. An intermittent-aware execution design explorer is presented, which finds the right balance between data reuse and the costs related to intermittent inference, and incorporates a preservation design search space into NAS, while ensuring the power-cycle energy budget is not exceeded. To assess an intermittent execution design, an intermittent-aware abstract performance model is presented, which formulates the key costs related to progress preservation and recovery during intermittent inference. We implement iNAS on top of an existing NAS framework and evaluate their respective solutions found for various datasets, energy budgets and latency requirements, on a Texas Instruments device. Compared to those NAS solutions that can safely complete the inference, the iNAS solutions reduce the intermittent inference latency by 60% on average while achieving comparable accuracy, with an average 7% increase in search overhead. © 2021 Association for Computing Machinery.",Deep neural networks; design space exploration; edge computing; energy harvesting; intermittent systems; neural architecture search,Budget control; Computer architecture; Edge computing; Energy harvesting; III-V semiconductors; Network architecture; Ambients; Data reuse; Design space exploration; Edge computing; Energy budgets; Intermittent systems; Network inference; Neural architecture search; Neural architectures; Paradigm shifts; Deep neural networks
"You only Traverse Twice: A YOTT Placement, Routing, and Timing Approach for CGRAs",2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115877366&doi=10.1145%2f3477038&partnerID=40&md5=53255098b62518731692ecd428a23afc,"Coarse-grained reconfigurable architecture (CGRA) mapping involves three main steps: placement, routing, and timing. The mapping is an NP-complete problem, and a common strategy is to decouple this process into its independent steps. This work focuses on the placement step, and its aim is to propose a technique that is both reasonably fast and leads to high-performance solutions. Furthermore, a near-optimal placement simplifies the following routing and timing steps. Exact solutions cannot find placements in a reasonable execution time as input designs increase in size. Heuristic solutions include meta-heuristics, such as Simulated Annealing (SA) and fast and straightforward greedy heuristics based on graph traversal. However, as these approaches are probabilistic and have a large design space, it is not easy to provide both run-time efficiency and good solution quality. We propose a graph traversal heuristic that provides the best of both: high-quality placements similar to SA and the execution time of graph traversal approaches. Our placement introduces novel ideas based on ""you only traverse twice""(YOTT) approach that performs a two-step graph traversal. The first traversal generates annotated data to guide the second step, which greedily performs the placement, node per node, aided by the annotated data and target architecture constraints. We introduce three new concepts to implement this technique: I/O and reconvergence annotation, degree matching, and look-ahead placement. Our analysis of this approach explores the placement execution time/quality trade-offs. We point out insights on how to analyze graph properties during dataflow mapping. Our results show that YOTT is 60.6, 9.7, and 2.3 faster than a high-quality SA, bounding box SA VPR, and multi-single traversal placements, respectively. Furthermore, YOTT reduces the average wire length and the maximal FIFO size (additional timing requirement on CGRAs) to avoid delay mismatches in fully pipelined architectures. © 2021 Association for Computing Machinery.",Coarse-grained reconfigurable architectures; graph traversal; placement,Computational complexity; Data flow analysis; Economic and social effects; Embedded systems; Mapping; Reconfigurable architectures; Timing circuits; Coarse grained reconfigurable architecture; Common strategy; Exact solution; Graph traversals; High quality; Near-optimal; Optimal placements; Performance; Placement; Routings; Simulated annealing
RT-ZooKeeper: Taming the Recovery Latency of a Coordination Service,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115879669&doi=10.1145%2f3477034&partnerID=40&md5=0b9946461034f092a5bb28da1c335fdf,"Fault-tolerant coordination services have been widely used in distributed applications in cloud environments. Recent years have witnessed the emergence of time-sensitive applications deployed in edge computing environments, which introduces both challenges and opportunities for coordination services. On one hand, coordination services must recover from failures in a timely manner. On the other hand, edge computing employs local networked platforms that can be exploited to achieve timely recovery. In this work, we first identify the limitations of the leader election and recovery protocols underlying Apache ZooKeeper, the prevailing open-source coordination service. To reduce recovery latency from leader failures, we then design RT-Zookeeper with a set of novel features including a fast-convergence election protocol, a quorum channel notification mechanism, and a distributed epoch persistence protocol. We have implemented RT-Zookeeper based on ZooKeeper version 3.5.8. Empirical evaluation shows that RT-ZooKeeper achieves 91% reduction in maximum recovery latency in comparison to ZooKeeper. Furthermore, a case study demonstrates that fast failure recovery in RT-ZooKeeper can benefit a common messaging service like Kafka in terms of message latency. © 2021 Association for Computing Machinery.",Apache ZooKeeper; Real-time fault tolerance; response time analysis,Edge computing; Recovery; Apache zookeeper; Cloud environments; Computing environments; Distributed applications; Edge computing; Fault-tolerant; Real- time; Real-time fault tolerance; Response-time analysis; Time sensitive applications; Fault tolerance
Synergistically Exploiting CNN Pruning and HLS Versioning for Adaptive Inference on Multi-FPGAs at the Edge,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115853792&doi=10.1145%2f3476990&partnerID=40&md5=0255c11bec452810efc803d09a76d14c,"FPGAs, because of their energy efficiency, reconfigurability, and easily tunable HLS designs, have been used to accelerate an increasing number of machine learning, especially CNN-based, applications. As a representative example, IoT Edge applications, which require low latency processing of resource-hungry CNNs, offload the inferences from resource-limited IoT end nodes to Edge servers featuring FPGAs. However, the ever-increasing number of end nodes pressures these FPGA-based servers with new performance and adaptability challenges. While some works have exploited CNN optimizations to alleviate inferences' computation and memory burdens, others have exploited HLS to tune accelerators for statically defined optimization goals. However, these works have not tackled both CNN and HLS optimizations altogether; neither have they provided any adaptability at runtime, where the workload's characteristics are unpredictable. In this context, we propose a hybrid two-step approach that, first, creates new optimization opportunities at design-time through the automatic training of CNN model variants (obtained via pruning) and the automatic generation of versions of convolutional accelerators (obtained during HLS synthesis); and, second, synergistically exploits these created CNN and HLS optimization opportunities to deliver a fully dynamic Multi-FPGA system that adapts its resources in a fully automatic or user-configurable manner. We implement this two-step approach as the AdaServ Framework and show, through a smart video surveillance Edge application as a case study, that it adapts to the always-changing Edge conditions: AdaServ processes at least 3.37× more inferences (using the automatic approach) and is at least 6.68× more energy-efficient (user-configurable approach) than original convolutional accelerators and CNN Models (VGG-16 and AlexNet). We also show that AdaServ achieves better results than solutions dynamically changing only the CNN model or HLS version, highlighting the importance of exploring both; and that it is always better than the best statically chosen CNN model and HLS version, showing the need for dynamic adaptability. © 2021 Association for Computing Machinery.",CNN inference; Edge computing; FPGA; High-Level Synthesis; pruning,Convolution; Edge computing; Energy efficiency; High level synthesis; Internet of things; Security systems; CNN inference; CNN models; Edge computing; High-level synthesis; Multi-FPGA; Optimisations; Pruning; Reconfigurability; Two-step approach; Versioning; Field programmable gate arrays (FPGA)
SIAM: Chiplet-based Scalable In-Memory Acceleration with Mesh for Deep Neural Networks,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115832331&doi=10.1145%2f3476999&partnerID=40&md5=cb35a3fdf5d067987af124e537699b2a,"In-memory computing (IMC) on a monolithic chip for deep learning faces dramatic challenges on area, yield, and on-chip interconnection cost due to the ever-increasing model sizes. 2.5D integration or chiplet-based architectures interconnect multiple small chips (i.e., chiplets) to form a large computing system, presenting a feasible solution beyond a monolithic IMC architecture to accelerate large deep learning models. This paper presents a new benchmarking simulator, SIAM, to evaluate the performance of chiplet-based IMC architectures and explore the potential of such a paradigm shift in IMC architecture design. SIAM integrates device, circuit, architecture, network-on-chip (NoC), network-on-package (NoP), and DRAM access models to realize an end-to-end system. SIAM is scalable in its support of a wide range of deep neural networks (DNNs), customizable to various network structures and configurations, and capable of efficient design space exploration. We demonstrate the flexibility, scalability, and simulation speed of SIAM by benchmarking different state-of-the-art DNNs with CIFAR-10, CIFAR-100, and ImageNet datasets. We further calibrate the simulation results with a published silicon result, SIMBA. The chiplet-based IMC architecture obtained through SIAM shows 130 and 72 improvement in energy-efficiency for ResNet-50 on the ImageNet dataset compared to Nvidia V100 and T4 GPUs. © 2021 Association for Computing Machinery.",Chiplet architecture; DNN acceleration; IMC benchmarking; in-memory compute; network-on-chip; network-on-package,"Benchmarking; Dynamic random access storage; Energy efficiency; Image enhancement; Integrated circuit design; Integrated circuit interconnects; Memory architecture; Network architecture; Network-on-chip; Program processors; Servers; Three dimensional integrated circuits; 2.5-D integration; Chiplet architecture; Deep neural network acceleration; In-memory compute; In-memory computing benchmarking; Interconnection costs; Model size; Monolithics; Network-on-ie, chiplet; Network-on-package; Deep neural networks"
Thermal-aware Adaptive Platform Management for Heterogeneous Embedded Systems,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115860642&doi=10.1145%2f3477028&partnerID=40&md5=3d3b6c95014edbaf9276620ab27c7ef5,"Recent trends in real-time applications have raised the demand for high-throughput embedded platforms with integrated CPU-GPU based Systems-On-Chip (SoCs). The enhanced performance of such SoCs, however, comes at the cost of increased power consumption, resulting in significant heat dissipation and high on-chip temperatures. The prolonged occurrences of high on-chip temperature can cause accelerated in-circuit ageing, which severely degrades the long-term performance and reliability of the chip. Violation of thermal constraints leads to on-board dynamic thermal management kicking-in, which may result in timing unpredictability for real-time tasks due to transient performance degradation. Recent work in adaptive software design have explored this issue from a control theoretic stand-point, striving for smooth thermal envelopes by tuning the core frequency.Existing techniques do not handle thermal violations for periodic real-time task sets in the presence of dynamic events like change of task periodicity, more so in the context of heterogeneous SoCs with integrated CPU-GPUs. This work presents an OpenCL runtime extension for thermal-aware scheduling of periodic, real-time tasks on heterogeneous multi-core platforms. Our framework mitigates dynamic thermal violations by adaptively tuning task mapping parameters, with the eventual control objective of satisfying both platform-level thermal constraints and task-level deadline constraints. We consider multiple platform-level control actions like task migration, frequency tuning and idle slot insertion as the task mapping parameters. To the best of our knowledge, this is the first work that considers such a variety of task mapping control actions in the context of heterogeneous embedded platforms. We evaluate the proposed framework on an Odroid-XU4 board using OpenCL benchmarks and demonstrate its effectiveness in reducing thermal violations. © 2021 Association for Computing Machinery.",adaptive thermal management; Heterogeneous computing; thermal violation,Embedded systems; Integrated circuit design; Mapping; Program processors; Real time systems; Software design; System-on-chip; Thermal management (electronics); Tuning; Adaptive thermal management; Embedded platforms; Heterogeneous computing; On-chip temperature; Real-time tasks; Systems-on-Chip; Tasks mapping; Thermal; Thermal constraints; Thermal violations; Temperature control
CICERO: A Domain-Specific Architecture for Efficient Regular Expression Matching,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115842418&doi=10.1145%2f3476982&partnerID=40&md5=fda3fca80f6a1a5ee607560813317f64,"Regular Expression (RE) matching is a computational kernel used in several applications. Since RE complexity and data volumes are steadily increasing, hardware acceleration is gaining attention also for this problem. Existing approaches have limited flexibility as they require a different implementation for each RE. On the other hand, it is complex to map efficient RE representations like non-deterministic finite-state automata onto software-programmable engines or parallel architectures. In this work, we present CICERO, an end-to-end framework composed of a domain-specific architecture and a companion compilation framework for RE matching. Our solution is suitable for many applications, such as genomics/proteomics and natural language processing. CICERO aims at exploiting the intrinsic parallelism of non-deterministic representations of the REs. CICERO can trade-off accelerators' efficiency and processors' flexibility thanks to its programmable architecture and the compilation framework. We implemented CICERO prototypes on embedded FPGA achieving up to 28.6× and 20.8× more energy efficiency than embedded and mainstream processors, respectively. Since it is a programmable architecture, it can be implemented as a custom ASIC that is orders of magnitude more energy-efficient than mainstream processors. © 2021 Association for Computing Machinery.",Domain-specific architecture; energy efficiency; non-deterministic automata; regular expressions,Economic and social effects; Natural language processing systems; Parallel architectures; Pattern matching; Pipeline processing systems; Computational kernels; Data volume; Deterministic finite state automata; Domain specific architectures; End to end; Hardware acceleration; Nondeterministic automata; Programmable architectures; Regular expressions; Regular-expression matching; Energy efficiency
"Killing Processes or Killing Flash? Escaping from the Dilemma Using Lightweight, Compression-Aware Swap for Mobile Devices",2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115868920&doi=10.1145%2f3477021&partnerID=40&md5=c5b241d296df7de6a4dc1b04448b7218,"Android apps become increasingly memory-demanding as software vendors add more and more new features to their apps. In the mean time, Android users often launch multiple apps and conveniently switch back and forth among the apps. Although running multiple apps imposes a high pressure on memory management, virtual-memory swap, an essential feature to improve the degree of multitasking, is disabled in fear of premature retirement of flash-based storage devices. Instead, Android employs a termination-based, process-level memory reclaiming method. We observed that process killing is, unfortunately, not effective in memory reclaiming and is highly negative to user experience. In this study, we advocate re-thinking using swap in Android for improved user experience with managed write stress on flash storage. Based on a series of empirical analyses of swap activities, we propose an enhanced page replacement policy and a page-compressing frontswap module. The proposed page replacement policy jointly considers page activeness and compressibility to boost the compression ratio of swap writes. A sampled-based method for page compressibility prediction is introduced so that decisions on page replacement can be made without compressing every page. We also design a frontswap module that strategically organizes compressed pages in the swap space for reducing the overhead of swap I/O operations. Experimental results showed that compared with process killing, our method improved the app launching time and energy consumption by 58% and 19%, respectively; compared with the original swap, our approach reduced the swap write stress by 65%. © 2021 Association for Computing Machinery.",Android; data compression; flash memory; swap,Android (operating system); Compressibility; Data compression ratio; Energy utilization; Virtual storage; Android; Android apps; High pressure; Memory-management; Page replacement; Replacement policy; Software vendors; Swap; Users' experiences; Virtual memory; Flash memory
MaxTracker: Continuously Tracking the Maximum Computation Progress for Energy Harvesting ReRAM-based CNN Accelerators,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115844390&doi=10.1145%2f347709&partnerID=40&md5=bf0f0a5ff0886b38b4c455f8a255838a,"There is an ongoing trend to increasingly offload inference tasks, such as CNNs, to edge devices in many IoT scenarios. As energy harvesting is an attractive IoT power source, recent ReRAM-based CNN accelerators have been designed for operation on harvested energy. When addressing the instability problems of harvested energy, prior optimization techniques often assume that the load is fixed, overlooking the close interactions among input power, computational load, and circuit efficiency, or adapt the dynamic load to match the just-in-time incoming power under a simple harvesting architecture with no intermediate energy storage.Targeting a more efficient harvesting architecture equipped with both energy storage and energy delivery modules, this paper is the first effort to target whole system, end-to-end efficiency for an energy harvesting ReRAM-based accelerator. First, we model the relationships among ReRAM load power, DC-DC converter efficiency, and power failure overhead. Then, a maximum computation progress tracking scheme (MaxTracker) is proposed to achieve a joint optimization of the whole system by tuning the load power of the ReRAM-based accelerator. Specifically, MaxTracker accommodates both continuous and intermittent computing schemes and provides dynamic ReRAM load according to harvesting scenarios.We evaluate MaxTracker over four input power scenarios, and the experimental results show average speedups of 38.4%/40.3% (up to 51.3%/84.4%), over a full activation scheme (with energy storage) and order-of-magnitude speedups over the recently proposed (energy storage-less) ResiRCA technique. Furthermore, we also explore MaxTracker in combination with the Capybara reconfigurable capacitor approach to offer more flexible tuners and thus further boost the system performance. © 2021 2021 Association for Computing Machinery.",CNN; computing schemes; DC-DC efficiency; Energy harvesting; maximum computation progress; ReRAM crossbar,Chemical activation; DC-DC converters; Dynamic loads; Internet of things; Memory architecture; RRAM; CNN; Computing scheme; DC-DC efficiency; Energy; Input power; Instability problems; Load power; Maximum computation progress; Power sources; ReRAM crossbar; Energy harvesting
Comparative Analysis and Enhancement of CFG-based Hardware-Assisted CFI Schemes,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115870568&doi=10.1145%2f3476989&partnerID=40&md5=43658e3fa462ec41b1f833306693ce12,"Subverting the flow of instructions (e.g., by use of code-reuse attacks) still poses a serious threat to the security of today's systems. Various control flow integrity (CFI) schemes have been proposed as a powerful technique to detect and mitigate such attacks. In recent years, many hardware-assisted implementations of CFI enforcement based on control flow graphs (CFGs) have been presented by academia. Such approaches check whether control flow transfers follow the intended CFG by limiting the valid target addresses. However, these papers all target different platforms and were evaluated with different sets of benchmark applications, which makes quantitative comparisons hardly possible.For this paper, we have implemented multiple promising CFG-based CFI schemes on a common platform comprising a RISC-V within FPGA. By porting almost 40 benchmark applications to this system we can present a meaningful comparison of the various techniques in terms of run-time performance, hardware utilization, and binary size. In addition, we present an enhanced CFI approach that is inspired by what we consider the best concepts and ideas of previously proposed mechanisms. We have made this approach more practical and feature-complete by tackling some problems largely ignored previously. We show with this fine-grained scheme that CFI can be achieved with even less overheads than previously demonstrated. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",code reuse attack; control flow graph; Control flow integrity; shadow stack,Benchmarking; Codes (symbols); Embedded systems; Flow graphs; Graphic methods; Benchmark applications; Code reuse; Code reuse attack; Comparative analyzes; Control-flow; Control-flow graphs; Control-flow integrities; Graph-based; Hardware-assisted; Shadow stack; Data flow analysis
RiSA: A Reinforced Systolic Array for Depthwise Convolutions and Embedded Tensor Reshaping,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115855982&doi=10.1145%2f3476984&partnerID=40&md5=f70cd6d8c70e9f993356893ff3e46632,"Depthwise convolutions are widely used in convolutional neural networks (CNNs) targeting mobile and embedded systems. Depthwise convolution layers reduce the computation loads and the number of parameters compared to the conventional convolution layers. Many deep neural network (DNN) accelerators adopt an architecture that exploits the high data-reuse factor of DNN computations, such as a systolic array. However, depthwise convolutions have low data-reuse factor and under-utilize the processing elements (PEs) in systolic arrays. In this paper, we present a DNN accelerator design called RiSA, which provides a novel mechanism that boosts the PE utilization for depthwise convolutions on a systolic array with minimal overheads.In addition, the PEs in systolic arrays can be efficiently used only if the data items (tensors) are arranged in the desired layout. Typical DNN accelerators provide various types of PE interconnects or additional modules to flexibly rearrange the data items and manage data movements during DNN computations. RiSA provides a lightweight set of tensor management tasks within the PE array itself that eliminates the need for an additional module for tensor reshaping tasks. Using this embedded tensor reshaping, RiSA supports various DNN models, including convolutional neural networks and natural language processing models while maintaining a high area efficiency.Compared to Eyeriss v2, RiSA improves the area and energy efficiency for MobileNet-V1 inference by 1.91× and 1.31×, respectively. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Accelerators; deep neural networks; depthwise convolution,Acceleration; Deep neural networks; Embedded systems; Energy efficiency; Natural language processing systems; Systolic arrays; Tensors; Accelerator design; Computation loads; Convolutional neural network; Data items; Data reuse; Depthwise convolution; Embedded-system; Mobile systems; Network computations; Processing elements; Convolution
Declarative Power Sequencing,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115836256&doi=10.1145%2f3477039&partnerID=40&md5=968c7428d2cd6a41888186a77c1168f3,"Modern computer server systems are increasingly managed at a low level by baseboard management controllers (BMCs). BMCs are processors with access to the most critical parts of the platform, below the level of OS or hypervisor, including control over power delivery to every system component. Buggy or poorly designed BMC software not only poses a security threat to a machine, it can permanently render the hardware inoperative. Despite this, there is little published work on how to rigorously engineer the power management functionality of BMCs so as to prevent this happening.This article takes a first step toward putting BMC software on a sound footing by specifying the hardware environment and the constraints necessary for safe and correct operation. This is best accomplished through automation: correct-by-construction power control sequences can be efficiently generated from a simple, trustworthy model of the platform's power tree that incorporates the sequencing requirements and safe voltage ranges of all components.We present both a modeling language for complex power-delivery networks and a tool to automatically generate safe, efficient power sequences for complex modern platforms. This not only increases the trustworthiness of a hitherto opaque yet critical element of platform firmware: regulator and chip power models are significantly simpler to produce than hand-written power sequences. This, combined with model reuse for common components, reduces both time and cost associated with platform bring-up for new hardware.We evaluate our tool using a new high-performance 2-socket server platform with >100W per socket TDP, tight voltage limits and 25 distinct power regulators needing configuration, showing both fast (<10s) tool runtime, and correct power sequencing of a live system. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",board management controller; declarative specification; Power sequencing; reliable firmware,Complex networks; Controllers; Electric power transmission; Modeling languages; Network security; Power control; Board management controller; Computer servers; Declarative power; Declarative specification; Management controller; Power; Power sequence; Power sequencing; Reliable firmware; Simple++; Firmware
REPAIR: Control Flow Protection based on Register Pairing Updates for SW-Implemented HW Fault Tolerance,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115832286&doi=10.1145%2f347701&partnerID=40&md5=84f7d5e89aa60f7d0eb271f16be15ce5,"Safety-critical embedded systems may either use specialized hardware or rely on Software-Implemented Hardware Fault Tolerance (SIHFT) to meet soft error resilience requirements. SIHFT has the advantage that it can be used with low-cost, off-the-shelf components such as standard Micro-Controller Units. For this, SIHFT methods apply redundancy in software computation and special checker codes to detect transient errors, so called soft errors, that either corrupt the data flow or the control flow of the software and may lead to Silent Data Corruption (SDC). So far, this is done by applying separate SIHFT methods for the data and control flow protection, which leads to large overheads in computation time.This work in contrast presents REPAIR, a method that exploits the checks of the SIHFT data flow protection to also detect control flow errors as well, thereby, yielding higher SDC resilience with less computational overhead. For this, the data flow protection methods entail duplicating the computation with subsequent checks placed strategically throughout the program. These checks assure that the two redundant computation paths, which work on two different parts of the register file, yield the same result. By updating the pairing between the registers used in the primary computation path and the registers in the duplicated computation path using the REPAIR method, these checks also fail with high coverage when a control flow error, which leads to an illegal jumps, occurs. Extensive RTL fault injection simulations are carried out to accurately quantify soft error resilience while evaluating Mibench programs along with an embedded case-study running on an OpenRISC processor. Our method performs slightly better on average in terms of soft error resilience compared to the best state-of-the-art method but requiring significantly lower overheads. These results show that REPAIR is a valuable addition to the set of known SIHFT methods. © 2021 Association for Computing Machinery.",code generation; embedded resilience; functional safety; Soft errors,Codes (symbols); Crime; Embedded systems; Error correction; Fault tolerance; Radiation hardening; Codegeneration; Computation paths; Control-flow; Dataflow; Embedded resilience; Error resilience; Functional Safety; Silent data corruptions; Soft error; Software-implemented hardware fault tolerances; Data transfer
SNR: Squeezing Numerical Range Defuses Bit Error Vulnerability Surface in Deep Neural Networks,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115862557&doi=10.1145%2f347707&partnerID=40&md5=e33445d57310a2e63e0950ebe60910c4,"As deep learning algorithms are widely adopted, an increasing number of them are positioned in embedded application domains with strict reliability constraints. The expenditure of significant resources to satisfy performance requirements in deep neural network accelerators has thinned out the margins for delivering safety in embedded deep learning applications, thus precluding the adoption of conventional fault tolerance methods. The potential of exploiting the inherent resilience characteristics of deep neural networks remains though unexplored, offering a promising low-cost path towards safety in embedded deep learning applications. This work demonstrates the possibility of such exploitation by juxtaposing the reduction of the vulnerability surface through the proper design of the quantization schemes with shaping the parameter distributions at each layer through the guidance offered by appropriate training methods, thus delivering deep neural networks of high resilience merely through algorithmic modifications. Unequaled error resilience characteristics can be thus injected into safety-critical deep learning applications to tolerate bit error rates of up to at absolutely zero hardware, energy, and performance costs while improving the error-free model accuracy even further. © 2021 Copyright held by the owner/author(s).",Error resilience; neural network quantization; neural network regularization,Bit error rate; Errors; Fault tolerance; Learning algorithms; Multilayer neural networks; Network security; Safety engineering; Bit-errors; Embedded application; Error resilience; Neural network quantization; Neural network regularization; Neural-networks; Numerical range; Quantisation; Regularisation; Deep neural networks
On-device Prior Knowledge Incorporated Learning for Personalized Atrial Fibrillation Detection,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115864618&doi=10.1145%2f3476987&partnerID=40&md5=d25c30dc6db842926c2e398fd880004a,"Atrial Fibrillation (AF), one of the most prevalent arrhythmias, is an irregular heart-rate rhythm causing serious health problems such as stroke and heart failure. Deep learning based methods have been exploited to provide an end-to-end AF detection by automatically extracting features from Electrocardiogram (ECG) signal and achieve state-of-the-art results. However, the pre-trained models cannot adapt to each patient's rhythm due to the high variability of rhythm characteristics among different patients. Furthermore, the deep models are prone to overfitting when fine-tuned on the limited ECG of the specific patient for personalization. In this work, we propose a prior knowledge incorporated learning method to effectively personalize the model for patient-specific AF detection and alleviate the overfitting problems. To be more specific, a prior-incorporated portion importance mechanism is proposed to enforce the network to learn to focus on the targeted portion of the ECG, following the cardiologists' domain knowledge in recognizing AF. A prior-incorporated regularization mechanism is further devised to alleviate model overfitting during personalization by regularizing the fine-tuning process with feature priors on typical AF rhythms of the general population. The proposed personalization method embeds the well-defined prior knowledge in diagnosing AF rhythm into the personalization procedure, which improves the personalized deep model and eliminates the workload of manually adjusting parameters in conventional AF detection method. The prior knowledge incorporated personalization is feasibly and semi-automatically conducted on the edge, device of the cardiac monitoring system. We report an average AF detection accuracy of 95.3% of three deep models over patients, surpassing the pre-trained model by a large margin of 11.5% and the fine-tuning strategy by 8.6%. © 2021 Association for Computing Machinery.",Arrhythmia detection; atrial fibrillation; neural networks; personalization; prior knowledge,Cardiology; Deep learning; Electrocardiography; Heart; Tuning; Arrhythmia detection; Atrial fibrillation; Fine tuning; Heart failure; Heart-rate; Learning-based methods; Neural-networks; Overfitting; Personalizations; Prior-knowledge; Diseases
Chauffeur: Benchmark Suite for Design and End-to-End Analysis of Self-Driving Vehicles on Embedded Systems,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115880885&doi=10.1145%2f347705&partnerID=40&md5=9fd2a7300eb42b5955f8090c6b7d4742,"Self-driving systems execute an ensemble of different self-driving workloads on embedded systems in an end-to-end manner, subject to functional and performance requirements. To enable exploration, optimization, and end-to-end evaluation on different embedded platforms, system designers critically need a benchmark suite that enables flexible and seamless configuration of self-driving scenarios, which realistically reflects real-world self-driving workloads' unique characteristics. Existing CPU and GPU embedded benchmark suites typically (1) consider isolated applications, (2) are not sensor-driven, and (3) are unable to support emerging self-driving applications that simultaneously utilize CPUs and GPUs with stringent timing requirements. On the other hand, full-system self-driving simulators (e.g., AUTOWARE, APOLLO) focus on functional simulation, but lack the ability to evaluate the self-driving software stack on various embedded platforms. To address design needs, we present Chauffeur, the first open-source end-to-end benchmark suite for self-driving vehicles with configurable representative workloads. Chauffeur is easy to configure and run, enabling researchers to evaluate different platform configurations and explore alternative instantiations of the self-driving software pipeline. Chauffeur runs on diverse emerging platforms and exploits heterogeneous onboard resources. Our initial characterization of Chauffeur on different embedded platforms-NVIDIA Jetson TX2 and Drive PX2-enables comparative evaluation of these GPU platforms in executing an end-to-end self-driving computational pipeline to assess the end-to-end response times on these emerging embedded platforms while also creating opportunities to create application gangs for better response times. Chauffeur enables researchers to benchmark representative self-driving workloads and flexibly compose them for different self-driving scenarios to explore end-to-end tradeoffs between design constraints, power budget, real-time performance requirements, and accuracy of applications. © 2021 Copyright held by the owner/author(s).",Autonomous vehicles; benchmark suite; self-driving vehicles,Benchmarking; Budget control; Embedded systems; Open source software; Open systems; Pipelines; Program processors; Autonomous Vehicles; Benchmark suites; Driving systems; Embedded platforms; Embedded-system; End to end; End-to-end analysis; Performance requirements; Self drivings; Self-driving vehicle; Autonomous vehicles
Rtkaller: State-aware Task Generation for RTOS Fuzzing,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115869866&doi=10.1145%2f3477014&partnerID=40&md5=374eaf0d0033b828999b27fdcb1c7254,"A real-time operating system (RTOS) is an operating system designed to meet certain real-time requirements. It is widely used in embedded applications, and its correctness is safety-critical. However, the validation of RTOS is challenging due to its complex real-time features and large code base.In this paper, we propose Rtkaller, a state-aware kernel fuzzer for the vulnerability detection in RTOS. First, Rtkaller implements an automatic task initialization to transform the syscall sequences into initial tasks with more real-time information. Then, a coverage-guided task mutation is designed to generate those tasks that explore more in-depth real-time related code for parallel execution. Moreover, Rtkaller realizes a task modification to correct those tasks that may hang during fuzzing. We evaluated it on recent versions of rt-Linux, which is one of the most widely used RTOS. Compared to the state-of-the-art kernel fuzzers Syzkaller and Moonshine, Rtkaller achieves the same code coverage at the speed of 1.7X and 1.6X, gains an increase of 26.1% and 22.0% branch coverage within 24 hours respectively. More importantly, Rtkaller has confirmed 28 previously unknown vulnerabilities that are missed by other fuzzers. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Fuzz testing; RTOS; task generation; vulnerability detection,Real time systems; Safety engineering; Embedded application; Fuzz Testing; Large code basis; Real time requirement; Real- time; Real-time features; Real-time information; Real.time operating system; Task generations; Vulnerability detection; Computer operating systems
Synthesis-guided Adversarial Scenario Generation for Gray-box Feedback Control Systems with Sensing Imperfections,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115875483&doi=10.1145%2f3477033&partnerID=40&md5=aaf71cfd7310e81b9f7a5fdbddeb111a,"In this paper, we study feedback dynamical systems with memoryless controllers under imperfect information. We develop an algorithm that searches for ""adversarial scenarios"", which can be thought of as the strategy for the adversary representing the noise and disturbances, that lead to safety violations. The main challenge is to analyze the closed-loop system's vulnerabilities with a potentially complex or even unknown controller in the loop. As opposed to commonly adopted approaches that treat the system under test as a black-box, we propose a synthesis-guided approach, which leverages the knowledge of a plant model at hand. This hence leads to a way to deal with gray-box systems (i.e., with known plant and unknown controller). Our approach reveals the role of the imperfect information in the violation. Examples show that our approach can find non-trivial scenarios that are difficult to expose by random simulations. This approach is further extended to incorporate model mismatch and to falsify vision-in-the-loop systems against finite-time reach-avoid specifications. © 2021 Association for Computing Machinery.",Adversarial scenarios; imperfect information games; safety,Adaptive control systems; Closed loop systems; Control system synthesis; Dynamical systems; Feedback control; Adversarial scenario; Closed-loop system; Feedback control system; Grey-box; Imperfect information; Imperfect information games; Memoryless; Safety violations; Scenarios generation; System vulnerability; Controllers
Cross-Layer Adaptation with Safety-Assured Proactive Task Job Skipping,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115831861&doi=10.1145%2f3477031&partnerID=40&md5=98bfc6425e0b93bdc5c3f0f6d3daa304,"During the operation of many real-time safety-critical systems, there are often strong needs for adapting to a dynamic environment or evolving mission objectives, e.g., increasing sampling and control frequencies of some functions to improve their performance under certain situations. However, a system's ability to adapt is often limited by tight resource constraints and rigid periodic execution requirements. In this work, we present a cross-layer approach to improve system adaptability by allowing proactive skipping of task executions, so that the resources can be either saved directly or re-allocated to other tasks for their performance improvement. Our approach includes three novel elements: (1) formal methods for deriving the feasible skipping choices of control tasks with safety guarantees at the functional layer, (2) a schedulability analysis method for assessing system feasibility at the architectural layer under allowed task job skippings, and (3) a runtime adaptation algorithm that efficiently explores job skipping choices and task priorities for meeting system adaptation requirements while ensuring system safety and timing correctness. Experiments demonstrate the effectiveness of our approach in meeting system adaptation needs. © 2021 Association for Computing Machinery.",adaptation; Cross-layer; safety; weakly hard,Data Link Layer; Real time systems; Safety engineering; Adaptation; Cross layer; Cross-layer adaptation; Dynamic environments; Job skipping; Performance; Real- time; Safety critical systems; System adaptation; Weakly hard; Formal methods
PHiLIP on the HiL: Automated Multi-Platform OS Testing with External Reference Devices,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115874676&doi=10.1145%2f3477040&partnerID=40&md5=ddbcfcd2388e9fed11df85c8fe5bec66,"Developing an operating systems (OSs) for low-end embedded devices requires continuous adaptation to new hardware architectures and components, while serviceability of features needs to be assured for each individual platform under tight resource constraints. It is challenging to design a versatile and accurate heterogeneous test environment that is agile enough to cover a continuous evolution of the code base and platforms. This mission is even more challenging when organized in an agile open-source community process with many contributors such as for the RIOT OS. Hardware in the Loop (HiL) testing and Continuous Integration (CI) are automatable approaches to verify functionality, prevent regressions, and improve the overall quality at development speed in large community projects.In this paper, we present PHiLIP (Primitive Hardware in the Loop Integration Product), an open-source external reference device together with tools that validate the system software while it controls hardware and interprets physical signals. Instead of focusing on a specific test setting, PHiLIP takes the approach of a tool-assisted agile HiL test process, designed for continuous evolution and deployment cycles. We explain its design, describe how it supports HiL tests, evaluate performance metrics, and report on practical experiences of employing PHiLIP in an automated CI test infrastructure. Our initial deployment comprises 22 unique platforms, each of which executes 98 peripheral tests every night. PHiLIP allows for easy extension of low-cost, adaptive testing infrastructures but serves testing techniques and tools to a much wider range of applications. © 2021 Association for Computing Machinery.",constrained devices; hardware in the loop; IoT; operating system,Costs; Embedded systems; Hardware-in-the-loop simulation; Integration testing; Open source software; Open systems; Synthetic apertures; Testing; Constrained devices; Continuous integrations; Embedded device; Hardware architecture; Hardware components; Hardware-in-the-loop tests; Multi-platform; Operating system; Reference devices; System testing; Internet of things
FLASH: Fast Neural Architecture Search with Hardware Optimization,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115841269&doi=10.1145%2f3476994&partnerID=40&md5=ff2dbcd3bea5e02f8e572d3be9d4d779,"Neural architecture search (NAS) is a promising technique to design efficient and high-performance deep neural networks (DNNs). As the performance requirements of ML applications grow continuously, the hardware accelerators start playing a central role in DNN design. This trend makes NAS even more complicated and time-consuming for most real applications. This paper proposes FLASH, a very fast NAS methodology that co-optimizes the DNN accuracy and performance on a real hardware platform. As the main theoretical contribution, we first propose the NN-Degree, an analytical metric to quantify the topological characteristics of DNNs with skip connections (e.g., DenseNets, ResNets, Wide-ResNets, and MobileNets). The newly proposed NN-Degree allows us to do training-free NAS within one second and build an accuracy predictor by training as few as 25 samples out of a vast search space with more than 63 billion configurations. Second, by performing inference on the target hardware, we fine-tune and validate our analytical models to estimate the latency, area, and energy consumption of various DNN architectures while executing standard ML datasets. Third, we construct a hierarchical algorithm based on simplicial homology global optimization (SHGO) to optimize the model-architecture co-design process, while considering the area, latency, and energy consumption of the target hardware. We demonstrate that, compared to the state-of-the-art NAS approaches, our proposed hierarchical SHGO-based algorithm enables more than four orders of magnitude speedup (specifically, the execution time of the proposed algorithm is about 0.1 seconds). Finally, our experimental evaluations show that FLASH is easily transferable to different hardware architectures, thus enabling us to do NAS on a Raspberry Pi-3B processor in less than 3 seconds. © 2021 Association for Computing Machinery.",hardware optimization; model-architecture co-design; network science; neural architecture search; Neural networks; resource-constrained devices,Computer aided design; Constrained optimization; Energy utilization; Global optimization; Memory architecture; Network architecture; Co-designs; Hardware optimization; Model-architecture co-design; Modeling architecture; Network science; Neural architecture search; Neural architectures; Neural-networks; Performance; Resourceconstrained devices; Deep neural networks
Heterogeneity-aware Multicore Synchronization for Intermittent Systems,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115844499&doi=10.1145%2f3476992&partnerID=40&md5=8298a251d62f346a09fc2145c36439a0,"Intermittent systems enable batteryless devices to operate through energy harvesting by leveraging the complementary characteristics of volatile (VM) and non-volatile memory (NVM). Unfortunately, alternate and frequent accesses to heterogeneous memories for accumulative execution across power cycles can significantly hinder computation progress. The progress impediment is mainly due to more CPU time being wasted for slow NVM accesses than for fast VM accesses. This paper explores how to leverage heterogeneous cores to mitigate the progress impediment caused by heterogeneous memories. In particular, a delegable and adaptive synchronization protocol is proposed to allow memory accesses to be delegated between cores and to dynamically adapt to diverse memory access latency. Moreover, our design guarantees task serializability across multiple cores and maintains data consistency despite frequent power failures. We integrated our design into FreeRTOS running on a Cypress device featuring heterogeneous dual cores and hybrid memories. Experimental results show that, compared to recent approaches that assume single-core intermittent systems, our design can improve computation progress at least 1.8x and even up to 33.9x by leveraging core heterogeneity. © 2021 Association for Computing Machinery.",batteryless devices; data consistency; intermittent computing; Multicore synchronization; task concurrency,Digital storage; Energy harvesting; Battery-less; Batteryless device; Complementary characteristics; Data consistency; Heterogeneous memory; Intermittent computing; Intermittent systems; Multi-cores; Multicore synchronization; Task concurrency; Synchronization
Towards an Integrated Vehicle Management System in DriveOS,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115839064&doi=10.1145%2f3477013&partnerID=40&md5=193791e6cf121fd7f766ecb1cc332644,"Modern automotive systems feature dozens of electronic control units (ECUs) for chassis, body and powertrain functions. These systems are costly and inflexible to upgrade, requiring ever increasing numbers of ECUs to support new features such as advanced driver assistance (ADAS), autonomous technologies, and infotainment. To counter these challenges, we propose DriveOS, a safe, secure, extensible, and timing-predictable system for modern vehicle management in a centralized platform. DriveOS is based on a separation kernel, where timing and safety-critical ECU functions are implemented in a real-time OS (RTOS) alongside non-critical software in Linux or Android. The system enforces the separation, or partitioning, of both software and hardware among different OSes.DriveOS runs on a relatively low-cost embedded PC-class platform, supporting multiple cores and hardware virtualization capabilities. Instrument cluster, in-vehicle infotainment and advanced driver assistance system services are implemented in a Yocto Linux guest, which communicates with critical real-time services via secure shared memory. The RTOS manages a real-time controller area network (CAN) interface that is inaccessible to Linux services except via well-defined and legitimate communication channels. In this work, we integrate three Qt-based services written for Yocto Linux, running in parallel with a real-time longitudinal controller task and multiple CAN bus concentrators, for vehicular sensor data processing and actuation. We demonstrate the benefits and performance of DriveOS with a hardware-in-the-loop CARLA simulation using a real car dataset. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Automotive systems; partitioning hypervisor; safety-criticality,Advanced driver assistance systems; Control system synthesis; Controllers; Linux; Vehicles; Advanced driver assistances; Automotive Systems; Autonomous technology; Electronics control unit; Hypervisors; Partitioning hypervisor; Real- time; Safety criticality; System features; Vehicle management systems; Automobile drivers
Tolerating Defects in Low-Power Neural Network Accelerators Via Retraining-Free Weight Approximation,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115882849&doi=10.1145%2f3477016&partnerID=40&md5=82d3558d749bfddf1e261029b0c6efd1,"Hardware accelerators are essential to the accommodation of ever-increasing Deep Neural Network (DNN) workloads on the resource-constrained embedded devices. While accelerators facilitate fast and energy-efficient DNN operations, their accuracy is threatened by faults in their on-chip and off-chip memories, where millions of DNN weights are held. The use of emerging Non-Volatile Memories (NVM) further exposes DNN accelerators to a non-negligible rate of permanent defects due to immature fabrication, limited endurance, and aging. To tolerate defects in NVM-based DNN accelerators, previous work either requires extra redundancy in hardware or performs defect-aware retraining, imposing significant overhead. In comparison, this paper proposes a set of algorithms that exploit the flexibility in setting the fault-free bits in weight memory to effectively approximate weight values, so as to mitigate defect-induced accuracy drop. These algorithms can be applied as a one-step solution when loading the weights to embedded devices. They only require trivial hardware support and impose negligible run-time overhead. Experiments on popular DNN models show that the proposed techniques successfully boost inference accuracy even in the face of elevated defect rates in the weight memory. © 2021 Association for Computing Machinery.",approximation; defect tolerance; memory faults; Neural network accelerator; reliability,Acceleration; Computer aided design; Defects; Energy efficiency; Low power electronics; Approximation; Defect tolerance; Embedded device; Energy efficient; Hardware accelerators; Low Power; Memory faults; Network operations; Neural network accelerator; Neural-networks; Deep neural networks
Excluding Parallel Execution to Improve Global Fixed Priority Response Time Analysis,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115848959&doi=10.1145%2f3477035&partnerID=40&md5=cf85e338d1880ac061778d8783555359,"Response Time Analysis (RTA) is an effective method for testing the schedulability of real-time tasks on multiprocessor platforms. Existing RTAs for global fixed priority scheduling calculate the upper bound of the worst case response time of each task. Given a target task, existing RTAs first calculate the workload upper bound of each higher priority task (than the target task), and then calculate the interference on the target task by each higher priority task according to the obtained workload upper bounds. The workload of a task consists of three parts: carry-in, body and carry-out. The interference from all these three parts may be overestimated in existing RTAs. However, although the overestimation of the interference from body is the major factor that causes the low accuracy of existing RTAs, all existing work only focuses on how to reduce the overestimation of the interference from carry-in, and there is no method to reduce the overestimation of the interference from body or carry-out. In this work, we propose a method to calculate the lower bound of the accumulative time in which the target task and higher priority tasks are executed in parallel. By excluding the parallel execution time from the interference, we derive a new RTA test that can reduce the overestimation of the interference from all three parts of the workload. Extensive experiments are conducted to verify the superior performance of the proposed RTA test. © 2021 Association for Computing Machinery.",global scheduling; multiprocessor; parallel execution; real-time systems; Response time analysis,Interactive computer systems; Multiprocessing systems; Response time (computer systems); Scheduling; Fixed priorities; Global scheduling; Multi-processor platforms; Parallel executions; Priority tasks; Real - Time system; Real-time tasks; Response-time analysis; Schedulability; Upper Bound; Real time systems
MARCO: A High-performance Task Mapping and Routing Co-optimization Framework for Point-to-Point NoC-based Heterogeneous Computing Systems,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115852011&doi=10.1145%2f3476985&partnerID=40&md5=168fe1e057ac913a578ede9046296350,"Heterogeneous computing systems (HCSs), which consist of various processing elements (PEs) that vary in their processing ability, are usually facilitated by the network-on-chip (NoC) to interconnect its components. The emerging point-to-point NoCs which support single-cycle-multi-hop transmission, reduce or eliminate the latency dependence on distance, addressing the scalability concern raised by high latency for long-distance transmission and enlarging the design space of the routing algorithm to search the non-shortest paths. For such point-to-point NoC-based HCSs, resource management strategies which are managed by compilers, scheduler, or controllers, e.g., mapping and routing, are complicated for the following reasons: (i) Due to the heterogeneity, mapping and routing need to optimize computation and communication concurrently (for homogeneous computing systems, only communication). (ii) Conducting mapping and routing consecutively cannot minimize the schedule length in most cases since the PEs with high processing ability may locate in the crowded area and suffer from high resource contention overhead. (iii) Since changing the mapping selection of one task will reconstruct the whole routing design space, the exploration of mapping and routing design space is challenging. Therefore, in this work, we propose MARCO, the mapping and routing co-optimization framework, to decrease the schedule length of applications on point-to-point NoC-based HCSs. Specifically, we revise the tabu search to explore the design space and evaluate the quality of mapping and routing. The advanced reinforcement learning (RL)algorithm, i.e., advantage actor-critic, is adopted to efficiently compute paths. We perform extensive experiments on various real applications, which demonstrates that the MARCO achieves a remarkable performance improvement in terms of schedule length (+44.94% ∼+50.18%) when compared with the state-of-the-art mapping and routing co-optimization algorithm for homogeneous computing systems. We also compare MARCO with different combinations of state-of-the-art mapping and routing approaches. © 2021 Association for Computing Machinery.",heterogeneous computing systems; Mapping; noc; routing,Integrated circuit design; Network-on-chip; Reinforcement learning; Tabu search; Co-optimization; Computing system; Design spaces; Heterogeneous computing system; Noc; Optimization framework; Processing ability; Processing elements; Routings; Schedule length; Mapping
A Hierarchical Hybrid Locking Protocol for Parallel Real-Time Tasks,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115838800&doi=10.1145%2f3477017&partnerID=40&md5=4d8fe1cb00705f5bf1b998b2b96d3ccb,"Parallel tasks have been paid growing attention in recent years, and the scheduling with shared resources is of significant importance to real-time systems. As an efficient mechanism to provide mutual exclusion for parallel processing, spin-locks are ubiquitous in multi-processor real-time systems. However, the spin-locks suffer the scalability problem, and the intra-task parallelism further exacerbates the analytical pessimism. To overcome such deficiencies, we propose a Hierarchical Hybrid Locking Protocol (H2LP) under federated scheduling. The proposed H2LP integrates the classical Multiprocessor Stack Resource Policy (MSRP) and uses a token mechanism to reduce global contentions. We provide a complete analysis framework supporting both heavy and light tasks under federated scheduling and develop a blocking analysis with the state-of-the-art linear optimization technique. Empirical evaluations showed that the H2LP outperformed the other state-of-the-art locking protocols in at least configurations when considering exclusive clustering. Furthermore, our partitioned approach for light tasks can substantially improve schedulability by mitigating the over-provisioning problem. © 2021 Association for Computing Machinery.",parallel tasks; Real-time embedded system; real-time scheduling; real-time synchronization,Embedded systems; Interactive computer systems; Linear programming; Locks (fasteners); Parallel processing systems; Real time systems; Locking protocols; Parallel task; Real - Time system; Real time scheduling; Real- time; Real-time embedded systems; Real-time synchronization; Spinlock; State of the art; Time synchronization; Scheduling
Skills Gaps in the Industry: Opinions of Embedded Software Practitioners,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111695458&doi=10.1145%2f3463340&partnerID=40&md5=d79909e37e7dd090e222c2b16de0a18f,"Many practitioners in the software-intensive embedded industry often face difficulties after beginning their careers due to misalignment of the skills learned at the university with what is required in the workplace. Companies spend crucial resources to train personnel whose academic backgrounds are not only based on ""computing disciplines""but also on non-computing ones. Analyzing the gap between the software industry and academia is important for three reasons: (1) for employers, hiring properly trained practitioners allows them to spend less time in training them while incorporating them more efficiently into the workforce; (2) for practitioners, knowing the most important skillset is helpful to increase their chance of employability; and (3) for academia, understanding the necessary skillset is critical to making curriculum changes. To achieve these objectives, we conducted a survey that yielded responses from 659 software professionals working worldwide in different roles. In this study, we only included the responses of 393 embedded software practitioners whose undergraduate degree was completed in Turkey, working in 10 countries. This article sheds light on the most important skills in the embedded software industry by presenting various cross-factor analyses. Understanding the coverage of these skills in the curriculum (mostly in Turkish universities) helps bridge the gaps, which can and should be achieved through more Industry Academia Collaborations (IACs).  © 2021 ACM.",Embedded software industry; Hard skills; Industry academia collaboration; Practitioner survey; Soft skills; Software engineering education,Curricula; Embedded software; Computing disciplines; Curriculum changes; Skills gaps; Software industry; Software practitioners; Turkishs; Undergraduate degrees; Firmware
A Distributed Real-time Scheduling System for Industrial Wireless Networks,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111619231&doi=10.1145%2f3464429&partnerID=40&md5=e7a3c913368d717e47087c808c640cad,"The concept of Industry 4.0 introduces the unification of industrial Internet-of-Things (IoT), cyber physical systems, and data-driven business modeling to improve production efficiency of the factories. To ensure high production efficiency, Industry 4.0 requires industrial IoT to be adaptable, scalable, real-time, and reliable. Recent successful industrial wireless standards such as WirelessHART appeared as a feasible approach for such industrial IoT. For reliable and real-time communication in highly unreliable environments, they adopt a high degree of redundancy. While a high degree of redundancy is crucial to real-time control, it causes a huge waste of energy, bandwidth, and time under a centralized approach and are therefore less suitable for scalability and handling network dynamics. To address these challenges, we propose DistributedHART - a distributed real-time scheduling system for WirelessHART networks. The essence of our approach is to adopt local (node-level) scheduling through a time window allocation among the nodes that allows each node to schedule its transmissions using a real-time scheduling policy locally and online. DistributedHART obviates the need of creating and disseminating a central global schedule in our approach, thereby significantly reducing resource usage and enhancing the scalability. To our knowledge, it is the first distributed real-time multi-channel scheduler for WirelessHART. We have implemented DistributedHART and experimented on a 130-node testbed. Our testbed experiments as well as simulations show at least 85% less energy consumption in DistributedHART compared to existing centralized approach while ensuring similar schedulability.  © 2021 ACM.",Distributed scheduling; Real-time networking; WirelessHART,Embedded systems; Energy utilization; Industry 4.0; Real time control; Redundancy; Scalability; Scheduling; Testbeds; Wireless networks; Business modeling; Centralized approaches; Industrial wireless; Industrial wireless network; Production efficiency; Real - time scheduling; Real-time communication; Time-window allocation; Industrial internet of things (IIoT)
The Predictable Execution Model in Practice: Compiling Real Applications for COTS Hardware,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111709771&doi=10.1145%2f3465370&partnerID=40&md5=eacb74106284a0c9c95db5eb70fcea45,"Adoption of multi- and many-core processors in real-time systems has so far been slowed down, if not totally barred, due do the difficulty in providing analytical real-time guarantees on worst-case execution times. The Predictable Execution Model (PREM) has been proposed to solve this problem, but its practical support requires significant code refactoring, a task better suited for a compilation tool chain than human programmers. Implementing a PREM compiler presents significant challenges to conform to PREM requirements, such as guaranteed upper bounds on memory footprint and the generation of efficient schedulable non-preemptive regions. This article presents a comprehensive description on how a PREM compiler can be implemented, based on several years of experience from the community. We provide accumulated insights on how to best balance conformance to real-time requirements and performance and present novel techniques that extend the applicability from simple benchmark suites to real-world applications. We show that code transformed by the PREM compiler enables timing predictable execution on modern commercial off-the-shelf hardware, providing novel insights on how PREM can protect 99.4% of memory accesses on random replacement policy caches at only 16% performance loss on benchmarks from the PolyBench benchmark suite. Finally, we show that the requirements imposed on the programming model are well-aligned with current coding guidelines for timing critical software, promoting easy adoption.  © 2021 ACM.",Commercial off-the-shelf systems; Freedom from interference; Memory interference; Multi-core systems; Predictable execution models,Benchmarking; Cache memory; Codes (symbols); Commercial off-the-shelf; Interactive computer systems; Program compilers; Code re-factoring; Commercial off-the-shelf hardwares; Many-core processors; Programming models; Random replacements; Real time guarantees; Real time requirement; Worst-case execution time; Real time systems
SLAQA: Quality-level Aware Scheduling of Task Graphs on Heterogeneous Distributed Systems,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111638891&doi=10.1145%2f3462776&partnerID=40&md5=2a2c0883c94c570a70126e188e294583,"Continuous demands for higher performance and reliability within stringent resource budgets is driving a shift from homogeneous to heterogeneous processing platforms for the implementation of today's cyber-physical systems (CPSs). These CPSs are typically represented as Directed-acyclic Task Graph (DTG) due to the complex interactions between their functional components that are often distributed in nature. In this article, we consider the problem of scheduling a real-time application modelled as a single DTG, where tasks may have multiple implementations designated as quality-levels, with higher quality-levels producing more accurate results and contributing to higher rewards/Quality-of-Service for the system. First, we introduce an optimal solution using Integer Linear Programming (ILP) for a DTG with multiple quality-levels, to be executed on a heterogeneous distributed platform. However, this ILP-based optimal solution exhibits high computational complexity and does not scale for moderately large problem sizes. Hence, we propose two low-overhead heuristic algorithms called Global Slack Aware Quality-level Allocator (G-SLAQA) and Total Slack Aware Quality-level Allocator (T-SLAQA), which are able to produce satisfactorily efficient as well as fast solutions within a reasonable time. G-SLAQA, the baseline heuristic, is greedier and faster than its counter-part T-SLAQA, whose performance is at least as efficient as G-SLAQA. The efficiency of all the proposed schemes have been extensively evaluated through simulation-based experiments using benchmark and randomly generated DTGs. Through the case study of a real-world automotive traction controller, we generate schedules using our proposed schemes to demonstrate their practical applicability.  © 2021 ACM.",Directed-acyclic task graphs; Distributed systems; Heterogeneous platform; Integer linear programming; Optimal scheduling; Quality-of-service,Budget control; Distributed computer systems; Distributed database systems; Embedded systems; Heuristic algorithms; Integer programming; Optimal systems; Scheduling; Traction control; Cyber physical systems (CPSs); Directed a-cyclic task graphs; Distributed platforms; Functional components; Heterogeneous distributed systems; Heterogeneous processing platforms; Integer Linear Programming; Performance and reliabilities; Directed graphs
SEAMS: Self-Optimizing Runtime Manager for Approximate Memory Hierarchies,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111650441&doi=10.1145%2f3466875&partnerID=40&md5=6a243d10bfaaa7a7481d92de906f21e0,"Memory approximation techniques are commonly limited in scope, targeting individual levels of the memory hierarchy. Existing approximation techniques for a full memory hierarchy determine optimal configurations at design-time provided a goal and application. Such policies are rigid: they cannot adapt to unknown workloads and must be redesigned for different memory configurations and technologies. We propose SEAMS: the first self-optimizing runtime manager for coordinating configurable approximation knobs across all levels of the memory hierarchy. SEAMS continuously updates and optimizes its approximation management policy throughout runtime for diverse workloads. SEAMS optimizes the approximate memory configuration to minimize energy consumption without compromising the quality threshold specified by application developers. SEAMS can (1) learn a policy at runtime to manage variable application quality of service (QoS) constraints, (2) automatically optimize for a target metric within those constraints, and (3) coordinate runtime decisions for interdependent knobs and subsystems. We demonstrate SEAMS' ability to efficiently provide functions (1)-(3) on a RISC-V Linux platform with approximate memory segments in the on-chip cache and main memory. We demonstrate SEAMS' ability to save up to 37% energy in the memory subsystem without any design-time overhead. We show SEAMS' ability to reduce QoS violations by 75% with < 5% additional energy.  © 2021 Owner/Author.",Approximate computing; Memory hierarchy; Model-free control; RISC-V,Computer operating systems; Energy utilization; Managers; Memory architecture; Quality of service; Application developers; Approximation techniques; Individual levels; Management policy; Memory configuration; Memory hierarchy; Memory subsystems; Variable applications; Cache memory
Determinism,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111624902&doi=10.1145%2f3453652&partnerID=40&md5=14d98bfd6534cd06ec11e3d5d16b456e,"This article is about deterministic models, what they are, why they are useful, and what their limitations are. First, the article emphasizes that determinism is a property of models, not of physical systems. Whether a model is deterministic or not depends on how one defines the inputs and behavior of the model. To define behavior, one has to define an observer. The article compares and contrasts two classes of ways to define an observer, one based on the notion of ""state""and another that more flexibly defines the observables. The notion of ""state""is shown to be problematic and lead to nondeterminism that is avoided when the observables are defined differently. The article examines determinism in models of the physical world. In what may surprise many readers, it shows that Newtonian physics admits nondeterminism and that quantum physics may be interpreted as a deterministic model. Moreover, it shows that both relativity and quantum physics undermine the notion of ""state""and therefore require more flexible ways of defining observables. Finally, the article reviews results showing that sufficiently rich sets of deterministic models are incomplete. Specifically, nondeterminism is inescapable in any system of models rich enough to encompass Newton's laws.  © 2021 Owner/Author.",Concurrency; Determinism; Distributed computing,Software engineering; Deterministic modeling; Deterministic models; Newton's Laws; Newtonian physics; Non-determinism; Physical systems; Physical world; Quantum physics; Quantum theory
Symbolic Loop Compilation for Tightly Coupled Processor Arrays,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111618060&doi=10.1145%2f3466897&partnerID=40&md5=0765077f9385e9fb9d9d60eaf0953cc2,"Tightly Coupled Processor Arrays (TCPAs), a class of massively parallel loop accelerators, allow applications to offload computationally expensive loops for improved performance and energy efficiency. To achieve these two goals, executing a loop on a TCPA requires an efficient generation of specific programs as well as other configuration data for each distinct combination of loop bounds and number of available processing elements (PEs). Since both these parameters are generally unknown at compile time - the number of available PEs due to dynamic resource management, and the loop bounds, because they depend on the problem size - both the programs and configuration data must be generated at runtime. However, pure just-in-time compilation is impractical, because mapping a loop program onto a TCPA entails solving multiple NP-complete problems. As a solution, this article proposes a unique mixed static/dynamic approach called symbolic loop compilation. It is shown that at compile time, the NP-complete problems (modulo scheduling, register allocation, and routing) can still be solved to optimality in a symbolic way resulting in a so-called symbolic configuration, a space-efficient intermediate representation parameterized in the loop bounds and number of PEs. This phase is called symbolic mapping. At runtime, for each requested accelerated execution of a loop program with given loop bounds and known number of available PEs, a concrete configuration, including PE programs and configuration data for all other components, is generated from the symbolic configuration according to these parameter values. This phase is called instantiation. We describe both phases in detail and show that instantiation runs in polynomial time with its most complex step, program instantiation, not directly depending on the number of PEs and thus scaling to arbitrary sizes of TCPAs. To validate the efficiency of this mixed static/dynamic compilation approach, we apply symbolic loop compilation to a set of real-world loop programs from several domains, measuring both compilation time and space requirements. Our experiments confirm that a symbolic configuration is a space-efficient representation suited for systems with little memory - in many cases, a symbolic configuration is smaller than even a single concrete configuration instantiated from it - and that the times for the runtime phase of program instantiation and configuration loading are negligible and moreover independent of the size of the available processor array. To give an example, instantiating a configuration for a matrix-matrix multiplication benchmark takes equally long for 4× 4 and 32× 32 PEs.  © 2021 ACM.",Compilation; Polyhedral model; Systolic arrays,Concretes; Energy efficiency; Information management; Mapping; NP-hard; Polynomial approximation; Program processors; Structured programming; Dynamic resource management; Intermediate representations; Just-in-time compilation; Massively parallels; Matrix matrix multiplications; Processing elements; Register allocation; Space requirements; Matrix algebra
TAMA: Turn-aware Mapping and Architecture - A Power-efficient Network-on-Chip Approach,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111650077&doi=10.1145%2f3462700&partnerID=40&md5=73bb78b23cb93ca0eb478234bb201708,"Nowadays, static power consumption in chip multiprocessor (CMP) is the most crucial concern of chip designers. Power-gating is an effective approach to mitigate static power consumption particularly in low utilization. Network-on-Chip (NoC) as the backbone of multi- and many-core chips has no exception. Previous state-of-the-art techniques in power-gating desire to decrease static power consumption alongside the lack of diminution in performance of NoC. However, maintaining the performance and utilization of the power-gating approach has not yet been addressed very well. In this article, we propose TAMA (Turn-Aware Mapping & Architecture) as an effective method to boost the performance of the TooT method that was only powering on a router during turning pass or packet injection. In other words, in the TooT method, straight and eject packets pass the router via a bypass route without powering on the router. By employing meta-heuristic approaches (Genetic and Ant Colony algorithms), we develop a specific application mapping that attempts to decrease the number of turns through interconnection networks. Accordingly, the average latency of packet transmission decreases due to fewer turns. Also, by powering on turn routers in advance with lightweight hardware, the latency of sending packets diminishes. The experimental results demonstrate that our proposed approach, i.e., TAMA achieves more than 13% reduction in packet latency of NoC in comparison with TooT. Besides the packet latency, the power consumption of TAMA is reduced by about 87% compared to the traditional approach.  © 2021 ACM.",Application mapping; Energy efficiency; Network-on-Chip; Power-gating,Ant colony optimization; Computer architecture; Electric power utilization; Genetic algorithms; Heuristic algorithms; Heuristic methods; Interconnection networks (circuit switching); Mapping; Network architecture; Routers; Servers; Ant colony algorithms; Effective approaches; Meta-heuristic approach; Network-on-chip(NoC); Packet transmissions; State-of-the-art techniques; Static power consumption; Traditional approaches; Network-on-chip
Reliability-aware Scheduling and Routing for Messages in Time-sensitive Networking,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111687698&doi=10.1145%2f3458768&partnerID=40&md5=b7ee86563159c14831f0f68975fe0c3c,"Time-sensitive Networking (TSN) on Ethernet is a promising communication technology in the automotive and industrial automation industries due to its real-time and high-bandwidth communication capabilities. Time-triggered scheduling and static routing are often adopted in these areas due to high requirements on predictability for safety-critical applications. Deadline-constrained routing and scheduling in TSN have been studied extensively in past research. However, scheduling and routing with reliability requirements in the context of transient faults are not yet studied. In this work, we propose an Satisfiability Modulo Theory-based technique to perform scheduling and routing that takes both reliability constraints and end-to-end deadline constraints into consideration. Heuristics have been applied to improve the scalability of the solution. Extensive experiments have been conducted to demonstrate the efficiency of our proposed technique.  © 2021 ACM.",Ethernet TSN; Safety-critical systems,Reliability theory; Safety engineering; Communication technologies; High bandwidth communication; Reliability constraints; Reliability requirements; Safety critical applications; Satisfiability modulo Theories; Scheduling and routing; Time-triggered scheduling; Scheduling
Integrated Hardware Garbage Collection,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111660325&doi=10.1145%2f3450147&partnerID=40&md5=e4bb988a8993211a42c4a54c68546458,"Garbage collected programming languages, such as Python and C#, have accelerated software development. These modern languages increase productivity and software reliability as they provide high-level data representation and control structures. Modern languages are widely used in software development for mobile, desktop, and server devices, but their adoption is limited in real-time embedded systems. There is clear interest in supporting modern languages in embedded devices as emerging markets, like the Internet of Things, demand ever smarter and more reliable products. Multiple commercial and open-source projects, such as Zerynth and MicroPython, are attempting to provide support. But these projects rely on software garbage collectors that impose high overheads and introduce unpredictable pauses, preventing their use in many embedded applications. These limitations arise from the unsuitability of conventional processors for performing efficient, predictable garbage collection. We propose the Integrated Hardware Garbage Collector (IHGC); a garbage collector tightly coupled with the processor that runs continuously in the background. Further, we introduce a static analysis technique to guarantee that real-time programs are never paused by the collector. Our design allocates a memory cycle to the collector when the processor is not using the memory. The IHGC achieves this by careful division of collection work into single-memory-access steps that are interleaved with the processor's memory accesses. As a result, our collector eliminates run-time overheads and enables real-time program analysis. The principles behind the IHGC can be used in conjunction with existing architectures. For example, we simulated the IHGC alongside the ARMv6-M architecture. Compared to a conventional processor, our experiments indicate that the IHGC offers 1.5-7 times better performance for programs that rely on garbage collection. The IHGC delivers the benefits of garbage-collected languages with real-time performance but without the complexity and overheads inherent in software collectors.  © 2021 ACM.",Garbage collection; Memory; Memory management; Real-time; Reliability; Static analysis,Application programs; Embedded systems; Memory architecture; Open source software; Real time systems; Refuse collection; Software design; Software reliability; Static analysis; Storage allocation (computer); Analysis techniques; Data representations; Embedded application; Existing architectures; Garbage collection; Open source projects; Real time performance; Real-time embedded systems; High level languages
A Composable Monitoring System for Heterogeneous Embedded Platforms,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111685694&doi=10.1145%2f3461647&partnerID=40&md5=242f195b60ebd0280559a30fb74cc01c,"Advanced computations on embedded devices are nowadays a must in any application field. Often, to cope with such a need, embedded systems designers leverage on complex heterogeneous reconfigurable platforms that offer high performance, thanks to the possibility of specializing/customizing some computing elements on board, and are usually flexible enough to be optimized at runtime. In this context, monitoring the system has gained increasing interest. Ideally, monitoring systems should be non-intrusive, serve several purposes, and provide aggregated information about the behavior of the different system components. However, current literature is not close to such ideality: For example, existing monitoring systems lack in being applicable to modern heterogeneous platforms. This work presents a hardware monitoring system that is intended to be minimally invasive on system performance and resources, composable, and capable of providing to the user homogeneous observability and transparent access to the different components of a heterogeneous computing platform, so system metrics can be easily computed from the aggregation of the collected information. Building on a previous work, this article is primarily focused on the extension of an existing hardware monitoring system to cover also specialized coprocessing units, and the assessment is done on a Xilinx FPGA-based System on Programmable Chip. Different explorations are presented to explain the level of customizability of the proposed hardware monitoring system, the tradeoffs available to the user, and the benefits with respect to standard de facto monitoring support made available by the targeted FPGA vendor.  © 2021 ACM.",FPGA; Heterogeneous system; HW monitoring; HW reconfiguration; Monitoring system,Embedded systems; Field programmable gate arrays (FPGA); Reconfigurable hardware; Application fields; Composable; Embedded device; Embedded platforms; Embedded-system; Heterogeneous systems; HW monitoring; HW reconfiguration; Monitoring system; System designers; Monitoring
Improving Power of DSP and CNN Hardware Accelerators Using Approximate Floating-point Multipliers,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111688446&doi=10.1145%2f3448980&partnerID=40&md5=38f8adeb5a78a1f37dc8a4956e9f9e3d,"Approximate computing has emerged as a promising design alternative for delivering power-efficient systems and circuits by exploiting the inherent error resiliency of numerous applications. The current article aims to tackle the increased hardware cost of floating-point multiplication units, which prohibits their usage in embedded computing. We introduce AFMU (Approximate Floating-point MUltiplier), an area/power-efficient family of multipliers, which apply two approximation techniques in the resource-hungry mantissa multiplication and can be seamlessly extended to support dynamic configuration of the approximation levels via gating signals. AFMU offers large accuracy configuration margins, provides negligible logic overhead for dynamic configuration, and detects unexpected results that may arise due to the approximations. Our evaluation shows that AFMU delivers energy gains in the range 3.6%-53.5% for half-precision and 37.2%-82.4% for single-precision, in exchange for mean relative error around 0.05%-3.33% and 0.01%-2.20%, respectively. In comparison with state-of-the-art multipliers, AFMU exhibits up to 4-6× smaller error on average while delivering more energy-efficient computing. The evaluation in image processing shows that AFMU provides sufficient quality of service, i.e., more than 50db PSNR and near 1 SSIM values, and up to 57.4% power reduction. When used in floating-point CNNs, the accuracy loss is small (or zero), i.e., up to 5.4% for MNIST and CIFAR-10, in exchange for up to 63.8% power gain.  © 2021 ACM.",Approximate computing; Approximate multiplier; Arithmetic circuits; Error analysis; Floating-point; Logic approximations; Low-power; Pareto analysis,Computation theory; Computer circuits; Digital signal processing; Energy efficiency; Errors; Image processing; Quality control; Quality of service; Approximation techniques; Design alternatives; Dynamic configuration; Energy efficient computing; Floating point multiplication; Hardware accelerators; Mean relative error; Power efficient systems; Digital arithmetic
SystemC Implementation of Stochastic Petri Nets for Simulation and Parameterization of Biological Networks,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108024028&doi=10.1145%2f3427091&partnerID=40&md5=b488ca81b7b9b4d37e974fd58c56b10e,"Model development and simulation of biological networks is recognized as a key task in Systems Biology. Integrated with in vitro and in vivo experimental data, network simulation allows for the discovery of the dynamics that regulate biological systems. Stochastic Petri Nets (SPNs) have become a widespread and reference formalism to model metabolic networks thanks to their natural expressiveness to represent metabolites, reactions, molecule interactions, and simulation randomness due to system fluctuations and environmental noise. In the literature, starting from the network model and the complete set of system parameters, there exist frameworks that allow for dynamic system simulation. Nevertheless, they do not allow for automatic model parameterization, which is a crucial task to identify, in silico, the network configurations that lead the model to satisfy specific temporal properties. To cover such a gap, this work first presents a framework to implement SPN models into SystemC code. Then, it shows how the framework allows for automatic parameterization of the networks. The user formally defines the network properties to be observed and the framework automatically extrapolates, through Assertion-based Verification (ABV), the parameter configurations that satisfy such properties. We present the results obtained by applying the proposed framework to model the complex metabolic network of the purine metabolism. We show how the automatic extrapolation of the system parameters allowed us to simulate the model under different conditions, which led to the understanding of behavioral differences in the regulation of the entire purine network. We also show the scalability of the approach through the modeling and simulation of four biological networks, each one with different structural characteristics. © 2021 ACM.",autoimmunity; electronic design automation; metabolic networks; Stochastic Petri Net; T cells,Extrapolation; Metabolism; Metabolites; Parameterization; Petri nets; Stochastic models; Stochastic systems; Assertion-based verification; Complex metabolic networks; Dynamic system simulation; Molecule interactions; Network configuration; Stochastic Petri Nets; Structural characteristics; Systemc implementations; Bioinformatics
Test Generation for Hardware Trojan Detection Using Correlation Analysis and Genetic Algorithm,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108030035&doi=10.1145%2f3446837&partnerID=40&md5=ae4ae8178aafdc2c7883b8c5bd84bcec,"Hardware Trojan (HT) is a major threat to the security of integrated circuits (ICs). Among various HT detection approaches, side channel analysis (SCA)-based methods have been extensively studied. SCA-based methods try to detect HTs by comparing side channel signatures from circuits under test with those from trusted golden references. The pre-condition for SCA-based HT detection to work is that the testers can collect extra signatures/anomalies introduced by activated HTs. Thus, activation of HTs and amplification of the differences between circuits under test and golden references are the keys to SCA-based HT detection methods. Test vectors are of great importance to the activation of HTs, but existing test generation methods have two major limitations. First, the number of test vectors required to trigger HTs is quite large. Second, the HT circuit's activities are marginal compared with the whole circuit's activities. In this article, we propose an optimized test generation methodology to assist SCA-based HT detection. Considering the HTs' inherent surreptitious nature, inactive nodes with low transition probability are more likely to be selected as HT trigger nodes. Therefore, the correlations between circuit inputs and inactive nodes are first exploited to activate HTs. Then a test reordering process based on the genetic algorithm (GA) is implemented to increase the proportion of the HT circuit's activities to the whole circuit's activities. Experiments on 10 selected ISCAS benchmarks, wb_conmax benchmark, and b17 benchmark demonstrate that the number of test vectors required to trigger HTs reduces 28.8% on average compared with the result of MERO and MERS methods. After the test vector reordering process, the proportion of the HT circuit's activities to the whole circuit's activities is improved by 95% on average, compared with the result of MERS method. © 2021 ACM.",correlation analysis; Hardware trojan detection; side channel analysis; test generation; test reordering,Chemical activation; Genetic algorithms; Hardware security; Integrated circuits; Malware; Network security; Vectors; Correlation analysis; Detection approach; Detection methods; Hardware Trojan detection; Integrated circuits (ICs); Side-channel analysis; Test generation methodology; Test vector reordering; Testing
Design Space Exploration for Secure IoT Devices and Cyber-Physical Systems,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108028475&doi=10.1145%2f3430372&partnerID=40&md5=002caaa3f3b08917023d1a17a3c9ae3e,"With the advent of the Internet of Things (IoT) and Cyber-Physical Systems (CPS), embedded devices have been gaining importance in our daily lives, as well as industrial processes. Independent of their usage, be it within an IoT system or a CPS, embedded devices are always an attractive target for security attacks, mainly due to their continuous network availability and the importance of the data they handle. Thus, the design of such systems requires a thorough consideration of the various security constraints they are liable to. Introducing these security constraints, next to other requirements, such as power consumption, and performance increases the number of design choices a system designer must consider. As the various constraints are often conflicting with each other, designers face the complex task of balancing them. System designers facilitate Design Space Exploration (DSE) tools to support a system designer in this job. However, available DSE tools only offer a limited way of considering security constraints during the design process. In this article, we introduce a novel DSE framework, which allows the consideration of security constraints, in the form of attack scenarios, and attack mitigations in the form of security tasks. Based on the descriptions of the system's functionality and architecture, possible attacks, and known mitigation techniques, the framework finds the optimal design for a secure IoT device or CPS. Our framework's functionality and its benefits are shown based on the design of a secure sensor system. © 2021 ACM.",Datasets; gaze detection; neural networks; text tagging,Availability; Cyber Physical System; Design; Embedded systems; Systems analysis; Cyber-physical systems (CPS); Design space exploration; Industrial processs; Internet of thing (IOT); Mitigation techniques; Network availability; Security attacks; Security constraint; Internet of things
Time Measurement and Control Blocks for Bare-Metal C++ Applications,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108030152&doi=10.1145%2f3434401&partnerID=40&md5=2b431485116f98b37b5b2b84329a803f,"Precisely timed execution of resource constrained bare-metal applications is difficult, because the embedded software developer usually has to implement and check the timeliness of the executed application through manual interaction with timers or counters. In the scope of this work, we propose a combined timing specification and concept for time annotation and control blocks in C++. Our proposed blocks can be used to measure and profile software block execution time. Furthermore, it can be used to control and enforce the software time behavior at runtime. After the application of these time blocks, a trace-based verification against the block-based timing specification can be performed to obtain evidence on the correct implementation and usage of the time blocks on the target platform. We have implemented our time block concept in a C++ library and tested it on an ARM Cortex A9 bare-metal platform. The combined usage of timing specification and our time block library has been successfully evaluated on a critical flight-control software for a multi-rotor system. © 2021 ACM.",real-Time bare-metal application; time control blocks; Time measurement; timing annotations,Application programs; Firmware; Specifications; Control blocks; Flight control software; Manual interaction; Measurement and control; Multi-rotor system; Software developer; Timed execution; Timing specifications; C++ (programming language)
Interactive Programmatic Modeling,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108026198&doi=10.1145%2f3431387&partnerID=40&md5=48d621dcba50088cf014aeb2c062f307,"Modeling and computational analyses are fundamental activities within science and engineering. Analysis activities can take various forms, such as simulation of executable models, formal verification of model properties, or inference of hidden model variables. Traditionally, tools for modeling and analysis have similar workflows: (i) a user designs a textual or graphical model or the model is inferred from data, (ii) a tool performs computational analyses on the model, and (iii) a visualization tool displays the resulting data. This article identifies three inherent problems with the traditional approach: The recomputation problem, the variable inspection problem, and the model expressiveness problem. As a solution, we propose a conceptual framework called Interactive Programmatic Modeling. We formalize the interface of the framework and illustrate how it can be used in two different domains: equation-based modeling and probabilistic programming. © 2021 ACM.",cyber-physical systems; Modeling languages; probabilistic programming,Software engineering; Computational analysis; Conceptual frameworks; Equation-based modeling; Modeling expressiveness; Probabilistic programming; Science and engineering; Traditional approaches; Visualization tools; Data visualization
LPWAN in the TV White Spaces,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108030337&doi=10.1145%2f3447877&partnerID=40&md5=3ce0c14d22b1c1990e84a1d619fb58c2,"Low-Power Wide-Area Network (LPWAN) is an enabling Internet-of-Things technology that supports long-range, low-power, and low-cost connectivity to numerous devices. To avoid the crowd in the limited ISM band (where most LPWANs operate) and cost of licensed band, the recently proposed Sensor Network over White Spaces (SNOW) is a promising LPWAN platform that operates over the TV white spaces. As it is a very recent technology and is still in its infancy, the current SNOW implementation uses the Universal Software Radio Peripheral devices as LPWAN nodes, which has high costs (≈$750 USD per device) and large form-factors, hindering its applicability in practical deployment. In this article, we implement SNOW using low-cost, low form-factor, low-power, and widely available commercial off-The-shelf (COTS) devices to enable its practical and large-scale deployment. Our choice of the COTS device (TI CC13x0: CC1310 or CC1350) consequently brings down the cost and form-factor of a SNOW node by 25× and 10×, respectively. Such implementation of SNOW on the CC13x0 devices, however, faces a number of challenges to enable link reliability and communication range. Our implementation addresses these challenges by handling peak-To-Average power ratio problem, channel state information estimation, carrier frequency offset estimation, and near-far power problem. Our deployment in the city of Detroit, Michigan, demonstrates that CC13x0-based SNOW can achieve uplink and downlink throughputs of 11.2 and 4.8 kbps per node, respectively, over a distance of 1 km. Also, the overall throughput in the uplink increases linearly with the increase in the number of SNOW nodes. © 2021 ACM.",LPWAN; OFDM; SNOW; white spaces,Approximation theory; Channel state information; Commercial off-the-shelf; Costs; Frequency allocation; Frequency estimation; Sensor networks; Snow; Software radio; Wide area networks; Carrier frequency offset estimation; Commercial off the shelf devices; Communication range; Internet of things technologies; Large-scale deployment; Link reliability; Peak to average power ratio; Peripheral devices; Low power electronics
Toward a Lingua Franca for Deterministic Concurrent Systems,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108029928&doi=10.1145%2f3448128&partnerID=40&md5=b308fe801b09f7f66c58278127d3c248,"Many programming languages and programming frameworks focus on parallel and distributed computing. Several frameworks are based on actors, which provide a more disciplined model for concurrency than threads. The interactions between actors, however, if not constrained, admit nondeterminism. As a consequence, actor programs may exhibit unintended behaviors and are less amenable to rigorous testing. We show that nondeterminism can be handled in a number of ways, surveying dataflow dialects, process networks, synchronous-reactive models, and discrete-event models. These existing approaches, however, tend to require centralized control, pose challenges to modular system design, or introduce a single point of failure. We describe ""reactors,""a new coordination model that combines ideas from several of these approaches to enable determinism while preserving much of the style of actors. Reactors promote modularity and allow for distributed execution. By using a logical model of time that can be associated with physical time, reactors also provide control over timing. Reactors also expose parallelism that can be exploited on multicore machines and in distributed configurations without compromising determinacy. © 2021 Owner/Author.",concurrency; coordination language; determinism; Polyglot,Data flow analysis; Centralized control; Discrete event models; Distributed configuration; Modular system design; Multi-core machines; Parallel and distributed computing; Programming framework; Unintended behavior; Discrete event simulation
Introduction to the Special Issue on Specification and Design Languages (FDL 2019),2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108097245&doi=10.1145%2f3458748&partnerID=40&md5=5f09a3bbde5166a9a22caa30259628e1,[No abstract available],,
Efficient External Sorting for Memory-Constrained Embedded Devices with Flash Memory,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108025851&doi=10.1145%2f3446976&partnerID=40&md5=41560ec163f3db4088f6f1f028daa9b3,"Embedded devices are ubiquitous in areas of industrial and environmental monitoring, health and safety, and consumer appliances. A common use case is data collection, processing, and performing actions based on data analysis. Although many Internet of Things (IoT) applications use the embedded device simply for data collection, there are benefits to having more data processing done closer to data collection to reduce network transmissions and power usage and provide faster response. This work implements and evaluates algorithms for sorting data on embedded devices with specific focus on the smallest memory devices. In devices with less than 4 KB of available RAM, the standard external merge sort algorithm has limited application as it requires a minimum of three memory buffers and is not flash-Aware. The contribution is a memory-optimized external sorting algorithm called no output buffer sort (NOBsort) that reduces the minimum memory required for sorting, has excellent performance for sorted or near-sorted data, and sorts on external memory such as SD cards or raw flash chips. When sorting large datasets, no output buffer sort reduces I/O and execution time by between 20% to 35% compared to standard external merge sort. © 2021 ACM.",Arduino; embedded; Internet of Things; performance; Sorting,Accident prevention; Data acquisition; Flash memory; Internet of things; Large dataset; Random access storage; Sorting; Embedded device; Environmental Monitoring; Health and safety; Internet of Things (IOT); Memory optimized; Merge-sort algorithm; Network transmission; Sorting algorithm; Data reduction
Toward Object-oriented Modeling in SCCharts,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108029654&doi=10.1145%2f3453482&partnerID=40&md5=1eb54ca4fc6b2caee2aae1c2c5784e28,"Object orientation is a powerful and widely used paradigm for abstraction and structuring in programming. Many languages are designed with this principle or support different degrees of object orientation. In synchronous languages, originally developed to design embedded reactive systems, there are only few object-oriented influences. However, especially in combination with a statechart notation, the modeling process can be improved by facilitating object orientation as we argue here. At the same time the graphical representation can be used to illustrate the object-oriented design of a system. Synchronous statechart dialects, such as the SCCharts language, provide deterministic concurrency for specifying safety-critical systems. Using SCCharts as an example, we illustrate how an object-oriented modeling approach that supports inheritance can be introduced. We further present how external, i.e., host language, objects can be included in the SCCharts language. Specifically, we discuss how the recently developed concepts of scheduling directives and scheduling policies can be used to ensure the determinism of objects while retaining encapsulation. © 2021 ACM.",determinacy; inheritance; object orientation; state machine modeling; Synchronous languages,Embedded systems; Safety engineering; Scheduling; Graphical representations; Modeling process; Object orientation; Object oriented design; Object oriented model; Safety critical systems; Scheduling policies; Synchronous languages; Object oriented programming
Event-B Hybridation A Proof and Refinement-based Framework for Modelling Hybrid Systems,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108022745&doi=10.1145%2f3448270&partnerID=40&md5=9ddf99677d006460f7f8ae0d53d2232d,"Hybrid systems are complex systems where a software controller interacts with a physical environment, usually named a plant, through sensors and actuators. The specification and design of such systems usually rely on the description of both continuous and discrete behaviours. From complex embedded systems to autonomous vehicles, these systems became quite common, including in safety critical domains. However, their formal verification and validation as a whole is still a challenge. To address this challenge, this article contributes to the definition of a reusable and tool supported formal framework handling the design and verification of hybrid system models that integrate both discrete (the controller part) and continuous (the plant part) behaviours. This framework includes the development of a process for defining a class of basic theories and developing domain theories and then the use of these theories to develop a generic model and system-specific models. To realise this framework, we present a formal proof tool chain, based on the Event-B correct-by-construction method and its integrated development environment Rodin, to develop a set of theories, a generic model, proof processes, and the required properties for designing hybrid systems in Event-B. Our approach relies on hybrid automata as basic models for such systems. Discrete and continuous variables model system states and behaviours are given using discrete state changes and continuous evolution following a differential equation. The proposed approach is based on refinement and proof using the Event-B method and the Rodin toolset. Two case studies borrowed from the literature are used to illustrate our approach. An assessment of the proposed approach is provided for evaluating its extensibility, effectiveness, scalability, and usability. © 2021 ACM.",continuous and discrete models; CPS; Event-B; formal methods; hybrid systems; refinement and proof; Rodin IDE,Differential equations; Formal verification; Hybrid systems; Safety engineering; Correct-by-construction; Integrated development environment; Physical environments; Safety-critical domain; Sensors and actuators; Specification and designs; Verification of hybrid systems; Verification-and-validation; Embedded systems
Editorial: Special Issue onQuality Assessment and Management in Big Data-Part i,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109044736&doi=10.1145%2f3449052&partnerID=40&md5=78c889e121c1c0f32e8951a8e4e668f4,[No abstract available],big data; Quality assessment; quality management,
Code-size-aware Scheduling of Synchronous Dataflow Graphs on Multicore Systems,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105149047&doi=10.1145%2f3440034&partnerID=40&md5=70d72809c2887153ffe383d217227d04,"Synchronous dataflow graphs are widely used to model digital signal processing and multimedia applications. Self-timed execution is an efficient methodology for the analysis and scheduling of synchronous dataflow graphs. In this article, we propose a communication-aware self-timed execution approach to solve the problem of scheduling synchronous dataflow graphs on multicore systems with communication delays. Based on this communication-aware self-timed execution approach, four communication-aware scheduling algorithms are proposed using different allocation rules. Furthermore, a code-size-aware mapping heuristic is proposed and jointly used with a proposed scheduling algorithm to reduce the code size of SDFGs on multicore systems. The proposed scheduling algorithms are experimentally evaluated and found to perform better than existing algorithms in terms of throughput and runtime for several applications. The experiments also show that the proposed code-size-aware mapping approach can achieve significant code size reduction with limited throughput degradation in most cases. © 2021 ACM.",code size; multicore systems; scheduling; Synchronous dataflow graphs; throughput,Codes (symbols); Digital signal processing; Mapping; Scheduling; Allocation rule; Code-size reduction; Communication delays; Communication-aware; Multi-core systems; Multimedia applications; Synchronous dataflow graphs; Throughput degradation; Data flow analysis
Editorial: Reimagining ACM Transactions on Embedded Computing Systems (TECS),2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105210782&doi=10.1145%2f3450438&partnerID=40&md5=67de5bb84bbb9ee74e462a7829307172,[No abstract available],,
"Real-time, High-resolution Depth Upsampling on Embedded Accelerators",2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105123521&doi=10.1145%2f3436878&partnerID=40&md5=4fa52de171b016fd5e76531f05361862,"High-resolution, low-latency apps in computer vision are ubiquitous in today's world of mixed-reality devices. These innovations provide a platform that can leverage the improving technology of depth sensors and embedded accelerators to enable higher-resolution, lower-latency processing for 3D scenes using depth-upsampling algorithms. This research demonstrates that filter-based upsampling algorithms are feasible for mixed-reality apps using low-power hardware accelerators. The authors parallelized and evaluated a depth-upsampling algorithm on two different devices: a reconfigurable-logic FPGA embedded within a low-power SoC; and a fixed-logic embedded graphics processing unit. We demonstrate that both accelerators can meet the real-time requirements of 11 ms latency for mixed-reality apps. © 2021 ACM.",depth upsampling; FPGA; GPU; high-level synthesis; image processing; Real time; time-of-flight sensor,Computer graphics; Graphics processing unit; Program processors; Reconfigurable hardware; Signal sampling; System-on-chip; Depth sensors; Depth upsampling; High resolution; High-resolution depth; Higher resolution; Lowpower hardware; Real time requirement; Reconfigurable logic; Mixed reality
Precise Cache Profiling for Studying Radiation Effects,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105234461&doi=10.1145%2f3442339&partnerID=40&md5=b696fdef2c1ea444cc7aae102dc65c82,"Increased access to space has led to an increase in the usage of commodity processors in radiation environments. These processors are vulnerable to transient faults such as single event upsets that may cause bit-flips in processor components. Caches in particular are vulnerable due to their relatively large area, yet are often omitted from fault injection testing because many processors do not provide direct access to cache contents and they are often not fully modeled by simulators. The performance benefits of caches make disabling them undesirable, and the presence of error correcting codes is insufficient to correct for increasingly common multiple bit upsets. This work explores building a program's cache profile by collecting cache usage information at an instruction granularity via commonly available on-chip debugging interfaces. The profile provides a tighter bound than cache utilization for cache vulnerability estimates (50% for several benchmarks). This can be applied to reduce the number of fault injections required to characterize behavior by at least two-thirds for the benchmarks we examine. The profile enables future work in hardware fault injection for caches that avoids the biases of existing techniques. © 2021 ACM.",Cache faults; cache profiling; single event upset,Program debugging; Radiation hardening; Software testing; Cache vulnerability; Commodity processors; Error correcting code; Fault injection testing; Multiple bit upset; Performance benefits; Radiation environments; Single event upsets; Radiation effects
Microcontroller Fingerprinting Using Partially Erased NOR Flash Memory Cells,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105120268&doi=10.1145%2f3448271&partnerID=40&md5=5ca818ac57f9a950cf5df5fe43fbe37d,"Electronic device fingerprints, unique bit vectors extracted from device's physical properties, are used to differentiate between instances of functionally identical devices. This article introduces a new technique that extracts fingerprints from unique properties of partially erased NOR flash memory cells in modern microcontrollers. NOR flash memories integrated in modern systems-on-a-chip typically hold firmware and read-only data, but they are increasingly in-system-programmable, allowing designers to erase and program them during normal operation. The proposed technique leverages partial erase operations of flash memory segments that bring them into the state that exposes physical properties of the flash memory cells through a digital interface. These properties reflect semiconductor process variations and defects that are unique to each microcontroller or a flash memory segment within a microcontroller. The article explores threshold voltage variation in NOR flash memory cells for generating fingerprints and describes an algorithm for extracting fingerprints. The experimental evaluation utilizing a family of commercial microcontrollers demonstrates that the proposed technique is cost-effective, robust, and resilient to changes in voltage and temperature as well as to aging effects. © 2021 ACM.",fingerprinting; microcontrollers; NOR flash memory,Controllers; Cost effectiveness; Firmware; Interface states; Microcontrollers; Petroleum reservoir evaluation; Semiconductor storage; System-on-chip; Threshold voltage; Digital interfaces; Electronic device; Experimental evaluation; Flash memory cell; Normal operations; Semiconductor process; Systems-on-a chips; Threshold voltage variation; Flash memory
SIKE in 32-bit ARM Processors Based on Redundant Number System for NIST Level-II,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105159568&doi=10.1145%2f3439733&partnerID=40&md5=c72f47b0baba49a598f7937438ca3392,"We present an optimized implementation of the post-quantum Supersingular Isogeny Key Encapsulation (SIKE) for 32-bit ARMv7-A processors supporting NEON engine (i.e., SIMD instruction). Unlike previous SIKE implementations, finite field arithmetic is efficiently implemented in a redundant representation, which avoids carry propagation and pipeline stall. Furthermore, we adopted several state-of-the-art engineering techniques as well as hand-crafted assembly implementation for high performance. Optimized implementations are ported to Microsoft SIKE library written in ""a non-redundant representation""and evaluated in high-end 32-bit ARMv7-A processors, such as ARM Cortex-A5, A7, and A15. A full key-exchange execution of SIKEp503 is performed in about 109 million cycles on ARM Cortex-A15 processors (i.e., 54.5 ms @2.0 GHz), which is about 1.58× faster than previous state-of-the-art work presented in CHES'18. © 2021 ACM.",ARM; parallel computation; Post quantum cryptography; SIDH; software implementation,Numbering systems; Assembly implementation; Carry propagation; Finite field arithmetic; Optimized implementation; Redundant number system; Redundant representation; SIMD instructions; State-of-the-art engineering; ARM processors
Sense Your Power: The ECO Approach to Energy Awareness for IoT Devices,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105159555&doi=10.1145%2f3441643&partnerID=40&md5=2643feb85de51dcbb5161f03473f47b5,"Energy-constrained sensor nodes can adaptively optimize their energy consumption if a continuous measurement is provided. This is of particular importance in scenarios of high dynamics such as with energy harvesting. Still, self-measuring of power consumption at reasonable cost and complexity is unavailable as a generic system service. In this article, we present ECO, a hardware-software co-design that adds autonomous energy management capabilities to a large class of low-end IoT devices. ECO consists of a highly portable hardware shield built from inexpensive commodity components and software integrated into the RIOT operating system. RIOT supports more than 200 popular microcontrollers. Leveraging this flexibility, we assembled a variety of sensor nodes to evaluate key performance properties for different device classes. An overview and comparison with related work shows how ECO fills the gap of in situ power attribution transparently for consumers and how it improves over existing solutions. We also report about two different real-world field trials, which validate our solution for long-term production use. © 2021 ACM.",Energy harvesting; energy management; IoT operating system; power measurement,Energy harvesting; Energy utilization; Hardware-software codesign; Sensor nodes; Commodity components; Continuous measurements; Energy awareness; Energy-constrained sensor nodes; Highly-portable; Long term production; Management capabilities; Performance properties; Internet of things
UBAR: User- And Battery-aware Resource Management for Smartphones,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105181769&doi=10.1145%2f3441644&partnerID=40&md5=defdc1b3f2bc406bcbb10c0633d5037d,"Smartphone users require high Battery Cycle Life (BCL) and high Quality of Experience (QoE) during their usage. These two objectives can be conflicting based on the user preference at run-time. Finding the best trade-off between QoE and BCL requires an intelligent resource management approach that considers and learns user preference at run-time. Current approaches focus on one of these two objectives and neglect the other, limiting their efficiency in meeting users' needs. In this article, we present UBAR, User- and Battery-aware Resource management, which considers dynamic workload, user preference, and user plug-in/out pattern at run-time to provide a suitable trade-off between BCL and QoE. UBAR personalizes this trade-off by learning the user's habits and using that to satisfy QoE, while considering battery temperature and State of Charge (SOC) pattern to maximize BCL. The evaluation results show that UBAR achieves 10% to 40% improvement compared to the existing state-of-the-art approaches. © 2021 ACM.",battery cycle life; heterogeneous multi-core systems; On-chip resource management; quality of experience; user-awareness,Charging (batteries); Economic and social effects; Natural resources management; Quality of service; Resource allocation; Secondary batteries; Smartphones; Battery cycle life; Battery temperature; Battery-aware; Evaluation results; Intelligent resource management; Resource management; State of charge; State-of-the-art approach; Battery management systems
Improving Performance-Power-Programmability in Space Avionics with Edge Devices: VBN on Myriad2 SoC,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105173691&doi=10.1145%2f3440885&partnerID=40&md5=a8356c0bb21f74130fe9584b8a72be36,"The advent of powerful edge devices and AI algorithms has already revolutionized many terrestrial applications; however, for both technical and historical reasons, the space industry is still striving to adopt these key enabling technologies in new mission concepts. In this context, the current work evaluates an heterogeneous multi-core system-on-chip processor for use on-board future spacecraft to support novel, computationally demanding digital signal processors and AI functionalities. Given the importance of low power consumption in satellites, we consider the Intel Movidius Myriad2 system-on-chip and focus on SW development and performance aspects. We design a methodology and framework to accommodate efficient partitioning, mapping, parallelization, code optimization, and tuning of complex algorithms. Furthermore, we propose an avionics architecture combining this commercial off-the-shelf chip with a field programmable gate array device to facilitate, among others, interfacing with traditional space instruments via SpaceWire transcoding. We prototype our architecture in the lab targeting vision-based navigation tasks. We implement a representative computer vision pipeline to track the 6D pose of ENVISAT using megapixel images during hypothetical spacecraft proximity operations. Overall, we achieve 2.6 to 4.9 FPS with only 0.8 to 1.1 W on Myriad2, i.e., 10-fold acceleration versus modern rad-hard processors. Based on the results, we assess various benefits of utilizing Myriad2 instead of conventional field programmable gate arrays and CPUs. © 2021 ACM.",computer vision; heterogeneous multi-core SoC; mixed-criticality; satellite pose tracking; Space avionics architecture; SW development framework; visual-based navigation,Aerospace industry; Avionics; Digital signal processors; Geodetic satellites; Logic gates; Program processors; Programmable logic controllers; Signal processing; Signal receivers; Space flight; System-on-chip; Enabling technologies; Heterogeneous multi-core systems; Improving performance; Low-power consumption; Performance aspects; Proximity operations; Terrestrial application; Vision based navigation; Field programmable gate arrays (FPGA)
Cooperative Coevolution-based Design Space Exploration for Multi-mode Dataflow Mapping,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105222208&doi=10.1145%2f3440246&partnerID=40&md5=c2a42f4efe6414c122219ce637fa1394,"Some signal processing and multimedia applications can be specified by synchronous dataflow (SDF) models. The problem of SDF mapping to a given set of heterogeneous processors has been known to be NP-hard and widely studied in the design automation field. However, modern embedded applications are becoming increasingly complex with dynamic behaviors changes over time. As a significant extension to the SDF, the multi-mode dataflow (MMDF) model has been proposed to specify such an application with a finite number of behaviors (or modes) and each behavior (mode) is represented by an SDF graph. The multiprocessor mapping of an MMDF is far more challenging as the design space increases with the number of modes. Instead of using traditional genetic algorithm (GA)-based design space exploration (DSE) method that encodes the design space as a whole, this article proposes a novel cooperative co-evolutionary genetic algorithm (CCGA)-based framework to efficiently explore the design space by a new problem-specific decomposition strategy in which the solutions of node mapping for each individual mode are assigned to an individual population. Besides, a problem-specific local search operator is introduced as a supplement to the global search of CCGA for further improving the search efficiency of the whole framework. Furthermore, a fitness approximation method and a hybrid fitness evaluation strategy are applied for reducing the time consumption of fitness evaluation significantly. The experimental studies demonstrate the advantage of the proposed DSE method over the previous GA-based method. The proposed method can obtain an optimization result with 2×-3× better quality using less (1/2-1/3) optimization time. © 2021 ACM.",Cooperative co-evolutionary algorithm; heterogeneous multiprocessor; multi-mode dataflow; task mapping,Computer aided design; Data flow analysis; Health; Mapping; NP-hard; Response time (computer systems); Signal processing; Cooperative co-evolution; Decomposition strategy; Design space exploration; Heterogeneous processors; Local search operators; Multimedia applications; Multiprocessor mapping; Traditional genetic algorithms; Genetic algorithms
Reducing Energy in GPGPUs through Approximate Trivial Bypassing,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102982247&doi=10.1145%2f3429440&partnerID=40&md5=ad57c7238829eb6684bbd52749bc13de,"General-purpose computing using graphics processing units (GPGPUs) is an attractive option for acceleration of applications with massively data-parallel tasks. While performance of modern GPGPUs is increasing rapidly, the power consumption of these devices is becoming a major concern. In particular, execution units and register file are among the top three most power-hungry components in GPGPUs. In this work, we exploit trivial instructions to reduce power consumption in GPGPUs. Trivial instructions are those instructions that do not need computations, i.e., multiplication by one. We found that, during the course of a program's execution, a GPGPU executes many trivial instructions. Execution of these instructions wastes power unnecessarily. In this work, we propose trivial bypassing which skips execution of trivial instructions and avoids unnecessary allocation of resources for trivial instructions. By power gating execution units and skipping trivial computing, trivial bypassing reduces both static and dynamic power. Also, trivial bypassing reduces dynamic energy of register file by avoiding access to register file for source and/or destination operands of trivial instructions. While trivial bypassing reduces energy of GPGPUs, it has detrimental impact on performance as a power-gated execution unit requires several cycles to resume its normal operation. Conventional warp schedulers are oblivious to the status of execution units. We propose a new warp scheduler that prioritizes warps based on availability of execution units. We also propose a set of new power management techniques to reduce performance penalty of power gating, further. To increase energy saving of trivial bypassing, we also propose approximating operands of instructions. We offer a set of new techniques to approximate both integer and floating-point instructions and increase the pool of trivial instructions. Our evaluations using a diverse set of benchmarks reveal that our proposed techniques are able to reduce energy of execution units by 11.2% and dynamic energy of register file by 12.2% with minimal performance and quality degradation. © 2021 ACM.",approximate computing; energy; GPGPUs; Trivial computing,Benchmarking; Computer graphics; Digital arithmetic; Electric power utilization; Energy conservation; Program processors; Scheduling; Execution units; Floating point instruction; General-purpose computing; Normal operations; Performance penalties; Power management techniques; Quality degradation; Trivial instructions; Graphics processing unit
Probabilistic Estimation of Threat Intrusion in Embedded Systems for Runtime Detection,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102980516&doi=10.1145%2f3432590&partnerID=40&md5=037a977b7caf31a14463130f9ba07db8,"With billions of networked connected embedded systems, the security historically provided by the isolation of embedded systems is no longer sufficient. Millions of new malware are created every month and zero-day attacks are becoming an increasing concern. Therefore, proactive security measures are no longer enough to provide protection to embedded systems. Instead, reactive approaches that detect attacks that can circumvent the proactive defenses and react upon them are needed. Anomaly-based detection is a common reactive approach employed to detect malware by monitoring anomalous deviations in the system execution. Timing-based anomaly detection detects malware by monitoring the system's internal timing, which offers unique protection against mimicry malware compared to sequence-based anomaly detection. However, previous timing-based anomaly detection methods focus on each operation independently at the granularity of tasks, function calls, system calls, or basic blocks. These approaches neither consider the entire software execution path nor provide a quantitative estimate of the presence of malware. This article presents a novel model for specifying the normal timing for execution paths in software applications using cumulative distribution functions of timing data in sliding execution windows. A probabilistic formulation is used to estimate the presence of malware for individual operations and sequences of operations within the paths. Operation and path-based thresholds are determined during the training process to minimize false positives. Finally, the article presents an optimization method to assist system developers in selecting which operations to monitor based on different optimization goals and constraints. Experimental results with a smart connected pacemaker, an unmanned aerial vehicle, and seven sophisticated mimicry malware implemented at different levels demonstrate the effectiveness of the proposed approach. © 2021 ACM.",anomaly detection; Embedded system security; medical device security; software security; timing-based detection,Anomaly detection; Antennas; Application programs; Distribution functions; Embedded systems; Intrusion detection; Anomaly based detection; Anomaly detection methods; Cumulative distribution function; Probabilistic estimation; Probabilistic formulation; Quantitative estimates; Sequences of operations; Software applications; Malware
ForSyDe-Atom: Taming Complexity in Cyber Physical System Design with Layers,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102980598&doi=10.1145%2f3424667&partnerID=40&md5=425bbf5883b34e8401ff3a95cbf05709,"We present ForSyDe-Atom, a formal framework intended as an entry point for disciplined design of complex cyber-physical systems. This framework provides a set of rules for combining several domain-specific languages as structured, enclosing layers to orthogonalize the many aspects of system behavior, yet study their interaction in tandem. We define four layers: One for capturing timed interactions in heterogeneous systems, one for structured parallelism, one for modeling uncertainty, and one for describing component properties. This framework enables a systematic exploitation of design properties in a design flow by facilitating the stepwise projection of certain layers of interest, the isolated analysis and refinement on projections, and the seamless reconstruction of a system model by virtue of orthogonalization. We demonstrate the capabilities of this approach by providing a compact yet expressive model of an active electronically scanned array antenna and signal processing chain, simulate it, validate its conformity with the design specifications, refine it, synthesize a sub-system to VHDL and sequential code, and co-simulate the generated artifacts. © 2021 Owner/Author.",Cyber-physical systems; design methodology; modeling; models of computation; simulation; synthesis; system design language; validation,Antenna arrays; Cyber Physical System; Problem oriented languages; Signal processing; Uncertainty analysis; Active electronically scanned array; Component properties; Design specification; Domain specific languages; Formal framework; Heterogeneous systems; Model uncertainties; Orthogonalization; Embedded systems
Analytical Program Power Characterization for Battery Depletion-time Estimation,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102981428&doi=10.1145%2f3421511&partnerID=40&md5=0e3c156d38130b28d3160aec4a22e72c,"Appropriate battery selection is a major design decision regarding the fast growth of battery-operated devices like space rovers, wireless sensor network nodes, rescue robots, and so on. Many such systems are mission critical, where estimation of the battery depletion time has an important role in the design efficiency with regard to the mission time. Accurate characterization of the system power usage pattern is essential for such an estimation. The following complexities exist: (1) The system behavior changes during interaction with the physical world, (2) the power consumption varies as the runtime progresses, (3) the total delivered battery charge has non-linear dependency on the power variability, and (4) design-time exhaustive study about runtime execution paths is almost impossible. This article presents an analytical method to first characterize the power variability of a given embedded program modeled by a directed acyclic graph, concerning the first and the second complexities. To include the third complexity, however, the concept of Worst-case Power Consumption Trace (WPCT) is proposed toward the worst-case scenario in terms of charge depletion for a given battery. A polynomial algorithm is also presented to construct WPCT and use it to estimate a tight lower bound for the system energy depletion time, i.e., its failure time, avoiding an exhaustive study. Comparisons between the analytical and simulation results reveal less than 3.4% of error in the bound estimations for the considered setups. © 2021 ACM.",battery unavailable charge; Battery-operated devices; failure time; power characterization; worst-case energy consumption,Directed graphs; Electric power utilization; Machine design; Sensor nodes; Analytical method; Battery operated devices; Design efficiency; Directed acyclic graph (DAG); Polynomial algorithm; Power characterization; Run-time execution; Worst case scenario; Electric batteries
Beyond Cache Attacks: Exploiting the Bus-based Communication Structure for Powerful On-Chip Microarchitectural Attacks,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102981269&doi=10.1145%2f3433653&partnerID=40&md5=29fcdcb9b37440c634521781e64ab8fc,"System-on-Chips (SoCs) are a key enabling technology for the Internet-of-Things (IoT), a hyper-connected world where on- A nd inter-chip communication is ubiquitous. SoCs usually integrate cryptographic hardware cores for confidentiality and authentication services. However, these components are prone to implementation attacks. During the operation of a cryptographic core, the secret key may passively be inferred through cache observations. Access-driven attacks exploiting these observations are therefore a vital threat to SoCs operating in IoT environments. Previous works have shown the feasibility of these attacks in the SoC context. Yet, the SoC communication structure can be used to further improve access-based cache attacks. The communication attacks are not as well-understood as other micro-architectural attacks. It is important to raise the awareness of SoC designers of such a threat. To this end, we present four contributions. First, we demonstrate an improved Prime+Probe attack on four different AES-128 implementations (original transformation tables, T0-Only, T2KB, and S-Box). As a novelty, this attack exploits the collisions of the bus-based SoC communication to further increase its efficiency. Second, we explore the impact of preloading on the efficiency of our communication-optimized attack. Third, we integrate three countermeasures (shuffling, mini-tables, and Time-Division Multiple Access (TDMA) bus arbitration) and evaluate their impact on the attack. Although shuffling and mini-tables countermeasures were proposed in previous work, their application as countermeasures against the bus-based attack was not studied before. In addition, TDMA as a countermeasure for bus-based attacks is an original contribution of this work. Fourth, we further discuss the implications of our work in the SoC design and its perspective with the new cryptographic primitives proposed in the ongoing National Institute of Standard and Technology Lightweight Cryptography competition. The results show that our improved communication-optimized attack is efficient, speeding up full key recovery by up to 400 times when compared to the traditional Prime+Probe technique. Moreover, the protection techniques are feasible and effectively mitigate the proposed improved attack. © 2021 ACM.",bus; cache attacks; communication; microarchitecture; MPSoC; side-channel,Efficiency; Internet of things; Probes; Programmable logic controllers; System-on-chip; Time division multiple access; Authentication services; Bus-based communication; Cryptographic primitives; Interchip communications; Internet of thing (IOT); Light-weight cryptography; National institute of standards; Time  division multiple accesses (TDMA); Cryptography
Facilitating Human Activity Data Annotation via Context-Aware Change Detection on Smartwatches,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102979953&doi=10.1145%2f3431503&partnerID=40&md5=7b692e98a7a51e66883caeff2a4e3544,"Annotating activities of daily living (ADL) is vital for developing machine learning models for activity recognition. In addition, it is critical for self-reporting purposes such as in assisted living where the users are asked to log their ADLs. However, data annotation becomes extremely challenging in real-world data collection scenarios, where the users have to provide annotations and labels on their own. Methods such as self-reports that rely on users' memory and compliance are prone to human errors and become burdensome since they increase users' cognitive load. In this article, we propose a light yet effective context-aware change point detection algorithm that is implemented and run on a smartwatch for facilitating data annotation for high-level ADLs. The proposed system detects the moments of transition from one to another activity and prompts the users to annotate their data. We leverage freely available Bluetooth low energy (BLE) information broadcasted by various devices to detect changes in environmental context. This contextual information is combined with a motion-based change point detection algorithm, which utilizes data from wearable motion sensors, to reduce the false positives and enhance the system's accuracy. Through real-world experiments, we show that the proposed system improves the quality and quantity of labels collected from users by reducing human errors while eliminating users' cognitive load and facilitating the data annotation process. © 2021 ACM.",activity recognition; BLE; Change point detection; context detection; data annotation; motion sensor; smartwatch app,Assisted living; Errors; Motion sensors; Signal detection; Activities of Daily Living; Activity recognition; Bluetooth low energies (BLE); Change point detection; Contextual information; Environmental contexts; Machine learning models; Real world experiment; Wearable computers
Optimization of Signal Processing Applications Using Parameterized Error Models for Approximate Adders,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102979449&doi=10.1145%2f3430509&partnerID=40&md5=0c4d3ddb50ea87f8689b42fa1353a086,"Approximate circuit design has gained significance in recent years targeting error-tolerant applications. In the literature, there have been several attempts at optimizing the number of approximate bits of each approximate adder in a system for a given accuracy constraint. For computational efficiency, the error models used in these routines are simple expressions obtained using regression or by assuming inputs or the error is uniformly distributed. In this article, we first demonstrate that for many approximate adders, these assumptions lead to an inaccurate prediction of error statistics for multi-level circuits. We show that mean error and mean square error can be computed accurately if static probabilities of adders at all stages are taken into account. Therefore, in a system with a certain type of approximate adder, any optimization framework needs to take into account not just the functionality of the adder but also its position in the circuit, functionality of its parents, and the number of approximate bits in the parent blocks. We propose a method to derive parameterized error models for various types of approximate adders. We incorporate these models within an optimization framework and demonstrate that the noise power is computed accurately. © 2021 ACM.",Approximate computing; error model; noise power; optimization; static probability,Adders; Computational efficiency; Errors; Integrated circuit manufacture; Mean square error; Signal processing; Circuit designs; Error tolerant; Multilevels; Noise power; Optimization framework; Parameterized; Signal processing applications; Simple expression; Error statistics
Heuristic Computation Offloading Algorithms for Mobile Users in Fog Computing,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102980310&doi=10.1145%2f3426852&partnerID=40&md5=272117040a9fcee29becb655bf7dc68b,"The investigation in this article makes the following important contributions to combinatorial optimization of computation offloading in fog computing. First, we rigorously define the two problems of optimal computation offloading with energy constraint and optimal computation offloading with time constraint. We do this in such a way that between execution time and energy consumption, we can fix one and minimize the other. We prove that our optimization problems are NP-hard, even for very special cases. Second, we develop a unique and effective approach for solving the proposed combinatorial optimization problems, namely, a two-stage method. In the first stage, we generate a computation offloading strategy. In the second stage, we decide the computation speed and the communication speeds. This method is applicable to both optimization problems. Third, we use a simple yet efficient greedy method to produce a computation offloading strategy by taking all aspects into consideration, including the properties of the communication channels, the power consumption models of computation and communication, the tasks already assigned and allocated, and the characteristics of the current task being considered. Fourth, we experimentally evaluate the performance of our heuristic algorithms. We observe that while various heuristics do exhibit noticeably different performance, there can be a single and simple heuristic that can perform very well. Furthermore, the method of compound algorithm can be applied to obtain slightly improved performance. Fifth, we emphasize that our problems and algorithms can be easily extended to study combined performance and cost optimization (such as cost-performance ratio and weighted cost-performance sum optimization) and to accommodate more realistic and complicated fog computing environments (such as preloaded mobile edge servers and multiple users) with little extra effort. To the best of our knowledge, there has been no similar study in the existing fog computing literature. © 2021 ACM.",Computation offloading; energy consumption; execution time; fog computing; heuristic algorithm; mobile edge computing; mobile user; performance evaluation,Combinatorial optimization; Energy efficiency; Energy utilization; Fog; Heuristic algorithms; NP-hard; Optimization; Tunneling (excavation); Combinatorial optimization problems; Computation offloading; Computing environments; Cost-Performance ratio; Effective approaches; Optimal computation; Optimization problems; Power consumption model; Fog computing
Lane Compression: A Lightweight Lossless Compression Method for Machine Learning on Embedded Systems,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102980498&doi=10.1145%2f3431815&partnerID=40&md5=cd6e466fdf53e411a416aaa0e30cdb26,"This article presents Lane Compression, a lightweight lossless compression technique for machine learning that is based on a detailed study of the statistical properties of machine learning data. The proposed technique profiles machine learning data gathered ahead of run-time and partitions values bit-wise into different lanes with more distinctive statistical characteristics. Then the most appropriate compression technique is chosen for each lane out of a small number of low-cost compression techniques. Lane Compression's compute and memory requirements are very low and yet it achieves a compression rate comparable to or better than Huffman coding. We evaluate and analyse Lane Compression on a wide range of machine learning networks for both inference and re-training. We also demonstrate the profiling prior to run-time and the ability to configure the hardware based on the profiling guarantee robust performance across different models and datasets. Hardware implementations are described and the scheme's simplicity makes it suitable for compressing both on-chip and off-chip traffic. © 2021 Owner/Author.",ASIC; deep neural networks; Machine learning,Embedded systems; Machine learning; Compression techniques; Hardware implementations; Lossless compression; Lossless compression techniques; Memory requirements; Robust performance; Statistical characteristics; Statistical properties; Embeddings
Improving the Performance of Hybrid Caches Using Partitioned Victim Caching,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099214652&doi=10.1145%2f3411368&partnerID=40&md5=d51f419c91755cfe52cfbf844c427f0c,"Non-Volatile Memory technologies are coming as a viable option on account of the high density and low-leakage power over the conventional SRAM counterpart. However, the increased write latency reduces their chances as a substitute for SRAM. To attenuate this problem, a hybrid STT-RAM-SRAM architecture is proposed where with large STT-RAM ways, the small SRAM ways are incorporated for handling the write operations. However, the performance gain obtained from such an architecture is not as much as expected on account of the larger miss rate caused by smaller SRAM partition. This, in turn, may limit the amount of cache capacity. This article attempts to reduce the miss penalty and improve the average memory access time by retaining the victims evicted from the hybrid cache in a smaller, fully associative SRAM structure called the victim cache. The victim cache is accessed on a miss in the primary hybrid cache. Hits in the victim cache require an exchange of the block between the main hybrid cache and the victim cache. In such cases, to effectively place the required block in the appropriate region of the main hybrid cache, we propose an access-based block placement technique. Besides, to manage the runtime load and the uneven evictions of the SRAM partition, we also present a dynamic region-based victim cache partitioning method to hold the victims dedicated to each region. Experimental evaluation on a full system simulator shows significant improvement in the performance and execution time along with a reduction in the overall miss rate. The proposed policy also increases the endurance of Hybrid Cache Architectures (HCA) by reducing writes in the STT partition.  © 2020 ACM.",Hybrid cache; non-volatile; partitioning; STT-RAM; victim cache,Memory architecture; Static random access storage; Average memory access time; Conventional sram; Experimental evaluation; Full system simulators; Low leakage power; Non-volatile memory technology; Performance Gain; Write operations; Cache memory
Verifying the Safety of Autonomous Systems with Neural Network Controllers,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099214800&doi=10.1145%2f3419742&partnerID=40&md5=15cf4e067b7396065656803764c4653f,"This article addresses the problem of verifying the safety of autonomous systems with neural network (NN) controllers. We focus on NNs with sigmoid/tanh activations and use the fact that the sigmoid/tanh is the solution to a quadratic differential equation. This allows us to convert the NN into an equivalent hybrid system and cast the problem as a hybrid system verification problem, which can be solved by existing tools. Furthermore, we improve the scalability of the proposed method by approximating the sigmoid with a Taylor series with worst-case error bounds. Finally, we provide an evaluation over four benchmarks, including comparisons with alternative approaches based on mixed integer linear programming as well as on star sets.  © 2020 ACM.",hybrid systems with neural network controllers; Neural network verification; safe autonomy,Differential equations; Error analysis; Hybrid systems; Integer programming; Petroleum reservoir evaluation; Autonomous systems; Mixed integer linear programming; Neural network (nn); Neural network controllers; Quadratic differential equations; System verifications; Worst case error; Neural networks
A TCAM-based Caching Architecture Framework for Packet Classification,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099174057&doi=10.1145%2f3409109&partnerID=40&md5=7434e3616a2c793650e5261bae2fadad,"Packet Classification is the enabling function for performing many networking applications like Integrated Services, Differentiated Services, Access Control/Firewalls, and Intrusion Detection. To cope with high-speed links and ever-increasing bandwidth requirements, time-efficient solutions are needed for which Ternary Content Addressable Memories (TCAMs) are popularly used. However, high cost, heavy power consumption, and poor scalability limit their use in many commercial switches. In this work, an efficient framework for caching the packet classification rules on TCAMs in accordance with traffic characteristics is proposed. The proposed design will have a two-level classification engine in which level-1 is a TCAM classifier with a smaller rule capacity and level-2 is a software classifier. The classifiers are assisted by a rule update engine that monitors the rule temporal behavior and performs timely updates of the rules onto level-1. Crucial challenges with respect to the proposed framework design are defined and addressed effectively in this work. Simulation results shows that the architecture can achieve a throughput of 250 Gbps on average by caching only 10% of the total rules for rule databases of sizes 10,000. The proposed architecture, to the best of our knowledge, is the only traffic-aware architecture using TCAMs that provides a completely deployable framework and also can scale for speeds beyond 250 Gbps (OC-1920 and beyond).  © 2020 ACM.",rule caching; statistical packet classification; Ternary CAMs,Access control; Engines; Intrusion detection; Logic gates; Quality of service; Bandwidth requirement; Caching architecture; Differentiated Services; Networking applications; Packet classification; Proposed architectures; Ternary content addressable memory; Traffic characteristics; Ternary content adressable memory
Golden Chip-Free Trojan Detection Leveraging Trojan Trigger s Side-Channel Fingerprinting,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099230939&doi=10.1145%2f3419105&partnerID=40&md5=bde276502bb56e59b586fe1ca3c7d0c9,"Hardware Trojans (HTs) have become a major threat for the integrated circuit industry and supply chain and have motivated numerous developments of HT detection schemes. Although the side-channel HT detection approach is among the most promising solutions, most of the previous methods require a trusted golden chip reference. Furthermore, detection accuracy is often influenced by environmental noise and process variations. In this article, a novel electromagnetic (EM) side-channel fingerprinting-based HT detection method is proposed. Different from previous methods, the proposed solution eliminates the requirement of a trusted golden fabricated chip. Rather, only the genuine RTL code is required to generate the EM signatures as references. A factor analysis method is utilized to extract the spectral features of the HT trigger's EM radiation, and then a k-means clustering method is applied for HT detection. Experimentation on two selected sets of Trust-Hub benchmarks has been performed on FPGA platforms, and the results show that the proposed framework can detect all dormant HTs with a high confidence level.  © 2020 ACM.",Electromagnetic side channel; factor analysis; golden chip free; hardware Trojan detection; k-means clustering,K-means clustering; Malware; Supply chains; Detection accuracy; Detection approach; Environmental noise; Factor analysis method; Integrated Circuit industries; K-means clustering method; Process Variation; Trojan detections; Palmprint recognition
Adaptive Task Allocation and Scheduling on NoC-based Multicore Platforms with Multitasking Processors,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099207930&doi=10.1145%2f3408324&partnerID=40&md5=b840d191dfd978a0d6cd670b152c2f6b,"The application workloads in modern multicore platforms are becoming increasingly dynamic. It becomes challenging when multiple applications need to be executed in parallel in such systems. Mapping and scheduling of these applications are critical for system performance and energy consumption, especially in Network-on-Chip- (NoC) based multicore systems. These systems with multitasking processors offer a better opportunity for parallel application execution. Mapping solutions generated at design time may be inappropriate for dynamic workloads. To improve the utilization of the underlying multicore platform and cope with the dynamism of application workload, often task allocation is carried out dynamically. This article presents a hybrid task allocation and scheduling strategy that exploits the design-time results at runtime. By considering the multitasking capability of the processors, communication energy, and timing characteristics of the tasks, different allocation options are obtained at design time. During runtime, based on the availability of the platform resources and application requirements, the design-time allocations are adapted for mapping and scheduling of tasks, which result in improved runtime performance. Experimental results demonstrate that the proposed approach achieves an on average 11.5%, 22.3%, 28.6%, and 34.6% reduction in communication energy consumption as compared to CAM [18], DEAMS [4], TSMM [38], and CPNN [32], respectively, for NoC-based multicore platforms with multitasking processors. Also, the deadline satisfaction of the tasks of allocated applications improves on an average by 32.8% when compared with the state-of-the-art dynamic resource allocation approaches.  © 2020 ACM.",communication energy; deadline; dynamic resource allocation; Multicore systems; network-on-chip,Availability; Energy utilization; Mapping; Multitasking; Network-on-chip; Parallel processing systems; Scheduling; Adaptive task allocations; Application requirements; Communication energy consumption; Dynamic resource allocations; Multi-core platforms; Multiple applications; Run-time performance; Timing characteristics; Integrated circuit design
Minimization of WCRT with Recovery Assurance from Hardware Trojans for Tasks on FPGA-based Cloud,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099219589&doi=10.1145%2f3409479&partnerID=40&md5=685278dc7231dc325ea4dfd229ffb911,"Dynamic partial reconfiguration (DPR) enabled FPGA-based Cloud architecture acts as a flexible and efficient shared environment to facilitates application support to users' request at low cost. While on one hand we need to handle a variety of tasks, such as periodic or sporadic, deadline or non-deadline, high or low critical tasks from the point of producing correct results, on the other hand we are constrained to use untrusted FPGA-based application IP blocks procured from various third-party vendors, which may contain hardware Trojan horse (HTH) affecting throughput and reliability of the Cloud. We propose Trojan-aware processing of tasks by monitored execution of a task on different untrusted cores, and then one more execution is done upon detection of hardware Trojan effects. For this stringent scheduling environment, the proposed dynamic scheduling algorithm is also properly extended to guarantee successful recovery from Trojan effects for all accepted tasks. Experimental results show that our algorithm improves worst-case-response-time for all tasks including non-deadline tasks and achieves lower task rejection rate for the deadline tasks, through judicious non-uniform partitioning of FPGAs based on supported jobs and subsequent better resource utilization, compared to that for existing Trojan-aware scheduling techniques.  © 2020 ACM.",cloud architecture; DPR-enabled FPGA; hardware Trojan attacks; power minimization; task scheduling; WCRT,Field programmable gate arrays (FPGA); Malware; Scheduling; Cloud architectures; Dynamic partial reconfiguration; Dynamic scheduling algorithms; Rejection rates; Resource utilizations; Scheduling techniques; Third party vendors; Worst case response time; Hardware security
MAGNETO: Fingerprinting USB Flash Drives via Unintentional Magnetic Emissions,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099216049&doi=10.1145%2f3422308&partnerID=40&md5=b2bb7ac90c4ebf9c0e9bab97ebf5c0e2,"Universal Serial Bus (USB) Flash Drives are nowadays one of the most convenient and diffused means to transfer files, especially when no Internet connection is available. However, USB flash drives are also one of the most common attack vectors used to gain unauthorized access to host devices. For instance, it is possible to replace a USB drive so that when the USB key is connected, it would install passwords stealing tools, root-kit software, and other disrupting malware. In such a way, an attacker can steal sensitive information via the USB-connected devices, as well as inject any kind of malicious software into the host. To thwart the above-cited raising threats, we propose MAGNETO, an efficient, non-interactive, and privacy-preserving framework to verify the authenticity of a USB flash drive, rooted in the analysis of its unintentional magnetic emissions. We show that the magnetic emissions radiated during boot operations on a specific host are unique for each device, and sufficient to uniquely fingerprint both the brand and the model of the USB flash drive, or the specific USB device, depending on the used equipment. Our investigation on 59 different USB flash drives - belonging to 17 brands, including the top brands purchased on Amazon in mid-2019 - reveals a minimum classification accuracy of 98.2% in the identification of both brand and model, accompanied by a negligible time and computational overhead. MAGNETO can also identify the specific USB Flash drive, with a minimum classification accuracy of 91.2%. Overall, MAGNETO proves that unintentional magnetic emissions can be considered as a viable and reliable means to fingerprint read-only USB flash drives. Finally, future research directions in this domain are also discussed.  © 2020 ACM.",critical infrastructures protection; hardware security; magnetic emissions; USB,Authentication; Drives; Electronic document exchange; Magnetism; Malware; Privacy by design; Classification accuracy; Computational overheads; Future research directions; Internet connection; Privacy preserving; Sensitive informations; Unauthorized access; Universal serial bus; System buses
Generalized Weakly Hard Schedulability Analysis for Real-Time Periodic Tasks,2021,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099241106&doi=10.1145%2f3404888&partnerID=40&md5=eac41674ebe42e61f3dd4ef2b309d1c0,"The weakly hard real-time model is an abstraction for applications, including control systems, that can tolerate occasional deadline misses, but can also be compromised if a sufficiently high number of late terminations occur in a given time window. The weakly hard model allows us to constrain the maximum number of acceptable missed deadlines in any set of consecutive task executions. A big challenge for weakly hard systems is to provide a schedulability analysis that applies to a general task model, while avoiding excessive pessimism. In this work, we develop a general weakly hard analysis based on a Mixed Integer Linear Programming (MILP) formulation. The analysis applies to constrained-deadline periodic real-time systems scheduled with fixed priority and no knowledge of the task activation offsets, while allowing for activation jitter. Our analysis considers two common policies for handling missed deadlines, i.e., (i) letting the job continue until completion or (ii) killing its execution immediately. For this policy, ours is the first and only m-k analysis currently available. Experiments conducted on randomly generated task sets show the applicability and accuracy of the proposed technique as well as the improvements with respect to competing techniques.  © 2020 ACM.",activation jitter; deadline-miss handling strategies; periodic real-time tasks; schedulability analysis; Weakly hard real-time systems,Activation analysis; Chemical activation; Integer programming; Interactive computer systems; Fixed priorities; Hard-modeling; Mixed-integer linear programming; Periodic tasks; Schedulability analysis; Task activations; Task executions; Weakly hard real-time; Real time systems
Exploring Impact of Profile Data on Code Quality in the HotSpot JVM,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097351034&doi=10.1145%2f3391894&partnerID=40&md5=cb2883564a9f79cfcd4726f16c15d1c5,"Managed language virtual machines (VM) rely on dynamic or just-in-time (JIT) compilation to generate optimized native code at run-time to deliver high execution performance. Many VMs and JIT compilers collect profile data at run-time to enable profile-guided optimizations (PGO) that customize the generated native code to different program inputs. PGOs are generally considered integral for VMs to produce high-quality and performant native code. In this work, we study and quantify the performance benefits of PGOs, understand the importance of profiling data quantity and quality/accuracy to effectively guide PGOs, and assess the impact of individual PGOs on VM performance. The insights obtained from this work can be used to understand the current state of PGOs, develop strategies to more efficiently balance the cost and exploit the potential of PGOs, and explore the implications of and challenges for the alternative ahead-of-time (AOT) compilation model used by VMs. © 2020 ACM.",profile-guided optimizations; Program profiling,Just in time production; Program compilers; Virtual machine; Code quality; Data quantity; Execution performance; High quality; JIT compiler; Just-in-time compilation; Performance benefits; Profile data; Codes (symbols)
DIAC: An Inter-app Conflicts Detector for Open IoT Systems,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097337553&doi=10.1145%2f3391895&partnerID=40&md5=10a1bac789dec1e71451a0940db84017,"This article tackles the problem of detecting and solving potential conflicts among independently developed apps that are to be installed into an open Internet-of-Things (IoT) environment. It provides a new set of definitions and categorizations of the conflicts to more precisely characterize the nature of the problem, and it proposes a representation named ""IA Graphs""for formally representing IoT controls and inter-app interplays. Based on the definitions and representation, it then describes an efficient conflict detection algorithm. Combining conflict categories, seriousness indicator, and conflict frequency, an innovative solution policy for solving various detected conflicts is developed, which also takes into account user preference and interest by providing interactive process. It implements a compiler and runtime software system that integrates all the proposed techniques together into a comprehensive solution. Experiments on SmartThings apps validate its significantly better detection efficacy over prior methods and effectiveness of conflict solution with user preference. © 2020 ACM.",compiler; conflicts detection; IoT,Software engineering; Conflict detection algorithms; Innovative solutions; Interactive process; Internet of Things (IOT); Potential conflict; Run-time software; Internet of things
Fast and Energy-Efficient State Checkpointing for Intermittent Computing,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097331607&doi=10.1145%2f3391903&partnerID=40&md5=71aa74b0fb2df49522e805e4bf17e53f,"Intermittently powered embedded devices ensure forward progress of programs through state checkpointing in non-volatile memory. Checkpointing is, however, expensive in energy and adds to the execution times. To minimize this overhead, we present DICE, a system that renders differential checkpointing profitable on these devices. DICE is unique because it is a software-only technique and efficient because it only operates in volatile main memory to evaluate the differential. DICE may be integrated with reactive (Hibernus) or proactive (MementOS, HarvOS) checkpointing systems, and arbitrary code can be enabled with DICE using automatic code-instrumentation requiring no additional programmer effort. By reducing the cost of checkpoints, DICE cuts the peak energy demand of these devices, allowing operation with energy buffers that are one-eighth of the size originally required, thus leading to benefits such as smaller device footprints and faster recharging to operational voltage level. The impact on final performance is striking: with DICE, Hibernus requires one order of magnitude fewer checkpoints and one order of magnitude shorter time to complete a workload in real-world settings. © 2020 ACM.",differential checkpointing; intermittent computing; Transiently powered computers,Digital storage; Automatic codes; Embedded device; Energy efficient; Non-volatile memory; Operational voltage; Peak energy demand; Real world setting; Software-only techniques; Energy efficiency
A Vector-Length Agnostic Compiler for the Connex-S Accelerator with Scratchpad Memory,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097329374&doi=10.1145%2f3406536&partnerID=40&md5=786d1e12a04035a71d12602563a4bf38,"Compiling sequential C programs for Connex-S, a competitive, scalable and customizable, wide vector accelerator for intensive embedded applications with 32 to 4,096 16-bit integer lanes and a limited capacity local scratchpad memory, is challenging. Our compiler toolchain uses the LLVM framework and targets OPINCAA, a JIT vector assembler and coordination C++ library for Connex-S accelerating computations for an arbitrary CPU. Therefore, we address in the compiler middle end aspects of efficient vectorization, communication, and synchronization. We perform quantitative static analysis of the program useful, among others, for the symbolic-size compiler memory allocator and the coordination mechanism of OPINCAA. We also discuss the LLVM back end for the Connex-S processor and the methodology to automatically generate instruction selection code for emulating efficiently arithmetic and logical operations for non-native types such as 32-bit integer and 16-bit floating-point. By using JIT vector assembling and by encoding the vector length of Connex-S as a parameter in the generated OPINCAA program, we achieve vector-length agnosticism to support execution on distinct embedded devices, such as several digital cameras with different resolutions, each equipped with custom-width Connex-S accelerators meant to save energy for the image processing kernels. Since Connex-S has a limited capacity local scratchpad memory of 256 KB normally, we present how we also use the PPCG C-to-C code generator to perform data tiling to minimize the total kernel execution time, subject to fitting larger program data in the local memory. We devise an accurate cost model for the Connex-S accelerator to choose optimal performance tile sizes at compile time. We successfully compile several simple benchmarks frequently used, for example, in high-performance and computer vision embedded applications. We report speedup factors of up to 11.33 when running them on a Connex-S accelerator with 128 16-bit integer lanes w.r.t. the dual-core ARM Cortex A9 host clocked at a frequency 6.67 times higher, with a total of two 128-bit Neon SIMD units. © 2020 ACM.",Connex-S vector (array) accelerator; LLVM; OPINCAA JIT vector assembler and coordination library; quantitative static analysis; vector-length agnostic compiler for the custom-width Connex-S accelerator; vectorization,Acceleration; Application programs; Benchmarking; C++ (programming language); Codes (symbols); Digital arithmetic; Digital devices; Image processing; Integer programming; Just in time production; Memory architecture; Static analysis; Vectors; Coordination mechanisms; Different resolutions; Embedded application; Instruction selection; Logical operations; Optimal performance; Scratch pad memory; Vector accelerators; Program compilers
Creating Hardware Component Knowledge Bases with Training Data Generation and Multi-task Learning,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097334272&doi=10.1145%2f3391906&partnerID=40&md5=89a14bcc645a1acfe67e2efb8e07086a,"Hardware component databases are vital resources in designing embedded systems. Since creating these databases requires hundreds of thousands of hours of manual data entry, they are proprietary, limited in the data they provide, and have random data entry errors. We present a machine learning based approach for creating hardware component databases directly from datasheets. Extracting data directly from datasheets is challenging because: (1) the data is relational in nature and relies on non-local context, (2) the documents are filled with technical jargon, and (3) the datasheets are PDFs, a format that decouples visual locality from locality in the document. Addressing this complexity has traditionally relied on human input, making it costly to scale. Our approach uses a rich data model, weak supervision, data augmentation, and multi-task learning to create these knowledge bases in a matter of days. We evaluate the approach on datasheets of three types of components and achieve an average quality of 77 F1 points-quality comparable to existing human-curated knowledge bases. We perform application studies that demonstrate the extraction of multiple data modalities including numerical properties and images. We show how different sources of supervision such as heuristics and human labels have distinct advantages that can be utilized together to improve knowledge base quality. Finally, we present a case study to show how this approach changes the way practitioners create hardware component knowledge bases. © 2020 ACM.",design tools; Knowledge base construction; machine learning,Database systems; Embedded systems; Knowledge based systems; Learning systems; Multi-task learning; Random errors; Turing machines; Application studies; Data augmentation; Hardware components; Knowledge base; Knowledge basis; Numerical properties; Technical jargon; Training data; Data mining
Demystifying Energy Consumption Dynamics in Transiently powered Computers,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097328709&doi=10.1145%2f3391893&partnerID=40&md5=143d2e53a103c987ecaf5e981a560a45,"Transiently powered computers (TPCs) form the foundation of the battery-less Internet of Things, using energy harvesting and small capacitors to power their operation. This kind of power supply is characterized by extreme variations in supply voltage, as capacitors charge when harvesting energy and discharge when computing. We experimentally find that these variations cause marked fluctuations in clock speed and power consumption. Such a deceptively minor observation is overlooked in existing literature. Systems are thus designed and parameterized in overly conservative ways, missing on a number of optimizations. We rather demonstrate that it is possible to accurately model and concretely capitalize on these fluctuations. We derive an energy model as a function of supply voltage and prove its use in two settings. First, we develop EPIC, a compile-time energy analysis tool. We use it to substitute for the constant power assumption in existing analysis techniques, giving programmers accurate information on worst-case energy consumption of programs. When using EPIC with existing TPC system support, run-time energy efficiency drastically improves, eventually leading up to a 350% speedup in the time to complete a fixed workload. Further, when using EPIC with existing debugging tools, it avoids unnecessary program changes that hurt energy efficiency. Next, we extend the MSPsim emulator and explore its use in parameterizing a different TPC system support. The improvements in energy efficiency yield up to more than 1000% time speedup to complete a fixed workload. © 2020 ACM.",energy modelling; intermittent computing; Transiently powered computers,Energy efficiency; Energy harvesting; Energy utilization; Program debugging; Analysis techniques; Constant power; Debugging tools; Harvesting energies; Parameterized; Parameterizing; Supply voltages; System supports; Green computing
EncoDeep: Realizing Bit-flexible Encoding for Deep Neural Networks,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097354495&doi=10.1145%2f3391901&partnerID=40&md5=8961485d52225efe80ec00213a752ad1,"This article proposes EncoDeep, an end-to-end framework that facilitates encoding, bitwidth customization, fine-tuning, and implementation of neural networks on FPGA platforms. EncoDeep incorporates nonlinear encoding to the computation flow of neural networks to save memory. The encoded features demand significantly lower storage compared to the raw full-precision activation values; therefore, the execution flow of EncoDeep hardware engine is completely performed within the FPGA using on-chip streaming buffers with no access to the off-chip DRAM. We further propose a fully automated optimization algorithm that determines the flexible encoding bitwidths across network layers. EncoDeep full-stack framework comprises a compiler that takes a high-level Python description of an arbitrary neural network. The compiler then instantiates the corresponding elements from EncoDeep Hardware library for FPGA implementation. Our evaluations on MNIST, SVHN, and CIFAR-10 datasets demonstrate an average of 4.65× throughput improvement compared to stand-alone weight encoding. We further compare EncoDeep with six FPGA accelerators on ImageNet, showing an average of 3.6× and 2.54× improvement in throughput and performance-per-watt, respectively. © 2020 ACM.",automated optimization; neural network customization; Resource-customized computing,Deep neural networks; Dynamic random access storage; Encoding (symbols); Field programmable gate arrays (FPGA); Image enhancement; Network coding; Network layers; Program compilers; Activation value; Computation flow; Fpga accelerators; FPGA implementations; Fpga platforms; Fully automated; Hardware engines; Throughput improvement; Neural networks
A Retargetable MATLAB-to-C Compiler Exploiting Custom Instructions and Data Parallelism,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097348413&doi=10.1145%2f3391898&partnerID=40&md5=a79a814d03bf4907205e8c9bd46263fc,"This article presents a MATLAB-to-C compiler that exploits custom instructions present in state-of-the-art processor architectures and supports semi-automatic vectorization. A parameterized processor model is used to describe the target instruction set architecture to achieve user-friendly retargetability. Custom instructions are represented via specialized intrinsic functions in the generated code, which can then be used as input to any C/C++ compiler supporting the target processor. In addition, the compiler supports the generation of data parallel/vectorized code through the introduction of data packing/unpacking statements. The compiler has been used for code generation targeting ARM and x86 architectures for several benchmarks. The vectorized code generated by the compiler achieves an average speedup of 4.1× and 2.7× for packed fixed and floating point data, respectively, compared to scalarized code for ARM architecture and an average speedup of 3.1× and 1.5× for packed fixed and floating point data, respectively, for x86 architecture. Implementing data parallel instructions directly in the assembly code would have required a lot of design effort, and it would not been sustainable across evolving platform variants. Thus, the compiler can be employed to efficiently speed up critical sections of the target application. The compiler is therefore potentially employable to raise the design abstraction and reduce development time for both embedded and general-purpose applications. © 2020 ACM.",ARM; auto-vectorization; compilation; compiler; MATLAB; retargetable; x86,ARM processors; C++ (programming language); Codes (symbols); Computer architecture; Digital arithmetic; Integrated circuit design; Parallel processing systems; Custom instruction; Design abstractions; Floating-point data; Instruction set architecture; Intrinsic functions; Processor architectures; Processor modeling; Target application; Program compilers
"Introduction to the Special Issue on Languages, Compilers, Tools, and Theory of Embedded Systems",2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097443696&doi=10.1145%2f3417734&partnerID=40&md5=f7221ef2603ef8ab5822c291628ab402,[No abstract available],,
RVSDG: An Intermediate Representation for Optimizing Compilers,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097349183&doi=10.1145%2f3391902&partnerID=40&md5=93a09e4d4e61594c4b68eb64fbce31c2,"Intermediate Representations (IRs) are central to optimizing compilers as the way the program is represented may enhance or limit analyses and transformations. Suitable IRs focus on exposing the most relevant information and establish invariants that different compiler passes can rely on. While control-flow centric IRs appear to be a natural fit for imperative programming languages, analyses required by compilers have increasingly shifted to understand data dependencies and work at multiple abstraction layers at the same time. This is partially evidenced in recent developments such as the Multi-Level Intermediate Representation (MLIR) proposed by Google. However, rigorous use of data flow centric IRs in general purpose compilers has not been evaluated for feasibility and usability as previous works provide no practical implementations. We present the Regionalized Value State Dependence Graph (RVSDG) IR for optimizing compilers. The RVSDG is a data flow centric IR where nodes represent computations, edges represent computational dependencies, and regions capture the hierarchical structure of programs. It represents programs in demand-dependence form, implicitly supports structured control flow, and models entire programs within a single IR. We provide a complete specification of the RVSDG, construction and destruction methods, as well as exemplify its utility by presenting Dead Node and Common Node Elimination optimizations. We implemented a prototype compiler and evaluate it in terms of performance, code size, compilation time, and representational overhead. Our results indicate that the RVSDG can serve as a competitive IR in optimizing compilers while reducing complexity. © 2020 ACM.",intermediate representation; LLVM; Regionalized value state dependence graph (RVSDG),Data transfer; Abstraction layer; Data dependencies; Hierarchical structures; Imperative programming languages; Intermediate representations; Limit analysis; Optimizing compilers; State dependence; Program compilers
SPECTRUM: A Software-defined Predictable Many-core Architecture for LTE/5G Baseband Processing,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096879542&doi=10.1145%2f3400032&partnerID=40&md5=eeb0a23d16121e30589685e27c82b31c,"Wireless communication standards such as Long-term Evolution (LTE) are rapidly changing to support the high data-rate of wireless devices. The physical layer baseband processing has strict real-time deadlines, especially in the next-generation applications enabled by the 5G standard. Existing basestation transceivers utilize customized DSP cores or fixed-function hardware accelerators for physical layer baseband processing. However, these approaches incur significant non-recurring engineering costs and are inflexible to newer standards or updates. Software-programmable processors offer more adaptability. However, it is challenging to sustain guaranteed worst-case latency and throughput at reasonably low-power on shared-memory many-core architectures featuring inherently unpredictable design choices, such as caches and Network-on-chip (NoC). We propose SPECTRUM, a predictable, software-defined many-core architecture that exploits the massive parallelism of the LTE/5G baseband processing workload. The focus is on designing scalable lightweight hardware that can be programmed and defined by sophisticated software mechanisms. SPECTRUM employs hundreds of lightweight in-order cores augmented with custom instructions that provide predictable timing, a purely software-scheduled NoC that orchestrates the communication to avoid any contention, and per-core software-controlled scratchpad memory with deterministic access latency. Compared to many-core architecture like Skylake-SP (average power 215 W) that drops 14% packets at high-traffic load, 256-core SPECTRUM by definition has zero packet drop rate at significantly lower average power of 24 W. SPECTRUM consumes 2.11× lower power than C66x DSP cores+accelerator platform in baseband processing. We also enable SPECTRUM to handle dynamic workloads with multiple service categories present in 5G mobile network (Enhanced Mobile Broadband (eMBB), Ultra-reliable and Low-latency Communications (URLLC), and Massive Machine Type Communications (mMTC)), using a run-time scheduling and mapping algorithm. Experimental evaluations show that our algorithm performs task/NoC mapping at run-time on fewer cores compared to the static mapping (that reserves cores exclusively for each service category) while still meeting the differentiated latency and reliability requirements.  © 2020 ACM.",5G; baseband processing; low-power; LTE; many-cores; Time-predictable architecture,Cache memory; Conformal mapping; Cost engineering; Drops; Integrated circuit design; Long Term Evolution (LTE); Low power electronics; Memory architecture; Mobile telecommunication systems; Network architecture; Network-on-chip; Physical layer; Radio transceivers; Experimental evaluation; Low-latency communication; Machine type communications; Next-Generation Applications; Non recurring engineering; Programmable processors; Reliability requirements; Wireless communication standards; 5G mobile communication systems
PANDORA: An Architecture-Independent Parallelizing Approximation-Discovery Framework,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096875858&doi=10.1145%2f3391899&partnerID=40&md5=41246f651329968d5a183e56a537025c,"In this article, we introduce a parallelizing approximation-discovery framework, PANDORA, for automatically discovering application- and architecture-specialized approximations of provided code. PANDORA complements existing compilers and runtime optimizers by generating approximations with a range of Pareto-optimal tradeoffs between performance and error, which enables adaptation to different inputs, different user preferences, and different runtime conditions (e.g., battery life). We demonstrate that PANDORA can create parallel approximations of inherently sequential code by discovering alternative implementations that eliminate loop-carried dependencies. For a variety of functions with loop-carried dependencies, PANDORA generates approximations that achieve speedups ranging from 2.3x to 81x, with acceptable error for many usage scenarios. We also demonstrate PANDORA's architecture-specialized approximations via FPGA experiments, and highlight PANDORA's discovery capabilities by removing loop-carried dependencies from a recurrence relation with no known closed-form solution.  © 2020 ACM.",approximate computing; machine learning; Symbolic regression,Architecture; Codes (symbols); Battery life; Closed form solutions; Discovery frameworks; Loop-carried dependencies; Parallelizing; Pareto-optimal; Recurrence relations; Usage scenarios; Pareto principle
Formal Verification of Spacecraft Control Programs,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096870967&doi=10.1145%2f3391900&partnerID=40&md5=43e9408c8fc0dd7442976ba1aa2c39c4,"Verification of correctness of control programs is an essential task in the development of space electronics; it is difficult and typically outweighs design and programming tasks in terms of development hours. This article presents a verification approach designed to help spacecraft engineers reduce the effort required for formal verification of low-level control programs executed on custom hardware. The verification approach is demonstrated on an industrial case study. We present a REDuced instruction set for Fixed-point and INteger arithmetic (REDFIN), a processing core used in space missions, and its formal semantics expressed using the proposed metalanguage for state transformers, followed by examples of verification of simple control programs.  © 2020 ACM.",domain-specific languages; Formal verification; functional programming; instruction set architecture,Fixed point arithmetic; Integer programming; Semantics; Space flight; Industrial case study; Integer arithmetic; Programming tasks; Reduced instruction sets; Space electronics; Spacecraft control; Spacecraft engineers; State-transformers; Formal verification
Dealing with Uncertainty in pWCET Estimations,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096033281&doi=10.1145%2f3396234&partnerID=40&md5=e92dad53c3558424c49affacdf61db24,"The problem of estimating a tight and safe Worst-Case Execution Time (WCET), needed for certification in safety-critical environment, is a challenging problem for modern embedded systems. A possible solution proposed in past years is to exploit statistical tools to obtain a probability distribution of the WCET. These probabilistic real-time analyses for WCET are, however, subject to errors, even when all the applicability hypotheses are satisfied and verified. This is caused by the uncertainties of the probabilistic-WCET distribution estimator. This article aims at improving the measurement-based probabilistic timing analysis approach providing some techniques to analyze and deal with such uncertainties. The so-called region of acceptance model based on state-of-the-art statistical test procedures is defined over the distribution space parameters. From this model, a set of strategies is derived and discussed to provide the methodology to deal with the trade-off safety/tightness of the WCET estimation. These techniques are then tested over real datasets, including industrial safety-critical applications, to show the increased value of using the proposed approach in probabilistic WCET analyses.  © 2020 ACM.",embedded systems; probabilistic real-time; pWCET,Acceptance tests; Accident prevention; Economic and social effects; Embedded systems; Probability distributions; Risk management; Statistical mechanics; Acceptance models; Critical environment; Distribution space; Modern embedded systems; Real time analysis; Safety critical applications; Statistical tools; Worst-case execution time; Uncertainty analysis
Crab-tree: A Crash Recoverable B+-tree Variant for Persistent Memory with ARMv8 Architecture,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096848631&doi=10.1145%2f3396236&partnerID=40&md5=c7d01458fd64ee0dfd0b0fe9147684b6,"In recent years, the next-generation non-volatile memory (NVM) technologies have emerged with DRAM-like byte addressability and disk-like durability. Computer architects have proposed to use them to build persistent memory that blurs the conventional boundary between volatile memory and non-volatile storage. However, ARM processors, ones that are widely used in embedded computing systems, start providing architectural supports to utilize NVM since ARMv8. In this article, we consider tailoring B+-tree for NVM operated by a 64-bit ARMv8 processor. We first conduct an empirical study of performance overhead in writing and reading data for a B+-tree with an ARMv8 processor, including the time cost of cache line flushes and memory fences for crash consistency as well as the execution time of binary search compared to that of linear search. We hence identify the key weaknesses in the design of B+-tree with ARMv8 architecture. Accordingly, we develop a new B+-tree variant, namely, <underline>c</underline>rash <underline>r</underline>ecoverable <underline>A</underline>RMv8-oriented <underline>B</underline>+-tree (Crab-tree). To insert and delete data at runtime, Crab-tree selectively chooses one of two strategies, i.e., copy on write and shifting in place, depending on which one causes less consistency cost. Crab-tree regulates a strict execution order in both strategies and recovers the tree structure in case of crashes. To further improve the performance of Crab-tree, we employ three methods to reduce software overhead, cache misses, and consistency cost, respectively. We have implemented and evaluated Crab-tree in Raspberry Pi 3 Model B+ with emulated NVM. Experiments show that Crab-tree significantly outperforms state-of-the-art B+-trees designed for persistent memory by up to 2.2× and 3.7× in write and read performances, respectively, with both consistency and scalability achieved.  © 2020 ACM.",ARMv8; B+-tree; non-volatile memory; persistent memory,ARM processors; Binary trees; Cache memory; Dynamic random access storage; Embedded systems; Forestry; Memory architecture; Nonvolatile storage; Shellfish; Architectural support; Computer architects; Embedded computing system; Empirical studies; Non-volatile memory technology; Persistent memory; Read performance; State of the art; Trees (mathematics)
Approximate Cache in GPGPUs,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096874072&doi=10.1145%2f3407904&partnerID=40&md5=21dce70bd59bf62d64364d0080c455c3,"There is a growing number of application domains ranging from multimedia to machine learning where a certain level of inexactness can be tolerated. For these applications, approximate computing is an effective technique that trades off some loss in output data integrity for energy and/or performance gains. In this article, we present the approximate cache, which approximates similar values and saves energy in the L2 cache of general-purpose graphics processing units (GPGPUs). The L2 cache is a critical component in memory hierarchy of GPGPUs, as it accommodates data of thousands of simultaneously executing threads. Simply increasing the size of the L2 cache is not a viable solution to keep up with the growing size of data in many-core applications. This work is motivated by the observation that threads within a warp write values into memory that are arithmetically similar. We exploit this property and propose a low-cost and implementation-efficient hardware to trade off accuracy for energy. The approximate cache identifies similar values during the runtime and allows only one thread writes into the cache in the event of similarity. Since the approximate cache is able to pack more data in a smaller space, it enables downsizing of the data array with negligible impact on cache misses and lower-level memory. The approximate cache reduces both dynamic and static energy. By storing data of a thread into a cache block, each memory instruction requires accessing fewer cache cells, thus reducing dynamic energy. In addition, the approximate cache increases frequency of bank idleness. By power gating idle banks, static energy is reduced. Our evaluations reveal that the approximate cache reduces energy by 52% with minimal quality degradation while maintaining performance of a diverse set of GPGPU applications.  © 2020 ACM.",Approximate computing; cache; energy; GPGPU; microarchitecture,Computer graphics; Costs; Economic and social effects; Graphics processing unit; Program processors; Weaving; Critical component; Dynamic energy; Memory hierarchy; Performance Gain; Power gatings; Quality degradation; Static energy; Viable solutions; Cache memory
Hardware Performance Counter-Based Fine-Grained Malware Detection,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096867829&doi=10.1145%2f3403943&partnerID=40&md5=5ee1f4e3a3097b63a75ceb6d9ed73f85,"Detection of malicious programs using hardware-based features has gained prominence recently. The tamper-resistant hardware metrics prove to be a better security feature than the high-level software metrics, which can be easily obfuscated. Hardware Performance Counters (HPC), which are inbuilt in most of the recent processors, are often the choice of researchers amongst hardware metrics. However, a lack of determinism in their counts, thereby affecting the malware detection rate, minimizes the advantages of HPCs. To overcome this problem, in our work, we propose a three-step methodology for fine-grained malware detection. In the first step, we extract the HPCs of each system call of an unknown program. Later, we make a dimensionality reduction of the fine-grained data to identify the components that have maximum variance. Finally, we use a machine learning based approach to classify the nature of the unknown program into benign or malicious. Our proposed methodology has obtained a 98.4% detection rate, with a 3.1% false positive. It has improved the detection rate significantly when compared to other recent works in hardware-based anomaly detection.  © 2020 ACM.",embedded systems; fine-grained analysis; Hardware performance counters; principal component analysis,Anomaly detection; Dimensionality reduction; Turing machines; Detection rates; False positive; Hardware performance counters; Malware detection; Maximum variance; Security features; Software metrics; Tamper resistant; Malware
Optimizing Tensor Contractions for Embedded Devices with Racetrack and DRAM Memories,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096079619&doi=10.1145%2f3396235&partnerID=40&md5=e1c7119f6dde965390609762d1b6f2ce,"Tensor contraction is a fundamental operation in many algorithms with a plethora of applications ranging from quantum chemistry over fluid dynamics and image processing to machine learning. The performance of tensor computations critically depends on the efficient utilization of on-chip/off-chip memories. In the context of low-power embedded devices, efficient management of the memory space becomes even more crucial, in order to meet energy constraints. This work aims at investigating strategies for performance-and energy-efficient tensor contractions on embedded systems, using racetrack memory (RTM)-based scratch-pad memory (SPM) and DRAM-based off-chip memory. Compiler optimizations such as the loop access order and data layout transformations paired with architectural optimizations such as prefetching and preshifting are employed to reduce the shifting overhead in RTMs. Optimizations for off-chip memory such as memory access order, data mapping and the choice of a suitable memory access granularity are employed to reduce the contention in the off-chip memory. Experimental results demonstrate that the proposed optimizations improve the SPM performance and energy consumption by 32% and 73%, respectively, compared to an iso-capacity SRAM. The overall DRAM dynamic energy consumption improvements due to memory optimizations amount to 80%. © 2020 ACM.",Compiler optimization; data transformation; DRAM mapping; embedded systems; matrix multiplication; prefetching; preshifting; racetrack memory; tensor contraction; tensors,Embedded systems; Energy efficiency; Energy utilization; Image processing; Machine learning; Memory architecture; Quantum chemistry; Static random access storage; Tensors; Compiler optimizations; Data layout transformations; Dynamic energy consumption; Efficient managements; Fundamental operations; Memory optimization; Scratch pad memory; Tensor contraction; Dynamic random access storage
Development Automation of Real-Time Java: Model-Driven Transformation and Synthesis,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096853565&doi=10.1145%2f3391897&partnerID=40&md5=8dbe8425fe02dd809b9306328473baa7,"Many applications in emerging scenarios, such as autonomous vehicles, intelligent robots, and industrial automation, are safety-critical with strict timing requirements. However, the development of real-time systems is error prone and highly dependent on sophisticated domain expertise, making it a costly process. This article utilises the principles of model-driven engineering (MDE) and proposes two methodologies to automate the development of real-time Java applications. The first one automatically converts standard time-sharing Java applications to real-time Java applications, using a series of transformations. It is in line with the observed industrial trend, such as for the big data technology, of redeveloping existing software without the real-time notion to realise the real-time features. The second one allows users to automatically generate real-time Java application templates with a lightweight modelling language, which can be used to define the real-time properties - essentially a synthesis process. This article opens up a new research direction on development automation of real-time programming languages and inspires many research questions that can be jointly investigated by the embedded systems, programming languages as well as MDE communities.  © 2020 ACM.",model-driven engineering; Real-time programming languages; real-time specification for Java,Accident prevention; Automation; Embedded systems; Industrial robots; Intelligent robots; Interactive computer systems; Java programming language; Modeling languages; Industrial automation; Model-driven Engineering; Real-time features; Real-time programming languages; Real-time properties; Research questions; Synthesis process; Timing requirements; Real time systems
GMAI: Understanding and Exploiting the Internals of GPU Resource Allocation in Critical Systems,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096877681&doi=10.1145%2f3391896&partnerID=40&md5=88ccc96795ef8284f68c74574caa24d3,"Critical real-time systems require strict resource provisioning in terms of memory and timing. The constant need for higher performance in these systems has led industry to recently include GPUs. However, GPU software ecosystems are by their nature closed source, forcing system engineers to consider them as black boxes, complicating resource provisioning. In this work, we reverse engineer the internal operations of the GPU system software to increase the understanding of their observed behaviour and how resources are internally managed. We present our methodology that is incorporated in GMAI (GPU Memory Allocation Inspector), a tool that allows system engineers to accurately determine the exact amount of resources required by their critical systems, avoiding underprovisioning. We first apply our methodology on a wide range of GPU hardware from different vendors showing its generality in obtaining the properties of the GPU memory allocators. Next, we demonstrate the benefits of such knowledge in resource provisioning of two case studies from the automotive domain, where the actual memory consumption is up to 5.6× more than the memory requested by the application.  © 2020 ACM.",critical systems; GPUs; resource allocation; reverse engineering,Computer hardware; Engineers; Graphics processing unit; Interactive computer systems; Program processors; Automotive domains; Critical systems; Internal operations; Memory allocators; Memory consumption; Software ecosystems; System engineers; System softwares; Real time systems
TrustFlow-X: A Practical Framework for Fine-grained Control-flow Integrity in Critical Systems,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096870351&doi=10.1145%2f3398327&partnerID=40&md5=e99d815dd910625760d07cb0d4cbbefd,"This article addresses the challenges of memory safety in life-critical medical devices. Since the last decade, healthcare manufacturers have embraced the Internet of Things, pushing technological innovations to increase market share. Medical devices, including the most critical ones, tend to be increasingly connected to the Internet. Unfortunately, as critical devices often rely on unsafe programming languages such as C, they are no exception to memory safety issues. Given a memory vulnerability, a skillful attacker can take over a system and perform remote code execution. Combined with the fact that medical devices directly impact the safety of their users, a security vulnerability can lead to disastrous scenarios. To address this issue, this article presents TrustFlow-X, a novel hardware/software co-designed framework that provides efficient fine-grained control-flow integrity protection against memory-based attacks. The TrustFlow-X framework is composed of an LLVM-based compiler toolchain that generates a secure code. This secure code is then executed on an extended RISC-V processor that keeps track of sensitive data using a trusted memory. The obtained results show that the contribution is practical, providing a high level of trust in life-critical embedded systems.  © 2020 ACM.",compiler; control-flow integrity; Memory safety; processor architecture,Codes (symbols); Competition; Data privacy; Hardware-software codesign; Critical device; Critical systems; Fine-grained control; Medical Devices; Novel hardware; Security vulnerabilities; Sensitive datas; Technological innovation; Embedded systems
"Introduction to the Special Issue on Languages, Compilers, Tools, and Theory of Embedded Systems: Part 1",2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096828742&doi=10.1145%2f3417732&partnerID=40&md5=dfe4a29285b62e8f338f6f1687362938,[No abstract available],,
Application of Logical Sub-networking in Congestion-aware Deadlock-free SDmesh Routing,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093979949&doi=10.1145%2f3387928&partnerID=40&md5=c62006ce5eba701ed56cbe0de8343d26,"An adaptive routing helps in evading early network saturation by steering data packets through the less congested area at the oppressive loaded situation. However, performances of adaptive routing are not always promising under all circumstances. Say for, given more freedom in choosing an alternate route on non-minimal paths for a substantially loaded network even may result in worsening network performances due to following longer route under adaptive routing. Here, underlying topology facilitates routing by offering more alternate short-cut routes on minimal or quasi-minimal paths. This work presents a congestion-aware (CA) adaptive routing for one-hop diagonally connected subnet-based mesh (SDmesh) network aiming to facilitate both performances and routing flexibility simultaneously. Our proposed technique on the selected system facilitates packet routing, offering more options in choosing an output link from minimal or quasi-minimal paths and hence helps in lowering packet delay by shortening the length of traversed traffic under the oppressive loaded situation. Furthermore, we have also employed a congestion-aware virtual input crossbar router aiming to split the entire network into two distinct logically separated sub-networks. It facilitates preserving important routing properties like deadlock, live-lock fairness, and other essential routing constraints. Experiments, conducted over two 8×8-and 12×12-sized networks, show an average improvement of 25-87.5% saturated latency and 60-83% throughput improvement under uniform traffic patterns for the proposed CA routing compared to centralized adaptive XY routing. Experimental results on application-specific PARSEC and SPLASH2 benchmark suites show an average of 22-50% latency and 23-30% throughput improvements by the proposed technique compared to centralized XY routing on the baseline mesh network. Moreover, experiments were also carried out to check the performance of the proposed routing method with different newly proposed deadlock-free adaptive routing approaches over the same subnet-based diagonal mesh (SDmesh) network and reported.  © 2020 ACM.",congestion-aware NoC design; Network-on-chip; NoC routing; performance,Benchmarking; Mesh generation; Adaptive routing; Alternate routes; Application specific; Congestion-aware; Deadlock-free adaptive routing; Network saturation; Routing flexibility; Throughput improvement; Network routing
LAMBDA: Lightweight Assessment of Malware for emBeddeD Architectures,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093931177&doi=10.1145%2f3390855&partnerID=40&md5=4af5482db0bd138c6700c36193f804d2,"Security is a critical aspect in many of the latest embedded and IoT systems. Malware is one of the severe threats of security for such devices. There have been enormous efforts in malware detection and analysis; however, occurrences of newer varieties of malicious codes prove that it is an extremely difficult problem given the nature of these surreptitious codes. In this article, instead of addressing a general solution, we aim at malware detection for platforms that have more than one core for performance enhancement. We investigate the utility of multiple cores from the point of view of security, where one of the cores operate as a watchdog. We define a notion of a new metric called LAMBDA (Lightweight Assessment of Malware for emBeddeD Architectures), denoted by λ, indicating a conceptual boundary between the programs which are allowed to run on a given platform, with the codes that are suspected as malwares. The metric λ is computed using carefully chosen monitors or features, which are tuples of high-level programs representing OS resources, along with low-level hardware performance counters. In comparison to heavy-weight machine learning techniques, we use an online hypothesis testing, in the form of t-test, to classify a given program-under-test. For applications where security is of prime concern, we propose an additional step based on multivariate analysis to classify the unknown programs that are closer to the threshold with a high degree of confidence. We present experimental results focusing on an ARM-based platform which validate that the proposed approach provides a lightweight, accurate assessment of malware codes for embedded platforms. In addition to it, we also present a security analysis to show the difficulty of a mimicry attack attempting to bypass LAMBDA.  © 2020 ACM.",embedded systems; hardware performance counters; hypothesis testing; Malware detection,Application programs; Embedded systems; Learning systems; Multivariant analysis; Software testing; Degree of confidence; Embedded architecture; Hardware performance counters; High-level program; Hypothesis testing; Machine learning techniques; Multi variate analysis; Performance enhancements; Malware
Game-Based Task Offloading of Multiple Mobile Devices with QoS in Mobile Edge Computing Systems of Limited Computation Capacity,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093914563&doi=10.1145%2f3398038&partnerID=40&md5=6b86c8d005a4d688bbfaab0be8e79d88,"Mobile edge computing (MEC) is becoming a promising paradigm of providing computing servers, like cloud computing, to Edge node. Compared to cloud servers, MECs are deployed closer to mobile devices (MDs) and can provide high quality-of-service (QoS; including high bandwidth, low latency, etc) for MDs with computation-intensive and delay-sensitive tasks. Faced with many MDs with high QoS requirements, MEC with limited computation capacity should consider how to allocate the computing resources to MDs to maximize the number of served MDs. Besides, for each MD, he/she wants to minimize the energy consumption within an acceptance delay range. To solve these issues, we propose a Game-based Computation Offloading (GCO) algorithm including a task offloading profile of MEC and the transmission power controlling of each MD. Specifically, we propose a Greedy-Pruning algorithm to determine the MDs that can offload the tasks to MEC. Meanwhile, each MD competes the computing resources by using his/her transmission power-controlling strategy. We illustrate the problem of task offloading for multi-MD as a non-cooperative game model, in which the information of each player (MDs) is incomplete for others and each player wishes to maximize his/her own benefit. We prove the existence of the Nash equilibrium solution of our proposed game model. Then, it is proved that the transmission power solution sequence obtained from GCO algorithm converges to the Nash equilibrium solution. Extensive simulated experiments are shown and the comparison experiments with the state-of-the-art and benchmark solutions validate and show the feasibility of the proposed method.  © 2020 ACM.",Mobile edge computing; Nash equilibrium; non-cooperative game theory; power controlling; task offloading,Computer games; Edge computing; Energy utilization; Game theory; Power control; Transmissions; Benchmark solutions; Computation capacity; Computation intensives; Computation offloading; Computing resource; Controlling strategies; Noncooperative game; Simulated experiments; Quality of service
Energy-efficient Real-time Scheduling on Multicores: A Novel Approach to Model Cache Contention,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093973528&doi=10.1145%2f3399413&partnerID=40&md5=e2b6b345281ecc4f634fd8782594413e,"With the increasing demand for higher performance, the adoption of multicores has been a major stepping stone in the evolution of hard real-time systems. Though the computational bandwidth is increased due to parallel processing, the indispensable interactivity between the hierarchical memory sub-system and multiple cores has further aggravated the already complex worst case execution time (WCET) analysis of tasks. Furthermore, caches have the biggest influence on task execution time, and the inclusion of shared caches further increases the unpredictability of the system. Cache partitioning techniques have been proposed as a counter-measure to decouple the shared cache latency from the WCET. However, existing energy-efficient scheduling algorithms are oblivious to the unpredictable nature of shared caches or cache partitioning techniques, thus, diminishing their applicability to real-world systems. Without considering inter-task cache contention, directly using existing algorithms or attempting to allocate and schedule a taskset with cache-partition assignments can result in cache violations. To overcome this dilemma, we propose a novel approach to model inter-task cache contention as a dependency graph to be used by well-established algorithms to minimize energy consumption. Extensive simulations demonstrate the effectiveness of our approach to minimize energy consumption while also avoiding cache violations.  © 2020 ACM.",cache scheduling; DAG; Embedded systems; energy-efficiency; real-time scheduling,Energy efficiency; Energy utilization; Graph algorithms; Interactive computer systems; Cache partitioning; Computational bandwidth; Extensive simulations; Hard real-time systems; Hierarchical memory; Parallel processing; Real - time scheduling; Worst-case execution time analysis; Real time systems
DSTL: A Demand-Based Shingled Translation Layer for Enabling Adaptive Address Mapping on SMR Drives,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093924096&doi=10.1145%2f3391892&partnerID=40&md5=c969577771a742a54d9732825e51f63d,"Shingled magnetic recording (SMR) is regarded as a promising technology for resolving the areal density limitation of conventional magnetic recording hard disk drives. Among different types of SMR drives, drive-managed SMR (DM-SMR) requires no changes on the host software and is widely used in today's consumer market. DM-SMR employs a shingled translation layer (STL) to hide its inherent sequential-write constraint from the host software and emulate the SMR drive as a block device via maintaining logical to physical block address mapping entries. However, because most existing STL designs do not simultaneously consider the access pattern and the data update frequency of incoming workloads, those mapping entries maintained within the STL cannot be effectively managed, thus inducing unnecessary performance overhead. To resolve the inefficiency of existing STL designs, this article proposes a demand-based STL (DSTL) to simultaneously consider the access pattern and update frequency of incoming data streams to enhance the access performance of DM-SMR. The proposed design was evaluated by a series of experiments, and the results show that the proposed DSTL can outperform other SMR management approach by up to 86.69% in terms of read/write performance.  © 2020 ACM.",demand-based; shingled magnetic recording; shingled translation layer; SMR; STL,Data streams; Delta modulation; Magnetic recording; Mapping; Access patterns; Address mappings; Areal densities; Consumer market; Hard Disk Drive; Read/write performance; Shingled magnetic recording (SMR); Translation layers; Digital storage
Applying Multiple Level Cell to Non-volatile FPGAs,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093979408&doi=10.1145%2f3400885&partnerID=40&md5=8ae6164a6ab7bb94705f31cdeedd43f4,"Static random access memory-(SRAM) based field programmable gate arrays (FPGAs) are currently facing challenges of limited capacity and high leakage power. To solve this problem, non-volatile memory (NVM) is proposed as the alternative to build non-volatile FPGAs (NVFPGAs). Even though the feasibility of NVFPGA has been confirmed, the utilization of multiple level cells (MLCs) has not been fully exploited yet. In this article, we study architecture of MLC-based NVFPGAs, and propose five cluster structures. To give detailed comparisons and extensive discussions, we conduct experiments for area, performance and leakage power evaluation. Based on explorations of the characteristics of MLC-based NVFPGAs, we further present MLC-aware timing-driven packing method to improve delay. In critical paths, our proposed method reduces the overhead of the additional delay in slow MLC cells. Experiments show that, compared to SRAM-based FPGAs, the proposed architecture with the proposed CAD flow can reduce the area, critical path delay and leakage power by 31%, 10%, and 95%, respectively.  © 2020 ACM.",CAD flow; FPGA; multiple level cell; non-volatile memory,Cluster computing; Computer aided logic design; Field programmable gate arrays (FPGA); Memory architecture; Cluster structure; Critical path delays; Limited capacity; Multiple-level cells; Non-volatile memory; Packing method; Proposed architectures; Static random access memory; Static random access storage
Firmness Analysis of Real-time Tasks,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093916089&doi=10.1145%2f3398328&partnerID=40&md5=d9cecaf9954b6aa4759a41952a2f3243,"(m,k)-firm real-time tasks require meeting the deadline of at least m jobs out of any k consecutive jobs. When compared to hard real-time tasks, (m,k)$-firm tasks open up the possibility of tighter resource-dimensioning in implementations. Firmness analysis verifies the satisfaction of (m,k)-firmness conditions. Scheduling policies under which a set of periodic tasks runs on a resource influence the number of deadline missed jobs. Therefore, the nature of the firmness analysis problem depends on scheduling policies. In this work, we present Firmness Analysis (FAn) methods for three common scheduling policies-synchronous and asynchronous Static Priority Preemptive (SPP) policies and Time Division Multiple Access (TDMA). We first introduce the Balloon and Rake problem-the problem of striking the maximum number of balloons in a balloon line with a rake. We show that the common core of firmness analysis problems can be abstracted as the Balloon and Rake problem. Next, we prove that the Finite Point method is a solution to the Balloon and Rake problem. We illustrate how existing FAn methods for the TDMA and asynchronous SPP policies can be adapted to use the same solution framework for the Balloon and Rake problem. Using the solution of the Balloon and Rake problem, we adapt the existing FAn methods to synchronous SPP scheduling policies. The scalability of the FAn methods is compared with that of a timed-automata approach, a brute-force approach, and a Mixed Integer Linear Programing method. The FAn methods scale substantially better to firmness analysis problem instances with a large k and a high number of tasks.  © 2020 ACM.","(m, k)-firm; Balloon and Rake problem; Deadline miss; finite point method; firmness analysis",Balloons; Integer programming; Scheduling; Analysis problems; Brute-force approach; Finite point method; Hard real-time task; Mixed integer linear programing; Resource dimensioning; Scheduling policies; Time  division multiple accesses (TDMA); Time division multiple access
Network-level Design Space Exploration of Resource-constrained Networks-of-Systems,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093982754&doi=10.1145%2f3387918&partnerID=40&md5=49bfaf47eeaa39a2bc99e403f01f9cf7,"Driven by recent advances in networking and computing technologies, distributed application scenarios are increasingly deployed on resource-constrained processing platforms. This includes networked embedded and cyber-physical systems as well as edge computing in mobile applications and the Internet of Things (IoT). In such resource-constrained Networks-of-Systems (NoS), computation and communication workloads need to be carefully co-optimized yet are tightly coupled. How to optimally partition and schedule application tasks among an appropriately designed NoS architecture requires a simultaneous consideration of design parameters from applications and processing platforms all the way to network configurations. Traditionally, however, systems and networks are designed in isolation and combined in an ad hoc manner, which ignores joint effects and optimization opportunities. To systematically explore and optimize NoS design spaces, a higher level of design abstraction on top of traditional system and network design is required. In this article, we propose a novel network-level design methodology for resource-constrained NoS optimization and design space exploration. A key component in such a design flow is fast yet accurate network/system co-simulation to rapidly evaluate NoS parameters with high fidelity. We first introduce a novel NoS simulator (NoSSim) that integrates source-level simulation models of applications with a host-compiled system simulation platform and a reconfigurable network simulation backplane to accurately capture system and network interactions. The co-simulation platform is further combined with model generation tools and a multi-objective genetic search algorithm to provide a comprehensive and fully automated NoS design space exploration framework. Finally, we apply our network-level design flow on several state-of-art IoT/mobile design case studies. Results show that NoSSim can achieve more than 86% simulation accuracy on average as compared to a real-world edge device cluster, where sensitivities to various design parameters are faithfully captured with high fidelity. When applying our network-level design space exploration methodology, design decisions are automatically optimized, where non-obvious NoS configurations are discovered outperforming manually designed solutions by more than 45%.  © 2020 ACM.",design space exploration; networks-of-systems; Source-level simulation,Arts computing; Constrained optimization; Embedded systems; Genetic algorithms; Internet of things; Simulation platform; Design space exploration; Distributed applications; Genetic search algorithms; Internet of thing (IOT); Model generation tools; Optimization and design; Reconfigurable network; Resource-constrained network; Design
Montgomery Multiplication for Public Key Cryptography on MSP430X,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089412095&doi=10.1145%2f3387919&partnerID=40&md5=4e0a60d7830290d5a868309e7bc2d9e5,"For traditional public key cryptography and post-quantum cryptography, such as elliptic curve cryptography and supersingular isogeny key encapsulation, modular multiplication is the most performance-critical operation among basic arithmetic of these cryptographic schemes. For this reason, the execution timing of such cryptographic schemes, which may highly determine that the service availability for low-end microprocessors (e.g., 8-bit AVR, 16-bit MSP430X, and 32-bit ARM Cortex-M), mainly relies on the efficiency of modular multiplication on target embedded processors. In this article, we present new optimal modular multiplication techniques based on the interleaved Montgomery multiplication on 16-bit MSP430X microprocessors, where the multiplication part is performed in a hardware multiplier and the reduction part is performed in a basic arithmetic logic unit (ALU) with the optimal modular multiplication routine, respectively. This two-step approach is effective for the special modulus of NIST curves, SM2 curves, and supersingular isogeny key encapsulation. We further optimized the Montgomery reduction by using techniques for ""Montgomery-friendly""prime. This technique significantly reduces the number of partial products. To demonstrate the superiority of the proposed implementation of Montgomery multiplication, we applied the proposed method to the NIST P-256 curve, of which the implementation improves the previous modular multiplication operation by 23.6% on 16-bit MSP430X microprocessors and to the SM2 curve as well (first implementation on 16-bit MSP430X microcontrollers). Moreover, secure countermeasures against timing attack and simple power analysis are also applied to the scalar multiplication of NIST P-256 and SM2 curves, which achieve the 8,582,338 clock cycles (0.53 seconds@16 MHz) and 10,027,086 clock cycles (0.62 seconds@16 MHz), respectively. The proposed Montgomery multiplication is a generic method that can be applied to other cryptographic schemes and microprocessors with minor modifications. © 2020 ACM.",Montgomery multiplication; MSP430X; public key cryptography; software implementation,Clocks; Logic circuits; Public key cryptography; Quantum cryptography; Arithmetic logic unit; Elliptic curve cryptography; Modular Multiplication; Modular multiplication techniques; Montgomery multiplication; Post quantum cryptography; Scalar multiplication; Simple power analysis; Side channel attack
Adapting Recursive Sinusoidal Software Oscillators for Low-power Fixed-point Processors,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089440292&doi=10.1145%2f3378559&partnerID=40&md5=b318e0bcd0f5ea9cafd579b7f9032f93,"The growing field of the Internet of Things relies at the bottom on components with very scarce computing resources that currently do not allow complex processing of sensed data. Any computation involving Fast Fourier Transforms (FFT), Wavelet Transforms (WT), or simple sines and cosines is considered impractical on low-end devices due to the lack of floating point and math libraries. This article presents new techniques that make it possible to use these functions also on severely constrained target platforms. Current literature abounds with schemes to compute sine and cosine functions, with focus on speed, hardware footprint, software size, target type, or precision. Even so, there is no practical exploration of the design space available for embedded devices with limited resources, in particular when only integer operations are possible. We select an efficient set of recursive sine and cosine generators and measure the frequency, amplitude, and phase error over a wide parameter range. We show that their simplicity allows them to be implemented on the most bare targets with good precision, reducing power consumption and size while being the fastest on integer-only processors. We also introduce specially tailored FFT and WT algorithms and show that they are usable in practice while having an extremely small code footprint, good precision, and high speed. © 2020 ACM.",cosine; fourier transform; intermediate euler; IoT; sine; wavelet transform,Cosine transforms; Data handling; Digital arithmetic; Wavelet transforms; Complex processing; Computing resource; Cosine functions; Efficient sets; Embedded device; Floating points; Integer operations; Parameter range; Fast Fourier transforms
DyVEDeep: Dynamic Variable Effort Deep Neural Networks,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089409477&doi=10.1145%2f3372882&partnerID=40&md5=5e4231885673fcf90dd029bac39661cc,"Deep Neural Networks (DNNs) have advanced the state-of-the-art in a variety of machine learning tasks and are deployed in increasing numbers of products and services. However, the computational requirements of training and evaluating large-scale DNNs are growing at a much faster pace than the capabilities of the underlying hardware platforms that they are executed upon. To address this challenge, one promising approach is to exploit the error resilient nature of DNNs by skipping or approximating computations that have negligible impact on classification accuracy. Almost all prior efforts in this direction propose static DNN approximations by either pruning network connections, implementing computations at lower precision, or compressing weights. In this work, we propose <u>Dy</u>namic <u>V</u>ariable <u>E</u>ffort <u>Deep</u> Neural Networks (DyVEDeep) to reduce the computational requirements of DNNs during inference. Complementary to the aforementioned static approaches, DyVEDeep is a dynamic approach that exploits heterogeneity in the DNN inputs to improve their compute efficiency with comparable classification accuracy and without requiring any re-training. DyVEDeep equips DNNs with dynamic effort mechanisms that identify computations critical to classifying a given input and focus computational effort only on the critical computations, while skipping or approximating the rest. We propose three dynamic effort mechanisms that operate at different levels of granularity viz. neuron, feature, and layer levels. We build DyVEDeep versions of six popular image recognition benchmarks (CIFAR-10, AlexNet, OverFeat, VGG-16, SqueezeNet, and Deep-Compressed-AlexNet) within the Caffe deep-learning framework. We evaluate DyVEDeep on two platforms-a high-performance server with a 2.7 GHz Intel Xeon E5-2680 processor and 128 GB memory, and a low-power Raspberry Pi board with an ARM Cortex A53 processor and 1 GB memory. Across all benchmarks, DyVEDeep achieves 2.47×-5.15× reduction in the number of scalar operations, which translates to 1.94×-2.23× and 1.46×-3.46× performance improvement over well-optimized baselines on the Xeon server and the Raspberry Pi, respectively, with comparable classification accuracy. © 2020 ACM.",deep neural networks; DyVEDeep,Benchmarking; Deep learning; Deep neural networks; Dynamics; Image recognition; Classification accuracy; Computational effort; Computational requirements; Dynamic approaches; Dynamic variables; Learning frameworks; Network connection; Products and services; Neural networks
Reliable and Secure Design-Space-Exploration for Cyber-Physical Systems,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089432325&doi=10.1145%2f3387927&partnerID=40&md5=32a9776e9920f9da330070c37f80b9c3,"Given the widespread deployment of cyber-physical systems and their safety-critical nature, reliability and security guarantees offered by such systems are of paramount importance. While the security of such systems against sensor attacks have garnered significant attention from researchers in recent times, improving the reliability of a control software implementation against transient environmental disturbances need to be investigated further. Scalable formal methods for verification of actual control performance guarantee offered by software implementations of control laws in the face of sensory faults have been explored in recent work [20]. However, the formal verification of the improvement of system reliability by incorporating sensor fault mitigation techniques like Kalman filtering [29] and sensor fusion [18, 52] remains to be explored. Moreover, system designers face complex tradeoff choices for deciding upon the usage of fault and attack mitigation techniques and scheduling them on available system resources as they incur extra computation load. In the present work, our contributions are threefold. We formally analyze the actual performance guarantee of control software implementations enabled with additional fault mitigation techniques. We consider task-level models of such implementations enabled with security and fault tolerance primitives and construct a timed automata-based model which checks for schedulability on heterogeneous multi-core platforms. We leverage these methodologies in the context of a novel Design-Space-Exploration (DSE) framework that considers target reliability and security guarantees for a control system and computes schedulable design options while considering well-known platform-level security improvement and fault mitigation techniques. We validate our contributions over several case studies from the automotive domain. © 2020 ACM.",Design-space-exploration; quality-of-control; real-time scheduling; system synthesis; timed automata,Control theory; Cyber Physical System; Embedded systems; Fault tolerance; Formal verification; Safety engineering; Automotive domains; Control performance; Environmental disturbances; Heterogeneous Multi-Cores; Mitigation techniques; Performance guarantees; Security improvement; Software implementation; Software reliability
Design Space Exploration for Ultra-Low-Energy and Secure IoT MCUs,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088888886&doi=10.1145%2f3384446&partnerID=40&md5=79945e3f639140c630de855b30972ec8,"This article explores the design space of secure communication in ultra-low-energy IoT devices based on Micro-Controller Units (MCUs). It tries to identify, benchmark, and compare security-related design choices in a Commercial-Off-The-Shelf (COTS) embedded IoT system which contributes to the energy consumption. We conduct a study over a large group of software crypto algorithms: symmetric, stream, hash, AEAD, MAC, digital signature, and key exchange. A comprehensive report of the targeted optimization attributes (memory, performance, and specifically energy) will be presented from over 450 experiments and 170 different crypto source codes. The article also briefly explores a few system-related choices which can affect the energy consumption of secure communication, namely, architecture choice, communication bandwidth, signal strength, and processor frequency. In the end, the article gives an overview of the obtained results and the contribution of all. Finally, it shows, in a case study, how the results could be utilized to have a secure communication in an exemplary IoT device. This article gives IoT designers insight into ultra-low-energy security, helps them to choose appropriate cryptographic algorithms, reduce trial-and-error of alternatives, save effort, and hence cut the design costs. © 2020 ACM.",benchmarking; Ciphers; cryptography; cyber-physical systems; embedded software; energy consumption,Commercial off-the-shelf; Computer architecture; Cryptography; Energy security; Energy utilization; Microcontrollers; Secure communication; Communication bandwidth; Cryptographic algorithms; Design space exploration; Micro controller units; Signal strengths; Targeted optimization; Trial and error; Ultra low energy; Internet of things
DEEPEYE: A Deeply Tensor-Compressed Neural Network for Video Comprehension on Terminal Devices,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089413000&doi=10.1145%2f3381805&partnerID=40&md5=02a00578c890857258c73e15873d5899,"Video object detection and action recognition typically require deep neural networks (DNNs) with huge number of parameters. It is thereby challenging to develop a DNN video comprehension unit in resource-constrained terminal devices. In this article, we introduce a deeply tensor-compressed video comprehension neural network, called DEEPEYE, for inference on terminal devices. Instead of building a Long Short-Term Memory (LSTM) network directly from high-dimensional raw video data input, we construct an LSTM-based spatio-temporal model from structured, tensorized time-series features for object detection and action recognition. A deep compression is achieved by tensor decomposition and trained quantization of the time-series feature-based LSTM network. We have implemented DEEPEYE on an ARM-core-based IOT board with 31 FPS consuming only 2.4W power. Using the video datasets MOMENTS, UCF11 and HMDB51 as benchmarks, DEEPEYE achieves a 228.1× model compression with only 0.47% mAP reduction; as well as 15k× parameter reduction with up to 8.01% accuracy improvement over other competing approaches. © 2020 ACM.",object detection and action recognition; structured; tensorized compression; tensorized time-series features; trained quantization; Video comprehension on board,Deep neural networks; Object detection; Object recognition; Tensors; Time series; Accuracy Improvement; Action recognition; Model compression; Parameter reduction; Spatio-temporal models; Tensor decomposition; Time series features; Video object detections; Long short-term memory
TECS Editorial: Rethinking and Re-evaluating in the Time of Crisis,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089427219&doi=10.1145%2f3395923&partnerID=40&md5=fe6d3aebce1d76ed38df8588ead74f76,[No abstract available],,
FFConv: An FPGA-based accelerator for fast convolution layers in convolutional neural networks,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082386484&doi=10.1145%2f3380548&partnerID=40&md5=9a53cee399947f35e209eba6702d9fa6,"Image classification is known to be one of the most challenging problems in the domain of computer vision. Significant research is being done on developing systems and algorithms improving accuracy, performance, area, and power consumption for related problems. Convolutional Neural Networks (CNNs) have shown to give outstanding accuracies for problems such as image classification, object detection, and semantic segmentation. While CNNs are pioneering the development of high accuracy systems, their excessive computational complexity presents a barrier for a more permeated deployment. Although Graphical Processing Units (GPUs), due to theirmassively parallel architecture, have shown to give performance orders ofmagnitude better than general purpose processors, the former are limited by their high power consumption and generality. Consequently, Field Programmable Gate Arrays (FPGAs) are being explored to implement CNN architectures, as they also provide massively parallel logic resources but with a relatively lower power consumption than GPUs. In this article, we present FFConv, an efficient FPGA-based fast convolutional layer accelerator for CNNs. We design a pipelined, high-throughput convolution engine based on the Winograd minimal filtering (also called Fast Convolution) algorithms for computing the convolutional layers of three popular CNN architectures: VGG16, Alexnet, and Shufflenet. We implement our accelerator on a Virtex-7 FPGA platform where we exploit the computational parallelization to the maximum while exploring optimizations aimed at improving performance. The resultant design loses only 0.43%, 0.47%, and 0.61% Top-1 classification accuracy for VGG16, Alexnet, and Shufflenet-v1, respectively, while significantly improving throughput, resource, and power efficiency compared to previous state-of-the-art designs. © 2020 BMJ Publishing Group. All rights reserved.",Convolutional neural networks; FPGA; Hardware acceleration,Convolution; Electric power utilization; Field programmable gate arrays (FPGA); General purpose computers; Graphics processing unit; Image classification; Image segmentation; Integrated circuit design; Multilayer neural networks; Network architecture; Object detection; Parallel architectures; Program processors; Semantics; Classification accuracy; General purpose processors; Graphical processing unit (GPUs); Hardware acceleration; High power consumption; Improving performance; Lower-power consumption; Semantic segmentation; Convolutional neural networks
Compiling for the worst case: Memory allocation for multi-task and multi-core hard real-time systems,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082399894&doi=10.1145%2f3381752&partnerID=40&md5=f0cae494427b948d5940f66e91d5b4a6,"Modern embedded hard real-time systems feature multiple tasks running on multiple processing cores. Schedulability analysis of such systems is usually performed on an abstract system level with each task being represented as a black box with fixed timing properties. If timing constraints are violated, then optimizing the system on a code-level to achieve schedulability is a tedious task. To tackle this issue, we propose an extension to the WCET-aware C Compiler framework WCC. We integrated an optimization framework based on Integer-Linear Programming into the WCC that is able to optimize a multi-core system with multiple tasks running on each core with regards to its schedulability.We evaluate the framework by providing two approaches on a schedulability aware static Scratchpad Memory (SPM) allocation: one based on Integer-Linear Programming (ILP) and one based on a genetic algorithm. © 2020 BMJ Publishing Group. All rights reserved.",Multi-core systems; Scheduling analysis; WCET optimization,C (programming language); Embedded systems; Genetic algorithms; Integer programming; Interactive computer systems; Memory architecture; Multicore programming; Embedded hard real-time system; Hard real-time systems; Integer Linear Programming; Multi-core systems; Multiple processing cores; Optimization framework; Schedulability analysis; Scheduling analysis; Real time systems
Synthesis of flexible accelerators for early adoption of ring-LWE post-quantum cryptography,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082382591&doi=10.1145%2f3378164&partnerID=40&md5=38e7c6337aae3b3565581c18c8fd0793,"The advent of the quantum computer makes current public-key infrastructure insecure. Cryptography community is addressing this problem by designing, efficiently implementing, and evaluating novel public-key algorithms capable of withstanding quantum computational power. Governmental agencies, such as NIST, are promoting standardization of quantum-resistant algorithms that is expected to run for 7 years. Several modern applications must maintain permanent data secrecy; therefore, they ultimately require the use of quantum-resistant algorithms. Because algorithms are still under scrutiny for eventual standardization, the deployment of the hardware implementation of quantum-resistant algorithms is still in early stages. In this article, we propose a methodology to design programmable hardware accelerators for lattice-based algorithms, and we use the proposed methodology to implement flexible and energy efficient post-quantum cache-based accelerators for NewHope, Kyber, Dilithium, Key Consensus from Lattice (KCL), and R.EMBLEM submissions to the NIST standardization contest. To the best of our knowledge, we propose the first efficient domain-specific, programmable cache-based accelerators for lattice-based algorithms.We design a single accelerator for a common kernel among various schemes with different kernel sizes, i.e., loop count, and data types. This is in contrast to the traditional approach of designing one special purpose accelerators for each scheme. We validate our methodology by integrating our accelerators into an HLS-based SoC infrastructure based on the X86 processor and evaluate overall performance. Our experiments demonstrate the suitability of the approach and allow us to collect insightful information about the performance bottlenecks and the energy efficiency of the explored algorithms. Our results provide guidelines for hardware designers, highlighting the optimization points to address for achieving the highest energy minimization and performance increase. At the same time, our proposed design allows us to specify and execute new variants of lattice-based schemes with superior energy efficiency compared to the main application processor without changing the hardware acceleration platform. For example, we manage to reduce the energy consumption up to 2.1× and energydelay product (EDP) up to 5.2× and improve the speedup up to 2.5×. © 2020 BMJ Publishing Group. All rights reserved.",Cache architecture; Domain specific acceleration; Ideal lattices; Post-quantum cryptography lattice-based cryptography; Public key cryptography,Acceleration; Computational efficiency; Computer hardware; Energy efficiency; Energy utilization; Integrated circuit design; Public key cryptography; Quantum computers; Quantum cryptography; Standardization; System-on-chip; Application processors; Cache architecture; Domain specific; Hardware implementations; Lattice-based cryptography; Performance bottlenecks; Post quantum cryptography; Public key infrastructure; Quantum efficiency
Energy modeling for the bluetooth low energy protocol,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082382744&doi=10.1145%2f3379339&partnerID=40&md5=ea6f0c57f02d9aadb7138d6561a505c1,"Bluetooth Low Energy (BLE) is a wireless protocol optimized for low-power communication. To design energy-efficient devices, the protocol provides a number of parameters that need to be optimized within an energy, latency, and throughput design space. Therefore, an energy model that can predict the energy consumption of a BLE-based wireless device for different parameter value settings is needed. As BLE differs from the well-known Bluetooth Basic Rate (BR) significantly, models for Bluetooth BR cannot be easily applied to the BLE protocol. In past years, there have been a couple of proposals on energy models for BLE. However, none of them can model all the operating modes of the protocol. This article presents an energy model of the BLE protocol, which allows the computation of a device's power consumption in all possible operating modes. To the best of our knowledge, our proposed model is not only one of the most accurate ones known so far (because it accounts for all protocol parameters), but it is also the only one thatmodels all the operating modes of BLE. Based on this model, guidelines for system designers are presented that help choose the right parameters for optimizing the energy consumption. The model is publicly available as a software library for download. © 2020 BMJ Publishing Group. All rights reserved.",Bluetooth; Bluetooth low energy; Energy model; Energy modelling; MANETs; Sensor networks; Wireless communication; Wireless networks,Bluetooth; Energy efficiency; Energy utilization; Mobile ad hoc networks; Sensor networks; Wireless networks; Wireless sensor networks; Bluetooth low energies (BTLE); Energy model; Energy modelling; MANETs; Wireless communications; Low power electronics
A machine learning methodology for cache memory design based on dynamic instructions,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082395479&doi=10.1145%2f3376920&partnerID=40&md5=a74bc1c4a19368706f69160749a3d846,"Cache memories are an essential component of modern processors and consume a large percentage of their power consumption. Its efficacy depends heavily on the memory demands of the software. Thus, finding the optimal cache for a particular program is not a trivial task and usually involves exhaustive simulation. In this article, we propose a machine learning-based methodology that predicts the optimal cache reconfiguration for any given application, based on its dynamic instructions. Our evaluation shows that our methodology reaches 91.1% accuracy. Moreover, an additional experiment shows that only a small portion of the dynamic instructions (10%) suffices to reach 89.71% accuracy. © 2020 BMJ Publishing Group. All rights reserved.",Cache memory; Cache memory design; Classification; Machine learning; Supervised learning,Buffer storage; Classification (of information); Learning systems; Machine learning; Supervised learning; Additional experiments; Cache reconfigurations; Dynamic instructions; Exhaustive simulation; Memory design; Modern processors; ON dynamics; Cache memory
Automated model-based optimization of data-adaptable embedded systems,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079570133&doi=10.1145%2f3372142&partnerID=40&md5=0d6f93731f7c29f873cdc2d4bb3a0a0c,"Dynamic data-driven applications such as object tracking, surveillance, and other sensing and decision applications are largely dependent on the characteristics of the data streams on which they operate. The underlying models and algorithms of data-driven applications must continually adapt at runtime to changes in data quality and availability to meet both functional and designer-specified performance requirements. Given the dynamic nature of these applications, point solutions produced by traditional design tools cannot be expected to perform adequately across varying execution scenarios. Additionally, the increasing diversity and interdependence of application requirements complicates the design and optimization process. To assist designers of data-driven applications, we present a modeling and optimization framework that enables developers to model an application’s data sources, tasks, and exchanged data tokens; specify application requirements through high-level design metrics and fuzzy logic–based optimization rules; and define an estimation framework to automatically optimize the application at runtime. We demonstrate the modeling and optimization process via an example application for video-based vehicle tracking and collision avoidance. We analyze the benefits of runtime optimization by comparing the performance of static point solutions to dynamic solutions over five distinct execution scenarios, showing improvements of up to 74% for dynamic over static configurations. Further, we show the benefits of using fuzzy logic–based rules over traditional weighted functions for the specification and evaluation of competing high-level metrics in optimization. © 2020 Association for Computing Machinery.",Design space exploration; Dynamic data-driven systems; Dynamic optimization; Fuzzy logic–based optimization rules; Software modeling,Availability; Computer circuits; Embedded systems; Fuzzy logic; Object tracking; Design space exploration; Dynamic data; Dynamic optimization; Optimization rules; Software model; Data streams
ELSA: A throughput-optimized design of an LSTM accelerator for energy-constrained devices,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079570378&doi=10.1145%2f3366634&partnerID=40&md5=82bf16d2d866928b4bb06d1ee4b9f51d,"The next significant step in the evolution and proliferation of artificial intelligence technology will be the integration of neural network (NN) models within embedded and mobile systems. This calls for the design of compact, energy efficient NN models in silicon. In this article, we present a scalable application-specific integrated circuit (ASIC) design of an energy-efficient Long Short-Term Memory (LSTM) accelerator, named ELSA, which is suitable for energy-constrained devices. It includes several architectural innovations to achieve small area and high energy efficiency. To reduce the area and power consumption of the overall design, the compute-intensive units of ELSA employ approximate multiplications and still achieve high performance and accuracy. The performance is further improved through efficient synchronization of the elastic pipeline stages to maximize the utilization. The article also includes a performance model of ELSA, as a function of the hidden nodes and timesteps, permitting its use for the evaluation of any LSTM application. ELSA was implemented in register transfer level (RTL) and was synthesized and placed and routed in 65nm technology. Its functionality is demonstrated for language modeling—a common application of LSTM. ELSA is compared against a baseline implementation of an LSTM accelerator with standard functional units and without any of the architectural innovations of ELSA. The article demonstrates that ELSA can achieve significant improvements in power, area, and energy-efficiency when compared to the baseline design and several ASIC implementations reported in the literature, making it suitable for use in embedded systems and real-time applications. © 2020 Association for Computing Machinery.",Accelerator; Deep learning; Domain-specific architecture; Embedded systems; Low power; LSTM; Recurrent neural network,Application specific integrated circuits; Constrained optimization; Deep learning; Deep neural networks; Embedded systems; Energy efficiency; Integrated circuit design; Low power electronics; Modeling languages; Natural language processing systems; Particle accelerators; Real time systems; Recurrent neural networks; Architectural innovation; Artificial intelligence technologies; Domain specific architectures; High energy efficiency; Low Power; LSTM; Real-time application; Register transfer level; Long short-term memory
Design and implementation of an escape analysis in the context of safety-critical embedded systems,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079573719&doi=10.1145%2f3372133&partnerID=40&md5=a016035ff719829de5619dfa6673fb03,"The use of a managed, type-safe language such as Standard ML, Ada Ravenscar, or Java in hard real-time and embedded systems offers productivity, safety, and dependability benefits at a reasonable cost. Static software systems, that is systems in which all relevant resource entities such as threads and their priorities, for instance, and the entire source code are known ahead of time, are particularly interesting for the deployment in safety-critical embedded systems: Code verification is rather maintainable in contrast to dynamic systems. Additionally, static analyses can incorporate information from all software and system layers to assist compilers in emitting code that is well suited to an application on a particular hardware device. It was shown in the past that a program composed in type-safe Java in combination with a static system setup can be as efficient as one that is written in C [30], which is still the most widely used language in the embedded domain. Escape analysis (EA) is one of several static-analysis techniques. It supports, for instance, runtime efficiency by enabling automated stack allocation of objects. In addition, Stilkerich et al. [27, 28] have argued that EA enables further applications in safety-critical embedded systems such as the computation of memory classes stated in the Real-Time Specification for Java (RTSJ) [6]. EA can be applied to any programming language but the quality of its results greatly benefits from the properties of a type-safe language. Notably, embedded multicore devices can positively be affected by the use of EA. Thus, we explore an ahead-of-time (AOT) escape analysis in the context of the KESO JVM featuring a Java AOT compiler targeting (deeply) embedded (hard) real-time systems. © 2020 Association for Computing Machinery.",Escape analysis; JVM; KESO; Memory management; Regional memory,Ada (programming language); Application programs; C (programming language); Embedded systems; Interactive computer systems; Java programming language; Program compilers; Safety engineering; Static analysis; Verification; Analysis techniques; Design and implementations; Escape analysis; KESO; Memory management; Real-time specification for javas; Safety-critical embedded systems; Type-safe languages; Real time systems
Design and optimization of energy-accuracy tradeoff networks for mobile platforms via pretrained deep models,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079571249&doi=10.1145%2f3366636&partnerID=40&md5=0704ddaaa8f68ac16a32df6585cc5fcf,"Many real-world edge applications including object detection, robotics, and smart health are enabled by deploying deep neural networks (DNNs) on energy-constrained mobile platforms. In this article, we propose a novel approach to trade off energy and accuracy of inference at runtime using a design space called Learning Energy Accuracy Tradeoff Networks (LEANets). The key idea behind LEANets is to design classifiers of increasing complexity using pretrained DNNs to perform input-specific adaptive inference. The accuracy and energy consumption of the adaptive inference scheme depends on a set of thresholds, one for each classifier. To determine the set of threshold vectors to achieve different energy and accuracy tradeoffs, we propose a novel multiobjective optimization approach. We can select the appropriate threshold vector at runtime based on the desired tradeoff. We perform experiments on multiple pretrained DNNs including ConvNet, VGG-16, and MobileNet using diverse image classification datasets. Our results show that we get up to a 50% gain in energy for negligible loss in accuracy, and optimized LEANets achieve significantly better energy and accuracy tradeoff when compared to a state-of-the-art method referred to as Slimmable neural networks. © 2020 Association for Computing Machinery.",Deep neural networks; Embedded systems; Hardware; Inference; Software codesign,Classification (of information); Computer hardware; Drilling platforms; Economic and social effects; Embedded systems; Energy utilization; Multiobjective optimization; Object detection; Classification datasets; Design and optimization; Design classifiers; Energy accuracy tradeoff; Energy-constrained; Inference; Software codesign; State-of-the-art methods; Deep neural networks
Optimizing deep learning inference on embedded systems through adaptive model selection,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079572857&doi=10.1145%2f3371154&partnerID=40&md5=6ba4311f3e3e98eb6624a6427b72952d,"Deep neural networks (DNNs) are becoming a key enabling technique for many application domains. However, on-device inference on battery-powered, resource-constrained embedding systems is often infeasible due to prohibitively long inferencing time and resource requirements of many DNNs. Offloading computation into the cloud is often unacceptable due to privacy concerns, high latency, or the lack of connectivity. Although compression algorithms often succeed in reducing inferencing times, they come at the cost of reduced accuracy. This article presents a new, alternative approach to enable efficient execution of DNNs on embedded devices. Our approach dynamically determines which DNN to use for a given input by considering the desired accuracy and inference time. It employs machine learning to develop a low-cost predictive model to quickly select a pre-trained DNN to use for a given input and the optimization constraint. We achieve this first by offline training a predictive model and then using the learned model to select a DNN model to use for new, unseen inputs. We apply our approach to two representative DNN domains: image classification and machine translation. We evaluate our approach on a Jetson TX2 embedded deep learning platform and consider a range of influential DNN models including convolutional and recurrent neural networks. For image classification, we achieve a 1.8x reduction in inference time with a 7.52% improvement in accuracy over the most capable single DNN model. For machine translation, we achieve a 1.34x reduction in inference time over the most capable single model with little impact on the quality of translation. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Adaptive computing; Deep learning; Embedded systems,Computational linguistics; Computer aided language translation; Convolutional neural networks; Deep learning; Deep neural networks; Embedded systems; Image classification; Image enhancement; Recurrent neural networks; Adaptive computing; Adaptive model selection; Compression algorithms; Enabling techniques; Machine translations; Offloading computations; Predictive modeling; Resource requirements; Learning systems
Pattern guided integrated scheduling and routing in multi-hop control networks,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079770403&doi=10.1145%2f3372134&partnerID=40&md5=59498fb811fbc896fb121c70fd7c6d64,"Executing a set of control loops over a shared multi-hop (wireless) control network (MCN) requires careful co-scheduling of the control tasks and the routing of sensory/actuation messages over the MCN. In this work, we establish pattern guided aperiodic execution of control loops as a resource-aware alternative to traditional fully periodic executions of a set of embedded control loops sharing a computation and the communication infrastructure. We provide a satisfiability modulo theory-based co-design framework that synthesizes loop execution patterns having optimized control cost as the underlying scheduling scheme together with the associated routing solution over the MCN. The routing solution implements the timed movement of the sensory/actuation messages of the control loops, generated according to those loop execution patterns. From the given settling time requirement of the control loops, we compute a control theoretically sound model using matrix inequalities, which gives an upper bound to the number of loop drops within the finite length loop execution pattern. Next, we show how the proposed framework can be useful for evaluating the fault tolerance of a resource-constrained shared MCN subject to communication link failure. © 2020 Association for Computing Machinery.",Control performance; Multi-hop control networks; Routing; Schedulability,Closed loop control systems; Computation theory; Fault tolerance; Scheduling; Communication infrastructure; Control network; Control performance; Integrated scheduling; Routing; Satisfiability modulo Theories; Schedulability; Settling time requirement; Network routing
BBB-CFI: Lightweight CFI approach against code-reuse attacks using basic block information,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079570854&doi=10.1145%2f3371151&partnerID=40&md5=2a542fbc02ea4629e391c1af7730a2fe,"Code-reuse attack is a concrete threat to computing systems because it can evade conventional security defenses. Control flow integrity (CFI) is proposed to repel this threat. However, former implementations of CFI suffer from two major drawbacks: complex offline processing on programs and high overheads at runtime. Therefore, it is impractical for performance-constrained devices to adopt the technology, leaving them vulnerable to exploitation. In this article, we develop a cross-layer approach named basic-block-boundary-based control flow integrity (BBB-CFI) to minimize the overheads of both offline analysis and runtime checking. Our approach employs basic block information inside the binary code and read-only data to enforce CFI. We identify a key binary-level property called basic block boundary, and based on it we propose the code-inspired method where short code sequences can endorse a control flow transition. Our solution enables quick application launching because it does not require control flow graph construction at the offline stage. We only demand a lightweight analysis on read-only data and a small amount of code of the application. According to the experiments, our approach incurs a negligible 0.11% runtime performance overhead with a minor processor extension, whereas it achieves an order of magnitude speedup in pre-preprocessing compared to a baseline approach. Without control flow analysis or recompilation, BBB-CFI still effectively reduces 90% of the attack surface in terms of gadget numbers. Besides this, we show that the Turing-completeness in the libc is unsustainable. Our approach also demonstrates high applicability to many programs, and it is capable of protecting striped binaries. © 2020 Association for Computing Machinery.",Basic block; Control flow integrity; Runtime security,Data flow analysis; Flow graphs; Information use; Basic blocks; Constrained devices; Control flow analysis; Control flow graphs; Control-flow integrities; Cross-layer approach; Run-time performance; Runtimes; Codes (symbols)
Quality estimation and optimization of adaptive stereo matching algorithms for smart vehicles,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079741841&doi=10.1145%2f3372784&partnerID=40&md5=2999d910a71d6931065f28d0d9da5260,"Stereo matching is a promising approach for smart vehicles to find the depth of nearby objects. Transforming a traditional stereo matching algorithm to its adaptive version has potential advantages to achieve the maximum quality (depth accuracy) in a best-effort manner. However, it is very challenging to support this adaptive feature, since (1) the internal mechanism of adaptive stereo matching (ASM) has to be accurately modeled, and (2) scheduling ASM tasks on multiprocessors to generate the maximum quality is difficult under strict real-time constraints of smart vehicles. In this article, we propose a framework for constructing an ASM application and optimizing its output quality on smart vehicles. First, we empirically convert stereo matching into ASM by exploiting its inherent characteristics of disparity-cycle correspondence and introduce an exponential quality model that accurately represents the quality-cycle relationship. Second, with the explicit quality model, we propose an efficient quadratic programming-based dynamic voltage/frequency scaling (DVFS) algorithm to decide the optimal operating strategy, which maximizes the output quality under timing, energy, and temperature constraints. Third, we propose two novel methods to efficiently estimate the parameters of the quality model, namely location similarity-based feature point thresholding and street scenario-confined CNN prediction. Results show that our DVFS algorithm achieves at least 1.61 times quality improvement compared to the state-of-the-art techniques, and average parameter estimation for the quality model achieves 96.35% accuracy on the straight road. © 2020 Association for Computing Machinery.",Adaptive application; Binocular stereo matching; Embedded systems; Smart vehicle,Dynamic frequency scaling; Embedded systems; Quadratic programming; Vehicles; Voltage scaling; Adaptive application; Adaptive stereo matching algorithm; Binocular stereo; Dynamic voltage/frequency scaling; Inherent characteristics; Smart vehicles; State-of-the-art techniques; Stereo matching algorithm; Dynamic programming
Blocking-aware partitioned real-time scheduling for uniform heterogeneous multicore platforms,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079572779&doi=10.1145%2f3366683&partnerID=40&md5=eda2bcae155fc37b669877b7bc283fe1,"Heterogeneous multicore processors have recently become de facto computing engines for state-of-the-art embedded applications. Nonetheless, very little research focuses on the scheduling of periodic (implicit-deadline) real-time tasks upon heterogeneous multicores under the requirements of task synchronization, which is stemmed from resource access conflicts and can greatly affect the schedulability of tasks. In view of partitioned Earliest Deadline First and Multiprocessor Stack Resource Policy, we first discuss the blocking-aware utilization bound for uniform heterogeneous multicores and then illustrate its non-monotonicity, where the bound may decrease with more deployed cores. Following the insights obtained from the bound analysis, taking the system heterogeneity into consideration, we propose a Synchronization-Aware Task Partitioning Algorithm for Heterogeneous Multicores (SA-TPA-HM)). Several resource-guided and heterogeneity-oriented mapping heuristics are incorporated to reduce the negative impacts of blocking interferences for better schedulability performance of tasks and balanced workload distribution across cores. The extensive simulation results show that SA-TPA-HM can obtain the schedulability ratios approximate to an Integer Non-Linear Programming–based solution, and much higher (e.g., 60% more) in contrast to the existing partitioning algorithms targeted at homogeneous multicores. The measurement results in Linux kernel further reveal the practical viability of SA-TPA-HM that can experience lower runtime overhead (e.g., 15% less) when compared to other mapping schemes. © 2020 Association for Computing Machinery.",Heterogeneous multicore platforms; Partitioned scheduling; Resource sharing; Utilization bound,Computer operating systems; Integer programming; Mapping; Nonlinear programming; Scheduling; Scheduling algorithms; Earliest deadline first; Heterogeneous Multi-Cores; Heterogeneous multicore; Heterogeneous multicore processors; Partitioning algorithms; Resource sharing; Stack resource policies; Utilization bounds; Multicore programming
3pxNet: Pruned-permuted-packed XNOR networks for edge machine learning,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079573909&doi=10.1145%2f3371157&partnerID=40&md5=f4ee08fc4ce83466724b5bd114f2dc7f,"As the adoption of Neural Networks continues to proliferate different classes of applications and systems, edge devices have been left behind. Their strict energy and storage limitations make them unable to cope with the sizes of common network models. While many compression methods such as precision reduction and sparsity have been proposed to alleviate this, they don’t go quite far enough. To push size reduction to its absolute limits, we combine binarization with sparsity in Pruned-Permuted-Packed XNOR Networks (3PXNet), which can be efficiently implemented on even the smallest of embedded microcontrollers. 3PXNets can reduce model sizes by up to 38X and reduce runtime by up to 3X compared with already compact conventional binarized implementations with less than 3% accuracy reduction. We have created the first software implementation of sparse-binarized Neural Networks, released as open source library targeting edge devices. Our library is complete with training methodology and model generating scripts, making it easy and fast to deploy. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Embedded systems; Image recognition; Neural networks,Embedded systems; Image recognition; Machine learning; Neural networks; Open source software; Common networks; Compression methods; Different class; Embedded microcontroller; Open-source libraries; Size reductions; Software implementation; Storage limitation; Open systems
Editorial: Embedded computing and society,2020,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078455228&doi=10.1145%2f3368250&partnerID=40&md5=6d7b67fab40ad612f9d8aa68704b4a27,[No abstract available],Climate Crisis and embedded computing; Democracy and computing; Inequality and computing; Socially responsible computing,
ICNN: The iterative convolutional neural network,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077777701&doi=10.1145%2f3355553&partnerID=40&md5=1c9166ee21d5215d7db1da3d57bc6d30,"Modern and recent architectures of vision-based Convolutional Neural Networks (CNN) have improved detection and prediction accuracy significantly. However, these algorithms are extremely computationally intensive. To break the power and performance wall of CNN computation, we reformulate the CNN computation into an iterative process, where each iteration processes a sub-sample of input features with smaller network and ingests additional features to improve the prediction accuracy. Each smaller network could either classify based on its input set or feed computed and extracted features to the next network to enhance the accuracy. The proposed approach allows early-termination upon reaching acceptable confidence. Moreover, each iteration provides a contextual awareness that allows an intelligent resource allocation and optimization for the proceeding iterations. In this article, we propose various policies to reduce the computational complexity of CNN through the proposed iterative approach. We illustrate how the proposed policies construct a dynamic architecture suitable for a wide range of applications with varied accuracy requirements, resources, and time-budget, without further need for network re-training. Furthermore, we carry out a visualization of the detected features in each iteration through deconvolution network to gain more insight into the successive traversal of the ICNN. © 2019 Association for Computing Machinery.",Convolutional neural networks; Energy-efficiency; Wavelets,Budget control; Convolution; Energy efficiency; Iterative methods; Neural networks; Convolutional neural network; Dynamic architecture; Intelligent resource; Iteration process; Iterative approach; Iterative process; Prediction accuracy; Wavelets; Network architecture
Weakly-hard real-time guarantees for earliest deadline first scheduling of independent tasks,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077767019&doi=10.1145%2f3356865&partnerID=40&md5=1225ad66616dfeeb02d02ece39252744,"The current trend in modeling and analyzing real-time systems is toward tighter yet safe timing constraints. Many practical real-time systems can de facto sustain a bounded number of deadline-misses, i.e., they have Weakly-Hard Real-Time (WHRT) constraints rather than hard real-time constraints. Therefore, we strive to provide tight Deadline Miss Models (DMMs) in complement to tight response time bounds for such systems. In this work, we bound the distribution of deadline-misses for task sets running on uniprocessors using the Earliest Deadline First (EDF) scheduling policy. We assume tasks miss their deadlines due to transient overload resulting from sporadic jobs, e.g., interrupt service routines. We use Typical Worst-Case Analysis (TWCA) to tackle the problem in this context. Also, we address the sources of pessimism in computing DMMs, and we discuss the limitations of the proposed analysis. This work is motivated by and validated on a realistic case study inspired by industrial practice (satellite on-board software) and on a set of synthetic test cases. The synthetic experiment is dedicated to extensively study the impact of EDF on DMMs by presenting a comparison between DMMs computed under EDF and Rate Monotonic (RM). The results show the usefulness of this approach for temporarily overloaded systems when EDF scheduling is considered. They also show that EDF is especially useful for WHRT tasks. © 2019 Copyright held by the owner/author(s).",Dynamic priority scheduling; Schedulability analysis; Weakly hard real-time systems,Fault tolerant computer systems; Interactive computer systems; Response time (computer systems); Scheduling; Scheduling algorithms; Software testing; Dynamic-priority scheduling; Earliest deadline first scheduling; Industrial practices; Interrupt service routine; Schedulability analysis; Synthetic experiments; Weakly hard real-time; Weakly hard real-time systems; Real time systems
Tÿcho: A framework for compiling stream programs,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077772546&doi=10.1145%2f3362692&partnerID=40&md5=3b3ee4f530632b971f30ddf8a13d9f2b,"Many application areas for embedded systems, such as DSP, media coding, and image processing, are based on stream processing. Stream programs in these areas are often naturally described as graphs, where nodes are computational kernels that send data over the edges. This structure also exhibits large amounts of concurrency, because the kernels can execute independently as long as there are data to process on the edges. The explicit data dependencies also help making efficient sequential implementations of such programs, allowing programs to be more portable between platforms with various degrees of parallelism. The kernels can be expressed in many different ways; for example, as imperative programs with read and write statements for the communication or as a set of actions that can be performed and conditions for when these actions can be executed. Traditionally, there has been a tension between how the kernels are expressed and how efficiently they can be implemented. There are very efficient implementation techniques for stream programs with restricted expressiveness, such as synchronous dataflow. In this article, we present a framework for building stream program compilers that we call Tÿcho. At the core of this framework is a common kernel representation, based on a machine model for stream program kernels called actor machine, on which transformations and optimizations are performed. Both imperative and action-based kernels are translated to this common representation, making the same optimizations applicable to different kinds of kernels, and even across source language boundaries. An actor machine is described by the steps of execution that a kernel can take, and the conditions for taking them, together with a controller that decides how the conditions are tested and the steps are taken. We outline how kernels of an imperative process language and an action-based language are decomposed and translated to the common kernel representation, and we describe a simple backend that generates sequential C code from this representation. We present optimization heuristics of the decision process in the controller that we evaluate using a few dozen kernels from a video decoder with various degrees of complexity. We also present kernel fusion, by merging the controllers of actor machines, as a way of scheduling kernels on the same processor, which we compare to prior art. © 2019 Association for Computing Machinery.",Compiler; Dataflow with firing; Kahn process networks; Stream program,Controllers; Embedded systems; Image coding; Natural language processing systems; Optimization; Process control; Program compilers; Translation (languages); Compiler; Dataflow; Efficient implementation; Kahn process networks; Kernel representation; Optimization heuristics; Sequential implementation; Stream program; C (programming language)
Toward customized hybrid fuel-cell and battery-powered mobile device for individual users,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075629451&doi=10.1145%2f3362033&partnerID=40&md5=343f6a99ed4013cadc0a59a159141627,"Rapidly evolving technologies and applications of mobile devices inevitably increase the power demands on the battery. However, the development of batteries can hardly keep pace with the fast-growing demands, leading to short battery life, which becomes the top complaints from customers. In this article, we investigate a novel energy supply technology, fuel cell (FC), and leverage its advantages of providing long-term energy storage to build a hybrid FC-battery power system. Therefore, mobile device operation time is dramatically extended, and users are no longer bothered by battery recharging.We examine real-world smartphone usage data and find that a naive hybrid power system cannot meet many users' highly diversified power demands. We thus propose an OS-level power management policy that reduces the device power consumption for each power peak to solve this mismatch. This technique trades the quality-of-service (QoS) for a larger FC ratio in the system and thus much longer device operation time. We further observe that the user's personality largely determines his/her satisfaction with the QoS degradation and the operation time extension. Thus, applying a hybrid system with fixed configuration (i.e., peak throttling level coupled with corresponding FC/battery ratio) fails to satisfy every user. We then explore customized hybrid system configuration based on each individual user's personality to deliver the optimal satisfaction for him/her. The experimental results show that our personality-aware hybrid FC-battery solution can achieve 4× longer operation time and 25% higher satisfaction score compared to the common setting for state-of-the-art mobile devices. © 2019 Copyright held by the owner/author(s).",Battery; Customize; Fuel cell; Mobile devices,Digital storage; Electric batteries; Electric power utilization; Fuel cells; Fuel storage; Hybrid systems; Mobile computing; Battery; Customize; Device operations; Energy supply technology; Hybrid power systems; Management policy; System configurations; Technologies and applications; Quality of service
Overcoming security vulnerabilities in deep learning-based indoor localization frameworks on mobile devices,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075638179&doi=10.1145%2f3362036&partnerID=40&md5=2f1c4902fcc6ccec30371d41a0791089,"Indoor localization is an emerging application domain for the navigation and tracking of people and assets. Ubiquitously available Wi-Fi signals have enabled low-cost fingerprinting-based localization solutions. Further, the rapid growth in mobile hardware capability now allows high-accuracy deep learning-based frameworks to be executed locally on mobile devices in an energy-efficient manner. However, existing deep learning-based indoor localization solutions are vulnerable to access point (AP) attacks. This article presents an analysis into the vulnerability of a convolutional neural network-based indoor localization solution to AP security compromises. Based on this analysis, we propose a novel methodology to maintain indoor localization accuracy, even in the presence of AP attacks. The proposed secured neural network framework (S-CNNLOC) is validated across a benchmark suite of paths and is found to deliver up to 10× more resiliency to malicious AP attacks compared to its unsecured counterpart. © 2019 Association for Computing Machinery. All rights reserved.",AP attacks; Deep learning; Indoor localization; Indoor navigation; Reliability; Security; Spoofing,Deep learning; Energy efficiency; Mobile security; Network security; Neural networks; Reliability; AP attacks; In-door navigations; Indoor localization; Security; Spoofing; Indoor positioning systems
Robust design and validation of cyber-physical systems,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075639547&doi=10.1145%2f3362098&partnerID=40&md5=321a02d6b78c68876e656ca821938783,"Co-simulation-based validation of hardware controllers adjoinedwith plant models,with continuous dynamics, is an important step inmodel-based design of controllers for Cyber-physical Systems (CPS). Co-simulation suffers from many problems, such as timing delays, skew, race conditions, and so on, making it unsuitable for checking timing properties of CPS. In our approach to validation of controllers, synthesised from their models, the synthesised controller is adjoined with a synthesised hardware plant unit. The synthesised plant and controller are then executed synchronously and Metric Interval Temporal Logic (MITL) properties are validated on the closed-loop system. The clock period is chosen, using robustness estimates, such that all timing properties that hold on the controller guiding the discretised plant model also hold on the original case of the continuous-time plant model guided by the controller. Benchmark results show that real-time MITL properties that are vacuously satisfied or violated due to co-simulation artefacts hold correctly in the proposed closed-loop validation framework. © 2019 Association for Computing Machinery. All rights reserved.",Co-simulation; Cyber-physical system; Robustness criteria; Synthesis,Closed loop systems; Continuous time systems; Controllers; Embedded systems; Synthesis (chemical); Temporal logic; Closed-loop validation; Continuous dynamics; Continuous-time; Cosimulation; Cyber-physical systems (CPS); Hardware plants; Interval temporal logic; Timing properties; Cyber Physical System
CxDNN: Hardware-software compensation methods for deep neural networks on resistive crossbar systems,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075631167&doi=10.1145%2f3362035&partnerID=40&md5=0abcf3d590180f84eb106aa0d1f08d57,"Resistive crossbars have shown strong potential as the building blocks of future neural fabrics, due to their ability to natively execute vector-matrix multiplication (the dominant computational kernel in DNNs). However, a key challenge that arises in resistive crossbars is that non-idealities in the synaptic devices, interconnects, and peripheral circuits of resistive crossbars lead to errors in the computations performed. When large-scale DNNs are executed on resistive crossbar systems, these errors compound and result in unacceptable degradation in application-level accuracy. We propose CxDNN, a hardware-software methodology that enables the realization of large-scale DNNs on crossbar systems by compensating for errors due to nonidealities, greatly mitigating the degradation in accuracy. CxDNN is composed of (i) an optimized mapping technique to convert floating-point weights and activations to crossbar conductances and input voltages, (ii) a fast one-time re-training method to recover accuracy loss due to this conversion, and (iii) low-overhead compensation hardware to mitigate dynamic and hardware-instance-specific errors. Unlike previous efforts that are limited to small networks and require the training and deployment of hardware-instance-specific models, CxDNN presents a scalable compensation methodology that can address large DNNs (e.g., ResNet-50 on ImageNet) and maintains the train-once-deploy-anywhere tenet of current DNN application.We evaluated CxDNN on six top DNNs on the ImageNet dataset with 0.5-13.8 million neurons and 0.5-15.5 billion connections. CxDNN achieves 16.9%-49% improvement in the top-1 classification accuracy, effectively mitigating a key challenge to the use of resistive crossbar-based neural fabrics. © 2019 Association for Computing Machinery. All rights reserved.",Crossbar non-idealities; Deep Neural Networks (DNNs); Emerging memories; In-memory computing; Resistive crossbars,Crossbar equipment; Digital arithmetic; Errors; Large dataset; Classification accuracy; Computational kernels; Emerging memory; Non-idealities; Resistive crossbars; Software compensation; Software methodologies; Vector-matrix multiplications; Deep neural networks
BTMonitor: Bit-time-based intrusion detection and attacker identification in controller area network,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075638779&doi=10.1145%2f3362034&partnerID=40&md5=1955c831be671acf2321ae7a0fda5175,"With the rapid growth of connectivity and autonomy for today's automobiles, their security vulnerabilities are becoming one of the most urgent concerns in the automotive industry. The lack ofmessage authentication in Controller Area Network (CAN), which is the most popular in-vehicle communication protocol, makes it susceptible to cyber attack. It has been demonstrated that the remote attackers can take over the maneuver of vehicles after getting access to CAN, which poses serious safety threats to the public. To mitigate this issue, we propose a novel intrusion detection system (IDS), called BTMonitor (Bit-timebased CAN Bus Monitor). It utilizes the small but measurable discrepancy of bit time in CAN frames to fingerprint their sender Electronic Control Units (ECUs). To reduce the requirement for high sampling rate, we calculate the bit time of recessive bits and dominant bits, respectively, and extract their statistical features as fingerprint. The generated fingerprint is then used to detect intrusion and pinpoint the attacker. BTMonitor can detect new types of masquerade attack that the state-of-the-art clock-skew-based IDS is unable to identify. We implement a prototype system for BTMonitor using Xilinx Spartan 6 FPGA for data collection. We evaluate our method on both a CAN bus prototype and a real vehicle. The results show that BTMonitor can correctly identify the sender with an average probability of 99.76% on the real vehicle. © 2019 Association for Computing Machinery. All rights reserved.",Attacker identification; Automotive security; Controller area network; Intrusion detection,Automotive industry; Computer crime; Control system synthesis; Controllers; Intrusion detection; Network security; Process control; Sampling; Vehicles; Attacker identification; Automotive security; Controller area network; Electronic control units; In-vehicle communication; Intrusion Detection Systems; Security vulnerabilities; Statistical features; Vehicle to vehicle communications
Hardware-software collaborative thermal sensing in optical network-on-chip-based manycore systems,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075629887&doi=10.1145%2f3362099&partnerID=40&md5=e75f16eabf1d3a8d0b3aada00fc60d45,"Continuous technology scaling in manycore systems leads to severe overheating issues. To guarantee system reliability, it is critical to accurately yet efficiently monitor runtime temperature distribution for effective chip thermal management. As an emerging communication architecture for new-generation manycore systems, optical network-on-chip (ONoC) satisfies the communication bandwidth and latency requirements with low power dissipation. Moreover, observation shows that it can be leveraged for runtime thermal sensing. In this article, we propose a brand-new on-chip thermal sensing approach for ONoC-based manycore systems by utilizing the intrinsic thermal sensitivity of optical devices and the inter-processor communications in ONoCs. It requires no extra hardware but utilizes existing optical devices in ONoCs and combines them with lightweight software computation in a hardware-software collaborative manner. The effectiveness of the our approach is validated both at the device level and the system level through professional photonic simulations. Evaluation results based on synthetic communication traces and realistic benchmarks showthat our approach achieves an average temperature inaccuracy of only 0.6648 K compared to ground-truth values and is scalable to be applied for large-size ONoCs. © 2019 Association for Computing Machinery. All rights reserved.",Chip temperature monitoring; Embedded systems; Hardware/software co-design; Micro-ring resonators; Optical network-on-chip,Embedded systems; Fiber optic networks; Hardware-software codesign; Low power electronics; Network-on-chip; Optical communication; Optical devices; Servers; Chip temperature; Communication architectures; Communication bandwidth; Communication traces; Inter processor communication; Low-power dissipation; Microring resonator; Optical network on chip (ONoC); Monitoring
REAL: Request arbitration in last level caches,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075638316&doi=10.1145%2f3362100&partnerID=40&md5=a795e803314958153da274b2ff2fd6e2,"Shared last level caches (LLC) of multicore systems-on-chip are subject to a significant amount of contention over a limited bandwidth, resulting in major performance bottlenecks that make the issue a first-order concern in modern multiprocessor systems-on-chip. Even though shared cache space partitioning has been extensively studied in the past, the problem of cache bandwidth partitioning has not received sufficient attention. We demonstrate the occurrence of such contention and the resulting impact on the overall system performance. To address the issue, we perform detailed simulations to study the impact of different parameters and propose a novel cache bandwidth partitioning technique, called REAL, that arbitrates among cache access requests originating from different processor cores. It monitors the LLC access patterns to dynamically assign a priority value to each core. Experimental results on different mixes of benchmarks show up to 2.13× overall system speedup over baseline policies, with minimal impact on energy. © 2019 Association for Computing Machinery. All rights reserved.",Cache bandwidth; Multi-core systems; Shared caches,System-on-chip; Cache bandwidth; Lastlevel caches (LLC); Limited bandwidth; Multi-core systems; Multiprocessor systems on chips; Performance bottlenecks; Shared cache; Space partitioning; Bandwidth
ALEXIA: A processor with lightweight extensions for memory safety,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075640752&doi=10.1145%2f3362064&partnerID=40&md5=d93db7cc6de1deb284be0db097b04c54,"Illegal use of memory pointers is a serious security vulnerability. A large number of malwares exploit the spatial and temporal nature of these vulnerabilities to subvert execution or glean sensitive data froman application. Recent countermeasures attach metadata to memory pointers, which define the pointer's capabilities. The metadata is used by the hardware to validate pointer-based memory accesses. However, recent works have considerable overheads. Further, the pointer validation is decoupled from the actual memory access.We show that this could open up vulnerabilities in multithreaded applications and introduce new vulnerabilities due to speculation in out-of-order processors. In this article, we demonstrate that the overheads can be reduced considerably by efficient metadata management. We show that the hardware can be designed in a manner that would remain safe in multithreaded applications and immune to speculative vulnerabilities.We achieve these by ensuring that the pointer validations and the corresponding memory access is always done atomically and in order. To evaluate our scheme, which we call ALEXIA, we enhance an OpenRISC processor to perform the memory validation at runtime and also add compiler support. ALEXIA is the first hardware countermeasure scheme for memory protection that provides such an end-to-end solution. We evaluate the processor on an Altera FPGA and show that the runtime overhead, on average, is 14%, with negligible impact on the processor's size and clock frequency. There is also a negligible impact on the program's code and data sizes. © 2019 Association for Computing Machinery. All rights reserved.",Buffer overflow attacks; Heap exploits; Memory corruption; Security,Crime; Malware; Metadata; Buffer overflow attacks; Hardware Countermeasures; Heap exploits; Memory corruption; Multi-threaded application; Out-of-order processors; Security; Security vulnerabilities; Program compilers
GRec: Automatic Computation of Reconfiguration Graphs for Multi-core Platforms,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075597938&doi=10.1145%2f3350533&partnerID=40&md5=eabd0695eb126af0bfb230255412fa18,[No abstract available],Multi-core; permanent failures; real-Time; reconfiguration,
Power-mode-Aware memory subsystem optimization for low-power system-on-chip design,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075550844&doi=10.1145%2f3356583&partnerID=40&md5=c059bc740a0248e03e4b8127735a9b34,"The memory subsystem is increasingly subject to an intensive energy minimization effort in embedded and System-on-Chip development.While the main focus is typically put on energy consumption reduction, there are other optimization aspects that become more and more relevant as well, e.g., peak power constraints or time budgets. In this regard, the present article makes the following contributions. Taking industrial-grade information into account, different Static Random-Access Memory (SRAM) power modes and their characteristics are presented at first. Using this information, a comprehensive optimization model with the main intention of energy minimization is defined. It is based on memory access statistics that represent the embedded software of interest, which allows for application-Tailored improvements. Further, it considers different power states of the memory subsystem and enables the definition of peak power and time corridor constraints. The presented two-stage implementation of this optimization model allows the handling of large design spaces. Clearly defined interfaces facilitate the exchange of individual workflow parts in a plug-And-play fashion and further enable a neat integration of our optimization method with existing hardware/software (HW/SW) codesign synthesis flows. A general evaluation for different technology nodes yields that the optimization potential of memory low-power modes increases with advancing miniaturization but also depends on the data footprint of the embedded software. Experimental results for a set of benchmark applications confirm these findings and provide energy savings of up to 90% and over 60% on average compared to a monolithic memory layout without low-power modes. © 2019 Copyright held by the owner/author(s).",Embedded memory subsystem; Embedded system design; HW/SW codesign; Memory low-power modes; Software-controlled power management; SRAM; Static optimization; System design automation; System-on-Chip,Application programs; Application specific integrated circuits; Benchmarking; Budget control; Embedded software; Embedded systems; Energy conservation; Energy utilization; Hardware-software codesign; Integrated circuit design; Memory architecture; Optimization; Power management; Programmable logic controllers; System-on-chip; Systems analysis; Design automations; Embedded memory; HW/SW Codesign; Low power modes; Static optimization; Static random access storage
Sample essentiality and its application to modeling attacks on arbiter PUFs,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075572809&doi=10.1145%2f3344148&partnerID=40&md5=55dd75d0425924abb4a74cecfc36388c,"Physically Unclonable Functions (PUFs), as an alternative hardware-based security method, have been challenged by some modeling attacks. As is known to all, samples are significant in modeling attacks on PUFs, and thus, some efforts have been made to expand sample sets therein to improve modeling attacks. A closer examination, however, reveals that not all samples contribute to modeling attacks equally. Therefore, in this article, we introduce the concept of sample essentiality for describing the contribution of a sample in modeling attacks and point out that any sample without sample essentiality cannot enhance some modeling attacks on PUFs. As a by-product, we find theoretically and empirically that the samples expanded by the procedures proposed by Chatterjee et al. do not satisfy our sample essentiality. Furthermore, we propose the notion of essential sample sets for datasets and discuss its basic properties. Finally, we demonstrate that our results about sample essentiality can be used to reduce samples efficiently and benefit sample selection in modeling attacks on arbiter PUFs. © 2019 Association for Computing Machinery.",Arbiter PUFs; Essential sample set; Hardware-based security analysis; Modeling attacks; Physical unclonable functions (PUFs); Sample essentiality,Sampling; Arbiter PUFs; ITS applications; Physically unclonable functions; Sample selection; Sample sets; Hardware security
Distill-Net: Application-specific distillation of deep convolutional neural networks for resource-constrained iot platforms,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075571241&doi=10.1145%2f3360512&partnerID=40&md5=6eafc8314afdb408534af984e8d0b345,"Many Internet-of-Things (IoT) applications demand fast and accurate understanding of a few key events in their surrounding environment. Deep Convolutional Neural Networks (CNNs) have emerged as an effective approach to understand speech, images, and similar high-dimensional data types. Algorithmic performance of modern CNNs, however, fundamentally relies on learning class-Agnostic hierarchical features that only exist in comprehensive training datasets with many classes. As a result, fast inference using CNNs trained on such datasets is prohibitive for most resource-constrained IoT platforms. To bridge this gap, we present a principled and practical methodology for distilling a complex modern CNN that is trained to effectively recognize many different classes of input data into an application-dependent essential core that not only recognizes the few classes of interest to the application accurately but also runs efficiently on platforms with limited resources. Experimental results confirm that our approach strikes a favorable balance between classification accuracy (application constraint), inference efficiency (platform constraint), and productive development of new applications (business constraint). © 2019 Association for Computing Machinery.",Convolutional neural network; Deep learning; Embedded systems; Resource scalable systems,Clustering algorithms; Convolution; Deep learning; Distillation; Embedded systems; Internet of things; Neural networks; Business constraints; Classification accuracy; Convolutional neural network; Hierarchical features; High dimensional data; Internet of Things (IOT); Scalable systems; Surrounding environment; Deep neural networks
Response time analysis for tasks with fixed preemption points under global scheduling,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075576138&doi=10.1145%2f3360513&partnerID=40&md5=0f77c34d6df1dd1f9ee0e0b35c9d1edc,"As an effective method for detecting the schedulability of real-Time tasks on multiprocessor platforms, Response time analysis (RTA) has been deeply researched in recent decades. Most of the existing RTA methods are designed for tasks that can be preempted at any time. However, in some real-Time systems, a task may have some fixed preemption points (FPPs) that divide its execution into a series of non-preemptive regions (NPRs). In such environments, the task can only be preempted at its FPPs, which makes existing RTA methods for arbitrary preemption tasks not applicable. In this article, we study the schedulability analysis on tasks with FPPs under both global fixed-priority (G-FP) scheduling and global earliest deadline first (G-EDF) scheduling. First, based on the idea of limiting the time interval between two consecutive executions of an NPR, a novel RTA method for tasks with FPPs under G-FP scheduling is proposed. Second, we propose an effective RTA method for tasks with FPPs under G-EDF scheduling. Finally, extensive simulations are conducted and the results validate the effectiveness of the proposed methods. © 2019 Association for Computing Machinery.",Fixed preemption points; Global scheduling; Nonpreemptive region; Response time analysis,Interactive computer systems; Response time (computer systems); Scheduling; Earliest deadline first; Extensive simulations; Fixed preemption points; Global scheduling; Multi-processor platforms; Non-preemptive; Response-time analysis; Schedulability analysis; Real time systems
A closed-loop controller to ensure performance and temperature constraints for dynamic applications,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075568493&doi=10.1145%2f3343030&partnerID=40&md5=4a22191dc20f3ad439d4772c6e0a2caa,"To secure correct system operation, a plethora of Reliability, Availability and Serviceability (RAS) techniques have been deployed by circuit designers. RAS mechanisms however, come with the cost of extra clock cycles. In addition, a wide variety of dynamic workloads and different input conditions often constitute preemptive dependability techniques hard to implement. To this end, we focus on a realistic case study of a closed-loop controller that mitigates performance variation with a reactive response. This concept has been discussed but was only illustrated on small benchmarks. In particular, the extension of the approach to manage performance of dynamic workloads on a target platform has not been shown earlier. We compare our scheme against the version of a Linux CPU frequency governor in terms of timing response and energy consumption. Finally, we move forward and suggest a new flavor of our controller to efficiently manage processor temperature. Again, the concept is illustrated with a realistic case study and compared to a modern temperature manager. © 2019 Association for Computing Machinery.",,Computer operating systems; Energy utilization; Availability and serviceability; Circuit designers; Clock cycles; Closed loop controllers; Dynamic applications; Performance variations; Processor temperatures; System operation; Controllers
Swaram: Portable energy and cost efficient embedded system for genomic processing,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073165572&doi=10.1145%2f3358211&partnerID=40&md5=c9aa504069b0a443909759c4af75a508,"Treatment of patients using high-quality precision medicine requires a thorough understanding of the genetic composition of a patient. Ideally, the identification of unique variations in an individual's genome is needed for specifying the necessary treatment. Variant calling workflow is a pipeline of tools, integrating state of the art software systems aimed at alignment, sorting and variant calling for the whole genome sequencing (WGS) data. This pipeline is utilized for identifying unique variations in an individual's genome (compared to a reference genome). Currently, such a workflow is implemented on high-performance computers (with additional GPUs or FPGAs) or in cloud computers. Such systems are large, have a high cost, and rely on the internet for genome data transfer which makes the system unusable in remote locations unequipped with internet connectivity. It further raises privacy concerns due to processing being carried out in a different facility. To overcome such limitations, in this paper, for the first time, we present a cost-efficient, offline, scalable, portable, and energy-efficient computing system named SWARAM for variant calling workflow processing. The system uses novel architecture and algorithms to match against partial reference genomes to exploit smaller memory sizes which are typically available in tiny processing systems. Extensive tests on a standard benchmark data-set (NA12878 Illumina platinum genome) confirm that the time consumed for the data transfer and completing variant calling workflow on SWARAM was competitive to that of a 32-core Intel Xeon server with similar accuracy, but costs less than a fifth, and consumes less than 40% of the energy of the server system. The original scripts and code we developed for executing the variant calling workflow on SWARAM are available in the associated Github repository https://github.com/Rammohanty/swaram. © 2019 Association for Computing Machinery.",Alignment; ARM; DNA analysis; Embedded system; Energy efficient system; Genetic analysis; Genome; Portable genome analysis; Variant calling,Alignment; Data transfer; Embedded systems; Energy efficiency; Genes; Genetic algorithms; HTTP; Patient treatment; Pipelines; Program processors; Statistical tests; DNA analysis; Energy efficient systems; Genetic analysis; Genome analysis; Variant calling; Data communication systems
Output-based intermediate representation for translation of test-pattern program,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073167875&doi=10.1145%2f3358186&partnerID=40&md5=ddf0620b53c665f86bb54015d8947aca,"An Intermediate Representation (IR) used by compilers is normally generated statically, as a result of parsing or analyzing the source program. This paper proposes a completely different type of IR, generated as a result of running the source program, the output-based IR. There is a practical translation problem where such an IR is useful, in the domain of test-pattern programs. Test-pattern programs run on ATE (automatic test equipment), a special embedded system to test semiconductors such as DRAMs. They generate a pattern for each clock, a bit vector input to the pins of the chip. One issue is that different ATEs require different programming since each ATE manufacturer has its own programming language. Nonetheless, we should be able to test a memory chip on different ATEs as long as they generate the same patterns with the same speed. Therefore, a memory chipmaker wants to make a pattern program portable across ATEs, to fully utilize their ATE resources. One solution is translating between pattern programs, for which we need an IR since there are multiple source ATEs and target ATEs. Instead of a conventional, static IR, we propose using the output pattern itself as an IR. Since the pattern is independent of ATEs and easily obtainable, the output-based IR obviates designing a static IR considering all ATE programming languages and hardware differences. Moreover, we might synthesize a better target program from the IR, more optimized to the target ATE. However, the full pattern generated by a product-level pattern program is huge, so we propose using an IR of abbreviated patterns, annotated with the repetition information obtained while executing the source program. Our experimental results with product-level pattern programs show that our approach is feasible. © 2019 Association for Computing Machinery.",Automatic test equipment; Domain-specific language; Intermediate representation; Test-pattern program; Translation,Automatic testing; Dynamic random access storage; Equipment testing; Problem oriented languages; Program compilers; Software testing; Translation (languages); Automatic test equipment; Bit vector; Domain specific languages; Hardware differences; Intermediate representations; Memory chips; Multiple source; Test Pattern; Program translators
"Igor, get me the optimum! Prioritizing important design decisions during the DSE of embedded systems",2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073170763&doi=10.1145%2f3358204&partnerID=40&md5=03aab7005ac810437141a55c740861df,"Design Space Exploration (DSE) techniques for complex embedded systems must cope with a huge variety of applications and target architectures as well as a wide spectrum of objectives and constraints. In particular, existing design automation approaches are either problem-independent, in that they do not exploit any knowledge about the optimization problem at hand, or are tailored to specific a priori assumptions about the problem and/or a specific set of design objectives. While the latter are only applicable within a very limited scope of design problems, the former may struggle to deliver high-quality solutions for problems with large design spaces and/or complex design objectives. As a remedy, we propose Importance-Guided Order Rearrangement (IGOR) as a novel approach for DSE of embedded systems. Instead of relying on an a priori problem knowledge, IGOR uses a machine-learning-inspired technique to dynamically analyze the importance of design decisions, i.e., the impact that these decisions-within the specific problem that is being optimized-have on the quality of explored problem solutions w.r.t. the given design objectives. Throughout the DSE, IGOR uses this information to guide the optimization towards the most promising regions of the design space. Experimental results for a variety of applications from different domains of embedded computing and for different optimization scenarios give evidence that the proposed approach is both scalable and adaptable, as it can be used for the optimization of systems described by several thousands constraints, where it outperforms both problem-specific and problem-independent optimization approaches and achieves ε-dominance improvements of up to 95%. © 2019 Association for Computing Machinery.",Constrained combinatorial problems; Design space exploration; Embedded systems; Multiobjective optimization,Computer aided design; Constrained optimization; Multiobjective optimization; Combinatorial problem; Design automations; Design space exploration; Embedded computing; High-quality solutions; Optimization approach; Optimization problems; Target architectures; Embedded systems
Locking the design of building blocks for quantum circuits,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073167363&doi=10.1145%2f3358184&partnerID=40&md5=edf5d012e293a9ab1d751bb50dce483e,"The research community expects that quantum computers will give economical results for particular problems on which the classical computers break down. Examples include factoring of large numbers, searching in a big database, or simulating chemical reactions to design new drugs. Attempts are ongoing to build up a practical quantum computer. Users (clients) can implement quantum circuits to run on these quantum computers. However, before running the quantum circuit on the quantum computer, the users (clients) should compile, optimize, decompose, and technology map the quantum circuit. In the current embodiment, the resulting quantum circuit runs on a remote and untrusted quantum computer server - introducing security risks. This study explores the risk of outsourcing the quantum circuit to the quantum computer by focusing on quantum oracles. Quantum oracles are pivotal building blocks and require specialized expertise and means to design. Hence, the designer may protect this proprietary quantum oracle intellectual property (IP) and hide his/her private information. We investigate how to manage that on a quantum computer server using the IBM project QX quantum computer and Qiskit tools as an exemplar. © 2019 Association for Computing Machinery.",IP piracy; Locking the oracle; Privacy; Quantum circuit; Quantum oracle; Reversible logic; Security,Architectural design; Data privacy; Electric network analysis; Locks (fasteners); Qubits; Timing circuits; Ip piracies; Locking the oracle; Quantum circuit; Quantum oracle; Reversible Logic; Security; Computer circuits
Enabling sequential-write-constrained B+-tree index scheme to upgrade shingled magnetic recording storage performance,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073168952&doi=10.1145%2f3358201&partnerID=40&md5=9fa35713958c6a123de954b9aa3aa3f1,"When a shingle magnetic recording (SMR) drive has been widely applied to modern computer systems (e.g., archive file systems, big data computing systems, and large-scale database systems), storage system developers should thoroughly review whether current designs (e.g., index schemes and data placements) are appropriate for an SMR drive because of its sequential write constraint. Through many prior works excellently manage data in an SMR drive by integrating their proposed solutions into the driver layer, an index scheme over an SMR drive has never been optimized by any previous works because managing index over the SMR drive needs to jointly consider the properties of B+-tree and SMR natures (e.g., sequential write constraint and zone partitions) in a host storage system. Moreover, poor index management will result in terrible storage performance because an index manager is extensively used in file systems and database applications. For optimizing the B+-tree index structure over an SMR storage, this work identifies performance overheads caused by the B+-tree index structure in an SMR drive. By such observation, this study proposes a sequential-write-constrained B+-tree index scheme, namely SW-B+tree, which consists of an address redirection data structure, an SMR-aware node allocation mechanism, and a frequency-aware garbage collection strategy. According to our experiments, the SW-B+tree can improve the SMR storage performance 55% on average. © 2019 Association for Computing Machinery.",B<sup>+</sup>-tree; File index; Sequential write; Shingled magnetic recording,Digital storage; Forestry; Magnetic recording; Magnetic storage; Magnetism; Storage management; Trees (mathematics); Allocation mechanism; Database applications; File index; Large-scale database; Modern computer systems; Sequential write; Shingled magnetic recordings; Storage performance; Information management
Cascade: High throughput data streaming via decoupled access-execute CGRA,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073167907&doi=10.1145%2f3358177&partnerID=40&md5=4957b6844e75b1d6f270768b10ab0342,"A Coarse-Grained Reconfigurable Array (CGRA) is a promising high-performance low-power accelerator for compute-intensive loop kernels. While the mapping of the computations on the CGRA is a well-studied problem, bringing the data into the array at a high throughput remains a challenge. A conventional CGRA design involves on-array computations to generate memory addresses for data access undermining the attainable throughput. A decoupled access-execute architecture, on the other hand, isolates the memory access from the actual computations resulting in a significantly higher throughput. We propose a novel decoupled access-execute CGRA design called CASCADE with full architecture and compiler support for high-throughput data streaming from an on-chip multi-bank memory. CASCADE offloads the address computations for the multi-bank data memory access to a custom designed programmable hardware. An end-to-end fully-automated compiler synchronizes the conflict-free movement of data between the memory banks and the CGRA. Experimental evaluations show on average 3× performance benefit and 2.2× performance per watt improvement for CASCADE compared to an iso-area conventional CGRA with a bigger processing array in lieu of a dedicated hardware memory address generation logic. © 2019 Association for Computing Machinery.",Coarse grained reconfigurable arrays; Decoupled access-execute architectures; Multi-bank memory partitioning,Computer hardware; Data reduction; Program compilers; Reconfigurable architectures; Address computation; Array computations; Coarse-grained reconfigurable arrays; Experimental evaluation; High-throughput data; Multi-bank memory; Performance benefits; Programmable hardware; Memory architecture
Hessle-free: Heterogeneous systems leveraging fuzzy control for runtime resource management,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073164220&doi=10.1145%2f3358203&partnerID=40&md5=de16f4db316d6e0580eb90a20a73c790,"As computing platforms increasingly embrace heterogeneity, runtime resource managers need to efficiently, dynamically, and robustly manage shared resources (e.g., cores, power budgets, memory bandwidth). To address the complexities in heterogeneous systems, state-of-the-art techniques that use heuristics or machine learning have been proposed. On the other hand, conventional control theory can be used for formal guarantees, but may face unmanageable complexity for modeling system dynamics of complex heterogeneous systems. We address this challenge through HESSLE-FREE (Heterogeneous Systems Leveraging Fuzzy Control for Runtime Resource Management): an approach leveraging fuzzy control theory that combines the strengths of classical control theory together with heuristics to form a light-weight, agile, and efficient runtime resource manager for heterogeneous systems. We demonstrate the efficacy of HESSLE-FREE executing on a NVIDIA Jetson TX2 platform (containing a heterogeneous multi-processor with a GPU) to show that HESSLE-FREE: 1) provides opportunity for optimization in the controller and stability analysis to enhance the confidence in the reliability of the system; 2) coordinates heterogeneous compute units to achieve desired objectives (e.g., QoS, optimal power references, FPS) efficiently and with lower complexity, and 3) eases the burden of system specification. © 2019 Association for Computing Machinery.",DVFS; Fuzzy control; Heterogeneous multi-core processor; Heterogeneous systems; MIMO control; Power management; QoS; Runtime resource management,Budget control; Fuzzy control; Managers; Natural resources management; Optimization; Power management; Quality of service; Reliability analysis; Resource allocation; Specifications; DVFS; Heterogeneous multi core processors; Heterogeneous systems; MIMO controls; Resource management; Control theory
Enabling and exploiting partition-level parallelism (PALP) in phase change memories,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073172243&doi=10.1145%2f3358180&partnerID=40&md5=7975e1a0e0884be107b2d95508bfbfca,"Phase-change memory (PCM) devices have multiple banks to serve memory requests in parallel. Unfortunately, if two requests go to the same bank, they have to be served one after another, leading to lower system performance. We observe that a modern PCM bank is implemented as a collection of partitions that operate mostly independently while sharing a few global peripheral structures, which include the sense amplifiers (to read) and the write drivers (to write). Based on this observation, we propose PALP, a new mechanism that enables partition-level parallelism within each PCM bank, and exploits such parallelism by using the memory controller's access scheduling decisions. PALP consists of three new contributions. First, we introduce new PCM commands to enable parallelism in a bank's partitions in order to resolve the read-write bank conflicts, with no changes needed to PCM logic or its interface. Second, we propose simple circuit modifications that introduce a new operating mode for the write drivers, in addition to their default mode of serving write requests. When configured in this new mode, the write drivers can resolve the read-read bank conflicts, working jointly with the sense amplifiers. Finally, we propose a new access scheduling mechanism in PCM that improves performance by prioritizing those requests that exploit partition-level parallelism over other requests, including the long outstanding ones. While doing so, the memory controller also guarantees starvation-freedom and the PCM's running-average-power-limit (RAPL). We evaluate PALP with workloads from the MiBench and SPEC CPU2017 Benchmark suites. Our results show that PALP reduces average PCM access latency by 23%, and improves average system performance by 28% compared to the state-of-the-art approaches. © 2019 Association for Computing Machinery.",Phase-change memories (PCM); Sense amplifiers; Write drivers,Scheduling; Access scheduling; Average power limit; Memory controller; Partition levels; Phase change memory (pcm); Sense amplifier; Starvation-freedom; State-of-the-art approach; Phase change memory
Robust reachable set: Accounting for uncertainties in linear dynamical systems,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073166496&doi=10.1145%2f3358229&partnerID=40&md5=b54a9da208ef43acb2419bfdbb72603f,"Reachable set computation is one of the primary techniques for safety verification of linear dynamical systems. In reality the underlying dynamics have uncertainties like parameter variations or modeling uncertainties. Therefore, the reachable set computation must consider the uncertainties in the dynamics to be useful i.e. the computed reachable set should be over or under approximation if not exact. This paper presents a technique to compute reachable set of linear dynamical systems with uncertainties. First, we introduce a construct called support of a matrix. Using this construct, we present a set of sufficient conditions for which reachable set for uncertain linear system can be computed efficiently; and safety verification can be performed using bi-linear programming. Finally, given a linear dynamical system, we compute robust reachable set, which accounts for all possible uncertainties that can be handled by the sufficient conditions presented. Experimental evaluation on benchmarks reveal that our algorithm is computationally very efficient. © 2019 Association for Computing Machinery.",Formal methods; Linear uncertain systems; Model checking; Reachable set computation; Robust reachable set; Safety verification,Dynamical systems; Formal methods; Formal verification; Linear control systems; Linear programming; Linear systems; Model checking; Uncertain systems; Experimental evaluation; Linear dynamical systems; Linear uncertain systems; Reachable set; Safety verification; Uncertain linear system; Under-approximation; Underlying dynamics; Uncertainty analysis
Efficient decentralized LTL monitoring framework using tableau technique,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073675300&doi=10.1145%2f3358219&partnerID=40&md5=6951535f47c714a1e146c7bdcf1dbcbd,"This paper presents a novel framework for decentralized monitoring of Linear Temporal Logic (LTL) formulas, under the situation where processes are synchronous and the formula is represented as a tableau. The tableau technique allows one to construct a semantic tree for the input LTL formula, which can be used to optimize the decentralized monitoring of LTL in various ways. Given a system P and an LTL formula φ, we construct a tableau Tφ. The tableau Tφ is used for two purposes: (a) to synthesize an efficient round-robin communication policy for processes, and (b) to find the minimal ways to decompose the formula and communicate observations of processes in an efficient way. In our framework, processes can propagate truth values of both atomic and compound formulas (non-atomic formulas) depending on the syntactic structure of the input LTL formula and the observation power of processes. We demonstrate that this approach of decentralized monitoring based on tableau construction is more straightforward, more flexible, and more likely to yield efficient solutions than alternative approaches. © 2019 Association for Computing Machinery.",Decentralized monitoring; Linear Temporal Logic (LTL); tableau,Computer circuits; Semantics; Syntactics; Communication policies; Decentralized monitoring; Linear temporal logic; Monitoring frameworks; Semantic tree; Syntactic structure; tableau; Truth values; Temporal logic
Analyzing variable entanglement for parallel simulation of SystemC TLM-2.0 models,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073171387&doi=10.1145%2f3358194&partnerID=40&md5=e3dd8d1dd276c3d46652bae87c6b814a,"The SystemC TLM-2.0 standard is widely used in modern electronic system level design for better interoperability and higher simulation speed. However, TLM-2.0 has been identified as an obstacle for parallel SystemC simulation due to the disappearance of channels. Without a containment construct, simulation threads are permitted to directly access data of other modules and that makes it difficult to synchronize such accesses as required by the SystemC execution semantics. In this paper, we propose a compile time approach to statically analyze potential conflicts among threads in SystemC TLM-2.0 loosely- and approximately-timed models. We introduce a new Socket Call Path technique which provides the compiler with socket binding information for precise static analysis. We also propose an algorithm to analyze entangled variable pairs. Experimental results show that our approach is able to support automatically safe parallel simulation of SystemC models with TLM-2.0 Blocking Transport Interface, Direct Memory Interface and Non-blocking Transport Interface, resulting in impressive simulation speeds. © 2019 Association for Computing Machinery.",Parallel discrete event simulation; PDES; SystemC; TLM-2.0,Discrete event simulation; Logic design; Semantics; System theory; Systems analysis; Approximately-timed; Electronic system level design; Execution semantics; Parallel discrete event simulations; Parallel simulations; PDES; SystemC; TLM-2.0; Interoperability
Accumulative display updating for intermittent systems,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073169219&doi=10.1145%2f3358190&partnerID=40&md5=c665200e973f9219f244b3614f16b409,"Electrophoretic displays are ideal for self-powered systems, but currently require an uninterrupted power supply to carry out the full display update cycle. Although sensible for battery-powered devices, when directly applied to intermittently-powered systems, guaranteeing display update atomicity usually results in repeated execution until completion or can incur high hardware/software overheads, heavy programmer intervention and large energy buffering requirements to provide sufficient display update energy. This paper introduces the concept, design and implementation of accumulative display updating, which relaxes the atomicity constraints of display updating, such that the display update process can be accumulatively completed across power cycles, without the need for sufficient energy for the entire display update. To allow for process logical continuity, we track the update progress during execution and facilitate a safe display shutdown procedure to overcome physical and operability issues related to abrupt power failure. Additionally, a context-aware updating policy is proposed to handle data freshness issues, where the delay in addressing new update requests can cause the display contents to be in conflict with new data available. Experimental results on a Texas Instruments device with an integrated electrophoretic display show that, compared to atomic display updating, our design can significantly increase accurate forward progress, decrease the average response time of display updating and reduce time and energy wastage when displaying fresh data. © 2019 Association for Computing Machinery.",Accumulative updating; Data freshness; Electrophoretic displays; Intermittent systems,Software engineering; Accumulative updating; Battery powered devices; Data freshness; Design and implementations; Intermittent systems; Self-powered systems; Shutdown procedures; Uninterrupted power supply; Electrophoretic displays
Compact: On-chip compression of activations for low power systolic array based CNN acceleration,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073170953&doi=10.1145%2f3358178&partnerID=40&md5=6f5f6afa10b504df26b8ecd665fe59c4,"This paper addresses the design of systolic array (SA) based convolutional neural network (CNN) accelerators for mobile and embedded domains. On- and off-chip memory accesses to the large activation inputs (sometimes called feature maps) of CNN layers contribute significantly to total energy consumption for such accelerators; while prior has proposed off-chip compression, activations are still stored on-chip in uncompressed form, requiring either large on-chip activation buffers or slow and energy-hungry off-chip accesses. In this paper, we propose CompAct, a new architecture that enables on-chip compression of activations for SA based CNN accelerators. CompAct is built around several key ideas. First, CompAct identifies an SA schedule that has nearly regular access patterns, enabling the use of a modified run-length coding scheme (RLC). Second, CompAct improves compression ratio of the RLC scheme using Sparse-RLC in later CNN layers and Lossy-RLC in earlier layers. Finally, CompAct proposes look-ahead snoozing that operates synergistically with RLC to reduce the leakage energy of activation buffers. Based on detailed synthesis results, we show that CompAct enables up to 62% reduction in activation buffer energy, and 34% reduction in total chip energy. © 2019 Association for Computing Machinery.",Deep neural networks; Low-power design; Systolic arrays,Activation energy; Deep neural networks; Electric power supplies to apparatus; Energy utilization; Low power electronics; Neural networks; Access patterns; Convolutional neural network; Embedded domains; Leakage energies; Low-power design; Off-chip memory; Run-length coding; Total energy consumption; Systolic arrays
"Honey, I shrunk the Elfs: Lightweight binary tailoring of shared libraries",2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073171489&doi=10.1145%2f3358222&partnerID=40&md5=877a53877cc99d4d864df11e53570fa5,"In the embedded domain, industrial sectors (i.e., automotive industry, avionics) are undergoing radical changes. They broadly adopt commodity hardware and move away from special-purpose control units. During this transition, heterogeneous software components are consolidated to run on commodity operating systems. To efficiently consolidate such components, a modular encapsulation of common functionality into reusable binary files (i.e., shared libraries) is essential. However, shared libraries are often unnecessarily large as they entail a lot of generic functionality that is not required in a narrowly defined scenario. As the source code of proprietary components is often unavailable and the industry is heading towards binary-only distribution, we propose an approach towards lightweight binary tailoring. As demonstrated in the evaluation, lightweight binary tailoring effectively reduces the amount of code in all shared libraries on a Linux-based system by 63 percent and shrinks their files by 17 percent. The reduction in size is beneficial to cut down costs (e.g., lower storage and memory footprint) and eases code analyses that are necessary for code audits. © 2019 Association for Computing Machinery.",Binary tailoring; Linux; Shared libraries,Automotive industry; Codes (symbols); Linux; Binary tailoring; Commodity hardware; Commodity operating systems; Embedded domains; Heterogeneous software; Industrial sector; Linux-based system; Shared libraries; Libraries
Authcropper: Authenticated image cropper for privacy preserving surveillance systems,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073164568&doi=10.1145%2f3358195&partnerID=40&md5=226b6386005c930cadf762c946705f48,"As surveillance systems are popular, the privacy of the recorded video becomes more important. On the other hand, the authenticity of video images should be guaranteed when used as evidence in court. It is challenging to satisfy both (personal) privacy and authenticity of a video simultaneously, since the privacy requires modifications (e.g., partial deletions) of an original video image while the authenticity does not allow any modifications of the original image. This paper proposes a novel method to convert an encryption scheme to support partial decryption with a constant number of keys and construct a privacy-aware authentication scheme by combining with a signature scheme. The security of our proposed scheme is implied by the security of the underlying encryption and signature schemes. Experimental results show that the proposed scheme can handle the UHD video stream with more than 17 fps on a real embedded system, which validates the practicality of the proposed scheme. © 2019 Association for Computing Machinery.",Authentication; Forward-secure signature; Privacy; Video,Authentication; Cryptography; Data privacy; Authentication scheme; Encryption schemes; Forward-secure signatures; Original images; Privacy preserving surveillance; Signature Scheme; Surveillance systems; Video; Security systems
Will my program break on this faulty processor?: Formal analysis of hardware fault activations in concurrent embedded software,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073167278&doi=10.1145%2f3358238&partnerID=40&md5=946d120e16d43c60b1e1f21c0a0a22da,"Formal verification is approaching a point where it will be reliably applicable to embedded software. Even though formal verification can efficiently analyze multi-threaded applications, multi-core processors are often considered too dangerous to use in critical systems, despite the many benefits they can offer. One reason is the advanced memory consistency model of such CPUs. Nowadays, most software verifiers assume strict sequential consistency, which is also the naïve view of programmers. Modern multi-core processors, however, rarely guarantee this assumption by default. In addition, complex processor architectures may easily contain design faults. Thanks to the recent advances in hardware verification, these faults are increasingly visible and can be detected even in existing processors, giving an opportunity to compensate for the problem in software. In this paper, we propose a generic approach to consider inconsistent behavior of the hardware in the analysis of software. Our approach is based on formal methods and can be used to detect the activation of existing hardware faults on the application level and facilitate their mitigation in software. The approach relies heavily on recent results of model checking and hardware verification and offers new, integrative research directions. We propose a partial solution based on existing model checking tools to demonstrate feasibility and evaluate their performance in this context. © 2019 Association for Computing Machinery.",Analysis; Concurrent; Fault; Litmus test; Memory consistency model,Application programs; Chemical activation; Embedded software; Faulting; Model checking; Multicore programming; Program processors; Analysis; Concurrent; Hardware verification; Litmus test; Memory consistency models; Multi- threaded applications; Processor architectures; Sequential consistency; Formal verification
Techniques and analysis for mixed-criticality scheduling with mode-dependent server execution budgets,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073170389&doi=10.1145%2f3358234&partnerID=40&md5=b90a8946d3fe734bcf2b6259a9f1b998,"In mixed-criticality systems, tasks of different criticality share system resources, mainly to reduce cost. Cost is further reduced by using adaptive mode-based scheduling arrangements, such as Vestal's model, to improve resource efficiency, while guaranteeing schedulability of critical functionality. To simplify safety certification, servers are often used to provide temporal isolation between tasks. In its simplest form, a server is a periodically recurring time window, in which some tasks are scheduled. A server's computational requirements may greatly vary in different modes, although state-of-the-art techniques and schedulability tests do not allow different budgets to be used by a server in different modes. This results in a single conservative execution budget for all modes, increasing system cost. The goal of this paper is to reduce the cost of mixed-criticality systems through three main contributions: (i) a scheduling arrangement for uniprocessor systems employing fixed-priority scheduling within periodic servers, whose budgets are dynamically adjusted at run-time in the event of a mode change, (ii) a new schedulability analysis for such systems, and (iii) heuristic algorithms for assigning budgets to servers in different modes and ordering the execution of the servers. Experiments with synthetic task sets demonstrate considerable improvements (up to 52.8%) in scheduling success ratio when using dynamic server budgets vs. static “one-size-fits-all-modes” budgets. © 2019 Association for Computing Machinery.",Dynamic execution Budget; Mixed-criticality scheduling; Mode change; Real-time systems; Server-based scheduling; Timing isolation; Vestal model,Cost benefit analysis; Cost reduction; Criticality (nuclear fission); Heuristic algorithms; Interactive computer systems; Real time systems; Scheduling; Dynamic execution; Mixed criticalities; Mode changes; Server-based; Timing isolation; Budget control
High-level synthesis of approximate designs under real-time constraints,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073170384&doi=10.1145%2f3358182&partnerID=40&md5=662cc09590a634017cc9f66ee3e781df,"The adoption of High-Level Synthesis (HLS) has increased as the latest HLS tools have evolved to provide high-quality results while improving productivity and time-to-market. Concurrently, many works have been proposing the incorporation of approximate computing techniques within HLS toolchains, allowing automated generation of inexact circuits for error-tolerant application domains with the aim of trading-off computation accuracy with area/power savings or performance improvements. Thus, when attempting to make a design meet timing requirements, designers of real-time systems using HLS may resort to approximation approaches. However, current approximate HLS tools do not allow specifying real-time constraints, being instead error-constrained to explore area, power, or performance optimizations. In this work, we propose an approximate HLS framework for real-time systems that can be integrated with state-of-the-art HLS tools. With this framework designers can specify real-time constraints and satisfy them while minimizing the output error. It uses scheduling information and Worst-Case Execution Time (WCET) analysis for iteratively exploring time-error trade-offs of approximations in the time-critical execution path. Experimental results on signal and image processing benchmarks show that we can reduce the WCET of exact designs by up to 35% with acceptable quality degradation. © 2019 Association for Computing Machinery.",Approximate computing; Design space exploration; High-level synthesis; Worst-case execution time,Benchmarking; Commerce; Constrained optimization; Economic and social effects; Errors; High level synthesis; Image processing; Interactive computer systems; Iterative methods; Approximate computing; Approximation approach; Design space exploration; Performance optimizations; Scheduling information; Signal and image processing; Worst-case execution time; Worst-case execution time analysis; Real time systems
Quality/latency-aware real-time scheduling of distributed streaming IoT applications,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073163962&doi=10.1145%2f3358209&partnerID=40&md5=d58415e1b6aabf10be052c62f2791009,"Embedded systems are increasingly networked and distributed, often, such as in the Internet of Things (IoT), over open networks with potentially unbounded delays. A key challenge is the need for real-time guarantees over such inherently unreliable and unpredictable networks. Generally, timeouts are used to provide timing guarantees while trading off data losses and quality. The schedule of distributed task executions and network timeouts thereby determines a fundamental latency-quality trade-off that is, however, not taken into account by existing scheduling algorithms. In this paper, we propose an approach for scheduling of distributed, real-time streaming applications under quality-latency goals. We formulate this as a problem of analytically deriving a static worst-case schedule of a given distributed dataflow graph that minimizes quality loss while meeting guaranteed latency constraints. Towards this end, we first develop a quality model that estimates SNR of distributed streaming applications under given network characteristics and an overall linearity assumption. Using this quality model, we then formulate and solve the scheduling of distributed dataflow graphs as a numerical optimization problem. Simulation results with random graphs show that quality/latency-aware scheduling improves SNR over a baseline schedule by 50% on average. When applied to a distributed neural network application for handwritten digit recognition, our scheduling methodology can improve classification accuracy by 10% over a naive distribution under tight latency constraints. © 2019 Association for Computing Machinery.",IoT; Open network; Real-time; Scheduling; Streaming,Acoustic streaming; Character recognition; Data flow analysis; Economic and social effects; Embedded systems; Optimization; Scheduling; Scheduling algorithms; Classification accuracy; Distributed neural networks; Handwritten digit recognition; Internet of thing (IOT); Network characteristics; Numerical optimizations; Open network; Real time; Internet of things
DWMACC: Accelerating shift-based CNNs with domain wall memories,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073165918&doi=10.1145%2f3358199&partnerID=40&md5=addf2d537a77df83072af7cb6ad458ac,"PIM (processing-in-memory) based hardware accelerators have shown great potentials in addressing the computation and memory access intensity of modern CNNs (convolutional neural networks). While adopting NVM (non-volatile memory) helps to further mitigate the storage and energy consumption overhead, adopting quantization, e.g., shift-based quantization, helps to tradeoff the computation overhead and the accuracy loss, integrating both NVM and quantization in hardware accelerators leads to sub-optimal acceleration. In this paper, we exploit the natural shift property of DWM (domain wall memory) to devise DWMAcc, a DWM-based accelerator with asymmetrical storage of weight and input data, to speed up the inference phase of shift-based CNNs. DWMAcc supports flexible shift operations to enable fast processing with low performance and area overhead. We then optimize it with zero-sharing, input-reuse, and weight-share schemes. Our experimental results show that, on average, DWMAcc achieves 16.6× performance improvement and 85.6× energy consumption reduction over a state-of-the-art SRAM based design. © 2019 Association for Computing Machinery.",CNN accelerators; Domain wall memory,Acceleration; Domain walls; Energy utilization; Neural networks; Computation overheads; Convolutional neural network; Hardware accelerators; Non-volatile memory; Optimal accelerations; Processing in memory; Shift operations; State of the art; Static random access storage
End-to-end timing analysis of sporadic cause-effect chains in distributed systems,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073163698&doi=10.1145%2f3358181&partnerID=40&md5=59ba25fc109fd1cd0d48849e71b0ff3c,"A cause-effect chain is used to define the logical order of data dependent tasks, which is independent from the execution order of the jobs of the (periodic/sporadic) tasks. Analyzing the worst-case End-to-End timing behavior, associated to a cause-effect chain, is an important problem in embedded control systems. For example, the detailed timing properties of modern automotive systems are specified in the AUTOSAR Timing Extensions. In this paper, we present a formal End-to-End timing analysis for distributed systems. We consider the two most important End-to-End timing semantics, i.e., the button-to-action delay (termed as the maximum reaction time) and the worst-case data freshness (termed as the maximum data age). Our contribution is significant due to the consideration of the sporadic behavior of job activations, whilst the results in the literature have been mostly limited to periodic activations. The proof strategy shows the (previously unexplored) connection between the reaction time (data age, respectively) and immediate forward (backward, respectively) job chains. Our analytical results dominate the state of the art for sporadic task activations in distributed systems and the evaluations show a clear improvement for synthesized task systems as well as for a real world automotive benchmark setting. © 2019 Association for Computing Machinery.",Distributed systems; Embedded control systems; End-to-End timing analysis; Sporadic cause-effect chains,Chemical activation; Control system analysis; Network security; Semantics; Timing circuits; Analytical results; Automotive Systems; Cause-effect; Distributed systems; Embedded control systems; State of the art; Timing Analysis; Timing properties; Embedded systems
RMW-F: A design of RMW-free cache using built-in NAND-flash for SMR storage,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073151606&doi=10.1145%2f3358210&partnerID=40&md5=edcfcfcd1ea508f6618e3d13364ce351,"Shingled Magnetic Recording (SMR) disks have been proposed as a high-density, non-volatile media and precede traditional hard disk drives in both storing capacity and cost. However, the intrinsic characteristics of SMR disks raise a major performance challenge named read-modify-write operations (RMWs) that are time-consuming and can significantly degrade the overall system performance. Current designs of SMR disks usually adopt a persistent cache to alleviate the negative effect brought by RMWs and the cache is used as a first-level cache to buffer all the incoming writes of the whole SMR storage system. In this paper, we propose to change the functionality of the cache, that is, the cache will no longer serve as a first-level cache like previous. Incoming data are distinguished according to their different write-back behavior and those data which will incur RMWs will be left in our built-in NAND flash cache called RMW-free Cache (RMW-F) to eliminate the need of RMWs. Besides, RMW-F improves the cleaning efficiency by a model that takes both write-back cost and data popularity into considerations. Our experimental results show that RMW-F can achieve both system performance and cleaning efficiency improvements. © 2019 Association for Computing Machinery.",Cache management; Flash memory; Shingled magnetic recording,Buffer storage; Efficiency; Magnetic recording; Memory architecture; NAND circuits; Cache management; Cleaning efficiency; Data popularity; Intrinsic characteristics; Performance challenges; Shingled magnetic recording (SMR); Shingled magnetic recordings; Write operations; Flash memory
Moos: A multi-objective design space exploration and optimization framework for NoC enabled manycore systems,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073065607&doi=10.1145%2f3358206&partnerID=40&md5=e7e68e44e66163848e319852f5a45216,"The growing needs of emerging applications has posed significant challenges for the design of optimized manycore systems. Network-on-Chip (NoC) enables the integration of a large number of processing elements (PEs) in a single die. To design optimized manycore systems, we need to establish suitable trade-offs among multiple objectives including power, performance, and thermal. Therefore, we consider multi-objective design space exploration (MO-DSE) problems arising in the design of NoC-enabled manycore systems: placement of PEs and communication links to optimize two or more objectives (e.g., latency, energy, and throughput). Existing algorithms to solve MO-DSE problems suffer from scalability and accuracy challenges as size of the design space and the number of objectives grow. In this paper, we propose a novel framework referred as Multi-Objective Optimistic Search (MOOS) that performs adaptive design space exploration using a data-driven model to improve the speed and accuracy of multi-objective design optimization process. We apply MOOS to design both 3D heterogeneous and homogeneous manycore systems using Rodinia, PARSEC, and SPLASH2 benchmark suites. We demonstrate that MOOS improves the speed of finding solutions compared to state-of-the-art methods by up to 13X while uncovering designs that are up to 20% better in terms of NoC. The optimized 3D manycore systems improve the EDP up to 38% when compared to 3D mesh-based designs optimized for the placement of PEs. © 2019 Association for Computing Machinery.",Design optimization; Machine learning; Manycore systems; Network-on-chip,Distributed computer systems; Economic and social effects; Learning systems; Multiobjective optimization; Network-on-chip; Servers; Systems analysis; Design optimization; Emerging applications; Manycore systems; Multi-objective design optimization; Multi-objective design space explorations; Network-on-chip(NoC); Optimization framework; State-of-the-art methods; Integrated circuit design
Counterexample guided abstraction refinement for polyhedral probabilistic hybrid systems,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073144910&doi=10.1145%2f3358217&partnerID=40&md5=2391bdab7161284d6184e1f1583f486f,"We consider the problem of safety analysis of probabilistic hybrid systems, which capture discrete, continuous and probabilistic behaviors. We present a novel counterexample guided abstraction refinement (CEGAR) algorithm for a subclass of probabilistic hybrid systems, called polyhedral probabilistic hybrid systems (PHS), where the continuous dynamics is specified using a polyhedral set within which the derivatives of the continuous executions lie. Developing a CEGAR algorithm for PHS is complex owing to the branching behavior due to the probabilistic transitions, and the infinite state space due to the real-valued variables. We present a practical algorithm by choosing a succinct representation for counterexamples, an efficient validation algorithm and a constructive method for refinement that ensures progress towards the elimination of a spurious abstract counterexample. The technical details for refinement are non-trivial since there are no clear disjoint sets for separation. We have implemented our algorithm in a Python toolbox called Procegar; our experimental analysis demonstrates the benefits of our method in terms of successful verification results, as well as bug finding. © 2019 Association for Computing Machinery.",Counterexample guided abstraction refinement; Probabilistic hybrid systems; Safety,Abstracting; Accident prevention; Hybrid systems; Constructive methods; Counterexample-guided abstraction refinement; Experimental analysis; Infinite state space; Probabilistic behavior; Real-valued variables; Succinct representation; Verification results; Model checking
Aggressive energy reduction for video inference with software-only strategies,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073159578&doi=10.1145%2f3358174&partnerID=40&md5=1dd06592c26e3a64dbcc2a3ce478aa27,"In the past years, several works have proposed custom hardware and software-based techniques for the acceleration of Convolutional Neural Networks (CNNs). Most of these works focus on saving computations by changing the used precision or modifying frame processing. To reach a more aggressive energy reduction, in this paper we propose software-only modifications to the CNNs inference process. Our approach exploits the inherent locality in videos by replacing entire frame computations with a movement prediction algorithm. Furthermore, when a frame must be processed, we avoid energy-demanding floating-point operations, and at the same time reduce memory accesses by employing look-up tables in place of the original convolutions. Using the proposed approach, one can reach significant energy gains of more than 25× for security cameras, and 12× for moving vehicles applications, with only small software modifications. © 2019 Association for Computing Machinery.",Computation reuse; Convolutional Neural Networks; Frame predicton,Convolution; Digital arithmetic; Motion estimation; Neural networks; Table lookup; Computation reuse; Convolutional neural network; Custom hardwares; Floating point operations; Frame predicton; Inference process; Movement prediction; Software modification; Application programs
ECAX: Balancing error correction costs in approximate accelerators,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073162455&doi=10.1145%2f3358179&partnerID=40&md5=58aa3982b33eca54bd68d1e5646ce33a,"Approximate computing has emerged as a design paradigm amenable to error-tolerant applications. It enables trading the quality of results for efficiency improvement in terms of delay, power, and energy consumption under user-provided tolerable quality degradation. Approximate accelerators have been proposed to expedite frequently executing code sections of error-resilient applications while meeting a defined quality level. However, these accelerators may produce unacceptable errors at run time if the input data changes or dynamic adjustments are made for a defined output quality constraint. State-of-the-art approaches in approximate computing address this issue by correctly re-computing those accelerator invocations that produce unacceptable errors; this is achieved by using the host processor or an alternate exact accelerator, which is activated on-demand. Nevertheless, such approaches can nullify the benefits of approximate computing, especially when input data variations are high at run time and errors due to approximations are above a tolerable threshold. As a robust and general solution to this problem, we propose ECAx, a novel methodology to explore low-overhead error correction in approximate accelerators by selectively correcting most significant errors, in terms of their magnitude, without losing the gains of approximations. We particularly consider the case of approximate accelerators built with approximate functional units such as approximate adders. Our novel methodology reduces the required exact re-computations on the host processor, achieving up to 20% performance gain compared to state-of-the-art approaches. © 2019 Association for Computing Machinery.",Approximate computing,Energy utilization; Error correction; Input output programs; Approximate computing; Dynamic adjustment; Efficiency improvement; General solutions; Novel methodology; Quality degradation; Quality of results; State-of-the-art approach; Acceleration
TF-Net: Deploying sub-byte deep neural networks on microcontrollers,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073161880&doi=10.1145%2f3358189&partnerID=40&md5=68762814041eed12739a1c0badd2fed6,"Deep Neural Networks (DNNs) have become an essential component of various applications. While today's DNNs are mainly restricted to cloud services, network connectivity, energy, and data privacy problems make it important to support efficient DNN computation on low-cost, low-power processors like microcontrollers. However, due to the constrained computation resources, it is challenging to execute large DNN models on microcontrollers. Using sub-byte low-precision input activations and weights is a typical method to reduce DNN computation. But on byte-addressable microcontrollers, the sub-byte computation is not well supported. The sub-byte inputs and weights need to be unpacked from bitstreams before computation, which incurs significant computation and energy overhead. In this paper, we propose the TF-Net pipeline to efficiently deploy sub-byte DNNs on microcontrollers. While TF-Net allows for a range of weight and input precision, we find Ternary weights and Four-bit inputs provide the optimal balance between model accuracy, computation performance, and energy efficiency. TF-Net first includes a training framework for sub-byte low-precision DNN models. Two algorithms are then introduced to accelerate the trained models. The first, direct buffer convolution, amortizes unpacking overhead by caching unpacked inputs. The second, packed sub-byte multiply-accumulate, utilizes a single multiplication instruction to perform multiple sub-byte multiply-accumulate computations. To further accelerate DNN computation, we propose two instructions, Multiply-Shift-Accumulate and Unpack, to extend the existing microcontroller instruction set. On the tested networks, TF-Net can help improve the computation performance and energy efficiency by 1.83× and 2.28× on average, respectively. © 2019 Association for Computing Machinery.",Deep neural networks; Microcontrollers; Sub-byte computation,Computational efficiency; Controllers; Data privacy; Distributed computer systems; Energy efficiency; Low power electronics; Microcontrollers; Computation performance; Computation resources; Energy overheads; Low power processors; Multiply accumulate; Network connectivity; Privacy problems; Training framework; Deep neural networks
Timing-anomaly free dynamic scheduling of conditional DAG tasks on multi-core systems,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073145056&doi=10.1145%2f3358236&partnerID=40&md5=5b7131108a00d60b333740e2e7a215c8,"In this paper, we propose a novel approach to schedule conditional DAG parallel tasks, with which we can derive safe response time upper bounds significantly better than the state-of-the-art counterparts. The main idea is to eliminate the notorious timing anomaly in scheduling parallel tasks by enforcing certain order constraints among the vertices, and thus the response time bound can be accurately predicted off-line by somehow “simulating” the runtime scheduling. A key challenge to apply the timing-anomaly free scheduling approach to conditional DAG parallel tasks is that at runtime it may generate exponentially many instances from a conditional DAG structure. To deal with this problem, we develop effective abstractions, based on which a safe response time upper bound is computed in polynomial time. We also develop algorithms to explore the vertex orders to shorten the response time bound. The effectiveness of the proposed approach is evaluated by experiments with randomly generated DAG tasks with different parameter configurations. © 2019 Association for Computing Machinery.",Conditional DAG; Dynamic scheduling; Response time analysis; Timing anomaly,Polynomial approximation; Timing circuits; Conditional DAG; Dynamic scheduling; Multi-core systems; Ordering constraints; Response-time analysis; Run-time scheduling; State of the art; Timing anomalies; Scheduling
A dual-mode strategy for performance-maximisation and resource-efficient CPS design,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073142860&doi=10.1145%2f3358213&partnerID=40&md5=be738610e12cdbebf21fbdd802efe76f,"The emerging scenarios of cyber-physical systems (CPS), such as autonomous vehicles, require implementing complex functionality with limited resources, as well as high performances. This paper considers a common setup in which multiple control and non-control tasks share one processor, and proposes a dual-mode strategy. The control task switches between two sampling periods when rejecting (coping with) a disturbance. We create an optimisation framework looking for the switching sampling periods and time instants that maximise the control performance (indexed by settling time) and resource efficiency (indexed by the number of tasks that are schedulable on the processor). The latter objective is enabled with schedulability analysis tailored for the dual-mode model. Experimental results show that (i) given a set of tasks, the proposed strategy improves the control performances whilst retaining schedulability; and (ii) given requirements on the control performances, the proposed strategy is able to schedule more tasks. © 2019 Association for Computing Machinery.",Cyber-physical systems; Dual-mode scheduling; Non-convex optimisation; Resource dimensioning; Schedulability analysis; Switching systems,Convex optimization; Cyber Physical System; Embedded systems; Switching systems; Control performance; Convex optimisation; Cyber-physical systems (CPS); Dual modes; Resource dimensioning; Resource efficiencies; Resource-efficient; Schedulability analysis; Integrated circuit design
Memory-efficient mixed-precision implementations for robust explicit model predictive control,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073145698&doi=10.1145%2f3358223&partnerID=40&md5=e2265028699c92d23ccb894a650ab3bc,"We propose an optimization for space-efficient implementations of explicit model-predictive controllers (MPC) for robust control of linear time-invariant (LTI) systems on embedded platforms. We obtain an explicit-form robust model-predictive controller as a solution to a multi-parametric linear programming problem. The structure of the controller is a polyhedral decomposition of the control domain, with an affine map for each domain. While explicit MPC is suited for embedded devices with low computational power, the memory requirements for such controllers can be high. We provide an optimization algorithm for a mixed-precision implementation of the controller, where the deviation of the implemented controller from the original one is within the robustness margin of the robust control problem. The core of the mixed-precision optimization is an iterative static analysis that co-designs a robust controller and a low-bitwidth approximation that is statically guaranteed to always be within the robustness margin of the original controller. We have implemented our algorithm and show on a set of benchmarks that our optimization can reduce space requirements by up to 20.9% and on average by 12.6% compared to a minimal uniform precision implementation of the original controller. © 2019 Association for Computing Machinery.",Fixed-point arithmetic; Model-predictive control; Robustness,Embedded systems; Fixed point arithmetic; Iterative methods; Linear control systems; Linear programming; Model predictive control; Predictive control systems; Robust control; Robustness (control systems); Space platforms; Static analysis; Computational power; Explicit model predictive control; Linear time-invariant system; Memory requirements; Optimization algorithms; Parametric linear programming; Robust control problems; Robust controllers; Controllers
Compositional design of multi-robot systems control software on ROS,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073156351&doi=10.1145%2f3358197&partnerID=40&md5=ed0a8ea60cddf2c43eb4dd8b5b82b973,"This paper presents a methodology that relies on Assume-Guarantee Contracts to decompose the problem of synthesizing control software for a multi-robot system. Initially, each contract describes either a component (e.g., a robot) or an aspect of the system. Then, the design problem is decomposed into different synthesis and verification sub-problems, allowing to tackle the complexity involved in the design process. The design problem is then recomposed by exploiting the rigorousness provided by contracts. This allows us to achieve system-level simulation capable to be used for validating the entire design. Once validated, the software synthesized during the process can be integrated into Robot Operating System (ROS) nodes and executed using state-of-the-practice packages and tools for modern robotic systems. We apply the methodology to generate a control strategy for an autonomous goods transportation system. Our results show a massive reduction of the time required to obtain automatically the control software implementing a multi-robot mission. © 2019 Association for Computing Machinery.",Contract-based design; Robotic operating system,Industrial robots; Model checking; Multipurpose robots; Robot learning; Robotics; Contract-based designs; Control strategies; Goods transportation; Multi-robot missions; Multi-robot systems; Robot operating systems (ROS); State of the practice; System level simulation; Machine design
Cache locking content selection algorithms for ARINC-653 compliant RTOS,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073153757&doi=10.1145%2f3358196&partnerID=40&md5=272a6cd27b083af5412b06c20043e031,"Avionic software is the subject of stringent real time, determinism and safety constraints. Software designers face several challenges, one of them being the interferences that appear in common situations, such as resource sharing. The interferences introduce non-determinism and delays in execution time. One of the main interference prone resources are cache memories. In single-core processors, caches comprise multiple private levels. This breaks the isolation principle imposed by avionic standards, such as the ARINC-653. This standard defines partitioned architectures where one partition should never directly interfere with another one. In cache-based architectures, one partition can modify the cache content of another partition. In this paper, we propose a method based on cache locking to reduce the non-determinism and the contention on lower level memories while improving the time performances. © 2019 Association for Computing Machinery.",Aerospace; ARINC-653; Cache locking; Critical systems; Interference,Avionics; Locks (fasteners); Wave interference; Aerospace; ARINC-653; Cache locking; Cache-based architectures; Critical systems; Selection algorithm; Single-core processors; Software designers; Cache memory
Treble: Fast software updates by creating an equilibrium in an active software ecosystem of globally distributed stakeholders,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073144699&doi=10.1145%2f3358237&partnerID=40&md5=a1e40e326cae09a8241ce4843388a1fc,"This paper presents our experience with Treble, a two-year initiative to build the modular base in Android, a Java-based mobile platform running on the Linux kernel. Our Treble architecture splits the hardware independent core framework written in Java from the hardware dependent vendor implementations (e.g., user space device drivers, vendor native libraries, and kernel written in C/C++). Cross-layer communications between them are done via versioned, stable inter-process communication interfaces whose backward compatibility is tested by using two API compliance suites. Based on this architecture, we repackage the key Android software components that suffered from crucial post-launch security bugs as separate images. That not only enables separate ownerships but also independent updates of each image by interested ecosystem entities. We discuss our experience of delivering Treble architectural changes to silicon vendors and device makers using a yearly release model. Our experiments and industry rollouts support our hypothesis that giving more freedom to all ecosystem entities and creating an equilibrium are a transformation necessary to further scale the world largest open source ecosystem with over two billion active devices. © 2019 Association for Computing Machinery.",Open source ecosystem; Software architecture; Software update,Android (operating system); Application programming interfaces (API); C++ (programming language); Ecosystems; Java programming language; Open systems; Program debugging; Software architecture; Architectural changes; Backward compatibility; Cross-layer communications; Hardware independent; Interprocess communication; Open sources; Software ecosystems; Software updates; Open source software
"Coherent extension, composition, and merging operators in contract models for system design",2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073151547&doi=10.1145%2f3358216&partnerID=40&md5=25500ac6b380f590b3f7c802f28618b9,"Contract models have been proposed to promote and facilitate reuse and distributed development. In this paper, we cast contract models into a coherent formalism used to derive general results about the properties of their operators. We study several extensions of the basic model, including the distinction between weak and strong assumptions and maximality of the specification. We then analyze the disjunction and conjunction operators, and show how they can be broken up into a sequence of simpler operations. This leads to the definition of a new contract viewpoint merging operator, which better captures the design intent in contrast to the more traditional conjunction. The adjoint operation, which we call separation, can be used to repartition the specification into different viewpoints. We show the symmetries of these operations with respect to composition and quotient. © 2019 Association for Computing Machinery.",Conformance; Contract; Interface; Merging; Separation,Contracts; Interfaces (materials); Separation; Specifications; Adjoints; Conformance; Design intent; Distributed development; Maximality; Merging operators; New contract; Merging
Ready: A fine-grained multithreading overlay framework for modern CPU-FPGA dataflow applications,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073155232&doi=10.1145%2f3358187&partnerID=40&md5=e8386c507eda12617f7622efc62edd82,"In this work, we propose a framework called REconfigurable Accelerator DeploY (READY), the first framework to support polynomial runtime mapping of dataflow applications in high-performance CPU-FPGA platforms. READY introduces an efficient mapping with fine-grained multithreading onto an overlay architecture that hides the latency of a global interconnection network. In addition to our overlay architecture, we show how this system helps solve some of the challenges for FPGA cloud computing adoption in high-performance computing. The framework encapsulates dataflow descriptions by using a target independent, high-level API, and a dataflow model that allows for explicit spatial and temporal parallelism. READY directly maps the dataflow kernels onto the accelerator. Our tool is flexible and extensible and provides the infrastructure to explore different accelerator designs. We validate READY on the Intel Harp platform, and our experimental results show an average 2x execution runtime improvement when compared to an 8-thread multi-core processor. © 2019 Association for Computing Machinery.",CGRA; Dataflow; Fine-grained multithreading; High performance,Data flow analysis; Distributed computer systems; Field programmable gate arrays (FPGA); Interconnection networks (circuit switching); Mapping; Network architecture; CGRA; Dataflow; High performance; High performance computing; Multi-core processor; Multi-threading; Overlay architecture; Temporal parallelism; Multitasking
Achieving super-linear speedup across multi-FPGA for real-time DNN inference,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073156951&doi=10.1145%2f3358192&partnerID=40&md5=dd9bc5b3dd72741761162c33f249c329,"Real-time Deep Neural Network (DNN) inference with low-latency requirement has become increasingly important for numerous applications in both cloud computing (e.g., Apple's Siri) and edge computing (e.g., Google/Waymo's driverless car). FPGA-based DNN accelerators have demonstrated both superior flexibility and performance; in addition, for real-time inference with low batch size, FPGA is expected to achieve further performance improvement. However, the performance gain from the single-FPGA design is obstructed by the limited on-chip resource. In this paper, we employ multiple FPGAs to cooperatively run DNNs with the objective of achieving super-linear speed-up against single-FPGA design. In implementing such systems, we found two barriers that hinder us from achieving the design goal: (1) the lack of a clear partition scheme for each DNN layer to fully exploit parallelism, and (2) the insufficient bandwidth between the off-chip memory and the accelerator due to the growing size of DNNs. To tackle these issues, we propose a general framework, “Super-LIP”, which can support different kinds of DNNs. In this paper, we take Convolutional Neural Network (CNN) as a vehicle to illustrate Super-LIP. We first formulate an accurate system-level model to support the exploration of best partition schemes. Then, we develop a novel design methodology to effectively alleviate the heavy loads on memory bandwidth by moving traffic from memory bus to inter-FPGA links. We implement Super-LIP based on ZCU102 FPGA boards. Results demonstrate that Super-LIP with 2 FPGAs can achieve 3.48× speedup, compared to the state-of-the-art single-FPGA design. What is more, as the number of FPGAs scales up, the system latency can be further reduced while maintaining high energy efficiency. © 2019 Association for Computing Machinery.",DNN inference; FPGA; Parallel computing; Real-time,Bandwidth; Deep neural networks; Distributed computer systems; Energy efficiency; Field programmable gate arrays (FPGA); Neural networks; Parallel processing systems; Convolutional neural network; DNN inference; High energy efficiency; Novel design methodology; Partition schemes; Real time; Real-time inference; System-level modeling; Integrated circuit design
Achieving lossless accuracy with lossy programming for efficient neural-network training on NVM-based systems,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073161104&doi=10.1145%2f3358191&partnerID=40&md5=494b0c730384816bf873cb6c65b60114,"Neural networks over conventional computing platforms are heavily restricted by the data volume and performance concerns. While non-volatile memory offers potential solutions to data volume issues, challenges must be faced over performance issues, especially with asymmetric read and write performance. Beside that, critical concerns over endurance must also be resolved before non-volatile memory could be used in reality for neural networks. This work addresses the performance and endurance concerns altogether by proposing a data-aware programming scheme. We propose to consider neural network training jointly with respect to the data-flow and data-content points of view. In particular, methodologies with approximate results over Dual-SET operations were presented. Encouraging results were observed through a series of experiments, where great efficiency and lifetime enhancement is seen without sacrificing the result accuracy. © 2019 Association for Computing Machinery.",Lossy programming; Neural network; Non-volatile memory; Phase-change memory,Neural networks; Nonvolatile storage; Approximate results; Computing platform; Data contents; Lifetime enhancement; Neural network training; Non-volatile memory; Performance issues; Set operation; Phase change memory
Aggregation strategies in reachable set computation of hybrid systems,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073149634&doi=10.1145%2f3358214&partnerID=40&md5=3b9e12bb6974598599a0c880681b44d6,"Computing the set of reachable states is a widely used technique for proving that a hybrid system satisfies its safety specification. Flow-pipe construction methods interleave phases of computing continuous successors and phases of computing discrete successors. Directly doing this leads to a combinatorial explosion problem, though, as with each discrete successor there may be an interval of time where the transition can occur, so that the number of paths becomes exponential in the number of discrete transitions. For this reason, most reachable set computation tools implement some form of set aggregation for discrete transitions, such as, performing a template-based overapproximation or convex hull aggregation. These aggregation methods, however, in theory can lead to unbounded error, and in practice are often the root cause of why a safety specification cannot be proven. This paper proposes techniques for improving the accuracy of the aggregation operations performed for reachable set computation. First, we present two aggregation strategies over generalized stars, namely convex hull aggregation and template based aggregation. Second, we perform adaptive deaggregation using a data structure called Aggregated Directed Acyclic Graph (AGGDAG). Our deaggregation strategy is driven by counterexamples and hence has soundness and relative completeness guarantees. We demonstrate the computational benefits of our approach through two case studies involving satellite rendezvous and gearbox meshing. © 2019 Association for Computing Machinery.",Adaptive deaggregation; Aggregations for reachable set; Hybrid systems; Linear differential equations; Reachable set,Computational geometry; Differential equations; Directed graphs; Explosions; Hybrid systems; Specifications; Aggregation operation; Combinatorial explosion; De aggregations; Directed acyclic graph (DAG); Linear differential equation; Reachable set; Safety specifications; Soundness and relative completeness; Computation theory
FPGA stream-monitoring of real-time properties,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073151283&doi=10.1145%2f3358220&partnerID=40&md5=405c0ca99127e02b1335e6722e8089e6,"An essential part of cyber-physical systems is the online evaluation of real-time data streams. Especially in systems that are intrinsically safety-critical, a dedicated monitoring component inspecting data streams to detect problems at runtime greatly increases the confidence in a safe execution. Such a monitor needs to be based on a specification language capable of expressing complex, high-level properties using only the accessible low-level signals. Moreover, tight constraints on computational resources exacerbate the requirements on the monitor. Thus, several existing approaches to monitoring are not applicable due to their dependence on an operating system. We present an FPGA-based monitoring approach by compiling an RTLola specification into synthesizable VHDL code. RTLola is a stream-based specification language capable of expressing complex real-time properties while providing an upper bound on the execution time and memory requirements. The statically determined memory bound allows for a compilation to an FPGA with a fixed size. An advantage of FPGAs is a simple integration process in existing systems and superb executing time. The compilation results in a highly parallel implementation thanks to the modular nature of RTLola specifications. This further increases the maximal event rate the monitor can handle. © 2019 Association for Computing Machinery.",FPGA; Real-time properties; Runtime verification; Stream specification languages,Computer hardware description languages; Embedded systems; Field programmable gate arrays (FPGA); High level languages; Online systems; Safety engineering; Specification languages; Specifications; Computational resources; Integration process; Memory requirements; Monitoring approach; On-line evaluation; Real-time data streams; Real-time properties; Run-time verification; Real time systems
Alleria: An advanced memory access profiling framework,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073161076&doi=10.1145%2f3358193&partnerID=40&md5=9aa2065929da00282b7e752b0a26627d,"Application analysis and simulation tools are used extensively by embedded system designers to improve existing optimization techniques or develop new ones. We propose the Alleria framework to make it easier for designers to comprehensively collect critical information such as virtual and physical memory addresses, accessed values, and thread schedules about one or more target applications. Such profilers often incur substantial performance overheads that are orders of magnitude larger than native execution time. We discuss how that overhead can be significantly reduced using a novel profiling mechanism called adaptive profiling. We develop a heuristic-based adaptive profiling mechanism and evaluate its performance using single-threaded and multi-threaded applications. The proposed technique can improve profiling throughput by up to 145% and by 37% on an average, enabling Alleria to be used to comprehensively profile applications with a throughput of over 3 million instructions per second. © 2019 Association for Computing Machinery.",Dynamic binary instrumentation; Memory access tracing; Persistent memory; Profiling,Virtual addresses; Application analysis; Dynamic binary instrumentation; Memory access; Million instructions per seconds; Multi- threaded applications; Optimization techniques; Persistent memory; Profiling; Memory architecture
Predictncool: Leakage aware thermal management for 3D memories using a lightweight temperature predictor,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073161052&doi=10.1145%2f3358208&partnerID=40&md5=dd3702e1402fad70677b77131a950ecd,"Recent research on mitigating thermal problems in 3D memories has covered reactive strategies that reduce memory power consumption, and thereby, performance, when the memory temperature reaches the maximum operating limit. Such techniques could benefit from temperature prediction and avoid unnecessary invocations and state transitions of the thermal management strategy. We develop an accurate steady state temperature predictor for thermal management of 3D memories. We utilize the symmetries in the floorplan, along with other design insights, to reduce the predictor's model parameters, making it lightweight and suitable for runtime thermal management. Using the temperature prediction, we introduce PredictNcool, a proactive thermal management strategy to reduce application runtime and memory energy. We compare PredictNcool with two recent thermal management strategies and our experiments show that the proposed optimization results in performance improvements of 28% and 5%, and memory subsystem energy reductions of 38% and 12% (on average). © 2019 Association for Computing Machinery.",3D memories; Energy efficiency; Thermal management,Energy efficiency; 3D memory; Memory subsystems; Model parameters; Recent researches; State transitions; Steady-state temperature; Temperature prediction; Thermal management strategy; Temperature control
Unified testing and security framework for wireless network-on-chip enabled multi-core chips,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073150674&doi=10.1145%2f3358212&partnerID=40&md5=908d40ac8f22561fd2abbbaeaadee9bc,"On-chip wireless interconnects have been demonstrated to improve the performance and energy consumption of data communication in Network-on-Chips (NoCs). However, the wireless interfaces (WIs) can be defective, rendering these broken links severely affect the performance. This makes manufacturing test of the WIs critical. While analog testing of the transceivers is possible, such methodologies are impractical in a Wireless NoC (WiNoC) due to large overheads. In addition to testing, security is another prominent challenge in WiNoCs, as the security breach can happen due to embedded hardware Trojans or through external attacker exploiting the wireless medium. The typical security measures used in general wireless networks are not practical in a WiNoC due to unique network architectures and performance requirements of such a system. However, both testing and security defense can potentially leverage a basic monitoring framework which, can detect malfunctions or anomalies. Based on this idea, we propose a unified architecture for testing and attack detection and protection of on-chip wireless interconnects. We adopt a Built-In-Self Test (BIST) methodology to enable online monitoring of the wireless interconnects which can also be reused for monitoring the security threats. We focus on manufacturing defects of the WIs for testing and persistent jamming attack for the security measures, as this kind of attack is most likely on wireless communication systems. The BIST methodology is capable of detecting faults in the wireless links with a low aliasing probability of 2.32 × 10−10. Additionally, the proposed unified architecture is able to detect the persistent jamming with an accuracy of 99.87% and suffer < 3% communication bandwidth degradation even in the presence of attacks from either internal or external sources. © 2019 Association for Computing Machinery.",DoS; Jamming; Machine learning; Network-on-chip; Security; Wireless interconnect,Built-in self test; Computer architecture; Defects; Distributed computer systems; DOS; Energy utilization; Fault tolerant computer systems; Integrated circuit interconnects; Jamming; Learning systems; Malware; Manufacture; Network architecture; Network-on-chip; Radio transceivers; Servers; Wireless interconnects; Wireless networks; Communication bandwidth; Manufacturing defects; Manufacturing tests; Monitoring frameworks; Performance requirements; Security; Unified architecture; Wireless communication system; Network security
An ultra-low energy human activity recognition accelerator for wearable health applications,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073160757&doi=10.1145%2f3358175&partnerID=40&md5=f1a6e29c01b0e034ce445330e4187ce9,"Human activity recognition (HAR) has recently received significant attention due to its wide range of applications in health and activity monitoring. The nature of these applications requires mobile or wearable devices with limited battery capacity. User surveys show that charging requirement is one of the leading reasons for abandoning these devices. Hence, practical solutions must offer ultra-low power capabilities that enable operation on harvested energy. To address this need, we present the first fully integrated custom hardware accelerator (HAR engine) that consumes 22.4 μJ per operation using a commercial 65 nm technology. We present a complete solution that integrates all steps of HAR, i.e., reading the raw sensor data, generating features, and activity classification using a deep neural network (DNN). It achieves 95% accuracy in recognizing 8 common human activities while providing three orders of magnitude higher energy efficiency compared to existing solutions. © 2019 Association for Computing Machinery.",Flexible hybrid electronics; Hardware accelerator; Health monitoring; Human activity recognition; Low-power design; Wearable electronics,Deep neural networks; Electric power supplies to apparatus; Energy efficiency; Flexible electronics; Health; Pattern recognition; Activity classifications; Activity monitoring; Hardware accelerators; Health monitoring; Human activity recognition; Low-power design; Practical solutions; Three orders of magnitude; Wearable technology
Statistical verification of hyperproperties for cyber-physical systems,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073146582&doi=10.1145%2f3358232&partnerID=40&md5=a7604b552fdd508dcf856c2211bb806a,"Many important properties of cyber-physical systems (CPS) are defined upon the relationship between multiple executions simultaneously in continuous time. Examples include probabilistic fairness and sensitivity to modeling errors (i.e., parameters changes) for real-valued signals. These requirements can only be specified by hyperproperties. In this article, we focus on verifying probabilistic hyperproperties for CPS. To cover a wide range of modeling formalisms, we first propose a general model of probabilistic uncertain systems (PUSs) that unify commonly studied CPS models such as continuous-time Markov chains (CTMCs) and probabilistically parametrized Hybrid I/O Automata (P2HIOA). To formally specify hyperproperties, we propose a new temporal logic, hyper probabilistic signal temporal logic (HyperPSTL) that serves as a hyper and probabilistic version of the conventional signal temporal logic (STL). Considering the complexity of real-world systems that can be captured as PUSs, we adopt a statistical model checking (SMC) approach for their verification. We develop a new SMC technique based on the direct computation of significance levels of statistical assertions for HyperPSTL specifications, which requires no a priori knowledge on the indifference margin. Then, we introduce SMC algorithms for HyperPSTL specifications on the joint probabilistic distribution of multiple paths, as well as specifications with nested probabilistic operators quantifying different paths, which cannot be handled by existing SMC algorithms. Finally, we show the effectiveness of our SMC algorithms on CPS benchmarks with varying levels of complexity, including the Toyota Powertrain Control System. © 2019 Association for Computing Machinery.",Cyber-physical systems; Embedded control software; Hyperproperties; Statistical model checking,Continuous time systems; Cyber Physical System; Markov processes; Model checking; Probability distributions; Specifications; Temporal logic; Continuous time Markov chain; Cyber-physical systems (CPS); Embedded control software; Hyperproperties; Probabilistic operator; Significance levels; Statistical model checking; Statistical verification; Embedded systems
Numerical representation of directed acyclic graphs for efficient dataflow embedded resource allocation,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073162654&doi=10.1145%2f3358225&partnerID=40&md5=752f64603109afecacbaf0bc18a1db68,"Stream processing applications running on Heterogeneous Multi-Processor Systems on Chips (HMPSoCs) require efficient resource allocation and management, both at compile-time and at runtime. To cope with modern adaptive applications whose behavior can not be exhaustively predicted at compile-time, runtime managers must be able to take resource allocation decisions on-the-fly, with a minimum overhead on application performance. Resource allocation algorithms often rely on an internal modeling of an application. Directed Acyclic Graph (DAGs) are the most commonly used models for capturing control and data dependencies between tasks. DAGs are notably often used as an intermediate representation for deploying applications modeled with a dataflow Model of Computation (MoC) on HMPSoCs. Building such intermediate representation at runtime for massively parallel applications is costly both in terms of computation and memory overhead. In this paper, an intermediate representation of DAGs for resource allocation is presented. This new representation shows improved performance for run-time analysis of dataflow graphs with less overhead in both computation time and memory footprint. The performances of the proposed representation are evaluated on a set of computer vision and machine learning applications. © 2019 Association for Computing Machinery.",Dataflow; Numerical modeling; Resource allocation,Directed graphs; Numerical models; Resource allocation; System-on-chip; Dataflow; Directed acyclic graph (DAG); Efficient resource allocation; Intermediate representations; Machine learning applications; Numerical representation; Resource allocation algorithms; Resource allocation decision; Data flow analysis
DMazerunner: Executing perfectly nested loops on dataflow accelerators,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073149210&doi=10.1145%2f3358198&partnerID=40&md5=2a205f34a7af1120d5aa17d845af0553,"Dataflow accelerators feature simplicity, programmability, and energy-efficiency and are visualized as a promising architecture for accelerating perfectly nested loops that dominate several important applications, including image and media processing and deep learning. Although numerous accelerator designs are being proposed, how to discover the most efficient way to execute the perfectly nested loop of an application onto computational and memory resources of a given dataflow accelerator (execution method) remains an essential and yet unsolved challenge. In this paper, we propose dMazeRunner - to efficiently and accurately explore the vast space of the different ways to spatiotemporally execute a perfectly nested loop on dataflow accelerators (execution methods). The novelty of dMazeRunner framework is in: i) a holistic representation of the loop nests, that can succinctly capture the various execution methods, ii) accurate energy and performance models that explicitly capture the computation and communication patterns, data movement, and data buffering of the different execution methods, and iii) drastic pruning of the vast search space by discarding invalid solutions and the solutions that lead to the same cost. Our experiments on various convolution layers (perfectly nested loops) of popular deep learning applications demonstrate that the solutions discovered by dMazeRunner are on average 9.16× better in Energy-Delay-Product (EDP) and 5.83× better in execution time, as compared to prior approaches. With additional pruning heuristics, dMazeRunner reduces the search time from days to seconds with a mere 2.56% increase in EDP, as compared to the optimal solution. © 2019 Association for Computing Machinery.",Analytical model; Coarse-grained reconfigurable array; Dataflow; Deep neural networks; Design space exploration; Energy-efficiency; Loop optimization; Mapping; Systolic arrays,Acceleration; Analytical models; Deep neural networks; Energy efficiency; Mapping; Optimization; Systolic arrays; Accelerator design; Coarse-grained reconfigurable arrays; Communication pattern; Dataflow; Design space exploration; Energy delay product; Loop optimizations; Perfectly nested loops; Data flow analysis
Deriving equations from sensor data using dimensional function synthesis,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073147310&doi=10.1145%2f3358218&partnerID=40&md5=f6881941041a95f98a41323fb806b654,"We present a new method for deriving functions that model the relationship between multiple signals in a physical system. The method, which we call dimensional function synthesis, applies to data streams where the dimensions of the signals are known. The method comprises two phases: a compile-time synthesis phase and a subsequent calibration using sensor data. We implement dimensional function synthesis and use the implementation to demonstrate efficiently summarizing multi-modal sensor data for two physical systems using 90 laboratory experiments and 10 000 synthetic idealized measurements. We evaluate the performance of the compile-time phase of dimensional function synthesis as well as the calibration phase overhead, inference latency, and accuracy of the models our method generates. The results show that our technique can generate models in less than 300 ms on average across all the physical systems we evaluated. When calibrated with sensor data, our models outperform traditional regression and neural network models in inference accuracy in all the cases we evaluated. In addition, our models perform better in training latency (over 8660× improvement) and required arithmetic operations in inference (over 34× improvement). These significant gains are largely the result of exploiting information on the physics of signals that has hitherto been ignored. © 2019 Association for Computing Machinery.",Dimensional analysis; Machine learning; Sensor data fusion,Calibration; Learning systems; Arithmetic operations; Dimensional analysis; Dimensional functions; Laboratory experiments; Multimodal sensor; Neural network model; Physical systems; Synthesis phase; Sensor data fusion
Efficient tracing methodology using automata processor,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073162031&doi=10.1145%2f3358200&partnerID=40&md5=78b35af2db213f402b14c4cfcb1ff483,"Tracing or trace interface has been used in various ways to find system defects or bugs. As embedded systems are increasingly used in safety-critical applications, tracing can provide useful information during system execution at runtime. Non-intrusive tracing that does not affect system performance has become especially important, but unfortunately, the biggest obstacle to this approach was the vast amount of real-time trace data, making it challenging to address complex requirements with relatively limited hardware implementations. Automata processors can be programmed with a memory-like structure of automata and have a structure specific to streaming data, large capacity, and parallel processing functions. This paper promotes the idea of high-level system-on-chip monitoring using automata processors. We used a safety-critical pacemaker application in the experiments, described timed automata (TA)-based requirements, and tested intentionally injected 4,000 random failures. The TA model converted for Automata Processor to monitor system, correctness, and safety properties achieved 100% failure detection rate in the experiment, and the detected failure is reported as fast enough to allow enough extent for failure recovery. © 2019 Association for Computing Machinery.",Runtime verification; Tracing; Tracing methodology,Automata theory; Distributed computer systems; Embedded systems; Program debugging; Safety engineering; System-on-chip; Failure detection; Hardware implementations; High-level systems; Parallel processing; Run-time verification; Safety critical applications; Tracing; Tracing methodology; Monitoring
Safety verification of cyber-physical systems with reinforcement learning control,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073143634&doi=10.1145%2f3358230&partnerID=40&md5=a4f65b592a9b9922570f3b3e41911e16,"This paper proposes a new forward reachability analysis approach to verify safety of cyber-physical systems (CPS) with reinforcement learning controllers. The foundation of our approach lies on two efficient, exact and over-approximate reachability algorithms for neural network control systems using star sets, which is an efficient representation of polyhedra. Using these algorithms, we determine the initial conditions for which a safety-critical system with a neural network controller is safe by incrementally searching a critical initial condition where the safety of the system cannot be established. Our approach produces tight over-approximation error and it is computationally efficient, which allows the application to practical CPS with learning enable components (LECs). We implement our approach in NNV, a recent verification tool for neural networks and neural network control systems, and evaluate its advantages and applicability by verifying safety of a practical Advanced Emergency Braking System (AEBS) with a reinforcement learning (RL) controller trained using the deep deterministic policy gradient (DDPG) method. The experimental results show that our new reachability algorithms are much less conservative than existing polyhedra-based approaches. We successfully determine the entire region of the initial conditions of the AEBS with the RL controller such that the safety of the system is guaranteed, while a polyhedra-based approach cannot prove the safety properties of the system. © 2019 Association for Computing Machinery.",Formal methods; Reinforcement learning; Verification,Control systems; Controllers; Cyber Physical System; Embedded systems; Formal methods; Formal verification; Geometry; Machine learning; Neural networks; Safety engineering; Verification; Advanced emergency braking systems; Computationally efficient; Cyber-physical systems (CPS); Neural network control; Neural network controllers; Reachability analysis; Reinforcement learning control; Safety critical systems; Reinforcement learning
Parametric scheduler characterization,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073155064&doi=10.1145%2f3358226&partnerID=40&md5=532ea83f47ac69c71d59b98d193cac0f,"Schedulers assign starting times to events in a system such that a set of constraints is met and system productivity is maximized. We characterize the scheduler behaviour for the case where decisions are made by comparing affine expressions of design parameters such as task workload, processing speed, robot travelling speed, or a controller's rise and settling time. Deterministic schedulers can be extended with symbolic execution, to keep track of the affine conditions on the parameters for which the scheduling decisions are made. We introduce a divide-and-conquer algorithm that uses this information to determine parameter regions for which the same sequence of decisions is taken given a particular scenario. The results provide designers insight in the impact of parameter changes on the performance of their system. The exploration can also be executed with the KLEE symbolic execution engine of the LLVM tool chain to extract the same results. We show that the divide-and-conquer approach provides the results much faster than the generic symbolic execution engine of KLEE. The results allow visualization of the sensitivity to all parameter combinations. The results of our approach therefore provide more insight in the sensitivity to parameters. © 2019 Association for Computing Machinery.",Re-entrant flexible manufacturing system; Real time scheduling; System design,Engines; Flexible manufacturing systems; Machine design; Model checking; Scheduling; Systems analysis; Divide-and-conquer algorithm; Divide-and-conquer approach; Parameter combination; Real - time scheduling; Scheduling decisions; Sensitivity to parameters; Symbolic execution; System productivity; Parameter estimation
Multi-objective exploration for practical optimization decisions in binary translation,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073154132&doi=10.1145%2f3358185&partnerID=40&md5=5ff2102b096614b814f911758662d3ef,"In the design of mobile systems, hardware/software (HW/SW) co-design has important advantages by creating specialized hardware for the performance or power optimizations. Dynamic binary translation (DBT) is a key component in co-design. During the translation, a dynamic optimizer in the DBT system applies various software optimizations to improve the quality of the translated code. With dynamic optimization, optimization time is an exposed run-time overhead and useful analyses are often restricted due to their high costs. Thus, a dynamic optimizer needs to make smart decisions with limited analysis information, which complicates the design of optimization decision models and often causes failures in human-made heuristics. In mobile systems, this problem is even more challenging because of strict constraints on computing capabilities and memory size. To overcome the challenge, we investigate an opportunity to build practical optimization decision models for DBT by using machine learning techniques. As the first step, loop unrolling is chosen as the representative optimization. We base our approach on the industrial strength DBT infrastructure and conduct evaluation with 17,116 unrollable loops collected from 200 benchmarks and real-life programs across various domains. By utilizing all available features that are potentially important for loop unrolling decision, we identify the best classification algorithm for our infrastructure with consideration for both prediction accuracy and cost. The greedy feature selection algorithm is then applied to the classification algorithm to distinguish its significant features and cut down the feature space. By maintaining significant features only, the best affordable classifier, which satisfies the budgets allocated to the decision process, shows 74.5% of prediction accuracy for the optimal unroll factor and realizes an average 20.9% reduction in dynamic instruction count during the steady-state translated code execution. For comparison, the best baseline heuristic achieves 46.0% prediction accuracy with an average 13.6% instruction count reduction. Given that the infrastructure is already highly optimized and the ideal upper bound for instruction reduction is observed at 23.8%, we believe this result is noteworthy. © 2019 Association for Computing Machinery.",Loop unrolling,Budget control; Classification (of information); Cost benefit analysis; Forecasting; Hardware-software codesign; Learning systems; Optimal systems; Classification algorithm; Dynamic binary translation; Dynamic instructions; Dynamic optimization; Feature selection algorithm; Loop unrolling; Machine learning techniques; Software optimization; Optimization
Synterface: Efficient chip-to-world interfacing for flow-based microfluidic biochips using pin-count minimization,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073161338&doi=10.1145%2f3358188&partnerID=40&md5=83c92c745b701feb093f4868684b1e09,"Flow-based microfluidic biochips can be used to perform bioassays by manipulating a large number of on-chip valves. These biochips are increasingly used today for biomolecular recognition, single-cell screening, and point-of-care disease diagnostics, and design-automation solutions for flow-based microfluidics enable the mapping and optimization of bimolecular protocols and software-based valve control. However, a key problem that has not received adequate attention is chip-to-world interfacing, which requires the use of off-chip control equipment to provide control signals for the on-chip valves. This problem is exacerbated by the increase in the number of valves as chips get more complex. To address the interfacing problem, we present an efficient pin-count minimization (synthesis) problem, referred to as Synterface, which uses on-chip microfluidic logic gates and optimization based on concepts from linear algebra. We present results to show that Synterface significantly reduces pin-count and simplifies the external interface for flow-based microfluidics. © 2019 Association for Computing Machinery.",Chip-to-world interfacing; Logic gates; Microfluidic biochips; Synthesis,Biochips; Computer circuits; Control equipment; Diagnosis; Disease control; Linear algebra; Logic gates; Logic Synthesis; Microprocessor chips; Synthesis (chemical); Biomolecular recognition; Chip-to-world interfacing; Control signal; Design automations; Disease diagnostics; Micro fluidic biochips; On-chip microfluidics; Point of care; Microfluidics
Structural test coverage criteria for deep neural networks,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073145142&doi=10.1145%2f3358233&partnerID=40&md5=6013514980df71bf18493940a3dd9860,"Deep neural networks (DNNs) have a wide range of applications, and software employing them must be thoroughly tested, especially in safety-critical domains. However, traditional software test coverage metrics cannot be applied directly to DNNs. In this paper, inspired by the MC/DC coverage criterion, we propose a family of four novel test coverage criteria that are tailored to structural features of DNNs and their semantics. We validate the criteria by demonstrating that test inputs that are generated with guidance by our proposed coverage criteria are able to capture undesired behaviours in a DNN. Test cases are generated using a symbolic approach and a gradient-based heuristic search. By comparing them with existing methods, we show that our criteria achieve a balance between their ability to find bugs (proxied using adversarial examples and correlation with functional coverage) and the computational cost of test input generation. Our experiments are conducted on state-of-the-art DNNs obtained using popular open source datasets, including MNIST, CIFAR-10 and ImageNet. © 2019 Association for Computing Machinery.",Neural networks; Test case generation; Test criteria,Application programs; Heuristic algorithms; Neural networks; Open source software; Safety engineering; Semantics; Software testing; Computational costs; Functional coverage; Safety-critical domain; Structural feature; Test case generation; Test coverage criteria; Test coverage metrics; Test criteria; Deep neural networks
Optode design space exploration for clinically-robust non-invasive fetal oximetry,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073149464&doi=10.1145%2f3358207&partnerID=40&md5=557aa4621879c0a5cb4ad097f5e54eb2,"Non-invasive transabdominal fetal oximetry (TFO) has the potential to improve delivery outcomes by providing physicians with an objective metric of fetal well-being during labor. Fundamentally, the technology is based on sending light through the maternal abdomen to investigate deep fetal tissue, followed by detection and processing of the light that returns (via scattering) to the outside of the maternal abdomen. The placement of the photodetector in relation to the light source critically impacts TFO system performance, including its operational robustness in the face of fetal depth variation. However, anatomical differences between pregnant women cause the fetal depths to vary drastically, which further complicates the optical probe (optode) design optimization. In this paper, we present a methodology to solve this problem. We frame optode design space exploration as a multi-objective optimization problem, where hardware complexity (cost) and performance across a wider patient population (robustness) form competing objectives. We propose a model-based approach to characterize the Pareto-optimal points in the optode design space, through which a specific design is selected. Experimental evaluation via simulation and in vivo measurement on pregnant sheep support the efficacy of our approach. © 2019 Association for Computing Machinery.",Design optimization; Design space exploration; Internet of medical things; Medical cyber-physical systems; Multi-objective optimization; Non-invasive fetal oximetry,Embedded systems; Light sources; Pareto principle; Design optimization; Design space exploration; Experimental evaluation; Hardware complexity; Medical cyber physical systems; Model based approach; Multi-objective optimization problem; Oximetry; Multiobjective optimization
Thermal-aware scheduling for integrated CPUS-GPU platforms,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073163292&doi=10.1145%2f3358235&partnerID=40&md5=59ae22e14482315434b258a55f543ced,"As modern embedded systems like cars need high-power integrated CPUs-GPU SoCs for various real-time applications such as lane or pedestrian detection, they face greater thermal problems than before, which may, in turn, incur higher failure rate and cooling cost. We demonstrate, via experimentation on a representative CPUs-GPU platform, the importance of accounting for two distinct thermal characteristics-the platform's temperature imbalance and different power dissipations of different tasks-in real-time scheduling to avoid any burst of power dissipations while guaranteeing all timing constraints. To achieve this goal, we propose a new Real-Time Thermal-Aware Scheduling (RT-TAS) framework. We first capture different CPU cores' temperatures caused by different GPU power dissipations (i.e., CPUs-GPU thermal coupling) with core-specific thermal coupling coefficients. We then develop thermally-balanced task-to-core assignment and CPUs-GPU co-scheduling. The former addresses the platform's temperature imbalance by efficiently distributing the thermal load across cores while preserving scheduling feasibility. Building on the thermally-balanced task assignment, the latter cooperatively schedules CPU and GPU computations to avoid simultaneous peak power dissipations on both CPUs and GPU, thus mitigating excessive temperature rises while meeting task deadlines. We have implemented and evaluated RT-TAS on an automotive embedded platform to demonstrate its effectiveness in reducing the maximum temperature by 6−12.2◦C over existing approaches without violating any task deadline. © 2019 Association for Computing Machinery.",Embedded systems; GPU; Real-time systems; Thermal management,Cooling systems; Electric losses; Embedded systems; Failure analysis; Graphics processing unit; Interactive computer systems; Program processors; Scheduling; Temperature control; Thermal management (electronics); Maximum temperature; Modern embedded systems; Pedestrian detection; Real - time scheduling; Real-time application; Scheduling feasibilities; Thermal characteristics; Thermal-aware scheduling; Real time systems
"Graph-based modeling, scheduling, and verification for intersection management of intelligent vehicles",2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073145900&doi=10.1145%2f3358221&partnerID=40&md5=40b17f93b1a692e1828615308dae6402,"Intersection management is one of the most representative applications of intelligent vehicles with connected and autonomous functions. The connectivity provides environmental information that a single vehicle cannot sense, and the autonomy supports precise vehicular control that a human driver cannot achieve. Intersection management solves the fundamental conflict resolution problem for vehicles-two vehicles should not appear at the same location at the same time, and, if they intend to do that, an order should be decided to optimize certain objectives such as the traffic throughput or smoothness. In this paper, we first propose a graph-based model for intersection management. The model is general and applicable to different granularities of intersections and other conflicting scenarios. We then derive formal verification approaches which can guarantee deadlock-freeness. Based on the graph-based model and the verification approaches, we develop a centralized cycle removal algorithm for the graph-based model to schedule vehicles to go through the intersection safely (without collisions) and efficiently without deadlocks. Experimental results demonstrate the expressiveness of the proposed model and the effectiveness and efficiency of the proposed algorithm. © 2019 Association for Computing Machinery.",Conflict resolution; Connected and autonomous vehicles; Cycle removal; Intersection management; Verification,Formal verification; Intelligent vehicle highway systems; Scheduling; Vehicles; Verification; Autonomous functions; Conflict Resolution; Different granularities; Effectiveness and efficiencies; Environmental information; Graph-based modeling; Intersection managements; Removal algorithms; Graphic methods
Analytical performance models for NoCs with multiple priority traffic classes,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073152155&doi=10.1145%2f3358176&partnerID=40&md5=55e179ee070ef8bef6d3e6d1104ff8ea,"Networks-on-chip (NoCs) have become the standard for interconnect solutions in industrial designs ranging from client CPUs to many-core chip-multiprocessors. Since NoCs play a vital role in system performance and power consumption, pre-silicon evaluation environments include cycle-accurate NoC simulators. Long simulations increase the execution time of evaluation frameworks, which are already notoriously slow, and prohibit design-space exploration. Existing analytical NoC models, which assume fair arbitration, cannot replace these simulations since industrial NoCs typically employ priority schedulers and multiple priority classes. To address this limitation, we propose a systematic approach to construct priority-aware analytical performance models using micro-architecture specifications and input traffic. Our approach decomposes the given NoC into individual queues with modified service time to enable accurate and scalable latency computations. Specifically, we introduce novel transformations along with an algorithm that iteratively applies these transformations to decompose the queuing system. Experimental evaluations using real architectures and applications show high accuracy of 97% and up to 2.5× speedup in full-system simulation. © 2019 Association for Computing Machinery.",NoC performance analysis; Priority-based NoC; Queuing networks,Analytical models; Computer architecture; Iterative methods; Network architecture; Program processors; Queueing networks; Queueing theory; Systems analysis; Analytical performance model; Design space exploration; Evaluation framework; Experimental evaluation; Full-system simulation; Performance analysis; Priority-based; Queuing network; Network-on-chip
Flora: Floorplan optimizer for reconfigurable areas in FPGAs,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073162759&doi=10.1145%2f3358202&partnerID=40&md5=b2620f024eda3a5c2405d9d9cd49c3e5,"Floorplanning is a mandatory step in the design of hardware accelerators for FPGA platforms, especially when adopting dynamic partial reconfiguration (DPR). This paper presents FLORA, an automated floorplanner based on optimization via Mixed-Integer Linear Programming (MILP). The floorplanning problem is solved by means of a novel fine-grained modeling strategy of FPGA resources. Furthermore, differently from other proposals, our approach takes into account several realistic Partial Reconfiguration (PR) floorplanning constraints on FPGAs. FLORA was compared against state-of-the-art floorplanners by means of benchmark suites, showing that it is capable of providing better performance in terms of resource consumption, maximum inter-region, wire-length, and running time required to produce the solutions. Finally, FLORA was utilized to generate placements for a partially-reconfigurable video processing engine that was implemented on a Xilinx Zynq-7020. © 2019 Association for Computing Machinery.",Dynamic reconfiguration; Floorplanning; FPGA; Heterogenous SoC; MILP; Partial-reconfiguration,Benchmarking; Dynamic models; Field programmable gate arrays (FPGA); Integer programming; Integrated circuit design; System-on-chip; Video signal processing; Dynamic partial reconfiguration; Dynamic re-configuration; Floor-planning; MILP; Mixed-integer linear programming; Partial reconfiguration; Resource consumption; Video processing engine; Reconfigurable hardware
Memory- And communication-aware model compression for distributed deep learning inference on IoT,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073146160&doi=10.1145%2f3358205&partnerID=40&md5=8b9c7f802555ff19078cde8474121eeb,"Model compression has emerged as an important area of research for deploying deep learning models on Internet-of-Things (IoT). However, for extremely memory-constrained scenarios, even the compressed models cannot fit within the memory of a single device and, as a result, must be distributed across multiple devices. This leads to a distributed inference paradigm in which memory and communication costs represent a major bottleneck. Yet, existing model compression techniques are not communication-aware. Therefore, we propose Network of Neural Networks (NoNN), a new distributed IoT learning paradigm that compresses a large pretrained 'teacher' deep network into several disjoint and highly-compressed 'student' modules, without loss of accuracy. Moreover, we propose a network science-based knowledge partitioning algorithm for the teacher model, and then train individual students on the resulting disjoint partitions. Extensive experimentation on five image classification datasets, for user-defined memory/performance budgets, show that NoNN achieves higher accuracy than several baselines and similar accuracy as the teacher model, while using minimal communication among students. Finally, as a case study, we deploy the proposed model for CIFAR-10 dataset on edge devices and demonstrate significant improvements in memory footprint (up to 24×), performance (up to 12×), and energy per node (up to 14×) compared to the large teacher model. We further show that for distributed inference on multiple edge devices, our proposed NoNN model results in up to 33× reduction in total latency w.r.t. a state-of-the-art model compression baseline. © 2019 Association for Computing Machinery.",Communities; Model compression; Network of neural networks,Budget control; Classification (of information); Ecosystems; Internet of things; Large dataset; Students; Classification datasets; Communication cost; Communication-aware; Disjoint partition; Distributed inference; Internet of Things (IOT); Knowledge partitioning; Model compression; Deep learning
Polar: Function code aware fuzz testing of ICS protocol,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073154339&doi=10.1145%2f3358227&partnerID=40&md5=f2fd79ba05e4945bff82e7157f1d5ff6,"Industrial Control System (ICS) protocols are widely used to build communications among system components. Compared with common internet protocols, ICS protocols have more control over remote devices by carrying a specific field called “function code”, which assigns what the receive end should do. Therefore, it is of vital importance to ensure their correctness. However, traditional vulnerability detection techniques such as fuzz testing are challenged by the increasing complexity of these diverse ICS protocols. In this paper, we present a function code aware fuzzing framework - Polar, which automatically extracts semantic information from the ICS protocol and utilizes this information to accelerate security vulnerability detection. Based on static analysis and dynamic taint analysis, Polar initiates the values of the function code field and identifies some vulnerable operations. Then, novel semantic aware mutation and selection strategies are designed to optimize the fuzzing procedure. For evaluation, we implement Polar on top of two popular fuzzers - AFL and AFLFast, and conduct experiments on several widely used ICS protocols such as Modbus, IEC104, and IEC 61850. Results show that, compared with AFL and AFLFast, Polar achieves the same code coverage and bug detection numbers at the speed of 1.5X-12X. It also gains increase with 0%-91% more paths within 24 hours. Furthermore, Polar has exposed 10 previously unknown vulnerabilities in those protocols, 6 of which have been assigned unique CVE identifiers in the US National Vulnerability Database. © 2019 Association for Computing Machinery.",Function code; Fuzz testing; Industrial control system protocol; Vulnerability detection,Codes (symbols); Control systems; Semantics; Static analysis; Dynamic Taint Analysis; Function code; Fuzz Testing; Industrial control systems; National vulnerability database; Security vulnerabilities; Semantic information; Vulnerability detection; Intelligent control
"MXU: Towards predictable, flexible, and efficient memory access control for the secure IoT",2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073147812&doi=10.1145%2f3358224&partnerID=40&md5=debd848799caef5f64436ba88d82725e,"The advanced functionality requirements of modern embedded and Internet of Things (IoT) devices - from autonomous vehicles, to city and power-grid management - are driving an ever-increasing software complexity. At the same time, the pervasive internet connections of these systems necessitate the fundamental design of security into these devices. The isolation of complex features from those that are critical through protection domains is an effective means to constrain the scope of faults and security breaches. Common hardware-provided memory facilities to enforce protection domains through memory access control - including Memory Management Units (MMUs) usually found in microprocessors, and Memory Protection Units (MPUs) usually found in microcontrollers - must meet the goals of enabling flexible, efficient and dynamic management of memory, and must enable tight bounds on the worst-case execution of critical code. Unfortunately, current system memory management facilities are ill-prepared to handle this challenge: MMUs that use extensive caches to achieve strong average-case performance suffer from debilitating worst-case and even average-case behavior under hefty interference, while MPUs struggle to provide flexible memory management. This paper details MxU, a memory protection and allocation abstraction that integrates temporal specifications into the memory management subsystem, to enable portable code to achieve both predictable, tightly-bounded execution and dynamic management across both MMU- and MPU-based systems. We implement MxU in the Composite microkernel, and evaluate its flexibility and predictability over two different architectures: a MPU-based Cortex-M7 microcontroller and a MMU-based Cortex-A9 microprocessor using a suite of modern applications including neural network-based inference, SQLite, and a javascript runtime. For MMU-based systems, MxU reduces application TLB stall by up to 68.0%. For MPU-based systems, MxU enables flexible dynamic memory management often with application overheads of 1%, increasing to 6.1% under significant interference. © 2019 Association for Computing Machinery.",IoT; Memory access control; MMU; MPU,Access control; Cache memory; Complex networks; Controllers; Electric power transmission networks; Internet of things; Microcontrollers; Physical addresses; Storage allocation (computer); Average case performance; Average-case behavior; Internet connection; Internet of Things (IOT); Memory access; Software complexity; Temporal specification; Worst-case execution; Memory management units
Is your bus arbiter really fair? Restoring fairness in axi interconnects for FPGA SOCs,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073149715&doi=10.1145%2f3358183&partnerID=40&md5=39f8a51bcbac574d21bdd7681c8f87ae,"AMBA AXI is a popular bus protocol that is widely adopted as the medium to exchange data in field-programmable gate array system-on-chips (FPGA SoCs). The AXI protocol does not specify how conflicting transactions are arbitrated and hence the design of bus arbiters is left to the vendors that adopt AXI. Typically, a round-robin arbitration is implemented to ensure a fair access to the bus by the master nodes, as for the popular SoCs by Xilinx. This paper addresses a critical issue that can arise when adopting the AXI protocol under round-robin arbitration; specifically, in the presence of bus transactions with heterogeneous burst sizes. First, it is shown that a completely unfair bandwidth distribution can be achieved under some configurations, making possible to arbitrarily decrease the bus bandwidth of a target master node. This issue poses serious performance, safety, and security concerns. Second, a low-latency (one clock cycle) module named AXI burst equalizer (ABE) is proposed to restore fairness. Our investigations and proposals are supported by implementations and tests upon three modern SoCs. Experimental results are reported to confirm the existence of the issue and assess the effectiveness of the ABE with bus traffic generators and hardware accelerators from the Xilinx's IP library. © 2019 Association for Computing Machinery.",Arbitration; AXI BUS; Embedded systems; FPGA,Bandwidth; Buses; Embedded systems; System-on-chip; Arbitration; Bandwidth distribution; Bus bandwidth; Bus protocol; Clock cycles; Critical issues; Hardware accelerators; Round-robin arbitration; Field programmable gate arrays (FPGA)
ReachNN: Reachability analysis of neural-network controlled systems,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073156643&doi=10.1145%2f3358228&partnerID=40&md5=7b8eb5d51f05808859563ef923e74b77,"Applying neural networks as controllers in dynamical systems has shown great promises. However, it is critical yet challenging to verify the safety of such control systems with neural-network controllers in the loop. Previous methods for verifying neural network controlled systems are limited to a few specific activation functions. In this work, we propose a new reachability analysis approach based on Bernstein polynomials that can verify neural-network controlled systems with a more general form of activation functions, i.e., as long as they ensure that the neural networks are Lipschitz continuous. Specifically, we consider abstracting feedforward neural networks with Bernstein polynomials for a small subset of inputs. To quantify the error introduced by abstraction, we provide both theoretical error bound estimation based on the theory of Bernstein polynomials and more practical sampling based error bound estimation, following a tight Lipschitz constant estimation approach based on forward reachability analysis. Compared with previous methods, our approach addresses a much broader set of neural networks, including heterogeneous neural networks that contain multiple types of activation functions. Experiment results on a variety of benchmarks show the effectiveness of our approach. © 2019 Association for Computing Machinery.",Bernstein polynomials; Neural network controlled systems; Reachability; Verification,Abstracting; Activation analysis; Chemical activation; Dynamical systems; Errors; Polynomials; Verification; Activation functions; Bernstein polynomial; Estimation approaches; Lipschitz continuous; Network-controlled; Neural network controllers; Reachability; Reachability analysis; Feedforward neural networks
Code-inherent traffic shaping for hard real-time systems,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073154588&doi=10.1145%2f3358215&partnerID=40&md5=f629555deb0fe2335995f8b9148a98de,"Modern hard real-time systems evolved from isolated single-core architectures to complex multi-core architectures which are often connected in a distributed manner. With the increasing influence of interconnections in hard real-time systems, the access behavior to shared resources of single tasks or cores becomes a crucial factor for the system's overall worst-case timing properties. Traffic shaping is a powerful technique to decrease contention in a network and deliver guarantees on network streams. In this paper we present a novel approach to automatically integrate a traffic shaping behavior into the code of a program for different traffic shaping profiles while being as least invasive as possible. As this approach is solely depending on modifying programs on a code-level, it does not rely on any additional hardware or operating system-based functions. We show how different traffic shaping profiles can be implemented into programs using a greedy heuristic and an evolutionary algorithm, as well as their influences on the modified programs. It is demonstrated that the presented approaches can be used to decrease worst-case execution times in multi-core systems and lower buffer requirements in distributed systems. © 2019 Association for Computing Machinery.",Event arrival functions; Multi-core; Real-time; Traffic shaping,Codes (symbols); Computer architecture; Heuristic programming; Interactive computer systems; Network architecture; Buffer requirements; Distributed systems; Hard real-time systems; Multi core; Multicore architectures; Real time; Traffic-shaping; Worst-case execution time; Real time systems
Worst-case satisfaction of STL specifications using feedforward neural network controllers: A lagrange multipliers approach,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073161036&doi=10.1145%2f3358239&partnerID=40&md5=f3b8a906e442a6e80afdcb37390af31d,"In this paper, a reinforcement learning approach for designing feedback neural network controllers for nonlinear systems is proposed. Given a Signal Temporal Logic (STL) specification which needs to be satisfied by the system over a set of initial conditions, the neural network parameters are tuned in order to maximize the satisfaction of the STL formula. The framework is based on a max-min formulation of the robustness of the STL formula. The maximization is solved through a Lagrange multipliers method, while the minimization corresponds to a falsification problem. We present our results on a vehicle and a quadrotor model and demonstrate that our approach reduces the training time more than 50 percent compared to the baseline approach. © 2019 Association for Computing Machinery.",Neural network controller; Reinforcement learning; Signal temporal logic,Computer circuits; Controllers; Lagrange multipliers; Machine learning; Reinforcement learning; Specifications; Temporal logic; Initial conditions; Lagrange multipliers approaches; Lagrange multipliers method; Neural network controllers; Neural network parameters; Quad rotors; Reinforcement learning approach; Training time; Feedforward neural networks
Specification mining and robust design under uncertainty: A stochastic temporal logic approach,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073149683&doi=10.1145%2f3358231&partnerID=40&md5=a78db9404f353052d5bdce8fb67a045f,"In this paper, we propose Stochastic Temporal Logic (StTL) as a formalism for expressing probabilistic specifications on time-varying behaviors of controlled stochastic dynamical systems. To make StTL a more effective specification formalism, we introduce the quantitative semantics for StTL to reason about the robust satisfaction of an StTL specification by a given system. Additionally, we propose using the robustness value as the objective function to be maximized by a stochastic optimization algorithm for the purpose of controller design. Finally, we formulate an algorithm for parameter inference for Parameteric-StTL specifications, which allows specifications to be mined from output traces of the underlying system. We demonstrate and validate our framework on two case studies inspired by the automotive domain. © 2019 Association for Computing Machinery.",Controller design; Robust satisfaction; Stochastic temporal logic,Dynamical systems; Inference engines; Optimization; Semantics; Specifications; Stochastic systems; Temporal logic; Controller designs; Objective functions; Probabilistic specifications; Robust satisfaction; Specification mining; Stochastic dynamical system; Stochastic optimization algorithm; Time varying behavior; Computer circuits
Editorial: Adversaries and robustness,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075560195&doi=10.1145%2f3345556&partnerID=40&md5=c5679b3c51194f73438c43bbe438e252,[No abstract available],,
NQA: A nested anti-collision algorithm for RFID systems,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069515877&doi=10.1145%2f3330139&partnerID=40&md5=9184caa6ee0d367f8f9cdfa020fd1039,"Radio frequency identification (RFID) systems, as one of the key components in the Internet of Things (IoT), have attracted much attention in the domains of industry and academia. In practice, the performance of RFID systems rather relies on the effectiveness and efficiency of anti-collision algorithms. A large body of studies have recently focused on the anti-collision algorithms, such as the Q-algorithm (QA), which has been successfully utilized in EPCglobal Class-1 Generation-2 protocol. However, the performance of those anticollision algorithms needs to be further improved. Observe that fully exploiting the pre-processing time can improve the efficiency of the QA algorithm. With an objective of improving the performance for anti-collision, we propose a Nested Q-algorithm (NQA), which makes full use of such pre-processing time and incorporates the advantages of both Binary Tree (BT) algorithm and QA algorithm. Specifically, based on the expected number of collision tags, the NQA algorithm can adaptively select either BT or QA to identify collision tags. Extensive simulation results validate the efficiency and effectiveness of our proposed NQA (i.e., less running time for processing the same number of active tags) when compared to the existing algorithms. © 2019 Association for Computing Machinery.",Anti-collision algorithm; Binary tree algorithm; Q-algorithm; RFID,Binary trees; Collision avoidance; Efficiency; Internet of things; Radio frequency identification (RFID); Radio systems; Anti collision; Anti-collision algorithms; Binary tree algorithm; Effectiveness and efficiencies; Extensive simulations; Internet of thing (IOT); Pre-processing; Q algorithms; Trees (mathematics)
A task failure rate aware dual-channel solar power system for nonvolatile sensor nodes,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069501352&doi=10.1145%2f3320270&partnerID=40&md5=c4d0f93f22c15f76c6a031772d8cf3fb,"In line with the rapid development of the Internet of Things (IoT), the maintenance of on-board batteries for a trillion sensor nodes has become prohibitive both in time and costs. Energy harvesting is a promising solution to this problem. However, conventional energy-harvesting systems with storage suffer from low efficiency because of conversion loss and storage leakage. Direct supply systems without energy buffer provide higher efficiency, but fail to satisfy quality of service (QoS) due to mismatches between input power and workloads. Recently, a novel dual-channel photovoltaic power system has paved the way to achieve both high energy efficiency and QoS guarantee. This article focuses on the design-time and run-time co-optimization of the dual-channel solar power system. At the design stage, we develop a task failure rate estimation framework to balance design costs and failure rate. At run-time, we propose a task failure rate aware QoS tuning algorithm to further enhance energy efficiency. Through the experiments on both a simulation platform and a prototype board, this study demonstrates a 27% task failure rate reduction compared with conventional architectures with identical design costs. And the proposed online QoS tuning algorithm brings up to 30% improvement in energy efficiency with nearly zero failure rate penalty. © 2019 Association for Computing Machinery.",Dual-channel power system; Energy efficiency; Energy harvesting; Nonvolatile processor,Energy harvesting; Failure analysis; Internet of things; Outages; Photovoltaic cells; Quality of service; Sensor nodes; Simulation platform; Solar energy; Dual channel; Energy harvesting systems; High energy efficiency; Internet of thing (IOT); Non-volatile; Nonvolatile sensor nodes; On-board batteries; Photovoltaic power systems; Energy efficiency
Optimization and implementation of wavelet-based algorithms for detecting high-voltage spindles in neuron signals,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069535048&doi=10.1145%2f3329864&partnerID=40&md5=3a29b76883abe5d7dfcd0b60e585d6d9,"This article presents a microcontroller unit (MCU) based simplified discrete wavelet transform (Sim-DWT) algorithm that can detect high-voltage spindles (HVSs) in local field potential (LFP) signals. The Sim-DWT algorithm operates in an 8-bit MCU, 8MHz operating clock and 16 sample points of buffers to detect HVSs with a frequency range of 5−15Hz. The requirement of only sixteen 8-bit sample points as the window length for calculation and no need for a multiplier render the Sim-DWT easy to implement in an MCU with limited hardware resources. The Sim-DWT is applied in an 8-bit MCU with 6mW power consumption (including IO ports) and was tested for detecting LFP signals in vivo. The design methods and the accuracy of three typical types of mother wavelet functions (Haar, DB4, Morlet) in the Sim-DWT were also tested and compared with those of a PC-based system. The experimental results showed that with appropriately designed cMW functions in the Sim-DWT, HVSs could be detected more accurately than they could be in PC-based software. The present study indicates that the optimized HVS detector (Sim-DWT) can be implemented in an 8-bit MCU with limited hardware resources and is suitable to serve as the digital core in a closed-loop deep brain stimulator microsystem in the future. © 2019 Association for Computing Machinery.",Discrete wavelet transform (DWT); High-voltage spindle (HVS); Parkinson disease (PD),Discrete wavelet transforms; Electrophysiology; Microcontrollers; Frequency ranges; Hardware resources; High voltage; Local field potentials; Microcontroller unit; Parkinson disease; PC-based software; Wavelet based algorithm; Signal reconstruction
Thermal-aware real-time scheduling using timed continuous Petri Nets,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069152591&doi=10.1145%2f3322643&partnerID=40&md5=f0b2838de6a76f7a244e80b7568a85d6,"We present a thermal-aware, hard real-time (HRT) global scheduler for a multiprocessor system designed upon three novel techinques. First, we present a modeling methodology based on Timed Continuous Petri nets (TCPN) that yields a complete state variable model, including job arrivals, CPU usage, power, and thermal behavior. The model is accurate and avoids the calibration stage of RC thermal models. Second, based on this model, a linear programming problem (LPP) determines the existence of a feasible HRT thermal-aware schedule. Last, a sliding-mode controller and an online discretization algorithm implement the global HRT scheduler, which is capable of managing thermal constraints, context switching, migrations, and disturbances. © 2019 Association for Computing Machinery.",Feedback control; Modeling; Multiprocessor; Real-time systems; Thermal-aware scheduling; Timed continuous petri nets,Continuous time systems; Feedback control; Interactive computer systems; Linear programming; Models; Multiprocessing systems; Petri nets; Scheduling; Sliding mode control; Linear programming problem; Multi processor systems; Multiprocessor; Online discretization; Real - time scheduling; Sliding mode controller; Thermal-aware scheduling; Timed continuous petri nets; Real time systems
Cooperative cache transfer-based on-demand network coded broadcast in vehicular networks,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069538245&doi=10.1145%2f3329865&partnerID=40&md5=805e53ae2717415842cff8b63e1fc790,"Real-time traffic updates, safety and comfort driving, infotainment, and so on, are some envisioned applications in vehicular networks. Unlike traditional broadcast, network-coding-assisted broadcast can satisfy multiple vehicles with different data items in a coded form. However, server side encoding requires the prior knowledge about vehicles’ cache information for the successful decoding at the vehicles’ sides. The explicit cache upload from vehicles to Road Side Unit (RSU) wastes upload bandwidth. In multi-RSU vehicular networks, we propose a Cooperative Cache Transfer-based On-demand Network Coded Broadcast called CCTCB. In the proposed CCTCB approach, vehicles do not need to upload their cache information to the server, rather the RSU server learns the vehicles’ cache intrinsically. We derive a probabilistic model to analyze the coding opportunity in the proposed cooperative cache transfer mechanism incorporating vehicle mobility. The comprehensive simulation results validate the superiority of the proposed approach. © 2019 Association for Computing Machinery.",Algorithm design; Cooperative cache transfer; Network coding; On-demand broadcast; Vehicular communication,Broadcasting; Codes (symbols); Cooperative communication; Solid wastes; Vehicles; Algorithm design; Cooperative cache; On-demand broadcast; Probabilistic modeling; Real time traffics; Upload bandwidths; Vehicular communications; Vehicular networks; Network coding
"Introduction to the special issue on cryptographic engineering for internet of things: Security foundations, lightweight solutions, and attacks",2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068833448&doi=10.1145%2f3322641&partnerID=40&md5=ebc0d506d8857407105058cb83bb11b9,[No abstract available],,
A survey of asynchronous programming using coroutines in the internet of things and embedded systems,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068884160&doi=10.1145%2f3319618&partnerID=40&md5=0517edeb2f18ba7120020f5d656cffbf,"Many Internet of Things and embedded projects are event driven, and therefore require asynchronous and concurrent programming. Current proposals for C++20 suggest that coroutines will have native language support. It is timely to survey the current use of coroutines in embedded systems development. This article investigates existing research which uses or describes coroutines on resource-constrained platforms. The existing research is analysed with regard to: software platform, hardware platform, and capacity; use cases and intended benefits; and the application programming interface design used for coroutines. A systematic mapping study was performed, to select studies published between 2007 and 2018 which contained original research into the application of coroutines on resource-constrained platforms. An initial set of 566 candidate papers, collated from on-line databases, were reduced to only 35 after filters were applied, revealing the following taxonomy. The C & C++ programming languages were used by 22 studies out of 35. As regards hardware, 16 studies used 8- or 16-bit processors while 13 used 32-bit processors. The four most common use cases were concurrency (17 papers), network communication (15), sensor readings (9), and data flow (7). The leading intended benefits were code style and simplicity (12 papers), scheduling (9), and efficiency (8). A wide variety of techniques have been used to implement coroutines, including native macros, additional tool chain steps, new language features, and non-portable assembly language. We conclude that there is widespread demand for coroutines on resource-constrained devices. Our findings suggest that there is significant demand for a formalised, stable, well-supported implementation of coroutines in C++, designed with consideration of the special needs of resource-constrained devices, and further that such an implementation would bring benefits specific to such devices. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",And Phrases: Embedded; Asynchronous; Direct style; Resource-constrained; Scheduling,Application programming interfaces (API); Application programs; Embedded systems; Internet of things; Scheduling; Surveys; And Phrases: Embedded; Asynchronous; Asynchronous programming; Direct styles; Network communications; Resource-constrained; Resourceconstrained devices; Systematic mapping studies; C++ (programming language)
Catching escapers: A detection method for advanced persistent escapers in industry internet of things based on identity-based broadcast encryption (IBBE),2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068838783&doi=10.1145%2f3319615&partnerID=40&md5=e5a8f7fac5cd4e6edc99509f90ef25b8,"As the Industry 4.0 or Internet of Things (IoT) era begins, security plays a key role in the Industry Internet of Things (IIoT) due to various threats, which include escape or Distributed Denial of Service (DDoS) attackers in the virtualization layer and vulnerability exploiters in the device layer. A successful cross-VM escape attack in the virtualization layer combined with cross-layer penetration in the device layer, which we define as an Advanced Persistent Escaper (APE), poses a great threat. Therefore, the development of detection and rejection methods for APEs across multiple layers in IIoT is an open issue. To the best of our knowledge, less effective methods are established, especially for vulnerability exploitation in the virtualization layer and backdoor leverage in the device layer. On the basis of this, we propose Escaper Cops (EscaperCOP), a detection method for cross-VM escapers in the virtualization layer and cross-layer penetrators in the device layer. In particular, a new detection method for guest-to-host escapers is proposed for the virtualization layer. Finally, a novel encryption method based on Identity-based Broadcast Encryption (IBBE) is proposed to protect the critical components in EscaperCOP, detection library, and control command library. To verify our method, experimental tests are performed for a large number of APEs in an IIoT framework. The test results have demonstrated the proposed method is effective with an acceptable level of detection ratio. © 2019 Association for Computing Machinery.",And Phrases: Advanced persistent escaper (APE); Identity-based broadcast encryption (IBBE); Industry internet of things (IIoT),Broadcasting; Denial-of-service attack; Internet of things; Knowledge management; Network security; Virtual machine; Virtual reality; Virtualization; And Phrases: Advanced persistent escaper (APE); Critical component; Detection methods; Distributed denial of service; Encryption methods; Identity-based broadcast encryptions; Internet of Things (IOT); Virtualization layers; Cryptography
Editorial: Reflections on the History of Cyber-Physical versus Embedded Systems,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068857307&doi=10.1145%2f3325115&partnerID=40&md5=3265ebc1989152bb3ae884d33a238a6f,[No abstract available],,
Cache Reconfiguration Using Machine Learning for Vulnerability-aware Energy Optimization,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065711756&doi=10.1145%2f3309762&partnerID=40&md5=71e6698136f27dcd7592672016425dbe,"Dynamic cache reconfiguration has been widely explored for energy optimization and performance improvement for single-core systems. Cache partitioning techniques are introduced for the shared cache in multicore systems to alleviate inter-core interference. While these techniques focus only on performance and energy, they ignore vulnerability due to soft errors. In this article, we present a static profiling based algorithm to enable vulnerability-aware energy-optimization for real-time multicore systems. Our approach can efficiently search the space of cache configurations and partitioning schemes for energy optimization while task deadlines and vulnerability constraints are satisfied. A machine learning technique has been employed to minimize the static profiling time without sacrificing the accuracy of results. Our experimental results demonstrate that our approach can achieve 19.2% average energy savings compared with the base configuration, while drastically reducing the vulnerability (49.3% on average) compared to state-of-the-art techniques. Furthermore, the machine learning technique enabled more than 10x speedup in static profiling time with a negligible prediction error of 3%. © 2019 Association for Computing Machinery.",Chine learning; Energy optimization; Multicore,Energy conservation; Learning algorithms; Machine learning; Radiation hardening; Cache configurations; Cache reconfigurations; Chine learning; Energy optimization; Machine learning techniques; Multi core; Multi-core systems; State-of-the-art techniques; Real time systems
CHIMP: A Learning-based Power-aware Communication Protocol for Wireless Body Area Networks,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065732462&doi=10.1145%2f3309763&partnerID=40&md5=60cc9ac796bf3efe513bd8a44564f85a,"Radio links in wireless body area networks (WBANs) commonly experience highly time-varying attenuation due to the dynamic network topology and frequent occlusions caused by body movements, making it challenging to design a reliable, energy-efficient, and real-time communication protocol for WBANs. In this article, we present Chimp, a learning-based power-aware communication protocol in which each sending node can self-learn the channel quality and choose the best transmission power level to reduce energy consumption and interference range while still guaranteeing high communication reliability. Chimp is designed based on learning automata that uses only the acknowledgment packets and motion data from a local gyroscope sensor to infer the real-time channel status. We design a new cost function that takes into account the energy consumption, communication reliability and interference and develop a new learning function that can guarantee to select the optimal transmission power level to minimize the cost function for any given channel quality. For highly dynamic postures such as walking and running, we exploit the correlation between channel quality and motion data generated by a gyroscope sensor to fastly estimate channel quality, eliminating the need to use expensive channel sampling procedures. We evaluate the performance of Chimp through experiments using TelosB motes equipped with the MPU-9250 motion sensor chip and compare it with the state-of-the-art protocols in different body postures. Experimental results demonstrate that Chimp outperforms existing schemes and works efficiently in most common body postures. In high-date-rate scenarios, it achieves almost the same performance as the optimal power assignment scheme in which the optimal power level for each transmission is calculated based on the collected channel measurements in an off-line manner. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Energy; Interference; Leaning automata; Optimal adaptive power control; Reliability; Wireless body sensor networks,Adaptive control systems; Automata theory; Body sensor networks; Cost functions; Energy efficiency; Energy utilization; Gyroscopes; Internet protocols; Power control; Radio links; Reliability; Wave interference; Wireless local area networks (WLAN); Adaptive power control; Communication reliabilities; Energy; Leaning automata; Optimal power assignments; State-of-the art protocols; Wireless body area network; Wireless body sensor networks; Power management (telecommunication)
Single- and multi-FPGA acceleration of dense stereo vision for planetary rovers,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063238688&doi=10.1145%2f3312743&partnerID=40&md5=92d47a95abe43495269c259d5d941620,"Increased mobile autonomy is a vital requisite for future planetary exploration rovers. Stereo vision is a key enabling technology in this regard, as it can passively reconstruct in three dimensions the surroundings of a rover and facilitate the selection of science targets and the planning of safe routes. Nonetheless, accurate dense stereo algorithms are computationally demanding.When executed on the low-performance, radiationhardened CPUs typically installed on rovers, slow stereo processing severely limits the driving speed and hence the science that can be conducted in situ. Aiming to decrease execution time while increasing the accuracy of stereo vision embedded in future rovers, this article proposes HW/SW co-design and acceleration on resource-constrained, space-grade FPGAs. In a top-down approach, we develop a stereo algorithm based on the space sweep paradigm, design its parallel HWarchitecture, implement it with VHDL, and demonstrate feasible solutions even on small-sized devices with our multi-FPGA partitioning methodology. To meet all cost, accuracy, and speed requirements set by the European Space Agency for this system, we customize our HW/SW co-processor by design space exploration and testing on a Mars-like dataset. Implemented on Xilinx Virtex technology, or European NG-MEDIUM devices, the FPGA kernel processes a 1,120 × 1,120 stereo pair in 1.7s-3.1s, utilizing only 5.4-9.3 LUT6 and 200-312 RAMB18. The proposed system exhibits up to 32× speedup over desktop CPUs, or 2,810× over space-grade LEON3, and achieves a mean reconstruction error less than 2cm up to 4m depth. Excluding errors exceeding 2cm (which are less than 4% of the total), the mean error is under 8mm. © 2019 Association for Computing Machinery.",Autonomous navigation; Multi-FPGA partitioning; Parallel architecture design; Planetary rovers; Rad-hard FPGA; Space sweep; Stereo vision,Errors; Field programmable gate arrays (FPGA); Hardware-software codesign; In situ processing; Integrated circuit design; Interplanetary spacecraft; Parallel architectures; Planetary landers; Program processors; Space flight; Statistical tests; Stereo image processing; Autonomous navigation; Multi-fpga partitioning; Parallel architecture design; Planetary rovers; Rad hard; Space sweep; Stereo vision
FPGA implementation of the ECC over GF(2m) for small embedded applications,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065138628&doi=10.1145%2f3310354&partnerID=40&md5=f220a4cc1e270afa8a4d6ec1f4dd5bc5,"In this article, we propose a compact elliptic curve cryptographic core over GF(2m). The proposed architecture is based on the Lopez-Dahab projective point arithmetic operations. To achieve efficiency in resources usage, an iterative method that uses a ROM-based state machine is developed for the elliptic curve cryptography (ECC) point doubling and addition operations. The compact ECC core has been implemented using Virtex FPGA devices. The number of the required slices is 2,102 at 321MHz and 6,738 slices at 262MHz for different GF(2m). Extensive experiments were conducted to compare our solution to existing methods in the literature. Our compact core consumes less area than all previously proposed methods. It also provides an excellent performance for scalar multiplication. In addition, the ECC core is implemented in ASIC 0.18μm CMOS technology, and the results show excellent performance. Therefore, our proposed ECC core method provides a balance in terms of speed, area, and power consumption. This makes the proposed design the right choice for cryptosystems in limited-resource devices such as cell phones, IP cores of SoCs, and smart cards. Moreover, side-channel attack resistance is implemented to prevent power analysis. © 2019 Association for Computing Machinery.",Application-specific integrated circuit (ASIC); Elliptic curve cryptography (ECC); Field multiplications; Field programmable gate arrays (FPGAS); Finite field operations; Internet of Things; Projective coordination; Security; Side-channel attack,Application specific integrated circuits; Field programmable gate arrays (FPGA); Geometry; Internet of things; Iterative methods; Public key cryptography; Smart cards; Elliptic Curve Cryptography(ECC); Field multiplications; Finite field operations; Projective coordination; Security; Side channel attack
Combining PUF with RLUTs: A Two-party pay-per-device IP licensing scheme on FPGAs,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063225894&doi=10.1145%2f3301307&partnerID=40&md5=00ee423e56e499a35e88c1d82b681220,"With the popularity of modern FPGAs, the business of FPGA specific intellectual properties (IP) is expanding rapidly. This also brings in the concern of IP protection. FPGA vendors are making serious efforts toward IP protection, leading to standardization schemes like IEEE P1735. However, efficient techniques to prevent unauthorized overuse of IP still remain an open question. In this article, we propose a two-party IP protection scheme combining the re-configurable look-up table primitive of modern FPGAs with physically unclonable functions (PUF). The proposed scheme works with the assumption that the FPGA vendor provides the assurance of confidentiality and integrity of the developed IP. The proposed scheme is considerably lightweight compared to existing schemes, prevents overuse, and does not involve FPGA vendors or trusted third parties for IP licensing. The validation of the proposed scheme is done on MCNC'91 benchmark and third-party IPs like AES and lightweight MIPS processors. © 2019 Association for Computing Machinery.",FPGA; IP security and licensing; PUF,Table lookup; FPGA vendors; IP protection; IP security; Look up table; Physically unclonable functions; Third parties; Trusted third parties; Field programmable gate arrays (FPGA)
Synergy: An HW/SW framework for high throughput CNNs on embedded heterogeneous SoC,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063193049&doi=10.1145%2f3301278&partnerID=40&md5=8cc2957542daa1937e359902faf7d20c,"Convolutional Neural Networks (CNN) have been widely deployed in diverse application domains. There has been significant progress in accelerating both their training and inference using high-performance GPUs, FPGAs, and custom ASICs for datacenter-scale environments. The recent proliferation of mobile and Internet of Things (IoT) devices have necessitated real-time, energy-efficient deep neural network inference on embedded-class, resource-constrained platforms. In this context, we present Synergy, an automated, hardware-software co-designed, pipelined, high-throughput CNN inference framework on embedded heterogeneous system-on-chip (SoC) architectures (Xilinx Zynq). Synergy leverages, through multi-threading, all the available on-chip resources, which includes the dual-core ARM processor along with the FPGA and the NEON Single-Instruction Multiple-Data (SIMD) engines as accelerators.Moreover, Synergy provides a unified abstraction of the heterogeneous accelerators (FPGA and NEON) and can adapt to different network configurations at runtimewithout changing the underlying hardware accelerator architecture by balancingworkload across accelerators through work-stealing. Synergy achieves 7.3X speedup, averaged across seven CNN models, over a well-optimized software-only solution. Synergy demonstrates substantially better throughput and energy-efficiency compared to the contemporary CNN implementations on the same SoC architecture. © 2019 Association for Computing Machinery.",Accelerator abstraction; CNNs; FPGAs; Hardware/software co-design; Heterogeneous computing; Multi-threading; Work stealing,Abstracting; Balancing; Computer architecture; Deep neural networks; Energy efficiency; Field programmable gate arrays (FPGA); Hardware-software codesign; Internet of things; Neon; Network architecture; Neural networks; Program processors; Programmable logic controllers; Throughput; CNNs; Convolutional neural network; Hardware accelerator architecture; Hardware software co-designed; Heterogeneous computing; Multi-threading; Single instruction multiple data; Work stealing; System-on-chip
Stigmergy-based security for SoC operations from runtime performance degradation of SoC components,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063188814&doi=10.1145%2f3301279&partnerID=40&md5=ea19c62177571fd3296784e6a4267ccb,"The semiconductor design industry of the embedded era has embraced the globalization strategy for system on chip (SoC) design. This involves incorporation of various SoC components or intellectual properties (IPs), procured from various third-party IP (3PIP) vendors. However, trust of an SoC is challenged when a supplied IP is counterfeit or implanted with a Hardware Trojan Horse. Both roots of untrust may result in sudden performance degradation at runtime. None of the existing hardware security approaches organize the behavior of the IPs at the low level, to ensure timely completion of SoC operations. However, real-time SoC operations are always associated with a deadline, and a deadline miss due to sudden performance degradation of any of the IPs may jeopardize mission-critical applications. We seek refuge to the stigmergic behavior exhibited in insect colonies to propose a decentralized self-aware security approach. The self-aware security modules attached with each IP works based on the Observe-Decide-Act paradigm and not only detects vulnerability but also organizes behavior of the IPs dynamically at runtime so that the high-level objective of task completion before a deadline is ensured. Experimental validation and low overhead of our proposed security modules over various benchmark IPs and crypto SoCs depict the prospects of our proposed mechanism. © 2019 Association for Computing Machinery.",Hardware Trojan Horses (HTH); Runtime SoC security; Stigmergy,Hardware security; Programmable logic controllers; System-on-chip; Experimental validations; Mission critical applications; Performance degradation; Run-time performance; Runtimes; Semiconductor design; Stigmergy; System on chip design; Integrated circuit design
Control flow checking or not? (for Soft Errors),2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062366650&doi=10.1145%2f3301311&partnerID=40&md5=8be1910a5c008090db15b0196c69cd6f,"Huge leaps in performance and power improvements of computing systems are driven by rapid technology scaling, but technology scaling has also rendered computing systems susceptible to soft errors. Among the soft error protection techniques, Control Flow Checking (CFC) based techniques have gained a reputation of being lightweight yet effective. The main idea behind CFCs is to check if the program is executing the instructions in the right order. In order to validate the protection claims of existing CFCs, we develop a systematic and quantitative method to evaluate the protection achieved by CFCs using the metric of vulnerability. Our quantitative analysis indicates that existing CFC techniques are not only ineffective in providing protection from soft faults, but incur additional performance and power overheads. Our results show that software-only CFC protection schemes increase system vulnerability by 18%-21% with 17%-38% performance overhead and hybrid CFC protection increases vulnerability by 5%. Although the vulnerability remains almost the same for hardware-only CFC protection, they incur overheads of design cost, area, and power due to the hardware modifications required for their implementations. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Error correction code; Reliability; Soft error; Transient fault; Vulnerability,Chlorofluorocarbons; Radiation hardening; Reliability; Control flow checking; Error correction codes; Hardware modifications; Soft error; Soft error protection; System vulnerability; Transient faults; Vulnerability; Error correction
Scratchpad-memory management for multi-threaded applications on many-core architectures,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061254489&doi=10.1145%2f3301308&partnerID=40&md5=1db8b937606ed91c881981ae1a558fa8,"Contemporary many-core architectures, such as Adapteva Epiphany and Sunway TaihuLight, employ percore software-controlled Scratchpad Memory (SPM) rather than caches for better performance-per-watt and predictability. In these architectures, a core is allowed to access its own SPM as well as remote SPMs through the Network-On-Chip (NoC). However, the compiler/programmer is required to explicitly manage the movement of data between SPMs and off-chip memory. Utilizing SPMs for multi-threaded applications is even more challenging, as the shared variables across the threads need to be placed appropriately. Accessing variables from remote SPMs with higher access latency further complicates this problem as certain links in the NoC may be heavily contended by multiple threads. Therefore, certain variables may need to be replicated in multiple SPMs to reduce the contention delay and/or the overall access time. We present Coordinated Data Management (CDM), a compile-time framework that automatically identifes shared/private variables and places them with replication (if necessary) to suitable on-chip or off-chip memory, taking NoC contention into consideration. We develop both an exact Integer Linear Programming (ILP) formulation as well as an iterative, scalable algorithm for placing the data variables in multi-threaded applications on many-core SPMs. Experimental evaluation on the Parallella hardware platform confrms that our allocation strategy reduces the overall execution time and energy consumption by 1.84× and 1.83×, respectively, when compared to the existing approaches. © 2019 Association for Computing Machinery.",Many-core architectures; Scratchpad memory management,Cache memory; Energy utilization; Information management; Integer programming; Iterative methods; Memory architecture; Network architecture; Allocation strategy; Experimental evaluation; Integer Linear Programming; Many-core architecture; Multi- threaded applications; Network-on-chip(NoC); Scalable algorithms; Scratch-pad memory managements; Network-on-chip
The bionode: A closed-loop neuromodulation implant,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062349663&doi=10.1145%2f3301310&partnerID=40&md5=1e86b79e48f9e25f29e40627eac2f578,"Implantable closed-loop neuromodulation devices for use in long-term chronic studies in a lab or clinical trial are expensive to acquire and difficult to modify for specific use cases. This article documents the design and fabrication of a wireless implantable device using only commercially available off-the-shelf (COTS) components. This device, called the Bionode, can record and transmit up to four channels of biopotential data while simultaneously providing biphasic constant-current stimulation. The Bionode is a viable, low-cost, reusable, and easily modifiable research tool with clinical implications that has gained widespread use in various research projects at Purdue University. © 2019 Association for Computing Machinery.",Biocompatibility; Implantable biomedical devices; Wireless communication; Wireless powering,Biocompatibility; Clinical research; Implants (surgical); Clinical trial; Constant current; Implantable biomedical devices; Neuromodulation; Purdue University; Wireless communications; Wireless implantable devices; Wireless powering; Commercial off-the-shelf
Editorial: Human factors in embedded computing,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062329366&doi=10.1145%2f3302888&partnerID=40&md5=5cdb00ed33ce09b8c3a5039529087f5f,[No abstract available],,
Self-adaptive QoS management of computation and communication resources in many-core SOCs,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067188413&doi=10.1145%2f3328755&partnerID=40&md5=3fae7e8e57c424f5d97d6092a4414c6d,"Providing quality of service (QoS) for many-core systems with dynamic application admission is challenging due to the high amount of resources to manage and the unpredictability of computation and communication events. Related works propose a self-adaptive QoS mechanism concerned either in communication or computation resources, lacking, however, a comprehensive QoS management of both. Assuming a many-core system with QoS monitoring, runtime circuit-switching establishment, task migration, and a soft real-time task scheduler, this work fills this gap by proposing a novel self-adaptive QoS management. The contribution of this proposal comes with the following features in the QoS management: (i) comprehensiveness, by covering communication and computation resources; (ii) online, adopting the ODA (Observe, Decide, Act) runtime closed-loop adaptation; and (iii) reactive and proactive decisions, by using a dynamic application profile extraction technique, which enables the QoS management to be aware of the profile of running applications, allowing it to take proactive decisions based on a prediction analysis. The proposed QoS management adopts a decentralized organization by partitioning the system in clusters, each one managed by a dedicated processor, making the proposal scalable. Results show that the proactive feature accurately extracts the applications’ profile, and can prevent future QoS violations. The synergy of reactive and proactive decisions was able to sustain QoS, reducing the deadline miss rate by 99.5% with a severe disturbance in communication and computation levels, and avoiding deadline misses up to 70% of system utilization. © 2019 Association for Computing Machinery.",Many-core; Prediction; Quality of service; Self-adaptation,Forecasting; Software engineering; Communication resources; Computation resources; Dedicated processors; Dynamic applications; Many core; Profile extraction; Running applications; Self adaptation; Quality of service
A lightweight cryptographic protocol with certificateless signature for the internet of things,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065800010&doi=10.1145%2f3301306&partnerID=40&md5=ce75cacce8216fc8c89f513380fc8010,"The universality of smart-devices has brought rapid development and the significant advancement of ubiquitous applications for the Internet of Things (IoT). Designing new types of IoT-compatible cryptographic protocols has become a more popular way to secure IoT-based applications. Significant attention has been dedicated to the challenge of implementing a lightweight and secure cryptographic protocol for IoT devices. In this study, we propose a lightweight cryptographic protocol integrating certificateless signature and bilinear pairing crypto-primitives. In the proposed protocol, we elegantly refine the processes to account for computation-limited IoT devices during security operations. Rigorous security analyses are conducted to guarantee the robustness of the proposed cryptographic protocol. In addition, we demonstrate a thorough performance evaluation, where an IoT-based test-bed, i.e., the Raspberry PI, is simulated as the underlying platform of the implementation of our proposed cryptographic protocol. The results show the practicability of the proposed protocol. © 2019 Association for Computing Machinery.",Bilinear pairing; Certificateless signature; Cryptographic protocol; Internet of things (IoT); Security,Internet protocols; Public key cryptography; Bilinear pairing; Certificateless signature; Cryptographic protocols; Internet of Things (IOT); Security; Internet of things
Enabling on-the-fly hardware tracing of data reads in multicores,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067210044&doi=10.1145%2f3322642&partnerID=40&md5=ac0ad359900456503e13eabbd982d899,"Software debugging is one of the most challenging aspects of embedded system development due to growing hardware and software complexity, limited visibility of system components, and tightening time-to-market. To find software bugs faster, developers often rely on on-chip trace modules with large buffers to capture program execution traces with minimum interference with program execution. However, the high volumes of trace data and the high cost of trace modules limit the visibility into the system operation to short program segments. This article introduces a new hardware/software technique for capturing and filtering read data value traces in multicores that enables a complete reconstruction of parallel program execution. The proposed technique exploits tracking of data reads in data caches and cache coherence protocol states to minimize the number of trace messages streamed out of the target platform to the software debugger. The effectiveness of the proposed technique is determined by analyzing the required trace port bandwidth and trace buffer sizes as a function of the data cache size and the number of processor cores. The results show that the proposed technique significantly reduces the required trace port bandwidth, from 12.2 to 73.9 times, when compared to the Nexus-like read data value tracing, thus enabling continuous on-the-fly data tracing at modest hardware cost. © 2019 Association for Computing Machinery.",Multicores; Program tracing; Real-time embedded systems; Software testing and debugging,Bandwidth; Cache memory; Computer debugging; Embedded systems; Multicore programming; Real time systems; Software testing; Visibility; Cache coherence protocols; Embedded system development; Hardware and software; Multi-cores; Parallel program execution; Program tracing; Real-time embedded systems; Software Testing and Debugging; Program debugging
Partitioning and selection of data consistency mechanisms for multicore real-time systems,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067185416&doi=10.1145%2f3320271&partnerID=40&md5=06e44ab6d7fa11a0a49b3f82804f0744,"Multicore platforms are becoming increasingly popular in real-time systems. One of the major challenges in designing multicore real-time systems is ensuring consistent and timely access to shared resources. Lock-based protection mechanisms such as MPCP and MSRP have been proposed to guarantee mutually exclusive access in multicore systems at the expense of blocking. In this article, we consider partitioning and scheduling in multicore real-time systems with resource sharing. We first propose a resource-aware task partitioning algorithm for systems with lock-based protection. Wait-free methods, which ensure consistent access to shared memory resources with negligible blocking at the expense of additional memory space, are a suitable alternative when the shared resource is a communication buffer. We propose several approaches to solve the joint problem of task partitioning and the selection of a data consistency mechanism (lock-based or wait-free). The problem is first formulated as an Integer Linear Programming (ILP). For large systems where an ILP solution is not scalable, we propose two heuristic algorithms. Experimental results compare the effectiveness of the proposed approaches in finding schedulable systems with low memory cost and show how the use of wait-free methods can significantly improve schedulability. © 2019 Association for Computing Machinery.",Multicore; Partitioning; Shared resources; Wait-free,Distributed computer systems; Heuristic algorithms; Integer programming; Interactive computer systems; Locks (fasteners); Time sharing systems; Communication buffer; Integer Linear Programming; Multi core; Multi-core platforms; Partitioning; Protection mechanisms; Shared resources; Wait free; Real time systems
Lightweight implementations of NIST P-256 and SM2 ECC on 8-bit resource-constraint embedded device,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065790499&doi=10.1145%2f3236010&partnerID=40&md5=4ccd62fd965d43ca314ef7645c2542e7,"Elliptic Curve Cryptography (ECC) now is one of the most important approach to instantiate asymmetric encryption and signature schemes, which has been extensively exploited to protect the security of cyber-physical systems. With the advent of the Internet of Things (IoT), a great deal of constrained devices may require software implementations of ECC operations. Under this circumstances, the SM2, a set of public key cryptographic algorithms based on elliptic curves published by Chinese Commercial Cryptography Administration Office, was standardized at ISO in 2017 to enhance the cyber-security. However, few research works on the implementation of SM2 for constrained devices have been conducted. In this work, we fill this gap and propose our efficient, secure, and compact implementation of scalar multiplication on a 256-bit elliptic curve recommended by the SM2, as well as a comparison implementation of scalar multiplication on the same bit-length elliptic curve recommended by NIST. We re-design some existent techniques to fit the low-end IoT platform, namely 8-bit AVR processors, and our implementations evaluated on the desired platform show that the SM2 algorithms have competitive efficiency and security with NIST, which would work well to secure the IoT world. © 2019 Association for Computing Machinery.",8-bit AVR microcontroller; Elliptic curve cryptography; NIST; SM2; Software implementation,Chromium compounds; Embedded systems; Geometry; Public administration; Public key cryptography; AVR microcontrollers; Commercial cryptography; Cryptographic algorithms; Elliptic curve cryptography; Elliptic Curve Cryptography(ECC); Internet of thing (IOT); NIST; Software implementation; Internet of things
Contention-detectable mechanism for receiver-initiated MAC,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067212927&doi=10.1145%2f3317683&partnerID=40&md5=d0edbac3ccc9a87ae252fbee99babdd1,"The energy efficiency and delivery robustness are two critical issues for low duty-cycled wireless sensor networks. The asynchronous receiver-initiated duty-cycling media access control (MAC) protocols have shown their effectiveness through various studies. In receiver-initiated MACs, packet transmission is triggered by the probe of receiver. However, it suffers from the performance degradation incurred by packet collision, especially under bursty traffic. Several protocols have been proposed to address this problem, but their performance is restricted by the unnecessary backoff time and long negotiation process. In this article, we present CD-MAC, an energy-efficient and robust contention-detectable mechanism for addressing the collision-catching problem in receiver-initiated MACs. By exploring the temporal diversity of the acknowledgments, a receiver recognizes the potential senders and subsequently polls individual senders one by one. On that basis, CD-MAC can successfully avoid packet collision even though multiple senders have data packets to transmit to the same receiver. We implement CD-MAC in TinyOS and evaluate its performance on an indoor testbed with single-hop and multi-hop network scenarios. The results show that CD-MAC can significantly improve throughput by 1.72 times compared with the state-of-the-art receiver-initiated MAC protocol under bursty traffic loads. The results also demonstrate that CD-MAC can effectively mitigate the influence of hidden terminal problem and adapt to network dynamics well. © 2019 Association for Computing Machinery.",Contention avoidance; Receiver-initiated MAC; Temporal diversity; Wireless sensor networks,Energy efficiency; Medium access control; Contention avoidance; Hidden terminal problems; Media access control protocol; Negotiation process; Packet transmissions; Performance degradation; Receiver-initiated; Temporal diversity; Wireless sensor networks
Ensuring secure application execution and platform-specific execution in embedded devices,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065782904&doi=10.1145%2f3284361&partnerID=40&md5=e7337589035a56275397fb44358459de,"The Internet of Things (IoT) is expanding at a large rate, with devices found in commercial and domestic settings from industrial sensors to home appliances. However, as the IoT market grows, so does the number of attacks made against it with some reports claiming an increase of 600% in 2017. This work seeks to prevent code replacement, injection, and exploitation attacks by ensuring correct and platform specific application execution. This combines two previously studied problems: secure application execution and binding hardware and software. We present descriptions of both problems and requirements for ensuring both simultaneously. We then propose a scheme extending previous work that meets these requirements, and describe our implementation of the soft-core Secure Execution Processor developed and tested on Xilinx Spartan-6 FPGA. Finally, we analyse the scheme and our implementation according to performance and the requirements listed. © 2019 Association for Computing Machinery.",Hardware-software binding; Internet of Things; Platform specific execution; Secure application execution,Application programs; Domestic appliances; Hardware security; Application execution; Embedded device; Hardware and software; Industrial sensor; Internet of thing (IOT); Platform specific execution; Secure execution; Soft-cores; Internet of things
Optimal power management with guaranteed minimum energy utilization for solar energy harvesting systems,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067206869&doi=10.1145%2f3317679&partnerID=40&md5=6f06ab1d5f4629e1a1ee8292299a3005,"In this work, we present a formal study on optimizing the energy consumption of energy harvesting embedded systems. To deal with the uncertainty inherent in solar energy harvesting systems, we propose the Stochastic Power Management (SPM) scheme, which builds statistical models of harvested energy based on historical data. The proposed stochastic scheme maximizes the lowest energy consumption across all time intervals while giving strict probabilistic guarantees on not encountering battery depletion. For situations where historical data is not available, we propose the use of (i) a Finite Horizon Control (FHC) scheme and (ii) a non-uniformly scaled energy estimator based on an astronomical model, which is used by FHC. Under certain realistic assumptions, the FHC scheme can provide guarantees on minimum energy usage that can be supported over all times. We further propose and evaluate a piece-wise linear approximation of FHC for efficient implementation in resource-constrained embedded systems. With extensive experimental evaluation for eight publicly available datasets and two datasets collected with our own deployments, we quantitatively establish that the proposed solutions are highly effective at providing a guaranteed minimum service level and significantly outperform existing solutions. © 2019 Copyright held by the owner/author(s).",Optimization; Power management,Embedded systems; Energy harvesting; Energy utilization; Information management; Optimization; Power management; Solar energy; Stochastic models; Stochastic systems; Efficient implementation; Experimental evaluation; Finite horizons; Historical data; Minimum energy; Piecewise linear approximations; Probabilistic guarantees; Resource-constrained embedded systems; Energy management systems
Design-level and code-level security analysis of IoT devices,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065778968&doi=10.1145%2f3310353&partnerID=40&md5=f4b15755f9ccbea80470b629e4615ecd,"The Internet of Things (IoT) is playing an important role in different aspects of our lives. Smart grids, smart cars, and medical devices all incorporate IoT devices as key components. The ubiquity and criticality of these devices make them an attractive target for attackers. Therefore, we need techniques to analyze their security so that we can address their potential vulnerabilities. IoT devices, unlike remote servers, are user-facing and, therefore, an attacker may interact with them more extensively, e.g., via physical access. Existing techniques for analyzing security of IoT devices either rely on a pre-defined set of attacks and, therefore, have limited effect or do not consider the specific capabilities the attackers have against IoT devices. Security analysis techniques may operate at the design-level, leveraging abstraction to avoid state-space explosion, or at the code-level for ensuring accuracy. In this article, we introduce two techniques, one at the design-level, and the other at the code-level, to analyze security of IoT devices, and compare their effectiveness. The former technique uses model checking, while the latter uses symbolic execution, to find attacks based on the attacker's capabilities. We evaluate our techniques on an open source smart meter. We find that our code-level analysis technique is able to find three times more attacks and complete the analysis in half the time, compared to the design-level analysis technique, with no false positives. © 2019 Association for Computing Machinery.",IoT; Model checking; Security analysis,Codes (symbols); Design; Model checking; Security systems; Analysis techniques; Code-level analysis; Internet of thing (IOT); Medical Devices; Remote servers; Security analysis; State-space explosion; Symbolic execution; Internet of things
Compositional dataflow circuits,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061216607&doi=10.1145%2f3274280&partnerID=40&md5=7ba8f5456644bebe94417fdc662a81b7,"We present a technique for implementing dataflow networks as compositional hardware circuits. We first define an abstract dataflow model with unbounded buffers that supports data-dependent blocks (mux, demux, and nondeterministicmerge);we then showhowto faithfully implement such networkswith bounded buffers and handshaking. Handshaking admits compositionality: Our circuits can be connected with or without buffers, and combinational cycles arise only from a completely unbuffered cycle. While bounding buffer sizes can cause the system to deadlock prematurely, the system is guaranteed to produce the same, correct, data before then. Thus, unless the system deadlocks, inserting or removing buffers only affects its performance. We demonstrate how this enables design space to be explored. © 2019 Copyright held by the owner/author(s).",dataflow; high-level synthesis; Kahn networks,High level synthesis; Timing circuits; Buffer sizes; Combinational cycles; Compositionality; Data dependent; Dataflow; Dataflow model; Design spaces; Hardware circuits; Data flow analysis
EACAN: Reliable and resource-efficient CAN communications,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061232155&doi=10.1145%2f3301309&partnerID=40&md5=86772cc0367c66251ef53289f57bcf9b,"Worst-case-based timing verifcation for the controller area network (CAN) has been the bottleneck to effcient use of its bandwidth. Especially, this inefciency comes from the worst-case transmission error rate (WCTER) when transmission errors are accounted for. To alleviate this inefciency, we propose a runtime adaptation scheme, error-adaptive CAN (EACAN). EACAN observes the behavior of transmission errors at runtime, and reconfgures the message period based on the observation to meet the timing-failure requirement. We experimentally evaluate the bandwidth utilization of both EACAN- and WCTER-based verifcation, showing that the former improves the bandwidth utilization by 14% over the latter. © 2019 Association for Computing Machinery.",Controller area network; In-vehicle network; Mixed-criticality,Control system synthesis; Controllers; Error analysis; Process control; Band-width utilization; CAN communications; Controller area network; In-vehicle networks; Mixed criticalities; Resource-efficient; Runtime adaptation; Transmission error; Bandwidth
XOR-based low-cost reconfigurable PUFs for IoT security,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062228152&doi=10.1145%2f3274666&partnerID=40&md5=814ccab7c8462f57c20eacaee41652d5,"With the rapid development of the Internet of Things (IoT), security has attracted considerable interest. Conventional security solutions that have been proposed for the Internet based on classical cryptography cannot be applied to IoT nodes as they are typically resource-constrained. A physical unclonable function (PUF) is a hardware-based security primitive and can be used to generate a key online or uniquely identify an integrated circuit (IC) by extracting its internal random differences using so-called challenge-response pairs (CRPs). It is regarded as a promising low-cost solution for IoT security. A logic reconfigurable PUF (RPUF) is highly efficient in terms of hardware cost. This article first presents a new classification for RPUFs, namely circuit-based RPUF (C-RPUF) and algorithm-based RPUF (A-RPUF); two Exclusive OR (XOR)-based RPUF circuits (an XOR-based reconfigurable bistable ring PUF (XRBR PUF) and an XOR-based reconfigurable ring oscillator PUF (XRRO PUF)) are proposed. Both the XRBR and XRRO PUFs are implemented on Xilinx Spartan-6 field-programmable gate arrays (FPGAs). The implementation results are compared with previous PUF designs and show good uniqueness and reliability. Compared to conventional PUF designs, the most significant advantage of the proposed designs is that they are highly efficient in terms of hardware cost. Moreover, the XRRO PUF is the most efficient design when compared with previous RPUFs. Also, both the proposed XRRO and XRBR PUFs require only 12.5% of the hardware resources of previous bitstable ring PUFs and reconfigurable RO PUFs, respectively, to generate a 1-bit response. This confirms that the proposed XRBR and XRRO PUFs are very efficient designs with good uniqueness and reliability. © 2019 Association for Computing Machinery.",Internet of Things (IoT); Low cost; Reconfigurable PUF; XOR,Computer hardware; Costs; Cryptography; Field programmable gate arrays (FPGA); Hardware security; Integrated circuits; Oscillators (electronic); Reconfigurable hardware; Challenge-response pair; Hardware resources; Internet of thing (IOT); Internet of Things (IOT); Low costs; Physical unclonable functions (PUF); Reconfigurable; Security solutions; Internet of things
Stochastic assume-guarantee contracts for cyber-physical system design,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061213912&doi=10.1145%2f3243216&partnerID=40&md5=eb2a01f705097dfb84c65bc97cdb3c8b,"We present an assume-guarantee contract framework for cyber-physical system design under probabilistic requirements. Given a stochastic linear system and a set of requirements captured by bounded Stochastic Signal Temporal Logic (StSTL) contracts, we propose algorithms to check contract compatibility, consistency, and refinement, and generate a sequence of control inputs that satisfies a contract. We leverage encodings of the verification and control synthesis tasks into mixed integer optimization problems, and conservative approximations of probabilistic constraints that produce sound and tractable problem formulations.We illustrate the effectiveness of our approach on three case studies, including the design of controllers for aircraft power distribution networks. © 2019 Copyright held by the owner/author(s).",assume-guarantee reasoning; contracts; cyber-physical systems; embedded systems; modeling; requirement engineering; specification; Stochastic systems; synthesis; verification,Contracts; Cyber Physical System; Integer programming; Linear systems; Model checking; Models; Probability distributions; Specifications; Stochastic control systems; Stochastic systems; Synthesis (chemical); Systems analysis; Verification; Aircraft power; Assume-guarantee reasoning; Control synthesis; Mixed integer optimization; Probabilistic constraints; Requirement engineering; Stochastic linear systems; Stochastic signals; Embedded systems
A lightweight and secure data collection serverless protocol demonstrated in an active RFIDS scenario,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065767561&doi=10.1145%2f3274667&partnerID=40&md5=a7cba557ecdff9956ddf358eb8745c24,"In the growing Internet of Things context, thousands of computing devices with various functionalities are producing data (from environmental sensors or other sources). However, they are also collecting, storing, processing and transmitting data to eventually communicate them securely to third parties (e.g., owners of devices or cloud data storage). The deployed devices are often battery-powered mobile or static nodes equipped with sensors and/or actuators, and they communicate using wireless technologies. Examples include unmanned aerial vehicles, wireless sensor nodes, smart beacons, and wearable health objects. Such resource-constrained devices include Active Radio Frequency IDentification (RFID) nodes, and these are used to illustrate our proposal. In most scenarios, these nodes are unattended in an adverse environment, so data confidentiality must be ensured from the sensing phase through to delivery to authorized entities: in other words, data must be securely stored and transmitted to prevent attack by active adversaries even if the nodes are captured. However, due to the scarce resources available to nodes in terms of energy, storage, and/or computation, the proposed security solution has to be lightweight. In this article, we propose a serverless protocol to enable Mobile Data Collectors (MDCs), such as drones, to securely collect data from mobile and static Active RFID nodes and then deliver them later to an authorized third party. The whole solution ensures data confidentiality at each step (from the sensing phase, before data collection by the MDC, once data have been collected by MDC, and during final delivery), while fulfilling the lightweight requirements for the resource-limited entities involved. To assess the suitability of the protocol against the performance requirements, it was implemented on the most resource-constrained devices to get the worst possible results. In addition, to prove the protocol fulfills the security requirements, it was analyzed using security games and also formally verified using the AVISPA and ProVerif tools. © 2019 Association for Computing Machinery.",Active RFID nodes; Data collection protocol; Data confidentiality; Lightweight cryptography; Serverless protocol,Cryptography; Data handling; Digital storage; Radio frequency identification (RFID); Sensor nodes; Unmanned aerial vehicles (UAV); Wearable antennas; Wearable sensors; Active radio frequency identifications; Active RFID; Data collection protocols; Data confidentiality; Light-weight cryptography; Mobile data collectors; Performance requirements; Resourceconstrained devices; Data acquisition
Compact and flexible FPGA implementation of ED25519 and X25519,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065776319&doi=10.1145%2f3312742&partnerID=40&md5=1ed9a033b1329b8f36f2c61f9f6a7198,"This article describes a field-programmable gate array (FPGA) cryptographic architecture, which combines the elliptic curve-based Ed25519 digital signature algorithm and the X25519 key establishment scheme in a single module. Cryptographically, these are high-security elliptic curve cryptography algorithms with short key sizes and impressive execution times in software. Our goal is to provide a lightweight FPGA module that enables them on resource-constrained devices, specifically for Internet of Things (IoT) applications. In addition, we aim at extensibility with customisable countermeasures against timing and differential power analysis side-channel attacks and fault-injection attacks. For the former, we offer a choice between time-optimised versus constant-time execution, with or without Z-coordinate randomisation and base-point blinding; and for the latter, we offer enabling or disabling default-case statements in the Finite State Machine (FSM) descriptions. To obtain compactness and at the same time fast execution times, we make maximum use of the Digital Signal Processing (DSP) slices on the FPGA. We designed a single arithmetic unit that is flexible to support operations with two moduli and non-modulus arithmetic. In addition, our design benefits in-place memory management and the local storage of inputs into DSP slices' pipeline registers and takes advantage of distributed memory. These eliminate a memory access bottleneck. The flexibility is offered by a microcode supported instruction-set architecture. Our design targets 7-Series Xilinx FPGAs and is prototyped on a Zynq System-on-Chip (SoC). The base design combining Ed25519 and X25519 in a single module, and its implementation requires only around 11.1K Lookup Tables (LUTs), 2.6K registers, and 16 DSP slices. Also, it achieves performance of 1.6ms for a signature generation and 3.6ms for a signature verification for a 1024-bit message with an 82MHz clock. Moreover, the design can be optimised only for X25519, which gives the most compact FPGA implementation compared to previously published X25519 implementations. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Curve25519; ECC; ECDH; Ed25519; EdDSA; FPGA; X25519,Digital signal processing; Field programmable gate arrays (FPGA); Geometry; Internet of things; Memory architecture; Network security; Programmable logic controllers; Public key cryptography; Side channel attack; System-on-chip; Table lookup; Curve25519; ECDH; Ed25519; EdDSA; X25519; Integrated circuit design
BlueIO: A scalable real-time hardware I/O virtualization system for many-core embedded systems,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065773968&doi=10.1145%2f3309765&partnerID=40&md5=b2ce30a5964eb22809b09c35c5ee6714,"In safety-critical systems, time predictability is vital. This extends to I/O operations that require predictability, timing-accuracy, parallel access, scalability, and isolation. Currently, existing approaches cannot achieve all these requirements at the same time. In this article, we propose a framework of hardware framework for real-time I/O virtualization-termed BlueIO-to meet all these requirements simultaneously. BlueIO integrates the functionalities of I/O virtualization, low-layer I/O drivers, and a clock cycle level timing-accurate I/O controller (using the GPIOCP [36]). BlueIO provides this functionality in the hardware layer, supporting abstract virtualized access to I/O from the software domain. The hardware implementation includes I/O virtualization and I/O drivers, provides isolation and parallel (concurrent) access to I/O operations, and improves I/O performance. Furthermore, the approach includes the previously proposed GPIOCP to guarantee that I/O operations will occur at a specific clock cycle (i.e., be timing-accurate and predictable). In this article, we present a hardware consumption analysis of BlueIO to show that it linearly scales with the number of CPUs and I/O devices, which is evidenced by our implementation in VLSI and FPGA. We also describe the design and implementation of BlueIO and demonstrate how a BlueIO-based system can be exploited to meet real-time requirements with significant improvements in I/O performance and a low running cost on different OSs. © 2019 Association for Computing Machinery.",Predictability; Real-time system; Safety-critical system; Scalability; Timing-accuracy; Virtualization,Abstracting; Clocks; Embedded systems; Interactive computer systems; Program processors; Safety engineering; Scalability; Security systems; Virtual reality; Virtualization; Design and implementations; Hardware framework; Hardware implementations; Predictability; Real time requirement; Safety critical systems; Time predictabilities; Timing-accuracy; Real time systems
Can android run on time? Extending and measuring the android platform's timeliness,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061106052&doi=10.1145%2f3289257&partnerID=40&md5=d71d7b83f32088ff071338abb64146fb,"Time predictability is difficult to achieve in the complex, layered execution environments that are common in modern embedded devices such as smartphones. We explore adopting the Android programming model for a range of embedded applications that extends beyond mobile devices, under the constraint that changes to widely used libraries should be minimized. The challenges we explore include the interplay between real-time activities and the rest of the system, how to express the timeliness requirements of components, and how well those requirements can be met on stock embedded platforms. We detail the design and implementation of our modifications to the Android framework along with a real-time VM and OS, and we provide experimental data validating feasibility over five applications. © 2019 Association for Computing Machinery.",Android; Mobile devices; Real-time,Mobile computing; Software engineering; Android; Design and implementations; Embedded application; Embedded platforms; Execution environments; Programming models; Real time; Time predictabilities; Android (operating system)
"Model-based, mutation-driven test-case generation via heuristic-guided branching search",2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061178400&doi=10.1145%2f3289256&partnerID=40&md5=22e3d39b519939cd49a294df9be0524d,"This work introduces a heuristic-guided branching search algorithm for model-based, mutation-driven testcase generation. The algorithm is designed towards the efficient and computationally tractable exploration of discrete, non-deterministic models with huge state spaces. Asynchronous parallel processing is a key feature of the algorithm. The algorithm is inspired by the successful path planning algorithm Rapidly exploring Random Trees (RRT).We adapt RRT in several aspects towards test-case generation. Most notably, we introduce parametrized heuristics for start and successor state selection, as well as a mechanism to construct test cases from the data produced during the search. We implemented our algorithm in the existing test-case generation framework MoMuT. We present an extensive evaluation of the proposed heuristics and parameters of the algorithm, based on a diverse set of demanding models obtained in an industrial context. In total, we continuously utilized 128 CPU cores on three servers for several weeks to gather the experimental data presented. We show that branching search works well and the use of multiple heuristics is justified. With our new algorithm, we are now able to process models consisting of over 2,300 concurrent objects. To our knowledge, there is no other mutation-driven test-case generation tool that is able to process models of this magnitude. © 2019 Copyright held by the owner/author(s).",heuristics; model-based testing; mutation testing; parallel search; search-based testing; Test case generation,Model checking; Motion planning; heuristics; Model based testing; Mutation testing; Parallel search; Search-based testing; Test case generation; Trees (mathematics)
DLSpace: Optimizing SSD lifetime via an efficient distributed log space allocation strategy,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061098510&doi=10.1145%2f3284749&partnerID=40&md5=02443c2fb1db4b28141a7586c4f89e24,"Due to limited numbers of program/erase cycles (i.e., P/Es) of NAND Flash, excessive out-of-place update and erase-before-write operations wear out these P/Es during garbage collections, which adversely shorten solid state disk (i.e., SSD) lifetime. The log space in NAND Flash space of an SSD performs as an updated pages buffer, which lowers garbage-collection frequency while reducing consumption of P/Es to extend SSD lifetime. In this article, we propose DLSpace, a novel distributed log space allocation strategy named distributed log space, which divides log space into block-level log space and page-level log space to significantly optimize SSD lifetime. DLSpaces log page space is dedicated to data pages in a data block. Such log page space only buffers page-update operations in this data block; thereby the use of log blocks for postponing garbage collection delays. DLSpace is conducive to fully utilizing pages in data and log blocks to avoid erasures of blocks with free pages. Consequently, DLSpace decreases write amplification by reducing excessive valid page-rewrite and block-erase operations under random-write-intensive workloads. We carried out quantitative research on the extension of SSD lifetime by virtue of three metrics (i.e., write amplification, the number of block-erase operations, and the delay time before the first garbage collection occurring). Experimental results reveal that compared with the existing traditional allocation strategy for log space (i.e., TLSpace), DLSpace reduces write amplification and the number of erase operations by up to 55.2% and 64.1% to the most extent, respectively. DLSpace also extends TLSpaces delay time of garbage collections by 73.3% to optimize SSD lifetime. © 2018 Association for Computing Machinery.",Distributed log space; Lifetime; Solid state disk; Traditional log space; Write amplification,Memory architecture; NAND circuits; Refuse collection; Allocation strategy; Garbage collection; Lifetime; Log space; Quantitative research; Solid state disks; Write amplifications; Write-intensive workloads; Flash-based SSDs
GroupSense: Recognizing and understanding group physical activities using multi-device embedded sensing,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061101029&doi=10.1145%2f3295747&partnerID=40&md5=e5cc9d39b4c6aeb892a01c413c2d8acb,"Human activity recognition using embedded mobile and embedded sensors is becoming increasingly important. Scaling up from individuals to groups, that is, Group Activity Recognition (GAR), has attracted significant attention recently. This article proposes a model and modeling language for GAR called GroupSense-L and a novel distributed middleware called GroupSense for mobile GAR. We implemented and tested GroupSense using smartphone sensors, smartwatch sensors, and embedded sensors in things, where we have a protocol for these different devices to exchange information required for GAR. A range of continuous group activities (from simple to fairly complex) illustrates our approach and demonstrates the feasibility of our model and richness of the proposed specialization. We then conclude with lessons learned for GAR and future work. © 2019 Association for Computing Machinery.",Context-aware computing; Group activity recognition (GAR); Sensor reasoning,Middleware; Modeling languages; Context-aware computing; Distributed middleware; Embedded sensing; Embedded sensors; Group activities; Group activity recognition; Human activity recognition; Physical activity; Pattern recognition
Exact WCRT analysis for message-processing tasks on gateway-integrated in-vehicle Can clusters,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061085190&doi=10.1145%2f3284178&partnerID=40&md5=1e120111cfa9accde533fbec6e4672b9,"A typical automotive integrated architecture is a controller area network (CAN) cluster integrated by a central gateway. This study proposes a novel and exact worst-case response time (WCRT) analysis method for message-processing tasks in the gateway. We first propose a round search method to obtain lower bound on response time (LBRT) and upper bound on response time (UBRT), respectively. We then obtain the exact WCRT belonging to the scope of the LBRT and UBRT with an effective non-exhaustive exploration. Experimental results on a real CAN message set reveal that the proposed exact analysis method can reduce 99.99999% combinations on large-scale CAN clusters. © 2018 Association for Computing Machinery.",Controller area network; Exact worst-case response time; Lower bound; Message-processing tasks; Upper bound; Worst case,Cluster computing; Control system synthesis; Controllers; Process control; Controller area network; Lower bounds; Message processing; Upper Bound; Worst case response time; Gateways (computer networks)
Momentum: Power-neutral Performance Scaling with Intrinsic MPPT for Energy Harvesting Computing Systems,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061085472&doi=10.1145%2f3281300&partnerID=40&md5=351750d6fd3e94cdf068e603965c6f7d,"Recent research has looked to supplement or even replace the batteries in embedded computing systems with energy harvesting, where energy is derived from the device's environment. However, such supplies are generally unpredictable and highly variable, and hence systems typically incorporate large external energy buffers (e.g., supercapacitors) to sustain computation; however, these pose environmental issues and increase system size and cost. This article proposes Momentum, a general power-neutral methodology, with intrinsic system-wide maximum power point tracking, that can be applied to a wide range of different computing systems, where the system dynamically scales its performance (and hence power consumption) to optimize computational progress depending on the power availability. Momentum enables the system to operate around an efficient operating voltage, maximizing forward application execution, without adding any external tracking or control units. This methodology combines at runtime (1) a hierarchical control strategy that utilizes available power management controls (such as dynamic voltage and frequency scaling, and core hot-plugging) to achieve efficient power-neutral operation; (2) a software-based maximum power point tracking scheme (unlike existing approaches, this does not require any additional hardware), which adapts the system power consumption so that it can work at the optimal operating voltage, considering the efficiency of the entire system rather than just the energy harvester; and (3) experimental validation on two different scales of computing system: a low power microcontroller (operating from the already-present 4.7μF decoupling capacitance) and a multi-processor system-on-chip (operating from 15.4mF added capacitance). Experimental results from both a controlled supply and energy harvesting source show that Momentum operates correctly on both platforms and exhibits improvements in forward application execution of up to 11% when compared to existing power-neutral approaches and 46% compared to existing static approaches. © 2019 Association for Computing Machinery.",Embedded computing systems; Energy harvesting; Maximum power point tracking; Performance adaptation; Power neutrality; Transient computing,Capacitance; Dynamic frequency scaling; Electric power utilization; Embedded systems; Energy efficiency; Green computing; Maximum power point trackers; Momentum; System-on-chip; Voltage scaling; Dynamic voltage and frequency scaling; Embedded computing system; Experimental validations; Low-power microcontrollers; Maximum Power Point Tracking; Multi processor system on chips; Performance adaptation; Power neutrality; Energy harvesting
Mining missing assumptions from counter-examples,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061192286&doi=10.1145%2f3288759&partnerID=40&md5=7c8207857f533affe16ee9d82d00f471,"During the formal functional verification of Register-Transfer Level designs, a false failure is often observed. Most of the time, this failure is caused by an underconstrained model. The analysis of the root cause for the verification error and the creation of missing assumptions are a significant time burden. In this article, we present a methodology to automatically mine these missing assumptions from counter-examples. First, multiple counter-examples are generated for the same property. Then, relevant behaviors are mined from the counter-examples. Finally, corresponding assumptions are filtered and a small amount is returned to the user for review. © 2019 Association for Computing Machinery.",Filtering criteria; Justification; Model checking; Property mining,Software engineering; Counter examples; Filtering criteria; Functional verification; Justification; Register transfer level; Root cause; Under-constrained; Model checking
Energy-efficient multicore scheduling for hard real-time systems: A survey,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061079190&doi=10.1145%2f3291387&partnerID=40&md5=8f55cd70d4203230aa5b1aa0df543248,"As real-time embedded systems are evolving in scale and complexity, the demand for a higher performance at a minimum energy consumption has become a necessity. Consequently, many embedded systems are now adopting multicore architectures into their design. However, scheduling on multicores is not a trivial task and scheduling to minimize the energy consumption further increases the complexity of the problem. This problem is especially aggravated for hard real-time systems where failure to meet a deadline can be catastrophic. Such scheduling algorithms yearn for a polynomial time complexity for the task-to-core assignment problem with an objective to minimize the overall energy consumption. There is now a trend toward heterogeneous multicores where cores differ in power, performance, and architectural capabilities. The desired performance and energy consumption is attained by assigning a task to the core that is best suited for it. In this article, we present a survey on energy-efficient multicore scheduling algorithms for hard real-time systems. We summarize various algorithms reported in the literature and classify them based on Partitioned, Semi-Partitioned, and Global scheduling techniques for both homogeneous and heterogeneous multicores. We also present a detailed discussion on various open issues within this domain. © 2018 Association for Computing Machinery.",Embedded systems; Energy-efficiency; Heterogeneous multicores; Homogeneous multicores; Real-time scheduling,Combinatorial optimization; Embedded systems; Energy efficiency; Energy utilization; Interactive computer systems; Polynomial approximation; Scheduling; Scheduling algorithms; Software architecture; Surveys; Hard real-time systems; Heterogeneous Multi-Cores; Minimum energy consumption; Multi-cores; Multicore architectures; Polynomial time complexity; Real - time scheduling; Real-time embedded systems; Real time systems
Editorial: Embedded security challenge: Cyber security contests in the embedded computing domain,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061094528&doi=10.1145%2f3293502&partnerID=40&md5=64c1934f0832254ebc4c6785de80e7c5,[No abstract available],,
An efficient UAV hijacking detection method using onboard inertial measurement unit,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061097345&doi=10.1145%2f3289390&partnerID=40&md5=ba2d0da3d77a24c76b6e76e318c6a6a7,"With the fast growth of civil drones, their security problems meet significant challenges. A commercial drone may be hijacked by a GPS-spoofing attack for illegal activities, such as terrorist attacks. The target of this article is to develop a technique that only uses onboard gyroscopes to determine whether a drone has been hijacked. Ideally, GPS data and the angular velocities measured by gyroscopes can be used to estimate the acceleration of a drone, which can be further compared with the measurement of the accelerometer to detect whether a drone has been hijacked. However, the detection results may not always be accurate due to some calculation and measurement errors, especially when no hijacking occurs in curve trajectory situations. To overcome this, in this article, we propose a novel and simple method to detect hijacking only based on gyroscopes' measurements and GPS data, without using any accelerometer in the detection procedure. The computational complexity of our method is very low, which is suitable to be implemented in the drones with micro-controllers. On the other hand, the proposed method does not rely on any accelerometer to detect attacks, which means it receives less information in the detection procedure and may reduce the results accuracy in some special situations. While the previous method can compensate for this flaw, the high detection results also can be guaranteed by using the above two methods. Experiments with a quad-rotor drone are conducted to show the effectiveness of the proposed method and the combination method. © 2018 Association for Computing Machinery.",,Accelerometers; Drones; Global positioning system; Gyroscopes; Terrorism; Combination method; Detection methods; Illegal activities; Inertial measurement unit; Security problems; SIMPLE method; Spoofing attacks; Terrorist attacks; Aircraft detection
The mechanized marriage of effects and monads with applications to high-assurance hardware,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059854756&doi=10.1145%2f3274282&partnerID=40&md5=154d2ecc19003508a63df29fb92e26fd,"Constructing high-assurance, secure hardware remains a challenge, because to do so relies on both a verifiable means of hardware description and implementation. However, production hardware description languages (HDL) lack the formal underpinnings required by formal methods in security. Still, there is no such thing as high-assurance systems without high-assurance hardware. We present a core calculus of secure hardware description with its formal semantics, security type system, and mechanization in Coq. This calculus is the core of the functional HDL, ReWire, shown in previous work to have useful applications in reconfigurable computing. This work supports a full-fledged, formal methodology for producing high-assurance hardware. © 2019 Copyright held by the owner/author(s).",Hardware verification; High-level synthesis; Security,Calculations; Computer hardware; Computer hardware description languages; Formal methods; High level synthesis; Machinery; Reconfigurable architectures; Semantics; Formal methods in security; Hardware description languages (HDL); Hardware descriptions; Hardware verification; High assurance systems; Reconfigurable computing; Security; Security type systems; Hardware security
Guest editorial: Special issue of ACM TECS on the ACM-IEEE international conference on formal methods and models for system design (MEMOCODE 2017),2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059854744&doi=10.1145%2f3292422&partnerID=40&md5=12bb3f1d3eca1baefef019388e5c73ab,[No abstract available],,
Quantifying the information leakage in cache attacks via symbolic execution,2019,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059872115&doi=10.1145%2f3288758&partnerID=40&md5=37fd417be633cfae8a251d634f456c96,"Cache attacks allow attackers to infer the properties of a secret execution by observing cache hits and misses. But how much information can actually leak through such attacks? For a given program, a cache model, and an input, our CHALICE framework leverages symbolic execution to compute the amount of information that can possibly leak through cache attacks. At the core of CHALICE is a novel approach to quantify information leakage that can highlight critical cache side-channel leakage on arbitrary binary code. In our evaluation on real-world programs from OpenSSL and Linux GDK libraries, CHALICE effectively quantifies information leakage: For an AES-128 implementation on Linux, for instance, CHALICE finds that a cache attack can leak as much as 127 out of 128 bits of the encryption key. © 2019 Association for Computing Machinery.",Cache; Security; Side channel; Symbolic execution,Linux; Model checking; Amount of information; Cache; Encryption key; Information leakage; Real world projects; Security; Side-channel; Symbolic execution; Side channel attack
A design-time/run-time application mapping methodology for predictable execution time in MPSoCs,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058305072&doi=10.1145%2f3274665&partnerID=40&md5=5ab39b42ad0c20ee32dda40c7ce4574a,"Executing multiple applications on a single MPSoC brings the major challenge of satisfying multiple quality requirements regarding real-time, energy, and so on. Hybrid application mapping denotes the combination of design-time analysis with run-time application mapping. In this article, we present such a methodology, which comprises a design space exploration coupled with a formal performance analysis. This results in several resource reservation configurations, optimized formultiple objectives, with verified real-time guarantees for each individual application. The Pareto-optimal configurations are handed over to run-time management, which searches for a suitable mapping according to this information. To provide any real-time guarantees, the performance analysis needs to be composable and the influence of the applications on each other has to be bounded. We achieve this either by spatial or a novel temporal isolation for tasks and by exploiting composable networks-on-chip (NoCs). With the proposed temporal isolation, tasks of different applications can be mapped to the same resource, while, with spatial isolation, one computing resource can be exclusively used by only one application. The experiments reveal that the success rate in finding feasible application mappings can be increased by the proposed temporal isolation by up to 30% and energy consumption can be reduced compared to spatial isolation. © 2018 Association for Computing Machinery.",Design space exploration; Hybrid mapping; Many-core; Networks-on-chip; Predictability,Energy utilization; Mapping; Multiprocessing systems; Network-on-chip; Pareto principle; Design space exploration; Hybrid mapping; Many core; Networks on chips; Predictability; Integrated circuit design
Packet aggregation real-time scheduling for large-scale WIA-PA industrial wireless sensor networks,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053764690&doi=10.1145%2f3266228&partnerID=40&md5=fba0947b1e32c3727da43bc4030bf05e,"The IEC standard WIA-PA is a communication protocol for industrial wireless sensor networks. Its special features, including a hierarchical topology, hybrid centralized-distributed management and packet aggregation make it suitable for large-scale industrial wireless sensor networks. Industrial systems place large real-time requirements on wireless sensor networks. However, the WIA-PA standard does not specify the transmission methods, which are vital to the real-time performance of wireless networks, and little work has been done to address this problem. In this article, we propose a real-time aggregation scheduling method for WIA-PA networks. First, to satisfy the real-time constraints on dataflows, we propose a method that combines the real-time theory with the classical bin-packing method to aggregate original packets into the minimum number of aggregated packets. The simulation results indicate that our method outperforms the traditional bin-packing method, aggregating up to 35% fewer packets, and improves the real-time performance by up to 10%. Second, to make it possible to solve the scheduling problem of WIA-PA networks using the classical scheduling algorithms, we transform the ragged time slots of WIA-PA networks to a universal model. In the simulation, a large number of WIA-PA networks are randomly generated to evaluate the performances of several real-time scheduling algorithms. By comparing the results, we obtain that the earliest deadline first real-time scheduling algorithm is the preferred method for WIA-PA networks. © 2018 Copyright held by the owner/author(s).",Industrial wireless sensor networks; Multi-channel; Packet aggregation; Real-time scheduling; TDMA,Packet networks; Real time systems; Scheduling; Scheduling algorithms; Time division multiple access; Distributed management; Earliest deadline first; Hierarchical topology; Industrial wireless sensor networks; Multi channel; Packet aggregation; Real - time scheduling; Real-time scheduling algorithms; Wireless sensor networks
Energy-efficient real-time scheduling of dag tasks,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053772054&doi=10.1145%2f3241049&partnerID=40&md5=eafe4f5e8e7b0d86438bdad2f3da9e65,"This work studies energy-aware real-time scheduling of a set of sporadic Directed Acyclic Graph (DAG) tasks with implicit deadlines. While meeting all real-time constraints, we try to identify the best task allocation and execution pattern such that the average power consumption of the whole platform is minimized. To our knowledge, this is the first work that addresses the power consumption issue in scheduling multiple DAG tasks on multi-cores and allows intra-task processor sharing. First, we adapt the decomposition-based framework for federated scheduling and propose an energy-sub-optimal scheduler. Then, we derive an approximation algorithm to identify processors to be merged together for further improvements in energy-efficiency. The effectiveness of the proposed approach is evaluated both theoretically via approximation ratio bounds and also experimentally through simulation study. Experimental results on randomly generated workloads show that our algorithms achieve an energy saving of 60% to 68% compared to existing DAG task schedulers. © 2018 Association for Computing Machinery.",Convex optimization; Energy minimization; Parallel task; Real-time scheduling,
Schedule adaptation for ensuring reliability in Rt-WiFi-based networked,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056408986&doi=10.1145%2f3236011&partnerID=40&md5=efa1d3237202bdc4f10e1419c03c376b,"With the ever-growing interests in applying wireless technologies for networked embedded systems to serve as the communication fabric, many real-time wireless technologies have been recently developed to support time-critical sensing and control applications. We proposed in previous work the RT-WiFi protocol that provides real-time high-speed predictable data delivery and enables designs to meet time-critical industrial needs. However, without explicit reliability enforcement mechanisms, our previous RT-WiFi design is either subject to uncontrolled packet loss due to noise and other interferences or may suffer from inefficient communication channel usage. In this article, we explicitly consider interference from both Wi-Fi and non-Wi-Fi based interference sources and propose two sets of effective solutions for reliable data transmissions in RT-WiFi-based networked embedded systems. To improve reliability against general non-Wi-Fi based interference, based on rate adaptation and retransmission techniques, we present an optimal real-time rate adaption algorithm together with a communication link scheduler that has low network management overhead. A novel technique called overbooking is introduced to further improve the schedulability of the communication link scheduler while maintaining the required communication reliability. For Wi-Fi-based interference, we present mechanisms that utilize virtual carrier sensing to provide reliable data transmission while co-existing with regular Wi-Fi networks. We have implemented the proposed algorithms in the RT-WiFi network management framework and demonstrated the system performance with a series of experiments. © 2018 Association for Computing Machinery.",Media access control; Real-time systems; Wireless networks,Data transfer; Embedded systems; Interactive computer systems; Medium access control; Network management; Networked control systems; Real time systems; Reliability; Scheduling; Wireless local area networks (WLAN); Wireless networks; Wireless telecommunication systems; Communication reliabilities; Control applications; Enforcement mechanisms; Media access control; Networked embedded systems; Reliable data transmission; Virtual carrier sensing; Wireless technologies; Wi-Fi
Providing accountability in heterogeneous systems-on-chip,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053755316&doi=10.1145%2f3241048&partnerID=40&md5=02a58b1860e6a5ec9c1bf09058c89984,"When modern systems-on-chip (SoCs), containing designs from different organizations, miscompute or un-derperform in the field, discerning the responsible component is a non-trivial task. A perfectly accountable system is one in which the on-chip component at fault is always unambiguously detected. The achievement of accountability can be greatly aided by the collection of runtime information that captures the events in the system that led to the error. Such information collection must be fair and impartial to all parties. In this article, we prove that logging messages communicated between components from different organizations is sufficient to provide accountability, provided the logs are authentic. We then construct a solution based on this premise, with an on-chip trusted auditing system to authenticate the logs. We present a thorough design of the auditing system, and demonstrate that its performance overhead is a mere 0.49%, and its area overhead is a mere 0.194% (in a heterogeneous 48 core, 400mm2 chip). We also demonstrate the viability of this solution using three representative bugs found in popular commercial SoCs. © 2018 Association for Computing Machinery.",Accelerators; Accountability; Auditing; Heterogeneous processors; SoC; Third-party IPs,
Editorial: Need for artifact verified articles in ACM transactions,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058290089&doi=10.1145%2f3282437&partnerID=40&md5=9e509ce7ff655b19d98902e2a969d054,[No abstract available],,
A reconfiguration-based fault-tolerant anti-lock brake-by-wire system,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056400357&doi=10.1145%2f3242178&partnerID=40&md5=018f9bd54e39a19aa5242f4deb060ef2,"Anti-Lock Braking Systems (ABS) and Brake-by-Wire Systems (BBW) are safety-critical applications by nature. Such systems are required to demonstrate high degrees of dependability. Fault-tolerance is the primary means to achieve dependability at runtime and has been an active research area for decades. Fault-tolerance is usually achieved in traditional embedded computing systems through redundancy and voting methods. In such systems, hardware units, actuators, sensors, and communication networks are replicated where special voters vote against faulty units. In addition to traditional hardware and software redundancy, hybrid and reconfiguration-based approaches to fault-tolerance are evolving. In this article, we present a reconfiguration-based fault-tolerant approach to achieve high dependability in ABS BBW braking systems. The proposed architecture makes use of other components of less safety-critical systems to maintain high dependability in the more safety-critical systems. This is achieved by migrating safety-critical software tasks from embedded computer hardware that runs into a malfunction to other embedded computing hardware running less-critical software tasks. Or by using a different configuration in terms of the used speed sensors and type of ABS. The proposed architecture is on average 20% more reliable than conventional ABS architectures assuming equal reliabilities of different components. © 2018 Association for Computing Machinery.",ABS; Brake-by-Wire; Distributed Embedded Systems; Fault Tolerance; Graceful Degradation; Reconfiguration-Based Fault Tolerance,Computer hardware; Embedded systems; Fault tolerance; Fault tolerant computer systems; Hardware; Locks (fasteners); Network architecture; Redundancy; Safety engineering; Security systems; Wire; Anti-lock braking systems (ABS); Brake-by-wires; Distributed embedded system; Embedded computing system; Graceful degradation; Safety critical applications; Safety critical software; Safety critical systems; Anti-lock braking systems
OpenCL-based virtual prototyping and simulation of many-accelerator architectures,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054224854&doi=10.1145%2f3242179&partnerID=40&md5=59e207e6ba0d306b86e963b1963ce3d5,"Heterogeneous architectures featuring multiple hardware accelerators have been proposed as a promising solution for meeting the ever-increasing performance and power requirements of embedded systems. However, the existence of numerous design parameters may result in different architectural schemes and thus in extra design effort. To address this issue, OpenCL-based frameworks have been recently utilized for FPGA programming, to enable the portability of a source code to multiple architectures. However, such OpenCL frameworks focus on RTL design, thus not enabling rapid prototyping and abstracted modeling of complex systems. Virtual Prototyping aims to overcome this problem by enabling the system modeling in higher abstraction levels. This article combines the benefits of OpenCL and Virtual Prototyping, by proposing an OpenCL-based prototyping framework for data-parallel many-accelerator systems, which (a) creates a SystemC Virtual Platform from OpenCL, (b) provides a co-simulation environment for the host and the Virtual Platform, (c) offers memory and interconnection models for parallel data processing, and (d) enables the system evaluation with alternative real number representations (e.g., fixed-point or 16-bit floating-point). © 2018 Association for Computing Machinery.",,Abstracting; Data handling; Digital arithmetic; Embedded systems; Fixed platforms; Large scale systems; Parallel processing systems; Virtual prototyping; Accelerator architectures; Accelerator system; Design parameters; Hardware accelerators; Heterogeneous architectures; Parallel data processing; Power requirement; System evaluation; Virtual reality
Exposing implementation details of embedded DRAM memory controllers through latency-based analysis,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056410289&doi=10.1145%2f3274281&partnerID=40&md5=dead51fa2dacdd1b93c7c21792441b86,"We explore techniques to reverse-engineer DRAM embedded memory controllers (MCs), including page policies, address mapping, and command arbitration. There are several benefits to knowing this information: They allow tightening worst-case bounds of embedded systems and platform-aware optimizations at the operating system, source-code, and compiler levels. We develop a latency-based analysis, which we use to devise algorithms and C programs to extract MC properties. We show the effectiveness of the proposed approach by reverse-engineering the MC details in the XUPV5-LX110T Xilinx platform. Furthermore, to cover a breadth of policies, we use a simulation framework and document our findings. © 2018 Association for Computing Machinery.",Analysis; DRAM; Inference; Memory controllers; Reverse engineering,C (programming language); Controllers; Embedded systems; Reverse engineering; Address mappings; Analysis; Embedded DRAM; Embedded memory; Inference; Memory controller; Simulation framework; Source codes; Dynamic random access storage
Declarative resilience: A holistic soft-error resilient multicore architecture that trades off program accuracy for efficiency,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052683020&doi=10.1145%2f3210559&partnerID=40&md5=f4b1a5e8f6f67bd7ba6fd9b9cb8a1d34,"To protect multicores from soft-error perturbations, research has explored various resiliency schemes that provide high soft-error coverage. However, these schemes incur high performance and energy overheads.We observe that not all soft-error perturbations affect program correctness, and some soft-errors only affect program accuracy, i.e., the program completes with certain acceptable deviations from error free outcome. Thus, it is practical to improve processor efficiency by trading off resiliency overheads with program accuracy. This article proposes the idea of declarative resilience that selectively applies strong resiliency schemes for code regions that are crucial for program correctness (crucial code) and lightweight resiliency for code regions that are susceptible to program accuracy deviations as a result of soft-errors (non-crucial code). At the application level, crucial and non-crucial code is identified based on its impact on the program outcome. A cross-layer architecture enables efficient resilience along with holistic soft-error coverage. Only program accuracy is compromised in the worst-case scenario of a soft-error strike during non-crucial code execution. For a set of machine-learning and graph analytic benchmarks, declarative resilience reduces performance overhead over a state-of-the-art system that applies strong resiliency for all program code regions from ∼ 1.43× to ∼ 1.2×. © 2018 ACM.",Graph analytics; Machine learning; Program accuracy; Soft-errors,Application programs; Artificial intelligence; Benchmarking; Codes (symbols); Commerce; Error correction; Learning systems; Program processors; Radiation hardening; Software architecture; Cross-layer architecture; Graph analytics; Multicore architectures; Program accuracy; Program correctness; Soft error; State-of-the-art system; Worst case scenario; Multicore programming
A mirroring-assisted channel-RAID5 SSD for mobile applications,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052714811&doi=10.1145%2f3209625&partnerID=40&md5=3afdce65965ab8f92e0c91685df41016,"Simply applying an existing redundant array of independent disks (RAID) technique to enhance data reliability within a single solid-state drive for safety-critical mobile applications significantly degrades performance. In this article, we first propose a new RAID5 architecture called channel-RAID5 with mirroring (CR5M) to alleviate the performance degradation problem. Next, an associated data reconstruction strategy called mirroring-assisted channel-level reconstruction (MCR) is developed to further shrink the window of vulnerability. Experimental results demonstrate that compared with channel-RAID5 (CR5), CR5M improves performance up to 40.2%. Compared with disk-oriented reconstruction, a traditional data reconstruction scheme, MCR on average improves data recovery speed by 7.5% while delivering a similar performance during reconstruction. © 2018 ACM.",Data reconstruction; NAND flash; RAID; SSD,Memory architecture; Mobile computing; Safety engineering; Servers; Data reconstruction; Mobile applications; NAND Flash; Performance degradation; RAID; Redundant array of independent disks; Solid state drives; Window of vulnerability; Digital storage
An analytical cache performance evaluation framework for embedded out-of-order processors using software characteristics,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052704638&doi=10.1145%2f3233182&partnerID=40&md5=1f576e82a18156a2703132027e76342a,"Utilizing analytical models to evaluate proposals or provide guidance in high-level architecture decisions is been becoming more and more attractive. A certain number of methods have emerged regarding cache behaviors and quantified insights in the last decade, such as the stack distance theory and the memory level parallelism (MLP) estimations. However, prior research normally oversimplified the factors that need to be considered in out-of-order processors, such as the effects triggered by reordered memory instructions, and multiple dependences among memory instructions, along with the merged accesses in the same MSHR entry. These ignored influences actually result in low and unstable precisions of recent analytical models. By quantifying the aforementioned effects, this article proposes a cache performance evaluation framework equipped with three analytical models, which can more accurately predict cache misses, MLPs, and the average cache miss service time, respectively. Similar to prior studies, these analytical models are all fed with profiled software characteristics in which case the architecture evaluation process can be accelerated significantly when compared with cycle-accurate simulations. We evaluate the accuracy of proposed models compared with gem5 cycle-accurate simulations with 16 benchmarks chosen from Mobybench Suite 2.0, Mibench 1.0, and Mediabench II. The average root mean square errors for predicting cache misses, MLPs, and the average cache miss service time are around 4%, 5%, and 8%, respectively. Meanwhile, the average error of predicting the stall time due to cache misses by our framework is as low as 8%. The whole cache performance estimation can be sped by about 15 times versus gem5 cycle-accurate simulations and 4 times when compared with recent studies. Furthermore, we have shown and studied the insights between different performance metrics and the reorder buffer sizes by using our models. As an application case of the framework, we also demonstrate how to use our framework combined with McPAT to find out Pareto optimal configurations for cache design space explorations. © 2018 ACM.",Analytical models; Cache miss service time; Cache misses; Memory level parallelism; Software characteristics,Analytical models; Forecasting; Mean square error; Memory architecture; Parallel processing systems; Pareto principle; Cache Miss; Cycle-accurate simulation; Memory level parallelisms; Out-of-order processors; Pareto-optimal configurations; Root mean square errors; Service time; Software characteristic; Cache memory
Scalable analysis for multi-scale dataflow models,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052673201&doi=10.1145%2f3233183&partnerID=40&md5=8d5614bc6a730a70a6392f50d86b0c74,"Multi-scale dataflow models have actors acting at multiple granularity levels, e.g., a dataflow model of a video processing application with operations on frame, line, and pixel level. The state of the art timing analysis methods for both static and dynamic dataflow types aggregate the behaviours across all granularity levels into one, often large iteration, which is repeated without exploiting the structure within such an iteration. This poses scalability issues to dataflow analysis, because behaviour of the large iteration is analysed by some form of simulation that involves a large number of actor firings. We take a fresh perspective of what is happening inside the large iteration. We take advantage of the fact that the iteration is a sequence of smaller behaviours, each captured in a scenario, that are typically repeated many times. We use the (max, +) linear model of dataflow to represent each of the scenarios with a matrix. This allows a compositional worst-case throughput analysis of the repeated scenarios by raising the matrices to the power of the number of repetitions, which scales logarithmically with the number of repetitions, whereas the existing throughput analysis scales linearly. We moreover provide the first exact worst-case latency analysis for scenario-aware dataflow. This compositional latency analysis also scales logarithmically when applied to multi-scale dataflow models. We apply our new throughput and latency analysis to several realistic applications. The results confirm that our approach provides a fast and accurate analysis. © 2018 ACM.",(MAX; +); Dataflow; Latency; Scalable analysis; Scenarios; Throughput,Iterative methods; Matrix algebra; Throughput; Video signal processing; (MAX; Dataflow; Latency; Scalable analysis; Scenarios; Data flow analysis
Design and analysis of battery-aware automotive climate control for electric vehicles,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052683698&doi=10.1145%2f3203408&partnerID=40&md5=b52e9bc4b21e435dfe85777b0a3800ea,"Electric Vehicles (EV) as a zero-emission means of transportation encounter challenges in battery design that cause a range anxieties for the drivers. Besides the electric motor, the Heating, Ventilation, and Air Conditioning (HVAC) system is another major contributor to the power consumption that may influence the EV battery lifetime and driving range. In the state-of-the-art methodologies for battery management systems, the battery performance is monitored and improved. While in the automotive climate control, the passenger's thermal comfort is the main objective. Hence, the influence of the HVAC power on the battery behavior for the purpose of jointly optimized battery management and climate control has not been considered. In this article, we propose an automotive climate control methodology that is aware of the battery behavior and performance, while maintaining the passenger's thermal comfort. In our methodology, battery parameters and cabin temperature are modeled and estimated, and the HVAC utilization is optimized and adjusted with respect to the electric motor and HVAC power requests. Therefore, the battery stress reduces, while the cabin temperature is maintained by predicting and optimizing the system states in the near-future. We have implemented our methodology and compared its performance to the state-of-the-art in terms of battery lifetime improvement and energy consumption reduction. We have also conducted experiments and analyses to explore multiple control window sizes, drive profiles, ambient temperatures, and modeling error rates in the methodology. It is shown that our battery-aware climate control can extend the battery lifetime by up to 13.2% and reduce the energy consumption by up to 14.4%. © 2018 ACM.",Battery; Climate control; Electric vehicle; HVAC; Model predictive control; Optimization; Quadratic programming,Air conditioning; Climate control; Climate models; Electric vehicles; Energy utilization; Model predictive control; Optimization; Quadratic programming; Secondary batteries; Thermal comfort; Traction motors; Automotive climate control; Battery; Battery Management; Battery performance; Design and analysis; HVAC; Means of transportations; State of the art; Battery management systems
Editorial: Early career researchers in embedded computing,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052704250&doi=10.1145%2f3241724&partnerID=40&md5=fce43d98cf7d43c5982eb0f390386e55,[No abstract available],,
Shared last-level cache management and memory scheduling for GPGPUs with hybrid main memory,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052689609&doi=10.1145%2f3230643&partnerID=40&md5=dab38953d6d9cd67431cdbeff3bde59a,"Memory intensive workloads become increasingly popular on general purpose graphics processing units (GPGPUs), and impose great challenges on the GPGPU memory subsystem design. On the other hand, with the recent development of non-volatile memory (NVM) technologies, hybridmemory combining both DRAM and NVM achieves high performance, low power, and high density simultaneously, which provides a promising main memory design for GPGPUs. In this article, we explore the shared last-level cache management for GPGPUs with consideration of the underlying hybrid main memory. To improve the overall memory subsystem performance, we exploit the characteristics of both the asymmetric read/write latency of the hybrid main memory architecture, as well as the memory coalescing feature of GPGPUs. In particular, to reduce the average cost of L2 cache misses, we prioritize cache blocks from DRAM or NVM based on observations that operations to NVM part of main memory have a large impact on the system performance. Furthermore, the cache management scheme also integrates the GPU memory coalescing and cache bypassing techniques to improve the overall system performance. To minimize the impact of memory divergence behaviors among simultaneously executed groups of threads, we propose a hybrid main memory and warp aware memory scheduling mechanism for GPGPUs. Experimental results show that in the context of a hybrid main memory system, our proposed L2 cache management policy and memory scheduling mechanism improve performance by 15.69% on average for memory intensive benchmarks, whereas the maximum gain can be up to 29% and achieve an average memory subsystem energy reduction of 21.27%. © 2018 ACM.",Cache bypassing; Cache management; GPGPU; Hybrid memory; Memory scheduling; NVM,Benchmarking; Computer graphics; Dynamic random access storage; Flocculation; Graphics processing unit; Integrated circuit design; Memory architecture; Program processors; Scheduling; Cache bypassing; Cache management; GPGPU; Hybrid memory; Memory scheduling; Cache memory
Device-free motion & trajectory detection via RFID,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052692319&doi=10.1145%2f3230644&partnerID=40&md5=a0b78cabaa7188da009f8dbde7ba5eab,"Compared with traditional methods that employ inertial sensors or wireless sensors, device-free approaches do not require that people carry devices, and they are considered a useful technique for indoor navigation and posture recognition. However, few existing methods can detect the trajectory and movements of humans at the same time. In this study, we propose a scheme called PADAR for addressing these two problems simultaneously by using passive radio frequency identification (RFID) tags but without attaching them to the human body. The idea is based on the principle of radio tomographic imaging, where the variance in a tag's backscattered radio frequency signal strength is influenced by human movement. We integrated a commodity off-the-shelf RFID reader with a two-dimensional phased array antenna and a matrix of passive tags to evaluate the performance of our scheme. We conducted experiments in a simulated indoor environment. The experimental results showed that PADAR achieved an accuracy of over 70%. © 2018 ACM.",Device-free; Indoor navigation; Motion detection; RFID; RSS,Antenna phased arrays; Indoor positioning systems; Radio waves; RSS; Commodity off the shelves; Device-free; In-door navigations; Motion detection; Passive radio frequency identification (RFID); Phased array antennas; Radiofrequency signals; Tomographic imaging; Radio frequency identification (RFID)
SLiSCP-light: Towards hardware optimized sponge-specific cryptographic permutations,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052709032&doi=10.1145%2f3233245&partnerID=40&md5=26edbfe64dd0815d722251c0c5e6bee7,"The emerging areas in which highly resource constrained devices are interacting wirelessly to accomplish tasks have led manufacturers to embed communication systems in them. Tiny low-end devices such as sensor networks nodes and Radio Frequency Identification (RFID) tags are of particular importance due to their vulnerability to security attacks, whichmakes protecting their communication privacy and authenticity an essentialmatter. In thiswork,we present a lightweight do-it-all cryptographic design that offers the basic underlying functionalities to secure embedded communication systems in tiny devices. Specifically, we revisit the design approach of the sLiSCP family of lightweight cryptographic permutations,whichwas proposed in SAC 2017. sLiSCP is designed to be used in a unified duplex sponge construction to provide minimal overhead for multiple cryptographic functionalitieswithin one hardware design. The design of sLiSCP follows a 4-subblock Type-2 Generalized Feistel-like Structure (GFS) with unkeyed round-reduced Simeck as the round function, which are extremely efficient building blocks in terms of their hardware area requirements. In sLiSCP-light, we tweak the GFS design and turn it into an elegant Partial Substitution-Permutation Network construction, which further reduces the hardware areas of the sLiSCP permutations by around 16% of their original values. The new design also enhances the bit diffusion and algebraic properties of the permutations and enables us to reduce the number of steps, thus achieving a better throughput in both the hashing and authentication modes. We perform a thorough security analysis of the new design with respect to its diffusion, differential and linear, and algebraic properties. For sLiSCP-light-192, we report parallel implementation hardware areas of 1,820 (respectively, 1,892)GE in CMOS 65nm (respectively, 130nm) ASIC. The areas for sLiSCP-light-256 are 2,397 and 2,500GE in CMOS 65nm and 130nm ASIC, respectively. Overall, the unified duplex sponge mode of sLiSCP-light-192, which provides (authenticated) encryption and hashing functionalities, satisfies the area (1,958GE), power (3.97μW ), and throughput (44.4kbps) requirements of passive RFID tags. © 2018 ACM.",Cryptographic permutations; Lightweight cryptography; Partial substitution and permutation network (PSPN); Simeck block cipher; Sponge duplexing,Algebra; Authentication; CMOS integrated circuits; Cryptography; Embedded systems; Hardware; Integrated circuit design; Radio frequency identification (RFID); Sensor networks; Sensor nodes; Block ciphers; Cryptographic permutations; Duplexing; Light-weight cryptography; Permutation network; Network security
Combining Software Cache Partitioning and Loop Tiling for Effective Shared Cache Management,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073580234&doi=10.1145%2f3202663&partnerID=40&md5=b5b567625b3de12fe3dab72b5d31f3c5,"One of the biggest challenges in multicore platforms is shared cache management, especially for data-dominant applications. Two commonly used approaches for increasing shared cache utilization are cache partitioning and loop tiling. However, state-of-the-art compilers lack efficient cache partitioning and loop tiling methods for two reasons. First, cache partitioning and loop tiling are strongly coupled together, and thus addressing them separately is simply not effective. Second, cache partitioning and loop tiling must be tailored to the target shared cache architecture details and the memory characteristics of the corunning workloads. To the best of our knowledge, this is the first time that a methodology provides (1) a theoretical foundation in the above-mentioned cache management mechanisms and (2) a unified framework to orchestrate these two mechanisms in tandem (not separately). Our approach manages to lower the number of main memory accesses by an order of magnitude keeping at the same time the number of arithmetic/addressing instructions to a minimal level. We motivate this work by showcasing that cache partitioning, loop tiling, data array layouts, shared cache architecture details (i.e., cache size and associativity), and the memory reuse patterns of the executing tasks must be addressed together as one problem, when a (near)-optimal solution is requested. To this end, we present a search space exploration analysis where our proposal is able to offer a vast deduction in the required search space.  © 2018 ACM.",Cache partitioning; data array layouts; loop tiling; memory management; page coloring,Architectural design; Memory architecture; Space research; Cache partitioning; Loop tiling methods; Main-memory access; Multi-core platforms; Optimal solutions; Shared cache managements; Theoretical foundations; Unified framework; Cache memory
Editorial: To Use or Not To? Embedded Systems for Voting,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094091149&doi=10.1145%2f3206342&partnerID=40&md5=6b5c5617d471bd5c78b555cd306db4c3,[No abstract available],,
A Hierarchical Distributed Runtime Resource Management Scheme for NoC-Based Many-Cores,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083708006&doi=10.1145%2f3182173&partnerID=40&md5=a2d9a1449f723b55f2ad9e58f2e25d50,"As technology constantly strengthens its presence in all aspects of human life, computing systems integrate a high number of processing cores, whereas applications become more complex and greedy for computational resources. Inevitably, this high increase in processing elements combined with the unpredictable resource requirements of executed applications at design time impose new design constraints to resource management of many-core systems, turning the distributed functionality into a necessity. In this work, we present a distributed runtime resource management framework for many-core systems utilizing a network-on-chip (NoC) infrastructure. Specifically, we couple the concept of distributed management with parallel applications by assigning different roles to the available computing resources. The presented design is based on the idea of local controllers and managers, whereas an on-chip intercommunication scheme ensures decision distribution. The evaluation of the proposed framework was performed on an Intel Single-Chip Cloud Computer, an actual NoC-based, many-core system. Experimental results show that the proposed scheme manages to allocate resources efficiently at runtime, leading to gains of up to 30% in application execution latency compared to relevant state-of-the-art distributed resource management frameworks.  © 2018 ACM.",distributed runtime resource management design; Many-core systems; parallel application speedup,Natural resources management; Network-on-chip; Resource allocation; Application execution; Computational resources; Distributed management; Distributed resource management; Resource management framework; Resource management schemes; Resource requirements; Single-chip cloud computers; Integrated circuit design
Compact Software Implementation of Public-Key Cryptography on MSP430X,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083756991&doi=10.1145%2f3190855&partnerID=40&md5=ce2de1bcd2410b8a484b141015ca9fd4,"On the low-end embedded processors, the implementations of Elliptic Curve Cryptography (ECC) are considered to be a challenging task due to the limited computation power and storage of the low-end embedded processors. Particularly, the multi-precision multiplication and squaring operations are the most expensive operations for ECC implementations. In order to enhance the performance, many works presented efficient multiplication and squaring routines on the target devices. Recent works show that 128-bit security level ECC is available within a second and this is practically fast enough for IoT services. However, previous approaches missed the other important storage issues (i.e., program size, ROM). Considering that the embedded processors only have a few KB ROM, we need to pay attention to the compact ROM size with reasonable performance. In this article, we present very compact and generic implementations of multiplication and squaring operations on the 16-bit MSP430X processors for the ECC. The implementations utilize the new 32-bit multiplier and advanced multiplication and squaring routines. Since the proposed routines are generic, the arbitrary length of operand is available with high-speed and small code size. With proposed multiplication and squaring routines, we implemented Curve25519 on the MSP430X processors. The scalar multiplication is performed within 6,666,895 clock cycles and 4,054 bytes. Compared with previous works based on the speed-optimized version, our memory-efficient version reduces the code size by 59.8%, sacrificing the execution timing by 20.5%.  © 2018 ACM.",32-bit hardware multiplier; Curve25519; MSP430X; multiplication; Public key cryptography; squaring,Codes (symbols); Public key cryptography; Computation power; Elliptic Curve Cryptography(ECC); Embedded processors; Generic implementation; Memory efficient; Scalar multiplication; Software implementation; Squaring operations; ROM
Schedulability Analysis of Tasks with Corunner-Dependent Execution Times,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061533967&doi=10.1145%2f3203407&partnerID=40&md5=b1928e2ce97d96fa87056e0858368201,"Consider fixed-priority preemptive partitioned scheduling of constrained-deadline sporadic tasks on a multiprocessor. A task generates a sequence of jobs and each job has a deadline that must be met. Assume tasks have Corunner-dependent execution times; i.e., the execution time of a job J depends on the set of jobs that happen to execute (on other processors) at instants when J executes. We present a model that describes Corunner-dependent execution times. For this model, we show that exact schedulability testing is co-NP-hard in the strong sense. Facing this complexity, we present a sufficient schedulability test, which has pseudo-polynomial-time complexity if the number of processors is fixed. We ran experiments with synthetic software benchmarks on a quad-core Intel multicore processor with the Linux/RK operating system and found that for each task, its maximum measured response time was bounded by the upper bound computed by our theory.  © 2018 ACM.",memory contention; multicore processor; Multiprocessor; real-time scheduling,Computer operating systems; NP-hard; Polynomial approximation; Fixed priority preemptive; Multi-core processor; Pseudo-polynomial time complexity; Schedulability analysis; Schedulability test; Schedulability testing; Upper Bound; Job shop scheduling
PTAT: An Efficient and Precise Tool for Tracing and Profiling Detailed TLB Misses,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080773481&doi=10.1145%2f3182174&partnerID=40&md5=820d27f0e73d1b624ceb7d5f6e2b5457,"As the memory access footprints of applications in areas like data analytics increase, the latency overhead of translation lookaside buffer (TLB) misses increases. Thus, the efficiency of TLB becomes increasingly critical for overall system performance. Analyzing TLB miss traces is useful for hardware architecture design and software application optimization. Utilizing cycle-accurate simulators or instrumentation tools is very time-consuming and/or inaccurate for tracing and profiling TLB misses. In this article, we propose an efficient and precise tool to collect and profile last-level TLB misses. This tool utilizes a novel software method called Page Table Access Tracing (PTAT), storing last-level page table entries of certain workload processes into a reserved uncached memory region. Therefore, each last-level TLB miss incurred by user process corresponds to one uncached page table access to main memory, which can be captured and recorded by a hardware memory bus monitor. The detected information is then dumped into offline storage. In this manner, full TLB miss traces are collected and can be analyzed flexibly. Compared to previous software-based methods, this method achieves higher performance. Experiments show that, compared with a state-of-the-art kernel instrumentation method (BadgerTrap), which lacks complete dumping trace function, the speedup is still up to 3.88-fold for memory-intensive benchmarks. Due to the improved efficiency and completeness of tracing, case studies validate that more flexible profiling can be conducted, which is of great significance for TLB performance optimization. The accuracy of PTAT is verified by both dedicated sequence and performance counters.  © 2018 ACM.",efficiency; hardware profiling tool; memory trace collector; precision; TLB misses,Application programs; Data Analytics; Efficiency; Memory architecture; Cycle-accurate simulators; Hardware architecture design; Instrumentation tools; Kernel instrumentation; Performance optimizations; Software applications; Software-based method; Translation lookaside buffer; Buffer storage
Self-Adaptive Filtering Algorithm with PCM-Based Memory Storage System,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074686881&doi=10.1145%2f3190856&partnerID=40&md5=003f1650c907df08760f9ec1f08be986,"This article proposes a new phase change memory-(PCM) based memory storage architecture with associated self-adaptive data filtering for various embedded devices to support energy efficiency as well as high computing power. In this approach, PCM-based memory storage can be used as working memory and mass storage layers simultaneously, and a self-adaptive data filtering module composed of small DRAM dual buffers was designed to improve unfavorable PCM features, such as asymmetric read/write access latencies and limited endurance and enhance spatial/temporal localities. In particular, the self-adaptive data filtering algorithm enhances data reusability by screening potentially high reusable data and predicting adequate lifetime of those data depending on current victim time decision value. We also propose the possibility that a small amount of DRAM buffer is embedded into mobile processors, keeping this as small as possible for cost effectiveness and energy efficiency. Experimental results show that by exploiting a small amount of DRAM space for dual buffers and using the self-adaptive filtering algorithm to manage them, the proposed system can reduce execution time by a factor of 1.9 compared to the unified conventional model with same the DRAM capacity and can be considered comparable to 1.5× DRAM capacity.  © 2018 ACM.",Dual buffers; Embedded memory storage systems; Emerging technologies,Adaptive filters; Cost effectiveness; Dynamic random access storage; Energy efficiency; Green computing; Phase change memory; Reusability; Adaptive filtering algorithms; Computing power; Conventional modeling; Data reusability; Embedded device; Mobile processors; Phase change memory (pcm); Working memory; Adaptive filtering
Runtime precomputation of data-dependent parameters in embedded systems,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079517246&doi=10.1145%2f3191311&partnerID=40&md5=f807347928d7ce5dfb370c3b898154a8,"In many modern embedded systems, the available resources (e.g., CPU clock cycles, memory, and energy) are consumed nonuniformly while the system is under exploitation. Typically, the resource requirements in the system change with different input data that the system process. These data trigger different parts of the embedded software, resulting in different operations executed that require different hardware platform resources to be used. A significant research effort has been dedicated to develop mechanisms for runtime resource management (e.g., branch prediction for pipelined processors, prefetching of data from main memory to cache, and scenario-based design methodologies). All these techniques rely on the availability of information at runtime about upcoming changes in resource requirements. In this article, we propose a method for detecting upcoming resource changes based on preliminary calculation of software variables that have the most dynamic impact on resource requirements in the system. We apply the method on a modified real-life biomedical algorithm with real input data and estimate a 40% energy reduction as compared to static DVFS scheduling. Comparing to dynamic DVFS scheduling, an 18% energy reduction is demonstrated. © 2018 ACM.",Dynamic embedded system; Parameter precomputation; Run-time resource management,Bioinformatics; Cache memory; Embedded systems; Input output programs; Natural resources management; Resource allocation; Scheduling; Bio-medical algorithms; Dynamic embedded systems; Modern embedded systems; Pipelined processor; Pre-computation; Resource management; Resource requirements; Scenario-based design; Information management
Extended Redundant-Digit Instruction Set for Energy-Efficient Processors,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105721930&doi=10.1145%2f3202664&partnerID=40&md5=87244e3069e8efb51a74304cbaf7878a,"The impact of extending the instruction set architecture (ISA) of a conventional binary processor by a set of redundant-digit arithmetic instructions is studied. Selected binary arithmetic instructions within a given code sequence are replaced with appropriate redundant-digit ones. The selection criteria is so enforced to lead to overall reduction of execution energy and energy-delay product (EDP). A special branch and bound algorithm is devised to modify the dataflow graph (DFG) to a new one that takes advantage of the extended redundant-digit instruction set. The DFG is obtained, via an in-house tool, from the intermediate code representation that is normally produced by the utilized compiler. The required redundant-digit arithmetic operations (including a multiplier, a multiply accumulator, and three-to four-operand redundant-digit adders specially designed for this work) have been synthesized on 45nm NanGate technology by a Synopsys Design Compiler. To evaluate the impact of the proposed ISA augmentation on actual code execution, the simulation and evaluation platform of our choice is an MIPS processor whose ISA is extended by the proposed redundant-digit instructions. Several digital signal processing benchmarks are utilized as the source of the baseline MIPS codes, which are converted (via the aforementioned algorithm) to the equivalent mixed binary/redundant-digit codes. Our experiments, as such, show up to 26% energy and 44% EDP savings.  © 2018 ACM.",Energy Improvement; Instruction set architecture extension; Redundant-digit number system; Speed Improvement,Branch and bound method; Data flow analysis; Digital signal processing; Energy efficiency; Graph algorithms; Program compilers; Arithmetic instructions; Arithmetic operations; Branch-and-bound algorithms; Code representation; Energy delay product; Evaluation platforms; Instruction set architecture; Multiply accumulators; Codes (symbols)
OnNetwork+: Network Delay-Aware Management for Mobile Systems,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077224730&doi=10.1145%2f3182171&partnerID=40&md5=01efb765794e6540760900e824c2a5aa,"Network errors such as packet losses consume large amounts of energy. We analyzed the reason for this through measurements using the latest smartphones and full-system simulation. We found that on packet losses the smartphones maintain high frequencies for CPU without doing useful work. To address this problem, we propose a method for reducing the energy consumption by lowering the performance level by exploiting a dynamic voltage and frequency scaling mechanism when long network delays are expected. According to our experiments, our method reduces the total energy consumption of web browsing on two different smartphones by up to 10.0% and 11.5%, respectively.  © 2018 ACM.",CPU frequency scaling; duplicate ACK; mobile systems; Network delay; packet loss,Energy utilization; Packet loss; Smartphones; Voltage scaling; Dynamic voltage and frequency scaling; Full-system simulation; High frequency HF; Mobile systems; Network delays; Network errors; Performance level; Total energy consumption; Dynamic frequency scaling
Dynamic Energy Management of FPGA Accelerators in Embedded Systems,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097350046&doi=10.1145%2f3182172&partnerID=40&md5=efeac87fe24312744a752e65ae9704dd,"In this article, we investigate how to utilise an Field-Programmable Gate Array (FPGA) in an embedded system to save energy. For this purpose, we study the energy efficiency of a hybrid FPGA-CPU device that can switch task execution between hardware and software with a focus on periodic tasks. To increase the applicability of this task switching, we also consider the voltage and frequency scaling (VFS) applied to the FPGA to reduce the system energy consumption. We show that in some cases, if the task's period is higher than a specific level, the FPGA accelerator cannot reduce the energy consumption associated to the task and the software version is the most energy efficient option. We have applied the proposed techniques to a robot map creation algorithm as a case study which shows up to 38% energy reduction compared to the FPGA implementation. Overall, experimental results show up to 48% energy reduction by applying the proposed techniques at runtime on 13 individual tasks.  © 2018 ACM.",dynamic energy management; dynamic power management; dynamic voltage and frequency scaling; FPGA; zynq-SoC,Dynamic frequency scaling; Embedded systems; Energy efficiency; Energy utilization; Dynamic energy managements; Energy efficient; Fpga accelerators; FPGA implementations; Frequency-scaling; Hardware and software; Software versions; System energy consumption; Field programmable gate arrays (FPGA)
Concentration-Resilient mixture preparation with digital microfluidic lab-on-Chip,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042537875&doi=10.1145%2f3157094&partnerID=40&md5=8e5f65b306b63c7bc1748fcb09e1e147,"Sample preparation plays a crucial role in almost all biochemical applications, since a predominant portion of biochemical analysis time is associated with sample collection, transportation, and preparation. Many sample-preparation algorithms are proposed in the literature that are suitable for execution on programmable digital microfluidic (DMF) platforms. In most of the existing DMF-based sample-preparation algorithms, a fixed target ratio is provided as input, and the corresponding mixing tree is generated as output. However, in many biochemical applications, target mixtures with exact component proportions May not be needed. From a biochemical perspective, it May be sufficient to prepare a mixture in which the input reagents May lie within a range of concentration factors. The choice of a particular valid ratio, however, strongly impacts solution-preparation cost and time. To address this problem, we propose a concentration-resilient ratio-selection method from the input ratio space so that the reactant cost is minimized. We propose an integer linear programming–based method that terminates very fast while producing the optimum solution, considering both uniform and weighted cost of reagents. Experimental results reveal that the proposed method can be used conveniently in tandem with several existing sample-preparation algorithms for improving their performance. © 2018 ACM.",Digital microfluidic biochip; Lab-on-a-chip; Sample preparation,Biochips; Costs; Integer programming; Lab-on-a-chip; Microfluidics; Mixtures; Bio-chemical applications; Biochemical analysis; Component proportion; Concentration factors; Digital microfluidic biochips; Integer Linear Programming; Sample preparation; Solution preparations; Digital microfluidics
Guest editorial: Special issue on formal methods and models for system design,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042529211&doi=10.1145%2f3162079&partnerID=40&md5=32b3a7d966c47b76782dd6b7672659e0,[No abstract available],,
Algorithm/architecture co-optimisation technique for automatic data reduction of wireless read-out in high-density electrode arrays,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078264317&doi=10.1145%2f3190854&partnerID=40&md5=ab62f316c357a447824d109cbc051fd9,"High-density electrode arrays used to read out neural activity will soon surpass the limits of the amount of data that can be transferred within reasonable energy budgets. This is true for wired brain implants when the required bandwidth becomes very high, and even more so for untethered brain implants that require wireless transmission of data. We propose an energy-efficient spike data extraction solution for high-density electrode arrays, capable of reducing the data to be transferred by over 85%. We combine temporal and spatial spike data analysis with low implementation complexity, where amplitude thresholds are used to detect spikes and the spatial location of the electrodes is used to extract potentially useful sub-threshold data on neighboring electrodes. We tested our method against a state-of-the-art spike detection algorithm, with prohibitively high implementation complexity, and found that the majority of spikes are extracted reliably. We obtain further improved quality results when ignoring very small spikes below 30% of the voltage thresholds, resulting in 91% accuracy. Our approach uses digital logic and is therefore scalable with an increasing number of electrodes. © 2018 ACM",Data reduction; Digital design; Embedded systems; Low energy; Neural probes,Budget control; Data mining; Electrodes; Embedded systems; Energy efficiency; Neurons; Amplitude threshold; Digital designs; Implementation complexity; Low energy; Neural probes; Optimisation techniques; Temporal and spatial; Wireless transmissions; Data reduction
ShaVe- ICE: Sharing distributed virtualized SPMs in many-Core embedded systems,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042545165&doi=10.1145%2f3157667&partnerID=40&md5=095b77f3a92d94426c1c690440704371,"Traditional approaches for managing software-programmable memories (SPMs) do not support sharing of distributed on-chip memory resources and, consequently, miss the opportunity to better utilize those memory resources. Managing on-chip memory resources in many-core embedded systems with distributed SPMs requires runtime support to share memory resources between various threads with different memory demands running concurrently. Runtime SPM managers cannot rely on prior knowledge about the dynamically changing mix of threads that will execute and therefore should be designed in a way that enables SPM allocations 7 for any unpredictable mix of threads contending for on-chip memory space. This article proposes ShaVe- ICE, an operating-system-level solution, along with hardware support, to virtualize and ultimately share SPM resources across a many-core embedded system to reduce the average memory latency. We present a number of simple allocation policies to improve performance and energy. Experimental results show that sharing SPMs could reduce the average execution time of the workload up to 19.5% and reduce the dynamic energy consumed in the memory subsystem up to 14%. © 2018 ACM.",Many-core architectures; Memory management; Scratchpad memory; SPM; Virtualization,Embedded systems; Memory architecture; Virtualization; Allocation policies; Average Execution Time; Improve performance; Many-core architecture; Memory management; Programmable memory; Scratch pad memory; Traditional approaches; Computer architecture
Enabling contactless detection of moving humans with dynamic speeds using CSI,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042538814&doi=10.1145%2f3157677&partnerID=40&md5=3c0f15a37675928fb59cb928844080fb,"Device-free passive detection is an emerging technology to detect whether there exist any moving entities in the areas of interest without attaching any device to them. It is an essential primitive for a broad range of applications including intrusion detection for safety precautions, patient monitoring in hospitals, child and elder care at home, and so forth. Despite the prevalent signal feature Received Signal Strength (RSS), most robust and reliable solutions resort to a finer-grained channel descriptor at the physical layer, e.g., the Channel State Information (CSI) in the 802.11n standard. Among a large body of emerging techniques, however, few of them have explored the full potential of CSI for human detection. Moreover, space diversity supported by nowadays popular multiantenna systems are not investigated to a comparable extent as frequency diversity. In this article, we propose a novel scheme for device-free PAssive Detection of moving humans with dynamic Speed (PADS). Both full information (amplitude and phase) of CSI and space diversity across multiantennas in MIMO systems are exploited to extract and shape sensitive metrics for accuracy and robust target detection. We prototype PADS on commercial WiFi devices, and experiment results in different scenarios demonstrate that PADS achieves great performance improvement in spite of dynamic human movements. © 2018 ACM.",Channel state information; Motion detection; Noninvasive; Phase difference,Intrusion detection; MIMO systems; Network layers; Patient monitoring; Contactless detection; Emerging technologies; Motion detection; Multi-antenna systems; Noninvasive; Performance improvements; Phase difference; Received signal strength; Channel state information
Efficient and reliable error detection architectures of hash-Counter-Hash tweakable enciphering schemes,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042528172&doi=10.1145%2f3159173&partnerID=40&md5=3c754871d59ba7fea1f8b62ac43ddaa9,"Through pseudorandom permutation, tweakable enciphering schemes (TES) constitute block cipher modes of operation which perform length-preserving computations. The state-of-the-art research has focused on different aspects of TES, including implementations on hardware [field-programmable gate array (FPGA)/ application-specific integrated circuit (ASIC)] and software (hard/soft-core microcontrollers) platforms, algorithmic security, and applicability to sensitive, security-constrained usage models. In this article, we propose efficient approaches for protecting such schemes against natural and malicious faults. Specifically, noting that intelligent attackers do not merely get confined to injecting multiple faults, one major benchmark for the proposed schemes is evaluation toward biased and burst fault models. We evaluate a variant of TES, i.e., the Hash-Counter-Hash scheme, which involves polynomial hashing as other variants are either similar or do not constitute finite field multiplication which, by far, is the most involved operation in TES. In addition, we benchmark the overhead and performance degradation on the ASIC platform. The results of our error injection simulations and ASIC implementations show the suitability of the proposed approaches for a wide range of applications including deeply embedded systems. © 2018 ACM.",Application-specific integrated circuit (ASIC); Low complexity; Reliability; Tweakable enciphering schemes,Application programs; Application specific integrated circuits; Embedded systems; Field programmable gate arrays (FPGA); Reliability; Block cipher modes; Finite field multiplication; Low complexity; Malicious faults; Performance degradation; Pseudorandom permutation; Security constrained; Tweakable enciphering schemes; Benchmarking
Memory-Constrained vectorization and scheduling of dataflow graphs for hybrid CPU-GPU platforms,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042527088&doi=10.1145%2f3157669&partnerID=40&md5=e9c4c2afa8f30f3434efa1ce44b88659,"The increasing use of heterogeneous embedded systems with multi-core CPUs and Graphics Processing Units (GPUs) presents important challenges in effectively exploiting pipeline, task, and data-level parallelism to meet throughput requirements of digital signal processing applications. Moreover, in the presence of system-level memory constraints, hand optimization of code to satisfy these requirements is inefficient and error prone and can therefore, greatly slow down development time or result in highly underutilized processing resources. In this article, we present vectorization and scheduling methods to effectively exploit multiple forms of parallelism for throughput optimization on hybrid CPU-GPU platforms, while conforming to system-level memory constraints. The methods operate on synchronous dataflow representations, which are widely used in the design of embedded systems for signal and information processing. We show that our novel methods can significantly improve system throughput compared to previous vectorization and scheduling approaches under the same memory constraints. In addition, we present a practical case-study of applying our methods to significantly improve the throughput of an orthogonal frequency division multiplexing receiver system for wireless communications. © 2018 ACM.",Dataflow models; Design optimization; Heterogeneous computing; Signal processing systems; Software synthesis,Computer graphics; Data flow analysis; Digital signal processing; Embedded systems; Graphics processing unit; Program processors; Scheduling; Signal processing; Throughput; Wireless telecommunication systems; Dataflow model; Design optimization; Heterogeneous computing; Signal processing systems; Software synthesis; Pipeline processing systems
A comparative study of predictable DRAM controllers,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042549652&doi=10.1145%2f3158208&partnerID=40&md5=e0e2db75aca94901b9a89e7dc64ef0aa,"Recently, the research community has introduced several predictable dynamic random-access memory (DRAM) controller designs that provide improved worst-case timing guarantees for real-time embedded systems. The proposed controllers significantly differ in terms of arbitration, configuration, and simulation environment, making it difficult to assess the contribution of each approach. To bridge this gap, this article provides the first comprehensive evaluation of state-of-the-art predictable DRAM controllers. We propose a categorization of available controllers, and introduce an analytical performance model based on worst-case latency. We then conduct an extensive evaluation for all state-of-the-art controllers based on a common simulation platform, and discuss findings and recommendations. © 2018 ACM.",Multicore; Real-time; SDRAM controller; WCET,Controllers; Embedded systems; Random access storage; Real time systems; Analytical performance model; Common simulation platform; Dynamic random access memory; Multi core; Real time; Real-time embedded systems; SDRAM controllers; WCET; Dynamic random access storage
Application deployment strategies for spatial isolation on many-Core accelerators,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042531694&doi=10.1145%2f3168383&partnerID=40&md5=646aa60e994d3df3a74633e77aa108c1,"Current cache Side-Channel Attacks (SCAs) countermeasures have not been designed for many-core architectures and need to be revisited in order to be practical for these new technologies. Spatial isolation of resources for sensitive applications has been proposed taking advantage of the large number of resources offered by these architectures. This solution avoids cache sharing with sensitive processes. Consequently, their cache activity cannot be monitored and cache SCAs cannot be performed. This work focuses on the implementation of this technique in order to minimize the induced performance overhead. Different strategies for the management of isolated secure zones are implemented and compared. © 2018 ACM.",Cache-based SCAs; Many-core accelerator,Side channel attack; Application deployment; Cache sharing; Cache-based SCAs; Many-core accelerators; Many-core architecture; New technologies; Sensitive application; Spatial isolation; Computer architecture
Improving SIMD parallelism via dynamic binary translation,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042553247&doi=10.1145%2f3173456&partnerID=40&md5=450974ecce875339ae57a3876a022c22,"Recent trends in SIMD architecture have tended toward longer vector lengths, and more enhanced SIMD features have been introduced in newer vector instruction sets. However, legacy or proprietary applications compiled with short- SIMD ISA cannot benefit from the long- SIMD architecture that supports improved parallelism and enhanced vector primitives, resulting in only a small fraction of potential peak performance. This article presents a dynamic binary translation technique that enables short- SIMD binaries to exploit benefits of new SIMD architectures by rewriting short- SIMD loop code. We propose a general approach that translates loops consisting of short- SIMD instructions to machine-independent IR, conducts SIMD loop transformation/optimization at this IR level, and finally translates to long- SIMD instructions. Two solutions are presented to enforce SIMD load/store alignment, one for the problem caused by the binary translator’s internal translation condition and one general approach using dynamic loop peeling optimization. Benchmark results show that average speedups of 1.51× and 2.48× are achieved for an ARM NEON to x86 AVX2 and x86 AVX-512 loop transformation, respectively. © 2018 ACM.",Compiler annotation; Dynamic binary translation; Dynamic loop peeling; SIMD; Vectorization,Architecture; Bins; Compiler annotation; Dynamic binary translation; Dynamic loops; SIMD; Vectorization; Memory architecture
A hardware pipeline with high energy and resource efficiency for FMM acceleration,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042552954&doi=10.1145%2f3157670&partnerID=40&md5=79a7255fa2e24c7db999b47ab6c7d7d3,"The fast multipole method (FMM) is a promising mathematical technique that accelerates the calculation of long-ranged forces in the large-sized n-body problem. Existing implementations of the FMM on general-purpose processors are energy and resource inefficient. To mitigate these issues, we propose a hardware pipeline that accelerates three key FMM steps. The pipeline improves energy efficiency by exploiting fine-granularity parallelism of the FMM. We reuse the pipeline for different FMM steps to reduce resource usage by 66%. Compared to the state-of-the-art implementations on CPUs and GPUs, our implementation requires 15% less energy and delivers 2.61 times more floating-point operations. © 2018 ACM.",Energy efficiency; Fast multipole method (FMM); Field programmable gate arrays (FPGAs); Pipeline; Resource efficiency,Digital arithmetic; Embedded systems; Field programmable gate arrays (FPGA); General purpose computers; Hardware; Pipelines; Program processors; Fast multipole method; Field programmable gate array (FPGAs); Fine granularity; Floating point operations; General purpose processors; N body problem; Resource efficiencies; State of the art; Energy efficiency
Attitude fusion of inertial and magnetic sensor under different magnetic filed distortions,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042540903&doi=10.1145%2f3157668&partnerID=40&md5=fdf2084707001109670ead6806f2344d,"By virtue of gravity measurement from a handheld inertial measurement unit (IMU) sensor, current indoor attitude estimation algorithms can provide accurate roll/pitch dimension angles. Acquisition of precise heading is limited by the absence of accurate magnetic reference. Consequently, initial stage magnetometer calibration is deployed to alleviate this bottleneck in attitude fusion. However, available algorithms tackle magnetic distortion based on time-invariant surroundings, casting the post-calibration magnetic data into unchanged ellipsoid centered in the calibration place. Consequently, inaccurate fusion results are formulated in a more common case of random walk in time-varying magnetic indoor environment. This article proposes a new fusion algorithm from various kinds of IMU sensors, namely gyroscope, accelerometer, and magnetometer. Compared to state-of-the-art attitude fusion approaches, this article addresses the indoor time-varying magnetic perturbation problem in a geometric view. We propose an extend Kalman filter–based algorithm based on this detailed geometric model to eliminate the position-dependent effect of a compass sensor. Experimental data demonstrate that, under different indoor magnetic distortion environments, our proposed attitude fusion algorithm has the maximum angle error of 2.02◦, outperforming 7.17◦ of a gradient-declining-based 8 algorithm. Additionally, this attitude fusion result is constructed in a low-cost handheld arduino core–based IMU device, which can be widely applied to embedded systems. © 2018 ACM.",Attitude fusion; Control laws; Embedded system; Sensor data fusion,Calibration; Data fusion; Embedded systems; Magnetism; Magnetometers; Units of measurement; Attitude estimation; Control laws; Extend Kalman filter; Inertial Measurement Unit (IMU); Magnetic distortions; Magnetic perturbation; Magnetometer calibration; Position dependents; Sensor data fusion
On the limitations of analyzing worst-Case dynamic energy of processing,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042533850&doi=10.1145%2f3173042&partnerID=40&md5=f738bf846876a991607c364cc3d52ec0,"This article examines dynamic energy consumption caused by data during software execution on deeply embedded microprocessors, which can be significant on some devices. In worst-case energy consumption analysis, energy models are used to find the most costly execution path. Taking each instruction’s worst-case energy produces a safe but overly pessimistic upper bound. Algorithms for safe and tight bounds would be desirable. We show that finding exact worst-case energy is NP-hard, and that tight bounds cannot be approximated with guaranteed safety. We conclude that any energy model targeting tightness must either sacrifice safety or accept overapproximation proportional to data-dependent energy. © 2018 ACM.",Complexity; Energy transparency; Worst case energy consumption,Hardware; Software engineering; Complexity; Data dependent; Dynamic energy; Dynamic energy consumption; Embedded microprocessors; Energy consumption analysis; Execution paths; Software execution; Energy utilization
Loop-Oriented pointer analysis for automatic SIMD vectorization,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042525150&doi=10.1145%2f3168364&partnerID=40&md5=2c034e37ac4238f3f43dc9761bf3468f,"Compiler-based vectorization represents a promising solution to automatically generate code that makes efficient use of modern CPUs with SIMD extensions. Two main auto-vectorization techniques, superword-level parallelism vectorization (SLP) and loop-level vectorization (LLV), require precise dependence analysis on arrays and structs to vectorize isomorphic scalar instructions (in the case of SLP) and reduce dynamic dependence checks at runtime (in the case of LLV). The alias analyses used in modern vectorizing compilers are either intra-procedural (without tracking inter-procedural data-flows) or inter-procedural (by using field-sensitive models, which are too imprecise in handling arrays and structs). This article proposes an inter-procedural Loop-oriented Pointer Analysis for C, called Lpa, for analyzing arrays and structs to support aggressive SLP and LLV optimizations effectively. Unlike field-insensitive solutions that pre-allocate objects for each memory allocation site, our approach uses a lazy memory model to generate access-based location sets based on how structs and arrays are accessed. Lpa can precisely analyze arrays and nested aggregate structures to enable SIMD optimizations for large programs. By separating the location set generation as an independent concern from the rest of the pointer analysis, Lpa is designed so that existing points-to resolution algorithms (e.g., flow-insensitive and flow-sensitive pointer analysis) can be reused easily. We have implemented Lpa fully in the LLVM compiler infrastructure (version 3.8.0). We evaluate Lpa by considering SLP and LLV, the two classic vectorization techniques, on a set of 20 C and Fortran CPU2000/2006 benchmarks. For SLP, Lpa outperforms LLVM’s BasicAA and ScevAA by discovering 139 and 273 more vectorizable basic blocks, respectively, resulting in the best speedup of 2.95% for 173.applu. For LLV, LLVM introduces totally 551 and 652 static bound checks under BasicAA and ScevAA, respectively. In contrast, Lpa has reduced these static checks to 220, with an average of 15.7 checks per benchmark, resulting in the best speedup of 7.23% for 177.mesa. © 2018 ACM.",Compiler optimisation; Loop-oriented; Pointer analysis; SIMD,Program compilers; Program processors; Aggregate structures; Loop-oriented; Optimisations; Pointer analysis; Resolution algorithms; SIMD; Superword Level Parallelism; Vectorization techniques; Data flow analysis
Trinity: Enabling self-Sustaining WSNs indoors with energy-Free sensing and networking,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042537084&doi=10.1145%2f3173039&partnerID=40&md5=812fe3803511e132b27afaab097a7d39,"Whereas a lot of efforts have been put on energy conservation in wireless sensor networks (WSNs), the limited lifetime of these systems still hampers their practical deployments. This situation is further exacerbated indoors, as conventional energy harvesting (e.g., solar) May not always work. To enable long-lived indoor sensing, we report in this article a self-sustaining sensing system that draws energy from indoor environments, adapts its duty-cycle to the harvested energy, and pays back the environment by enhancing the awareness of the indoor microclimate through an “energy-free” sensing. First of all, given the pervasive operation of heating, ventilation, and air conditioning (HVAC) systems indoors, our system harvests energy from airflow introduced by the HVAC systems to power each sensor node. Secondly, as the harvested power is tiny, an extremely low but synchronous duty-cycle has to be applied whereas the system gets no energy surplus to support existing synchronization schemes. So, we design two complementary synchronization schemes that cost virtually no energy. Finally, we exploit the feature of our harvester to sense the airflow speed in an energy-free manner. To our knowledge, this is the first indoor wireless sensing system that encapsulates energy harvesting, network operating, and sensing all together. © 2018 ACM.",Duty-cycle; Indoor energy harvesting; Sustainable sensor networks; Synchronization,Air conditioning; Climate control; Energy harvesting; Sensor nodes; Synchronization; Airflow speed; Duty-cycle; Indoor environment; Indoor sensing; Sensing systems; Synchronization scheme; Wireless sensing; Wireless sensor network (WSNs); Wireless sensor networks
Energy cooperation in battery-Free wireless communications with radio frequency energy harvesting,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042521522&doi=10.1145%2f3141249&partnerID=40&md5=0ea98177c4d842013487545135c1d5d1,"Radio frequency (RF) energy harvesting techniques are becoming a potential method to power battery-free wireless networks. In RF energy harvesting communications, energy cooperation enables shaping and optimization of the energy arrivals at the energy-receiving node to improve the overall system performance. In this article, we propose an energy cooperation scheme that enables energy cooperation in battery-free wireless networks with RF harvesting. We first study the battery-free wireless network with RF energy harvesting and then state the problem that optimizing the system performance with limited harvesting energy through new energy cooperation protocol. Finally, from the extensive simulation results, our energy cooperation protocol performs better than the original battery-free wireless network solution. © 2018 ACM.",Batter-free networks; Energy cooperation; RF energy harvesting,Electric batteries; Internet protocols; Radio communication; Radio waves; Wireless networks; Wireless telecommunication systems; Energy cooperation; Extensive simulations; Harvesting energies; Potential methods; Power batteries; Radio-frequency energy harvesting; RF energy harvesting; Wireless communications; Energy harvesting
Compact implementations of ARX-based block ciphers on IoT processors,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041816612&doi=10.1145%2f3173455&partnerID=40&md5=d810d7e7de7a0ee0f55ba8199a17ad90,"In this article, we present implementations for Addition, Rotation, and eXclusive-or (ARX)-based block ciphers, including LEA and HIGHT, on IoT devices, including 8-bit AVR, 16-bit MSP, 32-bit ARM, and 32-bit ARM-NEON processors. We optimized 32-/8-bitwise ARX operations for LEA and HIGHT block ciphers by considering variations in word size, the number of general purpose registers, and the instruction set of the target IoT devices. Finally, we achieved the most compact implementations of LEA and HIGHT block ciphers. The implementations were fairly evaluated through the Fair Evaluation of Lightweight Cryptographic Systems framework, and implementations won the competitions in the first and the second rounds. © 2018 ACM 1539-9087/2018/02-ART60 $15.00.",ARM; AVR; Block cipher; FELICS; HIGHT; Internet of Things; LEA; MSP; Software implementation,ARM processors; Cryptography; Lyapunov methods; Security of data; Block ciphers; Cryptographic systems; Exclusive-OR; FELICS; General purpose registers; HIGHT; Instruction set; Software implementation; Internet of things
Editorial: Trust and security must become a primary design concern in embedded computing,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042539888&doi=10.1145%2f3173385&partnerID=40&md5=1b4d127f554a33d3a4af8f390697a104,[No abstract available],,
"Efficient, long-term logging of rich data sensors using transient sensor nodes",2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033241720&doi=10.1145%2f3047499&partnerID=40&md5=82e93b695178fbf8026b446026cce31b,"While energy harvesting is generally seen to be the key to power cyber-physical systems in a low-cost, long-term, efficient manner, it has generally required large energy storage devices to mitigate the effects of the source's variability. The emerging class of transiently powered systems embrace this variability by performing computation in proportion to the energy harvested, thereby minimizing the obtrusive and expensive storage element. By using an efficient Energy Management Unit (EMU), small bursts of energy can be buffered in an optimally sized capacitor and used to supply generic loads, even when the average harvested power is only a fraction of that required for sustained system operation. Dynamic Energy Burst Scaling (DEBS) can be used by the load to dynamically configure the EMU to supply small bursts of energy at its optimal power point, independent from the harvester's operating point. Parameters like the maximum burst size, the solar panel's area, as well as the use of energy-efficient Non-Volatile Memory Hierarchy (NVMH) can have a significant impact on the transient system's characteristics such as the wake-up time and the amount of work that can be done per unit of energy. Experimental data from a solar-powered, long-term autonomous image acquisition application show that, regardless of its configuration, the EMU can supply energy bursts to a 43.4mW load with efficiencies of up to 79.7% and can work with input power levels as low as 140μW. When the EMU is configured to use DEBS and NVMH, the total energy cost of acquiring, processing and storing an image can be reduced by 77.8%, at the price of increasing the energy buffer size by 65%. © 2017 ACM.",Energy efficiency; Energy harvesting; Low power design,Costs; Digital storage; Electric power supplies to apparatus; Embedded systems; Energy harvesting; Sensor nodes; Solar energy; Energy efficient; Low-power design; Non-volatile memory; Operating points; Optimal power points; Storage elements; Sustained systems; Transient systems; Energy efficiency
Mining timed regular specifications from system traces,2018,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041430761&doi=10.1145%2f3147660&partnerID=40&md5=098e85d24140e2bbe8628f2abd81457b,"Temporal properties define the order of occurrence and timing constraints on event occurrence. Such specifications are important for safety-critical real-time systems.We propose a framework for automatically mining temporal properties that are in the form of timed regular expressions (TREs) from system traces. Using an abstract structure of the property, the framework constructs a finite state machine to serve as an acceptor.We analytically derive speedup for the fragment and confirm the speedup using empirical validation with synthetic traces. The framework is evaluated on industrial-strength safety-critical real-time applications using traces with more than 1 million entries. © 2018 ACM.",Realtime systems; Specification mining; Timed automata; Timed regular expressions,Accident prevention; Pattern matching; Real time systems; Safety engineering; Abstract structures; Empirical validation; Industrial strength; Real-time application; Specification mining; Timed Automata; Timed regular expression; Timing constraints; Specifications
Simulation-driven reachability using matrix measures,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041434707&doi=10.1145%2f3126685&partnerID=40&md5=747b16fbaa94a7007d74bca4264f9733,"Simulation-driven verification can provide formal safety guarantees for otherwise intractable nonlinear and hybrid system models. A key step in simulation-driven algorithms is to compute the reach set overapproximations from a set of initial states through numerical simulations and sensitivity analysis. This article addresses this problem by providing algorithms for computing discrepancy functions as the upper bound on the sensitivity, that is, the rate at which trajectories starting from neighboring states converge or diverge. The algorithms rely on computing local bounds on matrix measures as the exponential change rate of the discrepancy function. We present two techniques to compute the matrix measures under different norms: regular Euclidean norm or Euclidean norm under coordinate transformation, such that the exponential rate of the discrepancy function, and therefore, the conservativeness of the overapproximation, is locally minimized. The proposed algorithms enable automatic reach set computations of general nonlinear systems and have been successfully used on several challenging benchmark models. All proposed algorithms for computing discrepancy functions give soundness and relative completeness of the overall simulation-driven safety-bounded verification algorithm. We present a series of experiments to illustrate the accuracy and performance of the algorithms. © 2017 ACM.",Discrepancy function; Embedded System; Matrix measures; Nonlinear System; Reachability,Embedded systems; Hybrid systems; Linear transformations; Nonlinear systems; Sensitivity analysis; Bounded verifications; Co-ordinate transformation; Discrepancy functions; General nonlinear systems; Hybrid system models; Matrix measures; Reachability; Soundness and relative completeness; Matrix algebra
Formal requirement debugging for testing and verification of cyber-physical systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041434150&doi=10.1145%2f3147451&partnerID=40&md5=9961626cef433cbfd22ee614092f3967,A framework for the elicitation and debugging of formal specifications for Cyber-Physical Systems is presented. The elicitation of specifications is handled through a graphical interface. Two debugging algorithms are presented. The first checks for erroneous or incomplete temporal logic specifications without considering the system. The second can be utilized for the analysis of reactive requirements with respect to system test traces. The specification debugging framework is applied on a number of formal specifications collected through a user study. The user study establishes that requirement errors are common and that the debugging framework can resolve many insidious specification errors. © 2017 ACM.,CPS; LTL; MITL; SAT; SMT; STL,Cyber Physical System; Embedded systems; Formal specification; Specifications; Surface mount technology; Temporal logic; Debugging algorithms; Graphical interface; MITL; Requirement errors; System test; Temporal logic specifications; User study; Program debugging
Fault injection for test-driven development of robust SoC firmware,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041450862&doi=10.1145%2f3092943&partnerID=40&md5=5e8134a4ed458bc4b5e8428cf5e79152,"Robustness against errors in hardware must be considered from the very beginning of safety-critical systemon- chip firmware design. Therefore, we present fault injection for test-driven development (TDD) of robust firmware. As TDD is based on instant feedback to the designer, fault injection must execute within few minutes. In contrast to state-of-the-art approaches, we avoid long simulation scenarios and runtimes by injecting faults at the unit level and utilizing host-compiled simulation. Further, three static bit-level analyses of firmware source code and hardware specification reduce the fault set significantly. This accelerates fault injection by several orders of magnitude and enables robustness-aware TDD. © 2017 ACM.",Fault injection; Fault set reduction; Firmware; Host-compiled simulation; Robustness; Static code analysis; System-on-chip; Test-driven development,Computer programming; Firmware; Hardware; Programmable logic controllers; Robustness (control systems); Safety engineering; Software testing; Fault injection; Fault set; Host-compiled simulations; Static code analysis; Test driven development; System-on-chip
Model and program repair via sat solving,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041434813&doi=10.1145%2f3147426&partnerID=40&md5=cc1151fa67bc70f843d33f45aa3ee744,"We consider the subtractive model repair problem: given a finite Kripke structure M and a CTL formula η, determine if M contains a substructure M' that satisfies η. Thus, M can be ""repaired"" to satisfy η by deleting some transitions and states. We map an instance M, η of model repair to a Boolean formula repair (M, η) such that M, η has a solution iff repair (M, η) is satisfiable. Furthermore, a satisfying assignment determines which states and transitions must be removed from M to yield a model M of η. Thus, we can use any SAT solver to repair Kripke structures. Using a complete SAT solver yields a complete algorithm: it always finds a repair if one exists. We also show that CTL model repair is NP-complete. We extend the basic repair method in three directions: (1) the use of abstraction mappings, that is, repair a structure abstracted from M and then concretize the resulting repair to obtain a repair of M, (2) repair concurrent Kripke structures and concurrent programs: we use the pairwise method of Attie and Emerson to represent and repair the behavior of a concurrent program, as a set of ""concurrent Kripke structures"", with only a quadratic increase in the size of the repair formula, and (3) repair hierarchical Kripke structures: we use a CTL formula to summarize the behavior of each ""box,"" and CTL deduction to relate the box formula with the overall specification. © 2017 ACM.",Model checking; Model repair; Program repair; Temporal logic,Abstracting; Boolean algebra; Formal logic; Model checking; Temporal logic; Boolean formulae; Concurrent program; Kripke structure; Model repair; Repair methods; SAT solvers; Satisfying assignments; States and transitions; Repair
Analysis and design of adders for approximate computing,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041444978&doi=10.1145%2f3131274&partnerID=40&md5=34adf7a1c8959e180b20805cdd9fb8bf,"The concept of approximate computing, that is, to sacrifice computation quality for computation efforts, has recently emerged as a promising design approach. Over the past decade, several research works have explored approximate computing at both the software level and hardware level of abstraction with encouraging results. At the hardware level of abstraction, adders (being the fundamental and most widely used data operators in digital systems) have attracted a significant attention for approximation. In this article, we first explain briefly the need/significance of approximate adders. We then propose four Approximate Full Adders (AFAs) for high-performance energy-efficient approximate computing. The key design objective behind the proposed AFAs is to curtail the length of carry propagation subjected to minimal error rate. Next, we exploit one of the proposed AFAs (optimal one) to construct an N-bit approximate adder that hereinafter is referred as ""ApproxADD."" An emergent property of ApproxADD is that carries do not propagate in it, and, consequently, it provides bit-width-aware constant delay (O(1)). ApproxADD also provides improvement in dynamic power consumption by 46.31%and in area by 28.57% w.r.t. Ripple Carry Adder (RCA), which exhibits the lowest power and area. Although ApproxADD provides a significant improvement in delay, power, and area, it may not be preferred for some of the error-resilient applications because its: (i) Error Distance (ED) is too high; and (ii) Error Rate (ER) increases rapidly with bit-width (N). To improve ED and ER, we exploit the concept of carry-lifetime and Error Detection and Correction logic, respectively. In this way, we introduce two more (improved) versions of ApproxADD-ApproxADDv1 and ApproxADD.We call these as ApproxADDv1 and ApproxADDv2 with existing approximate adders based on conventional design metrics and approximate computing design metrics. Furthermore, to inspect effectiveness of the proposed approach in real-life applications, we demonstrate image compression and decompression by replacing the conventional addition operations in Discrete Cosine Transform (DCT) and Inverse Discrete Cosine Transform (IDCT) modules with ApproxADDv2. © 2017 ACM.",Approximate adders; Approximate computing; Delay-power-area-accuracy trade-off; Error-resilient applications,Abstracting; Bit error rate; Carry logic; Computation theory; Discrete cosine transforms; Economic and social effects; Energy efficiency; Errors; Hardware; Image compression; Inverse transforms; Approximate computing; Discrete Cosine Transform(DCT); Dynamic power consumption; Error detection and correction; Error-resilient; Inverse discrete cosine transforms; Real-life applications; Trade off; Adders
Time and sequence integrated runtime anomaly detection for embedded systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041431136&doi=10.1145%2f3122785&partnerID=40&md5=9d68387809a07a640fa7a47fd2c45f79,"Network-connected embedded systems grow on a large scale as a critical part of Internet of Things, and these systems are under the risk of increasing malware. Anomaly-based detection methods can detect malware in embedded systems effectively and provide the advantage of detecting zero-day exploits relative to signature-based detection methods, but existing approaches incur significant performance overheads and are susceptible to mimicry attacks. In this article, we present a formal runtime security model that defines the normal system behavior including execution sequence and execution timing. The anomaly detection method in this article utilizes on-chip hardware to non-intrusively monitor system execution through trace port of the processor and detect malicious activity at runtime. We further analyze the properties of the timing distribution for control flow events, and select subset of monitoring targets by three selection metrics to meet hardware constraint. The designed detection method is evaluated by a network-connected pacemaker benchmark prototyped in FPGA and simulated in SystemC, with several mimicry attacks implemented at different levels. The resulting detection rate and false positive rate considering constraints on the number of monitored events supported in the on-chip hardware demonstrate good performance of our approach. © 2017 ACM.",Anomaly detection; Embedded system security; Medical device security; Software security; Timing based detection,Biomedical equipment; Computer crime; Hardware; Malware; Monitoring; Anomaly based detection; Anomaly detection; Anomaly detection methods; Hardware constraints; Malicious activities; Medical Devices; Signature based detections; Software security; Embedded systems
Symbolic multi-level loop mapping of loop programs for massively parallel processor arrays,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041449462&doi=10.1145%2f3092952&partnerID=40&md5=78a1054b659b98f21753155ac60e31c7,"Today's MPSoCs (multiprocessor systems-on-chip) have brought up massively parallel processor array accelerators that may achieve a high computational efficiency by exploiting multiple levels of parallelism and different memory hierarchies. Such parallel processor arrays are perfect targets, particularly for the acceleration of nested loop programs due to their regular and massively parallel nature. However, existing loop parallelization techniques are often unable to exploit multiple levels of parallelism and are either I/O or memory bounded. Furthermore, if the number of available processing elements becomes only known at runtime-as in adaptive systems-static approaches fail. In this article, we solve some of these problems by proposing a hybrid compile/runtime multi-level symbolic parallelization technique that is able to: (a) exploit multiple levels of parallelism as well as (b) different memory hierarchies, and (c) to match the I/O or memory capabilities of the target architecture for scenarios where the number of available processing elements is only known at runtime. Our proposed technique consists of two compile-time transformations: (a) symbolic hierarchical tiling followed by (b) symbolic multi-level scheduling. The tiling levels scheduled in parallel exploit different levels of parallelism, whereas the sequential one, different memory hierarchies. Furthermore, by tuning the size of the tiles on the individual levels, a tradeoff between the necessary I/O-bandwidth and memory is possible, which facilitates obeying resource constraints. The resulting schedules are symbolic with respect to the problem size and tile sizes. Thus, the number of processing elements to map onto does not need to be known at compile time. At runtime, when the number of available processors becomes known, a simple prologue chooses a feasible schedule with respect to I/O and memory constraints that is latency-optimal for the chosen tile size. In summary, our approach determines the set of feasible, latency-optimal symbolic loop schedule candidates at compile time, from which one is dynamically selected at runtime. This approach exploits multiple levels of parallelism, is independent of the problem size of the loop nest, and thereby avoids any expensive re-compilation at runtime. This is particularly important for low cost and memory-scarce embedded MPSoC platforms that may not afford to host a just-in-time compiler. © 2017 ACM.",Loop programs; Mapping; Processor arrays; Symbolic parallelization,C (programming language); Computational efficiency; Computer architecture; Fault tolerance; Mapping; Multiprocessing systems; Structured programming; System-on-chip; Just in time compilers; Massively parallel processors; Multiple levels of parallelisms; Multiprocessor systems on chips; Parallelization techniques; Parallelizations; Processor array; Target architectures; Parallel processing systems
SCEst: Sequentially Constructive Esterel,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041442391&doi=10.1145%2f3063129&partnerID=40&md5=1c779c7e3caaf628f07ebd767a8966d9,"The synchronous language Esterel provides determinate concurrency for reactive systems. Determinacy is ensured by the signal coherence rule, which demands that signals have a stable value throughout one reaction cycle. This is natural for the original application domains of Esterel, such as controller design and hardware development; however, it is unnecessarily restrictive for software development. Sequentially Constructive Esterel (SCEst) overcomes this restriction by allowing values to change instantaneously, as long as determinacy is still guaranteed, adopting the recently proposed Sequentially Constructive model of computation. SCEst is grounded in the minimal Sequentially Constructive Language (scl), which also provides a novel semantic definition and compilation approach for Esterel. © 2017 ACM.",Esterel; Sequential constructiveness; Synchronous languages,Application programs; Semantics; Software design; Constructive model; Controller designs; Esterel; Hardware development; Reaction cycles; Sequential constructiveness; Signal coherence; Synchronous languages; Esters
Exploiting approximate MLC-PCM in low-power embedded systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041447044&doi=10.1145%2f3105926&partnerID=40&md5=484b76b42356ca484b7cbf6d3902aed0,"Multi-level cell phase change memory (MLC-PCM), because of its very low leakage power and high density, is promising for embedded systems. Furthermore, for applications with inherent low sensitivity to errors, approximate write operations can be exploited in MLC-PCM to improve endurance and performance. However, data that reside in the approximate MLC-PCM for a rather long time without refreshing are prone to soft errors due to resistance drift phenomenon, while even for an application with inherent low sensitivity to errors, a high soft error rate can degrade its Quality of Result (QoR). The architecture-level approaches to decrease the drift effect incur considerable power overhead (about 100%), which is a prominent issue in embedded systems, and are dependent on the number of logic levels stored in the PCM cell (e.g., most of them are designed for 4LC-PCM). This article, taking a different approach, proposes a drift-aware frequency and voltage management to alleviate the drift-based soft-error rate. To this end, first we characterize the application data based on the degree of being exposed to the drift to identify the drift-prone application data. Then we assign the execution frequency and voltage to different regions of the application considering the drift. This frequency assignment speeds up the application regions wherein the drift-prone data are accessed to shorten the lifetime of the drift-prone data, thereby decreasing the soft error rate. An integer linear programming model implements our proposed Dynamic Voltage Frequency Scaling (DVFS). Also, the proposed approach is independent of the number of levels of PCM cells and can be applied to any MLC-PCM system. To evaluate the approach, the approximate MLC-PCM is simulated using empirical models and is integrated into a full-system simulator as data memory. The experimental results show that, by exploiting the approach, QoR is in the acceptable range, while its power overhead is about 84% (on average) less than that of the architecture-level approach. © 2017 ACM.",Approximate computing; Embedded systems; Low-power design; Memory management; Non-volatile memory; Phase change memory,Computation theory; Data storage equipment; Digital storage; Dynamic frequency scaling; Electric power supplies to apparatus; Error correction; Errors; Integer programming; Memory architecture; Phase change memory; Radiation hardening; Voltage scaling; Approximate computing; Dynamic voltage frequency scaling; Full system simulators; Integer linear programming models; Low power embedded systems; Low-power design; Memory management; Non-volatile memory; Embedded systems
Underminer: A framework for automatically identifying nonconverging behaviors in black-box system models,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041431884&doi=10.1145%2f3122787&partnerID=40&md5=883570a3e255620c2be8bd4bbcf70488,"Evaluation of industrial embedded control system designs is a time-consuming and imperfect process. While an ideal process would apply a formal verification technique such as model checking or theorem proving, these techniques do not scale to industrial design problems, and it is often difficult to use these techniques to verify performance aspects of control system designs, such as stability or convergence. For industrial designs, engineers rely on testing processes to identify critical or unexpected behaviors. We propose a novel framework called Underminer to improve the testing process; this is an automated technique to identify nonconverging behaviors in embedded control system designs. Underminer treats the system as a black box and lets the designer indicate the model parameters, inputs, and outputs that are of interest. It differentiates convergent from nonconvergent behaviors using Convergence Classifier Functions (CCFs). The tool can be applied in the context of testing models created late in the controller development stage, where it assumes that the given model displays mostly convergent behavior and learns a CCF in an unsupervised fashion from such convergent model behaviors. This CCF is then used to guide a thorough exploration of the model with the help of optimization-guided techniques or adaptive sampling techniques, with the goal of identifying rare nonconvergent model behaviors. Underminer can also be used early in the development stage, where models may have some significant nonconvergent behaviors. Here, the framework permits designers to indicate their mental model for convergence by labeling behaviors as convergent/nonconvergent and then constructs a CCF using a supervised learning technique. In this use case, the goal is to use the CCF to test an improved design for the model. Underminer supports a number of convergence-like notions, such as those based on Lyapunov analysis and temporal logic, and also CCFs learned directly from labeled output behaviors using machine-learning techniques such as support vector machines and neural networks. We demonstrate the efficacy of Underminer by evaluating its performance on several academic as well as industrial examples. © 2017 ACM.",Automatic testing; Formal methods; Machine learning; Stability,Artificial intelligence; Automatic testing; Control systems; Convergence of numerical methods; Education; Formal methods; Learning algorithms; Learning systems; Product design; Supervised learning; Systems analysis; Automated techniques; Controller development; Convergent behavior; Development stages; Embedded control systems; Machine learning techniques; Performance aspects; Verification techniques; Model checking
Low overhead CS-based heterogeneous framework for big data acceleration,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041431808&doi=10.1145%2f3092944&partnerID=40&md5=3fdc3358540e0ffd13b004a9d0f8295a,"Big data processing on hardware gained immense interest among the hardware research community to take advantage of fast processing and reconfigurability. Though the computation latency can be reduced using hardware, big data processing cost is dominated by data transfers. In this article, we propose a low overhead framework based on compressive sensing (CS) to reduce data transfers up to 67% without affecting signal quality. CS has two important kernels: ""sensing"" and ""reconstruction."" In this article, we focus on CS reconstruction is using orthogonal matching pursuit (OMP) algorithm.We implement the OMP CS reconstruction algorithm on a domain-specific PENC many-core platform and a low-power Jetson TK1 platform consisting of an ARM CPU and a K1 GPU. Detailed performance analysis of OMP algorithm on each platform suggests that the PENC many-core platform has 15× and 18× less energy consumption and 16× and 8× faster reconstruction time as compared to the low-power ARM CPU and K1 GPU, respectively. Furthermore, we implement the proposed CS-based framework on heterogeneous architecture, in which the PENC many-core architecture is used as an ""accelerator"" and processing is performed on the ARM CPU platform. For demonstration, we integrate the proposed CS-based framework with a hadoop MapReduce platform for a face detection application. The results showthat the proposed CS-based frameworkwith the PENC many-core as an accelerator achieves a 26.15% data storage/transfer reduction, with an execution time and energy consumption overhead of 3.7% and 0.002%, respectively, for 5, 000 image transfers. Compared to the CS-based framework implementation on the low-power Jetson TK1 ARM CPU+GPU platform, the PENC many-core implementation is 2.3× faster for the image reconstruction part, while achieving 29% higher performance and 34% better energy efficiency for the complete face detection application on the Hadoop MapReduce platform. © 2017 ACM Categories and Subject Descriptors: C.1.4 [Computer Systems Organization]: Processor Architectures- Parallel Architectures General Terms: Design, Algorithms, Performance.",Compressive sensing; Heterogeneous architecture; Many-core,ARM processors; Big data; Compressed sensing; Cost reduction; Data handling; Data reduction; Data transfer; Digital storage; Energy efficiency; Energy utilization; Face recognition; Graphics processing unit; Hardware; Image reconstruction; Compressive sensing; Heterogeneous architectures; Many core; Many-core architecture; Orthogonal matching pursuit; Performance analysis; Reconstruction algorithms; Research communities; Computer architecture
Effective verification for low-level software with competing interrupts,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041450824&doi=10.1145%2f3147432&partnerID=40&md5=52978777ea0f1d8b4394cc7dcf5b1d11,"Interrupt-driven software is difficult to test and debug, especially when interrupts can be nested and subject to priorities. Interrupts can arrive at arbitrary times, leading to an exponential blow-up in the number of cases to consider. We present a new formal approach to verifying interrupt-driven software based on symbolic execution. The approach leverages recent advances in the encoding of the execution traces of interacting, concurrent threads.We assess the performance of our method on benchmarks drawn from embedded systems code and device drivers, and experimentally compare it to conventional approaches that use source-to-source transformations. Our results show that our method significantly outperforms these techniques. To the best of our knowledge, our work is the first to demonstrate effective verification of low-level embedded software with nested interrupts. 2017 Copyright is held by the owner/author(s).",Concurrency; Interrupt-driven software; Interrupts; Model checking; Program instrumentation; Symbolic execution,Benchmarking; Embedded systems; Model checking; Software testing; Verification; Concurrency; Interrupt-driven software; Interrupts; Program instrumentations; Symbolic execution; Program debugging
D-PUF: An intrinsically reconfigurable DRAM PUF for device authentication and random number generation,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041435706&doi=10.1145%2f3105915&partnerID=40&md5=f34c5dde96a02ff9cc430dfe64826d4d,"Physically Unclonable Functions (PUFs) have proved to be an effective and low-cost measure against counterfeiting by providing device authentication and secure key storage services. Memory-based PUF implementations are an attractive option due to the ubiquitous nature of memory in electronic devices and the requirement of minimal (or no) additional circuitry. Dynamic Random Access Memory- (DRAM) based PUFs are particularly advantageous due to their large address space and multiple controllable parameters during response generation.However, priorworks onDRAMPUFs use a static response-generation mechanism making them vulnerable to security attacks. Further, they result in slow device authentication, are not applicable to commercial off-the-shelf devices, or require DRAM power cycling prior to authentication. In this article, we propose D-PUF, an intrinsically reconfigurable DRAM PUF based on the idea of DRAM refresh pausing. A key feature of the proposed DRAM PUF is reconfigurability, that is, by varying the DRAM refresh-pause interval, the challenge-response behavior of the PUF can be altered, making it robust to various attacks. The article is broadly divided into two parts. In the first part, we demonstrate the use of D-PUF in performing device authentication through a secure, low-overhead methodology. In the second part, we show the generation of true random numbers using D-PUF. The design is implemented and validated using an Altera Stratix IV GX FPGA-based Terasic TR4-230 development board and several off-the-shelf 1GB DDR3 DRAM modules. Our experimental results demonstrate a 4.3 × -6.4 × reduction in authentication time compared to prior work. Using controlled temperature and accelerated aging tests, we also demonstrate the robustness of our authentication mechanism to temperature variations and aging effects. Finally, the ability of the design to generate random numbers is verified using the NIST Statistical Test Suite. © 2017 ACM.",Authentication; Hardware security; Physically unclonable functions,Authentication; Dynamic random access storage; Electron devices; Hardware security; Integrated circuit design; Network security; Random access storage; Random number generation; Static random access storage; Accelerated aging test; Authentication mechanisms; Commercial off the shelf devices; Controllable parameters; Controlled temperature; Device authentications; Dynamic random access memory; Physically unclonable functions; Cryptography
Symbolic WCET computation,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041447016&doi=10.1145%2f3147413&partnerID=40&md5=9bbe79c88120abd0d1f61cc8ed055ccd,"Parametric Worst-case execution time (WCET) analysis of a sequential program produces a formula that represents the worst-case execution time of the program, where parameters of the formula are user-defined parameters of the program (as loop bounds, values of inputs, or internal variables, etc). In this article we propose a novel methodology to compute the parametric WCET of a program. Unlike other algorithms in the literature, our method is not based on Integer Linear Programming (ILP). Instead, we follow an approach based on the notion of symbolic computation of WCET formulae. After explaining our methodology and proving its correctness, we present a set of experiments to compare our method against the state of the art.We show that our approach dominates other parametric analyses and produces results that are very close to those produced by non-parametric ILP-based approaches, while keeping very good computing time. © 2017 ACM.",Symbolic evaluation; Worst-case execution time,Hardware; Software engineering; Integer Linear Programming; Parametric -analysis; Sequential programs; Symbolic computation; Symbolic evaluation; User-defined parameters; Worst-case execution time; Worst-case execution time analysis; Integer programming
DC4CD: A platform for distributed computing on constrained devices,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041441272&doi=10.1145%2f3105923&partnerID=40&md5=b6d36873999ac5c74023c16e4cae40d5,"In this article, we present Distributed Computing for Constrained Devices (DC4CD), a novel software architecture that supports symbolic distributed computing on wireless sensor networks. DC4CD integrates the functionalities of a high-level symbolic interpreter, a compiler, and an operating system, and includes networking abstractions to exchange high-level symbolic code among peer devices. Contrarily to other architectures proposed in the literature, DC4CD allows for changes at runtime, even on deployed nodes of both application and system code. Experimental results show that DC4CD is more efficient in terms of memory usage than existing architectures, with which it also compares well in terms of execution efficiency.",Distributed symbolic processing; Forth; High-level event handling; Interpretation and compilation of symbolic code on resource-constrained devices; Wireless programming of sensor network nodes,Codes (symbols); Computer architecture; FORTH (programming language); Network architecture; Sensor nodes; Wireless sensor networks; Event handling; Forth; Resourceconstrained devices; Sensor network nodes; Symbolic processing; Distributed computer systems
Exploiting sparsity to accelerate fully connected layers of CNN-based applications on mobile SoCs,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041441260&doi=10.1145%2f3122788&partnerID=40&md5=9f2af8efbeaffdae3c3bf953e2e109d4,"Convolutional neural networks (CNNs) are widely employed in many image recognition applications. With the proliferation of embedded and mobile devices, such applications are becoming commonplace on mobile devices. Network pruning is a commonly used strategy to reduce the memory and storage footprints of CNNs on mobile devices. In this article, we propose customized versions of the sparse matrix multiplication algorithm to speed up inference on mobile devices and make it more energy efficient. Specifically, we propose a Block Compressed Sparse Column algorithm and a bit-representation-based algorithm (BitsGEMM) that exploit sparsity to accelerate the fully connected layers of a network on the NVIDIA Jetson TK1 platform. We evaluate the proposed algorithms using real-world object classification and object detection applications. Experiments show that performance speedups can be achieved over the original baseline implementation using cuBLAS. On object detection CNNs, an average speedup of 1.82× is obtained over baseline cuBLAS in the fully connected layer of the VGG model, whereas on classification CNNs, an average speedup of 1.51× is achieved for the fully connected layer of the pruned-VGG model. Energy consumption reduction of 43-46% is also observed due to decreased computational and memory bandwidth demands. © 2017 ACM.",Convolutional neural networks; Matrix tiling; Mobile GPU; Sparse matrix compression; Sparse matrix multiplication; Sparse matrix-vector multiplication,Convolution; Energy efficiency; Energy utilization; Image recognition; Inference engines; Matrix algebra; Multiprocessing systems; Neural networks; Object detection; Object recognition; Convolutional neural network; Embedded and mobile devices; Energy efficient; Memory bandwidths; Network pruning; Real-world objects; Sparse matrices; Sparse matrix-vector multiplication; Network layers
A majority-based reliability-aware task mapping in high-performance homogenous NoC architectures,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041432035&doi=10.1145%2f3131273&partnerID=40&md5=1f8b8cdb01f3f15c1180e81bc76bac55,"This article presents a new reliability-aware task mapping approach in a many-core platform at design time for applications with DAG-based task graphs. The main goal is to devise a task mapping which meets a predefined reliability threshold considering a minimized performance degradation. The proposed approach uses a majority-voting replication technique to fulfill error-masking capability. A quantitative reliability model is also proposed for the platform. Our platform is a homogenous many-core architecture with mesh-based interconnection using traditional deterministic XY routing algorithm. Our iterative approach is applicable to an unlimited number of system fault types. All parts of the platform, including cores, links, and routers, are assumed to be prone to failures. We used the MNLP optimization technique to find the optimal mapping of the presented task graph. Experimental results show that our suggested task mappings not only comply with predefined reliability thresholds but also achieve notable time complexity reduction with respect to exhaustive space exploration. © 2017 ACM.",High performance; Many core; Network-on-chip; Reliability; Task mapping,Iterative methods; Mapping; Network architecture; Network-on-chip; Reliability; Routers; Space research; High performance; Many core; Many-core architecture; Optimization techniques; Performance degradation; Reliability threshold; Replication techniques; Task mapping; Computer architecture
Predictable shared cache management for multi-core real-time virtualization,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041447192&doi=10.1145%2f3092946&partnerID=40&md5=e80c6f8e49e2dadbc4aba87b4d87019e,"Real-time virtualization has gained much attention for the consolidation of multiple real-time systems onto a single hardware platform while ensuring timing predictability. However, a shared last-level cache (LLC) on modern multi-core platforms can easily hamper the timing predictability of real-time virtualization due to the resulting temporal interference among consolidated workloads. Since such interference caused by the LLC is highly variable and may have not even existed in legacy systems to be consolidated, it poses a significant challenge for real-time virtualization. In this article, we propose a predictable shared cache management framework for multi-core real-time virtualization. Our framework introduces two hypervisorlevel techniques, vLLC and vColoring, that enable the cache allocation of individual tasks running in a virtual machine (VM), which is not achievable by the current state of the art. Our framework also provides a cache management scheme that determines cache allocation to tasks, designs VMs in a cache-aware manner, and minimizes the aggregated utilization of VMs to be consolidated. As a proof of concept, we implemented vLLC and vColoring in the KVM hypervisor running on x86 and ARM multi-core platforms. Experimental results with three different guest OSs (i.e., Linux/RK, vanilla Linux, and MS Windows Embedded) show that our techniques can effectively control the cache allocation of tasks in VMs. Our cache management scheme yields a significant utilization benefit compared to other approaches while satisfying timing constraints. © 2017 ACM.",Cache management; Cyber-physical systems; Multi-core architectures; Real-time systems; Shared cache; Virtualization,Computer architecture; Computer operating systems; Cyber Physical System; Embedded systems; Interactive computer systems; Legacy systems; Linux; Virtual machine; Virtual reality; Virtualization; Cache management; Cache management schemes; Lastlevel caches (LLC); Multi-core platforms; Multicore architectures; Shared cache; Shared cache managements; Temporal interference; Real time systems
Coverage preservation with rapid forwarding in energy-harvesting wireless sensor networks for critical rare events,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040313199&doi=10.1145%2f3140961&partnerID=40&md5=5ac33f91e8586e16727c6cd9058217b7,"Wireless sensor networks for rarely occurring critical events must maintain sensing coverage and low-latency network connectivity to ensure event detection and subsequent rapid propagation of notification messages. Few algorithms have been proposed that address both coverage and forwarding and those that do are either unconcerned with rapid propagation or are not optimised to handle the constant changes in topology observed in duty-cycled networks. This article proposes an algorithm for Coverage Preservation with Rapid Forwarding (CPRF). The algorithm is shown to deliver perfect coverage maintenance and low-latency guaranteed message propagation whilst allowing stored-charge conservation via collaborative duty cycling in energy-harvesting networks. Favourable comparisons are made against established and recently proposed algorithms in both sparse planned and dense random distributions. Further, an implementation for commercially available wireless sensing devices is evaluated for detection and notification of damage to highway light poles caused by vortex shedding. © 2017 ACM.",Duty cycling; Energy harvesting; Rare events; Wireless sensor networks,Damage detection; Wireless sensor networks; Coverage maintenance; Duty-cycled networks; Duty-cycling; Low-latency networks; Message propagation; Random distribution; Rare events; Wireless sensing; Energy harvesting
Synergistic CPU-GPU frequency capping for energy-efficient mobile games,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040312606&doi=10.1145%2f3145337&partnerID=40&md5=ee6043e636da85397862846b7d37f2b0,"Mobile platforms are increasingly using Heterogeneous Multiprocessor Systems-on-Chip (HMPSoCs) with differentiated processing cores and GPUs to achieve high performance for graphics-intensive applications such as mobile games. Traditionally, separate CPU and GPU governors are deployed in order to achieve energy efficiency through Dynamic Voltage Frequency Scaling (DVFS) but miss opportunities for further energy savings through coordinated system-level application of DVFS. We present a cooperative CPU-GPU DVFS strategy (called Co-Cap) that orchestrates energy-efficient CPU and GPU DVFS through synergistic CPU and GPU frequency capping to avoid frequency overprovisioning while maintaining desired performance. Unlike traditional approaches that target a narrow set of mobile games, our Co-Cap approach is applicable across a wide range of microbenchmarks and mobile games. Our methodology employs a systematic training phase using fine-grained refinement steps with evaluations of frequency capping tables followed by a deployment phase, allowing deployment across a wide range of microbenchmarks and mobile games with varying graphics workloads. Our experimental results across multiple sets of over 200 microbenchmarks and 40 mobile games show that Co-Cap improves energy per frame by on average 8.9% (up to 18.3%) and 7.8% (up to 27.6%) (16.6% and 15.7% in CPU-dominant applications) and achieves minimal frames-per-second (FPS) loss by 0.9% and 0.85% (1.3% and 1.5% in CPU-dominant applications) on average in training and deployment sets, respectively, compared to the default CPU and GPU governors, with negligible overhead in execution time and power consumption on the ODROID-XU3 platform. © 2017 ACM.",DVFS; Integrated GPU; Power management policies,Computer graphics equipment; Dynamic frequency scaling; Energy conservation; Fault tolerance; Governors; Graphics processing unit; Program processors; System-on-chip; Voltage scaling; Coordinated system; DVFS; Dynamic voltage frequency scaling; Frames per seconds; Heterogeneous multiprocessor systems; Management policy; Systematic training; Traditional approaches; Energy efficiency
Guest editorial for the special issue of ESWEEK 2016,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041580046&doi=10.1145%2f3152097&partnerID=40&md5=125552f30a2682f83ff61fd129b6d657,[No abstract available],,
Improving write performance and extending endurance of object-based NAND flash devices,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040326818&doi=10.1145%2f3105924&partnerID=40&md5=05377906082ce522ea98db4e60953e6a,"Write amplification is a major cause of performance and endurance degradations in NAND flash-based storage systems. In an object-based NAND flash device (ONFD), two causes of write amplification are onode partial update and cascading update. Here, onode is a type of small-sized object metadata, and multiple onodes are stored in one NAND flash page. Updating one onode invokes partial page update (i.e., onode partial update), incurring unnecessary migration of the un-updated data. Cascading update denotes updating object metadata in a cascading manner due to object data update or migration. Although there are only several bytes that need to be updated in the object metadata, one or more pages have to be re-written accordingly. In this work, we propose a system design to alleviate the write amplification issue in the object-based NAND flash device. The proposed design includes (1) a multi-level garbage collection technique to minimize unnecessary data migration incurred by onode partial update and (2) a B+ table tree, Semantics-Aware Flexible (SAF) data layout, and selective cache design to reduce the write operations associated with cascading update. To guarantee system consistency, we also propose a power failure handling technique. Experiment results show that our proposed design can achieve up to 20% write reduction compared to the best states of the art. © 2017 ACM.",NAND flash memories; Performance; Write amplification,Flash memory; Memory architecture; Metadata; Semantics; Trees (mathematics); Data migration; Garbage collection; NAND flash memory; Partial updates; Performance; Storage systems; Write amplifications; Write operations; NAND circuits
CaffePresso: Accelerating convolutional networks on embedded SoCs,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041439324&doi=10.1145%2f3105925&partnerID=40&md5=a0a6ba5250a0a525384c6e2d2e8daff2,"Auto-tuning and parametric implementation of deep learning kernels allow off-the-shelf accelerator-based embedded platforms to deliver high-performance and energy-efficient mappings of the inference phase of lightweight neural networks. Low-complexity classifiers are characterized by operations on small image maps with two to three deep layers and few class labels. For these use cases, we consider a range of embedded systems with 20W power budgets such as the Xilinx ZC706 (FPGA), NVIDIA Jetson TX1 (GPU), TI Keystone II (DSP), and Adapteva Parallella (RISC+NoC). In CaffePresso, we combine auto-tuning of the implementation parameters, and platform-specific constraints deliver optimized solutions for each input ConvNet specification. © 2017 ACM.",Deep learning; DSP; Embedded; FPGA; GPU; NoC,Budget control; Deep learning; Embedded systems; Energy efficiency; Field programmable gate arrays (FPGA); Graphics processing unit; Class labels; Convolutional networks; Embedded; Embedded platforms; Energy efficient; Learning kernels; Optimized solutions; Power budgets; Network-on-chip
Fault recovery time analysis for coarse-grained reconfigurable architectures,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041436658&doi=10.1145%2f3140944&partnerID=40&md5=683a64466837b066722572d135c798bf,"Coarse-grained reconfigurable architectures (CGRAs) have drawn increasing attention due to their performance and flexibility advantages. Typically, CGRAs incorporate many processing elements in the form of an array, which is suitable for implementing spatial redundancy, as used in the design of fault-tolerant systems. This article introduces a recovery time model for transient faults in CGRAs. The proposed fault-tolerant CGRAs are based on triple modular redundancy and coding techniques for error detection and correction. To evaluate the model, several kernels from space computing are mapped onto the suggested architecture. We demonstrate the tradeoff between recovery time, performance, and area. In addition, the average execution time of an application including recovery time is evaluated using area-based error-rate estimates in harsh radiation environments. The results show that task partitioning is important for bounding the recovery time of applications that have long execution times. It is also shown that error-correcting code (ECC) is of limited practical value for tasks with long execution times in high radiation environments, or when the degree of task partitioning is high. © 2017 ACM.",Coarse-grained reconfigurable architecture; Triple modular redundancy,Architecture; Embedded systems; Errors; Fault tolerance; Fault tolerant computer systems; Recovery; Redundancy; Average Execution Time; Coarse grained reconfigurable architecture; Coarse grained reconfigurable architecture (CGRAs); Error detection and correction; Fault tolerant systems; Harsh radiation environment; High radiation environment; Triple modular redundancy; Reconfigurable architectures
Testing programs with contextual unfoldings,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041438041&doi=10.1145%2f2810000&partnerID=40&md5=5591d9bfe4eae82748681e64c2f1ab0b,"In this article, we present a new algorithm that combines contextual unfoldings and dynamic symbolic execution to systematically test multithreaded programs. The approach uses symbolic execution to limit the number of input values and unfoldings to thus limit the number of thread interleavings that are needed to cover reachable local states of threads in the program under test.We show that the use of contextual unfoldings allows interleavings of threads to be succinctly represented. This can in some cases lead to a substantial reduction in the number of needed test executions when compared to previous approaches. © 2017 ACM.",Contextual unfoldings; Dynamic symbolic execution; Testing,Multitasking; Testing; Dynamic symbolic executions; Multi-threaded programs; Number of threads; Substantial reduction; Symbolic execution; Test execution; Testing programs; Unfoldings; Model checking
Fixed-priority scheduling for two-phase mixed-criticality systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041437597&doi=10.1145%2f3105921&partnerID=40&md5=89f7081200f3945a81e901b69f09fe46,"In this article, a two-phase execution model is proposed for mixed-criticality (MC) tasks. Different from traditional MC tasks with a computation phase only, the two-phase execution model requires a memory-access phase first to fetch the instructions and data, and then computation. Theoretical foundations are first established for a schedulability test under given memory-access and computation priority assignment. Based on the established theoretical conclusions, a two-stage priority assignment algorithm, which can find the best priority assignment for both memory-access and computation phases under fixed-priority scheduling, is further developed. Extensive experiments have been conducted and the experimental results validate the effectiveness of our proposed approach. © 2017 ACM.",Embedded system; Memory-access; Mixed-criticality; Real-time,Computer architecture; Embedded systems; Memory architecture; Scheduling; Scheduling algorithms; Fixed priority scheduling; Memory access; Mixed criticalities; Mixed-criticality systems; Priority assignment; Real time; Schedulability test; Theoretical foundations; Criticality (nuclear fission)
Runtime Performance and Power Optimization of Parallel Disparity Estimation on Many-Core Platforms,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034622892&doi=10.1145%2f3133560&partnerID=40&md5=8d023103bb56d82b5e7bb42fc5aec68f,"This article investigates the use of many-core systems to execute the disparity estimation algorithm, used in stereo vision applications, as these systems can provide flexibility between performance scaling and power consumption. We present a learning-based runtime management approach that achieves a required performance threshold while minimizing power consumption through dynamic control of frequency and core allocation. Experimental results are obtained from a 61-core Intel Xeon Phi platform for the aforementioned investigation. The same performance can be achieved with an average reduction in power consumption of 27.8% and increased energy efficiency by 30.04% when compared to Dynamic Voltage and Frequency Scaling control alone without runtime management. © 2017 ACM.",Computer vision; Many-core platforms; Power optimization; Runtime management,Computer architecture; Computer vision; Dynamic frequency scaling; Electric power utilization; Energy efficiency; Stereo image processing; Voltage scaling; Disparity estimations; Dynamic controls; Dynamic voltage and frequency scaling; Many core; Power Optimization; Run-time performance; Runtime management; Vision applications; Stereo vision
LOCUS: Low-power customizable many-core architecture for wearables,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041440672&doi=10.1145%2f3122786&partnerID=40&md5=2c221c4659cc884a52904f40b6441931,"Application requirements, such as real-time response, are pushing wearable devices to leverage more powerful processors inside the SoC (system on chip). However, existing wearable devices are not well suited for such challenging applications due to poor performance, and the conventional powerful many-core architectures are not appropriate either due to the stringent power budget in this domain. We propose LOCUS-a low-power, customizable, many-core processor for next-generation wearable devices. LOCUS combines customizable processor cores with a customizable network on a message-passing architecture to deliver very competitive performance/watt-an average 3.1 × compared to quad-core ARM processors used in state-ofthe- art wearable devices. A combination of full system simulation with representative applications from the wearable domain and RTL synthesis of the architecture show that 16-core LOCUS achieves an average 1.52 × performance/watt improvement over a conventional 16-core shared memory many-core architecture. A dynamic power management mechanism is proposed to further decrease the power consumption in both computation and communication, which improves the performance/watt of LOCUS by 1.17 ×.",Customization; Network on chip; Power efficiency; Wearables,Arts computing; Budget control; Distributed computer systems; Memory architecture; Message passing; Network architecture; Network-on-chip; Programmable logic controllers; System-on-chip; Wearable computers; Wearable technology; Application requirements; Competitive performance; Customizable processors; Customization; Dynamic power management; Message passing architectures; Power efficiency; Wearables; Computer architecture
Operational models for piecewise-smooth systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033238530&doi=10.1145%2f3126506&partnerID=40&md5=24c9a75c39d9b6978622d07d441b0a92,"In this article we study ways of constructing meaningful operational models of piecewise-smooth systems (PWS). The systems we consider are described by polynomial vector fields defined on non-overlapping semialgebraic sets, which form a partition of the state space. Our approach is to give meaning to motion in systems of this type by automatically synthesizing operational models in the form of hybrid automata (HA). Despite appearances, it is in practice often difficult to arrive at satisfactory HA models of PWS. The different ways of building operationalmodels that we explore in our approach can be thought of as defining different semantics for the underlying PWS. These differences have a number of interesting nuances related to phenomena such as chattering, non-determinism, so-called mythical modes and sliding behaviour. © 2017 ACM.",Discontinuous differential equations; Hybrid automata; Operational models; Piecewise-smooth systems,Automata theory; Differential equations; Semantics; Discontinuous differential equations; HA model; Hybrid automatons; Non Determinism; Operational model; Piecewise-smooth systems; Polynomial vector field; Semi-algebraic sets; Vector spaces
Managing the performance/error tradeoff of floating-point intensive applications,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033214521&doi=10.1145%2f3126519&partnerID=40&md5=80259498e27bd2c219851cabdfe6e99f,"Modern embedded systems are becoming more reliant on real-valued arithmetic as they employ mathematically complex vision algorithms and sensor signal processing. Double-precision floating point is the most commonly used precision in computer vision algorithm implementations. A single-precision floating point can provide a performance boost due to less memory transfers, less cache occupancy, and relatively faster mathematical operations on some architectures. However, adopting it can result in loss of accuracy. Identifying which parts of the program can run in single-precision floating point with low impact on error is a manual and tedious process. In this paper, we propose an automatic approach to identify parts of the program that have a low impact on error using shadow-value analysis. Our approach provides the user with a performance/error tradeoff, using which the user can decide how much accuracy can be sacrificed in return for performance improvement.We illustrate the impact of the approach using a well known implementation of Apriltag detection used in robotics vision. We demonstrate that an average 1.3× speedup can be achieved with no impact on tag detection, and a 1.7× speedup with only 4% false negatives. © 2017 ACM.",performance; precision; Real-valued arithmetic; robotic vision; tra,Computer vision; Embedded systems; Robotics; Signal processing; Automatic approaches; Computer vision algorithms; Mathematical operations; Modern embedded systems; performance; precision; Robotic vision; Vision algorithms; Digital arithmetic
Transmission adaptation for battery-free relaying,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033214242&doi=10.1145%2f3055513&partnerID=40&md5=4d94f8109e900ff9f62afa8f23cbdd9e,"Energy harvesting (EH)-enabled relaying has attracted considerable attention as an effective way to prolong the operation time of energy-constrained networks and extend coverage beside desired survivability and rate of transmission. In related literature, the Harvest-Store-Use (HSU) model is usually utilized to describe the energy flow behavior of the EH system. However, the half-duplex (HD) constraint of HSU that harvested energy can only be used after being temporally stored in energy buffer may reduce effective transmission time. Thus, we first construct the full-duplex (FD) energy flow behavior model of the EH system where the harvested energy can be tuned to power load and being stored simultaneously. The FD model is then proved to be equivalent with the HSU model when time interval is small enough. Considering some key physical variabilities, for example, the wireless channel and the amount of harvested energy, the transmission adaptation problem for multiple relays embedded with FD EH systems is formulated with the objective to improve the utilization of the harvested energy. We tackle the problem by using a centralized optimization algorithm by jointly tuning the factors, including power control for source and relay nodes, relay selection and dynamic switching among four relay transmission mode, namely HD amplify-and-forward (AF), HD decode-and-forward (DF), FD AF, and FD DF. The centralized optimization algorithm is proposed on the basis of dual decomposition and serves as a benchmark. To enable relays to individually make their own decisions, a distributed algorithm with relatively higher complexity is given by using consensus optimization in conjunction with the alternating direction method of multipliers, and a sub-optimal algorithm with low complexity is provided. The proposed algorithms are shown to have good performance via simulations for a range of different EH rates and prediction errors. © 2017 ACM.",Amplify-and-forward/decode-and-forward relaying; Cooperative; Energy harvesting; Full-duplex/half-duplex; Transmission adaptation,Complex networks; Computational complexity; Digital television; Electric batteries; Embedded systems; Energy harvesting; Finite difference method; Power control; Alternating direction method of multipliers; Amplify and forward; Centralized optimization; Cooperative; Effective transmission; Energy-constrained networks; Full-duplex; Sub-optimal algorithms; Optimization
Oppc: An optimal path planning charging scheme based on schedulability evaluation for wrsns,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033237047&doi=10.1145%2f3126684&partnerID=40&md5=e318e2ba3a26133fe9740b31d31ec623,"The lack of schedulability evaluation of previous charging schemes in wireless rechargeable sensor networks (WRSNs) degrades the charging efficiency, leading to node exhaustion.We propose an Optimal Path Planning Charging scheme, namely OPPC, for the on-demand charging architecture. OPPC evaluates the schedulability of a charging mission, which makes charging scheduling predictable. It provides an optimal charging path which maximizes charging efficiency. When confronted with a non-schedulable charging mission, a node discarding algorithm is developed to enable the schedulability. Experimental simulations demonstrate that OPPC can achieve better performance in successful charging rate as well as charging efficiency. © 2017 ACM.",Charging efficiency; Charging path planning; On-demand charging architecture; Schedulability evaluation; Wireless rechargeable sensor networks,Efficiency; Network architecture; Sensor networks; Sensor nodes; Wireless sensor networks; Charging efficiency; Charging scheme; Experimental simulations; On demands; Optimal charging; Optimal path planning; Rechargeable sensor networks; Schedulability; Motion planning
Response time analysis for sporadic server based budget scheduling in real time virtualization environments,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030693398&doi=10.1145%2f3126559&partnerID=40&md5=e0af43fd9b5c8e8bb4a479d9e0d867ac,"Virtualization techniques for embedded real-time systems typically employ TDMA scheduling to achieve temporal isolation among different virtualized applications. Recent work already introduced sporadic server based solutions relying on budgets instead of a fixed TDMA schedule. While providing better average-case response times for IRQs and tasks, a formal response time analysis for the worst-case is still missing. In order to confirm the advantage of a sporadic server based budget scheduling, this paper provides aworst-case response time analysis. To improve the sporadic server based budget scheduling even more, we provide a background scheduling implementation which will also be covered by the formal analysis. We show correctness of the analysis approach and compare it against TDMA based systems. In addition to that, we provide response time measurements from a working hypervisor implementation on an ARM based development board. © 2017 ACM.",Formal analysis; Hypervisor; RTOS; Sporadic server; Virtualization,Budget control; Embedded systems; Interactive computer systems; Response time (computer systems); Scheduling; Time division multiple access; Virtual reality; Virtualization; Embedded real time systems; Formal analysis; Hypervisor; Response-time analysis; RTOS; Sporadic servers; Temporal isolation; Virtualization Techniques; Real time systems
Complete and practical universal instruction selection,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030692980&doi=10.1145%2f3126528&partnerID=40&md5=8fbfdf1946a977267dc96e1b88cb2aef,"In code generation, instruction selection chooses processor instructions to implement a program under compilation where code quality crucially depends on the choice of instructions. Using methods from combinatorial optimization, this paper proposes an expressive model that integrates global instruction selection with global code motion. The model introduces (1) handling of memory computations and function calls, (2) a method for inserting additional jump instructions where necessary, (3) a dependency-based technique to ensure correct combinations of instructions, (4) value reuse to improve code quality, and (5) an objective function that reduces compilation time and increases scalability by exploiting bounding techniques. The approach is demonstrated to be complete and practical, competitive with LLVM, and potentially optimal (w.r.t. the model) for medium-sized functions. The results show that combinatorial optimization for instruction selection is well-suited to exploit the potential of modern processors in embedded systems. © 2017 ACM.",Code generation; Combinatorial optimization; Constraint programming; Instruction selection,Combinatorial optimization; Computer programming; Constraint theory; Embedded systems; Program processors; Bounding techniques; Code Generation; Constraint programming; Function calls; Instruction selection; Memory computations; Modern processors; Objective functions; Codes (symbols)
FlowPaP and FlowReR: Improving energy efficiency and performance for STT-MRAM-based handheld devices under read disturbance,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030694005&doi=10.1145%2f3126532&partnerID=40&md5=862f6111e999fcb1ca222b7c1ab6ef46,"Handheld devices, such as smartphones and tablets, currently dominate the semiconductor market. The memory access patterns of CPU and IP cores are dramatically different in a handheld device, making the main memory a critical bottleneck of the entire system. As a result, non-volatile memories, such as spin transfer torque magnetoresistive random-access memory (STT-MRAM), are emerging as a replacement for the existing DRAM-based main memory, achieving a wide variety of advantages. However, replacing DRAM with STT-MRAM also results in new design challenges including read disturbance. A simple read-and-restore scheme preserves data integrity under read disturbance, but incurs significant performance and energy overheads. Consequently, by utilizing unique characteristics of mobile applications, we propose FlowPaP, a flow pattern prediction scheme to dynamically predict the write-to-last-read distances for data frames running on a handheld device. FlowPaP identifies and removes unnecessary memory restores originally required for preventing read disturbance, significantly improving energy efficiency and performance for STT-MRAM-based handheld devices. In addition, we propose a flow-based data retention time reduction scheme named FlowReR to further lower energy consumption of STT-MRAM at the expense of reducing its data retention time. FlowReR imposes a second step that marginally trades off the already improved energy efficiency for performance improvements. Experimental results show that, compared to the original read-and-restore scheme, the application of FlowPaP and FlowReR together can simultaneously improve energy efficiency by 34% and performance by 17% for a set of commonly used Android applications. © 2017 ACM.",Flowbased applications; Handheld devices; Main memory; Read disturbance; STT-MRAM,Commerce; Data storage equipment; Dynamic random access storage; Energy efficiency; Energy utilization; Flow patterns; Hand held computers; Magnetic recording; Magnetic storage; Random access storage; Restoration; Semiconductor device manufacture; Semiconductor devices; Efficiency and performance; Hand held device; Main memory; Memory access patterns; Read disturbance; Semi-conductor market; Spin transfer torque; STT-MRAM; MRAM devices
Joint optimization of sensing and power allocation in energy-harvesting cognitive radio networks,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033239316&doi=10.1145%2f3070709&partnerID=40&md5=49625bd5c973a1ec12009a3f78affd76,"The energy-harvesting cognitive radio (CR) network is proposed to improve the spectrum efficiency and energy efficiency. We focus on the optimization of sensing time and power allocation to maximize the throughput of the energy-harvesting CR network subject to the energy causality constraint and collision constraint. Based on the classification of operating regions, the optimization problem is divided into two subproblems. Then, the efficient iterative Algorithm 1 and Algorithm 2 are proposed to solve sub-problem (A) and sub-problem (B), respectively. Numerical results show that a significant improvement in the throughput is achieved via joint optimization of sensing time and power allocation. © 2017 ACM.",Cognitive radio networks; Energy-harvesting; Power allocation; Sensing time; Spectrum efficiency; Throughput,Cognitive radio; Energy harvesting; Iterative methods; Optimization; Problem solving; Throughput; Causality constraint; Cognitive radio network; Iterative algorithm; Optimization problems; Power allocations; Sensing time; Spectrum efficiency; Time and power allocations; Energy efficiency
Tightening contention delays while scheduling parallel applications on multi-core architectures,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030726207&doi=10.1145%2f3126496&partnerID=40&md5=ae1744b9a4b0df65093a737f8fca55fc,"Multi-core systems are increasingly interesting candidates for executing parallel real-time applications, in avionic, space or automotive industries, as they provide both computing capabilities and power efficiency. However, ensuring that timing constraints are met on such platforms is challenging, because some hardware resources are shared between cores. Assuming worst-case contentions when analyzing the schedulability of applications may result in systems mistakenly declared unschedulable, although the worst-case level of contentions can never occur in practice. In this paper, we present two contention-aware scheduling strategies that produce a time-triggered schedule of the application's tasks. Based on knowledge of the application's structure, our scheduling strategies precisely estimate the effective contentions, in order to minimize the overall makespan of the schedule. An Integer Linear Programming (ILP) solution of the scheduling problem is presented, as well as a heuristic solution that generates schedules very close to ones of the ILP (5% longer on average), with a much lower time complexity. Our heuristic improves by 19% the overall makespan of the resulting schedules compared to a worst-case contention baseline. © 2017 ACM.",Contention-aware scheduling; Real-time system,Automotive industry; Integer programming; Interactive computer systems; Parallel architectures; Real time systems; Scheduling; Scheduling algorithms; Computing capability; Contention-aware; Heuristic solutions; Integer Linear Programming; Multicore architectures; Parallel application; Real-time application; Scheduling strategies; Computer architecture
The CURE: Cluster communication using registers,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030721673&doi=10.1145%2f3126527&partnerID=40&md5=4f9d7e726b0fd94b28c1f3b25a8e8406,"VLIW processors typically deliver high performance on limited budget making them ideal for a variety of communication and signal processing solutions. These processors typically need large multi-ported register files that can have side effects of increased cycle time and high power consumption. The access delay and energy of these register files can also become prohibitive when increasing the register count or the access ports, thus limiting the overall performance of the processor. Most prior art circumvent this problem by using multiple clusters with private register files, to lower the access delay and reduce energy consumption. However, clustering artifacts, like increased inter-cluster communication operations and spill-recovery code, result in a performance penalty. This paper proposes CURE - a novel technique to considerably reduce the negative effects of clustering. CURE augments the ISA to expose the communication registers to the compilers to increase availability of architectural register state to all functional units. The inter-cluster communication operations are integrated into regular ALU and memory operations to improve instruction encoding efficiency. We also propose a new code scheduling heuristic to handle the ISA changes, and to realize the improvements in processor's performance and energy consumption. Our quantitative analysis estimates that CURE, when compared to the baseline 8-issue uni-cluster processor, boosts average performance by 61% while reducing the average register dynamic energy by 77%. © 2017 ACM.",Inter-cluster communication; Register file architecture,Budget control; Cluster computing; Curing; Energy utilization; Signal processing; Cluster communication; Communication registers; High power consumption; Instruction encoding; Intercluster communication; Performance penalties; Reduce energy consumption; Register files; Very long instruction word architecture
Using criticality of GPU accesses in memory management for CPU-GPU heterogeneous multi-core processors,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030699825&doi=10.1145%2f3126540&partnerID=40&md5=ebb0a49543e1bc811f58f3566f351421,"Heterogeneous chip-multiprocessors with CPU and GPU integrated on the same die allow sharing of critical memory system resources among the CPU and GPU applications. Such architectures give rise to challenging resource scheduling problems. In this paper, we explore memory access scheduling algorithms driven by criticality of GPU accesses in such systems. Different GPU access streams originate from different parts of the GPU rendering pipeline, which behaves very differently from the typical CPU pipeline requiring new techniques for GPU access criticality estimation. We propose a novel queuing network model to estimate the performance-criticality of the GPU access streams. If a GPU application performs below the quality of service requirement (e.g., frame rate in 3D scene rendering), the memory access scheduler uses the estimated criticality information to accelerate the critical GPU accesses. Detailed simulations done on a heterogeneous chip-multiprocessor model with one GPU and four CPU cores running heterogeneous mixes of DirectX, OpenGL, and CPU applications show that our proposal improves the GPU performance by 15% on average without degrading the CPU performance much. Extensions proposed for the mixes containing GPGPU applications, which do not have any quality of service requirement, improve the performance by 7% on average for these mixes. © 2017 ACM.",3D rendering; CPU-GPU heterogeneous multi-core; DRAM access scheduling; GPGPU; GPU access criticality,Adaptive systems; Application programming interfaces (API); Criticality (nuclear fission); Distributed computer systems; Dynamic random access storage; Memory architecture; Multiprocessing systems; Pipelines; Program processors; Quality of service; Rendering (computer graphics); Scheduling; Scheduling algorithms; Three dimensional computer graphics; 3-D rendering; Access scheduling; GPGPU; Heterogeneous chip multiprocessor; Heterogeneous multi core processors; Heterogeneous Multi-Cores; Memory access scheduling; Queuing network model; Graphics processing unit
A high-speed accelerator for Homomorphic Encryption using the Karatsuba algorithm,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030700191&doi=10.1145%2f3126558&partnerID=40&md5=6a3cc9470919b805fce76db978559676,"Somewhat Homomorphic Encryption (SHE) schemes can be used to carry out operations on ciphered data. In a cloud computing scenario, personal information can be processed secretly, inferring a high level of confidentiality. The principle limitation of SHE is the size of ciphertext compared to the size of the message. This issue can be addressed by using a batching technique that ""packs"" several messages into one ciphertext. However, this method leads to important drawbacks in standard implementations. This paper presents a fast hardware/software co-design implementation of an encryption procedure using the Karatsuba algorithm. Our hardware accelerator is 1.5 times faster than the state of the art for 1 encryption and 4 times faster for 4 encryptions. © 2017 ACM.",FV; Hardware accelerator; Homomorphic encryption; Karatsuba,Hardware; Hardware-software codesign; Security of data; Batching techniques; Encryption procedure; Hardware accelerators; Ho-momorphic encryptions; Karatsuba; Karatsuba algorithm; Personal information; Somewhat homomorphic encryptions; Cryptography
P-Alloc: Process-variation tolerant reliability management for 3D charge-trapping flash memory,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030706463&doi=10.1145%2f3126554&partnerID=40&md5=fb2f7f0a1e5152e568209757d38f7c53,"Three-dimensional (3D) flash memory is an emerging memory technology that enables a number of improvements to conventional planar NAND flash memory, including larger capacity, less program disturbance, and lower access latency. In contrast to conventional planar flash memory, 3D flash memory adopts chargetrapping mechanism. NAND strings punch through multiple stacked layers to form the three-dimensional infrastructure. However, the etching processes for NAND strings are unable to produce perfectly vertical features, especially on the scale of 20 nanometers or less. The process variation will cause uneven distribution of electrons, which poses a threat to the integrity of data stored in flash. This paper present P-Alloc, a process-variation tolerant reliability management strategy for 3D chargetrapping flash memory. P-Alloc offers both hardware and software support to allocate data to the 3D flash in the presence of process variation. P-Alloc predicts the state of a physical page, i.e., the basic unit for each write or read operation in flash memory, and tries to assign critical data to more reliable pages. A hardwarebased voltage threshold compensation scheme is also proposed to further reduce the faults. We demonstrate the viability of the proposed scheme using a variety of realistic workloads. Our extensive evaluations show that, P-Alloc significantly enhances the reliability and reduces the access latency compared to the baseline scheme. © 2017 ACM.",Charge-trapping flash; Fault tolerant; Flash translation layer; Garbage collection; Process variation; Space allocation; Three-dimensional flash memory,Charge trapping; Hardware; NAND circuits; Reliability; Security of data; Charge-trapping flashes; Fault-tolerant; Flash translation layer; Garbage collection; Process Variation; Space allocation; Flash memory
A program interference error aware LDPC scheme for improving NAND flash decoding performance,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030717635&doi=10.1145%2f3126563&partnerID=40&md5=533069343cafa31761ebd03323d1fa52,"By scaling down to smaller cell size, NAND flash has significantly increased the storage capacity in order to lower the unit cost down. However, the reliability is sacrificed due to much higher raw bit error rates. As a result, conventional error correction codes (ECCs), such as BCH codes, are not sufficient. Low-density parity check (LDPC) codes with stronger error correction capability are adopted in NAND flash to guarantee data reliability. However, read performance using LDPC is poor because of its decoding complexity. It has been found that flash cells with fewer electrons are more prone to program interference errors. As a result, program interference errors show the characteristic of value dependence. This characteristic can be exploited and translated into extra information facilitating the decoding convergence. Motivated by this observation, we propose PEAL: a flash program interference error aware LDPC scheme to enhance the decoding performance. PEAL integrates the obtained extra information from the value dependence into the soft-to-hard decision process in LDPC decoding to decrease decoding iterations and improve the decoding convergence speed. Simulation results show that decoding iterations are reduced by up to 69.37% and the decoding convergence speed is improved by up to 2.5×, compared with the normalized min-sum (NMS) algorithm with 2KB information lengths at an approximate raw bit error rate of 11.5 × 10-3. © 2017 ACM.",LDPC codes; Program interference error; Value dependence,Codes (symbols); Convolutional codes; Decoding; Digital storage; Error correction; Errors; Forward error correction; Iterative decoding; Memory architecture; NAND circuits; Satellite communication systems; Decoding complexity; Decoding performance; Error correction capability; Error correction codes (ECCs); LDPC codes; Low-density parity-check (LDPC) codes; Raw bit error rates; Value dependence; Bit error rate
Security-aware scheduling of embedded control tasks,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030725701&doi=10.1145%2f3126518&partnerID=40&md5=5fa8db19bc6ffa6d46e972913e1e1f17,"In this work, we focus on securing cyber-physical systems (CPS) in the presence of network-based attacks, such as Man-in-the-Middle (MitM) attacks, where a stealthy attacker is able to compromise communication between system sensors and controllers. Standard methods for this type of attacks rely on the use of cryptographic mechanisms, such as Message Authentication Codes (MACs) to ensure data integrity. However, this approach incurs significant computation overhead, limiting its use in resource constrained systems. Consequently, we consider the problem of scheduling multiple control tasks on a shared processor while providing a suitable level of security guarantees. Specifically, by security guarantees we refer to control performance, i.e., Quality-of-Control (QoC), in the presence of attacks. We start by mapping requirements for QoC under attack into constraints for security-aware control tasks that, besides standard control operations, intermittently perform data authentication. This allows for the analysis of the impact that security-related computation overhead has on both schedulability of control tasks and QoC. Building on this analysis, we introduce a mixed-integer linear programming-based technique to obtain a schedulable task set with predefined QoC requirements. Also, to facilitate optimal resource allocation, we provide a method to analyze interplay between available computational resources and the overall QoC under attack, and show how to obtain a schedulable task set that maximizes the overall QoC guarantees. Finally, we prove usability of our approach on a case study with multiple automotive control components. © 2017 ACM.",CPS security; Mixed integer linear programming; Quality-of-control; Real-time scheduling,Authentication; Embedded systems; Integer programming; Scheduling; CPS security; Cyber-physical systems (CPS); Man in the middles (MITM); Message authentication codes; Mixed integer linear programming; Optimal resource allocation; Quality of controls; Real - time scheduling; Quality control
QLUT: Input-aware quantized table lookup for energy-efficient approximate accelerators,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030702420&doi=10.1145%2f3126531&partnerID=40&md5=caaa9d591e2ad399ff6a6aae32630c5f,"Approximate computing has emerged as a popular design paradigm for optimizing the performance and energy consumption of error-resilient applications in domains such as machine learning, graphics, data analytics, etc. Numerous techniques for approximate computing have been proposed at different layers of the system stack, from circuits to architecture to software. In this work, we propose a new technique, called quantized table lookup, for approximating the meta-functions used in the core computational kernels of error-resilient applications. In contrast to prior work that directly approximates the functionality of the meta-functions, the proposed technique instead approximates the input data to the meta-functions by reducing/quantizing them to a much smaller set of values that we call quantized inputs. The small number of quantized inputs enables us to completely replace the energy-intensive arithmetic units in the meta-function with small and energyefficient lookup tables (called quantized lookup tables or qLUT) that contain precomputed output values corresponding to the quantized inputs. The proposed approximation technique is not only highly generic, but also inherently quality-configurable and input-aware. Quality-configurability and input-awareness are achieved bymodulating the size of the qLUT aswell as selecting the values of the quantized inputs judiciously based on the statistics of the original input data. To evaluate the proposed technique, we have implemented the dominant meta-functions of nine error-resilient application benchmarks as quantized table lookup based hardware accelerators using 45nm technology. Experimental results demonstrate average energy savings of 46% at the application-level for minimal (<1%) loss in output quality. © 2017 ACM.",Accelerators; Approximate computing; Lookup table; Low-power design,Application programs; Benchmarking; Electric power supplies to apparatus; Energy conservation; Energy efficiency; Energy utilization; Errors; Green computing; Input output programs; Learning systems; Particle accelerators; Application level; Approximate computing; Approximation techniques; Computational kernels; Different layers; Energy efficient; Hardware accelerators; Low-power design; Table lookup
Minimising access conflicts on shared multi-bank memory,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030697857&doi=10.1145%2f3126535&partnerID=40&md5=c85383720917c70835d4e4f37fa44272,"A common multi-core pattern consists of processors communicating through shared, multi-banked on-chip memory. Two approaches exist: Interleaved address mapping, which spreads consecutive data over all banks, and contiguous address mapping, which stores consecutive data on a single bank. In this work, we compare both approaches on the Kalray MPPA-256 platform. For contiguous mapping, we propose an algorithm, based on graph colouring techniques, to automatically perform the assignment of data blocks to memory banks with the goal of minimising access collisions and delays. Experiments with representative, parallel real-world benchmarks show that 69% of the tested configurations, when optimised for contiguous mapping by our algorithm, run up to 86% faster on average than with interleaved mapping. © 2017 ACM.",Contention; Interleaved addressing; Memory allocation; Memory banks,Hardware; Software engineering; Storage allocation (computer); Address mappings; Contention; Graph colouring; Interleaved addressing; Interleaved mappings; Memory banks; Multi-bank memory; On chip memory; Mapping
Energy-efficient run-time mapping and thread partitioning of concurrent OpenCL applications on CPU-GPU MPSoCs,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030694480&doi=10.1145%2f3126548&partnerID=40&md5=0285db0e4704bd1833155186af58c660,"Heterogeneous Multi-Processor Systems-on-Chips (MPSoCs) containing CPU and GPU cores are typically required to execute applications concurrently. However, as will be shown in this paper, existing approaches are not well suited for concurrent applications as they are developed either by considering only a single application or they do not exploit both CPU and GPU cores at the same time. In this paper, we propose an energyefficient run-time mapping and thread partitioning approach for executing concurrent OpenCL applications on both GPU and GPU cores while satisfying performance requirements. Depending upon the performance requirements, for each concurrently executing application, the mapping process finds the appropriate number of CPU cores and operating frequencies of CPU and GPU cores, and the partitioning process identifies an efficient partitioning of the applications' threads between CPU and GPU cores. We validate the proposed approach experimentally on the Odroid-XU3 hardware platform with various mixes of applications from the Polybench benchmark suite. Additionally, a case-study is performed with a real-world application SLAMBench. Results show an average energy saving of 32% compared to existing approaches while still satisfying the performance requirements. © 2017 ACM.",Energy consumption; Heterogeneous MPSoC; OpenCL applications; Performance; Run-time management,Computer hardware; Embedded systems; Energy conservation; Energy efficiency; Energy utilization; Graphics processing unit; Mapping; Multiprocessing systems; System-on-chip; Hardware platform; Heterogeneous mpsoc; Multi processor systems; Operating frequency; Performance; Performance requirements; Runtime management; Thread partitioning; Benchmarking
GPU performance estimation using software rasterization and machine learning,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030706913&doi=10.1145%2f3126557&partnerID=40&md5=2212941d8cacf7a41d986683b8dcbb45,"This paper introduces a predictive modeling framework to estimate the performance of GPUs during presilicon design. Early-stage performance prediction is useful when simulation times impede development by rendering driver performance validation, API conformance testing and design space explorations infeasible. Our approach builds a Random Forest regression model to analyze DirectX 3D workload behavior when executed by a software rasterizer, which we have extended with a workload characterizer to collect further performance information via program counters. In addition to regression models, this work produces detailed feature rankings which can provide valuable architectural insight, and accurate performance estimates for an Intel integrated Skylake generation GPU. Our models achieve reasonable out-of-sample-error rates of 14%, with an average simulation speedup of 327x. © 2017 ACM.",GPU simulation; Predictive model; Random forest regression,Decision trees; Graphics processing unit; Learning systems; Program processors; Regression analysis; Accurate performance; Design space exploration; GPU simulation; Out-of-sample errors; Performance estimation; Predictive modeling; Random forests; Simulation speed-up; Rasterization
Predictive retransmissions for intermittently connected sensor networks with transmission diversity,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033242387&doi=10.1145%2f3092947&partnerID=40&md5=998f0bbe11cdce17a0a7af31c3e0c511,"Batteryless wireless sensor networks that rely on energy harvested from the environment often exhibit random power outages due to limitations of energy resources, which give rise to intermittent connectivity and long transmission delays. To improve the delay performance in such networks, we consider a design strategy that uses predictive retransmissions to maximize the probability of success for each transmission. This is applied to two different transmission diversity schemes: cooperative relaying over unicast routes and opportunistic routing. Performance evaluations from theoretical models and simulations are presented that show that significant gains can be achieved using the proposed approach in such networks. © 2017 ACM.",Cooperative relaying; Energy harvesting; Intermittent connectivity; Opportunistic routing; Retransmission; Wireless sensor networks,Energy harvesting; Energy resources; Outages; Relay control systems; Cooperative relaying; Design strategies; Intermittent connectivity; Opportunistic routing; Probability of success; Retransmissions; Transmission delays; Transmission diversity; Wireless sensor networks
On the design and application of Thermal Isolation Servers,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030705957&doi=10.1145%2f3126512&partnerID=40&md5=08fa71c2a4d4d8a6f82fb859be13ae29,"Recently, there has been an increasing trend towards executing real-time applications on multi-core platforms. However, this complicates the design problem, as applications running on different cores can interfere due to shared resources and mediums. In this paper, we focus on thermal interference, where a given task (τ1) heats the processor, resulting in reduced service (due to Dynamic Thermal Management (DTM)) to another task (τ2). In real-time domain, where tasks have deadline constraints, thermal interference is a substantial problem as it directly impacts the Worst Case Execution Time (WCET) of the effected application (τ2). The problem exacerbates as we move to mixed-criticality systems, where the criticality of τ2 may be greater than the criticality of τ1, complicating the certification process. In this paper, we propose a server based strategy (Thermal Isolation Server (TI Server)) which can be used to avoid thermal interference of applications. We also present a heuristic to design TI Servers to meet the timing constraints of all tasks and the thermal constraints of the system. TI Servers are time/space composable, and can be applied to a variety of task models. We also evaluate TI Servers on a hardware test-bed for validation purposes. © 2017 ACM.",Mixed-criticality systems; Thermal modelling,Criticality (nuclear fission); Certification process; Design and application; Dynamic thermal management; Mixed-criticality systems; Real-time application; Thermal interferences; Thermal modelling; Worst-case execution time; Temperature control
Modular compilation of hybrid systems for emulation and large scale simulation,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030716282&doi=10.1145%2f3126536&partnerID=40&md5=201093a6f8275d78de2c32a8710824ec,"Hybrid systems combine discrete controllers with adjoining physical processes. While many approaches exist for simulating hybrid systems, there are few approaches for their emulation, especially when the actual physical plant is not available. This paper develops the first formal framework for emulation along with a new compiler that enables large-scale (1000+ components) simulation. We propose a formalmodel called Synchronous Emulation Automaton (SEA) specifically for modular compilation and parallel execution. SEA combines Linear Time Invariant (LTI) systems with discrete mode switches and has the following semantic differences with Hybrid Automata: 1 the Ordinary Differential Equations are solved analytically and the solutions are sampled at the Worst-Case Reaction Time of the model and 2 we develop a new composition semantics, which allows individual SEAs to execute in parallel with each other. The proposed semantics eliminates: a the need for dynamic numerical solvers, and b the Zeno-phenomenon by construction. Experimental results show that process models designed using our tool (Piha) give a 3.6 times execution speedup over SimulinkR, and upto 26 times speedup on manycore architectures. © 2017 ACM.",Code generation; Emulation; Hybrid Automata; Multi-core,Automata theory; Differential equations; Hybrid systems; Program compilers; Semantics; Time switches; Code Generation; Composition semantics; Emulation; Hybrid automatons; Large scale simulations; Linear time-invariant system; Many-core architecture; Multi core; Ordinary differential equations
MC-ADAPT: Adaptive task dropping in mixed-criticality scheduling,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030703655&doi=10.1145%2f3126498&partnerID=40&md5=0045d4d8ed27529447dd22f973c10c3e,"Recent embedded systems are becoming integrated systems with components of different criticality. To tackle this, mixed-criticality systems aim to provide different levels of timing assurance to components of different criticality levels while achieving efficient resource utilization. Many approaches have been proposed to execute more lower-criticality tasks without affecting the timeliness of higher-criticality tasks. Those previous approaches however have at least one of the two limitations; i) they penalize all lower-criticality tasks at once upon a certain situation, or ii) they make the decision how to penalize lower-criticality tasks at design time. As a consequence, they under-utilize resources by imposing an excessive penalty on low-criticality tasks. Unlike those existing studies, we present a novel framework, called MC-ADAPT, that aims to minimally penalize lower-criticality tasks by fully reflecting the dynamically changing system behavior into adaptive decision making. Towards this, we propose a new scheduling algorithm and develop its runtime schedulability analysis capable of capturing the dynamic system state. Our proposed algorithm adaptively determines which task to drop based on the runtime analysis. To determine the quality of task dropping solution, we propose the speedup factor for task dropping while the conventional use of the speedup factor only evaluates MC scheduling algorithms in terms of the worst-case schedulability. We apply the speedup factor for a newly-defined task dropping problem that evaluates task dropping solution under different runtime scheduling scenarios. We derive that MC-ADAPT has a speedup factor of 1.619 for task drop. This implies that MC-ADAPT can behave the same as the optimal scheduling algorithm with optimal task dropping strategy does under any runtime scenario if the system is sped up by a factor of 1.619. © 2017 ACM.",Mixed criticality systems; Processor speedup factor; Real-time scheduling,Criticality (nuclear fission); Decision making; Drops; Embedded systems; Quality control; Real time systems; Scheduling; Adaptive decision making; Mixed criticalities; Mixed-criticality systems; Optimal scheduling algorithm; Real - time scheduling; Resource utilizations; Schedulability analysis; Speed-up factors; Scheduling algorithms
Demystifying soft-error mitigation by control-flow checking - A new perspective on its effectiveness,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030697676&doi=10.1145%2f3126503&partnerID=40&md5=c0435d2d17779511618b2566b4bfba67,"Soft errors are a challenging and urging problem in the domain of safety-critical embedded systems. For decades, checking schemes have been investigated and improved to mitigate soft-error effects for the class of control-flow faults, with current industrial standards strongly recommending their use. However, reality looks different: Taking a systems perspective, we implemented four representative Control-Flow Checking (CFC) schemes and put them through their paces in 396 fault-injection campaigns. In contrast to previous work, which typically relied on probability-based vulnerability metrics, we accounted for the influence of memory and time overheads on the fault-space dimensions and applied those in full-scan fault injections. This change in procedure alone severely degraded the perceived effectiveness of CFC. In addition, we expanded the perspective to data-flow faults and their influence on the overall susceptibility, an aspect that so far has been largely ignored. Our results suggest that, without accompanying measures, any improvement regarding control-flow faults is dominated by the increase in data faults caused by the increased attack surface in terms of memory and runtime overhead. Moreover, CFC performance less depended on the detection capabilities than on general aspects of the concrete binary compilation and execution. In conclusion, incorporating CFC is not as straightforward as often assumed and the vulnerability of systems with hardened control-flow may in many cases even be increased by the schemes themselves. © 2017 ACM.",Absolute-failurecount metrics; CFC; CFCSS; Control-flow checking; Fault-coverage; Fault-injection experiments; Reliability metrics; Soft error mitigation; Software-based fault tolerance; YACCA,Chlorofluorocarbons; Error correction; Errors; Fault tolerance; Radiation hardening; Safety engineering; Software reliability; Software testing; Absolute-failurecount metrics; CFCSS; Control flow checking; Fault coverages; Fault injection; Reliability metrics; Soft error mitigations; YACCA; Embedded systems
A see-through-wall system for device-free human motion sensing based on battery-free RFID,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033211763&doi=10.1145%2f3055515&partnerID=40&md5=0a7e23d853000982535453dd8615307d,"A see-through-wall system can be used in life detection, military fields, elderly people surveillance. and gaming. The existing systems are mainly based on military devices, customized signals or pre-deployed sensors inside the room, which are very expensive and inaccessible for general use. Recently, a low-cost RFID technology has gained a lot of attention in this field. Since phase estimates of a battery-free RFID tag collected by a commercial off-the-shelf (COTS) RFID reader are sensitive to external interference, the RFID tag could be regarded as a battery-free sensor that detects reflections off targeted objects. The existing RFID-based system, however, needs to first learn the environment of the empty room beforehand to separate reflections off the tracked target. Besides, it can only track low-speed metal objects with high-positioning accuracy. Since the human body with its complex surface has a weaker ability to reflect radio frequency (RF) signals than metal objects, a battery-free RFID tag can capture only a subset of the reflections off the human body. To address these challenges, a RFID-based human motion sensing technology, called RF-HMS, is presented to track device-free human motion through walls. At first, we construct transfer functions of multipath channel based on phase and RSSI measurements to eliminate device noise and reflections off static objects like walls and furniture without learning the environment of the empty room before. Then a tag planar array is grouped by many battery-free RFID tags to improve the sensing performance. RF-HMS combines reflections from each RFID tag into a reinforced result. On this basis, we extract phase shifts to detect the absence or presence of any moving persons and further derive the reflections off a single moving person to identify his/her forward or backward motion direction. The results show that RF-HMS can effectively detect the absence or presence of moving persons with 100% accuracy and keep a high accuracy of more than 90% to track human motion directions. © 2017 ACM.",Motion detection; Multipath channels; Radio frequency identification; RF signals; Transfer functions,Electric batteries; Metal implants; Multipath propagation; Object detection; Radio waves; Target tracking; Transfer functions; Commercial off-the shelves; External interference; Human motion sensing; Motion detection; Positioning accuracy; Radiofrequency signals; RF signal; Sensing performance; Radio frequency identification (RFID)
Testing cyber-physical systems through Bayesian optimization,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030709248&doi=10.1145%2f3126521&partnerID=40&md5=62b10df76adf3381407d157950ae52ad,"Many problems in the design and analysis of cyber-physical systems (CPS) reduce to the following optimization problem: given a CPS which transforms continuous-time input traces in ℝm to continuous-time output traces in ℝn and a cost function over output traces, find an input trace which minimizes the cost. Cyber-physical systems are typically so complex that solving the optimization problem analytically by examining the system dynamics is not feasible. We consider a black-box approach, where the optimization is performed by testing the input-output behaviour of the CPS. We provide a unified, tool-supported methodology for CPS testing and optimization. Our tool is the first CPS testing tool that supports Bayesian optimization. It is also the first to employ fully automated dimensionality reduction techniques. We demonstrate the potential of our tool by running experiments on multiple industrial case studies. We compare the effectiveness of Bayesian optimization to state-of-the-art testing techniques based on CMA-ES and Simulated Annealing. © 2017 ACM.",Bayesian optimization; Black-box optimization; CMA-ES; Cyber-physical systems; Dimensionality reduction; Gaussian processes; Simulated annealing; Testing,Continuous time systems; Cost benefit analysis; Cost functions; Cyber Physical System; Embedded systems; Optimization; Problem solving; Simulated annealing; Statistical tests; Testing; Bayesian optimization; Black-box optimization; Cyber-physical systems (CPS); Dimensionality reduction; Dimensionality reduction techniques; Gaussian Processes; Industrial case study; Optimization problems; Black-box testing
Low-cost memory fault tolerance for IoT devices,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030702216&doi=10.1145%2f3126534&partnerID=40&md5=3e14d3b11a967f50a80262e6c13200db,"IoT devices need reliable hardware at low cost. It is challenging to efficiently cope with both hard and soft faults in embedded scratchpad memories. To address this problem, we propose a two-step approach: FaultLink and Software-Defined Error-Localizing Codes (SDELC). FaultLink avoids hard faults found during testing by generating a custom-tailored application binary image for each individual chip. During software deploymenttime, FaultLink optimally packs small sections of program code and data into fault-free segments of the memory address space and generates a custom linker script for a lazy-linking procedure. During run-time, SDELC dealswith unpredictable soft faults via novel and inexpensive Ultra-Lightweight Error-Localizing Codes (UL-ELCs). These require fewer parity bits than single-error-correcting Hamming codes. Yet our UL-ELCs are more powerful than basic single-error-detecting parity: they localize single-bit errors to a specific chunk of a codeword. SDELC then heuristically recovers from these localized errors using a small embedded C library that exploits observable side information (SI) about the application's memory contents. SI can be in the form of redundant data (value locality), legal/illegal instructions, etc. Our combined FaultLink+SDELC approach improves min-VDD by up to 440 mV and correctly recovers from up to 90% (70%) of random single-bit soft faults in data (instructions) with just three parity bits per 32-bit word. © 2017 ACM.",Approximate computing; Defects; ECC; Fault tolerance; IoT; Scratchpad memory; Soft errors,Binary images; Codes (symbols); Defects; Fault tolerance; Internet of things; Memory architecture; Multiprocessing systems; Radiation hardening; Application binaries; Approximate computing; Memory address space; Memory fault tolerance; Scratch pad memory; Soft error; Two-step approach; Ultra lightweights; Errors
Harvest energy from thewater: A self-sustained wireless waterquality sensing system,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033220474&doi=10.1145%2f3047646&partnerID=40&md5=6c26f029f67a0a1fa88a9b02d32fa1ff,"Water quality data is incredibly important and valuable, but its acquisition is not always trivial. A promising solution is to distribute a wireless sensor network in water to measure and collect the data; however, a drawback exists in that the batteries of the system must be replaced or recharged after being exhausted. To mitigate this issue, we designed a self-sustained water quality sensing system that is powered by renewable bioenergy generated from microbial fuel cells (MFCs). MFCs collect the energy released from native magnesium oxidizing microorganisms (MOMs) that are abundant in natural waters. The proposed energyharvesting technology is environmentally friendly and can provide maintenance-free power to sensors for several years. Despite these benefits, an MFC can only provide microwatt-level power that is not sufficient to continuously power a sensor. To address this issue, we designed a power management module to accumulate energy when the input voltage is as low as 0.33V. We also proposed a radio-frequency (RF) activation technique to remotely activate sensors that otherwise are switched off in default.With this innovative technique, a sensor's energy consumption in sleep mode can be completely avoided. Additionally, this design can enable on-demand data acquisitions from sensors.We implement the proposed system and evaluate its performance in a stream. In 3-month field experiments, we find the system is able to reliably collect water quality data and is robust to environment changes. © 2017 ACM.",Energy harvesting; Microbial fuel cell; Power management; Radiofrequency (RF) activation; Water quality monitoring,Chemical activation; Data acquisition; Energy harvesting; Energy management; Energy utilization; Fuel cells; Power management; Water quality; Wireless sensor networks; Activation techniques; Environment change; Innovative techniques; Maintenance free; Microbial fuel cells (MFCs); Radio frequencies; Water quality data; Water quality monitoring; Microbial fuel cells
Timestamp Temporal Logic (TTL) for testing the timing of Cyber-Physical Systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030695910&doi=10.1145%2f3126510&partnerID=40&md5=52b06ce3aff3d63a6a6c1efe229755a9,"In order to test the performance and verify the correctness of Cyber-Physical Systems (CPS), the timing constraints on the system behavior must be met. Signal Temporal Logic (STL) can efficiently and succinctly capture the timing constraints of a given system model. However, many timing constraints on CPS are more naturally expressed in terms of events on signals. While it is possible to specify event-based timing constraints in STL, such statements can quickly become long and arcane in even simple systems. Timing constraints for CPS, which can be large and complex systems, are often associated with tolerances, the expression of which can make the timing constraints even more cumbersome using STL. This paper proposes a new logic, Timestamp Temporal Logic (TTL), to provide a definitional extension of STL that more intuitively expresses the timing constraints of distributed CPS. TTL also allows for a more natural expression of timing tolerances. Additionally, this paper outlines a methodology to automatically generate logic code and programs to monitor the expressed timing constraints. Since our TTL monitoring logic evaluates the timing constraints using only the timestamps of the required events on the signal, the TTL monitoring logic has significantly less memory footprint when compared to traditional STL monitoring logic, which stores the signal value at the required sampling frequency. The key contribution of this paper is a scalable approach for online monitoring of the timing constraints. We demonstrate the capabilities of TTL and our methodology for online monitoring of TTL constraints on two case studies: 1) Synchronization and phase control of two generators and, 2) Simultaneous image capture using distributed cameras for 3D image reconstruction. © 2017 ACM.",CPS; Cyber-physical systems; Real-time systems; Safety critical systems; Time testing; Timing constraints; Verification,Computer circuits; Cyber Physical System; Embedded systems; Image reconstruction; Interactive computer systems; Partial discharges; Real time systems; Safety engineering; Safety testing; Timing circuits; Transistor transistor logic circuits; Verification; 3D image reconstruction; Cyber-physical systems (CPS); Event-based timing; Online monitoring; Safety critical systems; Sampling frequencies; Scalable approach; Timing constraints; Temporal logic
Weakly hard schedulability analysis for fixed priority scheduling of periodic real-time tasks,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030705981&doi=10.1145%2f3126497&partnerID=40&md5=dc14f282964fc70c461e37a198098173,"The hard deadline model is very popular in real-time research, but is representative or applicable to a small number of systems. Many applications, including control systems, are capable of tolerating occasional deadline misses, but are seriously compromised by a repeating pattern of late terminations. The weakly hard real-time model tries to capture these requirements by analyzing the conditions that guarantee that a maximum number of deadlines can be possibly missed in any set of consecutive activations. We provide a new weakly hard schedulability analysis method that applies to constrained-deadline periodic real-time systems scheduled with fixed priority and without knowledge of the task activation offsets. The analysis is based on a Mixed Integer Linear Programming (MILP) problem formulation; it is very general and can be adapted to include the consideration of resource sharing and activation jitter. A set of experiments conducted on an automotive engine control application and randomly generated tasksets show the applicability and accuracy of the proposed technique. © 2017 ACM.",Activation jitter; Periodic real-time tasks; Resource sharing; Schedulability analysis; Weakly hard real-time systems,Activation analysis; Chemical activation; Integer programming; Interactive computer systems; Jitter; Time sharing systems; Automotive engine control; Fixed priority scheduling; Mixed-integer linear programming; Real-time tasks; Resource sharing; Schedulability analysis; Weakly hard real-time; Weakly hard real-time systems; Real time systems
Antlab: A multi-robot task server,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030716751&doi=10.1145%2f3126513&partnerID=40&md5=c7111679e8a037e21d918d15501abbff,"We present Antlab, an end-to-end system that takes streams of user task requests and executes them using collections of robots. In Antlab, each request is specified declaratively in linear temporal logic extended with quantifiers over robots. The user does not program robots individually, nor know how many robots are available at any time or the precise state of the robots. The Antlab runtime system manages the set of robots, schedules robots to perform tasks, automatically synthesizes robot motion plans from the task specification, and manages the co-ordinated execution of the plan. We provide a constraint-based formulation for simultaneous task assignment and plan generation for multiple robots working together to satisfy a task specification. In order to scalably handle multiple concurrent tasks, we take a separation of concerns view to plan generation. First, we solve each planning problem in isolation, with an ""ideal world"" hypothesis that says there are no unspecified dynamic obstacles or adversarial environment actions. Second, to deal with imprecisions of the real world, we implement the plans in receding horizon fashion on top of a standard robot navigation stack. The motion planner dynamically detects environment actions or dynamic obstacles from the environment or from other robots and locally corrects the ideal planned path. It triggers a re-planning step dynamically if the current path deviates from the planned path or if planner assumptions are violated. We have implemented Antlab as a C++ and Python library on top of robots running on ROS, using SMT-based and AI planning-based implementations for task and path planning. We evaluated Antlab both in simulation as well as on a set of TurtleBot robots. We demonstrate that it can provide a scalable and robust infrastructure for declarative multi-robot programming. © 2017 ACM.",Cyber-physical systems; Multi-robot systems; Planning; Programming abstractions for robotics,C++ (programming language); Computer programming; Cyber Physical System; Embedded systems; Industrial robots; Motion planning; Multipurpose robots; Planning; Robots; Specifications; Technology transfer; Adversarial environments; Dynamic obstacles; End-to-end systems; Linear temporal logic; Multi-robot systems; Programming abstractions; Separation of concerns; Task specifications; Robot programming
Flexible PV-cell modeling for energy harvesting in wearable IoT applications,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030702749&doi=10.1145%2f3126568&partnerID=40&md5=907efa8f17cfa0462849d528714e8042,"Wearable devices with sensing, processing and communication capabilities have become feasible with the advances in internet-of-things (IoT) and low power design technologies. Energy harvesting is extremely important for wearable IoT devices due to size and weight limitations of batteries. One of the most widely used energy harvesting sources is photovoltaic cell (PV-cell) owing to its simplicity and high output power. In particular, flexible PV-cells offer great potential for wearable applications. This paper models, for the first time, how bending a PV-cell significantly impacts the harvested energy. Furthermore, we derive an analytical model to quantify the harvested energy as a function of the radius of curvature. We validate the proposed model empirically using a commercial PV-cell under a wide range of bending scenarios, light intensities and elevation angles. Finally, we show that the proposed model can accelerate maximum power point tracking algorithms and increase the harvested energy by up to 25.0%. © 2017 ACM.",Flexible hybrid electronics (FHE); MPPT; Power estimation; PV-cell model; Wearable IoT devices,Cells; Cytology; Electric power supplies to apparatus; Energy harvesting; Flexible electronics; Maximum power point trackers; Photoelectrochemical cells; Photovoltaic cells; Wearable technology; Communication capabilities; Flexible hybrid electronics (FHE); Internet of Things (IOT); Maximum Power Point Tracking algorithms; Power estimations; Pv cells; Wearable applications; Wearable IoT devices; Internet of things
CAMsure: Secure content-addressable memory for approximate search,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030720320&doi=10.1145%2f3126547&partnerID=40&md5=279024f30720deb943f9fb88b0134683,"We introduce CAMsure, the first realization of secure Content Addressable Memory (CAM) in the context of approximate search using near-neighbor algorithms. CAMsure provides a lightweight solution for practical secure (approximate) search with a minimal drop in the accuracy of the search results. CAM has traditionally been used as a hardware search engine that explores the entire memory in a single clock cycle. However, there has been little attention to the security of the data stored in CAM. Our approach stores distance-preserving hash embeddings within CAM to ensure data privacy. The hashing method provides data confidentiality while preserving similarity in the sense that a high resemblance in the data domain is translated to a small Hamming distance in the hash domain. Consequently, the objective of near-neighbor search is converted to approximate lookup table search which is compatible with the realizations of emerging content addressable memories. Our methodology delivers on average two orders of magnitude faster response time compared to RAM-based solutions that preserve the privacy of data owners. © 2017 ACM.",Approximate search; Content-addressable memory; In-memory computation; Near-neighbor search; Privacy-Preserving computing,Data privacy; Hamming distance; Memory architecture; Random access storage; Search engines; Security of data; Table lookup; Telecommunication industry; Approximate search; Data confidentiality; Hashing method; Memory computations; Near neighbor searches; Orders of magnitude; Privacy preserving; Single-clock-cycle; Associative storage
Efficient control-flow subgraph matching for detecting hardware trojans in RTL models,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030721258&doi=10.1145%2f3126552&partnerID=40&md5=ca131b53a777676486c56689148b1464,"Only few solutions for Hardware Trojan (HT) detection work at Register-Transfer Level (RTL), thus delaying the identification of possible security issues at lower abstraction levels of the design process. In addition, the most of existing approaches work only for specific kinds of HTs. To overcome these limitations, we present a verification approach that detects different types of HTs in RTL models by exploiting an efficient controlflow subgraph matching algorithm. The prototypes of HTs that can be detected are modelled in a library by using Control-Flow Graphs (CFGs) that can be parametrised and extended to cover several variants of Trojan patterns. Experimental results show that our approach is effective and efficient in comparison with other state-of-the-art solutions. © 2017 ACM.",Control-flowsub-graph matching; Hardware trojan detection; RTL models,Flow graphs; Hardware; Malware; Pattern matching; Abstraction level; Control flow graphs; Efficient control; Graph matchings; Hardware Trojan detection; Register transfer level; State of the art; Subgraph matching; Hardware security
R3: Reliable over-the-air reprogramming on computational RFIDs,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032980375&doi=10.1145%2f3070720&partnerID=40&md5=68ee9f6d9ff3a578d15a3bc85510456c,"Computational Radio Frequency Identification (CRFID) tags operate solely on harvested energy and have emerged as viable platforms for a variety of ubiquitous sensing and computation applications. Due to their battery-less nature, these tags can be permanently deployed in hard-to-reach places where the possibility of tag access is eliminated. In such scenarios, maintaining and upgrading the tag's firmware becomes infeasible because programming tools, including wired interface and PC-based software, are required to erase, modify, or reprogram the microcontroller unit's memory. Such limitations necessitate the demand for an over-the-air (OTA) scheme, which can wirelessly reprogram or upgrade the firmware in CRFID tags. In this article, we present R3-a reliable OTA reprogramming scheme that is compliant with EPC protocol and requires no hardware upgrade to RFID reader or CRFID tag. We demonstrate our scheme on three platforms, which include both software-defined as well as chip-based CRFID tags, that is, WISP5.1 and Optimized WISP (Opt-WISP), and Spider tag, respectively. The selection also includes both the FLASH- and FRAM-based microcontrollers. We extensively evaluate our scheme in terms of several metrics, including overall system delay, time and energy overhead, and success rate in line with interrogation range. We foresee our endeavor to offer the viability of OTA reprogramming and firmware upgrade for CRFID tokens under practical situations. © 2017 ACM.",Computational RFID; EPC; firmware upgrade; OTA reprogramming,Ferroelectric devices; Microcontrollers; Radio frequency identification (RFID); Computational rfid; Energy overheads; Firmware upgrades; Microcontroller unit; OTA reprogramming; Over-the-air reprogramming; PC-based software; Programming tools; Firmware
Formal verification of a timing enforcer implementation,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030692775&doi=10.1145%2f3126517&partnerID=40&md5=0924e13a259327b45a09bc97c596d9c1,"A timing enforcer is a scheduler that not only allocates CPU cycles to threads, but also uses timers to enforce time budgets. An approach for verifying safety properties of timing enforcers at the source code level is presented. We assume that the enforcer is implemented as a set of ""enforcer"" functions that are executed atomically on critical system-level events, such as the arrival and departure of jobs, and triggering of timers. The key idea is to express the safety property as an invariant, and prove that it is inductive across all the enforcer functions. A formal semantics of timing enforcers is presented, including the semantics of functions used to read the system clock and set timers. Using this semantics, the verification approach is presented, and its soundness proved. Further, the approach also takes into consideration the periodicity of tasks. It is validated by proving the correctness of the enforcement of CPU cycle budgets for tasks by the Zero-Slack Rate Monotonic (zsrm) scheduler, which is implemented in C as a Linux kernel module. The inductiveness of the necessary zsrm invariants is proved by expressing them as function contracts using the acsl specification language, and verifying the contracts using the frama-c tool. © 2017 ACM.",Cyber-physical systems; Real-time scheduler; Software verification,Budget control; Computer operating systems; Cyber Physical System; Embedded systems; Formal methods; Formal verification; Real time systems; Scheduling; Semantics; Specification languages; Timing circuits; Verification; Critical systems; Formal Semantics; Linux kernel; Rate-monotonic; Real-time schedulers; Safety property; Software verification; System clock; C (programming language)
Compositional relational abstraction for nonlinear hybrid systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030703040&doi=10.1145%2f3126522&partnerID=40&md5=d46be78b19a098b1c7f103ed97476c57,"We propose techniques to construct abstractions for nonlinear dynamics in terms of relations expressed in linear arithmetic. Such relations are useful for translating the closed loop verification problem of control software with continuous-time, nonlinear plant models into discrete and linear models that can be handled by efficient software verification approaches for discrete-time systems. We construct relations using Taylor model based flowpipe construction and the systematic composition of relational abstractions for smaller components. We focus on developing efficient schemes for the special case of composing abstractions for linear and nonlinear components. We implement our ideas using a relational abstraction system, using the resulting abstraction inside the verification tool NuXMV, which implements numerous SAT/SMT solver-based verification techniques for discrete systems. Finally, we evaluate the application of relational abstractions for verifying properties of time triggered controllers, comparing with the Flow∗ tool. We conclude that relational abstractions are a promising approach towards nonlinear hybrid system verification, capable of proving properties that are beyond the reach of tools such as Flow∗. At the same time, we highlight the need for improvements to existing linear arithmetic SAT/SMT solvers to better support reasoning with large relational abstractions. © 2017 ACM.",Bounded model checking; Hybrid systems; Nonlinear systems; Relational abstraction; SMT,Abstracting; Closed loop control systems; Continuous time systems; Digital control systems; Hybrid systems; Model checking; Nonlinear systems; Surface mount technology; Verification; Bounded model checking; Discrete - time systems; Nonlinear components; Nonlinear hybrid systems; Relational abstraction; Software verification; Verification problems; Verification techniques; Discrete time control systems
A study of dynamic phase adaptation using a Dynamic Multicore Processor,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030674757&doi=10.1145%2f3126523&partnerID=40&md5=e39340a9fe13e07d2e965af160078376,"Heterogeneous processors such as ARM's big.LITTLE have become popular for embedded systems. They offer a choice between running workloads on a high performance core or a low-energy core leading to increased energy efficiency. However, the core configurations are fixed at design time which offers a limited amount of adaptation. Dynamic Multicore Processors (DMPs) bridge the gap between homogeneous and fully reconfigurable systems. Cores can fuse dynamically to adapt the computational resources to the needs of differentworkloads. There exists multiple examples of DMPs in the literature, yet the focus has mainly been on static partitioning. This paper conducts the first thorough study of the potential for dynamic reconfiguration of DMPs at runtime. We study how performance varies with static partitioning and what software optimizations are required to achieve high performance. We show that energy consumption is reduced considerably when adapting the number of cores to program phases, and introduce a simple online model which predicts the optimal number of cores to use to minimize energy consumption while maintaining high performance. Using the San Diego Vision Benchmark Suite as a use case, the dynamic scheme leads to ∼40% energy savings on average without decreasing performance. © 2017 ACM.",Dynamic multicore processor; Limit study,Benchmarking; Dynamic models; Embedded systems; Energy conservation; Energy efficiency; Energy utilization; Structural design; Computational resources; Dynamic re-configuration; Fully reconfigurable systems; Heterogeneous processors; Limit study; Multi-core processor; Software optimization; Static partitioning; Multicore programming
FlashKV: Accelerating KV performance with open-channel SSDs,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030679526&doi=10.1145%2f3126545&partnerID=40&md5=1a116ae97ce1b3e8fb5e613d12ae4e08,"As the cost-per-bit of solid state disks is decreasing quickly, SSDs are supplanting HDDs in many cases, including the primary storage of key-value stores. However, simply deploying LSM-tree-based key-value stores on commercial SSDs is inefficient and induces heavy write amplification and severe garbage collection overhead under write-intensive conditions. The main cause of these critical issues comes from the triple redundant management functionalities lying in the LSM-tree, file system and flash translation layer, which block the awareness between key-value stores and flash devices. Furthermore, we observe that the performance of LSM-tree-based key-value stores is improved little by only eliminating these redundant layers, as the I/O stacks, including the cache and scheduler, are not optimized for LSM-tree's unique I/O patterns. To address the issues above, we propose FlashKV, an LSM-tree based key-value store running on openchannel SSDs. FlashKV eliminates the redundant management and semantic isolation by directly managing the raw flash devices in the application layer. With the domain knowledge of LSM-tree and the open-channel information, FlashKV employs a parallel data layout to exploit the internal parallelism of the flash device, and optimizes the compaction, caching and I/O scheduling mechanisms specifically. Evaluations show that FlashKV effectively improves system performance by 1.5× to 4.5× and decreases up to 50% write traffic under heavy write conditions, compared to LevelDB. © 2017 ACM.",Application-managed flash; Hardware-software co-design; LSM-tree-based key-value store; Open-channel SSD,Application programs; Digital storage; Hardware-software codesign; Scheduling; Semantics; Application layers; Co-designs; Flash translation layer; Key-value stores; Management functionality; Open channels; Scheduling mechanism; Write amplifications; Trees (mathematics)
An inexact ultra-low power bio-signal processing architecture with lightweight error recovery,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030674229&doi=10.1145%2f3126565&partnerID=40&md5=fcf233b23437ec693e1ec0a57d93f8f5,"The energy efficiency of digital architectures is tightly linked to the voltage level (Vdd) at which they operate. Aggressive voltage scaling is therefore mandatory when ultra-low power processing is required. Nonetheless, the lowest admissible Vdd is often bounded by reliability concerns, especially since static and dynamic non-idealities are exacerbated in the near-threshold region, imposing costly guard-bands to guarantee correctness under worst-case conditions. A striking alternative, explored in this paper, waives the requirement for unconditional correctness, undergoing more relaxed constraints. First, after a run-time failure, processing correctly resumes at a later point in time. Second, failures induce a limited Quality-of-Service (QoS) degradation. We focus our investigation on the practical scenario of embedded bio-signal analysis, a domain in which energy efficiency is key, while applications are inherently error-tolerant to a certain degree. Targeting a domainspecific multi-core platform, we present a study of the impact of inexactness on application-visible errors. Then, we introduce a novel methodology to manage them, which requires minimal hardware resources and a negligible energy overhead. Experimental evidence show that, by tolerating 900 errors/hour, the resulting inexact platform can achieve an efficiency increase of up to 24%, with a QoS degradation of less than 3%. © 2017 ACM.",Inexact computing; Low-power architectural optimization; Wireless body sensor nodes,Energy efficiency; Errors; Quality of service; Sensor nodes; Signal processing; Signal reconstruction; Voltage scaling; Aggressive voltage scaling; Bio-signal processing; Digital architecture; Experimental evidence; Inexact computing; Low Power; Multi-core platforms; Wireless body sensor nodes; Computer architecture
Timing analysis of synchronous programs using WCRT algebra: Scalability through abstraction,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030659642&doi=10.1145%2f3126520&partnerID=40&md5=fe2378dc474f9a2dc2047492e1773909,"Synchronous languages are ideal for designing safety-critical systems. Static Worst-Case Reaction Time (WCRT) analysis is an essential component in the design flow that ensures the real-time requirements are met. There are a few approaches for WCRT analysis, and the most versatile of all is explicit path enumeration. However, as synchronous programs are highly concurrent, techniques based on this approach, such as model checking, suffer from state explosion as the number of threads increases. One observation on this problem is that these existing techniques analyse the program by enumerating a functionally equivalent automaton while WCRT is a non-functional property. This mismatch potentially causes algorithm-induced state explosion. In this paper, we propose a WCRT analysis technique based on the notion of timing equivalence, expressed using WCRT algebra. WCRT algebra can effectively capture the timing behaviour of a synchronous program by converting its intermediate representation Timed Concurrent Control Flow Graph (TCCFG) into a Tick Cost Automaton (TCA), a minimal automaton that is timing equivalent to the original program. Then the WCRT is computed over the TCA. We have implemented our approach and benchmarked it against state-of-the-art WCRT analysis techniques. The results show that the WCRT algebra is 3.5 times faster on average than the fastest published technique. © 2017 ACM.",Synchronous languages; Timing algebra; WCRT analysis,Algebra; Data flow analysis; Flow graphs; Model checking; Safety engineering; Timing circuits; Analysis techniques; Intermediate representations; Non functional properties; Real time requirement; Safety critical systems; Synchronous languages; Synchronous program; WCRT analysis; Concurrency control
A novel emulation model of the cardiac conduction system,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030676649&doi=10.1145%2f3126542&partnerID=40&md5=6613c0fd25105f901074defc6a8a29d3,"Models of the cardiac conduction system are usually at two extremes: (1) high fidelity models with excellent precision but lacking a real-time response for emulation (hardware in the loop simulation); or (2) models amenable for emulation, but that do not exhibit appropriate dynamic response, which is necessary for arrhythmia susceptibility. We introduce two abstractions to remedy the situation. The first abstraction is a new cell model, which is a semi-linear hybrid automata. The proposed model is as computationally efficient as current state-of-the-art cell models amenable for emulation. Yet, unlike these models, it is also able to capture the dynamic response of the cardiac cell like the higher-fidelity models. The second abstraction is the use of smooth-tokens to develop a new path model, connecting cells, which is efficient in terms of memory consumption. Moreover, the memory requirements of the path model can be statically bounded and are invariant to the emulation step size. Results show that the proposed semi-linear abstraction for the cell reduces the execution time by up to 44%. Furthermore, the smooth-tokens based path model reduces the memory consumption by 40 times when compared to existing path models. This paves the way for the emulation of complex cardiac conduction systems, using hardware code-generators. © 2017 ACM.",Hybrid automata; Medical devices; Model-in-the-loop; Pacemaker; Validation,Abstracting; Automata theory; Biomedical equipment; Cells; Cytology; Dynamic response; Hardware; Pacemakers; Traction (friction); Computationally efficient; Hardware in-the-loop simulation; High fidelity models; Hybrid automatons; Linear hybrid automata; Medical Devices; Model in the loops; Validation; Heart
An efficient WCET-aware instruction scheduling and register allocation approach for clustered VLIW processors,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030674587&doi=10.1145%2f3126524&partnerID=40&md5=986d4b978494d1f2f66c331790943ce7,"In real-time embedded system design, one major goal is to construct a feasible schedule. Whether a feasible schedule exists depends on the Worst-Case Execution Time (WCET) of each task. Consequently, it is important to minimize the WCET of each task. We investigate the problem of instruction scheduling and register allocation for a program executed on a clustered Very Long Instruction Word (VLIW) processor such that the WCET of the program is minimized, and propose a novel, unified instruction scheduling and register allocation heuristic approach. Our heuristic approach is underpinned by a set of novel techniques, including spanning graph-based WCET-aware live range splitting, WCET-aware dynamic register pressure control, WCET-aware basic block prioritization for performing integrated instruction scheduling and register allocation, and WCET-aware spill code handling. We have implemented our approach in Trimaran 4.0, and compared it with the state-of-the-art approach by using a set of 20 benchmarks. The experimental results show that our approach achieves the maximum WCET improvement of 29.61% and the average WCET improvement of 10.23%, respectively. © 2017 ACM.",Clustered VLIW processor; Instruction scheduling; Live range splitting; Register allocation; Worst-case execution time,Embedded software; Embedded systems; Graphic methods; Heuristic methods; Program compilers; Program processors; Scheduling; Clustered VLIW; Instruction scheduling; Live range splitting; Register allocation; Worst-case execution time; Very long instruction word architecture
A fast method to compute disjunctive quadratic invariants of numerical programs,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030671671&doi=10.1145%2f3126502&partnerID=40&md5=3292f9605968095903ea1a8d9e17281d,"We introduce a new method to compute non-convex invariants of numerical programs, which includes the class of switched affine systems with affine guards. We obtain disjunctive and non-convex invariants by associating different partial execution traces with different ellipsoids. A key ingredient is the solution of non-monotone fixed points problems over the space of ellipsoids with a reduction to small size linear matrix inequalities. This allows us to analyze instances that are inaccessible in terms of expressivity or scale by earlier methods based on semi-definite programming. © 2017 ACM.",Abstract interpretation; Hybrid and switched linear systems; Invariant generation; Stability; Static analysis,Convergence of numerical methods; Linear matrix inequalities; Linear systems; Static analysis; Abstract interpretations; Invariant generations; Numerical programs; Partial executions; Quadratic invariant; Semi-definite programming; Switched affine systems; Switched linear system; Numerical methods
Round-trip DRAM access fairness in 3D NoC-based many-core systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030680902&doi=10.1145%2f3126561&partnerID=40&md5=3ec5401575bb9fa71ed0333de1076cdd,"In 3D NoC-based many-core systems, DRAM accesses behave differently due to their different communication distances and the latency gap of different DRAM accesses becomes bigger as the network size increases, which leads to unfair DRAM access performance among different nodes. This phenomenon may lead to high latencies for some DRAM accesses that become the performance bottleneck of the system. The paper addresses the DRAM access fairness problem in 3D NoC-based many-core systems by narrowing the latency difference of DRAM accesses as well as reducing the maximum latency. Firstly, the latency of a round-trip DRAM access is modeled and the factors causing DRAM access latency difference are discussed in detail. Secondly, the DRAM access fairness is further quantitatively analyzed through experiments. Thirdly, we propose to predict the network latency of round-trip DRAM accesses and use the predicted round-trip DRAM access time as the basis to prioritize the DRAM accesses in DRAM interfaces so that the DRAM accesses with potential high latencies can be transferred as early and fast as possible, thus achieving fair DRAM access. Experiments with synthetic and application workloads validate that our approach can achieve fair DRAM access and outperform the traditional First-Come-First-Serve (FCFS) scheduling policy and the scheduling policies proposed by reference [7] and [24] in terms of maximum latency, Latency Standard Deviation (LSD)1 and speedup. In the experiments, the maximum improvement of the maximum latency, LSD, and speedup are 12.8%, 6.57%, and 8.3% respectively. Besides, our proposal brings very small extra hardware overhead (<0.6%) in comparison to the three counterparts. © 2017 ACM.",3D networks-on-chip (NoC); DRAM access fairness; DRAM scheduling; Round-trip,Scheduling; 3D networks; Communication distance; First come first serves; Hardware overheads; Performance bottlenecks; Round trip; Scheduling policies; Standard deviation; Network-on-chip
COSMOS: Coordination of high-level synthesis and memory optimization for hardware accelerators,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030679552&doi=10.1145%2f3126566&partnerID=40&md5=79212c7227c3e9931c02b669d0fbe732,"Hardware accelerators are key to the efficiency and performance of system-on-chip (SoC) architectures. With high-level synthesis (HLS), designers can easily obtain several performance-cost trade-off implementations for each component of a complex hardware accelerator. However, navigating this design space in search of the Pareto-optimal implementations at the system level is a hard optimization task. We present COSMOS, an automatic methodology for the design-space exploration (DSE) of complex accelerators, that coordinates both HLS and memory optimization tools in a compositional way. First, thanks to the co-design of datapath and memory, COSMOS produces a large set of Pareto-optimal implementations for each component of the accelerator. Then, COSMOS leverages compositional design techniques to quickly converge to the desired trade-off point between cost and performance at the system level. When applied to the system-level design (SLD) of an accelerator for wide-area motion imagery (WAMI), COSMOS explores the design space as completely as an exhaustive search, but it reduces the number of invocations to the HLS tool by up to 14.6×. © 2017 ACM.",Design-space exploration; Hardware accelerators; High-level synthesis; Specialized hardware; System-level design,Acceleration; Coordination reactions; Distributed computer systems; Economic and social effects; Hardware; Integrated circuit design; Pareto principle; Programmable logic controllers; Search engines; System-on-chip; Systems analysis; Design space exploration; Efficiency and performance; Hardware accelerators; Memory optimization; Optimization task; Specialized hardware; System level design; Wide-area motion imageries; High level synthesis
RISE: An automated framework for real-time intelligent video surveillance on FPGA,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030665796&doi=10.1145%2f3126549&partnerID=40&md5=dad909e12c30a223de58b7a347bb3100,"This paper proposes RISE, an automated Reconfigurable framework for real-time background subtraction applied to Intelligent video SurveillancE. RISE is devised with a new streaming-based methodology that adaptively learns/updates a corresponding dictionary matrix from background pixels as new video frames are captured over time. This dictionary is used to highlight the foreground information in each video frame. A key characteristic of RISE is that it adaptively adjusts its dictionary for diverse lighting conditions and varying camera distances by continuously updating the corresponding dictionary. We evaluate RISE on natural-scene vehicle images of different backgrounds and ambient illuminations. To facilitate automation, we provide an accompanying API that can be used to deploy RISE on FPGA-based system-on-chip platforms. We prototype RISE for end-to-end deployment of three widely-adopted image processing tasks used in intelligent transportation systems: License Plate Recognition (LPR), image denoising/reconstruction, and principal component analysis. Our evaluations demonstrate up to 87-fold higher throughput per energy unit compared to the prior-art software solution executed on ARM Cortex-A15 embedded platform. © 2017 ACM.",Background subtraction; Data streaming; Intelligent video surveillance; License plate recognition; Reconfigurable computing,Automation; Distributed computer systems; Field programmable gate arrays (FPGA); Image denoising; Image processing; Image segmentation; Intelligent systems; License plates (automobile); Monitoring; Object recognition; Optical character recognition; Principal component analysis; Reconfigurable architectures; System-on-chip; Background subtraction; Data streaming; Intelligent video surveillance; License plate recognition; Reconfigurable computing; Security systems
Approximate memristive in-memory computing,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030663005&doi=10.1145%2f3126526&partnerID=40&md5=2202ce48ca87a74f7b6a5e56dfec2bba,"The bottleneck between the processing elements and memory is the biggest issue contributing to the scalability problem in computing. In-memory computation is an alternative approach that combines memory and processor in the same location, and eliminates the potential memory bottlenecks. Associative processors are a promising candidate for in-memory computation, however the existing implementations have been deemed too costly and power hungry. Approximate computing is another promising approach for energyefficient digital system designs where it sacrifices the accuracy for the sake of energy reduction and speedup in error-resilient applications. In this study, approximate in-memory computing is introduced in memristive associative processors. Two approximate computing methodologies are proposed; bit trimming and memristance scaling. Results show that the proposed methods not only reduce energy consumption of in-memory parallel computing but also improve their performance. As compared to other existing approximate computing methodologies on different architectures (e.g., CPU, GPU, and ASIC), approximate memristive in-memory computing exhibits better results in terms of energy reduction (up to 80x) and speedup (up to 20x) on a variety of benchmarks from different domains when quality degradation is limited to 10% and it confirms that memristive associative processors provide a highly-promising platform for approximate computing. © 2017 ACM.",Approximate computing; Approximate storage; Architecture; In-memory computing; Memristance scaling; Memristive associative processor (MAP); Memristor,Architecture; Associative storage; Benchmarking; Energy utilization; Green computing; Memory architecture; Systems analysis; Approximate computing; Computing methodologies; Digital system design; Memristance; Memristive associative processor (MAP); Memristor; Reduce energy consumption; Scalability problems; Computer architecture
A synchronous look at the Simulink standard library,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030661464&doi=10.1145%2f3126516&partnerID=40&md5=2b2717b5efa1167cc40ecdf2fd6a51e9,"Hybrid systems modelers like Simulink come with a rich collection of discrete-time and continuous-time blocks. Most blocks are not defined in terms of more elementary ones-and some cannot be-but are instead written in imperative code and explained informally in a reference manual. This raises the question of defining a minimal set of orthogonal programming constructs such that most blocks can be programmed directly and thereby given a specification that is mathematically precise, and whose compiled version performs comparably to handwritten code. In this paper, we showthat a fairly large set of blocks of a standard library like the one provided by Simulink can be programmed in a precise, purely functional language using stream equations, hierarchical automata, Ordinary Differential Equations (ODEs), and deterministic synchronous parallel composition. Some blocks cannot be expressed in our setting as they mix discrete-time and continuous-time signals in unprincipled ways that are statically forbidden by the type checker. The experiment is conducted in Zélus, a synchronous language that conservatively extends Lustre with ODEs to program systems that mix discrete-time and continuous-time signals. © 2017 ACM.",Block diagrams; Hybrid systems; Synchronous languages,Computer hardware description languages; Differential equations; Hybrid systems; Ordinary differential equations; Block diagrams; Continuous-time; Continuous-time signal; Program systems; Purely functional; Standard libraries; Synchronous languages; Synchronous parallel compositions; Continuous time systems
Power-temperature stability and safety analysis for multiprocessor systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030665149&doi=10.1145%2f3126567&partnerID=40&md5=1b48bb91dfbe661c5e56370add21a89f,"Modern multiprocessor system-on-chips (SoCs) integrate multiple heterogeneous cores to achieve high energy efficiency. The power consumption of each core contributes to an increase in the temperature across the chip floorplan. In turn, higher temperature increases the leakage power exponentially, and leads to a positive feedback with nonlinear dynamics. This paper presents a power-temperature stability and safety analysis technique for multiprocessor systems. This analysis reveals the conditions under which the powertemperature trajectory converges to a stable fixed point. We also present a simple formula to compute the stable fixed point and maximum thermally-safe power consumption at runtime. Hardwaremeasurements on a state-of-the-art mobile processor show that our analytical formulation can predict the stable fixed point with an average error of 2.6%. Hence, our approach can be used at runtime to ensure thermally safe operation and guard against thermal threats. © 2017 ACM.",Dynamic thermal and power management; Mobile platforms; Multi-core architectures; Power-temperature stability analysis,Electric power system stability; Electric power utilization; Energy efficiency; Fault tolerance; Feedback; Multiprocessing systems; System stability; System-on-chip; Analytical formulation; High energy efficiency; Mobile platform; Multi processor systems; Multicore architectures; Multiprocessor system on chips; Temperature increase; Temperature stability; Computer architecture
Runtime enforcement of cyber-physical systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030688077&doi=10.1145%2f3126500&partnerID=40&md5=38125280f08bb1b3860ffec72adf6bb9,"Many implantable medical devices, such as pacemakers, have been recalled due to failure of their embedded software. This motivates rethinking their design and certification processes. We propose, for the first time, an additional layer of safety by formalising the problem of run-time enforcement of implantable pacemakers. While recent work has formalised run-time enforcement of reactive systems, the proposed framework generalises existing work along the following directions: (1) we develop bi-directional enforcement, where the enforced policies depend not only on the status of the pacemaker (the controller) but also of the heart (the plant), thus formalising the run-time enforcement problem for cyber-physical systems (2) we express policies using a variant of discrete timed automata (DTA), which can cover all regular properties unlike earlier frameworks limited to safety properties, (3) we are able to ensure the timing safety of implantable devices through the proposed enforcement, and (4) we show that the DTA-based approach is efficient relative to its dense time variant while ensuring that the discretisation error is relatively small and bounded. The developed approach is validated through a prototype system implemented using the open source KIELER framework. The experiments show that the framework incurs minimal runtime overhead. © 2017 ACM.",Automata; Cyber-physical systems; Runtime enforcement; Runtime monitoring; SCCharts; Synchronous programming; Timed properties,Automata theory; Biomedical equipment; Computer system firewalls; Cyber Physical System; Embedded systems; Implants (surgical); Open source software; Pacemakers; Automata; Runtime enforcements; Runtime Monitoring; SCCharts; Synchronous programming; Timed properties; Open systems
DyPO: Dynamic Pareto-optimal configuration selection for heterogeneous MpSoCs,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030683186&doi=10.1145%2f3126530&partnerID=40&md5=ac3e2bc86e52934d1eada13784feb482,"Modern multiprocessor systems-on-chip (MpSoCs) offer tremendous power and performance optimization opportunities by tuning thousands of potential voltage, frequency and core configurations. As the workload phases change at runtime, different configurations may become optimal with respect to power, performance or other metrics. Identifying the optimal configuration at runtime is infeasible due to the large number of workloads and configurations. This paper proposes a novel methodology that can find the Pareto-optimal configurations at runtime as a function of the workload. To achieve this, we perform an extensive offline characterization to find classifiers that map performance counters to optimal configurations. Then, we use these classifiers and performance counters at runtime to choose Pareto-optimal configurations. We evaluate the proposed methodology by maximizing the performance per watt for 18 single- and multi-threaded applications. Our experiments demonstrate an average increase of 93%, 81% and 6% in performance per watt compared to the interactive, ondemand and powersave governors, respectively. © 2017 ACM.",Basic blocks; Clang; DPM; DVFS; Energy; LLVM; Logistic regression; Mobile platforms; Multi-cores; PAPI; Pareto optimization; Performance per watt; Power,Fault tolerance; Heat engines; Multiobjective optimization; System-on-chip; Basic blocks; Clang; DVFS; Energy; LLVM; Logistic regressions; Mobile platform; Multi core; PAPI; Pareto optimization; Performance per watt; Power; Pareto principle
SoftRM: Self-organized fault-tolerant resource management for failure detection and recovery in NoC based many-cores,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030662280&doi=10.1145%2f3126562&partnerID=40&md5=7a10c70d956d60616bc81cb48d177511,"Many-core systems are envisioned to leverage the ever-increasing demand for more powerful computing systems. To provide the necessary computing power, the number of Processing Elements integrated onchip increases and NoC based infrastructures are adopted to address the interconnection scalability. The advent of these new architectures surfaces the need for more sophisticated, distributed resource management paradigms, which in addition to the extreme integration scaling, make the new systems more prone to errors manifested both at hardware and software. In this work, we highlight the need for Run-Time Resource management to be enhanced with fault tolerance features and propose SoftRM, a resource management framework which can dynamically adapt to permanent failures in a self-organized, workload-aware manner. Self-organization allows the resource management agents to recover from a failure in a coordinated way by electing a new agent to replace the failed one, while workload awareness optimizes this choice according to the status of each core. We evaluate the proposed framework on Intel Single-chip Cloud Computer (SCC), a NoC based many-core system and customize it to achieve minimum interference on the resource allocation process. We showcase that its workload-aware features manage to utilize free resources in more that 90% of the conducted experiments. Comparison with relevant state-of-the-art fault tolerant frameworks shows decrease of up to 67% in the imposed overhead on application execution. © 2017 ACM.",Distributed run-time resource management; Fault tolerance; Many-core; Network-on-Chip,Fault tolerance; Fault tolerant computer systems; Natural resources management; Network-on-chip; Occupational risks; Outages; Resource allocation; Distributed resource management; Failure detection and recoveries; Many core; Resource allocation process; Resource management; Resource management agents; Resource management framework; Single-chip cloud computers; Distributed computer systems
Application-aware swapping for mobile systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030673201&doi=10.1145%2f3126509&partnerID=40&md5=9d1f3e306d9c1b6f7d924c793d0dd429,"There has been a constant demand for memory in modern mobile systems to provide users with better experience. Swapping is one of the cost-effective software solutions to provide extra usable memory by reclaiming inactive pages and improving memory utilization. However, swapping has not been actively adopted to mobile systems since it incurs a significant amount of I/O, which in fact impairs system performance as well as user experience. In this paper, we propose a novel scheme to properly harness the swapping to mobile systems. We identify that a vast amount of I/O for swapping comes from the conflict of the traditional page-level approach of the swapping and the process-level memory management scheme tailored to mobile systems. Moreover, we find out that the current victim page selection policy is not effective due to the process-level policy. To address these problems, we revise the victim selection policy to resolve the conflict and to selectively perform swapping according to the efficacy of swapping. Evaluation using a running prototype with realistic workloads indicates that the propose scheme effectively reduces the paging traffic, thereby improving user experience as well as energy consumption. © 2017 ACM.",Memory management; Mobile systems; Swapping,Energy utilization; Memory management; Memory utilization; Mobile systems; Page selection; Selection policies; Software solution; Swapping; User experience; Cost effectiveness
A structured methodology for pattern based adaptive scheduling in embedded control,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030683903&doi=10.1145%2f3126514&partnerID=40&md5=0acb63b128910cde97e626c8e79a1157,"Software implementation of multiple embedded control loops often share compute resources. The control performance of such implementations have been shown to improve if the sharing of bandwidth between control loops can be dynamically regulated in response to input disturbances. In the absence of a structured methodology for planning such measures, the scheduler may spend too much time in deciding the optimal scheduling pattern. Our work leverages well known results in the domain of network control systems and applies them in the context of bandwidth sharing among controllers. We provide techniques that may be used a priori for computing co-schedulable execution patterns for a given set of control loops such that stability is guaranteed under all possible disturbance scenarios. Additionally, the design of the control loops optimize the average case control performance by adaptive sharing of bandwidth under time varying input disturbances. © 2017 ACM.",Adaptive scheduling; Control performance; Embedded control; Schedulability analysis,Bandwidth; Closed loop control systems; Adaptive scheduling; Control performance; Embedded control; Network control systems; Optimal scheduling; Schedulability analysis; Software implementation; Time varying input; Scheduling
Probabilistic safety verification of stochastic hybrid systems using barrier certificates,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030674530&doi=10.1145%2f3126508&partnerID=40&md5=38f30340ebd262eb3737bdd8e79ba6ba,"The problem of probabilistic safety verification of stochastic hybrid systems is to check whether the probability that a given system will reach an unsafe region from certain initial states can be bounded by some given probability threshold. The paper considers stochastic hybrid systems where the behavior is governed by polynomial equalities and inequalities, as for usual hybrid systems, but the initial states follow some stochastic distributions. It proposes a new barrier certificate based method for probabilistic safety verification which guarantees the absolute safety in a infinite time horizon that is beyond the reach of existing techniques using either statistical model checking or probabilistic reachable set computation. It also gives a novel computational approach, by building and solving a constrained optimization problem coming from verification conditions of barrier certificates, to compute the lower bound on safety probabilities which can be compared with the given threshold. Experimental evidence is provided demonstrating the applicability of our approach on several benchmarks. © 2017 ACM.",Barrier certificate; Safety verification; Stochastic hybrid systems,Constrained optimization; Hybrid systems; Model checking; Optimization; Probability; Barrier certificates; Computational approach; Constrained optimi-zation problems; Safety verification; Statistical model checking; Stochastic distribution; Stochastic hybrid systems; Verification condition; Stochastic systems
Adaptive power management in solar energy harvesting sensor node using reinforcement learning,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030676791&doi=10.1145%2f3126495&partnerID=40&md5=530b30dda0a508b2fc9464e99fbcf48a,"In this paper, we present an adaptive power manager for solar energy harvesting sensor nodes. We use a simplified model consisting of a solar panel, an ideal battery and a general sensor node with variable duty cycle. Our power manager uses Reinforcement Learning (RL), specifically SARSA(λ) learning, to train itself from historical data. Once trained, we show that our power manager is capable of adapting to changes in weather, climate, device parameters and battery degradation while ensuring near-optimal performance without depleting or overcharging its battery. Our approach uses a simple but novel general reward function and leverages the use of weather forecast data to enhance performance. We show that our method achieves near perfect energy neutral operation (ENO) with less than 6% root mean square deviation from ENO as compared to more than 23% deviation that occur when using other approaches. © 2017 ACM.",IoT; Power management; Reinforcement learning; Wireless sensor nodes,Electric batteries; Energy harvesting; Energy management; Managers; Power management; Secondary batteries; Sensor nodes; Solar energy; Weather forecasting; Adaptive power management; Battery degradation; Device parameters; Historical data; Near-optimal performance; Reward function; Root mean square deviations; Wireless sensor node; Reinforcement learning
Response-time analysis for task chains with complex precedence and blocking relations,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030689606&doi=10.1145%2f3126505&partnerID=40&md5=a986b225abf7114780ac8697df8a49d5,"For the development of complex software systems, we often resort to component-based approaches that separate the different concerns, enhance verifiability and reusability, and for which microkernel-based implementations are a good fit to enforce these concepts. Composing such a system of several interacting software components will, however, lead to complex precedence and blocking relations, which must be taken into account when performing latency analysis. When modelling these systems by classical task graphs, some of these effects are obfuscated and tend to render such an analysis either overly pessimistic or even optimistic. We therefore firstly present a novel task (meta-)model that is more expressive and accurate w.r.t. these (functional) precedence and mutual blocking relations. Secondly, we apply the busy-window approach and formulate a modular response-time analysis on task-chain level suitable but not restricted to static-priority scheduled systems. We show that the conjunction of both concepts allows the calculation of reasonably tight latency bounds for scenarios not adequately covered by related work. © 2017 ACM.",Component-based software systems; Response-time analysis; Service-oriented architectures,Chains; Computer software; Computer software reusability; Information services; Reusability; Service oriented architecture (SOA); Complex software systems; Component based approach; Component-based software systems; Interacting softwares; Latency Analysis; Response-time analysis; Scheduled systems; Static priority; Response time (computer systems)
HiCH: Hierarchical fog-assisted computing architecture for healthcare IoT,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030659662&doi=10.1145%2f3126501&partnerID=40&md5=9f6dc6a273c86dc6d0ff0e743de6b61f,"The Internet of Things (IoT) paradigm holds significant promises for remote health monitoring systems. Due to their life- or mission-critical nature, these systems need to provide a high level of availability and accuracy. On the one hand, centralized cloud-based IoT systems lack reliability, punctuality and availability (e.g., in case of slow or unreliable Internet connection), and on the other hand, fully outsourcing data analytics to the edge of the network can result in diminished level of accuracy and adaptability due to the limited computational capacity in edge nodes. In this paper, we tackle these issues by proposing a hierarchical computing architecture, HiCH, for IoT-based health monitoring systems. The core components of the proposed system are 1) a novel computing architecture suitable for hierarchical partitioning and execution of machine learning based data analytics, 2) a closed-loop management technique capable of autonomous system adjustments with respect to patient's condition. HiCH benefits from the features offered by both fog and cloud computing and introduces a tailored management methodology for healthcare IoT systems. We demonstrate the efficacy of HiCH via a comprehensive performance assessment and evaluation on a continuous remote health monitoring case study focusing on arrhythmia detection for patients suffering from CardioVascular Diseases (CVDs). © 2017 ACM.",Fog computing; Hierarchical computing; Internet of things; Machine learning; MAPE-K; Remote patient monitoring,Artificial intelligence; Computer architecture; Diseases; Distributed computer systems; Fog; Health; Health care; Hierarchical systems; Information management; Learning systems; Machine components; Monitoring; Network architecture; Patient monitoring; Remote patient monitoring; Comprehensive performance assessments; Health monitoring system; Hierarchical computing; Hierarchical partitioning; Internet of thing (IOT); Management methodologies; Mape; Remote health monitoring; Internet of things
CGPredict: Embedded GPU performance estimation from single-threaded applications,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030668929&doi=10.1145%2f3126546&partnerID=40&md5=7222c6b005f59d4e919b78ab65012fe8,"Heterogeneous multiprocessor system-on-chip architectures are endowed with accelerators such as embedded GPUs and FPGAs capable of general-purpose computation. The application developers for such platforms need to carefully choose the accelerator with the maximum performance benefit. For a given application, usually, the reference code is specified in a high-level single-threaded programming language such as C. The performance of an application kernel on an accelerator is a complex interplay among the exposed parallelism, the compiler, and the accelerator architecture. Thus, determining the performance of a kernel requires its redevelopment into each accelerator-specific language, causing substantial wastage of time and effort. To aid the developer in this early design decision, we present an analytical framework CGPredict to predict the performance of a computational kernel on an embedded GPU architecture from un-optimized, single-threaded C code. The analytical approach provides insights on application characteristics which suggest further application-specific optimizations. The estimation error is as low as 2.66% (average 9%) compared to the performance of the same kernel written in native CUDA code running on NVIDIA Kepler embedded GPU. This low performance estimation error enables CGPredict to provide an early design recommendation of the accelerator starting from C code. © 2017 ACM.",Analytical model; Cross-platform prediction; GPGPU; Heterogenous platform; Mobile platform; Performance modeling,Acceleration; Analytical models; Codes (symbols); Distributed computer systems; Graphics processing unit; High level languages; Integrated circuit design; Program compilers; Program processors; System-on-chip; Cross-platform; GPGPU; Heterogenous platform; Mobile platform; Performance Model; C (programming language)
Lightweight data compression for mobile flash storage,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030672981&doi=10.1145%2f3126511&partnerID=40&md5=fc9a255f3639104267cd9221fd2bb31b,"Data compression is beneficial to flash storage lifespan. However, because the design of mobile flash storage is highly cost-sensitive, hardware compression becomes a less attractive option. This study investigates the feasibility of data compression on mobile flash storage. It first characterizes data compressibility based on mobile apps, and the analysis shows that write traffic bound for mobile storage volumes is highly compressible. Based on this finding, a lightweight approach is introduced for firmware-based data compression in mobile flash storage. The controller and flash module work in a pipelined fashion to hide the data compression overhead. Together with this pipelined design, the proposed approach selectively compresses incoming data of high compressibility, while leaving data of low compressibility to a compression-aware garbage collector. Experimental results show that our approach greatly reduced the frequency of block erase by 50.5% compared to uncompressed flash storage. Compared to unconditional data compression, our approach improved the write latency by 10.4% at a marginal cost of 4% more block erase operations. © 2017 ACM.",Compressibility; Data compression; Flash memory; Mobile device,Compressibility; Digital storage; Firmware; Flash memory; Mobile devices; Cost-sensitive; Erase operation; Flash storage; Garbage collectors; Life span; Marginal costs; Mobile apps; Mobile storage; Data compression
Improving invariant mining via static analysis,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030672741&doi=10.1145%2f3126504&partnerID=40&md5=90a7859f3145db54d7de039f97e29d99,"This paper proposes the use of static analysis to improve the generation of invariants from test data extracted from Simulink models. Previous work has shown the utility of such automatically generated invariants as a means for updating and completing system specifications; they also are useful as a means of understanding model behavior. This work shows how the scalability and accuracy of the data mining process can be dramatically improved by using information from data/control flow analysis to reduce the search space of the invariant mining and to eliminate false positives. Comparative evaluations of the process show that the improvements significantly reduce execution time and memory consumption, thereby supporting the analysis of more complex models, while also improving the accuracy of the generated invariants. © 2017 ACM.",Automated test generation; Invariant mining; Model-based development; Verification and validation,Software testing; Specifications; Static analysis; Automated test generations; Automatically generated; Comparative evaluations; Data mining process; Memory consumption; Model based development; System specification; Verification-and-validation; Data mining
An automated security-aware approach for design of embedded systems on MPSoC,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030683958&doi=10.1145%2f3126553&partnerID=40&md5=290e2204a7294df7e68b472914932b7b,"MPSoC-based embedded systems design is becoming increasingly complex. Not only do we need to satisfy multiple design objectives, we increasingly need to address potential security risks. In thiswork, we propose a security-aware systematic design approach which explores the design space, given a system-level application description, by generating potential architecture configurations of execution platform nodes that are interconnected using a NoC. We then perform automated security analysis to check the generated configurations against designer-specified security constraints. Following the analysis, we use an automated architecture configuration refinement process to generate a list of security additions that are inserted into the initial configuration so that the security constraints are satisfied. By performing this refinement on several candidate configuration options, we can explore the trade-off between resource cost and security. In this paper, we illustrate the proposed approach using a Smart Home Control System application. © 2017 ACM.",Architecture exploration; Automated; MPSoC; Security-aware,Automation; Economic and social effects; Embedded software; Embedded systems; Intelligent buildings; Multiprocessing systems; Network-on-chip; System-on-chip; Systems analysis; Application descriptions; Architecture configuration; Architecture exploration; Automated; Initial configuration; MPSoC; Security-aware; Systematic design approach; Integrated circuit design
Efficient virtual memory sharing via on-accelerator page table walking in heterogeneous embedded SoCs,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030661528&doi=10.1145%2f3126560&partnerID=40&md5=34205e72e090994dff1ebb459a875c5c,"Shared virtual memory is key in heterogeneous systems on chip (SoCs) that combine a general-purpose host processor with a many-core accelerator, both for programmability and performance. In contrast to the fullblown, hardware-only solutions predominant in modern high-end systems, lightweight hardware-software co-designs are better suited in the context of more power- and area-constrained embedded systems and provide additional benefits in terms of flexibility and predictability. As a downside, the latter solutions require the host to handle in software synchronization in case of page misses as well as miss handling. This may incur considerable run-time overheads. In this work, we present a novel hardware-software virtual memory management approach for many-core accelerators in heterogeneous embedded SoCs. It exploits an accelerator-side helper thread concept that enables the accelerator tomanage its virtualmemory hardware autonomously while operating cache-coherently on the page tables of the user-space processes of the host. This greatly reduces overhead with respect to host-side solutions while retaining flexibility. We have validated the design with a set of parameterizable benchmarks and real-world applications covering various application domains. For purely memory-bound kernels, the accelerator performance improves by a factor of 3.8 compared with host-based management and lies within 50% of a lower-bound ideal memory management unit. © 2017 ACM.",Embedded systems; Heterogeneous SoCs; Linux; Shared virtual memory; TLB management,Acceleration; Benchmarking; Computer operating systems; Embedded systems; Hardware; Hardware-software codesign; Linux; Physical addresses; System-on-chip; Heterogeneous systems; High-end systems; Many-core accelerators; Novel hardware; Programmability; Shared virtual memory; Virtual memory; Virtual memory management; Memory management units
Nucleus: Finding the sharing limit of heterogeneous cores,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030669773&doi=10.1145%2f3126544&partnerID=40&md5=bc9b603856dc9ea4fbf8f166b19df50e,"Heterogeneous multi-processors are designed to bridge the gap between performance and energy efficiency in modern embedded systems. This is achieved by pairing Out-of-Order (OoO) cores, yielding performance through aggressive speculation and latency masking, with In-Order (InO) cores, that preserve energy through simpler design. By leveraging migrations between them, workloads can therefore select the best setting for any given energy/delay envelope. However, migrations introduce execution overheads that can hurt performance if they happen too frequently. Finding the optimal migration frequency is critical to maximize energy savings while maintaining acceptable performance. We develop a simulation methodology that can 1) isolate the hardware effects of migrations from the software, 2) directly compare the performance of different core types, 3) quantify the performance degradation and 4) calculate the cost of migrations for each case. To showcase our methodology we run mibench, a microbenchmark suite, and show that migrations can happen as fast as every 100k instructions with little performance loss. We also show that, contrary to numerous recent studies, hypothetical designs do not need to share all of their internal components to be able to migrate at that frequency. Instead, we propose a feasible system that shares level 2 caches and a translation lookaside buffer that matches performance and efficiency. Our results show that there are phases comprising up to 10% that a migration to the OoO core leads to performance benefits without any additional energy cost when running on the InO core, and up to 6% of phases where a migration to the InO core can save energy without affecting performance. When considering a policy that focuses on improving the energy-delay product, results show that on average 66% of the phases can be migrated to deliver equal or better system operation without having to aggressively share the entire memory system or to revert to migration periods finer than 100k instructions. © 2017 ACM.",Embedded systems; Gem5; Heterogeneous multiprocessing; HMP; Migration; Out-of-Order; Simulation methodology,Bridges; Computer software; Energy conservation; Energy efficiency; Multiprocessing systems; Program processors; Gem5; Heterogeneous multiprocessing; Migration; Out of order; Simulation methodology; Embedded systems
User-aware frame rate management in Android smartphones,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030669809&doi=10.1145%2f3126539&partnerID=40&md5=b33474b880e31c8e0c1d1516bde1ad01,"Frame rate has a direct impact on the energy consumption of smartphones: the higher the frame rate, the higher the power consumption. Hence, reducing display refreshes will reduce the power consumption. However, it is risky to manipulate frame rate drastically as it can deteriorate user satisfaction with the device. In this work, we introduce a screen management system that controls the frame rate on smartphone displays based on a model that detects user dissatisfaction due to display refreshes. This approach is based on understanding when higher frame rates are necessary, and providing lower frame rates-thus, saving power-if the lower rate is predicted not to cause user dissatisfaction. According to the results of our first user survey with 20 participants, individuals show highly varying requirements: while some users require high frame rates for the highest satisfaction, others are equally satisfied with lower frame rates. Based on this observation, we develop a system that predicts user dissatisfaction on the runtime and either increases or decreases the maximum frame rate setting. For user dissatisfaction predictions, we have compared two different approaches: (1) static model, which uses dissatisfaction characteristics of a fixed group of people, and (2) user-specific model, which is learning only from the specific user. Our second set of experiments with 20 participants shows that users report 32% less dissatisfaction and 4% more dissatisfaction than the default Android system with user-specific and static systems, respectively. These experiments also show that, compared to the default scheme, our mechanisms reduce the power consumption of the phone by 7.2% and 1.8% on average with the user-specific and static models, respectively. © 2017 ACM.",Android; Frame rate; Frames per second (FPS); Personalized systems; Power consumption; Smartphone,Behavioral research; Electric power utilization; Energy utilization; Smartphones; Surveys; Android; Android systems; Frame rate; Frames per seconds; High frame rate; Screen management; Static systems; User satisfaction; Android (operating system)
P-BMS: A bad block management scheme in parallelized flash memory storage devices,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030670233&doi=10.1145%2f3126550&partnerID=40&md5=7726af628652839ce55e4aa0759c0763,"Flash memory is used as a main data storage medium in increasingly large areas of applications, rapidly replacing hard disk drives because of its low power consumption, fast random access, and high shock resistance. Such flash-based storage devices generally incorporate multiple flash memory chips to meet the ever growing capacity demands. Using multiple chips in a single storage device, at the same time, opens an opportunity to boost the performance based on multi-unit parallelism. However, parallel execution of multiple flash operations introduces complications when bad blocks occur, which is unavoidable due to flash memory's physical characteristics. The situation gets even worse when bad block occurrences are accompanied by sudden power failures. We propose a bad block management scheme called P-BMS that can fully utilize flash-level parallelism, while guaranteeing provably correct block replacement. Experiments show that our P-BMS achieves a throughput that is more than 95% of the maximum bandwidth of the flash controller, even with bad block occurrences far heavier than in real flash memory. © 2017 ACM.",Bad block; Bad block management; Flash memory; Flash translation layer (FTL); Hardware acceleration; Parallel processing; Solid-state drive (SSD); Storage system,Digital storage; Hard disk storage; Random access storage; Storage management; Virtual storage; Bad block managements; Bad blocks; Flash translation layer; Hardware acceleration; Parallel processing; Solid state drives (SSD); Storage systems; Flash memory
Using Efficient Path Profiling to optimize memory consumption of on-chip debugging for High-Level Synthesis,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030692266&doi=10.1145%2f3126564&partnerID=40&md5=09ca4eb80a694645cf8982573906c35c,"High-Level Synthesis (HLS) for FPGAs is attracting popularity and is increasingly used to handle complex systems with multiple integrated components. To increase performance and efficiency, HLS flows now adopt several advanced optimization techniques. Aggressive optimizations and system level integration can cause the introduction of bugs that are only observable on-chip. Debugging support for circuits generated with HLS is receiving a considerable attention. Among the data that can be collected on chip for debugging, one of the most important is the state of the Finite State Machines (FSM) controlling the components of the circuit. However, this usually requires a large amount of memory to trace the behavior during the execution. This work proposes an approach that takes advantage of the HLS information and of the structure of the FSM to compress control flow traces and to integrate optimized components for on-chip debugging. The generated checkers analyze the FSM execution on-fly, automatically notifying when a bug is detected, localizing it and providing data about its cause. The traces are compressed using a software profiling technique, called Efficient Path Profiling (EPP), adapted for the debugging of hardware accelerators generated with HLS. With this technique, the size of the memory used to store control flow traces can be reduced up to 2 orders of magnitude, compared to state-of-the-art. © 2017 ACM.",Automated bug detection; Efficient path profiling; High-level synthesis; Memory optimization; On-chip debugging,Computer debugging; Program debugging; Bug detection; Efficient path; Hardware accelerators; Memory optimization; On-chip-debugging; Optimization techniques; Orders of magnitude; System level integration; High level synthesis
Online scheduling of 2-re-entrant flexible manufacturing systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030675121&doi=10.1145%2f3126551&partnerID=40&md5=16d2a7e9bca2dd56770caf556bdd88d5,Online scheduling of operations is essential to optimize productivity of flexible manufacturing systems (FMSs) where manufacturing requests arrive on the fly. An FMS processes products according to a particular flow through processing stations. This work focusses on online scheduling of re-entrant FMSs with flows using processing stations where products pass twice and with limited buffering between processing stations. This kind of FMS is modelled as a re-entrant flow shop with due dates and sequence-dependent set-up times. Such flow shops can benefit from minimization of the time penalties incurred from set-up times. On top of an existing greedy scheduling heuristic we apply a meta-heuristic that simultaneously explores several alternatives considering trade-offs between the used metrics by the scheduling heuristic. We identify invariants to efficiently remove many infeasible scheduling options so that the running time of online implementations is improved. The resulting algorithm is much faster than the state of the art and produces schedules with on average 4.6% shorter makespan. © 2017 ACM.,Bounded horizon scheduling; Flexible manufacturing systems; Re-entrant flow shops,Economic and social effects; Machine shop practice; Manufacture; Online systems; Scheduling; Metaheuristic; Online implementation; Online scheduling; Processing stations; Re-entrant flow shops; Scheduling heuristics; Sequence-dependent set-up time; State of the art; Flexible manufacturing systems
Implementation of partitioned mixed-criticality scheduling on a multi-core platform,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030685986&doi=10.1145%2f3126533&partnerID=40&md5=8302ee6d2d67520e191b9a57d639927e,"Recent industrial trends favor the adoption of multi-core architectures for mixed-criticality applications. Although several mixed-criticality multi-core scheduling approaches have been proposed, currently there are few implementations on hardware that demonstrate efficient resource utilization and the ability to bound interference on shared resources. To address this necessity, we develop a mixed-criticality runtime environment on the Kalray MPPA-256 Andey many-core platform. The runtime environment implements a scheduling policy based on adaptive temporal partitioning. We develop models, methods and implementation principles to implement the necessary scheduling primitives, to achieve high platform utilization and to perform a compositional worst-case execution time analysis. The bounds account for scheduling overheads and for the inter-task interference on the platform's shared memory. Using realistic benchmarks from avionics and signal processing, we validate the correctness and tightness of the bounds and demonstrate a high platform utilization. © 2017 ACM.",Adaptive temporal partitioning; Mixed-criticality; MPPA-256; Multi-core,Criticality (nuclear fission); Memory architecture; Scheduling; Signal processing; Mixed criticalities; MPPA-256; Multi core; Multicore architectures; Platform utilizations; Resource utilizations; Temporal partitioning; Worst-case execution time analysis; Computer architecture
Edge-TM: Exploiting transactional memory for error tolerance and energy efficiency,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030658195&doi=10.1145%2f3126556&partnerID=40&md5=9bcc0599440d3ff5c0298e1e00262a03,"Scaling of semiconductor devices has enabled higher levels of integration and performance improvements at the price of making devices more susceptible to the effects of static and dynamic variability. Adding safety margins (guardbands) on the operating frequency or supply voltage prevents timing errors, but has a negative impact on performance and energy consumption. We propose Edge-TM, an adaptive hardware/software error management policy that (i) optimistically scales the voltage beyond the edge of safe operation for better energy savings and (ii) works in combination with a Hardware Transactional Memory (HTM)-based error recovery mechanism. The policy applies dynamic voltage scaling (DVS) (while keeping frequency fixed) based on the feedback provided by HTM, which makes it simple and generally applicable. Experiments on an embedded platform show our technique capable of 57% energy improvement compared to using voltage guardbands and an extra 21-24% improvement over existing state-of-the-art error tolerance solutions, at a nominal area and time overhead. © 2017 ACM.",Energy efficiency; Error tolerance; Reliability; Transactional memory; Variability,Energy conservation; Energy utilization; Errors; Hardware; Reliability; Semiconductor devices; Storage allocation (computer); Voltage scaling; Dynamic voltage scaling (DVS); Error recovery mechanisms; Error tolerance; Hardware transactional memory; Operating frequency; Performance improvements; Transactional memory; Variability; Energy efficiency
A DWM-based stack architecture implementation for energy harvesting systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030675518&doi=10.1145%2f3126543&partnerID=40&md5=bdce6916b0e5c77ed5b9210b3707db6d,"Energy harvesting systems tend to use non-volatile processors to conduct computation under intermittent power supplies. While previous implementations of non-volatile processors are based on register architectures, stack architecture, known for its simplicity and small footprint, seems to be a better fit for energy harvesting systems. In this work, Domain Wall Memory (DWM) is used to implement ZPU, the world's smallest working CPU. Not only does DWM offer ultra-high density and SRAM-comparable access latency, but the sequential access structure of DWM also makes it well suited for a stack whose accesses display high temporal locality. As the performance and energy of DWM are determined by the number of shift operations performed to access the stack, this paper further reduces shift operations through novel data placement and micro-code transformation optimizations. The impact of compiler optimization techniques on the number of shift operations is also investigated so as to select the most effective optimizations for DWM-based stack machine. Experimental studies confirm the effectiveness of the proposed DWM-based stack architectures in improving the performance and energy-efficiency of energy harvesting systems. © 2017 ACM.",Compiler optimization; Data placement; Domain wall memory; Micro-code optimization,Cosine transforms; Energy efficiency; Memory architecture; Metadata; Program compilers; Static random access storage; Compiler optimizations; Data placement; Energy harvesting systems; Register architecture; Sequential access; Stack architecture; Temporal locality; Ultrahigh density; Energy harvesting
Diagonal Component Expansion for flow-layer placement of flow-based microfluidic biochips,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030691500&doi=10.1145%2f3126529&partnerID=40&md5=ba5ada655cb6c6dd4b1bc9b95d88eaaf,"Continuous flow-based microfluidic devices have seen a huge increase in interest because of their ability to automate and miniaturize biochemistry and biological processes, as well as their promise of creating a programmable platform for chemical and biological experimentation. The major hurdle in the adoption of these types of devices is in the design, which is largely done by hand using tools such as AutoCAD or SolidWorks, which require immense domain knowledge and are hard to scale. This paper investigates the problem of automated physical design for continuous flow-based microfluidic very large scale integration (mVLSI) biochips, starting from a netlist specification of the flow layer. After an initial planar graph embedding, vertices in the netlist are expanded into two-dimensional components, followed by fluid channel routing. A new heuristic, DIagonal Component Expansion (DICE) is introduced for the component expansion step. Compared to a baseline expansion method, DICE improves area utilization by a factor of 8.90x and reduces average fluid routing channel length by 47.4%. © 2017 ACM.",Microfluidics; MVLSI; Planar placement,Bioassay; Biochips; Computer aided design; Graph theory; Microarrays; Biological process; Chemical and biologicals; Expansion methods; Micro fluidic biochips; Micro-fluidic devices; MVLSI; Planar placement; Programmable platforms; Microfluidics
BenchPrime: Effective building of a hybrid benchmark suite,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030665192&doi=10.1145%2f3126499&partnerID=40&md5=bd4dd17f32c99af593ad65d5f8469519,"This paper presents BenchPrime, an automated benchmark analysis toolset that is systematic and extensible to analyze the similarity and diversity of benchmark suites. BenchPrime takes multiple benchmark suites and their evaluation metrics as inputs and generates a hybrid benchmark suite comprising only essential applications. Unlike prior work, BenchPrime uses linear discriminant analysis rather than principal component analysis, as well as selects the best clustering algorithm and the optimized number of clusters in an automated and metric-tailored way, thereby achieving high accuracy. In addition, BenchPrime ranks the benchmark suites in terms of their application set diversity and estimates how unique each benchmark suite is compared to other suites. As a case study, this work for the first time compares the DenBench with the MediaBench and MiBench using four different metrics to provide a multi-dimensional understanding of the benchmark suites. For each metric, BenchPrime measures to what degree DenBench applications are irreplaceable with those in MediaBench and MiBench. This provides means for identifying an essential subset from the three benchmark suites without compromising the application balance of the full set. The experimental results show that the necessity of including DenBench applications varies across the targetmetrics and that significant redundancy exists among the three benchmark suites. © 2017 ACM.",Benchmark; Linear discriminant analysis; Principle component analysis,Clustering algorithms; Discriminant analysis; Principal component analysis; Benchmark analysis; Benchmark suites; Evaluation metrics; High-accuracy; Linear discriminant analysis; Multi dimensional; Number of clusters; Principle component analysis; Benchmarking
Machine intelligence on resource-constrained IoT devices: The case of thread granularity optimization for CNN inference,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030662915&doi=10.1145%2f3126555&partnerID=40&md5=179b5d135e9fde803d5d272aa1742479,"Despite their remarkable performance in various machine intelligence tasks, the computational intensity of Convolutional Neural Networks (CNNs) has hindered their widespread utilization in resource-constrained embedded and IoT systems. To address this problem, we present a framework for synthesis of efficient CNN inference software targeting mobile SoC platforms. We argue that thread granularity can substantially impact the performance and energy dissipation of the synthesized inference software, and demonstrate that launching the maximum number of logical threads, often promoted as a guiding principle by GPGPU practitioners, does not result in an efficient implementation for mobile SoCs. We hypothesize that the runtime of a CNN layer on a particular SoC platform can be accurately estimated as a linear function of its computational complexity, which may seem counter-intuitive, as modern mobile SoCs utilize a plethora of heterogeneous architectural features and dynamic resource management policies. Consequently, we develop a principled approach and a data-driven analytical model to optimize granularity of threads during CNN software synthesis. Experimental results with several modern CNNs mapped to a commodity Android smartphone with a Snapdragon SoC show up to 2.37X speedup in application runtime, and up to 1.9X improvement in its energy dissipation compared to existing approaches. © 2017 ACM.",Convolutional neural networks; Mobile GPUs; Thread coarsening; Thread granularity,Artificial intelligence; Convolution; Embedded systems; Energy dissipation; Internet of things; Neural networks; Program processors; System-on-chip; Architectural features; Computational intensity; Convolutional neural network; Dynamic resource management; Efficient implementation; Machine intelligence; Mobile GPUs; Thread granularity; Constrained optimization
An abstraction-refinement theory for the analysis and design of real-time systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030680392&doi=10.1145%2f3126507&partnerID=40&md5=8d53b0adb69e187ef28f8fd27af7a9e5,"Component-based and model-based reasonings are key concepts to address the increasing complexity of real-time systems. Bounding abstraction theories allow to create efficiently analyzable models that can be used to give temporal or functional guarantees on non-deterministic and non-monotone implementations. Likewise, bounding refinement theories allowto create implementations that adhere to temporal or functional properties of specification models. For systems in which jitter plays a major role, both best-case and worst-case bounding models are needed. In this paper we present a bounding abstraction-refinement theory for real-time systems. Compared to the state-of-the-art TETB refinement theory, our theory is less restrictive with respect to the automatic lifting of properties from component to graph level and does not only support temporal worst-case refinement, but evenhandedly temporal and functional, best-case and worst-case abstraction and refinement. © 2017 ACM.",,Abstracting; Interactive computer systems; Abstraction refinement; Bounding models; Component based; Functional properties; Model-based Reasoning; Refinement theory; Specification models; State of the art; Real time systems
Synthesis of error-recovery protocols for micro-electrode-dot-array digital microfluidic biochips,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030681528&doi=10.1145%2f3126538&partnerID=40&md5=e5b5142784ca470d052f6536705dfda4,"A digital microfluidic biochip (DMFB) is an attractive technology platform for various biomedical applications. However, a conventional DMFB is limited by: (i) the number of electrical connections that can be practically realized, (ii) constraints on droplet size and volume, and (iii) the need for special fabrication processes and the associated reliability/yield concerns. To overcome the above challenges, DMFBs based on a microelectrode-dot-array (MEDA) architecture have been proposed and fabricated recently. Error recovery is of key interest for MEDA biochips due to the need for system reliability. Errors are likely to occur during droplet manipulation due to defects, chip degradation, and the uncertainty inherent in biochemical experiments. In this paper, we first formalize error-recovery objectives, and then synthesize optimal error-recovery protocols using a model based on Stochastic Multiplayer Games (SMGs). We also present a global error-recovery technique that can update the schedule of fluidic operations in an adaptive manner. Using three representative real-life bioassays, we show that the proposed approach can effectively reduce the bioassay completion time and increase the probability of success for error recovery. © 2017 ACM.",Controller synthesis; Digital microfluidics; Formal methods; Lab-on-chip; Micro-electrodedot-array (MEDA); Stochastic games,Bioassay; Biochips; Biodegradation; Drop formation; Drops; Electric connectors; Electrodes; Errors; Formal methods; Medical applications; Microarrays; Microelectrodes; Microfluidics; Recovery; Reliability; Stochastic models; Stochastic systems; Biochemical experiments; Biomedical applications; Controller synthesis; Digital microfluidic biochips; Lab on chip; Micro-electrodedot-array (MEDA); Probability of success; Stochastic game; Digital microfluidics
Reinforcement learning-assisted garbage collection to mitigate long-tail latency in SSD,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030673760&doi=10.1145%2f3126537&partnerID=40&md5=d839d3ef2003009986173329efa12983,"NAND flash memory is widely used in various systems, ranging from real-time embedded systems to enterprise server systems. Because the flash memory has erase-before-write characteristics, we need flash-memory management methods, i.e., address translation and garbage collection. In particular, garbage collection (GC) incurs long-tail latency, e.g., 100 times higher latency than the average latency at the 99th percentile. Thus, real-time and quality-critical systems fail to meet the given requirements such as deadline and QoS constraints. In this study, we propose a novel method of GC based on reinforcement learning. The objective is to reduce the long-tail latency by exploiting the idle time in the storage system. To improve the efficiency of the reinforcement learning-assisted GC scheme, we present new optimization methods that exploit finegrained GC to further reduce the long-tail latency. The experimental results with real workloads show that our technique significantly reduces the long-tail latency by 29-36% at the 99.99th percentile compared to state-of-the-art schemes. © 2017 ACM.",Flash storage system; Garbage collection; Long-tail latency; Reinforcement learning; SSD,Embedded systems; Flash memory; Real time systems; Refuse collection; Address translation; Enterprise servers; Flash storage; Garbage collection; Long tail; Optimization method; Real-time embedded systems; State-of-the-art scheme; Reinforcement learning
Editorial: Security of mobile devices,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029803330&doi=10.1145%2f3129534&partnerID=40&md5=594247ae62730f00452df01d7fa9b6e2,[No abstract available],,
An out-of-order load-storequeue for spatial computing,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030662199&doi=10.1145%2f3126525&partnerID=40&md5=1585aa0edfb107e6120ba084ecc4fb34,"The efficiency of spatial computing depends on the ability to achieve maximal parallelism. This necessitates memory interfaces that can correctly handle memory accesses that arrive in arbitrary order while still respecting data dependencies and ensuring appropriate ordering for semantic correctness. However, a typical memory interface for out-of-order processors (i.e., a load-store queue) cannot immediately meet these requirements: a different allocation policy is needed to achieve out-of-order execution in spatial systems that naturally omit the notion of sequential program order, a fundamental piece of information for correct execution. We show a novel and practical way to organize the allocation for an out-of-order load-store queue for spatial computing. The main idea is to dynamically allocate groups of memory accesses (depending on the dynamic behavior of the application), where the access order within the group is statically predetermined (for instance by a high-level synthesis tool). We detail the construction of our load-store queue and demonstrate on a few practical cases its advantages over standard accelerator-memory interfaces. © 2017 ACM.",Allocation; Dynamic scheduling; Load-store queue; Spatial computing,High level synthesis; Program processors; Semantics; Allocation; Allocation policies; Dynamic scheduling; Load store queues; Out-of-order execution; Out-of-order processors; Sequential programs; Spatial computing; Queueing theory
Optimization of real-time software implementing multi-rate synchronous finite state machines,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030672307&doi=10.1145%2f3126515&partnerID=40&md5=6df8f6fffa655333afcda3b27339be62,"Model-based design using Synchronous Reactive (SR) models is becoming widespread for control software development in industry. However, software synthesis is challenging for multi-rate SR models consisting of blocks modeled with finite state machines, due to the complexity of validating the system's real-time schedulability. The existing approach uses the simplified periodic task model to allow an efficient schedulability analysis, which leads to pessimistic and suboptimal solutions. Instead, in this paper, we adopt a more accurate but more complex schedulability analysis. We develop several optimization techniques to improve the algorithm's efficiency. Experimental results on synthetic systems and an industrial case study show that the proposed optimization framework preserves the solution optimality but is much faster (e.g., 1000× for systems with 15 blocks) than the branch-and-bound algorithm, and it generates better control software than the existing approach. © 2017 ACM.",Audsley's algorithm; Finite state machine; Model-based design; Optimization; Real-time schedulability; Synchronous reactive models,Branch and bound method; Finite automata; Optimization; Branch-and-bound algorithms; Industrial case study; Model- based designs; Optimization framework; Optimization techniques; Reactive model; Schedulability; Schedulability analysis; Software design
Green-energy-powered cognitive radio networks: Joint time and power allocation,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028620618&doi=10.1145%2f3092949&partnerID=40&md5=b92a7e72b26cad3879b438b15315cfab,"This article studies a green-energy-powered cognitive radio network (GCRN) in an underlay paradigm, wherein multiple battery-free secondary users (SUs) capture both the spectrum and the energy of primary users (PUs) to communicate with an access point (AP). By time division multiple access, each SU transmits data to AP in the allocated time and harvests energy from the RF signals of PUs otherwise, all in the same licensed spectrum concurrently with PUs. Thus, the transmit power of each SU is jointly constrained by the peak interference power at PU and the harvested energy of SU. With the formulated green coexistence paradigm, we investigate the sum-Throughput maximization problem with respect to time and power allocation, which is non-convex. To obtain the optimal resource allocation, we propose a joint optimal time and power allocation (JOTPA) algorithm that first transforms the original problem into a convex optimization problem with respect to time and energy allocation, and then solve it by iterative Lagrange dual decomposition. To comprehensively evaluate the performance of the GCRN with JOTPA, we deploy the GCRN in three typical scenarios and compare JOTPA with the equal time and optimal power allocation (ETOPA) algorithm. Extensive simulations show that the deployment of the GCRN significantly influences the throughput performance and JOTPA outperforms ETOPA under all considered scenarios. © 2017 ACM.",Cognitive radio networks; Energy harvesting; Green energy; Resource allocation; Throughput,Convex optimization; Energy harvesting; Iterative methods; Optimization; Radio; Radio systems; Resource allocation; Throughput; Time division multiple access; Cognitive radio network; Convex optimization problems; Green energy; Lagrange dual decompositions; Optimal power allocation; Optimal resource allocation; Throughput maximization problem; Time and power allocations; Cognitive radio
Guest editorial for ACM TECS: Special issue on autonomous battery-free sensing and communication,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028602685&doi=10.1145%2f3127494&partnerID=40&md5=165298c9e7560f08676672c0270e41ca,[No abstract available],,
Near-optimal co-deployment of chargers and sink stations in rechargeable sensor networks,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028614772&doi=10.1145%2f3070721&partnerID=40&md5=c500f6a9e36fe85b2bed5a2394e49079,"Wireless charging technology has drawn great attention of both academia and industry in recent years, due to its potential of significantly improving the system performance of sensor networks. The emergence of an open-source experimental platform for wireless rechargeable sensor networks, Powercast, has made the theoretical research closer to reality. This pioneering platform is able to recharge sensor nodes much more efficiently and allows different communication protocols to be implemented upon users' demands. Different from the RFID-based model widely used in the existing works, Powercast designs the charger and sink station separately. This leads to a new design challenge of cooperatively deploying minimum number of chargers and sink stations in wireless rechargeable sensor networks. Such a co-deployment issue is extremely challenging, since the deployments of chargers and sink stations are coupled, and each subproblem is known to be NP-hard. The key to the design is to understand the intrinsic relationship between data flow and energy flow, which is interdependent. In this article, we tackle this challenge by dividing it into two subproblems and optimizing charger and sink station deployment iteratively. Specifically, we first transform each subproblem to a max-flow problem. With this, we are able to select chargers or sink stations according to their contributions to the total flow rate. We design greedy-based algorithms with a guaranteed worst-case bound ln R ξ for the subproblems of charger deployment and sink station deployment, respectively. Further, we address the original problem by designing an iterative algorithm that solves two subproblems alternatively to achieve a near optimal performance. We corroborate our analysis by extensive simulations under practical coefficient settings and demonstrate the advantage of the proposed algorithm. © 2017 ACM.",Deployment cost; Greedy-based max-flow; Iterative method; Wireless rechargeable sensor networks,Iterative methods; Optimization; Sensor networks; Sensor nodes; Deployment costs; Experimental platform; Extensive simulations; Greedy-based max-flow; Iterative algorithm; Near-optimal performance; Rechargeable sensor networks; Theoretical research; Wireless sensor networks
Operating energy-neutral real-time systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028591165&doi=10.1145%2f3078631&partnerID=40&md5=4ac50fb03f0771a390aa963f5803e1d5,"Energy-neutral real-Time systems harvest the entire energy they use from their environment. In such systems, energy must be treated as an equally important resource as time, which creates the need to solve a number of problems that so far have not been addressed by traditional real-Time systems. In particular, this includes the scheduling of tasks with both time and energy constraints, the monitoring of energy budgets, as well as the survival of blackout periods during which not enough energy is available to keep the system fully operational. In this article, we address these issues presenting EnOS, an operating-system kernel for energy-neutral real-Time systems. EnOS considers mixed time criticality levels for different energy criticality modes, which enables a decoupling of time and energy constraints when one is considered less critical than the other.When switching the energy criticality mode, the system also changes the set of executed tasks and is therefore able to dynamically adapt its energy consumption depending on external conditions. By keeping track of the energy budget available, EnOS ensures that in case of a blackout the system state is safely stored to persistent memory, allowing operations to resume at a later point when enough energy is harvested again. © 2017 ACM.",Energy budget monitoring; Energy harvesting,Budget control; Criticality (nuclear fission); Energy harvesting; Energy utilization; Interactive computer systems; Energy budgets; External conditions; Fully operational; Operating energies; Operating system kernel; Persistent memory; System state; Time and energy constraints; Real time systems
Efficient schedulability test for dynamic-priority scheduling of mixed-criticality real-time systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040459180&doi=10.1145%2f3105922&partnerID=40&md5=2ee5315424468518428c9d77b3006514,"Systems in many safety-critical application domains are subject to certification requirements. In such a system, there are typically different applications providing functionalities that have varying degrees of criticality. Consequently, the certification requirements for functionalities at these different criticality levels are also varying, with very high levels of assurance required for a highly critical functionality, whereas relatively low levels of assurance are required for a less critical functionality. Considering the timing assurance given to various applications in the form of guaranteed budgets within deadlines, a theory of real-time scheduling for such multi-criticality systems has been recently under development. In particular, an algorithm called Earliest Deadline First with Virtual Deadlines (EDF-VD) has shown a lot of promise for systems with two criticality levels, especially in terms of practical performance demonstrated through experiment results. In this article, we design a new schedulability test for EDF-VD that extends these performance benefits to multi-criticality systems. We propose a new test based on demand bound functions and also present a novel virtual deadline assignment strategy. Through extensive experiments, we show that the proposed technique significantly outperforms existing strategies for a variety of generic real-time systems. © 2017 ACM.",Demand bound function; Mixed-criticality system,Budget control; Criticality (nuclear fission); Interactive computer systems; Response time (computer systems); Safety engineering; Scheduling; Bound function; Certification requirements; Dynamic-priority scheduling; Earliest deadline first; Mixed-criticality systems; Performance benefits; Real - time scheduling; Safety critical applications; Real time systems
Toward a practical regularity-based model: The impact of evenly distributed temporal resource partitions,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028544976&doi=10.1145%2f3092945&partnerID=40&md5=2125b1e55fe22d6820a245253fe3d63f,"Most Hierarchical Real-time Scheduling (HiRTS) techniques have focused on temporal resource partitions in which time units are periodically distributed. Although such periodic partitions could provide great flexibility for the resource-level scheduling, engineers face significant obstacles when trying to determine the schedulability of real-time tasks running on them. The main reason is that periodic partitions fail to effectively bound the difference between the ideal and the actual resource allocation. To solve this problem, some researchers introduced the Regular Partition, a type of temporal resource partition that is almost evenly distributed. Recent research has shown that it achieves maximal transparency for task scheduling-some classical real-time scheduling problems on a regular partition can be easily transformed into equivalent problems on a dedicated single resource. However, the resource partitioning problem for regular partitions is much more complicated than the one for periodic partitions. Based on a practical two-layer HiRTS platform, this article introduces MulZ (Multiple Z-seqences), which is the first to solve this problem with a partitioned scheduling strategy. By using a more complicated approximation methodology, our experimental results show that MulZ outperforms the current best global scheduling algorithm on this problem. After that, it compares the overall performance of the periodic partition and the regular partition. We conclude that the regular partition is a better choice for the integration of real-time applications. © 2017 ACM.",Hierarchical Real-time Scheduling; Real-time task scheduling; Resource utilization; Schedulability rate; Temporal resource partition,Approximation algorithms; Multitasking; Scheduling; Scheduling algorithms; Approximation methodology; Real - time scheduling; Real-time application; Real-time tasks; Resource partitioning; Resource utilizations; Schedulability; Scheduling strategies; Problem solving
Exploiting stable data dependency in stream processing acceleration on FPGAs,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026629539&doi=10.1145%2f3092950&partnerID=40&md5=a95c0d167efafba6d1792ad5561ebcdc,"With the unique feature of fine-grained parallelism, field-programmable gate arrays (FPGAs) show great potential for streaming algorithm acceleration. However, the lack of a design framework, restrictions on FPGAs, and ineffective tools impede the utilization of FPGAs in practice. In this study, we provide a design paradigm to support streaming algorithm acceleration on FPGAs. We first propose an abstract model to describe streaming algorithms with homogeneous sub-functions (HSF) and stable data dependency (SDD), which we call the HSF-SDD model. Using this model, we then develop an FPGA framework, PE-Ring, that has the advantages of (1) fully exploiting algorithm parallelism to achieve high performance, (2) leveraging block RAM to serve large scale parameters, and (3) enabling flexible parameter adjustments. Based on the proposed model and framework, we finally implement a specific converter to generate the register-transfer level representation of the PE-Ring. Experimental results show that our method outperforms ordinary FPGA design tools by one to two orders of magnitude. Experiments also demonstrate the scalability of the PE-Ring. © 2017 ACM.",,Integrated circuit design; Abstract modeling; Data dependencies; Fine-grained parallelism; Orders of magnitude; Parameter adjustments; Register transfer level; Stream processing; Streaming algorithm; Field programmable gate arrays (FPGA)
Analyzing the fault injection sensitivity of secure embedded software,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027374967&doi=10.1145%2f3063311&partnerID=40&md5=feffd1ddf89de011229f143c7bc9fcd4,"Fault attacks on cryptographic software use faulty ciphertext to reverse engineer the secret encryption key. Although modern fault analysis algorithms are quite efficient, their practical implementation is complicated because of the uncertainty that comes with the fault injection process. First, the intended fault effect may not match the actual fault obtained after fault injection. Second, the logic target of the fault attack, the cryptographic software, is above the abstraction level of physical faults. The resulting uncertainty with respect to the fault effects in the software may degrade the efficiency of the fault attack, resulting in many more trial fault injections than the amount predicted by the theoretical fault attack. In this contribution, we highlight the important role played by the processor microarchitecture in the development of a fault attack. We introduce the microprocessor fault sensitivity model to systematically capture the fault response of a microprocessor pipeline. We also propose Microarchitecture-Aware Fault Injection Attack (MAFIA). MAFIA uses the fault sensitivity model to guide the fault injection and to predict the fault response. We describe two applications for MAFIA. First, we demonstrate a biased fault attack on an unprotected Advanced Encryption Standard (AES) software program executing on a seven-stage pipelined Reduced Instruction Set Computer (RISC) processor. The use of the microprocessor fault sensitivity model to guide the attack leads to an order of magnitude fewer fault injections compared to a traditional, blind fault injection method. Second, MAFIA can be used to break known software countermeasures against fault injection. We demonstrate this by systematically breaking a collection of state-of-the-art software fault countermeasures. These two examples lead to the key conclusion of this work, namely that software fault attacks become much more harmful and effective when an appropriate microprocessor fault sensitivity model is used. This, in turn, highlights the need for better fault countermeasures for software. © 2017 ACM.",AES; Embedded system security; Fault attack; Microarchitecture-aware fault analysis; RISC,Computer architecture; Computer software; Data privacy; Embedded systems; Pipeline processing systems; Program processors; Reduced instruction set computing; Side channel attack; Software testing; Uncertainty analysis; Advanced Encryption Standard; Cryptographic software; Fault analysis; Fault injection attacks; Fault sensitivity; Micro architectures; Microprocessor faults; Reduced instruction set computers; Cryptography
A DFA-resistant and masked PRESENT with area optimization for RFID applications,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027343553&doi=10.1145%2f3035543&partnerID=40&md5=a4d100e31d0ca73cb025056bafd41228,"Radio-Frequency Identification (RFID) tag-based applications are usually resource constrained and security sensitive. However, only about 2,000 gate equivalents in a tag can be budgeted for implementing security components [27]. This requires not only lightweight cryptographic algorithms such as PRESENT (around 1,000 gate equivalents) but also lightweight protections against modern Side Channel Attacks (SCAs). With this budget, the first-order masking and fault detection are two suitable countermeasures to be developed for PRESENT. However, if both countermeasures are applied without any optimization, it will significantly exceed the given area budget. In this work, we optimize area to include both countermeasures to maximize the security for PRESENT within this RFID area budget. The most area-consuming parts of the proposed design are the masked S-boxes and the inverse masked S-boxes. To optimize the area, we have deduced a computational relationship between these two parts, which enables us to reuse the hardware resource of the masked S-boxes to implement the inverse masked S-boxes. The proposed design takes up only 2,376 gates with UMC 65nm CMOS technology. Compared with the unoptimized design, our implementation reduces the overall area by 28.45%. We have tested the effectiveness of the first-order Differential Power Analysis (DPA) and Differential Fault Analysis (DFA) -resistant countermeasures. Experimental results show that we have enhanced the SCA resistance of our PRESENT implementation. © 2017 ACM.",DFA-resistant; Lightweight; PRESENT cipher; RFID; Side channel attacks,Budget control; Fault detection; Radio frequency identification (RFID); Cryptographic algorithms; DFA-resistant; Differential fault analyses (DFA); First-order differentials; Lightweight; Lightweight protection; PRESENT cipher; Radio-frequency-identification tags (RFID); Side channel attack
Harmonicity-aware task partitioning for fixed priority scheduling of probabilistic real-time tasks on multi-core platforms,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027382944&doi=10.1145%2f3064813&partnerID=40&md5=685912bc2a19f36f18b6c1d3f450916e,"The uncertainty due to performance variations of IC chips and resource sharing on multi-core platforms have significantly degraded the predictability of real-time systems. Traditional deterministic approaches based on the worst-case assumptions become extremely pessimistic and thus unpractical. In this article, we address the problem of scheduling a set of fixed-priority periodic real-time tasks on multi-core platforms in a probabilistic manner. Specifically, we consider task execution time as a probabilistic distribution and study how to schedule these tasks on multi-core platforms with guaranteed Quality of Service (QoS) requirements in terms of deadline-missing probabilities. Moreover, it is a well-known fact that the relationship among task periods, if exploited appropriately, can significantly improve the processor utilization. To this end, we present a novel approach to partition real-time tasks that can take both task execution time distributions and their period relationships into consideration. From our extensive experiment results, our proposed methods can greatly improve the schedulability of real-time tasks when compared with existing approaches.",Harmonic; Multi-core; Probabilistic; Real-time systems; Task partitions,Fixed platforms; Interactive computer systems; Probability distributions; Quality of service; Scheduling; Time sharing systems; Deterministic approach; Fixed priority scheduling; Harmonic; Multi core; Performance variations; Probabilistic; Probabilistic distribution; Task partition; Real time systems
High-performance ideal lattice-based cryptography on 8-bit AVR microcontrollers,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026664356&doi=10.1145%2f3092951&partnerID=40&md5=19d064b275252d9d505938f473e169e0,"Over recent years lattice-based cryptography has received much attention due to versatile average-case problems like Ring-LWE or Ring-SIS that appear to be intractable by quantum computers. In this work, we evaluate and compare implementations of Ring-LWE encryption and the bimodal lattice signature scheme (BLISS) on an 8-bit Atmel ATxmega128 microcontroller. Our implementation of Ring-LWE encryption provides comprehensive protection against timing side-channels and takes 24.9ms for encryption and 6.7ms for decryption. To compute a BLISS signature, our software takes 317ms and 86ms for verification. These results underline the feasibility of lattice-based cryptography on constrained devices. © 2017 ACM.",ATxmega; BLISS; Ideal lattices; NTT; RLWE,Microcontrollers; Quantum computers; Side channel attack; Verification; ATxmega; AVR microcontrollers; BLISS; Constrained devices; Lattice-based cryptography; RLWE; Signature Scheme; Timing side channels; Cryptography
Seamless vision-assisted placement calibration for wearable inertial sensors,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025153871&doi=10.1145%2f3023364&partnerID=40&md5=c0d3ef88a43a70b914d10685ece90c53,"Wearable inertial devices are being widely used in the applications of activity tracking, health care, and professional sports, and their usage is on a rapid rise. Signal processing algorithms for these devices are often designed to work with a known location of the wearable sensor on the body. However, in reality, the wearable sensor may be worn at different body locations due to the user's preference or unintentional misplacement. The calibration of the sensor location is important to ensure that the algorithms operate correctly. In this article, we propose an auto-calibration technique for determining the location of wearables on the body by fusing the 3-axis accelerometer data from the devices and three-dimensional camera (i.e., Kinect) information obtained from the environment. The automatic calibration is achieved by a cascade decision-tree-based classifier on top of the minimum least-squares errors obtained by solving Wahba's problem, operating on heterogeneous sensors. The core contribution of our work is that there is no extra burden on the user as a result of this technique. The calibration is done seamlessly, leveraging sensor fusion in an Internet-of-Things setting opportunistically when the user is present in front of an environmental camera performing arbitrary movements. Our approach is evaluated with two different types of movements: simple actions (e.g., sit-tostand or picking up phone) and complicated tasks (e.g., cooking or playing basketball), yielding 100% and 82.56% recall for simple actions and for complicated tasks, respectively, in determining the correct location of sensors. © 2017 ACM.",Inertial measurement unit (IMU); Kinect; On-body device localization; Vision-assisted calibration; Wahba's problem,Calibration; Cameras; Classification (of information); Location; Signal processing; Sports; Units of measurement; Wearable technology; Inertial measurement unit; Kinect; On-body; Vision-assisted; Wahba's problems; Wearable sensors
Collaborative PCA/DCA learning methods for compressive privacy,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025155283&doi=10.1145%2f2996460&partnerID=40&md5=d9830cab21b0038b32bdd68a754d75b7,"In the Internet era, the data being collected on consumers like us are growing exponentially, and attacks on our privacy are becoming a real threat. To better ensure our privacy, it is safer to let the data owner control the data to be uploaded to the network as opposed to taking chance with data servers or third parties. To this end, we propose compressive privacy, a privacy-preserving technique to enable the data creator to compress data via collaborative learning so that the compressed data uploaded onto the Internet will be useful only for the intended utility and not be easily diverted to malicious applications. For data in a high-dimensional feature vector space, a common approach to data compression is dimension reduction or, equivalently subspace projection. The most prominent tool is principal component analysis (PCA). For unsupervised learning, PCA can best recover the original data given a specific reduced dimensionality. However, for the supervised learning environment, it is more effective to adopt a supervised PCA, known as discriminant component analysis (DCA), to maximize the discriminant capability. The DCA subspace analysis embraces two different subspaces. The signal-subspace components of DCA are associated with the discriminant distance/power (related to the classification effectiveness), whereas the noise subspace components of DCA are tightly coupled with recoverability and/or privacy protection. This article presents three DCA-related data compression methods useful for privacy-preserving applications: -Utility-driven DCA: Because the rank of the signal subspace is limited by the number of classes, DCA can effectively support classification using a relatively small dimensionality (i.e., high compression). -Desensitized PCA: By incorporating a signal-subspace ridge into DCA, it leads to a variant especially effective for extracting privacy-preserving components. In this case, the eigenvalues of the noise-space are made to become insensitive to the privacy labels and are ordered according to their corresponding component powers. -Desensitized K-means/SOM: Since the revelation of the K-means or SOM cluster structure could leak sensitive information, it is safer to perform K-means or SOM clustering on a desensitized PCA subspace. © 2017 ACM.",Compressive privacy; DCA; Face-recognition; K-means; KDCA; PCA,Computer aided instruction; Data compression; Data handling; Education; Eigenvalues and eigenfunctions; Face recognition; Principal component analysis; Vector spaces; Vectors; Collaborative learning; Compression methods; Discriminant component analysis; High dimensional feature; K-means; KDCA; Learning environments; Sensitive informations; Data privacy
The design and implementation of the synchronous language CÉU,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026513870&doi=10.1145%2f3035544&partnerID=40&md5=c3c9d21a87fbd3a619609691890912f8,"CÉU is a synchronous language targeting soft real-time systems. It is inspired by Esterel and has a simple semantics with fine-grain control over program execution. CÉU uses an event-triggered notion of time that enables compile-time checks to detect conflicting concurrent statements, resulting in deterministic and concurrency-safe programs. We present the particularities of our design in comparison to Esterel, such as stack-based internal events, concurrency checks, safe integration with C, and first-class timers. We also present two implementation back ends: one aiming for resource efficiency and interoperability with C, and another as a virtual machine that allows remote reprogramming. © 2017 ACM",Concurrency; Determinism; Embedded systems; Esterel; Reactivity; Synchronous,Concurrency control; Embedded systems; Esters; Interactive computer systems; Reactivity (nuclear); Real time systems; Semantics; Concurrency; Design and implementations; Determinism; Esterel; Resource efficiencies; Soft real-time systems; Synchronous; Synchronous languages; C (programming language)
"Editorial: Cyber security, IoT, block chains-risks and opportunities",2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025089901&doi=10.1145%2f3087913&partnerID=40&md5=d072abf7bf71a774d2b2e5328b02f7d8,[No abstract available],,
Serial arithmetic strategies for improving FPGA throughput,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025128951&doi=10.1145%2f2996459&partnerID=40&md5=e2ec5cb7e36365cf3b2d90501c3ab2cf,"Serial arithmetic has been shown to offer attractive advantages in area for field-programmable gate array (FPGA) datapaths but suffers from a significant reduction in throughput compared to traditional bit-parallel designs. In this work, we perform a performance and trade-off analysis that counterintuitively shows that, despite the decreased throughput of individual serial operators, replication of serial arithmetic can provide a 2.1× average increase in throughput compared to bit-parallel pipelines for common FPGA applications. We complement this analysis with a novel SerDes architecture that enables existing FPGA pipelines to be replaced with serial logic with potentially higher throughput. We also present a serialized sliding-window architecture that improves average throughput 2.4× compared to existing bit-parallel work. © 2017 ACM.",FPGA; SerDes; Serial arithmetic; Sliding window,Pipelines; Throughput; Timing jitter; Average throughput; Bit parallel designs; Bit-parallel; Data-paths; FPGA applications; SerDes; Sliding Window; Trade-off analysis; Field programmable gate arrays (FPGA)
Efficient automated code partitioning for microcontrollers with switchable memory banks,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027004775&doi=10.1145%2f3055511&partnerID=40&md5=e380bfa7b60ad0f644e077db753659bf,"Switching active memory banks at runtime allows a processor with a narrow address bus to access memory that exceeds ranges normally addressable via the bus. Switching code memory banks is regaining interest in microcontrollers for the Internet of Things (IoT), which have to run continuously growing software, while at the same time consuming ultra-small amounts of energy. To make use of bank switching, such software must be partitioned among the available banks and augmented with bank-switching instructions. In contrast to the augmenting, which is done automatically by a compiler, today the partitioning is normally done manually by programmers. However, since IoT software is cross-compiled on much more powerful machines than its target microcontrollers, it becomes possible to partition it automatically during compilation. In this article, we thus study the problem of partitioning program code among banks such that the resulting runtime performance of the program is maximized. We prove that the problem is NP-hard and propose a heuristic algorithm with a low complexity, so it enables fast compilation and hence interactive software development. The algorithm decomposes the problem into three subproblems and introduces a heuristic for each of them: (1) which pieces of code to partition, (2) which of them to assign to permanently mapped banks, and (3) how to divide the remaining ones among switchable banks. We integrate the algorithm, together with earlier ones, in an open-source compiler and test the resulting solution on synthetic as well as actual commercial IoT software bases, thereby demonstrating its advantages and drawbacks. In particular, the results show that the performance of partitions produced by our algorithm comes close to that of partitions created manually by programmers with expert knowledge on the partitioned code. © 2017 ACM.",Bank switching; Code banking; Code partitioning; Heuristic algorithm; Internet of things; IoT; MCU; Microcontroller; Partitioned memory; Switchable memory; Trampoline function,Codes (symbols); Controllers; Heuristic algorithms; Internet of things; Microcontrollers; Open source software; Program compilers; Random access storage; Software design; Software testing; Switching; Automated code; Code banking; Code partitioning; Expert knowledge; Interactive software; Internet of thing (IOT); Run-time performance; Switchable; Open systems
Efficient kernel management on GPUs,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026996717&doi=10.1145%2f3070710&partnerID=40&md5=9eb34586e9c66bd70d9e474090b0c1a2,"Graphics Processing Units (GPUs) have been widely adopted as accelerators for compute-intensive applications due to its tremendous computational power and high memory bandwidth. As the complexity of applications continues to grow, each new generation of GPUs has been equipped with advanced architectural features and more resources to sustain its performance acceleration capability. Recent GPUs have been featured with concurrent kernel execution, which is designed to improve the resource utilization by executing multiple kernels simultaneously. However, it is still a challenge to find a way to manage the resources on GPUs for concurrent kernel execution. Prior works only achieve limited performance improvement as they do not optimize the thread-level parallelism (TLP) and model the resource contention for the concurrently executing kernels. In this article, we design an efficient kernel management framework that optimizes the performance for concurrent kernel execution on GPUs. Our kernel management framework contains two key components: TLP modulation and cache bypassing. The TLP modulation is employed to adjust the TLP for the concurrently executing kernels. It consists of three parts: kernel categorization, static TLP modulation, and dynamic TLP modulation. The cache bypassing is proposed to mitigate the cache contention by only allowing a subset of a kernel's blocks to access the L1 data cache. Experiments indicate that our framework can improve the performance by 1.51× on average (energy-efficiency by 1.39× on average), compared with the default concurrent kernel execution framework. © 2017 ACM.",Energy-efficiency; General purpose graphics processing unit (GPGPU); Kernel management,Computer graphics; Graphics processing unit; Image coding; Modulation; Network function virtualization; Program processors; Architectural features; General purpose graphics processing unit (GPGPU); High memory bandwidth; Management frameworks; Performance acceleration; Resource contention; Resource utilizations; Thread-level parallelism; Energy efficiency
Guest editorial: Special issue on embedded computing for IoT,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026998978&doi=10.1145%2f3065713&partnerID=40&md5=66b874004d320febad4d6b26ace4568b,[No abstract available],,
"Corrections to and discussion of ""implementation and evaluation of mixed-criticality scheduling approaches for sporadic tasks""",2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026999412&doi=10.1145%2f2974020&partnerID=40&md5=0060c4184cedfd3fceb124959ff3a57a,"The AMC-IA mixed-criticality scheduling analysis was proposed as an improvement to the AMC-MAX adaptive mixed-criticality scheduling analysis. However, we have identified several necessary corrections to the AMC-IA analysis. In this article, we motivate and describe those corrections, and discuss and illustrate why the corrected AMC-IA analysis cannot be shown to outperform AMC-MAX. © 2017 ACM.",Adaptive mixed-criticality scheduling; Real-time systems,Criticality (nuclear fission); Interactive computer systems; Scheduling; Mixed criticalities; Scheduling analysis; Real time systems
An FPGA-based architecture for high-speed compressed signal reconstruction,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027057737&doi=10.1145%2f3056481&partnerID=40&md5=de134d71afac9a223d4a7a9922087448,"Compressive Sensing (CS) is an emerging research area that allows efficient signal acquisition under the sub-Nyquist rate while still promising reliable data recovery. However, practical applications of CS in hardware platforms are limited as signal reconstruction is still challenging due to its high computational complexity, especially for autonomous real-time signal recovery. In this article, we propose an algorithmic transformation technique referred to as Matrix Inversion Bypass (MIB) to improve the signal recovery efficiency of the Orthogonal Matching Pursuit (OMP)-based CS reconstruction. The basic idea of MIB is to decouple the computations of intermediate signal estimates and matrix inversions, thereby enabling parallel processing of these two time-consuming operations in the OMP algorithm. The proposed MIB naturally leads to a parallel architecture for high-speed dedicated hardware implementations. An FPGA-based implementation is developed with the optimized structure aimed at the efficient utilization of hardware resources while realizing high-speed signal recovery. The proposed architecture can perform the signal recovery at up to 1.4× faster than the OMP-based implementation using almost the same hardware resources. © 2017 ACM.",Compressive sensing; FPGA implementation; High-speed architecture; Orthogonal matching pursuit; Parallel computing,Compressed sensing; Field programmable gate arrays (FPGA); Hardware; Linear transformations; Matrix algebra; Parallel architectures; Parallel processing systems; Recovery; Signal reconstruction; Algorithmic transformation; Compressive sensing; FPGA implementations; FPGA-based architectures; FPGA-based implementation; High-speed architectures; Orthogonal matching pursuit; Proposed architectures; Signal processing
Schedulability of Bounded-Rate Multimode Systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022206228&doi=10.1145%2f2996797&partnerID=40&md5=585518103f185269e10067199b85236a,"Bounded-rate multimode systems are hybrid systems that switch freely among a finite set of modes, and whose dynamics are specified by a finite number of real-valued variables with mode-dependent rates that vary within given bounded sets. The scheduler repeatedly proposes a time and a mode, while the environment chooses an allowable rate for that mode; the state of the system changes linearly in the direction of the rate. The scheduler aims to keep the state within a safe set, while the environment aims to leave it. We study the problem of existence of a winning scheduler strategy and associated complexity questions. © 2017 ACM.",Constrained control; Controller synthesis; Green scheduling; Invariant sets; Multimode systems; Phrases; Stability,Convergence of numerical methods; Hybrid systems; Constrained controls; Controller synthesis; Green scheduling; Invariant set; Multimode system; Phrases; Scheduling
Fault-tolerant dynamic task mapping and scheduling for Network-on-Chip-based multicore platform,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027073478&doi=10.1145%2f3055512&partnerID=40&md5=db4b31ddb3382cc3920c2da60a0d8600,"In Network-on-Chip (NoC)-based multicore systems, task allocation and scheduling are known to be important problems, as they affect the performance of applications in terms of energy consumption and timing. Advancement of deep submicron technology has made it possible to scale the transistor feature size to the nanometer range, which has enabled multiple processing elements to be integrated onto a single chip. On the flipside, it has made the integrated entities on the chip more susceptible to different faults. Although a significant amount of work has been done in the domain of fault-tolerant mapping and scheduling, existing algorithms either precompute reconfigured mapping solutions at design time while anticipating fault(s) scenarios or adopt a hybrid approach wherein a part of the fault mitigation strategy relies on the design-time solution. The complexity of the problem rises further for real-time dynamic systems where new applications can arrive in the multicore platform at any time instant. For real-time systems, the validity of computation depends both on the correctness of results and on temporal constraint satisfaction. This article presents an improved fault-tolerant dynamic solution to the integrated problem of application mapping and scheduling for NoC-based multicore platforms. The developed algorithm provides a unified mapping and scheduling method for real-time systems focusing on meeting application deadlines and minimizing communication energy. A predictive model has been used to determine the failure-prone cores in the system for which a fault-tolerant resource allocation with task redundancy has been performed. By selectively using a task replication policy, the reliability of the application, executing on a given NoC platform, is improved. A detailed evaluation of the performance of the proposed algorithm has been conducted for both real and synthetic applications. When compared with other fault-tolerant algorithms reported in the literature, performance of the proposed algorithm shows an average reduction of 56.95% in task re-execution time overhead and an average improvement of 31% in communication energy. Further, for time-constrained tasks, deadline satisfaction has also been achieved for most of the test cases by the developed algorithm, whereas the techniques reported in the literature failed to meet deadline in about 45% test cases. © 2017 ACM.",Communication energy; Deadline; Dynamic mapping and scheduling; Fault tolerance; Network-on-chip; Task replication,Distributed computer systems; Energy utilization; Fault tolerance; Interactive computer systems; Mapping; Network-on-chip; Real time systems; Scheduling; Scheduling algorithms; Servers; Communication energy; Deadline; Deep sub-micron technology; Dynamic mapping; Fault tolerant algorithms; Synthetic application; Task allocation and scheduling; Task replications; Integrated circuit design
Petri net models and collaborativeness for parallel processes with resource sharing and message passing,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027053597&doi=10.1145%2f2810001&partnerID=40&md5=f79aa75417b5ba8a245bc79a167887c6,"Petri nets are widely used to model and analyse concurrent systems. There exist two distinct classes of Petri nets that focus on different features of concurrent systems. The first one features multiple parallel processes sharing a group of common resources but not interacting/collaborating with each other. The second one allows multiple parallel processes to interact/collaborate with each other via message exchange but does not share any common resources. However, in many distributed environments, multiple processes both interact/collaborate with each other and share some common resources. To model and analyse such systems, this article defines a new class of Petri nets called Parallel Process Nets (P2Ns) that may be viewed as a generalization of the two mentioned above. We propose collaborativeness and close collaborativeness for P2Ns. The former guarantees that a modelled system is both deadlock-free and livelock-free, and the latter guarantees that it is deadlock-free, livelock-free, and starvation-free. These concepts and ideas are illustrated through some classical examples such as Producer-Consumer Problem and Dinning Philosophers Problem. Algorithms are developed to decide them. At last, P2Ns are applied to the modelling and analysis of two real systems: hospital information system and elevator scheduling system. © 2017 ACM.",Concurrent systems; Deadlock; Livelock; Petri nets; Starvation,Petri nets; Philosophical aspects; Concurrent systems; Deadlock; Distributed environments; Hospital information systems; Livelock; Modelling and analysis; Multiple parallel process; Starvation; Message passing
Preserving smart sink-location privacy with delay guaranteed routing scheme for WSNs,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021085483&doi=10.1145%2f2990500&partnerID=40&md5=cbce7e27d0a0fd8c76193d0a54800897,"A Semi Random Circle routing for mobile Sink joint Ray Routing for data (SRCRR) scheme is proposed for preserving sink-location privacy with a delay guaranteed. In the SRCRR scheme, the data are directionally routed along ray paths and stored at intermediate nodes probabilistically. The Sink moves in a semirandom circular pattern to collect data from the local nodes occasionally, which guarantees that the data will be collected with an acceptable delay and prevents attackers from predicting their locations and movements. The experimental results indicate that the performance of the SRCRR scheme is better than that of the previous schemes. © 2017 ACM.",Data collection; Delay; Preserving sink-location privacy; Routing; Security,Data acquisition; Location; Data collection; Delay; Routing; Security; Sink locations; Data privacy
Distributed multi-representative re-fusion approach for heterogeneous sensing data collection,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021074762&doi=10.1145%2f2974021&partnerID=40&md5=c596ad494836b01763a01810af4b04cc,"Amulti-representative re-fusion (MRRF) approximate data collection approach is proposed in whichmultiple nodes with similar readings form a data coverage set (DCS). The reading value of the DCS is represented by an R-node. The set near the Sink is smaller, while the set far from the Sink is larger, which can reduce the energy consumption in hotspot areas. Then, a distributed data-aggregation strategy is proposed that can re-fuse the value of R-nodes that are far from each other but have similar readings. Both comprehensive theoretical and experimental results indicate that the MRRF approach increases lifetime and energy efficiency. © 2017 ACM.",Approximate data collection; Multirepresentative re-fusion; Network lifetime; Wireless sensor networks,Energy efficiency; Energy utilization; Sensor data fusion; Sensor nodes; Wireless sensor networks; Approximate data collections; Data coverage; Distributed data; Heterogeneous sensing; Hot spot; Network lifetime; Data acquisition
"Guest editorial: Special issue on ""secure and fault-tolerant embedded computing""",2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022177675&doi=10.1145%2f307556&partnerID=40&md5=e3bdda56cba1daee3d6861ae7d336fa7,[No abstract available],,
"Area, throughput, and power trade-offs for FPGA- and ASIC-based execution stream compression",2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019841273&doi=10.1145%2f3063312&partnerID=40&md5=ab139df3a0d54954cca67a3271607edc,"An emerging trend in safety-critical computer system design is the use of compression-for example, using cyclic redundancy check (CRC) or Fletcher checksum (FC)- to reduce the state that must be compared to verify correct redundant execution. We examine the costs and performance of CRC and FC as compression algorithms when implemented in hardware for embedded safety-critical systems. Todo so, wehave developed parameterizable hardware-generation tools targeting CRC and two novel FC implementations. We evaluate the resulting designs implemented for FPGA and ASIC and analyze their efficiency. While CRC is often best, FC dominates when high throughput is needed. © 2017 ACM.",CRC; Fletcher checksum; Redundancy; Safety-critical systems,Application specific integrated circuits; Computer hardware; Economic and social effects; Embedded systems; Field programmable gate arrays (FPGA); Hardware; Redundancy; Security systems; Systems analysis; Check sums; Compression algorithms; Cyclic redundancy check; Emerging trends; Generation tools; High throughput; Safety critical systems; Stream compression; Safety engineering
A novel method for online detection of faults affecting execution-time in multicore-based systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019936407&doi=10.1145%2f3063313&partnerID=40&md5=eb3418821cae95b4e7439c1971155c15,"This article proposes a bounded interference method, based on statistical evaluations, for online detection and tolerance of any fault capable of causing a deadline miss. The proposed method requires data that can be gathered during the profiling and worst-case execution time (WCET) analysis phase. This article describes the method, its application, and then it presents an avionic mixed-criticality use case for experimental evaluation, considering both dual-core and quad-core platforms. Results show that faults that can cause a timing violation are correctly identified while other faults that do not introduce a significant temporal interference can be tolerated to avoid high recovery overheads. © 2017 ACM.",Hard real-time applications; Mixed-criticality applications; Multicore-based systems; Safety-critical systems; Software implemented hardware fault tolerance,Application programs; Criticality (nuclear fission); Fault detection; Fault tolerance; Multicore programming; Online systems; Safety engineering; Hard real time applications; Mixed criticalities; Multi core; Safety critical systems; Software implemented hardware fault tolerance; Real time systems
Formal model-based synthesis of application-specific static RTOS,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019894921&doi=10.1145%2f3015777&partnerID=40&md5=e7e32af42baeeac32146eea5cc778f81,"In an embedded system, the specialization of the code of the real-time operating system (RTOS) according to the requirements of the application allows one to remove unused services and other sources of dead code from the binary program. The typical specialization process is based on a mix of precompiler macros and build scripts, both of which are known for being sources of errors. In this article, we present a new model-based approach to the design of application-specific RTOS. Starting with finite state models describing the RTOS and the application requirements, the set of blocks in the RTOS code actually used by the application is automatically computed. This set is used to build an applicationspecific RTOS model. This model is fed into a code generator to produce the source code of an applicationspecific RTOS. It is also used to carry on model-based validations and verifications, including the formal verification that the specialization process did not introduce unwanted behaviors or suppress expected ones. To demonstrate the feasibility of this approach, it is applied to specialize Trampoline, an open-source implementation of the AUTOSAR OS standard, to an industrial case study from the automotive domain. © 2017 ACM.",Application-specific RTOS; Formal methods; Formal synthesis; Model-based verification and verification; OSEK/VDX and AUTOSAR RTOS,Application programs; Codes (symbols); Computer operating systems; Formal methods; Open source software; Application requirements; Application specific; AutoSAR; Formal synthesis; Model based verification; Model-based validation; Open source implementation; Real time operating system; Formal verification
Lightweight architectures for reliable and fault detection Simon and Speck cryptographic algorithms on FPGA,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019937854&doi=10.1145%2f3055514&partnerID=40&md5=5c58a31c58aab1a88f559830008fdc27,"The widespread use of sensitive and constrained applications necessitates lightweight (low-power and lowarea) algorithms developed for constrained nano-devices. However, nearly all of such algorithms are optimized for platform-based performance and may not be useful for diverse and flexible applications. The National Security Agency (NSA) has proposed two relatively recent families of lightweight ciphers, that is, Simon and Speck, designed as efficient ciphers on both hardware and software platforms. This article proposes concurrent error detection schemes to provide reliable architectures for these two families of lightweight block ciphers. The research work on analyzing the reliability of these algorithms and providing fault diagnosis approaches has not been undertaken to date to the best of our knowledge. The main aim of the proposed reliable architectures is to provide high error coverage while maintaining acceptable area and power consumption overheads. To achieve this, we propose a variant of recomputing with encoded operands. These low-complexity schemes are suited for low-resource applications such as sensitive, constrained implantable and wearable medical devices. We perform fault simulations for the proposed architectures by developing a fault model framework. The architectures are simulated and analyzed on recent field-programmable grate array (FPGA) platforms, and it is shown that the proposed schemes provide high error coverage. The proposed low-complexity concurrent error detection schemes are a step forward toward more reliable architectures for Simon and Speck algorithms in lightweight, secure applications. © 2017 ACM.",Field-programmable gate array (FPGA); Low complexity; Reliability; Simon; Speck,Biomedical equipment; Convolutional codes; Cryptography; Diagnosis; Error detection; Errors; Field programmable gate arrays (FPGA); Hardware security; National security; Reliability; Concurrent error detection schemes; Lightweight architecture; Lightweight block ciphers; Low complexity; National security agencies; Simon; Speck; Wearable medical devices; Fault detection
Parallel sparse subspace clustering via joint sample and parameter blockwise partition,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019963744&doi=10.1145%2f3063316&partnerID=40&md5=dd54ce71d2a7e112a7145127727c0796,"Sparse subspace clustering (SSC) is a classical method to cluster data with specific subspace structure for each group. It has many desirable theoretical properties and has been shown to be effective in various applications. However, under the condition of a large-scale dataset, learning the sparse sample affinity graph is computationally expensive. To tackle the computation time cost challenge, we develop a memoryefficient parallel framework for computing SSC via an alternating direction method of multiplier (ADMM) algorithm. The proposed framework partitions the data matrix into column blocks and then decomposes the original problem into parallel multivariate Lasso regression subproblems and samplewise operations. The proposed method allows us to allocate multiple cores/machines for the processing of individual column blocks. We propose a stochastic optimization algorithm to minimize the objective function. Experimental results on real-world datasets demonstrate that the proposed blockwise ADMM framework is substantially more efficient than its matrix counterpart used by SSC, without sacrificing performance in applications. Moreover, our approach is directly applicable to parallel neighborhood selection for Gaussian graphical models structure estimation. © 2017 ACM.",Parallel optimization; semi-supervised learning; sparsity; subspace clustering,Clustering algorithms; Matrix algebra; Supervised learning; Alternating direction method of multipliers; Gaussian graphical models; Parallel optimization; Semi- supervised learning; sparsity; Stochastic optimization algorithm; Structure estimation; Sub-Space Clustering; Optimization
Task transition scheduling for data-adaptable systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019868341&doi=10.1145%2f3047498&partnerID=40&md5=1fd24fd91523ad9c975ed4bb468acf2f,"Data-adaptable embedded systems operate on a variety of data streams, which requires a large degree of configurability and adaptability to support runtime changes in data stream inputs. Data-adaptable reconfigurable embedded systems, when decomposed into a series of tasks, enable a flexible runtime implementation in which a system can transition the execution of certain tasks between hardware and software while simultaneously continuing to process data during the transition. Efficient runtime scheduling of task transitions is needed to optimize system throughput and latency of the reconfiguration and transition periods. In this article, we provide an overview of a runtime framework enabling the efficient transition of tasks between software and hardware in response to changes in system inputs. We further present and analyze several runtime transition scheduling algorithms and highlight the latency and throughput tradeoffs for two dataadaptable systems. To evaluate the task transition selection algorithms, a case study was performed on an adaptable JPEG2000 implementation as well as three other synchronous dataflow systems characterized by transition latency and communication load. © 2017 ACM.",Data adaptability; Hardware/software codesign; Model-based design; Runtime transition scheduling,Computer hardware; Data communication systems; Hardware; Hardware-software codesign; Scheduling; Scheduling algorithms; Data adaptability; Hardware and software; Model- based designs; Reconfigurable embedded systems; Runtimes; Selection algorithm; Software and hardwares; Synchronous Dataflow; Embedded systems
Data-driven synchronization for internet-of-things systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019901121&doi=10.1145%2f2983627&partnerID=40&md5=df51d8b50b02503961e2c471f98a463b,"The Internet of Things (IoT) is fueled by the growth of sensors, actuators, and services that collect and process raw sensor data. Wearable and environmental sensors will be a major component of the IoT and provide context about people and activities that are occurring. It is imperative that sensors in the IoT are synchronized, which increases the usefulness and value of the sensor data and allows data from multiple sources to be combined and compared. Due to the heterogeneous nature of sensors (e.g., synchronization protocols, communication channels, etc.), synchronization can be difficult. In this article, we present novel techniques for synchronizing data from multi-sensor environments based on the events and interactions measured by the sensors. We present methods to determine which interactions can likely be used for synchronization and methods to improve synchronization by removing erroneous synchronization points. We validate our technique through experiments with wearable and environmental sensors in a laboratory environment. Experiments resulted in median drift error reduction from 66% to 98% for sensors synchronized through physical interactions. © 2017 ACM.",Internet of things; sensor networks; time synchronization,Internet of things; Sensor networks; Synchronization; Wearable technology; Environmental sensor; Internet of thing (IOT); Laboratory environment; Novel techniques; Physical interactions; Synchronization points; Synchronization protocols; Time synchronization; Wearable sensors
A multi-quadcopter cooperative cyber-physical system for timely air pollution localization,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019899569&doi=10.1145%2f3005716&partnerID=40&md5=91fbb768c824576074500b894e03c8e8,"We propose a cyber-physical system of unmanned quadcopters to locate air pollution sources in a timely manner. The system consists of a physical part and a cyber part. The physical part includes unmanned quadcopters equipped with multiple sensors. The cyber part carries out control laws.We simplify the control laws by decoupling the quadcopters' horizontal-plane motion control from vertical motion control. To control the quadcopter's horizontal-plane motions, we propose a controller that combines pollutant dynamics with quadcopter physics. To control the quadcopter's vertical motions, we adopt an anti-windup proportionalintegral (PI) controller. We further extend the horizontal-plane control laws from a single quadcopter to multiple quadcopters. The multi-quadcopter control laws are distributed and convergent. We implement a prototype quadcopter and carry out experiments to verify the vertical control laws. We also carry out simulations to evaluate the horizontal-plane control laws. With quadcopter parameters set commensurate with our prototype implementation's, our simulations show that the control laws can drive quadcopters to locate pollution source(s) in a timely way. © 2017 ACM.",air pollution source; control laws; Cyber-physical system; multiquadcopter; pollutant dynamics,Air pollution; Control theory; Controllers; Cyber Physical System; Embedded systems; Motion control; Pollution; Pollution control; Air pollution sources; Control laws; Horizontal plane motions; multiquadcopter; Pollution sources; Proportional integral controllers; Prototype implementations; Vertical motions; Air pollution control
Exploiting multiple write modes of Nonvolatile main memory in embedded systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019921250&doi=10.1145%2f3063130&partnerID=40&md5=adb1979495278bb03c4dd4652c7ca6d5,"Existing Nonvolatile Memories (NVMs) have many attractive features to be the main memory of embedded systems. These features include low power, high density, and better scalability. Recently, Multilevel Cell (MLC) NVM has gained more and more popularity as it can provide a higher density than the traditional Single-Level Cell (SLC) NVM. However, there are also drawbacks in MLC NVM, namely, limited write endurance and expensive write operation. These two drawbacks have to be overcome before MLC NVM can be practically adopted as the main memory. In MLC Nonvolatile Main Memory (NVMM), two different types of write operations with very diverse data retention times are allowed. The first type maintains data for years but takes a longer time to write and is detrimental to the endurance. The second type maintains data for a short period but takes a shorter time to write. By observing that much of the data written to main memory is temporary and does not need to last long during the execution of a program, in this article, we propose novel task scheduling and write operation selection algorithms to improve MLC NVMM endurance and program efficiency. An Integer Linear Programming (ILP) formulation is first proposed to obtain optimal results. Since ILP takes exponential time to solve, we also propose the Multiwrite Mode-Aware Scheduling (MMAS) algorithm to achieve a near-optimal solution in polynomial time. Additionally, the Dynamical Memory Block Screening (DMS) algorithm is proposed to achieve wear leveling. The experimental results demonstrate that the proposed techniques can greatly improve the lifetime of the MLC NVMM as well as the efficiency of the program. © 2017 ACM.",Multilevel Cell (MLC); Multiwrite modes; Nonvolatile memory; Wear leveling,Efficiency; Embedded systems; Flash memory; Integer programming; Polynomial approximation; Polynomials; Integer Linear Programming; Multi level cell (MLC); Multiwrite modes; Near-optimal solutions; Non-volatile main memory; Non-volatile memory; Selection algorithm; Wear leveling; Multitasking
"Guest editorial for ACM TECS special issue on effective divide-and-conquer, incremental, or distributed mechanisms of embedded designs for extremely big data in large-scale devices",2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019951315&doi=10.1145%2f3068457&partnerID=40&md5=a35d93fe69e8ca41af91cc5e851f9038,[No abstract available],,
Wcet-aware function-level dynamic code management on Scratchpad memory,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019904220&doi=10.1145%2f3063383&partnerID=40&md5=b0ef2bb3c0e14ff2eea8e956cccbad52,"Scratchpad memory (SPM) is a promising on-chip memory choice in real-time and cyber-physical systems where timing is of the utmost importance. SPM has time-predictable characteristics since its data movement between the SPM and the main memory is entirely managed by software. One way of such management is dynamic management. In dynamic management of instruction SPMs, code blocks are dynamically copied from the main memory to the SPM at runtime by executing direct memory access (DMA) instructions. Code management techniques try to minimize the overhead of DMA operations by finding an allocation scheme that leads to efficient utilization. In this article, we present three function-level code management techniques. These techniques perform allocation at the granularity of functions, with the objective of minimizing the impact of DMA overhead to the worst-case execution time (WCET) of a given program. The first technique finds an optimal mapping of each function to a region using integer linear programming (ILP), whereas the second technique is a polynomial-time heuristic that is suboptimal. The third technique maps functions directly to SPM addresses, not using regions, which can further reduce the WCET. Based on ILP, it can also find an optimal mapping. We evaluate our techniques using the Mälardalen WCET suite, MiBench suite, and proprietary automotive applications from industry. The results show that our techniques can significantly reduce the WCET estimates compared to caches with the state-of-the-art cache analysis. © 2017 ACM.",Code management; Hard real-time systems; Scratchpad memory (SPM); Worst-case execution time (WCET),Codes (symbols); Embedded systems; Industrial management; Integer programming; Interactive computer systems; Mapping; Memory architecture; Optimization; Polynomial approximation; Automotive applications; Code management; Direct memory access; Dynamic management; Hard real-time systems; Integer Linear Programming; Scratch pad memory; Worst-case execution time; Real time systems
PMC: A requirement-aware dram controller for multicore Mixed Criticality Systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019951047&doi=10.1145%2f3019611&partnerID=40&md5=9ea35bc20e32f6fcaa912534246a2b57,"We propose a novel approach to schedule memory requests in Mixed Criticality Systems (MCS). This approach supports an arbitrary number of criticality levels by enabling the MCS designer to specify memory requirements per task. It retains locality within large-size requests to satisfy memory requirements of all tasks. To achieve this target, we introduce a compact time-division-multiplexing scheduler, and a framework that constructs optimal schedules to manage requests to off-chip memory. We also present a static analysis that guarantees meeting requirements of all tasks. We compare the proposed controller against state-of-the-art memory controllers using both a case study and synthetic experiments. © 2017 ACM.",Dram; MCS; Memory; Memory controller; Mixed Criticality; Real-time,Criticality (nuclear fission); Data storage equipment; Dynamic random access storage; Time division multiplexing; Arbitrary number; Memory controller; Memory requirements; Mixed criticalities; Mixed-criticality systems; Real time; Requirement-Aware; Synthetic experiments; Controllers
A load-balancing divide-and-conquer SVM solver,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019898400&doi=10.1145%2f3005347&partnerID=40&md5=f7d251231ea76db09c145f65be89a465,"Scaling up kernel support vector machine (SVM) training has been an important topic in recent years. Despite its theoretical elegance, training kernel SVM is impractical when facing millions of data. The divideand- conquer (DC) strategy is a natural framework of handling gigantic problems, and the divide-and-conquer solver for kernel SVM (DC-SVM) is able to train kernel SVM with millions of data with limited time cost. However, there are some drawbacks of the DC-SVM approach. First, it used an unsupervised clustering method to partition the whole problem, which is prone to construct singular subsets, and, second, it is hard to balance the computation load between sub-problems. To address these issues, this article proposed a load-balancing partition method for kernel SVM. First, it clusters sample from one class and then assigns data samples to the cluster centers by a distance measure and construct sub-problems; in this way, it is able to control the computation load and avoid singular problems. Experimental results show that the proposed method has better load-balancing performance than DC-SVM, which implies that it is suitable for distributed and embedding systems. © 2017 ACM.",divide-and-conquer solver; Kernel support vector machine; load balancing,Hardware; Resource allocation; Software engineering; Cluster centers; Computation loads; Distance measure; Divide and conquer; Partition methods; Singular problem; Sub-problems; Unsupervised clustering methods; Support vector machines
Protecting caches from soft errors: A microarchitect's perspective,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019840029&doi=10.1145%2f3063180&partnerID=40&md5=b6c3fdcf7f1aa194f0b5d9e531e6355d,"Soft error is one of the most important design concerns in modern embedded systems with aggressive technology scaling. Among various microarchitectural components in a processor, cache is the most susceptible component to soft errors. Error detection and correction codes are common protection techniques for cache memory due to their design simplicity. In order to design effective protection techniques for caches, it is important to quantitatively estimate the susceptibility of caches without and even with protections. At the architectural level, vulnerability is the metric to quantify the susceptibility of data in caches. However, existing tools and techniques calculate the vulnerability of data in caches through coarse-grained block-level estimation. Further, they ignore common cache protection techniques such as error detection and correction codes. In this article, we demonstrate that our word-level vulnerability estimation is accurate through intensive fault injection campaigns as compared to block-level one. Further, our extensive experiments over benchmark suites reveal several counter-intuitive and interesting results. Parity checking when performed over just reads provides reliable and power-efficient protection than that when performed over both reads and writes. On the other hand, checking error correcting codes only at reads alone can be vulnerable even for single-bit soft errors, while that at both reads and writes provides the perfect reliability. © 2017 ACM.",Cache; Error correction code; Parity code; Reliability; Simulation; Soft error; Transient fault; Vulnerability,Cache memory; Codes (symbols); Embedded systems; Error detection; Radiation hardening; Reliability; Cache; Error correction codes; Parity codes; Simulation; Soft error; Transient faults; Vulnerability; Error correction
The perfect getaway: Using escape analysis in embedded real-time systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019942220&doi=10.1145%2f3035542&partnerID=40&md5=180af22cd931ce7e3d7f9e4621408462,"The use of a managed, type-safe language such as Java in real-time and embedded systems offers productivity and, in particular, safety and dependability benefits at a reasonable cost. It has been shown for commodity systems that Escape Analysis (EA) enables a set of useful optimizations, and benefits from the properties of a type-safe language. In this article, we explore the application of escape analysis in KESO [Stilkerich et al. 2012], a Java ahead-of-time compiler targeting embedded real-time systems. We present specific applications of EA for embedded programs that go beyond the widely known stack-allocation and synchronization optimizations such as extended remote-procedure-call (RPC) support for software-isolated applications, automated inference of immutable data, or improved upper space and time bounds for worst-case estimations. © 2017 ACM.",Escape Analysis; JVM; KESO; Memory management; Regional memory,Application programs; Embedded systems; Interactive computer systems; Java programming language; Embedded real time systems; Escape analysis; KESO; Memory management; Real-time and embedded systems; Remote procedure calls; Safety and dependability; Type-safe languages; Real time systems
Automatic synthesis of switching controllers for linear hybrid systems: Reachability control,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019861346&doi=10.1145%2f3047500&partnerID=40&md5=a1c9c0698fd9fcf978656130a36f0341,"We consider the problem of computing the controllable region of a Linear Hybrid Automaton with controllable and uncontrollable transitions, w.r.t. a reachability objective. We provide an algorithm for the finite-horizon version of the problem, based on computing the set of states that must reach a given non-convex polyhedron while avoiding another one, subject to a polyhedral constraint on the slope of the trajectory. Experimental results are presented, based on an implementation of the proposed algorithm on top of the tool SpaceEx. © 2017 ACM.",Controller synthesis; Hybrid automata; Reachability games,Hybrid systems; Automatic synthesis; Controller synthesis; Hybrid automatons; Linear hybrid automata; Linear hybrid systems; Polyhedral constraints; Reachability; Switching controllers; Linear systems
Dynamic power and energy management for energy harvesting nonvolatile processor systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019883189&doi=10.1145%2f3077575&partnerID=40&md5=7797e907ccd672f8ca705dd4524805cd,"Self-powered systems running on scavenged energy will be a key enabler for pervasive computing across the Internet of Things. The variability of input power in energy-harvesting systems limits the effectiveness of static optimizations aimed at maximizing the input-energy-to-computation ratio. We show that the resultant gap between available and exploitable energy is significant, and that energy storage optimizations alone do not significantly close the gap. We characterize these effects on a real, fabricated energy-harvesting system based on a nonvolatile processor. We introduce a unified energy-oriented approach to first optimize the number of backups, by more aggressively using the stored energy available when power failure occurs, and then optimize forward progress via improving the rate of input energy to computation via dynamic voltage and frequency scaling and self-learning techniques. We evaluate combining these schemes and show capture of up to 75.5% of all input energy toward processor computation, an average of 1.54× increase over the best static ""Forward Progress"" baseline system. Notably, our energy-optimizing policy combinations simultaneously improve both the rate of forward progress and the rate of backup events (by up to 60.7% and 79.2% for RF power, respectively, and up to 231.2% and reduced to zero, respectively, for solar power). This contrasts with static frequency optimization approaches in which these two metrics are antagonistic. © 2017 ACM.",Dynamic power and energy management; Energy harvesting; Intermittent power supply; Nonvolatile processor,Dynamic frequency scaling; Energy management; Solar energy; Ubiquitous computing; Voltage scaling; Dynamic Power; Dynamic voltage and frequency scaling; Energy harvesting systems; Non-volatile; Optimizing policies; Power supply; Self-powered systems; Storage optimization; Energy harvesting
Real-time simulation support for runtime verification of cyber-physical systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019898160&doi=10.1145%2f3063382&partnerID=40&md5=8898952061f11e40aed6c7187b28fdc4,"In Cyber-Physical Systems (CPS), cyber and physical components must work seamlessly in tandem. Runtime verification of CPS is essential yet very difficult, due to deployment environments that are expensive, dangerous, or simply impossible to use for verification tasks. A key enabling factor of runtime verification of CPS is the ability to integrate real-time simulations of portions of the CPS into live running systems. We propose a verification approach that allows CPS application developers to opportunistically leverage real-time simulation to support runtime verification. Our approach, termed BRACEBIND, allows selecting, at runtime, between actual physical processes or simulations of them to support a running CPS application. To build BRACEBIND, we create a real-time simulation architecture to generate and manage multiple realtime simulation environments based on existing simulation models in a manner that ensures sufficient accuracy for verifying a CPS application. Specifically, BRACEBIND aims to both improve simulation speed and minimize latency, thereby making it feasible to integrate simulations of physical processes into the running CPS application. BRACEBIND then integrates this real-time simulation architecture with an existing runtime verification approach that has low computational overhead and high accuracy. This integration uses an aspect-oriented adapter architecture that connects the variables in the cyber portion of the CPS application with either sensors and actuators in the physical world or the automatically generated real-time simulation. Our experimental results show that, with a negligible performance penalty, our approach is both efficient and effective in detecting program errors that are otherwise only detectable in a physical deployment. © 2017 ACM.",Runtime verification,Architecture; Cyber Physical System; Embedded systems; Transients; Application developers; Automatically generated; Computational overheads; Cyber-physical systems (CPS); Performance penalties; Real time simulations; Run-time verification; Sensors and actuators; Real time systems
Refining cache behavior prediction using cache miss paths,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019959872&doi=10.1145%2f3035541&partnerID=40&md5=50ca099eca2510315c57a9136a17d2b1,"Worst-Case Execution Time (WCET) is an important metric for programs running on real-time systems, and finding precise estimates of a program's WCET is crucial to avoid wastage of hardware resources and to improve the schedulability of task sets. Caches have a major impact on a program's execution time, and accurate estimation of a program's cache behavior can lead to significant reduction in its estimated WCET. The traditional approach to cache analysis generally targets the worst-case cache behavior of individual cache accesses and provides a safe hit-miss classification for every individual access. In this work, we show that these classifications are not sufficient to precisely capture cache behavior, since they apply to individual accesses, and often, more precise predictions can be made about groups of accesses. Further, memory accesses inside loops may show the worst-case behavior only for a subset of the iteration space. In order to predict such behavior in a scalable fashion, we use the fact that the cache behavior of an access mostly depends only on the memory accesses made in the immediate vicinity, and hence we analyze a small, fixed-size neighborhood of every access with complete precision and summarize the resulting information in the form of cache miss paths. A variety of analyses are then performed on the cache miss paths to make precise predictions about cache behavior. We also demonstrate precision issues in Abstract Interpretation-based Must and Persistence cache analysis that can be easily solved using cache miss paths. Experimental results over a wide range of benchmarks demonstrate precision improvement in WCET of multipath programs over previous approaches, and we also show how to integrate our approach with other microarchitectural analysis such as pipeline analysis. © 2017 ACM.",Cache analysis; Worst case execution time estimation,Forecasting; Interactive computer systems; Real time systems; Abstract interpretations; Accurate estimation; Cache analysis; Micro-architectural analysis; Precision improvement; Traditional approaches; Worst-case behaviors; Worst-case execution time; Cache memory
On space utilization enhancement of file systems for embedded storage systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017522395&doi=10.1145%2f2820488&partnerID=40&md5=9c6e28541bfb432bc4ce2fd9981dc2f1,"Since the mid-2000s, mobile/embedded computing systems conventionally have limited computing power, Random Access Memory (RAM) space, and storage capacity due to the consideration of their cost, energy consumption, and physical size. Recently, some of these systems, such as mobile phone and embedded consumer electronics, have more powerful computing capability, so they manage their data in small flash storage devices (e.g., Embedded Multi Media Card (eMMC) and Secure Digital (SD) cards) with a simple file system. However, the existing file systems usually have low space utilization for managing small files and the tail data of large files. In this work, we thus propose a dynamic tail packing scheme to enhance the space utilization of file systems over flash storage devices in embedded computing systems by dynamically aggregating/packing the tail data of (small) files together. To evaluate the benefits and overheads of the proposed scheme, we theoretically formulate analysis equations for obtaining the best settings in the dynamic tail packing scheme. Additionally, the proposed scheme was implemented in the file system of Linux operating systems to evaluate its capability. The results demonstrate that the proposed scheme could significantly improve the space utilization of existing file systems. © 2017 ACM.",Byte-addressibility; D.4.2 [operating system]: storage management; D.4.3 [operating system]: file systems management; Design; Embedded file systems; Experimentation; Management; Measurement; Performance; Small files; Space utilization; Tail packing,Cellular telephone systems; Computer operating systems; Design; Digital devices; Digital storage; Embedded systems; Energy utilization; File organization; Green computing; Management; Measurements; Random access storage; Storage management; Virtual storage; Byte-addressibility; Embedded file systems; Experimentation; File systems; Performance; Small files; Space utilization; Information management
A PUF-based secure communication protocol for IoT,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019548440&doi=10.1145%2f3005715&partnerID=40&md5=336aabe4f7b683f9abb04d8ebefff3d9,"Security features are of paramount importance for the Internet of Things (IoT), and implementations are challenging given the resource-constrained IoT setup. We have developed a lightweight identity-based cryptosystem suitable for IoT to enable secure authentication and message exchange among the devices. Our scheme employs a Physically Unclonable Function (PUF) to generate the public identity of each device, which is used as the public key for each device for message encryption. We have provided formal proofs of security in the Session Key Security and Universally Composable Framework of the proposed protocol, which demonstrates the resilience of the scheme against passive and active attacks. We have demonstrated the setup required for the protocol implementation and shown that the proposed protocol implementation incurs low hardware and software overhead. © 2017 ACM.",Communication protocol; Identity-based cryptography; Internet-ofthings; Physically unclonable function; Security analysis,Cryptography; Hardware security; Network protocols; Security systems; Identity based cryptography; Identity-based cryptosystem; Internet of thing (IOT); Passive and active attacks; Physically unclonable functions; Protocol implementation; Security analysis; Universally composable; Internet of things
Axiom: DTLS-based secure IoT group communication,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019542588&doi=10.1145%2f3047413&partnerID=40&md5=9824cdc6c0ace02c74077ffb81726843,"This article presents Axiom, a DTLS-based approach to efficiently secure multicast group communication among IoT-constrained devices. Axiom provides an adaptation of the DTLS record layer, relies on key material commonly shared among the group members, and does not require one to perform any DTLS handshake. We made a proof-of-concept implementation of Axiom based on the tinyDTLS library for the Contiki OS and used it to experimentally evaluate performance of our approach on real IoT hardware. Results show that Axiom is affordable on resource-constrained platforms and performs significantly better than related alternative approaches. © 2017 ACM.",Dtls; Group communication; Internet of things; Multicast; Security,Multicasting; Constrained devices; Dtls; Group communications; Group members; Key materials; Proof of concept; Secure multicasts; Security; Internet of things
LiBrA-CAN: Lightweight broadcast authentication for controller area networks,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017516663&doi=10.1145%2f3056506&partnerID=40&md5=57c365c28b52d544a7408f3e457224ff,"Despite realistic concerns, security is still absent from vehicular buses such as the widely used Controller Area Network (CAN). We design an efficient protocol based on efficient symmetric primitives, taking advantage of two innovative procedures: splitting keys between nodes and mixing authentication tags. This results in a higher security level when compromised nodes are in the minority, a realistic assumption for automotive networks. Experiments are performed on state-of-the-art Infineon TriCore controllers, contrasted with low-end Freescale S12X cores, while simulations are provided for the recently released CAN-FD standard. To gain compatibility with existent networks, we also discuss a solution based on CAN+. © 2017 ACM.",Algorithms; Authentication; Broadcast; CAN bus; Cryptography; Performance; Security,Algorithms; Authentication; Broadcasting; Control system synthesis; Controllers; Cryptography; Process control; Authentication tags; Automotive networks; Broadcast authentication; CAN bus; Controller area network; Efficient protocols; Performance; Security; Network security
Fault-tolerant preemptive aperiodic RT scheduling by supervisory control of TDES on multiprocessors,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017520475&doi=10.1145%2f3012278&partnerID=40&md5=f45c470ad000014c8af4f53ffa7c4da3,"Safety-critical real-time systems must meet stringent timing and fault-tolerance requirements. This article proposes a methodology for synthesizing an optimal preemptive multiprocessor aperiodic task scheduler using a formal supervisory control framework. The scheduler can tolerate single/multiple permanent processor faults. Further, the synthesis framework has been empowered with a novel BDD-based symbolic computation mechanism to control the exponential state-space complexity of the optimal exhaustive enumeration-oriented synthesis methodology. © 2017 ACM.",Discrete event systems; Faulttolerance; Formal methods; Multiprocessors; Preemptive scheduling; Supervisory control,Discrete event simulation; Fault tolerance; Formal methods; Interactive computer systems; Multiprocessing systems; Safety engineering; Scheduling; Aperiodic task; Exhaustive enumeration; Multiprocessors; Pre-emptive scheduling; Supervisory control; Symbolic computation; Synthesis methodology; Tolerance requirement; Real time systems
Low-cost standard signatures for energy-harvesting wireless sensor networks,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019645277&doi=10.1145%2f2994603&partnerID=40&md5=7ac268dad2f03cebbfffb916bd236150,"This work is motivated by a general question: can micro-scale energy-harvesting techniques be exploited to support low-cost standard security solutions on resource-constrained devices? We focus on guaranteeing integrity and authentication in Internet of Things (IoT) and Wireless Sensor Network (WSN) applications. In this article, we propose techniques to make ECDSA signatures low cost and implementable on resourceconstrained devices. By combining precomputation techniques and energy-harvesting capabilities of modern sensor nodes, we achieve significant improvement over prior works. In addition, we show that the cost of ECDSA signatures can be reduced by up to a factor 10 by using harvesting-Aware optimizations. © 2017 ACM.",Energy harvesting; Iot and wsns security; Low-energy authentication; Low-energy digital signatures; Ultra-low-energy systems; Wireless sensor networks,Authentication; Costs; Cryptography; Energy harvesting; Internet of things; Low power electronics; Network security; Sensor nodes; Internet of Things (IOT); Low costs; Precomputation techniques; Resourceconstrained devices; Security solutions; Ultra low energy; WSNs securities; Wireless sensor networks
Energy-aware memory mapping for hybrid FRAM-SRAM MCUs in intermittently-powered IoT devices,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019879953&doi=10.1145%2f2983628&partnerID=40&md5=4e8dd159cd83ebff7987b9ac84964095,"Forecasts project that by 2020, there will be around 50 billion devices connected to the Internet of Things (IoT), most of which will operate untethered and unplugged. While environmental energy harvesting is a promising solution to power these IoT edge devices, it introduces new complexities due to the unreliable nature of ambient energy sources. In the presence of an unreliable power supply, frequent checkpointing of the system state becomes imperative, and recent research has proposed the concept of in-situ checkpointing by using ferroelectric RAM (FRAM), an emerging non-volatile memory technology, as unified memory in these systems. Even though an entirely FRAM-based solution provides reliability, it is energy inefficient compared to SRAM due to the higher access latency of FRAM. On the other hand, an entirely SRAM-based solution is highly energy efficient but is unreliable in the face of power loss. This paper advocates an intermediate approach in hybrid FRAM-SRAM microcontrollers that involves judicious memory mapping of program sections to retain the reliability benefits provided by FRAM while performing almost as efficiently as an SRAM-based system. We propose an energy-aware memory mapping technique that maps different program sections to the hybrid FRAM-SRAM microcontroller such that energy consumption is minimized without sacrificing reliability. Our technique consists of eM-map, which performs a one-time characterization to find the optimal memory map for the functions that constitute a program and energy-align, a novel hardware-software technique that aligns the system's powered-on time intervals to function execution boundaries, which results in further improvements in energy efficiency and performance. Experimental results obtained using the MSP430FR5739 microcontroller demonstrate a significant performance improvement of up to 2x and energy reduction of up to 20% over a state-of-the-art FRAM-based solution. Finally, we present a case study that shows the implementation of our techniques in the context of a real IoT application. © 2017 ACM.",batteryless systems; checkpointing; energy harvesting; ferroelectric RAM; Intermittently powered systems; internet of things,Controllers; Data storage equipment; Digital storage; Electric power systems; Energy efficiency; Energy harvesting; Energy utilization; Ferroelectricity; Internet of things; Mapping; Microcontrollers; Power management; Random access storage; Reliability; Static random access storage; Batteryless systems; Check pointing; Efficiency and performance; Emerging Non-volatile memory technology; Environmental energy; Ferro-electric rams; Internet of thing (IOT); Reliability benefits; Ferroelectric devices
DMS-based energy optimizations for clustered WSNs,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017587176&doi=10.1145%2f2998179&partnerID=40&md5=c67fa016453987b3ba2d7384227c311c,"In this article, we consider clustered wireless sensor networks where the nodes harvest energy from the environment. We target performance-sensitive applications that have to collectively send their information to a cluster head by a predefined deadline. The nodes are equipped with Dynamic Modulation Scaling (DMS)-capable wireless radios. DMS provides a tuning knob, allowing us to trade off communication latency with energy consumption. We consider two optimization objectives, maximizing total energy reserves and maximizing the minimum energy level across all nodes. For both objectives, we show that optimal solutions can be obtained by solving Mixed Integer Linear Programming problems. We also develop several fast heuristics that are shown to provide approximate solutions experimentally. © 2017 ACM.",Dynamic modulation scaling; Energy management; Wireless sensor networks,Economic and social effects; Energy management; Energy utilization; Integer programming; Modulation; Optimization; Sensor nodes; Approximate solution; Clustered wireless sensor networks; Communication latency; Dynamic modulation; Energy optimization; Mixed integer linear programming problems; Optimal solutions; Sensitive application; Wireless sensor networks
System-level design optimization for security-critical cyber-physical-social systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018472089&doi=10.1145%2f2925991&partnerID=40&md5=700bad7cb687f3d9b0bd8fd2123d6c23,"Cyber-physical-social systems (CPSS), an emerging computing paradigm, have attracted intensive attentions from the research community and industry. We are facing various challenges in designing secure, reliable, and user-satisfied CPSS. In this article, we consider these design issues as a whole and propose a system-level design optimization framework for CPSS design where energy consumption, security-level, and user satisfaction requirements can be fulfilled while satisfying constraints for system reliability. Specifically, we model the constraints (energy efficiency, security, and reliability) as the penalty functions to be incorporated into the corresponding objective functions for the optimization problem. A smart office application is presented to demonstrate the feasibility and effectiveness of our proposed design optimization approach. © 2017 ACM.",Algorithms; B.8.2 [performance and reliability]: performance analysis and design aids; Cyber-physical-social systems; Design; Design optimization; Performance; Pervasive computing; Security critical; System-level design,Algorithms; Cyber Physical System; Design; Energy efficiency; Energy utilization; Optimization; Reliability analysis; Ubiquitous computing; Design optimization; Performance; Performance analysis and design aids; Security-critical; Social systems; System level design; Reliability
Hardware architectures for embedded speaker recognition applications: A survey,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019594352&doi=10.1145%2f2975161&partnerID=40&md5=d04899892ab78f31e04d6129008ff23e,"Authentication technologies based on biometrics, such as speaker recognition, are attracting more and more interest thanks to the elevated level of security offered by these technologies. Despite offering many advantages, such as remote use and low vulnerability, speaker recognition applications are constrained by the heavy computational effort and the hard real-Time constraints. When such applications are run on an embedded platform, the problem becomesmore challenging, as additional constraints inherent to this specific domain are added. In the literature, different hardware architectures were used/designed for implementing a process with a focus on a given particularmetric. In this article, we give a survey of the state-of-The-Art works on implementations of embedded speaker recognition applications. Our aim is to provide an overview of the different approaches dealing with acceleration techniques oriented towards speaker and speech recognition applications and attempt to identify the past, current, and future research trends in the area. Indeed, on the one hand, many flexible solutions were implemented, using either General Purpose Processors or Digital Signal Processors. In general, these types of solutions suffer from low area and energy efficiency. On the other hand, high-performance solutions were implemented on Application Specific Integrated Circuits or Field Programmable Gate Arrays but at the expense of flexibility. Based on the available results, we compare the application requirements vis-  a-vis the performance achieved by the systems. This leads to the projection of new research trends that can be undertaken in the future. © 2017 ACM.",Acceleration; Classification algorithms and implementations; Embedded hardware; Speaker recognition,Acceleration; Digital signal processors; Embedded systems; Energy efficiency; Field programmable gate arrays (FPGA); General purpose computers; Hardware; Real time systems; Signal processing; Surveys; Acceleration technique; Application requirements; Authentication technology; Classification algorithm; Embedded hardware; General purpose processors; Hardware architecture; Speaker recognition; Speech recognition
Configurable detection of SDC-causing errors in programs,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017269987&doi=10.1145%2f3014586&partnerID=40&md5=43de59cced6ccdc232e1c9a6f640a7c3,"Silent Data Corruption (SDC) is a serious reliability issue in many domains, including embedded systems. However, current protection techniques are brittle and do not allow programmers to trade off performance for SDC coverage. Further, many require tens of thousands of fault-injection experiments, which are highly time- and resource-intensive. In this article, we propose two empirical models, SDCTune and SDCAuto, to predict the SDC proneness of a program's data. Both models are based on static and dynamic features of the program alone and do not require fault injections to be performed. The main difference between them is that SDCTune requires manual tuning while SDCAuto is completely automated, using machine-learning algorithms. We then develop an algorithm using both models to selectively protect the most SDC-prone data in the program subject to a given performance overhead bound. Our results show that both models are accurate at predicting the relative SDC rate of an application compared to fault injection, for a fraction of the time taken. Further, in terms of efficiency of detection (i.e., ratio of SDC coverage provided to performance overhead), our technique outperforms full duplication by a factor of 0.78x to 1.65x with the SDCTune model and 0.62x to 0.96x with SDCAuto model. © 2017 ACM.",Compiler; Error detection; Fault tolerance; Modeling; Reliability,Economic and social effects; Embedded systems; Error detection; Learning algorithms; Learning systems; Models; Reliability; Software testing; Compiler; Current protection; Dynamic features; Empirical model; Fault injection; Manual tuning; Silent data corruption (SDC); Trade off; Fault tolerance
Accelerators for breast cancer detection,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017221107&doi=10.1145%2f2983630&partnerID=40&md5=a8b7fd582a39c0b15a8d944f96e59a87,"Algorithms used in microwave imaging for breast cancer detection require hardware acceleration to speed up execution time and reduce power consumption. In this article, we present the hardware implementation of two accelerators for two alternative imaging algorithms that we obtain entirely from SystemC specifications via high-level synthesis. The two algorithms present opposite characteristics that stress the design process and the capabilities of commercial HLS tools in different ways: the first is communication bound and requires overlapping and pipelining of communication and computation in order to maximize the application throughput; the second is computation bound and uses complex mathematical functions that HLS tools do not directly support. Despite these difficulties, thanks to HLS, in the span of only 4 months we were able to explore a large design space and derive about 100 implementations with different cost-performance profiles, targeting both a Field-Programmable Gate Array (FPGA) platform and a 32-nm standard-cell Application Specific Integrated Circuit (ASIC) library. In addition, we could obtain results that outperform a previous Register-Transfer Level (RTL) implementation, which confirms the remarkable progress of HLS tools. © 2017 ACM.",Microwave imaging for breast cancer detection,Diseases; Field programmable gate arrays (FPGA); Functions; Hardware; Imaging systems; Integrated circuit design; Medical imaging; Multiprocessing systems; Breast cancer detection; Cost performance; Hardware acceleration; Hardware implementations; Imaging algorithm; Mathematical functions; Microwave imaging; Register transfer level; High level synthesis
Task-FIFO co-scheduling of streaming applications on MPSoCs with predictable memory hierarchy,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017183693&doi=10.1145%2f3038484&partnerID=40&md5=deca87b05d84a7c6172492ddb5eb8186,"This article studies the scheduling of real-time streaming applications on multiprocessor systems-on-chips with predictable memory hierarchy. An iteration-based task-FIFO co-scheduling framework is proposed for this problem. We obtain FIFO size distributions using Pareto space searching, based on which the task-toprocessor mapping is obtained with the potential FIFO allocation being taken into account; then, the FIFOto-memory allocation is optimized to minimize the total memory access cost; finally, a self-timed throughput analysis method that considers memory and direct memory access controller contention is utilized to analyze the throughput. Our methods are validated by a set of synthesized and practical applications on different platforms. © 2017 ACM.",FIFO; Scheduling; Scratch pad memory; SDFG; Self-timed,Computer architecture; Fault tolerance; Iterative methods; Memory architecture; Real time systems; Scheduling; System-on-chip; Direct-memory-access controllers; FIFO; Multiprocessor systems on chips; Scratch pad memory; SDFG; Self-timed; Streaming applications; Throughput analysis; Multitasking
On static binary translation of ARM/Thumb Mixed ISA binaries,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017209808&doi=10.1145%2f2996458&partnerID=40&md5=dd77d95f0deabfebb5ed91677f870205,"Code discovery has been amain challenge for static binary translation, especially when the source instruction set architecture has variable-length instructions, such as the x86 architectures. Due to embedded data such as PC (program counter)-relative data, jump tables, or paddings in the code section, a binary translator may be misled to translate data as instructions. For variable-length instructions, once a piece of data is mis-translated as instructions, decoding subsequent bytes could also go wrong. We are concerned with static binary translation for the very popular Advanced RISC Machine (ARM) architectures. Although ARM is considered a reduced instruction set computer architecture, it does allow the mix of 32-bit (ARM) instructions and 16-bit (Thumb) instructions in the same executables. In addition to different instruction lengths, the ARM and Thumb instructions are located at 4-byte or 2-byte aligned addresses, respectively. Furthermore, because ARM and Thumb instructions share the same encoding space, a 4-byte word could sometimes be decoded as one ARM instruction or two Thumb instructions. The correct decoding of this 4-byte word is actually determined at runtime by the least-significant bit of the program counter. For unstripped binaries, the mapping symbols can be used to identify ARM code regions and Thumb code regions. However, for stripped binaries, such mapping symbols are unavailable. We propose a novel solution to statically translate stripped ARM/Thumb mixed executables. Our solution is implemented in a static binary translator. The binary translator further generates multiple versions of translated code for the code regions whose types cannot be determined with our solution. One of the code versions is selected during runtime. The binary translator also includes a series of analyses that enable the removal ofmost useless code versions. Based on the experimental results on stripped ARM/Thumb mixed binaries in the SPEC2006 and Embedded Microprocessor Benchmark Consortium (EEMBC) benchmark suites, our static binary translator achieves impressive performance when migrating them to run on x86 machines and the space overhead is no more than 10%. © 2017 ACM.",Code discovery problem; Reverse engineering; Static binary translation,Benchmarking; Bins; Codes (symbols); Computer architecture; Decoding; Mapping; Program translators; Reverse engineering; Advanced RISC machines; Benchmark suites; Binary translation; Code discovery; Embedded microprocessors; Instruction set architecture; Least significant bits; Reduced instruction set computers; ARM processors
An indoor test methodology for solar-powered wireless sensor networks,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017256685&doi=10.1145%2f2994604&partnerID=40&md5=b94ca6be60bbfd583a1f735a4b073aaa,"Repeatable and accurate tests are important when designing hardware and algorithms for solar-powered wireless sensor networks (WSNs). Since no two days are exactly alike with regard to energy harvesting, tests must be carried out indoors. Solar simulators are traditionally used in replicating the effects of sunlight indoors; however, solar simulators are expensive, have lighting elements that have short lifetimes, and are usually not designed to carry out the types of tests that hardware and algorithm designers require. As a result, hardware and algorithm designers use tests that are inaccurate and not repeatable (both for others and also for the designers themselves). In this article, we propose an indoor test methodology that does not rely on solar simulators. The test methodology has its basis in astronomy and photovoltaic cell design. We present a generic design for a test apparatus that can be used in carrying out the test methodology. We also present a specific design that we use in implementing an actual test apparatus. We test the efficacy of our test apparatus and, to demonstrate the usefulness of the test methodology, perform experiments akin to those required in projects involving solar-powered WSNs. Results of the said tests and experiments demonstrate that the test methodology is an invaluable tool for hardware and algorithm designers working with solar-powered WSNs. © 2017 ACM.",Astronomical models; Energy harvesting; Energy neutrality; Lifetime; Power management; Solar simulators,Energy harvesting; Hardware; Photoelectrochemical cells; Photovoltaic cells; Power management; Simulators; Solar energy; Testing; Energy neutrality; Generic design; Lifetime; Lighting elements; Solar simulator; Solar-powered wireless sensor networks; Test apparatus; Test methodology; Wireless sensor networks
AdaFT: A framework for adaptive fault tolerance for cyber-physical systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017228207&doi=10.1145%2f2980763&partnerID=40&md5=dc670597d33445cf8e7662d3da5533d4,"Cyber-physical systems (CPS) frequently have to use massive redundancy to meet application requirements for high reliability. While such redundancy is required, it can be activated adaptively, based on the current state of the controlled plant. Most of the time, the plant is in a state that allows for a lower level of fault tolerance. Avoiding the continuous deployment of massive fault tolerance will greatly reduce the workload of the CPS, and lower the operating temperature of the cyber sub-system, thus increasing its reliability. In this article, we extend our prior research by demonstrating a software simulation framework Adaptive Fault Tolerance (AdaFT) that can automatically generate the sub-spaces within which our adaptive fault tolerance can be applied. We also show the theoretical benefits of AdaFT and its actual implementation in several real-world CPSs. © 2017 ACM.",Cyber-physical system; Fault tolerance; Processor thermal reliability,Computer software; Cyber Physical System; Embedded systems; Redundancy; Reliability; Adaptive fault tolerances; Application requirements; Cyber-physical systems (CPS); High reliability; Operating temperature; Software simulation; Sub-systems; Thermal reliability; Fault tolerance
Editorial: Continuing the Course,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017093639&doi=10.1145%2f3043965&partnerID=40&md5=a64b7d24cc15578ff8a730242b08eaf1,[No abstract available],,
Guest Editorial: Special Issue on LCTES 2015,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017113278&doi=10.1145%2f3041038&partnerID=40&md5=311a741c2668e90bf88dbdd7bd6105eb,[No abstract available],,
Minimizing test suites with unfoldings of multithreaded programs,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017112090&doi=10.1145%2f3012281&partnerID=40&md5=8d05745b3fbf06297109b3492ce3a571,"This article focuses on computing minimal test suites for multithreaded programs. Based on previous work on test case generation for multithreaded programs using unfoldings, this article shows how this unfolding can be used to generateminimal test suites covering all local states of the program. Generating suchminimal test suites is shown to be NP-complete in the size of the unfolding. We propose an SMT encoding for this problem and two methods based on heuristics which only approximate the solution, but scale better in practice. Finally, we apply our methods to compute the minimal test suites for several benchmarks. © 2017 ACM.",Event structure; SMT-encoding; Testing; Unfolding,Encoding (symbols); Heuristic methods; Multitasking; Testing; Event structures; Local state; Multi-threaded programs; NP Complete; Test case generation; Unfolding; Unfoldings; Software testing
Message from the Guest Editors,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017125268&doi=10.1145%2f3037413&partnerID=40&md5=e38861cdd460e9ec4c7cdff69a37f5fa,[No abstract available],,
Detecting software cache coherence violations in MPSoC using traces captured on virtual platforms,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008925662&doi=10.1145%2f2990193&partnerID=40&md5=f5ad8846593cc9ebde28a5a818af671c,"Software cache coherence schemes tend to be the solution of choice in dedicated multi/many core systems on chip, as they make the hardware much simpler and predictable. However, despite the developers' effort, it is hard to make sure that all preventive measurements are taken to ensure coherence. In this work, we propose a method to identify the potential cache coherence violations using traces obtained from virtual platforms. These traces contain causality relations among events, which allow first to simplify the analysis, and second to avoid relying on timestamps. Our method identifies potential violations that may occur during a given execution for write-through and write-back cache policies. Therefore, it is independent of the software coherence protocol. We conducted experiments on parallel applications running on a lightweight SMP operating system, and we were able to detect coherence issues that we could then solve. © 2017 ACM.",Cache coherence; MPSoC; Trace analysis; Virtual platforms,Cache memory; System-on-chip; Trace analysis; Cache Coherence; Core systems; MPSoC; Parallel application; Software cache coherence; Software coherences; Time stamps; Virtual platform; Multiprocessing systems
SA-EAST: Security-aware efficient data transmission for ITS in mobile heterogeneous cloud computing,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008951952&doi=10.1145%2f2979677&partnerID=40&md5=40afb6d0757d6c7f74670a949505d929,"The expected advanced network explorations and the growing demand for mobile data sharing and transferring have driven numerous novel applications in Cyber-Physical Systems (CPSs), such as Intelligent Transportation Systems (ITSs). However, current ITS implementations are restricted by the conflicts between security and communication efficiency. Focusing on this issue, this article proposes a Security-Aware Efficient Data Sharing and Transferring (SA-EAST) model, which is designed for securing cloud-based ITS implementations. In applying this approach, we aim to obtain secure real-time multimedia data sharing and transferring. Our experimental evaluation has shown that our proposed model provides an effective performance in securing communications for ITS. © 2017 ACM.","Cyber-physical systems; D.2.1 [network architecture and design]: distributed networks; D.4.6 [security and protection]: information flow controls; Efficient data sharing and transferring; H.2.0 [general]: security, integrity, and protection; Intelligent transportation system; Mobile heterogeneous cloud computing; Security; Security-aware; Theory","Cloud computing; Computation theory; Distributed computer systems; Embedded systems; Intelligent systems; Intelligent vehicle highway systems; Mobile cloud computing; Network architecture; Transportation; Vehicle locating systems; Cyber physical systems (CPSs); Data Sharing; Distributed networks; H.2.0 [general]: security, integrity, and protection; Information flow control; Intelligent transportation systems; Security; Security-aware; Theory; Network security"
Global optimization of fixed-priority real-time systems by RTOS-aware control-flow analysis,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008476027&doi=10.1145%2f2950053&partnerID=40&md5=c68df78eaae2da3d877b3e74d0cbd9df,"Cyber-physical systems typically target a dedicated purpose; their embedded real-time control system, such as an automotive control unit, is designed with a well-defined set of functionalities. On the software side, this results in a large amount of implicit and explicit static knowledge about the system and its behavior already at compile time. Compilers have become increasingly better at extracting and exploiting such static knowledge. For instance, many optimizations have been lifted up to the interprocedural or even to the whole-program level. However, whole-program optimizations generally stop at the application-kernel boundary: control-flow transitions between different threads are not yet analyzed. In this article, we cross the application-kernel boundary by combining the semantics of a real-time operating system (RTOS) with deterministic fixed-priority scheduling (e.g., OSEK/AUTOSAR, ARINC 653, μITRON, POSIX.4) and the explicit application knowledge to enable system-wide, flow-sensitive compiler optimizations. We present two methods to extract a cross-kernel, control-flow-graph that provides a global view on all possible execution paths of a real-time system. Having this knowledge at hand, we tailor the operating system kernel more closely to the particular application scenario. For the example of a real-world safety-critical control system, we present three possible use cases. (1) Runtime optimizations, by means of specialized system calls for each call site, allow one speed up the kernel execution path by 28% in our benchmark scenario. Furthermore, we target transient hardware fault tolerance with two automated software-based countermeasures: (2) generation of OS state assertions on the expected system behavior, and (3) a system-wide dominator-region based control-flow error detection, both of which leverage significant robustness improvements. © 2017 ACM.",AUTOSAR; Global control-flow graph; OSEK; Static analysis; Static system tailoring; Whole-system optimization,Application programs; Codes (symbols); Computer operating systems; Control systems; Data flow analysis; Embedded systems; Fault tolerance; Fault tolerant computer systems; Flow graphs; Global optimization; Graphic methods; Interactive computer systems; Program compilers; Real time control; Safety engineering; Semantics; Static analysis; AutoSAR; Global control; OSEK; Static systems; System optimizations; Real time systems
"Protecting mobile health records in cloud computing: A secure, efficient, and anonymous design",2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008970464&doi=10.1145%2f2983625&partnerID=40&md5=8f3296c06aab2deae97717fa1196b8dd,"Electronic healthcare (eHealth) systems have replaced traditional paper-based medical systems due to attractive features such as universal accessibility, high accuracy, and low cost. As a major constituent part of eHealth systems, mobile healthcare (mHealth) applies Mobile Internet Devices (MIDs) and Embedded Devices (EDs), such as tablets, smartphones, and other devices embedded in the bodies of individuals, to improve the quality of life and provide more convenient healthcare services for patients. Unfortunately, MIDs and EDs have only limited computational capacity, storage space, and power supply. By taking this into account, we present a new design to guarantee the integrity of eHealth records and the anonymity of the data owner in a more efficient and flexible way. The essence of our design is a general method which can convert any secure Attribute-Based Signature (ABS) scheme into a highly efficient and secure Online/Offline Attribute-Based Signature (OOABS) scheme. We prove the security and analyze the efficiency improvement of the new design. Additionally, we illustrate the proposed generic construction by applying it to a specific ABS scheme. © 2017 ACM.",Cloud computing; Embedded devices (EDs); Mobile health records (MHRs); Online/offline attribute-based signature,Cloud computing; Digital storage; Embedded systems; Emergency rooms; mHealth; Attribute-based signatures; Computational capacity; Efficiency improvement; Electronic healthcare; Embedded device; Generic construction; Health records; Mobile internet devices; Mobile cloud computing
A principled approach to secure multi-core processor design with rewire,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011406127&doi=10.1145%2f2967497&partnerID=40&md5=1647acc699e69b20f88206de337b24de,"There is no such thing as high assurance without high assurance hardware. High assurance hardware is essential because any and all high assurance systems ultimately depend on hardware that conforms to, and does not undermine, critical system properties and invariants. And yet, high assurance hardware development is stymied by the conceptual gap between formal methods and hardware description languages used by engineers. This article advocates a semantics-directed approach to bridge this conceptual gap. We present a case study in the design of secure processors, which are formally derived via principled techniques grounded in functional programming and equational reasoning. The case study comprises the development of secure single-and dual-core variants of a single processor, both based on a common semantic specification of the ISA. We demonstrate via formal equational reasoning that the dual-core processor respects a ""nowrite-down"" information flow policy. The semantics-directed approach enables a modular and extensible style of system design and verification. The secure processors require only a very small amount of additional code to specify and implement, and their security verification arguments are concise and readable. Our approach rests critically on ReWire, a functional programming language providing a suitable foundation for formal verification of hardware designs. This case study demonstrates both ReWire's expressiveness as a programming language and its power as a framework for formal, high-level reasoning about hardware systems. © 2017 ACM.",Equational reasoning; Hardware security; Monads; Reconfigurable computing,Computer hardware; Computer hardware description languages; Computer programming languages; Design; Formal verification; Functional programming; Hardware; Hardware security; High level languages; Information dissemination; Integrated circuit design; Multicore programming; Reconfigurable architectures; Semantics; Equational reasoning; High assurance systems; Information flow policies; Monads; Reconfigurable computing; Security verification; Semantic specification; System design and verification; Formal methods
"Stop it, and be stubborn!",2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011350059&doi=10.1145%2f3012279&partnerID=40&md5=9a676c9cc5dd405ea000203a4cd429bf,"This publication discusses how automatic verification of concurrent systems can be made more efficient by focusing on always may-terminating systems. First, making a system always may-terminating is a method formeeting a modelling need that exists independently of this publication. It is illustrated that without doing so, non-progress errors may be lost. Second, state explosion is often alleviated with stubborn, ample, and persistent set methods. They use expensive cycle or terminal strong component conditions in many cases. It is proven that for many important classes of properties, if the systems are always may-terminating, then these conditions can be left out. © 2017 ACM.",Ignoring problem; Safety/progress/liveness properties; Stubborn set/ample set/persistent set/partial order methods,Software engineering; Automatic verification; Concurrent systems; Ignoring problem; nocv1; State explosion; Stubborn set/ample set/persistent set/partial order methods; Hardware
Securely reinforcing synchronization for embedded online contests,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011383090&doi=10.1145%2f2899000&partnerID=40&md5=1d147fc1704930520a0d5b767a77002d,"When competing in eBay bidding, online games, or e-exams in embedded computing environments, people naturally face asynchronous starts from different computing devices, which is treated as a security risk of online contests. The security risks of online contests also include eavesdropping during data transmission without intended rights, and false starts by malicious competitors, which also means asynchrony in contests. Accordingly, online contests need security guarantees, especially on synchronization. In this article, for synchronic and secure starts in a contest, we update security requirements of confidentiality, anonymity, and synchrony, comparing the current work to our previous work. Based on the updated requirements, we propose a general framework for the Advanced Secure Synchronized Reading (ASSR) system, which can hold multiple contests simultaneously in the cloud. It is important to note that the system can ignore the impacts of heterogeneity among competitors. Considering the heterogeneity both on transmission and computing, we construct a novel Randomness-reused Identity Based Key Encapsulation Mechanism (RIBKEM) to support separable decapsulation, which can shorten both decryption delay and transmission delay with the best efforts. Finally, ASSR enhances synchronization achievement for contest starts with heterogeneous delays of competitors while satisfying other security requirements. As a complement, the analysis on the provable security of ASSR is given, as well as a further analysis on the achievement of synchronization. © 2017 ACM.",Embedded computing; Online contest; Randomnessreused identity-based key encapsulation mechanism; Synchronization,Computer games; Cryptography; Public key cryptography; Computing devices; Embedded computing; Heterogeneous delays; Key encapsulation mechanisms; Online contest; Provable security; Security requirements; Transmission delays; Synchronization
On-board format-independent security of functional magnetic resonance images,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011339326&doi=10.1145%2f2893474&partnerID=40&md5=879a98223e023814e0528e3eaac7faa2,"Functional magnetic resonance imaging (fMRI) provides an effective and noninvasive tool for researchers to understand cerebral functions and correlate them with brain activities. In addition, with the ever-increasing diffusion of the Internet, such images may be exchanged in several ways, allowing new research and medical services. On the other hand, ensuring the security of exchanged fMRI data becomes a main concern due to their special characteristics arising from strict ethics and legislative and diagnostic implications. Again, the risks increase when dealing with open environments like the Internet. For this reason, security mechanisms that ensure protection of such data are strongly required. However, we remark that the mechanisms commonly employed for data protection are doomed to fail when dealing with imaging data. In this article, we propose a novel watermarking scheme explicitly addressed for this type of imaging. Such a scheme can be used for several purposes, particularly to ensure authenticity and integrity. Moreover, we show how to integrate our scheme within commercial off-the-shelf fMRI system. Finally, the validity and the efficiency of our scheme has been assessed through testing. © 2017 ACM.",fMRI; Functional magnetic resonance imaging; On-board protection; Reversible watermarking scheme; Security,Brain; Diagnosis; Digital watermarking; Magnetic resonance imaging; Magnetism; Medical imaging; Network security; Resonance; Security of data; fMRI; Functional magnetic resonance imaging; On-board protection; Reversible watermarking; Security; Functional neuroimaging
Dynamic data-cache locking for minimizing the WCET of a single task,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008474173&doi=10.1145%2f2994602&partnerID=40&md5=42adc726a23e5fa60208ba47e4ee4e9a,"Caches have been widely used in modern embedded processors to bridge the increasing speed gap between processors and off-chip memory. In real-time embedded systems, computing the Worst-Case Execution Time (WCET) of a task is essential for the task scheduler to construct a valid schedule for a task set. Unfortunately, caches make it much harder to compute the WCET of a task. Cache locking has been proposed to alleviate the timing unpredictability problem caused by caches. In this article, we investigate the following WCET-aware data-cache locking problem for a single task. Given a task, select a set of variables as locked cache contents such that the WCET of the task is minimized. We propose two dynamic full cache-locking approaches. The first formulates the problem as a global Integer Linear Programming (ILP) problem that simultaneously selects a minimum set of memory blocks of variables as locked cache contents and allocates them to the data cache. The second iteratively constructs a subgraph of the Control Flow Graph (CFG) of the task in which the lengths of all the paths are close to the longest path length, uses an ILP formulation to select a minimum set of memory blocks of variables in the subgraph as locked cache contents, and allocates the selected memory blocks to the data cache. We also propose two novel, efficient data-cache allocation algorithms for the global ILP approach and the iterative ILP approach, respectively. We have implemented both approaches and compared them with two state-of-the-art approaches, the longest path-based dynamic cache-locking approach and the static WCET analysis approach without cache locking by using a set of benchmarks from the Mälardalen WCET benchmark suite, SNU real-time benchmarks, and Powerstone benchmarks. Compared to the static WCET analysis approach, the average WCET improvements of the first approach range between 11.4% and 26.4%. Compared to the longest path-based, dynamic cache-locking approach, the average WCET improvements of the first approach range between 5.0% and 15.4%. The second approach performs slightly better than the first approach. © 2017 ACM.",Algorithms; B.3.3 [performance analysis and design aids]:worst-case analysis; C.3 [special-purpose and application-based systems]:real-time and embedded systems; Dynamic data-cache locking; False dependency; Graph orientation; Integer linear programming; Interference graph; Performance; Worst-case execution time,Algorithms; Computer systems programming; Data flow analysis; Embedded systems; Flow graphs; Integer programming; Iterative methods; Locks (fasteners); Program compilers; Real time systems; Dynamic data; False dependency; Graph orientations; Integer Linear Programming; Interference graphs; Performance; Real-time and embedded systems; Worst-case analysis; Worst-case execution time; Cache memory
SIPF: A secure installment payment framework for drive-thru internet,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011347242&doi=10.1145%2f3014584&partnerID=40&md5=1f9835213eea9734923026117d6a41b8,"Ensuring the security and privacy of vehicular ad hoc networks (VANETs) and related services such as secure payment has been the focus of recent research efforts. Existing secure payment solutions generally require stable and reliable network connection. This is, however, a challenge in a VANET setting. Drive-thru Internet, a secure payment solution for VANETs, involves a great number of fast-moving vehicles competing for connections/communications simultaneously. Thus, service providers may find it challenging to provide real-time payment services or may have to sacrifice the confidentiality and the authenticity of payment vouchers for usability. In this article, we propose a secure installment payment framework for drive-thru Internet deployment in a VANET setting. The framework also provides the capability to embody properties such as confidentiality of payment vouchers, offline signature verification, periodical reconciliation, and installment payment. Performance evaluation and security analysis demonstrate the utility of the framework in a VANET setting. &copy; 2017 ACM.",Drive-thru Internet; Homomorphic encryption; Installment payment; Offline signature,Ad hoc networks; Cryptography; Internet; Vehicular ad hoc networks; Drive-thru; Ho-momorphic encryptions; Installment payment; Off-line signature verification; Offline signatures; Security analysis; Security and privacy; Vehicular Adhoc Networks (VANETs); Network security
From a formalized parallel action language to its efficient code generation,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011340615&doi=10.1145%2f2990195&partnerID=40&md5=9be040ea71f3c4af6a83b41460521c47,"Modeling languages propose convenient abstractions and transformations to handle the complexity of today's embedded systems. Based on the formalism of the Hierarchical State Machine, they enable the expression of hierarchical control parallelism. However, they face two important challenges when it comes to modeling data-intensive applications: no unified approach that also accounts for data-parallel actions and no effective code optimization and generation flows. We propose a modeling language extended with parallel action semantics and hierarchical indexed-state machines suitable for computationally intensive applications. Together with its formal semantics, we present an optimizing model compiler aiming for the generation of efficient data-parallel implementations. © 2017 ACM.",Action language; Model driven engineering; Parallels languages,Embedded systems; Formal methods; Program compilers; Semantics; Software design; Action language; Code optimization; Formal Semantics; Hierarchical control; Hierarchical state machines; Model-driven Engineering; Optimizing models; Unified approach; Modeling languages
Why data deletion fails? A study on deletion flaws and data remanence in android systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011333664&doi=10.1145%2f3007211&partnerID=40&md5=823cc0c7d2d59ddf9b8c16454dc6d6f7,"Smart mobile devices are becoming the main vessel of personal privacy information. While they carry valuable information, data erasure is somehow much more vulnerable than was predicted. The security mechanisms provided by the Android system are not flexible enough to thoroughly delete sensitive data. In addition to the weakness among several provided data-erasing and file-deleting mechanisms, we also target the Android OS design flaws in data erasure, and unveil that the design of the Android OS contradicts some secure data-erasure demands. We present the data-erasure flaws in three typical scenarios on mainstream Android devices, such as the data clearing flaw, application uninstallation flaw, and factory reset flaw. Some of these flaws are inherited data-deleting security issues from the Linux kernel, and some are new vulnerabilities in the Android system. Those scenarios reveal the data leak points in Android systems. Moreover, we reveal that the data remanence on the disk is rarely affected by the user's daily operation, such as file deletion and app installation and uninstallation, by a real-world data deletion latency experiment. After one volunteer used the Android phone for 2 months, the data remanence amount was still considerable. Then, we proposed DataRaider for file recovering from disk fragments. It adopts a file-carving technique and is implemented as an automated sensitive information recovering framework. DataRaider is able to extract private data in a raw disk image without any file system information, and the recovery rate is considerably high in the four test Android phones. We propose some mitigation for data remanence issues, and give the users some suggestions on data protection in Android systems. © 2017 ACM.",Data recovery; File carving; Mobile security; Secure deletion,Android (operating system); Computer operating systems; Recovery; Remanence; Security of data; Side channel attack; Telephone sets; Data recovery; File carving; Personal privacy; Secure deletion; Security issues; Security mechanism; Sensitive datas; Sensitive informations; Mobile security
Incremental inductive verification of parameterized timed systems,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011347304&doi=10.1145%2f2984640&partnerID=40&md5=b34de9e4cc84d3543c063799f99999eb,"We propose and extend an approach for the verification of safety properties for parameterized timed systems modeled as networks of timed automata. For this task, we introduce an incremental workflow that is based on our algorithm IC3 with Zones. It proceeds in a cycle in which single models of the system are verified, and the verification results are employed for the reasoning about the entire system. Starting with the smallest instances, the verification of the safety property is carried out fast and efficient.On successful verification, the algorithm produces an inductive strengthening of the safety property. We reuse this result and try to reason about the entire parameterized timed system. To this end, we extrapolate the inductive strengthening into a candidate for the next-larger model. In case this candidate is a valid inductive strengthening for the next larger model, our main theorem reasons about all models of the parameterized timed system, stating that the safety property holds true for all models. Otherwise, themain cycle starts over with the verification of the next larger model. This workflow is iterated indefinitely, until able to reason about the entire parameterized timed system, until a counterexample trace is found, or until the single models become too large to be handled in the verification. We reuse the intermediate results in a Feedback-loop in order to accelerate the verification runs for the single models. Furthermore, we consider an extended formalism in comparison to our previous publications. © 2017 ACM.",IC3; Networks of timed automata; Parameterized timed systems; PDR; Verification,Automata theory; Time sharing systems; Verification; Entire system; Feed-back loop; Inductive verification; Intermediate results; Parameterized timed systems; Safety property; Timed Automata; Verification results; Parameterization
Multi-valued simulation and abstraction using lattice operations,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008881153&doi=10.1145%2f3012282&partnerID=40&md5=8e9a0ad585e569bc8e1c9c3150b0c1f2,"Abstractions can cause spurious results, which need to be verified in the concrete system to gain conclusive results. Verification based on a multi-valued logic can distinguish between conclusive and inconclusive results, provides increased precision, and allows for encoding additional information into the model. To ensure a correct abstraction, one can use a mixed simulation [Meller et al. 2009]. We extend mixed simulation to include inconsistent values, thereby resolving an asymmetry and allowing for abstractions with increased precision when inconsistent values are available. In addition, we present a set of abstraction rules, compatible with the extended notion, for constructing abstract models. © 2017 ACM.",Bilattices; Kripke models; Mixed simulation; Multi-valued logics,Abstracting; Abstract models; Bilattices; Concrete system; Kripke model; Lattice operations; Mixed simulation; Multi-valued; Many valued logics
When do we not need complex assume-guarantee rules?,2017,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008893493&doi=10.1145%2f3012280&partnerID=40&md5=5a102598eb37d3331b702395a03a60a0,"We study the need for complex circular assume-guarantee (AG) rules in formalisms that already provide the simple precongruence rule. We first investigate the question for two popular formalisms: Labeled Transition Systems (LTSs) with weak simulation and Interface Automata (IA) with alternating simulation. We observe that, in LTSs, complex circular AG rules cannot always be avoided, but, in the IA world, the simple precongruence rule is all we need. Based on these findings, we introduce modal IA with cut states, a novel formalism that not only generalizes IA and LTSs but also allows for compositional reasoning without complex AG rules. © 2017 ACM.",Circular assume-guarantee reasoning; Compositional reasoning; Cut state; Modal interface automata; Refinement checking,Automata theory; Interface states; Assume-guarantee reasoning; Compositional reasoning; Cut state; Interface automata; Refinement checking; Model checking
Eager synching: A selective logging strategy for fast fsync() on flash-based android devices,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008932513&doi=10.1145%2f2930668&partnerID=40&md5=7eeb01b347219537c50707e770fc262c,"Flash storage has been a standard component in Android devices. Recent research has reported that application data management in Android involves frequent fsync() operations. The current fsync() implementations, including those of ext4 and F2FS, have several common drawbacks. Specifically, ext4 commits a transaction every time to sync a file, whereas F2FS commits a checkpoint to sync a directory. Committing a transaction or checkpoint flushes all dirty data from the page cache to the flash storage via many small, random block write requests. The resultant high I/O frequency and excessive write traffic cause a high fsync() latency. This study presents an efficient fsync() method, called eager synching, which is based on a simple idea: write less, and write sequentially. To sync a file, eager synching writes only a subset of all dirty data in the page cache to a sequential log space using a few sequential block write requests. It does not involve transaction or checkpoint committing. We successfully implemented eager synching in ext4 and F2FS, and our experimental results show that, compared with the original fsync() methods of ext4 and F2FS, eager synching reduced the average and maximum fsync() latencies by up to 72% and 91%, respectively, block-level write traffic by up to 35%, and I/O frequency by up to 66%. Through enhanced crash recovery procedures, eager synching can successfully recover all previously synched files while still guaranteeing the file system integrity. We also conducted live application replays using the proposed eager synching approach and observed that this approach significantly improved the application frame updating rate and application execution time. © 2016 ACM.",Android; eMMC; File system; Flash storage; Fsync(),Digital storage; File organization; Information management; Android; eMMC; File systems; Flash storage; Fsync(); Android (operating system)
Efficient elliptic curve cryptography for embedded devices,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008889013&doi=10.1145%2f2967103&partnerID=40&md5=d8c589e5e464eaf99dcb76424f75205b,"Many resource-constrained embedded devices, such as wireless sensor nodes, require public key encryption or a digital signature, which has induced plenty of research on efficient and secure implementation of elliptic curve cryptography (ECC) on 8-bit processors. In this work, we study the suitability of a special class of finite fields, called optimal prime fields (OPFs), for a ""lightweight"" ECC implementation with a view toward high performance and security. First, we introduce a highly optimized arithmetic library for OPFs that includes two implementations for each finite field arithmetic operation, namely a performance-optimized version and a security-optimized variant. The latter is resistant against simple power analysis attacks in the sense that it always executes the same sequence of instructions, independent of the operands. Based on this OPF library, we then describe a performance-optimized and a security-optimized implementation of scalar multiplication on the elliptic curve over OPFs at several security levels. The former uses the Gallant-Lambert-Vanstone method on twisted Edwards curves and reaches an execution time of 3.14M cycles (over a 160-bit OPF) on an 8-bit ATmega128 processor, whereas the latter is based on a Montgomery curve and executes in 5.53M cycles. © 2016 ACM.",8-bit AVR microcontroller; Elliptic curve; Optimal prime fields; Scalar multiplication,Cryptography; Digital devices; Geometry; Public key cryptography; Sensor nodes; Side channel attack; AVR microcontrollers; Elliptic curve; Elliptic curve cryptography; Elliptic Curve Cryptography(ECC); Finite field arithmetic; Optimized implementation; Prime field; Scalar multiplication; Curve fitting
Compiler-directed soft error detection and recovery to avoid DUE and SDC via tail-DMR,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008950198&doi=10.1145%2f2930667&partnerID=40&md5=0682b8960898412d32d998bd5a877517,"This article presents Clover, a compiler-directed soft error detection and recovery scheme for lightweight soft error resilience. The compiler carefully generates soft-error-tolerant code based on idempotent processing without explicit checkpoints. During program execution, Clover relies on a small number of acoustic wave detectors deployed in the processor to identify soft errors by sensing the wave made by a particle strike. To cope with DUEs (detected unrecoverable errors) caused by the sensing latency of error detection, Clover leverages a novel selective instruction duplication technique called tail-DMR (dual modular redundancy) that provides a region-level error containment. Once a soft error is detected by either the sensors or the tail-DMR, Clover takes care of the error as in the case of exception handling. To recover from the error, Clover simply redirects program control to the beginning of the code region where the error is detected. The experimental results demonstrate that the average runtime overhead is only 26%, which is a 75% reduction compared to that of the state-of-the-art soft error resilience technique. In addition, this article evaluates an alternative technique called tail-wait, comparing it to Clover. According to the evaluation with the different processor configurations and the various error detection latencies, Clover turns out to be a superior technique, achieving 1.06 to 3.49× speedup over the tail-wait. © 2016 ACM.",Acoustic wave detectors; Compilers; Idempotent processing; Soft error resilience; Tail-DMR frontier,Acoustic waves; Acoustics; Error correction; Errors; Program compilers; Program processors; Radiation hardening; Recovery; Detected unrecoverable errors; Dual modular redundancy; Error detection latency; Idempotent processing; Soft error; Soft error detection; Tail-DMR frontier; Wave detectors; Error detection
Differential fault attack on ITUbee block cipher,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008940806&doi=10.1145%2f2967610&partnerID=40&md5=7f2de7511e191f53bce7f0e19c721c3c,"Differential Fault Attack (DFA) is a powerful cryptanalytic technique to retrieve secret keys by exploiting the faulty ciphertexts generated during encryption procedure. This article proposes a novel DFA attack that is effective on ITUbee, a software-oriented block cipher for resource-constrained devices. Different from other DFA, our attack makes use of not only faulty values, but also differences between fault-free intermediate values corresponding to 2 plaintexts, which combine traditional differential analysis with DFA. The possible injection positions with different number of faults are discussed. The most efficient attack takes 225 round function operations with 4 faults, which is achieved in a few seconds on a PC. © 2016 ACM.",Differential fault attack; ITUbee,Security of data; Block ciphers; Cryptanalytic techniques; Differential analysis; Differential fault attack; Encryption procedure; ITUbee; Resourceconstrained devices; Round functions; Cryptography
DLSeF: A dynamic key-length-based efficient real-time security verification model for big data stream,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008884270&doi=10.1145%2f2937755&partnerID=40&md5=1f417a01e84245fc081e3c3fb7844faa,"Applications in risk-critical domains such as emergency management and industrial control systems need near-real-time stream data processing in large-scale sensing networks. The key problem is how to ensure online end-to-end security (e.g., confidentiality, integrity, and authenticity) of data streams for such applications. We refer to this as an online security verification problem. Existing data security solutions cannot be applied in such applications as they cannot deal with data streams with high-volume and high-velocity data in real time. They introduce a significant buffering delay during security verification, resulting in a requirement for a large buffer size for the stream processing server. To address this problem, we propose a Dynamic Key-Length-Based Security Framework (DLSeF) based on a shared key derived from synchronized prime numbers; the key is dynamically updated at short intervals to thwart potential attacks to ensure end-to-end security. Theoretical analyses and experimental results of the DLSeF framework show that it can significantly improve the efficiency of processing stream data by reducing the security verification time and buffer usage without compromising security. © 2016 ACM.",Big data stream; Efficient; Key exchange; Security; Sensor networks; Time synchronization,Big data; Data communication systems; Data handling; Information management; Real time systems; Risk management; Sensor networks; Data stream; Efficient; Key exchange; Security; Time synchronization; Network security
Preserving partial-order runs in parametric time petri nets,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008925699&doi=10.1145%2f3012283&partnerID=40&md5=448d7e2a5cdf8629fac5e68cb9454bfb,"Parameter synthesis for timed systems aims at deriving parameter valuations satisfying a given property. In this article, we target concurrent systems.We use partial-order semantics for parametric time Petri nets as a way to both cope with the well-known state-space explosion due to concurrency and significantly enhance the result of an existing synthesis algorithm. Given a reference parameter valuation, our approach synthesizes other valuations preserving the partial-order executions of the reference parameter valuation. We show the applicability of our approach using a tool applied to asynchronous circuits. © 2016 ACM.",Concurrency; Inverse method; Parametric time Petri nets; Robustness; Timed and hybrid systems; Unfolding semantics,Hybrid systems; Inverse problems; Robustness (control systems); Semantics; Time sharing systems; Asynchronous circuits; Concurrency; Inverse methods; Partial order semantics; State-space explosion; Synthesis algorithms; Time Petri nets; Unfolding semantics; Petri nets
Non-interference in partial order models,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008873364&doi=10.1145%2f2984639&partnerID=40&md5=69fe30e7e48cd95441554eeb97d6b057,"Non-interference (NI) is a property of systems stating that confidential actions should not cause effects observable by unauthorized users. Several variants of NI have been studied for many types of models but rarely for true concurrency or unbounded models. This work investigatesNI for High-levelMessage Sequence Charts (HMSCs), a scenario language for the description of distributed systems, based on composition of partial orders. We first propose a general definition of security properties in terms of equivalence among observations of behaviors. Observations are naturally captured by partial order automata, a formalism that generalizes HMSCs and permits assembling partial orders.We show that equivalence or inclusion properties for HMSCs (and hence for partial order automata) are undecidable, which means in particular that NI is undecidable for HMSCs. We hence consider decidable subclasses of partial order automata and HMSCs. Finally, we define weaker local properties, describing situations where a system is attacked by a single agent, and show that local NI is decidable. We then refine local NI to a finer notion of causal NI that emphasizes causal dependencies between confidential actions and observations and extend it to causal NI with (selective) declassification of confidential events. Checking whether a system satisfies local and causal NI and their declassified variants are PSPACE-complete problems. © 2016 ACM.",Non-interference; Partial orders; Security; Verification,Computability and decidability; Network security; Verification; Causal dependencies; Distributed systems; Inclusion properties; Non interference; Partial order; PSPACE-complete problems; Security; Security properties; Automata theory
Reduction in the number of fault injections for blind fault attack on SPN block ciphers,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008873127&doi=10.1145%2f3014583&partnerID=40&md5=4e8f2b73ee8e7435fc865aa3501bcdc9,"In 2014, a new fault analysis called blind fault attack (BFA) was proposed, in which attackers can only obtain the number of different faulty outputs without knowing the public data. The original BFA requires 480,000 fault injections to recover a 128-bit AES key. This work attempts to reduce the number of fault injections under the same attack assumptions. We analyze BFA from an information theoretical perspective and introduce a new probability-based distinguisher. Three approaches are proposed for different attack scenarios. The best one realized a 66.8% reduction of the number of fault injections on AES. © 2016 ACM.",AES; Blind fault attack; Fault analysis; Information theory,Cryptography; Information theory; Software testing; Block ciphers; Different attacks; Distinguishers; Fault analysis; Fault injection; Public data; Side channel attack
Minimizing cost of scheduling tasks on heterogeneous multicore embedded systems,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008939494&doi=10.1145%2f2935749&partnerID=40&md5=a3cd39c2bda240e8b4bd07a4a4f0f689,"Cost savings are very critical in modern heterogeneous computing systems, especially in embedded systems. Task scheduling plays an important role in cost savings. In this article, we tackle the problem of scheduling tasks on heterogeneous multicore embedded systems with the constraints of time and resources for minimizing the total cost, while considering the communication overhead. This problem is NP-hard and we propose several heuristic techniques-ISGG, RLD, and RLDG-to address the problem. Experimental results show that the proposed algorithms significantly outperform the existing approaches in terms of cost savings. © 2016 ACM.",Graph grouping; Heterogeneous multicore systems; Task scheduling; Time and resource constraints,Costs; Heuristic methods; Multitasking; Scheduling; Scheduling algorithms; Communication overheads; Graph grouping; Heterogeneous computing system; Heterogeneous multi-core systems; Heterogeneous multicore; Heuristic techniques; Resource Constraint; Task-scheduling; Embedded systems
Testing preorders for dMTS: Deadlock- and the new deadlock-/divergencetesting,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008867821&doi=10.1145%2f2984641&partnerID=40&md5=483e61c43ebf0421b37162353e1df744,"Testing preorders on component specifications ensure that replacing a specification by a refined one does not introduce unwanted behavior in an overall system. Considering deadlocks as unwanted, the preorder can be characterized by a failure semantics on Labeled Transition Systems (LTSs). In previous work, we have generalized this to Modal Transition Systems (MTSs) with a new, MTS-specific testing idea. In the present article, we generalize this idea further to DMTS, a subclass of disjunctive MTSs. On the one hand, the testing preorder can be characterized by the same failure semantics, and dMTS have no additional expressivity in our setting. On the other hand, the technical treatment is significantly harder and, surprisingly, the preorder is not compositional. Furthermore, we regard deadlocks and divergence (infinite unobservable runs) as unwanted and characterize the testing preorder with an unusual failure-divergence semantics. This preorder is already on LTSs strictly coarser-and hence arguably better-than the traditional failure-divergence preorder. It is a precongruence on dMTS, also for hiding, and much easier to handle than the deadlock-based preorder. It arises as well from a new variant of De Nicola's and Hennessy's must-testing. © 2016 ACM.",Deadlock; Disjunctive modal transition systems; Divergence; Failure-divergence semantics; Testing preorder,Specifications; Deadlock; Divergence; Divergence semantics; Modal Transition Systems; Preorders; Semantics
Fault detection architectures for post-quantum cryptographic stateless hash-based secure signatures benchmarked on ASIC,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006977242&doi=10.1145%2f2930664&partnerID=40&md5=752125e12a49ce75559502e0b27023aa,"Symmetric-key cryptography can resist the potential post-quantum attacks expected with the not-so-faraway advent of quantum computing power. Hash-based, code-based, lattice-based, and multivariate-quadratic equations are all other potential candidates, the merit of which is that they are believed to resist both classical and quantum computers, and applying ""Shor's algorithm""-the quantum-computer discrete-logarithm algorithm that breaks classical schemes-to them is infeasible. In this article, we propose, assess, and benchmark reliable constructions for stateless hash-based signatures. Such architectures are believed to be one of the prominent post-quantum schemes, offering security proofs relative to plausible properties of the hash function; however, it is well known that their confidentiality does not guarantee reliable architectures in the presence natural and malicious faults. We propose and benchmark fault diagnosis methods for this postquantum cryptography variant through case studies for hash functions and present the simulations and implementations results (through application-specific integrated circuit evaluations) to show the applicability of the presented schemes. The proposed approaches make such hash-based constructions more reliable against natural faults and help protecting them against malicious faults and can be tailored based on the resources available and for different reliability objectives. © 2016 ACM.",Application-specific integrated circuit (ASIC); Reliability; Secure hash-based signatures,Application specific integrated circuits; Benchmarking; Cryptography; Hash functions; Network security; Quantum computers; Quantum cryptography; Reliability; Classical schemes; Discrete logarithms; Fault diagnosis method; Multivariate quadratic equations; Post quantum cryptography; Secure hash; Shor's algorithms; Symmetric key cryptography; Fault detection
Embedded device forensics and security,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006983769&doi=10.1145%2f3015662&partnerID=40&md5=f3aaa2c735981b79ed2b3dd87d8f4ff0,"While the increasing digitalization of our society and amalgamation of embedded devices into the ever-increasing facets of our daily life (e.g., in smart and intelligent vehicles, smart cities and smart nations, and critical infrastructure sectors) have resulted in improved productivity and quality of life, the trend has also resulted in a trend of increasing frequency and sophistication of cyber exploitation and cyber threats. Hence, there is a need for coordinated efforts from the research community to address resulting concerns using both cryptographic and non-cryptographic solutions, such as those presented in this special section. © 2016 is held by the owner/author(s).",Digital forensics; Embedded device forensics; Embedded device security,Cryptography; Metals; Cyber threats; Daily lives; Embedded device; Embedded-device securities; Forensics and securities; Quality of life; Research communities; Special sections; Digital forensics
FREE RIDER: A source-level transformation tool for retargeting platform-specific intrinsic functions,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007002429&doi=10.1145%2f2990194&partnerID=40&md5=b7f80b5c59833450999851649789e31a,"Short-vector SMD and DSP instructions are popular extensions to common ISAs. These extensions deliver excellent performance and compact code for some compute-intensive applications, but they require specialized compiler support. To enable the programmer to explicitly request the use of such an instruction, many C compilers provide platform-specific intrinsic functions, whose implementation is handled specially by the compiler. The use of such intrinsics, however, inevitably results in nonportable code. In this article, we develop a novel methodology for retargeting such nonportable code, which maps intrinsics from one platform to another, taking advantage of similar intrinsics on the target platform. We employ a description language to specify the signature and semantics of intrinsics and perform graph-based pattern matching and high-level code transformations to derive optimized implementations exploiting the target's intrinsics, wherever possible. We demonstrate the effectiveness of our new methodology, implemented in the FREE RIDER tool, by automatically retargeting benchmarks derived from OPENCV samples and a complex embedded application optimized to run on an ARM CORTEX-M4 to an INTEL EDISON module with SSE4.2 instructions (and vice versa). We achieve a speedup of up to 3.73 over a plain C baseline, and on average 96.0% of the speedup of manually ported and optimized versions of the benchmarks. © 2016 ACM.",Graph matching; Intrinsic functions; Retargeting; Source-level transformations,Benchmarking; Codes (symbols); Cosine transforms; Graphic methods; Pattern matching; Program compilers; Semantics; Code transformation; Complex embedded applications; Description languages; Graph matchings; Intrinsic functions; Optimized implementation; Retargeting; Source level; C (programming language)
Sleep-mode voltage scaling: Enabling SRAM data retention at ultra-low power in embedded microcontrollers,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997269700&doi=10.1145%2f2950054&partnerID=40&md5=c37667232f6f68999c7f309947bf8c17,"In heavily duty-cycled embedded systems, the energy consumed by the microcontroller in idle mode is often the bottleneck for battery lifetime. Existing solutions address this problem by placing the microcontroller in a low-power (sleep) mode when idle and preserving application state either by retaining the data in situ in Static Random Access Memory (SRAM) or by checkpointing it to FLASH. However, both of these approaches have notable drawbacks. In situ data retention requires the SRAM to remain powered in sleep mode, while checkpointing to FLASH involves significant energy and time overheads. This article proposes a new ultralow-power sleep mode for microcontrollers that overcomes the limitations of both of these approaches. Our technique, HYPNOS, is based on the key observation that the on-chip SRAM in a microcontroller exhibits 100% data retention even at a much lower supply voltage (as much as 10x lower) than the typical operating voltage of the microcontroller. HYPNOS exploits this observation by performing extreme voltage scaling when the microcontroller is in sleep mode. We implement and evaluate HYPNOS for the TI MSP430G2452 microcontroller and show that the Microcontroller (MCU) draws only 26nA in the proposed sleep mode, which is 4xlower than a baseline sleep mode that preserves SRAM contents. Further, to reduce the overheads associated with performing the voltage scaling, we propose the use of an energy harvesting source for providing the scaled supply voltage and demonstrate (using a light sensing photodiode) that the current consumption in the proposed sleep mode can be reduced to 1nA, which is 100x lower than the current consumption in the baseline low-power mode. We also show that the decrease in sleep-mode power consumption translates to a reduction in application-level energy consumption by as much as 6.45x. By decreasing the average power consumption to such minuscule levels, HYPNOS takes a significant step forward in making perpetual systems a reality through the use of energy harvesting. © 2016 ACM.",Checkpointing; Energy harvesting; Internet of things; Low power design; Microcontroller sleep modes; SRAM retention; Wireless sensor networks,Controllers; Electric power supplies to apparatus; Electric power utilization; Embedded systems; Energy harvesting; Energy utilization; Flash memory; Internet of things; Low power electronics; Microcontrollers; Sleep research; Voltage scaling; Wireless sensor networks; Application level; Check pointing; Current consumption; Embedded microcontroller; Low-power design; Operating voltage; SLEEP mode; Static random access memory; Static random access storage
Integrated through-silicon via placement and application mapping for 3D mesh-based NoC design,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997221440&doi=10.1145%2f2968446&partnerID=40&md5=1624795d705d3350aabd7180ba1bbd7a,"This article proposes a solution to the integrated problem of Through-Silicon Via (TSV) placement and mapping of cores to the routers in a three-dimensional mesh-based Network-on-Chip (NoC) system. TSV geometry restricts their number in three-dimensional (3D) ICs. As a result, only about 25% of routers in a 3D NoC can possess vertical connections. Mapping plays an important role in evolving good system solutions in such a situation. TSVs have been placed with detailed consultation with the application mapping process. The integrated problem was first solved using the exact method of Integer Liner Programming (ILP). Next, a solution was obtained via a Particle Swarm Optimization (PSO) formulation. Several augmentations to the basic PSO strategy have been proposed to generate good-quality solutions. The results obtained are better than many of the contemporary approaches and close to the theoretical situation in which all routers are 3D in nature. © 2016 ACM.",3D NoC; Application mapping; Network-on-chip (NoC); TSV placement,Electronics packaging; Integer programming; Integrated circuit design; Integrated circuit interconnects; Integrated circuit manufacture; Mapping; Mesh generation; Network-on-chip; Particle swarm optimization (PSO); Routers; Servers; Silicon; Application mapping; Liner programming; Mesh-based networks; Network on chip (NoC); Threedimensional (3-d); Through-Silicon-Via; Through-Silicon-Via (TSV); TSV placement; Three dimensional integrated circuits
Editorial: Distributed public ledgers and block chains - What good are they for embedded systems?,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997497903&doi=10.1145%2f3001902&partnerID=40&md5=9c48caa5680dd4a81570b84ff9e2092e,[No abstract available],,
Modeling distributed real-time systems in TIOA and UPPAAL,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994495031&doi=10.1145%2f2964202&partnerID=40&md5=d63f632af48a65c571c0f5fb444d081a,"The mission- and life-critical properties of distributed real-time systems require concurrent modeling, analysis, and formal verification in the design stage. The timed input/output automata (TIOA) framework and the UPPAAL software package are two widely used modeling and verification tools for this purpose. To this end, we develop the algorithm TUConvert for converting distributed TIOA models to UPPAAL behavioral models and formally prove its correctness. We demonstrate the applicability of our algorithm by the formal verification of a distributed real-time industrial communication protocol that is modeled by TIOA. © 2016 ACM.",Distributed real-time systems; Formal verification; Timed input/output automata; UPPAAL,Automata theory; Formal verification; Interactive computer systems; Behavioral model; Concurrent modeling; Critical properties; Distributed real time system; Industrial communication protocols; Modeling and verifications; Timed input/output automaton; UPPAAL; Real time systems
Cache-partitioned preemption threshold scheduling,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994501926&doi=10.1145%2f2950057&partnerID=40&md5=b8db6e50fd56a639d14fed75beb2ef5c,"For preemptive scheduling with shared cache, different tasks may cause interference in the shared cache, leading to Cache-Related Preemption Overhead (CRPD). Cache partitioning can be used to reduce or eliminate CRPD. We propose integration of cache partitioning and Preemption Threshold Scheduling to optimize schedulability for both Fixed-Priority and Earliest Deadline First scheduling algorithms on a uniprocessor. We let each subset of tasks assigned the same cache partition be a nonpreemptive group by assigning the same preemption threshold to them, which eliminates CRPD both within each cache partition and between different cache partitions. © 2016 ACM.",Cache partitioning; Preemption threshold scheduling; Real-time scheduling,Scheduling; Cache partitioning; Cache partitions; Earliest-deadline-first scheduling algorithms; Fixed priorities; Non-preemptive; Pre-emptive scheduling; Preemption thresholds; Real - time scheduling; Scheduling algorithms
Analysis and scheduling of a battery-less mixed-criticality system with energy uncertainty,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994479745&doi=10.1145%2f2964201&partnerID=40&md5=01e061bfe82bd2320f0c63ef38281b4e,"We consider a battery-less real-Time embedded system equipped with an energy harvester. It scavenges energy from an environmental resource according to some stochastic patterns. The success of jobs is threatened in the case of energy shortage, which might be due to lack of harvested energy, losses originated from the super-capacitor self-discharge, as well as power consumption of executed tasks. The periodic real-Time tasks of the system follow a dual-criticality model. In addition, each task has a minimum required success ratio that needs to be satisfied in steady state. We analytically evaluate the behavior of such a system in terms of its energy-related success ratio for a given schedule. Based on these results, we propose a scheduling algorithm that satisfies both temporal and success-ratio constraints of the jobs, while respecting task criticalities and corresponding system modes. The accuracy of the analytical method as well as its dependence on the numerical computations and other model assumptions are extensively discussed through comparison with simulation results. Also, the efficacy of the proposed scheduling algorithm is studied through comparison to some existing non-mixed-And mixed-criticality scheduling algorithms. © 2016 ACM.",Energy harvesting; Mixed criticality; Real-Time scheduling; Stochastic analysis,Criticality (nuclear fission); Electric batteries; Embedded systems; Energy harvesting; Numerical methods; Scheduling; Stochastic systems; Uncertainty analysis; Analytical method; Environmental resources; Mixed criticalities; Mixed-criticality systems; Numerical computations; Real - time scheduling; Real-time embedded systems; Stochastic analysis; Scheduling algorithms
GPUrpc: Exploring transparent access to remote GPUs,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992215216&doi=10.1145%2f2950056&partnerID=40&md5=048258520773314a4e132520d79b346f,"Graphics processing units (GPUs) are increasingly used for high-performance computing. Programming frameworks for general-purpose computing on GPUs (GPGPU), such as CUDA and OpenCL, are also maturing. Driving this trend is the recent proliferation of mobile devices such as smartphones and wearable computers. These devices are increasingly incorporating computationally intensive applications that involve some form of environmental recognition such as augmented reality (AR) or voice recognition. However, devices with low computational power cannot satisfy such demanding computing requirements. The CPU load of these devices could be reduced by offloading computation onto GPUs on the cloud. This paper presents GPUrpc, a remote procedure call (RPC) extension to Gdev, which is a rich set of runtime libraries and device drivers for achieving first-class GPU resource management. GPUrpc allows developers to use CUDA for GPGPU development work. Existing research uses RPCs based on the CUDA application programming interfaces (APIs); hence, all CUDA APIs require communication. To reduce communication overhead, we use an RPC based on a low-level API than CUDA API and reduced API that does not require communication. Our evaluation conducted on Linux and NVIDIA GPUs shows that the basic performance of our prototype implementation is reliable in comparison with the existing method. Evaluation using the Rodinia benchmark suite designed for research in heterogeneous parallel computing showed that GPUrpc is effective for applications such as image processing and data mining. GPUrpc also can improve power consumption to approximately 1/6 that of CPU processing for performing 512 x 512 matrix multiplication. © 2016 ACM.",Cloud computing; Distributed computing; GPU; High performance computing; Parallel computing,Augmented reality; Benchmarking; Cloud computing; Computer graphics; Computer operating systems; Data handling; Data mining; Distributed computer systems; Image processing; Mobile devices; Parallel processing systems; Program processors; Speech recognition; Communication overheads; Environmental recognition; General-purpose computing; Graphics processing units; Heterogeneous parallel computing; High performance computing; Offloading computations; Prototype implementations; Application programming interfaces (API)
SPMPool: Runtime SPM management for memory-intensive applications in embedded many-cores,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994479637&doi=10.1145%2f2968447&partnerID=40&md5=21a1543a05c0323a54dec0b78faad2e5,"Distributed Scratchpad Memories (SPMs) in embedded many-core systems require careful selection of data placement to achieve good performance. Applications mapped to these platforms have varying memory requirements based on their runtime behavior, resulting in under-or overutilization of the local SPMs. We propose SPMPool to share the available on-chip SPMs on many-cores among concurrently executing applications in order to reduce the overall memory access latency. By pooling SPM resources, we can assign underutilized memory resources, due to idle cores or low memory usage, to applications dynamically. SPM-Pool is the first workload-aware SPM mapping solution for many-cores that dynamically allocates data at runtime - using profiled data - to address the unpredictable set of concurrently executing applications. Our experiments on workloads with varying interapplication memory intensity show that SPMPool can achieve up to 76% reduction in memory access latency for configurations ranging from 16 to 256 cores, compared to the traditional approach that limits executing cores to use their local SPMs. © 2016 ACM.",Many-core; Memory mapping; Runtime system; Scratchpad memory,Embedded systems; Mapping; Multiprocessing systems; Many core; Memory access latency; Memory mapping; Memory requirements; Runtime behaviors; Runtime systems; Scratch pad memory; Traditional approaches; Memory architecture
Simulating reconfigurable multiprocessor systems-on-chip with MPSoCSim,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994524997&doi=10.1145%2f2972952&partnerID=40&md5=544862a5c4bc035a87c62e5a9ef9d71f,"Upcoming reconfigurable Multiprocessor Systems-on-Chip (MPSoCs) present new challenges for the design and early estimation of technology requirements due to their runtime adaptive hardware architecture. The usage of simulators offers capabilities to overcome these issues. In this article, MPSoCSim, a SystemC simulator for Network-on-Chip (NoC) based MPSoCs is extended to support the simulation of reconfigurable MPSoCs. Processors, such as ARM and MicroBlaze, and peripheral models used within the virtual platform are provided by Imperas/OVP and attached to the NoC. Moreover, traffic generators are available to analyze the system. The virtual platform currently supports mesh topology with wormhole switching and several routing algorithms such as XY-, a minimal West-First algorithm, and an adaptive West-First algorithm. Amongst the impact of routing algorithms regarding performance, reconfiguration processes can be examined using the presented simulator. A mechanism for dynamic partial reconfiguration is implemented that is oriented towards the reconfiguration scheme on real FPGA platforms. It includes the simulation of the undefined behavior of the hardware region during reconfiguration and allows the adjustment of parameters. During runtime, dynamic partial reconfiguration interfaces are used to connect the Network-on-Chip infrastructure with reconfigurable regions. The configuration access ports can be modeled by the controller for the dynamic partial reconfiguration in form of an application programming interface. An additional SystemC component enables the readout of simulation time from within the application. For evaluation of the simulator timing and power consumption of the simulated hardware are estimated and compared with a real hardware implementation on a Xilinx Zynq FPGA. The comparison shows that the simulator improves the development of reconfigurable MPSoCs by early estimation of system requirements. The power estimations show a maximum deviation of 9mW at 1.9W total power consumption.",Dynamic partial reconfiguration; Heterogeneous MPSoC; MPSoC simulator; Network-on-chip; OVP; Reconfigurable computing; Routing algorithms; Virtual platform,Application programming interfaces (API); Computer hardware; Electric power utilization; Fault tolerance; Field programmable gate arrays (FPGA); Hardware; Multiprocessing systems; Reconfigurable architectures; Reconfigurable hardware; Routing algorithms; Servers; Simulators; System-on-chip; Dynamic partial reconfiguration; Hardware implementations; Heterogeneous mpsoc; Multiprocessor systems on chips; Reconfigurable computing; Reconfiguration process; Reconfiguration schemes; Virtual platform; Network-on-chip
Game-theory-based active defense for intrusion detection in cyber-physical embedded systems,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992213754&doi=10.1145%2f2886100&partnerID=40&md5=b4f449c907b02753eda8e2396bb01f30,"Cyber-Physical Embedded Systems (CPESs) are distributed embedded systems integrated with various actuators and sensors. When it comes to the issue of CPES security, the most significant problem is the security of Embedded Sensor Networks (ESNs). With the continuous growth of ESNs, the security of transferring data from sensors to their destinations has become an important research area. Due to the limitations in power, storage, and processing capabilities, existing security mechanisms for wired or wireless networks cannot apply directly to ESNs. Meanwhile, ESNs are likely to be attacked by different kinds of attacks in industrial scenarios. Therefore, there is a need to develop new techniques or modify the current security mechanisms to overcome these problems. In this article, we focus on Intrusion Detection (ID) techniques and propose a new attack-defense game model to detect malicious nodes using a repeated game approach. As a direct consequence of the game model, attackers and defenders make different strategies to achieve optimal payoffs. Importantly, error detection and missing detection are taken into consideration in Intrusion Detection Systems (IDSs), where a game tree model is introduced to solve this problem. In addition, we analyze and prove the existence of pure Nash equilibrium and mixed Nash equilibrium. Simulations show that the proposed model can both reduce energy consumption by up to 50% compared with the existing All Monitor (AM) model and improve the detection rate by up to 10% to 15% compared with the existing Cluster Head (CH) monitor model. © 2016 ACM.",Cyber-physical embedded systems; Embedded sensor network; Game theory; Intrusion detection; Network security; Optimal active defense,Computation theory; Digital storage; Embedded systems; Energy utilization; Game theory; Intrusion detection; Mercury (metal); Problem solving; Sensor networks; Trees (mathematics); Active defense; Actuators and sensors; Cyber physicals; Distributed embedded system; Embedded sensors; Intrusion Detection Systems; Pure Nash equilibrium; Reduce energy consumption; Network security
Frequency-aware ESL power estimation for ARM cortex-A9 using a black box processor model,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992225396&doi=10.1145%2f2987375&partnerID=40&md5=e30ce70b0ed1eeb90dcbfbef868aa064,"Power estimation has become a strongly desired feature in Electronic System Level (ESL) simulations. Most existing power estimation approaches for this abstraction level require component models with observable internals. However, most ESL models of modern processors are delivered as black box components. This work presents a tool-based ESL power estimation methodology for black box models and its extension for multiple clock frequencies. The evaluation uses hardware measurements of the ARM Cortex-A9 subsystem of the OMAP4460 chip for reference. The achieved estimation error is 5% on average for fixed-frequency power models and 7% for multifrequency power models. © 2016 ACM.",Black box; Electronic system level; Frequency; Power estimation; Power model; Processor model,ARM processors; Estimation; System theory; Black boxes; Electronic system level; Frequency; Power estimations; Power model; Processor modeling; Frequency estimation
Automatic parallelization of multirate block diagrams of control systems on multicore platforms,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992178569&doi=10.1145%2f2950055&partnerID=40&md5=6c3e30997ea9fec3c6e75e19f7de2fdd,"This article addresses the problem of parallelizing model block diagrams for real-time embedded applications on multicore architectures. We describe a Mixed Integer Linear Programming formulation for finding a feasible mapping of the blocks to different CPU cores. For single-rate models, we use an objective function that minimizes the overall worst-case execution time. We introduce a set of heuristics to solve the problem for large models in a reasonable time. For multirate models, we solve the feasibility problem for finding a valid mapping. We study the scalability and efficiency of our approach with synthetic benchmarks and an engine controller from Toyota. © 2016 ACM.",Embedded control systems; Model-based development; Multicore platforms; Multirate; Optimization; Scheduling; Simulink; Task allocation,Control systems; Embedded systems; Integer programming; Mapping; Optimization; Scheduling; Software architecture; Embedded control systems; Model based development; Multi rate; Multi-core platforms; Simulink; Task allocation; Problem solving
Parallel SystemC simulation for ESL design,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992189571&doi=10.1145%2f2987374&partnerID=40&md5=3ece9d0df7977f2f3cc8c43aeff42c11,"Virtual platforms have become essential tools for the design of embedded systems. Developers rely on them for design space exploration and software debugging. However, with rising HW/SW complexity and the need to simulate more and more processors simultaneously, the performance of virtual platforms degrades rapidly. Parallel simulation techniques can help to counter this by leveraging multicore PCs, which are widely available today. This work presents a novel parallel simulation approach that is targeted toward acceleration of virtual platforms from the ESL domain. By trading some timing accuracy, multiprocessor virtual platforms can be accelerated by up to 3.4x on regular quad-core workstations. © 2016 ACM.",Electronic system level; Parallel discrete event simulation; Parallel SystemC simulation,Discrete event simulation; Logic design; Program debugging; System theory; Systems analysis; Design space exploration; Electronic system level; Parallel discrete event simulations; Parallel simulation techniques; Parallel simulations; Software debugging; SystemC; Virtual platform; Embedded systems
Image-content-aware I/O optimization for mobile virtualization,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992176539&doi=10.1145%2f2950059&partnerID=40&md5=de060c1829cf2c713ca549919c0ecf02,"Mobile virtualization introduces extra layers in software stacks, which leads to performance degradation. Notably, each I/O operation has to pass through several software layers to reach the NAND-flash-based storage systems. This article targets at optimizing I/O for mobile virtualization, since I/O becomes one of major performance bottlenecks that seriously affects the performance of mobile devices. Among all the I/O operations, a large percentage is to update metadata. Frequently updated metadata not only degrade overall I/O performance but also severely reduce flash memory lifetime. In this article, we propose a novel I/O optimization technique to identify the metadata of a guest file system that is stored in a virtual machine image file and frequently updated. Then, these metadata are stored in a small additional non-volatile memory (NVM), which is faster and more endurable to greatly improve flash memory's performance and lifetime. To the best of our knowledge, this is the first work to identify the file system metadata from regular data in a guest OS image file with NVM optimization. The proposed scheme is evaluated on a real hardware embedded platform. The experimental results show that the proposed techniques can improve write performance to 45.21% in mobile devices with virtualization. © 2016 ACM.",Flash memory; Metadata; Mobile virtualization; Non-volatile memory,Data storage equipment; Digital storage; File organization; Metadata; Mobile devices; Nonvolatile storage; Virtual reality; Embedded platforms; Non-volatile memory; Optimization techniques; Performance bottlenecks; Performance degradation; Software stacks; Virtual machines; Virtualizations; Flash memory
Space-efficient index scheme for PCM-based multiversion databases in cyber-physical systems,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992187025&doi=10.1145%2f2950060&partnerID=40&md5=63c9e6771d76127915efab74bc4b6a95,"In this article, we study the indexing problem of using PCM as the storage medium for embedded multiversion databases in cyber-physical systems (CPSs). Although the multiversion B+-tree (MVBT) index has been shown to be efficient in managing multiple versions of data items in a database, MVBT is designed for databases residing in traditional block-oriented storage devices. It can have serious performance problems when the databases are on phase-change memory (PCM). Since the embedded multiversion database in CPSs may have limited storage space and are update intensive, to resolve the problems of MVBT of lack of space efficiency and heavy update cost, we propose a new index scheme, called space-efficient multiversion index (SEMI), to enhance the space utilization and access performance in serving various types of queries. In SEMI, since the number of keys in the database may be small, instead of using a B-tree index, we propose to use a binary-search tree to organize the index keys. Furthermore, multiple versions of the same data item may be stored consecutively and indexed by a single entry to maximize the space utilization and at the same time to enhance the performance in serving version-range queries. Analytical studies have been conducted on SEMI, and a series of experiments have been performed to evaluate its performance as compared with MVBT under different workloads. The experimental results have demonstrated that SEMI can achieve very high space utilization and has better performance in serving update transactions and range queries as compared with MVBT. © 2016 ACM.",Cyber-physical systems; Embedded database; Multiversion index; Phase-change memory (PCM),Binary trees; Database systems; Digital storage; Phase change memory; Query languages; Trees (mathematics); Virtual storage; Binary search trees; Cyber physical systems (CPSs); Embedded database; Multi-version; Performance problems; Phase change memory (pcm); Space efficiencies; Update transaction; Embedded systems
A framework for interconnection-aware domain-specific many-accelerator synthesis,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992184140&doi=10.1145%2f2983624&partnerID=40&md5=e44760cade1d1b6e9a630c45ea646413,"Many-accelerator Systems-on-Chip (SoC) have recently emerged as a promising platform paradigm that combines parallelization with heterogeneity, in order to cover the increasing demands for high performance and energy efficiency. To exploit the full potential of many-accelerator systems, automated design verification and analysis frameworks are required, targeted to both computational and interconnection optimization. Accurate simulation of interconnection schemes should use real stimuli, which are produced from fully functional nodes, requiring the prototyping of the processing elements and memories of the many-accelerator system. In this article, we argue that the Hierarchical Network-on-Chip (HNoC) scheme forms a very promising solution for many-accelerator systems in terms of scalability and data-congestion minimization. We present a parameterizable SystemC prototyping framework for HNoCs, targeted to domain-specific many-accelerator systems. The framework supports the prototyping of processing elements, memory modules, and underlying interconnection infrastructure, while it provides an API for their easy integration to the HNoC. Finally, it enables holistic system simulation using real node data. Using as a case study a many-accelerator system of an MRI pipeline, an analysis on the proposed framework is presented to demonstrate the impact of the system parameters on the system. Through extensive experimental analysis, we show the superiority of HNoC schemes in comparison to typical interconnection architectures. Finally, we show that, adopting the proposed many-accelerator design flow, significant performance improvements are achieved, from 1.2x up to 26x, as compared to a x86 software implementation of the MRI pipeline. © 2016 ACM.",Hierarchical networks-on-chip; Simulation; SystemC; Virtual prototyping,Acceleration; Energy efficiency; Hierarchical systems; Integrated circuit design; Integrated circuit interconnects; Memory architecture; Network-on-chip; Pipelines; Programmable logic controllers; Systems analysis; Virtual prototyping; Congestion minimization; Experimental analysis; Hierarchical network; Interconnection architecture; Performance improvements; Simulation; Software implementation; SystemC; Pipeline processing systems
Crosstalk-aware automated mapping for optical networks-on-chip,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992202532&doi=10.1145%2f2930666&partnerID=40&md5=863033de223d5a9384c91d502ac79345,"Optical networks-on-chip (NoCs) provide a promising answer to address the increasing requirements of ultra-high bandwidth and extremely low power consumption. Designing a photonic interconnect, however, involves a number of challenges that have no equivalent in the electronic domain, particularly the crosstalk noise, which affects the signal-to-noise ratio (SNR) possibly resulting in an inoperable architecture and hence constraining the network scalability. In this article, we point out the implications of application-driven task mapping on crosstalk effects. We motivate the main rationale of our work and provide a formalization of the problem. Then we propose a class of algorithms that automatically map the application tasks onto a generic mesh-based photonic NoC architecture such that the worst-case crosstalk is minimized. We also present a purpose-built experimental setup used for evaluating several architectural solutions in terms of crosstalk noise and SNR. The setup is used to collect extensive results from several real-world applications and case studies. The collected results show that the crosstalk noise can be significantly reduced by adopting our approach, thereby allowing higher network scalability, and can exhibit encouraging improvements over application-oblivious architectures. © 2016 ACM.",Application mapping; Crosstalk; Design automation; On-chip interconnects; Optical network-on-chip; Power loss; Silicon photonics,Computer aided design; Crosstalk; Fiber optic networks; Integrated circuit interconnects; Low power electronics; Mapping; Multiprocessing systems; Network architecture; Scalability; Signal to noise ratio; Silicon photonics; Application mapping; Applications and case studies; Architectural solutions; Design automations; Low-power consumption; On-chip interconnects; Optical networks on chips; Power-losses; Network-on-chip
MPSoC software debugging on virtual platforms via execution control with event graphs,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992206892&doi=10.1145%2f2950052&partnerID=40&md5=4638f4cd1890c70131676eccef369913,"Virtual Platforms (VPs) are advantageous to develop and debug complex software for multi- and many-processor systems-on-chip (MPSoCs). VPs provide unrivaled controllability and visibility of the target, which can be exploited to examine bugs that cannot be reproduced easily in real hardware (e.g., bugs originating from races or happening during a processor stand-by state). However, VPs as employed in practice for debugging are generally underutilized. The accompanying debug ecosystem is based mostly on traditional tools, such as step-based debuggers and traces, that fall short to address the enormous complexity of modern MPSoCs and their parallel software. Finding a bug is still largely left to the developer's experience and intuition, using manual means rather than automated or systematic solutions that exploit the controllability and visibility of VPs. Profiting from VPs for MPSoC software debugging is an open question. To bridge this gap, this article presents a novel framework for debug visualization and execution control that, relying on the many benefits of VPs, helps to identify and test possible concurrency-related bug scenarios. The framework allows examining and steering the target system by manipulating an abstract graph that highlights relevant inter-component interactions and dependencies. The proposed framework reduces the effort required to understand complex concurrency patterns and helps to expose bugs. Its efficacy is demonstrated on (i) a shared memory symmetric multi-processing platform executing Linux and parallel benchmarks, and (ii) a distributed automotive system for driver assistance applications. © 2016 ACM.",Concurrency; Debug; MPSoC; SystemC; Virtual platform,Abstracting; Automobile drivers; Benchmarking; Computer operating systems; Concurrency control; Distributed parameter control systems; Embedded systems; Multiprocessing systems; Parallel processing systems; System-on-chip; Visibility; Concurrency; Debug; MPSoC; SystemC; Virtual platform; Program debugging
A lightweight framework for the dynamic creation and configuration of virtual platforms in SystemC,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992168844&doi=10.1145%2f2983626&partnerID=40&md5=a56a737b9463dc26ba25389dbd97c592,"Virtual prototypes leverage SystemC/TLM for simulating programmable platforms comprising hundreds of modules. Their efficient creation and configuration is vital for acceptable turnaround times, for example, during performance exploration or software development. Therefore, our lightweight framework provides a factory that creates designs from abstract descriptions of module instances, properties, and connections. Modules mark properties as creation or runtime parameters. The resulting generic design descriptions are usable by non-experts and enable front-ends. The infrastructure is a small C++ library with only 1,350 lines of code that can be combined with existing SystemC/TLM models and simulation kernels. An industrial case study of a complex multiprocessor SoC shows a distinct productivity gain. © 2016 ACM.",CCI; SystemC; TLM; Virtual platforms,C++ (programming language); Computer software; Productivity; System-on-chip; Turnaround time; Industrial case study; Lightweight frameworks; Multi-processor SoC; Productivity gain; Programmable platforms; Run time parameters; SystemC; Virtual platform; Software design
Reducing power consumption and latency in mobile devices using an event stream model,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992190549&doi=10.1145%2f2964203&partnerID=40&md5=62e8050cfefa80e1d86a82b541d99264,"Most consumer-based mobile devices use asynchronous events to awaken apps. Currently, event handling is implemented in either an application or an application framework such as Java's virtual machine (VM) or Microsoft's .NET, and it uses a ""polling loop"" that periodically queries an event queue to determine if an event has occurred. These loops must awaken the process, check for an event, and then put the process back to sleep many times per second. This constant arousal prevents the CPU from being put into a deep sleep state, which increases power consumption. Additionally, the process cannot check for events while it sleeps, and this delay in handling events increases latency, which is the time that elapses between when an event occurs and when the application responds to the event. We call this model of event handling a ""pull"" model because it needs to query hardware devices or software queues in order to ""pull"" events from them. Recent advances in input devices support direct, informative interrupts to the kernel when an event occurs. This allows us to develop a much more efficient event-handling model called the ""Event Stream Model"" (ESM). This model is a push model that allows a process to sleep as long as no event occurs but then immediately awakens a process when an event occurs. This model eliminates the polling loop, thus eliminating latency-inducing sleep between polls and reducing unnecessary power consumption. To work properly, the ESM model must be implemented in the kernel rather than in the application. In this article, we describe how we implemented the ESM model in Android operating system (OS). Our results show that with the event stream model, power consumption is reduced by up to 23.8% in certain circumstances, and latency is reduced by an average of 13.6ms. © 2016 ACM.",Graphical user interfaces; Kernel event stream model; Mobile devices; Power conservation; Push model; Reduced latency,Electric power utilization; Mobile devices; Sleep research; Application frameworks; Asynchronous event; Event handling; Event streams; Hardware devices; Input devices; Power conservation; Reduced latencies; Graphical user interfaces
VirtualSoC: A research tool for modern MPSoCs,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992183836&doi=10.1145%2f2930665&partnerID=40&md5=604f14c24bc9ec7886f609c7ac990374,"Architectural heterogeneity has proven to be an effective design paradigm to cope with an ever-increasing demand for computational power within tight energy budgets, in virtually every computing domain. Programmable manycore accelerators are currently widely used not only in high-performance computing systems, but also in embedded devices, in which they operate as coprocessors under the control of a general-purpose CPU (the host processor). Clearly, such powerful hardware architectures are paired with sophisticated and complex software ecosystems, composed of operating systems, programming models plus associated runtime engines, and increasingly complex user applications with related libraries. System modeling has always played a key role in early architectural exploration or software development when the real hardware is not available. The necessity of efficiently coping with the huge HW/SW design space provided by the described heterogeneous Systems on Chip (SoCs) calls for advanced full-system simulation methodologies and tools, capable of assessing various metrics for the functional and nonfunctional properties of the target system. In this article, we describe VirtualSoC, a simulation tool targeting the full-system simulation of massively parallel heterogeneous SoCs. We also describe how VirtualSoC has been successfully adopted in several research projects. © 2016 ACM.",Accuracy; Full-system simulation; Manycore accelerators; SystemC modeling; Virtual platforms,Application programs; Budget control; Embedded systems; Hardware; Hardware-software codesign; Integrated circuit design; Software design; System-on-chip; Accuracy; Full-system simulation; Many-core accelerators; SystemC modeling; Virtual platform; Computer systems programming
Autonomous OA removal in real-time from single channel EEG data on a wearable device using a hybrid algebraic-wavelet algorithm,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992195285&doi=10.1145%2f2983629&partnerID=40&md5=edcef21d9842b707cdaccd197341edd5,"Electroencephalography (EEG) is a non-invasive technique to record brain activities in natural settings. Ocular Artifacts (OA) usually contaminates EEG signals, removal of which is critical for accurate feature extraction and classification. With the increasing adoption of wearable technologies, single-channel real-time EEG systems that often require real-time signal processing for immediate real-time feedback are becoming more prevalent. However, traditional OA removal algorithms usually require multiple channels of EEG data, are computationally expensive, and do not perform well in real-time. In this article, a new hybrid algorithm is proposed that autonomously detects OA and subsequently removes OA from a single-channel steaming EEG data in real-time. The proposed single EEG channel algorithm also does not require additional reference electrooculography (EOG) channel. The algorithm has also been implemented on an embedded hardware platform of single channel wearable EEG system (NeuroMonitor). The algorithm first detects the OA zones using an Algebraic approach and then removes these artifacts from the detected OA zones using the Discrete Wavelet Transform (DWT) decomposition method. The de-noising technique is applied only to the OA zone, which minimizes loss of neural information outside the OA zone. A qualitative and quantitative performance evaluation was carried out with a 0.5s epoch in overlapping sliding window technique using time-frequency analysis, mean square coherence, and correlation coefficient statistics. The hybrid OA removal algorithm demonstrated real-time operation with 3s latency on the PSoC-3-microcontroller-based EEG system. Successful implementation of OA removal from single-channel real-time EEG data using the proposed algorithm shows promise for real-time feedback applications of wearable EEG devices. © 2016 ACM.",Artifact removal; EEG signal processing; Real-time algorithm; Wearable,Algebra; Biomedical signal processing; Brain; Discrete wavelet transforms; Electroencephalography; Electrophysiology; System-on-chip; Wavelet decomposition; Wearable computers; Artifact removal; EEG signal processing; Feature extraction and classification; Real time algorithms; Real-time signal processing; Sliding window techniques; Time frequency analysis; Wearable; Real time systems
Guest editorial: Special issue on virtual prototyping of parallel and embedded systems (ViPES),2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992209627&doi=10.1145%2f2991466&partnerID=40&md5=cccee501eea46649fd7685324dce283e,[No abstract available],,
Adaptive workload management in mixed-criticality systems,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992195970&doi=10.1145%2f2950058&partnerID=40&md5=a4e8ecf122daa75e75f8865b9170bc56,"Due to the efficient resource usage of integrating tasks with different criticality onto a shared platform, the integration with mixed-criticality tasks is becoming an increasingly important trend in the design of real-time systems. One challenge in such a mixed-criticality system is to maximize the service for low-critical tasks, while meeting the timing constraints of high-critical tasks. In this article, we investigate how to adaptively manage the low-critical workload during runtime to meet both goals, that is, providing the service for low-critical tasks as much as possible and guaranteeing the hard real-time requirements for high-critical tasks. Unlike previous methods, which enforce an offline bound towards the low-critical workload, runtime adaptation approaches are proposed in which the incoming workload of low-critical tasks is adaptively regulated by considering the actual demand of high-critical tasks. This actual demand of the high-critical tasks, in turn, is adaptively updated using their historical arrival information. Based on this adaptation scheme, two scheduling policies-the priority-adjustment policy and the workload-shaping policy-are proposed to do the workload management. In order to reduce online management overhead, a lightweight scheme with O(n• log(n)) complexity is developed. Extensive simulation results are presented to demonstrate the effectiveness of our proposed workload management approaches. © 2016 ACM.",Mixed-criticality systems; Real-time event streams; Real-time interface; Workload bounding; Workload management,Criticality (nuclear fission); Interactive computer systems; Scheduling; Mixed-criticality systems; Real time; Real time interfaces; Workload bounding; Workload management; Real time systems
Guest editorial: Special issue on emerging technologies in embedded software and systems,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992187864&doi=10.1145%2f2991464&partnerID=40&md5=12893d6fb8cd0abac4d1333e3d047578,[No abstract available],,
A scriptable standard-compliant reporting and logging framework for SystemC,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992184065&doi=10.1145%2f2983623&partnerID=40&md5=7ecaa61e091a6422bf44fbf7e2062675,"With the ever-increasing complexity of digital designs, debugging and evaluation face likewise increasing challenges. While recent advances in hardware/software co-simulation have been made, solutions for corresponding debugging and evaluation did not mature and improve in a similar fashion. In this article, we present a dedicated solution to ease the debugging and evaluation efforts, particularly focusing on full-system simulation. Improving significantly over existing solutions, the presented approach features a standards-compliant powerful and flexible method of deriving, logging, and filtering detailed status information from SystemC-based models. At the core of this approach are flexible scripting capabilities that may change all logging parameters during runtime, thus not requiring re-compiling the to-be-simulated model, as in many competing solutions. The approach is tested and benchmarked with a real-world full-system example, demonstrating the overall benefits. The presented solution is published as open source via github (see text) and, by strictly adhering to existing standards, is generally compatible with existing SystemC simulation environments. © 2016 ACM.",Automation; Logging; Python; Scripting; SystemC; TLM,Automation; Computer debugging; Filtration; Information filtering; Logging (forestry); Program debugging; Full-system simulation; Hardware/software; Logging framework; Python; Scripting; Simulation environment; Status informations; SystemC; Open systems
ScorePlus: A software-hardware hybrid and federated experiment environment for Smart Grid,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992189479&doi=10.1145%2f2964200&partnerID=40&md5=cedf3031ce12ababc9b054757d987131,"We present ScorePlus, a software-hardware hybrid and federated experiment environment for Smart Grid. ScorePlus incorporates both a software emulator and hardware testbed, such that they all follow the same architecture, and the same Smart Grid application program can be tested on either of them without any modification; ScorePlus provides a federated environment such that multiple software emulators and hardware testbeds at different locations are able to connect and form a unified Smart Grid system; ScorePlus software is encapsulated as a resource plugin in the OpenStack cloud computing platform, such that it supports massive deployments with large-scale test cases in cloud infrastructure. © 2016 ACM.",Smart Grid; Testbed,Application programs; Distributed computer systems; Electric power transmission networks; Hardware; Testbeds; Cloud computing platforms; Cloud infrastructures; Hardware testbeds; Large scale tests; Massive deployment; Smart grid; Smart grid applications; Smart grid systems; Smart power grids
Harmonic segment-based semi-partitioning scheduling on multi-core real-time systems,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055775497&doi=10.1145%2f2933388&partnerID=40&md5=70c8b79362ab8bba44fd047801ddc15b,"Nowadays, the issue of scheduling multi-core real-time systems has become the focus of such research in industrial, biomedical, military, and other fields. As a consequence, a new semi-partitioning algorithm that uses a static Rate-Monotonic criterion to schedule real-time tasks on multi-core platforms is proposed. The improvement in the performance of real-time systems is achieved by exploitingthe fact that the utilization boundary of a task set increases to fully utilize the processors if the periods of tasks have harmonic nature among each other. Experimental results on randomly generated datasets and real-world datasets show that the proposed algorithm inevitably outperforms other competitive algorithms. © 2016 ACM",And Phrases: Segment-based; Harmonic tasks; Rate monotonic scheduling; Semi-partitioning approach,Harmonic analysis; Industrial research; Interactive computer systems; Scheduling; Competitive algorithms; Harmonic tasks; Multi-core platforms; Partitioning algorithms; Rate-monotonic scheduling; Real-world datasets; Segment-based; Semi-partitioning approach; Real time systems
On the improved hard real-time scheduling of cyclo-static dataflow,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057882792&doi=10.1145%2f2932188&partnerID=40&md5=d8592a050fe4a293499c7577eb971fdf,"Recently, it has been shown that the hard real-time scheduling theory can be applied to streaming applications modeled as acyclic Cyclo-Static Dataflow (CSDF) graphs. However, this recent approach is not always efficient in terms of throughput and processor utilization. Therefore, in this article, we propose an improved hard real-time scheduling approach to schedule streaming applications modeled as acyclic CSDF graphs on a Multiprocessor System-on-Chip (MPSoC) platform. The proposed approach converts each actor in a CSDF graph to a set of real-time periodic tasks. The conversion enables application of many hard real-time scheduling algorithms that offer fast calculation of the required number of processors for scheduling the tasks. In addition, we propose a method to reduce the graph latency when the converted tasks are scheduled as real-time periodic tasks. We evaluate the performance and time complexity of our approach in comparison to several existing scheduling approaches. Experiments on a set of real-life streaming applications demonstrate that our approach (1) results in systems with higher throughput and better processor utilization in comparison to the existing hard real-time scheduling approach for CSDF graphs, while requiring comparable time for the system derivation; (2) delivers shorter application latency by applying the proposed method for graph latency reduction while providing better throughput and processor utilization when compared to the existing hard real-time scheduling approach; (3) gives the same throughput as the existing periodic scheduling approach for CSDF graphs, but requires much shorter time to derive the task schedule and tasks’ parameters (periods, start times, and so on); and (4) gives the throughput that is equal to or very close to the maximum achievable throughput of an application obtained via self-timed scheduling, but requires much shorter time to derive the schedule. The total time needed for the proposed conversion approach and the calculation of the minimum number of processors needed to schedule the tasks and the calculation of the size of communication buffers between tasks is in the range of seconds. © 2016 ACM.",Cyclo-static dataflow; Hard real-time scheduling; Multiprocessor system-on-chip; Streaming applications,Data flow analysis; Graphic methods; Multiprocessing systems; Programmable logic controllers; Scheduling; Scheduling algorithms; System-on-chip; Throughput; Achievable throughputs; Communication buffer; Cyclo-static dataflow; Hard real-time; Multiprocessor system on chips; Periodic scheduling; Processor utilization; Streaming applications; Real time systems
A novel embedded interpolation algorithm with negative squared distance for real-time endomicroscopy,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987933776&doi=10.1145%2f2905367&partnerID=40&md5=9d1d7c5e5b1648ef2641b81ea7b12407,"Interpolation is the most executed operation and one of the main bottlenecks in embedded imaging, registration, and rendering systems. Existing methods either lack parallelization and scalability capabilities or are too computationally complex to execute efficiently. Acknowledging that improving execution time leads to degradation in image quality, we formulate a novel Negative Squared Distance (NSD) interpolation method that exhibits excellent performance by exploiting Look-Up Table (LUT) optimization for Field Programmable Gate Array (FPGA) speedup, with a balanced trade-off in quality in our embedded endomicroscopic imaging system. Quantitative analysis on performance and resource utilization of NSD against existing methods is reported through an implementation on a Xilinx ML605 platform. Functional validation using practical image resizing and rotation applications to compare qualitative performance against existing algorithms is performed and presented with visual and numerical results. Our method is shown to have a smaller design size and produces a maximum throughput of over twofold against trilinear interpolation with on-par image quality as the baseline method. © 2016 ACM.",Embedded systems; Image processing,Economic and social effects; Field programmable gate arrays (FPGA); Image processing; Image quality; Interpolation; Optimization; Table lookup; Functional validation; Interpolation algorithms; Interpolation method; Maximum through-put; Numerical results; Resource utilizations; Squared distances; Trilinear interpolation; Embedded systems
An efficient technique of application mapping and scheduling on real-time multiprocessor systems for throughput optimization,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981500051&doi=10.1145%2f2950051&partnerID=40&md5=6e487ac124a6d8234e69009555ef3346,"Multiprocessor systems are becoming ubiquitous in today's embedded systems design. In this article, we address the problem of mapping an application represented by a Homogeneous Synchronous Dataflow (HSDF) graph onto a real-time multiprocessor platform with the objective of maximizing total throughput. We propose that the optimal solution to the problem is composed of three components: actor-to-processor mapping, retiming, and actor ordering on each processor. The entire problem is systematically modeled into a Boolean Satisfiability (SAT) problem such that the optimal solution can be guaranteed theoretically. In order to explore the vast solution space more efficiently, we develop a specific HSDF theory solver based on the special characteristics of the timed HSDF, and integrate it into the general search framework of the SAT solver. Two alternative integration methods based on branch-and-bound are presented to achieve early branch pruning in the search space; thus, the scalability is greatly improved. Extensive performance evaluation on synthetic examples and a case study on the realistic H.264 Video Decoder show that our approach provides as much as 76.9% throughput improvement, and is scalable to industry-sized applications.",Multiprocessor; Optimization; Satisfiability; Scheduling,Data flow analysis; Embedded software; Formal logic; Mapping; Multiprocessing systems; Optimal systems; Optimization; Real time systems; Scheduling; Throughput; Boolean satisfiability; Multi processor systems; Multi-processor platforms; Multiprocessor; Satisfiability; Synchronous Dataflow; Throughput improvement; Throughput optimization; Embedded systems
A methodology for estimating performance and power consumption of embedded flash file systems,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029544010&doi=10.1145%2f2903139&partnerID=40&md5=c718691cc09762f2b6820f55b6852d87,"In the embedded systems domain, obtaining performance and power consumption estimations is extremely valuable in numerous cases. This is particularly true during the design stage, as designers of complex embedded systems face an increasingly large design space. Secondary storage is a well-known performance bottleneck and has also been reported as an important factor of power consumption. Flash memory is the main secondary storage media in an embedded system and exhibits specific constraints in its usage. One popular way to manage these constraints is to use dedicated Flash File Systems (FFS). In this article, we propose a methodology to estimate the performance and power consumption of applicative I/Os on an FFS-based storage system within embedded Linux. The methodology is divided into three sequential steps. In the exploration phase, the main factors of an FFS storage system impacting performance and power consumption are identified. In the modeling phase, this impact is formalized into models. Finally, in the last phase, the models are implemented in a simulator named OpenFlash. OpenFlash allows obtaining performance and power consumption estimations for an applicative workload processed by the Linux FFS storage stack on an embedded platform. The simulator is validated against real measurements and the estimation error stays below 10%. © 2016 ACM",And Phrases: NAND flash memory; Embedded Linux; Estimation; Flash file systems; Modeling; Performance; Power consumption; Simulation,Electric power utilization; Estimation; File organization; Flash memory; Linux; Models; Embedded Linux; FLASH file systems; NAND flash memory; Performance; Simulation; Embedded systems
A universal application storage system based on smart card,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987971678&doi=10.1145%2f2886116&partnerID=40&md5=9dde137cb1f1dff7a3494ceb6f23192c,"Nowadays, electronic commerce (e-commerce) has brought facilitation to people's daily lives. Smart-cardbased systems are widely used as an implementation,where smart cards act as a secure carrier for small-sized data. However, most of these systems are developed and managed by each service provider individually and repeatedly, which causes both unnecessary work and difficulties in future maintenance. Besides, advantages of smart card technology are not full-fledged for the lack of enough consideration in flexibility and security. To propose a solution, this article presents a Universal Application Storage System, including card side, terminal side, and back-end system. The card side provides a universal and secured infrastructure for data storage, where data are organized and stored in a card file system with several security mechanisms. In the terminal side, a framework for accessing various forms of secure element is presented to simplify the procedures involved in manipulating smart cards. Through this framework, the back-end system is able to establish a direct connection to the card, and performs authorized operations by exchanging commands in a secure channel. The validity of the proposed system is verified at the end of this article, illustrated by an e-coupon system. © 2016 ACM.",Application protocol unit; File system; Java card; Security; Smart card,Commerce; Electronic commerce; File organization; Java programming language; Smart cards; Transportation; Application protocols; File systems; JAVA card; Secured infrastructures; Security; Security mechanism; Smart-card technology; Universal application; Digital storage
Editorial: Security of embedded systems and cyber irons-embedded systems for security,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987941356&doi=10.1145%2f2976731&partnerID=40&md5=172efedaca1f4c6b2f898490857723f1,[No abstract available],,
VecRA: A vector-aware register allocator for GPU shader processors,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987984459&doi=10.1145%2f2961026&partnerID=40&md5=a19f3851e542ed10bb7f5f4daee0e848,"Graphics processing units (GPUs) are now widely used in embedded systems for manipulating computer graphics and even for general-purpose computation. However, many embedded systems have to manage highly restricted hardware resources in order to achieve high performance or energy efficiency. The number of registers is one of the common limiting factors in an embedded GPU design. Programs that run with a low number of registers may suffer from high register pressure if register allocation is not properly designed, especially on a GPU in which a register is divided into four elements and each element can be accessed separately, because allocating a register for a vector-type variable that does not contain values in all elements wastes register spaces. In this article, we present a vector-aware register allocation framework to improve register utilization on shader architectures. The framework involves two major components: (1) element-based register allocation that allocates registers based on the element requirement of variables and (2) register packing that rearranges elements of registers in order to increase the number of contiguous free elements, thereby keeping more live variables in registers. Experimental results on a cycle-approximate simulator showed that the proposed framework decreased 92% of register spills in total and made 91.7% of 14 common shader programs spill free. These results indicate an opportunity for energy management of the space that is used for storing spilled variables, with the framework improving the performance by a geometric mean of 8.3%, 16.3%, and 29.2% for general shader processors in which variables are spilled to memory with 5-, 10-, and 20-cycle access latencies, respectively. Furthermore, the reduction in the register requirement of programs enabled another 11 programs with high register pressure to be runnable on a lightweight GPU. © 2016 ACM.",Register allocation; Register packing; Shader processors,Computer graphics; Computer graphics equipment; Embedded systems; Energy efficiency; Program processors; Vector spaces; Access latency; General-purpose computations; Geometric mean; Hardware resources; Register allocation; Register pressure; Register requirement; Type variables; Graphics processing unit
Loosely time-triggered architectures: Improvements and comparisons,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025141439&doi=10.1145%2f2932189&partnerID=40&md5=8fbca200e7c1b0d53054ac4cf78bb32b,"Loosely Time-Triggered Architectures (LTTAs) are a proposal for constructing distributed embedded control systems. They build on the quasi-periodic architecture, where computing units execute nearly periodically, by adding a thin layer of middleware that facilitates the implementation of synchronous applications. In this article, we show how the deployment of a synchronous application on a quasi-periodic architecture can be modeled using a synchronous formalism. Then we detail two protocols, Back-Pressure LTTA, reminiscent of elastic circuits, and Time-Based LTTA, based on waiting. Compared to previous work, we present controller models that can be compiled for execution, a simplified version of the Time-Based protocol and optimizations for systems using broadcast communication. We also compare the LTTA approach with architectures based on clock synchronization. © 2016 ACM.",Back-pressure LTTA; Loosely time-triggered architecture; Quasi-periodic architecture; Time-based LTTA,Embedded systems; Middleware; Back pressures; Broadcast communication; Clock Synchronization; Controller models; Distributed embedded control system; Quasi-periodic; Time based; Time-triggered architectures; Computer architecture
Reliability-aware adaptations for shared last-level caches in multi-cores,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988014492&doi=10.1145%2f2961059&partnerID=40&md5=e932c9d75222949ba07cef1fa5800695,"On account of their large footprint, on-chip last-level caches in multi-core systems are one of the most vulnerable components to soft errors. However, vulnerability to soft errors highly depends on the configuration and parameters of the last-level cache, especially when executing different applications concurrently. In this article we propose a novel reliability-aware reconfigurable last-level cache architecture (R2Cache) and cache vulnerability model for multi-cores. R2Cache supports various reliability-wise efficient cache configurations (i.e., cache parameter selection and cache partitioning) for different concurrently executing applications. The proposed vulnerability model takes into account the vulnerability of both the data and tag arrays as well as the active cache area for applications in different execution phases. To enable runtime adaptations, we introduce a lightweight online vulnerability predictor that exploits the knowledge of performance metrics like number of L2 misses to accurately estimate the cache vulnerability to soft errors. Based on the predicted vulnerabilities of different concurrently executing applications in the current execution epoch, our runtime reliability manager reconfigures the cache such that, for the next execution epoch, the total vulnerability for all concurrently executing applications is minimized under user-provided tolerable performance/energy overheads. In scenarios where single-bit error correction for cache lines may be afforded, vulnerability-aware reconfigurations can be leveraged to increase the reliability of the last-level cache against multi-bit errors. Compared to state-of-the-art vulnerability-minimizing and reconfigurable caches, the proposed architecture provides 35.27% and 23.42% vulnerability savings, respectively, when averaged across numerous experiments, while reducing the vulnerability by more than 65% and 60%, respectively, for selected applications and application phases. © 2016 ACM.",Cache; Energy; Modeling; Multi-cores; Optimization; Performance; Reliability; Soft errors; Vulnerability,Cache memory; Error correction; Errors; Models; Optimization; Radiation hardening; Reconfigurable architectures; Reconfigurable hardware; Cache; Energy; Multi core; Performance; Soft error; Vulnerability; Reliability
CurA: A framework for quality-retaining power saving on mobile OLED displays,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021198143&doi=10.1145%2f2909875&partnerID=40&md5=edd33435c6f1b07d2f3d508fa1f6d24b,"Organic Light-Emitting Diode (OLED) technology is regarded as a promising alternative to mobile displays. In this article, we introduce the design, algorithm, and implementation of a novel framework called CURA for quality-retaining power saving on mobile OLED displays. First, we link human visual attention to OLED power saving and model the OLED image scaling optimization problem. The objective is to minimize the power required to display an image without adversely impacting the user’s visual experience. Then, we present the algorithm used to solve the modeled problem, and prove its optimality even without an accurate power model. Finally, based on the framework, we implement two practical applications on a commercial OLED mobile tablet. The results of experiments conducted on the tablet with real images demonstrate that CURA can reduce significant OLED power consumption while retaining the visual quality of images. © 2016 ACM.",And Phrases: OLED displays; Human visual attention; Low power; Mobile systems,Behavioral research; Human visual attention; Low Power; Mobile Displays; Mobile systems; OLED displays; Optimization problems; Visual experiences; Visual qualities; Organic light emitting diodes (OLED)
JoM: A joint operation mechanism for NAND flash memory,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029610396&doi=10.1145%2f2915916&partnerID=40&md5=f47e0e0e4fad10527938c05bb110feca,"In the storage systems of NAND flash memory, an intermediate software called a Flash Translation Layer (FTL) is adopted to hide the characteristics of NAND flash memory and provide efficient management for NAND flash memory. Current flash translation layers can be classified into a page-mapping FTL, a block-mapping FTL, and a hybrid-mapping FTL. In order to utilize the advantages of the page-mapping FTL and the block-mapping FTL, the hybrid-mapping FTL is proposed to store data to the appropriate mapping mechanism by switching the mapping information between the page-mapping mechanism and the block-mapping mechanism. In the article, we propose a joint operation mechanism to rethink the advantages of the page-mapping FTL, the block-mapping FTL, and the hybrid-mapping FTL. With the joint operation mechanism, a flash translation layer can consider the main memory requirements, improve the system performance, and reduce the garbage collection overhead. The experimental results show that the proposed joint operation mechanism can achieve the goal under realistic workloads and benchmarks. © 2016 ACM",Embedded systems; Flash translation layers; NAND flash memory; Storage systems,Embedded systems; Mapping; Memory architecture; NAND circuits; Efficient managements; Flash translation layer; Garbage collection; Joint operations; Mapping information; Mapping mechanism; NAND flash memory; Storage systems; Flash memory
Guest editorial for special issue of ESWEEK 2015,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988025932&doi=10.1145%2f2968218&partnerID=40&md5=87862dc998f40136fa4091cffab8ed24,[No abstract available],,
Static analysis of runtime errors in interrupt-driven programs via sequentialization,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988027141&doi=10.1145%2f2914789&partnerID=40&md5=1090e079af4d442235eb657b3166bbf0,"Embedded software often involves intensive numerical computations and suffers from a number of runtime errors. The technique of numerical static analysis is of practical importance for checking the correctness of embedded software. However, most of the existing approaches of numerical static analysis consider sequential programs, while interrupts are a commonly used facility that introduces concurrency in embedded systems. Therefore, a numerical static analysis approach is highly desired for embedded software with interrupts. In this article, we propose a static analysis approach specifically for interrupt-driven programs based on sequentialization techniques.We present a method to sequentialize interrupt-driven programs into nondeterministic sequential programs according to the semantics of interrupts. The key benefit of using sequentialization is the ability to leverage the power of state-of-the-art analysis and verification techniques for sequential programs to analyze interrupt-driven programs, for example, the power of numerical abstract interpretation to analyze numerical properties of the sequentialized programs. Furthermore, to improve the analysis precision and scalability, we design specific abstract domains to analyze sequentialized interruptdriven programs by considering their specific features. Finally, we present encouraging experimental results obtained by our prototype implementation. © 2016 ACM.",Abstract interpretation; Embedded software; Interrupt-driven programs; Runtime errors; Sequentialization; Static analysis,Abstracting; Embedded software; Embedded systems; Errors; Model checking; Semantics; Abstract interpretations; Numerical computations; Numerical properties; Practical importance; Prototype implementations; Run-time errors; Sequential programs; Sequentialization; Static analysis
A scalable algebraic method to infer quadratic invariants of switched systems,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988009550&doi=10.1145%2f2932187&partnerID=40&md5=c46dd54c86285d3a35d7ab98ddb6be2c,"We present a new numerical abstract domain based on ellipsoids designed for the formal verification of switched linear systems. Unlike the existing approaches, this domain does not rely on a user-given template. We overcome the difficulty that ellipsoids do not have a lattice structure by exhibiting a canonical operator overapproximating the union. This operator is the only one that permits the performance of analyses that are invariant with respect to a linear transformation of state variables. It provides the minimum volume ellipsoid enclosing two given ellipsoids. We show that it can be computed in O(n3) elementary algebraic operations. We finally develop a fast nonlinear power-type algorithm, which allows one to determine sound quadratic invariants on switched systems in a tractable way, by solving fixed-point problems over the space of ellipsoids. We test our approach on several benchmarks, and compare it with the standard techniques based on linear matrix inequalities, showing an important speedup on typical instances. © 2016 ACM.",Abstract interpretation; Hybrid and switched linear systems; Invariant generation; Matrix information geometry; Stability; Static analysis,Algebra; Convergence of numerical methods; Linear matrix inequalities; Linear systems; Linear transformations; Mathematical transformations; Static analysis; Switching systems; Yield stress; Abstract interpretations; Algebraic operations; Fixed-point problem; Information geometry; Invariant generations; Minimum volume ellipsoids; Quadratic invariant; Switched linear system; Matrix algebra
High-performance and energy-efficient network-on-chip architectures for graph analytics,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987941439&doi=10.1145%2f2961027&partnerID=40&md5=13eec100a406b527acc3475a5a1f3768,"With its applicability spanning numerous data-driven fields, the implementation of graph analytics on multicore platforms is gaining momentum. One of the most important components of a multicore chip is its communication backbone. Due to inherent irregularities in data movements manifested by graph-based applications, it is essential to design efficient on-chip interconnection architectures for multicore chips performing graph analytics. In this article, we present a detailed analysis of the traffic patterns generated by graph-based applications when mapped to multicore chips. Based on this analysis, we explore the designspace for the Network-on-Chip (NoC) architecture to enable an efficient implementation of graph analytics. We principally consider three types of NoC architectures, viz., traditional mesh, small-world, and high-radix networks.We demonstrate that the small-world-network-enabled wireless NoC (WiNoC) is the most suitable platform for executing the considered graph applications. The WiNoC achieves an average of 38% and 18% full-system Energy Delay Product savings compared to wireline-mesh and high-radix NoCs, respectively. © 2016 ACM.",Community detection; Graph analytics; Graph coloring; Wireless NoCs,Computer architecture; Distributed computer systems; Energy efficiency; Graphic methods; Mesh generation; Network architecture; Servers; Small-world networks; Wireless interconnects; Analysis of the traffic; Community detection; Efficient implementation; Energy efficient network on chips; Graph analytics; Graph colorings; Network-on-chip architectures; On-chip interconnection; Network-on-chip
Editorial: Fence itself grazing the field-security from the sentries,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979872970&doi=10.1145%2f2953045&partnerID=40&md5=d679f8d8cf9f23173a4e39afb872fbff,[No abstract available],,
RunStream: A high-level rapid prototyping framework for stream ciphers,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975493533&doi=10.1145%2f2891412&partnerID=40&md5=2d18c0fd7fb4e7d6df4ef713f2d554e9,"We present RunStream, a rapid prototyping framework for realizing stream cipher implementations based on algorithmic specifications and architectural customizations desired by the users. In the dynamic world of cryptography where newer recommendations are frequently proposed, the need of such tools is imperative. It carries out design validation and generates an optimized software implementation and a synthesizable Register Transfer Level Verilog description. Our framework enables speedy benchmarking against critical resources like area, throughput, power, and latency and allows exploration of alternatives. Using RunStream, we successfully implemented various stream ciphers and benchmarked the quality of results to be at par with published hand-optimized implementations. © 2016 ACM.",eSTREAM; Hardware generation; High-level synthesis; Rapid prototyping; Stream cipher,Algorithms; Cryptography; High level synthesis; Rapid prototyping; Critical resources; eSTREAM; Exploration of alternatives; Optimized implementation; Quality of results; Register transfer level; Software implementation; Stream Ciphers; Computer hardware description languages
Byte-addressable update scheme to minimize the energy consumption of PCM-based storage systems,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975517151&doi=10.1145%2f2910590&partnerID=40&md5=6e6128b2421b3ebcfd861bb1d585638c,"In recent years, phase-change memory (PCM) has generated a great deal of interest because of its byte addressability and nonvolatility properties. It is regarded as a good alternative storage medium that can reduce the performance gap between the main memory and the secondary storage in computing systems. However, its high energy consumption on writes is a challenging issue in the design of battery-powered mobile computing systems. To reduce the energy consumption, we exploit the byte addressability and the asymmetric read-write energy/latency of PCM in an energy-efficient update scheme for journaling file systems. We also introduce a concept called the 50% rule to determine/recommend the best update strategy for block updates. The proposed scheme only writes modified data, instead of the whole updated block, to PCM-based storage devices without extra hardware support. Moreover, it guarantees the sanity/integrity of file systems even if the computing system crashes or there is a power failure during the data update process. We implemented the proposed scheme on the Linux system and conducted a series of experiments to evaluate the scheme. The results are very encouraging. © 2016 ACM.",Asymmetry; Byte addressability; Energy consumption; File system; Journaling; Nonvolatility; Performance; Phase change memory; Reliability; Storage system,Computer operating systems; Digital storage; Energy efficiency; Energy utilization; File organization; Mobile computing; Reliability; Virtual storage; Asymmetry; Byte addressability; File systems; Journaling; Nonvolatility; Performance; Storage systems; Phase change memory
Accurate prediction of available battery time for mobile applications,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974652533&doi=10.1145%2f2875423&partnerID=40&md5=db876a61267a63f7f692416d1f2ab878,"Energy consumption in mobile devices is an important issue for both system developers and users. Users are aware of the battery-related information of their mobile devices and tend to take appropriate actions to increase the battery life. In this article, we propose a framework that accurately estimates the remaining battery time of applications at runtime. The framework profiles the power behavior of applications tied with activated hardware components and estimates the remaining battery budget utilizing the battery-related data provided by the device. The experiments validate that our method predicts the remaining battery time for applications with approximately 93% of accuracy. © 2016 ACM.",Energy constraints; Mobile devices; Predictability,Budget control; Energy utilization; Mobile devices; Accurate prediction; Battery life; Energy constraint; Hardware components; Mobile applications; Power behaviors; Predictability; System developers; Electric batteries
Model-based design of correct controllers for dynamically reconfigurable architectures,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974628509&doi=10.1145%2f2873056&partnerID=40&md5=48865efc40e6b6529622e77fd2a60ccd,"Dynamically reconfigurable hardware has been identified as a promising solution for the design of energy-efficient embedded systems. However, its adoption is limited by costly design effort, including verification and validation, which is even more complex than for nondynamically reconfigurable systems. In this article, we propose a tool-supported formal method to automatically design a correct-by-construction control of the reconfiguration. By representing system behaviors with automata, we exploit automated algorithms to synthesize controllers that safely enforce reconfiguration strategies formulated as properties to be satisfied by control. We design generic modeling patterns for a class of reconfigurable architectures, taking into account both hardware architecture and applications, as well as relevant control objectives. We validate our approach on two case studies implemented on FPGAs. © 2016 ACM.",Automata models; Dynamical partial reconfiguration,Automata theory; Computer hardware; Controllers; Embedded systems; Energy efficiency; Formal methods; Hardware; Reconfigurable architectures; Structural design; Automata models; Correct-by-construction; Dynamically reconfigurable architecture; Hardware architecture; Model- based designs; Partial reconfiguration; Reconfigurable systems; Verification-and-validation; Reconfigurable hardware
An efficient multidimensional big data fusion approach in machine-to-machine communication,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975263444&doi=10.1145%2f2834118&partnerID=40&md5=9ed5b9e439ea883028e00c5bc1cb8f70,"Machine-to-Machine communication (M2M) is nowadays increasingly becoming a world-wide network of interconnected devices uniquely addressable, via standard communication protocols. The prevalence ofM2M is bound to generate a massive volume of heterogeneous, multisource, dynamic, and sparse data, which leads a system towards major computational challenges, such as, analysis, aggregation, and storage. Moreover, a critical problem arises to extract the useful information in an efficient manner from the massive volume of data. Hence, to govern an adequate quality of the analysis, diverse and capacious data needs to be aggregated and fused. Therefore, it is imperative to enhance the computational efficiency for fusing and analyzing the massive volume of data. Therefore, to address these issues, this article proposes an efficient, multidimensional, big data analytical architecture based on the fusion model. The basic concept implicates the division ofmagnitudes (attributes), i.e., big datasets with complex magnitudes can be altered into smaller data subsets using five levels of the fusion model that can be easily processed by the Hadoop Processing Server, resulting in formalizing the problem of feature extraction applications using earth observatory system, social networking, or networking applications. Moreover, a four-layered network architecture is also proposed that fulfills the basic requirements of the analytical architecture. The feasibility and efficiency of the proposed algorithms used in the fusion model are implemented onHadoop single-node setup onUBUNTU 14.04 LTS core i5 machine with 3.2GHz processor and 4GB memory. The results show that the proposed system architecture efficiently extracts various features (such as land and sea) from the massive volume of satellite data. © 2016 ACM.",Big data; Data fusion; Hadoop processing server; M2M,Computational efficiency; Data fusion; Digital storage; Efficiency; Machine-to-machine communication; Memory architecture; Network architecture; Network layers; Quality control; Architecture-based; Computational challenges; Critical problems; Earth observatory; Networking applications; Satellite data; System architectures; World-wide networks; Big data
Correlation-aware probabilistic timing analysis for the dynamic segment of FlexRay,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974593627&doi=10.1145%2f2870635&partnerID=40&md5=03e5a0b104a0ac6169d1739a136de83e,"We propose an analytical framework for probabilistic timing analysis of the event-triggered Dynamic segment of the FlexRay communication protocol. Specifically, our framework computes the Deadline Miss Ratio of each message. The core problem is formulated as a Mixed Integer Linear Program (MILP). Given the intractability of the problem, we also propose several techniques that help to mitigate the running times of our tool. This includes the re-engineering of the problem to run it on GPUs as well as reformulating the MILP itself. Most importantly, we also show how our framework can handle correlations between the queuing events of messages. This is challenging because one cannot apply the convolution operator in the same way as in the case of independent queuing events. © 2016 ACM.",Automotive networks; Correlations; Deadline miss ratio; Dynamic segment; Flexray; Probabilistic analysis; Timing analysis,Program processors; Automotive networks; Correlations; Deadline miss ratio; Dynamic segments; Flexray; Probabilistic analysis; Timing Analysis; Integer programming
Hybrid Montgomery reduction,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974559415&doi=10.1145%2f2890502&partnerID=40&md5=81e5f2fd7a53ff7e7ddf9a66ac6a9c94,"In this article, we present a hybrid method to improve the performance of the Montgomery reduction by taking advantage of the Karatsuba technique. We divide the Montgomery reduction into two sub-parts, including one for the conventional Montgomery reduction and the other one for Karatsuba-aided multiplication. This approach reduces the multiplication complexity of n-limb Montgomery reduction from θ(n2 + n) to asymptotic complexity θ(7n2/8 + n). Our practical implementation results over an 8-bit microcontroller also show performance enhancements by 11%. © 2016 ACM.",Karatsuba multiplication; Montgomery reduction; Public key cryptography; Software implementation,Hardware; Software engineering; 8-bit microcontrollers; Asymptotic complexity; Hybrid method; Karatsuba multiplication; Montgomery reduction; Multiplication complexity; Performance enhancements; Software implementation; Public key cryptography
A collaborative energy-aware sensor management system using team theory,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974624218&doi=10.1145%2f2910574&partnerID=40&md5=b63b9fa4e6734cb3eb454ee7ceaa4bb0,"With limited battery supply, power is a scarce commodity in wireless sensor networks. Thus, to prolong the lifetime of the network, it is imperative that the sensor resources are managed effectively. This task is particularly challenging in heterogeneous sensor networks for which decisions and compromises regarding sensing strategies are to be made under time and resource constraints. In such networks, a sensor has to reason about its current state to take actions that are deemed appropriate with respect to its mission, its energy reserve, and the survivability of the overall network. Sensor Management controls and coordinates the use of the sensory suites in a manner that maximizes the success rate of the system in achieving its missions. This article focuses on formulating and developing an autonomous energy-aware sensor management system that strives to achieve network objectives while maximizing its lifetime. A team-theoretic formulation based on the Belief-Desire-Intention (BDI) model and the Joint Intention theory is proposed as a mechanism for effective and energy-aware collaborative decision-making. The proposed system models the collective behavior of the sensor nodes using the Joint Intention theory to enhance sensors' collaboration and success rate. Moreover, the BDI modeling of the sensor operation and reasoning allows a sensor node to adapt to the environment dynamics, situation-criticality level, and availability of its own resources. The simulation scenario selected in this work is the surveillance of the Waterloo International Airport. Various experiments are conducted to investigate the effect of varying the network size, number of threats, threat agility, environment dynamism, as well as tracking quality and energy consumption, on the performance of the proposed system. The experimental results demonstrate the merits of the proposed approach compared to the state-of-the-art centralized approach adapted from Atia et al. [2011] and the localized approach in Hilal and Basir [2015] in terms of energy consumption, adaptability, and network lifetime. The results show that the proposed approach has 12x less energy consumption than that of the popular centralized approach. © 2016 ACM.",Belief-desire-intention model; C.2.3 [network operations]: network management; Distributed reasoning; Energy-aware computing; Joint intention theory; Pervasive systems; Power-aware algorithms; Sensor management; Team-theoretic; Wireless networks; Wireless sensor network,Airport security; Computation theory; Decision making; Decision theory; Distributed computer systems; Energy management systems; Energy utilization; Human resource management; Intelligent agents; Network management; Power management; Power management (telecommunication); Sensor nodes; Wireless networks; Belief-desire-intention models; Distributed reasonings; Energy-aware computing; Joint intention theory; Network operations; Pervasive systems; Power-aware; Sensor management; Team-theoretic; Wireless sensor networks
Live-out register fencing: Interrupt-triggered soft error correction based on the elimination of register-to-register communication,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969945277&doi=10.1145%2f2873058&partnerID=40&md5=aab5828e0fda7ed382475a9d1c0e4fba,"This article introduces Live-Out Register Fencing (LoRF), a soft error correction mechanism that uses the novel Spill Register File as a container of checkpointing data. LoRF's Spill Register File holds the values shared among basic blocks in the program, and, coupled with a new compilation strategy, LoRF allows for error correction in the same basic block where the error was detected. In LoRF, error correction is triggered by a hardware interrupt that restores the registers of a basic block from the Spill Register File. After these registers are restored, the basic block where the error was detected can just be re-executed, thus reducing the costs of error recovery. LoRF's error correction policy eliminates the need for expensive architectural support for checkpointing and rollback, reducing the performance overhead of online soft error correction. LoRF relies on both a modified processor architecture and a corresponding compiler. The architecture was implemented in synthesizable VHDL, whereas the compiler was developed as an extension of the LLVM framework. Fault injection experiments support an error correction coverage of 99.35% and a mean performance overhead of 1.33 for the entire life cycle of an error from its occurrence to its elimination from the system. © 2016 ACM.",Checkpointing; Compiler; Error correction; Fault recovery; Hardening; Liveness; Register file; Soft error,Architecture; Cost reduction; Hardening; Life cycle; Memory architecture; Program compilers; Radiation hardening; Restoration; Check pointing; Compiler; Fault recovery; Liveness; Register files; Soft error; Error correction
Evaluation and improvements of runtime monitoring methods for real-time event streams,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971384319&doi=10.1145%2f2890503&partnerID=40&md5=24a08f7e142b4e51c569d2ec7509dfb3,"Runtime monitoring is of great importance as a safeguard to guarantee the correctness of system runtime behaviors. Two state-of-the-art methods, dynamic counters and l-repetitive function, were recently developed to tackle the runtime monitoring for real-time systems. While both are reported to be efficient in monitoring arbitrary events, the monitoring performance between them has not yet been evaluated. This article evaluates both methods in depth, to identify their strengths and weaknesses. New methods are proposed to efficiently monitor the many-to-one connections that are abstracted as AND and OR components on multiple inputs. Representative scenarios are used as our case studies to quantitatively demonstrate the evaluations. Both methods are implemented in hardware FPGA. The timing overhead and resource usages of implementing the two methods are evaluated. © 2016 ACM.",C.3 [special-purpose and application-based systems]: Real-time and embedded systems; Dynamic counters; Evaluation; Event stream model; Improvements; l-repetitive function; Runtime monitoring,Embedded systems; Function evaluation; Interactive computer systems; Monitoring; Network components; Evaluation; Event streams; Improvements; Real-time and embedded systems; Runtime Monitoring; Real time systems
SmartLMK: A memory reclamation scheme for improving user-perceived app launch time,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969962196&doi=10.1145%2f2894755&partnerID=40&md5=1ef167617a06adc33a4cd7283a91c0b7,"As the mobile computing environment evolves, users demand high-quality apps and better user experience. Consequently, memory demandinmobile devices has soared. Device manufacturers have fulfilled the demand by equipping devices with more RAM. However, such a hardware approach is only a temporary solution and does not scale well in the resource-constrained mobile environment. Meanwhile, mobile systems adopt a new app life cycle and a memory reclamation scheme tailored for the life cycle. When a user leaves an app, the app is not terminated but cached in memory as long as there is enough free memory. If the free memory gets low, a victim app is terminated and the associated memory to the app is reclaimed. This process-level approach has worked well in the mobile environment. However, user experience can be impaired severely because the victim selection policy does not consider the user experience. In this article, we propose a novel memory reclamation scheme called SmartLMK. SmartLMK minimizes the impact of the process-level reclamation on user experience. The worthiness to keep an app in memory is modeled by means of user-perceived app launch time and app usage statistics. The memory footprint and impending memory demand are estimated from the history of the memory usage. Using these values and memory models, SmartLMK picks up the least valuable apps and terminates them at once. Our evaluation on a real Android-based smartphone shows that SmartLMK efficiently distinguishes the valuable apps among cached apps and keeps those valuable apps in memory. As a result, the user-perceived app launch time can be improved by up to 13.2%. © 2016 ACM.",Android; Memory management; Mobile device; Smartphone,Life cycle; Mobile devices; Random access storage; Reclamation; Smartphones; Android; Associated memory; Memory management; Mobile computing environment; Mobile environments; Reclamation scheme; Selection policies; Usage statistics; Android (operating system)
A system-level modeling and design for cyber-physical-social systems,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969988986&doi=10.1145%2f2834119&partnerID=40&md5=13caa6475159934da4140e8c52dca20f,"The design of cyber-physical-social systems (CPSS) is a novel and challenging research field due that it emphasizes the deep fusion of cyberspace, physical space, and social space. In this article, we extend our previously proposed system-level design framework [Zeng et al. 2015] to tailor it to the needs of social scenario of multiple users. Ahierarchical Petri net-based model and social flow are presentedtoextend the controlflow and formally describe the social interactions of multiple users, respectively. By using the extended model, the system-level optimization for CPSS can be achieved by the improved design flow. Specifically, object emplacement and user satisfaction are further extended into the social environment. Also maximal power estimation algorithm is improved, leveraging the extended intermediate representation model. Finally, we use a smart office case to demonstrate the feasibility and effectiveness of our improved design approach for multiple users. © 2016 ACM 1539-9087/2016/05-ART35 $15.00.",Cyber-physical-social systems; IoT; Multi-objective optimization; Pervasive computing; System-level design,Multiobjective optimization; Petri nets; Ubiquitous computing; Intermediate representations; Power estimations; Social environment; Social interactions; Social systems; System level design; System level optimization; System-level modeling; Design
Editorial: Science of the big and small and embedded computing systems,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969926539&doi=10.1145%2f2901293&partnerID=40&md5=29e2bac56afe8af8bab68cb63490e9e0,[No abstract available],,
Usage-specific semantic integration for cyber-physical robot systems,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973884807&doi=10.1145%2f2873057&partnerID=40&md5=1b58cd50882b62e4b4a27963407a6b6a,"The multidisciplinary nature and time criticality of computing in Cyber-Physical Robot Systems (CPRS) makes it significantly different from traditional computer systems. This article attempts to create a usage-specific language called Cyber-Physical Robot Language (CPRL), which supports the CPRS design and implementation in an integrative and swift way. Multiview description and integration strategies as well as formal execution semantics for usage-specific simulation and verification are outlined. A graphic unified environment for CPRS modeling is supplied, in which several tools are integrated. A 6-DOF distributed robot system development in the environment is presented. The approach is an attempt to support CPRS design in an effective way, at the same time guaranteeing the system function and performance requirements. © 2016 ACM.",Cyber-physical robot systems; Formal semantics; Usage-specific integration,Computational linguistics; Formal methods; Integration; Robots; Semantics; Cyber physicals; Design and implementations; Distributed robot systems; Execution semantics; Formal Semantics; Integration strategy; Performance requirements; Traditional computers; Machine design
Vector coprocessor virtualization for simultaneous multithreading,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973887139&doi=10.1145%2f2898364&partnerID=40&md5=8969b3998e823c9ef6004987b5ba0818,"Vector coprocessors (VPs), commonly being assigned exclusively to a single thread/core, are not often performance and energy efficient due to mismatches with the vector needs of individual applications. We present in this article an easy-to-implement VP virtualization technique that, when applied, enables a multithreaded VP to simultaneously execute multiple threads of similar or arbitrary vector lengths to achieve improved aggregate utilization. With a vector register file (VRF) virtualization technique invented to dynamically allocate physical vector registers to threads, our VP virtualization approach improves programmer productivity by providing at runtime a distinct physical register name space to each competing thread, thus eliminating the need to solve register-name conflicts statically. We applied our virtualization technique to a multithreaded VP and prototyped an FPGA-based multicore processor system that supports VP sharing as well as power gating for better energy efficiency. Under the dynamic creation of disparate threads, our benchmarking results show impressive VP speedups of up to 333% and total energy savings of up to 37% with proper thread scheduling and power gating compared to a similar-sized system that allows VP access to just one thread at a time. © 2016 ACM.",Energy savings; FPGA prototyping; Multicore; Vector processor; Virtualization,Energy conservation; Field programmable gate arrays (FPGA); Multitasking; Program processors; Reconfigurable hardware; Vector spaces; Vectors; Virtual reality; Coprocessor virtualization; FPGA prototyping; Multi core; Programmer productivity; Simultaneous multi-threading; Vector processors; Virtualization Techniques; Virtualizations; Energy efficiency
Necessary and sufficient conditions for thermal schedulability of periodic real-time tasks under fluid scheduling model,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973442053&doi=10.1145%2f2883612&partnerID=40&md5=3b69f852ba6d25bf0e53012b1228ca08,"With the growing need to address the thermal issues in modern processing platforms, various performance throttling schemes have been proposed in literature (DVFS, clock gating, and so on) to manage temperature. In real-time systems, such methods are often unacceptable, as they can result in potentially catastrophic deadline misses. As a result, real-time scheduling research has recently focused on developing algorithms that meet the compute deadline while satisfying power and thermal constraints. Basic bounds that can determine if a set of tasks can be scheduled or not were established in the 1970s based on computation utilization. Similar results for thermal bounds have not been forthcoming. In this article, we address the problem of thermal constraint schedulability of tasks and derive necessary and sufficient conditions for thermal feasibility of periodic tasksets on a unicore system. We prove that a GPS-inspired fluid scheduling scheme is thermally optimal when context switch/preemption overhead is ignored. Extension of sufficient conditions to a nonfluid model is still an open problem. We also extend some of the results to a multicore processing environment. We demonstrate the efficacy of our results through extensive simulations. We also evaluate the proposed concepts on a hardware testbed. © 2016 ACM.",Algorithms; C.3.3 [Special-Purpose and Application-Based Systems]: Real-Time and Embedded Systems; Real-Time Systems; Schedulability analysis; Scheduling; Thermal Analysis; Thermal constraints,Algorithms; Embedded systems; Interactive computer systems; Scheduling; Thermoanalysis; Extensive simulations; Multi-core processing; Processing platform; Real - time scheduling; Real-time and embedded systems; Schedulability analysis; Scheduling schemes; Thermal constraints; Real time systems
Integrated exploration methodology for data interleaving and data-to-memory mapping on SIMD architectures,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973885010&doi=10.1145%2f2894754&partnerID=40&md5=f5bb391d9e4ee862d1292afa225f34bf,"This work presents a methodology for efficient exploration of data interleaving and data-to-memory mapping options for Single Instruction Multiple Data (SIMD) platform architectures. The system architecture consists of a reconfigurable clustered scratch-pad memory and a SIMD functional unit, which performs the same operation on multiple input data in parallel. The memory accesses contribute substantially to the overall energy consumption of an embedded system executing a data intensive task. The scope of this work is the reduction of the overall energy consumption by increasing the utilization of the functional units and decreasing the number of memory accesses. The presented methodology is tested using a number of benchmark applications with holes in their access scheme. Potential gains are calculated based on the energy models, both for the processing and the memory part of the system. The reduction in energy consumption after efficient interleaving and mapping of data is between 40% and 80% for the complete system and the studied benchmarks. © 2016 ACM.","Algorithms; B.7.1 [types and design styles]: memory technologies, design; B.8.2 [performance analysis and design aids]; C.3 [special purpose and application-based systems]: real-time and embedded systems; D.2.2 [design tools and techniques]: design, methodologies; Data interleaving; Data layout; Design; Design space exploration; E.1 [data structures]: arrays; Energy optimization; Memory reconfiguration; Performance; Single instruction multiple data (SIMD) architectures",Algorithms; Benchmarking; Design; Digital signal processing; Embedded systems; Energy utilization; Mapping; Real time systems; Reconfigurable architectures; Reconfigurable hardware; Data interleaving; Data layouts; Design space exploration; Design tools and techniques; E.1 [data structures]: arrays; Energy optimization; Memory technology; Performance; Performance analysis and design aids; Real-time and embedded systems; Single-instruction multiple-data architectures; Memory architecture
Parallelizing industrial hard real-time applications for the parMERASA multicore,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973867629&doi=10.1145%2f2910589&partnerID=40&md5=49580080b0792cb3364170cc0cb36128,"The EC project parMERASA (Multicore Execution of Parallelized Hard Real-Time Applications Supporting Analyzability) investigated timing-analyzable parallel hard real-time applications running on a predictable multicore processor. A pattern-supported parallelization approach was developed to ease sequential to parallel program transformation based on parallel design patterns that are timing analyzable. The parallelization approach was applied to parallelize the following industrial hard real-time programs: 3D path planning and stereo navigation algorithms (Honeywell International s.r.o.), control algorithm for a dynamic compaction machine (BAUER Maschinen GmbH), and a diesel engine management system (DENSO AUTOMOTIVE Deutschland GmbH). This article focuses on the parallelization approach, experiences during parallelization with the applications, and quantitative results reached by simulation, by static WCET analysis with the OTAWA tool, and by measurement-based WCET analysis with the RapiTime tool. © 2016 ACM.",Hard real-time; Parallelisation; Real-time system architecture; Timing-analyzable; Timing-predictable multicore; WCET analysis,Air navigation; Diesel engines; Interactive computer systems; Motion planning; Multicore programming; Hard real-time; Multi core; Parallelisation; Timing-analyzable; Wcet analysis; Real time systems
"Guest editorial: Challenges of embedded systems as they evolve into M2M, internet of things",2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964388617&doi=10.1145%2f2886417&partnerID=40&md5=8a3cf92c24209f55e9ecb797ba081f59,"With the rapid progress in information communication technology (ICT), multidisciplinary research fields such as Internet of Things (IoT), machine to machine (M2M), and embedded systems have been widely explored in recent years. As a result, a wide range of research opportunities have surfaced, with new challenges and research issues in IoT areas to be investigated and evaluated. This special issue on new technologies and research trends of embedded computing and systems for IoT and M2M provides high-quality contributions addressing related theoretical and practical aspects of associated technologies and their applications.",,Embedded systems; Internet; Embedded computing; Information communication technology; Internet of Things (IOT); Machine-to-machine (M2M); Multi-disciplinary research; Research issues; Research opportunities; Research trends; Internet of things
Identifying region-wide functions using urban taxicab trajectories,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964688917&doi=10.1145%2f2821507&partnerID=40&md5=46ac227ecf0d83eae79ed0a40c748a9d,"With the urban development and enlargement, various regions such as residential zones and administrative districts now appear as parts of cities. People exhibit different mobility patterns in each region, which is closely relevant to region-wide functions. In this article, we propose a scheme to discover region-wide functions using large-scale Shanghai taxicab trajectories that capture enormous traces for more than 13,000 taxicabs over a period of about 3 years. We investigate these taxicab trajectories and conduct an extensive preliminary study. Then, we divide the city into disjointed regions using Voronoi decomposition. By incorporating people's pick-up and drop-off information, we refine the Voronoi partitioning results to identify region-wide functional areas. Finally, we study people's movement frequency on weekdays and weekends for every kind of urban functional regions. We also look into human mobility within or across the identified urban functional regions. Experimental results show that human movement is bounded with the function of urban regions, and more than 90% of people visit neighboring (less than 20km travel distance) functional regions with high probability.",Region-wide functions; Social networks; Taxicab trajectories; Urban computing; Vehicular networks,Social networking (online); Social sciences computing; Trajectories; Urban growth; Functional areas; Mobility pattern; Movement frequencies; Urban computing; Urban development; Vehicular networks; Voronoi decomposition; Wide functions; Taxicabs
An integrated exploration and virtual platform framework for many-accelerator heterogeneous systems,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964691197&doi=10.1145%2f2866578&partnerID=40&md5=f3421f536d1d3ce4a4eef09423aa385b,"The recent advent ofmany-accelerator systems-on-chip (SoC), driven by the need formaximizing throughput and power efficiency, has led to an exponential increase in the hardware/software co-design complexity. The reason of this increase is that the designer has to explore a vast number of architectural parameter combinations for each single accelerator, as well as inter-accelerator configuration combinations under specific area, throughput, and power constraints, given that each accelerator has different computational requirements. In such a case, the design space size explodes. Thus, existing design space exploration (DSE) techniques give poor-quality solutions, as the design space cannot be adequately covered in a fair time. This problem is aggravated by the very long simulation time of the many-accelerator virtual platforms (VPs). This article addresses these design issues by (a) presenting a virtual prototyping solution that decreases the exploration time by enabling the evaluation of multiple configurations per VP simulation and (b) proposing a DSE methodology that efficiently explores the design space of many-accelerator systems. With the use of two fully developed use cases, namely an H.264 decoding server for multiple video streams and a parallelized denoising system for MRI scans, we show that the proposed DSE methodology either leads to Pareto points that dominate over those of a typical DSE scenario or finds new solutions that might not be found by the typical DSE. In addition, the proposed virtual prototyping solution leads to DSE runtime reduction reaching 10× for H.264 and 5× for Rician denoise. © 2016 ACM.",Analytical models; Design space exploration; Design space reduction; Many accelerator; MultiDynamic; Simulation; Systemc; Virtual platforms,Acceleration; Analytical models; Design; Embedded systems; Hardware-software codesign; Logic design; Programmable logic controllers; Space platforms; System-on-chip; Video streaming; Virtual addresses; Virtual prototyping; Design space exploration; Design spaces; MultiDynamic; Simulation; SystemC; Virtual platform; Integrated circuit design
UPDATE: User-profile-driven adaptive transfer for mobile devices,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960953789&doi=10.1145%2f2889489&partnerID=40&md5=a6f47faf124e67b817de6adb5dbb0ae9,"Existing channel-Aware scheduling work has mainly focused on scheduling in small timescales, that is, tens to hundreds of seconds.We propose to use long-Term user profiles to provide useful statistical information on future network conditions in large timescales. We design scheduling algorithms based on Markov decision theory. We collect and use a large set of real-life traces from the general public. Extensive trace-driven evaluations show that many real mobile users can benefit from our framework. In addition, we compare our framework against state-of-The-Art algorithms and observe significant performance differences because the existing algorithms were not designed for the large timescale scenario. © 2016 ACM.",Opportunistic transfers; resource conservation; scheduling; user profiling,Conservation; Decision theory; Mobile devices; Mobile telecommunication systems; Natural resources; Scheduling; Channelaware Scheduling; Future networks; Markov decision theory; Opportunistic transfers; Resource conservation; State-of-the-art algorithms; Statistical information; User profiling; Scheduling algorithms
Parallelizing the chambolle algorithm for performance-optimized mapping on FPGA Devices,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960962535&doi=10.1145%2f2851497&partnerID=40&md5=4d6bfb35162dbd73f1fae5d52575571b,"The performance and the efficiency of recent computing platforms have been deeply influenced by the widespread adoption of hardware accelerators, such as graphics processing units (GPUs) or fieldprogrammable gate arrays (FPGAs), which are often employed to support the tasks of general-purpose processors (GPPs). One of the main advantages of these accelerators over their sequential counterparts (GPPs) is their ability to perform massive parallel computation. However, to exploit this competitive edge, it is necessary to extract the parallelism from the target algorithm to be executed, which generally is a very challenging task. This concept is demonstrated, for instance, by the poor performance achieved on relevant multimedia algorithms, such as Chambolle, which is a well-known algorithm employed for the optical flow estimation. The implementations of this algorithm that can be found in the state of the art are generally based on GPUs but barely improve the performance that can be obtained with a powerful GPP. In this article, we propose a novel approach to extract the parallelism from computation-intensivemultimedia algorithms, which includes an analysis of their dependency schema and an assessment of their data reuse. We then perform a thorough analysis of the Chambolle algorithm, providing a formal proof of its inner data dependencies and locality properties. Then, we exploit the considerations drawn from this analysis by proposing an architectural template that takes advantage of the fine-grained parallelism of FPGA devices.Moreover, since the proposed template can be instantiated with different parameters, we also propose a design metric, the expansion rate, to help the designer in the estimation of the efficiency and performance of the different instances, making it possible to select the right one before the implementation phase. We finally show, by means of experimental results, how the proposed analysis and parallelization approach leads to the design of efficient and highperformance FPGA-based implementations that are orders of magnitude faster than the state-of-The-Art ones. © 2016 ACM.",Chambolle; custom hardware; field programmable gate arrays; optical flow; parallel architectures; TV-L<sup>1</sup>,Algorithms; Computer graphics; Efficiency; General purpose computers; Hardware; Integrated circuit design; Optical flows; Parallel architectures; Parallel flow; Program processors; Reconfigurable hardware; Chambolle; Custom hardwares; Efficiency and performance; Fine-grained parallelism; FPGA-based implementation; General-purpose processors; Graphics processing units; Optical flow estimation; Field programmable gate arrays (FPGA)
Bandwidth optimization and energy management in real-Time wireless networks,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960948988&doi=10.1145%2f2851498&partnerID=40&md5=5f97617c64bb1380bb429aeeaa62ff28,"In embedded systems operated by battery and interacting with the environment, a fundamental issue is the enforcement of real-Time and energy constraints to guarantee a desired lifetime with a given performance. A lot of research has focused on energy management at the communication level; however, not many authors considered both real-Time and energy requirements in wireless communication systems. This article proposes El-SMan, a power-Aware framework working in combination withMAC layer communication protocols for maximizing battery lifetime in wireless networks of embedded systems with real-Time constraints. Exploiting the flexibility in bandwidth requirements, El-SMan adapts stream parameters to balance performance versus energy consumption, taking both lifetime and message deadlines into account. © 2016 ACM.","adaptive resource management; Energymanagement; real-Time embedded systems,media access control",Access control; Adaptive control systems; Bandwidth; Electric batteries; Embedded systems; Energy management; Energy utilization; Medium access control; Power management; Power management (telecommunication); Wireless networks; Wireless telecommunication systems; Adaptive Resource Management; Bandwidth optimization; Bandwidth requirement; Communication levels; Energy requirements; Media access control; Real time constraints; Wireless communication system; Real time systems
Evaluating the design of a VLIW processor for real-Time systems,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960964834&doi=10.1145%2f2889490&partnerID=40&md5=9f5ba805df7f79fe57aa277f1e857ded,"Nowadays, many real-Time applications are very complex and as the complexity and the requirements of those systems become more demanding, more hardware processing capacity is necessary. Unfortunately, the correct functioning of real-Time systems depends not only on the logically correct response but also on the time when it is produced. General-purpose processor design fails to deliver analyzability due to their nondeterministic behavior caused by the use of cache memories, dynamic branch prediction, speculative execution, and out-of-order pipelines. In this article, we investigate the pipeline performance of Very Long Instruction Word (VLIW) architectures for real-Time systems with an in-order pipeline considering Worst-Case Execution Time (WCET) performance. Techniques on obtaining the WCET of VLIW machines are also considered and we make a quantification on how important are hardware techniques such as static branch prediction, predication, and pipeline speed of complex operations such as memory access and multiplication for high-performance real-Time systems. The memory hierarchy is out of the scope of this article and we used a classic deterministic structure formed by a direct mapped instruction cache and a data scratchpad memory. A VLIW prototype was implemented in VHDL from scratch considering the HP VLIW ST231 ISA. We also show some compiler insights and we use a representative subset of the Mälardalen's WCET benchmarks for validation and performance quantification. © 2016 ACM.",Real time systems; very long instruction word (VLIW) processor; worstcase execution time (WCET) analysis,Benchmarking; Cache memory; Digital signal processing; Embedded systems; General purpose computers; Hardware; Integrated circuit design; Interactive computer systems; Memory architecture; Pipeline processing systems; Pipelines; Program compilers; Reconfigurable hardware; Very long instruction word architecture; General purpose processors; Nondeterministic behavior; Processing capacities; Real-time application; Speculative execution; Very long instruction words; Worst-case execution time; Worst-case execution time analysis; Real time systems
Unified medium access control architecture for resource-constrained machine-to-machine devices,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964771601&doi=10.1145%2f2876958&partnerID=40&md5=7f63d90c2e3466babe88afbaadea9846,"In capillary machine-to-machine (M2M) communications, which is being considered as a feasible network solution for M2M applications, because of physical resource constraints and deployment conditions, an energy-efficient and scalable medium access control (MAC) protocol is crucial for numerous M2M devices to concurrently access wireless channels. Therefore, this paper presents a unified MAC layer architecture for resource-constrained M2M devices in capillary M2M networks [named as resource-constrained MAC architecture (RCMA)], which has a unified (monolithic) framework consisting of essential functional components to support MAC-related operations of M2M devices: multi-channel hybrid MAC (McHM), logical link control (LLC), time synchronizer (TS), and device on-off scheduler (DO2S). McHM provides a baseline MAC protocol for an entire capillary M2M system that combines the benefit of both contention-based carrier sense multiple access and schedule-based time division multiple access schemes, whereas the other three components help in the McHM operations. To demonstrate the effectiveness of the RCMA, we implement the whole stack using the QualNet simulator. Experimental results show that the RCMA outperforms the conventional ZigBee stack in terms of energy efficiency and scalability, even under heavy traffic conditions. © 2016 ACM.",Capillary machine-to-machine; Carrier sense multiple access; Machine-to-machine communications; Medium access control; Time division multiple access; Unified architecture,Access control; Automation; Carrier communication; Carrier sense multiple access; Energy efficiency; Network architecture; Time division multiple access; Functional components; Heavy traffic conditions; Machine to machines; Machine-to-machine communications; Medium access control protocols; Physical resources; Qualnet simulators; Unified architecture; Medium access control
A socioecological model for advanced service discovery in machine-to-machine communication networks,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964690938&doi=10.1145%2f2811264&partnerID=40&md5=808d67c03668e5d497265cb1aa4a2e53,"The new development of embedded systems has the potential to revolutionize our lives and will have a significant impact on future Internet of Thing (IoT) systems if required services can be automatically discovered and accessed at runtime in Machine-to-Machine (M2M) communication networks. It is a crucial task for devices to perform timely service discovery in a dynamic environment of IoTs. In this article, we propose a Socioecological Service Discovery (SESD) model for advanced service discovery in M2M communication networks. In the SESD network, each device can perform advanced service search to dynamically resolve complex enquires and autonomously support and co-operate with each other to quickly discover and self-configure any services available in M2M communication networks to deliver a real-time capability. The proposed model has been systematically evaluated and simulated in a dynamic M2M environment. The experiment results show that SESD can self-adapt and self-organize themselves in real time to generate higher flexibility and adaptability and achieve a better performance than the existing methods in terms of the number of discovered service and a better efficiency in terms of the number of discovered services per message.",Machine-to-machine communication networks; Service discovery; Social-ecological model,Embedded systems; Dynamic environments; Ecological modeling; Future internet; Machine-to-machine communications; Real time capability; Self-organize; Service discovery; Socio-ecological; Complex networks
Fast and precise worst-case interference placementfor shared cache analysis,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960962279&doi=10.1145%2f2854151&partnerID=40&md5=ebc5e459c77dab5410f25fc6d25e6d5a,"Real-Time systems require a safe and precise estimate of the worst-case execution time (WCET) of programs. In multicore architectures, the precision of a program's WCET estimate highly depends on the precision of its predicted shared cache behavior. Prediction of shared cache behavior is difficult due to the uncertain timing of interfering shared cache accesses made by programs running on other cores. Given the assignment of programs to cores, the worst-case interference placement (WCIP) technique tries to find the worst-case timing of interfering accesses, which would cause the maximum number of cache misses on the worst case path of the program, to determine its WCET. Although WCIP generates highly precise WCET estimates, the current ILP-based approach is also known to have very high analysis time. In this work, we investigate the WCIP problem in detail and determine its source of hardness. We show that performing WCIP is an NP-hard problem by reducing the 0-1 knapsack problem. We use this observation to make simplifying assumptions, which make theWCIP problem tractable, and we propose an approximate greedy technique for WCIP, whose time complexity is linear in the size of the program.We perform extensive experiments to show that the assumptions do not affect the precision of WCIP but result in significant reduction of analysis time. © 2016 ACM.",multicore real-Time systems; Shared cache analysis; worst-case execution time estimation,Buffer storage; Combinatorial optimization; Computational complexity; Interactive computer systems; Multicore programming; Software architecture; 0-1 knapsack problem; Analysis time; Multi core; Multicore architectures; Shared cache; Simplifying assumptions; Time complexity; Worst-case execution time; Real time systems
ASTROLABE: A rigorous approach for system-level performance modeling and analysis,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968807544&doi=10.1145%2f2885498&partnerID=40&md5=f36f898954241968c54d08210306711f,"Building abstract system-level models that faithfully capture performance and functional behavior for embedded systems design is challenging. Unlike functional aspects, performance details are rarely available during the early design phases, and no clear method is known to characterize them. Moreover, once such models are built, they are inherently complex as they mix software models, hardware constraints, and environment abstractions. Their analysis by using traditional performance evaluation methods is reaching the limit. In this article, we present a systematic approach for building stochastic abstract performance models using statistical inference and model calibration, and we propose statistical model checking as a scalable performance evaluation technique for them. © 2016 ACM.",Many-cores; Model-based design; Performance evaluation,Abstracting; Embedded software; Embedded systems; Stochastic models; Stochastic systems; Systems analysis; Hardware constraints; Many core; Model- based designs; Performance evaluation; Scalable performance evaluations; Statistical inference; Statistical model checking; System-level performance; Model checking
Near-Static Shading Exploration for Smart Photovoltaic Module Topologies Based on Snake-like Configurations,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968761382&doi=10.1145%2f2837026&partnerID=40&md5=f5130d49b631eea7bbc7ef5bf48c7261,"Variable shading due to clouds and nearby objects has a severe impact on the energy yield of photovoltaic installations. Due to the industry's standard of permanently series-connected cells in a photovoltaic (PV) module, partial shading creates mismatches between the Current-Voltage (I-V) characteristics of cells. This article proposes an alternative configurable intramodule cell interconnection topology whereby cell connections can be adapted during operation to allow an optimized power production. The proposed configurable topology outperforms significantly a conventional 10∗6module under heavy shade. Moreover, this is achieved in a quite flexible way and with negligible overhead under uniform irradiation conditions. © 2016 ACM.",configurable topology; Modelling; MPPT; performance; PV module; shading,Cells; Cytology; Maximum power point trackers; Models; Photovoltaic cells; Cell interconnection; Irradiation conditions; performance; Photovoltaic installation; Photovoltaic modules; Power production; PV modules; shading; Topology
Dynamic logging with dylog in networked embedded systems,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964903205&doi=10.1145%2f2807698&partnerID=40&md5=e16fd14d40c831e4d837a2e6ef773369,"Event logging is an important technique for networked embedded systems like wireless sensor networks. It can greatly help developers to understand complex system behaviors and diagnose program bugs. Existing logging facilities do not well satisfy three practical requirements: flexibility, efficiency, and high synchronization accuracy. To simultaneously satisfy these requirements, we present Dylog, a dynamic logging facility for networked embedded systems. Dylog employs several techniques. First, Dylog uses binary instrumentation for dynamically inserting or removing logging statements, enabling flexible and interactive debugging at runtime. Second, Dylog incorporates an efficient storage system and log collection protocol for recording and transferring the logging messages. Third, Dylog employs a lightweight data-driven approach for reconstructing the synchronized time of the logging messages. Dylog uses MAC-layer timestamping and drift compensation to achieve high synchronization accuracy. We implement Dylog on the TinyOS 2.1.1/TelosB platform. Results show the following: (1) Dylog incurs a small overhead. Indirections in Dylog incur an additional execution overhead of less than 1%. Dylog reduces the logging storage size by approximately 50% compared with the standard TinyOS radio printf library. Dylog reduces the patch size by more than 90%, compared with incremental reprogramming. (2) Dylog reduces the synchronization overhead by 78% in terms of transmission cost, compared with a traditional time synchronization protocol, FTSP, and it can achieve a high time synchronization accuracy of 5.4μs. (3) Dylog can help diagnose system problems effectively at the source-code level for three real-world scenarios. © 2015 ACM.",Diagnosis; Logging; Networked embedded systems,Complex networks; Diagnosis; Digital storage; Logging (forestry); Program debugging; Synchronization; Wireless sensor networks; Binary instrumentations; Data-driven approach; Interactive debugging; Networked embedded systems; Practical requirements; Real-world scenario; Time synchronization; Time synchronization protocols; Embedded systems
Attack-resilient sensor fusion for safety-critical cyber-physical systems,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964864953&doi=10.1145%2f2847418&partnerID=40&md5=db15aa3dc48e7bf5bd76b0a5ebb749c3,"This article focuses on the design of safe and attack-resilient Cyber-Physical Systems (CPS) equipped with multiple sensors measuring the same physical variable. A malicious attacker may be able to disrupt system performance through compromising a subset of these sensors. Consequently, we develop a precise and resilient sensor fusion algorithm that combines the data received from all sensors by taking into account their specified precisions. In particular, we note that in the presence of a shared bus, in which messages are broadcast to all nodes in the network, the attacker's impact depends on what sensors he has seen before sending the corrupted measurements. Therefore, we explore the effects of communication schedules on the performance of sensor fusion and provide theoretical and experimental results advocating for the use of the Ascending schedule, which orders sensor transmissions according to their precision starting from the most precise. In addition, to improve the accuracy of the sensor fusion algorithm, we consider the dynamics of the system in order to incorporate past measurements at the current time. Possible ways of mapping sensor measurement history are investigated in the article and are compared in terms of the confidence in the final output of the sensor fusion. We show that the precision of the algorithm using history is never worse than the no-history one, while the benefits may be significant. Furthermore, we utilize the complementary properties of the two methods and show that their combination results in a more precise and resilient algorithm. Finally, we validate our approach in simulation and experiments on a real unmanned ground robot. © 2016 ACM.",Cyber-physical systems security; Fault-tolerance; Fault-tolerant algorithms; Sensor fusion,Algorithms; Embedded systems; Fault tolerance; Network security; Complementary property; Cyber physical systems (CPSs); Cyber-physical systems (CPS); Fault tolerant algorithms; Multiple sensors; Physical variables; Sensor fusion; Sensor fusion algorithms; Sensor data fusion
RQNoC: A resilient quality-of-service network-on-chip with service redirection,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964758705&doi=10.1145%2f2846097&partnerID=40&md5=62a7734160606be8e7c60b0c4a44e11c,"In this article, we describe RQNoC, a service-oriented Network-on-Chip (NoC) resilient to permanent faults. We characterize the network resources based on the particular service that they support and, when faulty, bypass them, allowing the respective traffic class to be redirected. We propose two alternatives for service redirection, each having different advantages and disadvantages. The first one, Service Detour, uses longer alternative paths through resources of the same service to bypass faulty network parts, keeping traffic classes isolated. The second approach, Service Merge, uses resources of other services providing shorter paths but allowing traffic classes to interfere with each other. The remaining network resources that are common for all services employ additional mechanisms for tolerating faults. Links tolerate faults using additional spare wires combined with a flit-shifting mechanism, and the router control is protected with Triple-Modular-Redundancy (TMR). The proposed RQNoC network designs are implemented in 65nm technology and evaluated in terms of performance, area, power consumption, and fault tolerance. Service Detour requires 9% more area and consumes 7.3% more power compared to a baseline network, not tolerant to faults. Its packet latency and throughput is close to the fault-free performance at low-fault densities, but fault tolerance and performance drop substantially for 8 or more network faults. Service Merge requires 22% more area and 27% more power than the baseline and has a 9% slower clock. Compared to a faultfree network, a Service Merge RQNoC with up to 32 faults has increased packet latency up to 1.5 to 2.4× and reduced throughput to 70% or 50%. However, it delivers substantially better fault tolerance, having a mean network connectivity above 90% even with 32 network faults versus 41% of a Service Detour network. Combining Serve Merge and Service Detour improves fault tolerance, further sustaining a higher number of network faults and reduced packet latency. © 2016 ACM.",Fault tolerance; Network-on-chip; Quality-of-service,Distributed computer systems; Fault tolerance; Fault tolerant computer systems; Mergers and acquisitions; Network-on-chip; Quality of service; Routers; Servers; Telecommunication networks; VLSI circuits; 65-nm technologies; Alternative path; Baseline network; Network connectivity; Packet latencies; Permanent faults; Service oriented network; Triple modular redundancy; Telecommunication services
Editorial: Special issue on innovative design methods for smart embedded systems,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964389312&doi=10.1145%2f2885505&partnerID=40&md5=e5ef825e9a102807621fbd37e1cd346a,[No abstract available],,
User-centric scheduling and governing on mobile devices with big.LITTLE processors,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964831525&doi=10.1145%2f2829946&partnerID=40&md5=59b62f6c6c6fdecaa00791b15bc75aa1,"Mobile applications will become progressively more complicated and diverse. Heterogeneous computing architectures like big.LITTLE are a hardware solution that allows mobile devices to combine computing performance and energy efficiency. However, software solutions that conform to the paradigm of conventional fair scheduling and governing are not applicable to mobile systems, thereby degrading user experience or reducing energy efficiency. In this article, we exploit the concept of application sensitivity, which reflects the user's attention on each application, and devise a user-centric scheduler and governor that allocate computing resources to applications according to their sensitivity. Furthermore, we integrate our design into the Android operating system. The results of experiments conducted on a commercial big.LITTLE smartphone with real-world mobile apps demonstrate that the proposed design can achieve significant gains in energy efficiency while improving the quality of user experience. © 2016 ACM.",Big.LITTLE; DPM; DVFS; Energy efficiency; Mobile systems; Scheduling; User experience,Computer architecture; Hardware; Integrated circuit design; Mobile devices; Scheduling; Big.LITTLE; Computing performance; Computing resource; DVFS; Heterogeneous computing; Mobile applications; Mobile systems; User experience; Energy efficiency
Exploiting page correlations for write buffering in page-mapping multichannel SSDs,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964904154&doi=10.1145%2f2815622&partnerID=40&md5=523eb5a03af8fbe946719490d9af36e6,"Advanced solid-state disks (SSDs) have been equipped with page-mapping flash translation layers and multichannel architectures. The SSDs employ a RAM-based write buffer, which delays write requests for reducing write traffic, reorders requests for mitigating garbage-collection overhead, and produces parallel page writes for improving channel time utilization. This work presents a novel write buffer algorithm that exploits temporal and spatial correlations among buffer pages. The write-buffer groups temporally or spatially correlate buffer pages and then write the grouped buffer pages to the same flash block. In this way, when the correlated page data are updated in the future, flash blocks will receive bulk page invalidations and become good candidates for garbage collection. With multichannel architectures, the write buffer adaptively disperses read-most sequential data over channels for high page-level parallelism of sequential reads, while clustering write-most sequential data in the same channel for a reduced cost of garbage collection. We evaluated the proposed method and previously proposed buffer algorithms. Our method was shown to outperform the existing methods by up to 134%. We also implemented our buffer design on the OpenSSD platform; the time and space overheads of our design were reported to be very low. © 2016 ACM.",Flash memory; Multichannel; Solid-state disks; Write buffering,Cost reduction; Flash memory; Mapping; Memory architecture; Random access storage; Refuse collection; Flash translation layer; Garbage collection; Multichannel; Multichannel architecture; Sequential data; Solid state disks; Temporal and spatial correlation; Write buffering; Flash-based SSDs
On memory reuse between inputs and outputs of dataflow actors,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964780484&doi=10.1145%2f2871744&partnerID=40&md5=d7f463309b6655320dd9fb4f4d621411,"This article introduces a new technique to minimize the memory footprints of Digital Signal Processing (DSP) applications specified with Synchronous Dataflow (SDF) graphs and implemented on shared-memory Multiprocessor System-on-Chip (MPSoCs). In addition to the SDF specification, which captures data dependencies between coarse-grained tasks called actors, the proposed technique relies on two optional inputs abstracting the internal data dependencies of actors: Annotations of the ports of actors, and script-based specifications of merging opportunities between input and output buffers of actors. Experimental results on a set of applications show a reduction of the memory footprint by 48% compared to state-of-the-art minimization techniques. © 2016 ACM.",Buffer merging; Dataflow; Memory optimization,Application specific integrated circuits; Data flow analysis; Digital signal processing; Multiprocessing systems; Response time (computer systems); Signal processing; Specifications; System-on-chip; Based specification; Data dependencies; Dataflow; Digital signal processing (DSP) applications; Memory optimization; Minimization techniques; Shared memory multiprocessor systems; Synchronous Dataflow; Merging
Data flow transformation for energy-efficient implementation of givens rotation-based QRD,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964896553&doi=10.1145%2f2837025&partnerID=40&md5=e5c57be2cb4f4ba7a3872b7b284346c8,"QR decomposition (QRD), a matrix decomposition algorithm widely used in embedded application domain, can be realized in a large number of valid processing sequences that differ significantly in the number of memory accesses and computations, and hence the overall implementation energy. With modern low-power embedded processors evolving toward register files with wide memory interfaces and vector functional units (FUs), data flow in these algorithms needs to be carefully devised to efficiently utilize the costly wide memory accesses and the vector FUs. In this article, we present an energy-efficient data flow transformation strategy for the Givens rotation-based QRD. © 2016 ACM.",Data flow transformation; Energy optimization; Matrix decomposition; SIMD architectures,Algorithms; Data transfer; Linear transformations; Matrix algebra; Metadata; Optimization; Data-flow transformations; Embedded application; Embedded processors; Energy optimization; Functional units; Matrix decomposition; QR decomposition; SIMD architecture; Energy efficiency
Power-aware design techniques of secure multimode embedded systems,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964827875&doi=10.1145%2f2801152&partnerID=40&md5=eda828b9bb92574c34ca194939e36e26,"Nowadays, embedded systems have been widely usedin all types of application areas, some ofwhich belong to the safety and reliability critical domains. The functional correctness and design robustness of the embedded systems involved in such domains are crucial for the safety of personal/enterprise property or even human lives. Thereby, a holistic design procedure that considers all the important design concerns is essential. In this article, we approach embedded systems design from an integral perspective. We consider not only the classic real-time and quality of service requirements, but also the emerging security and power efficiency demands. Modern embedded systems are not any more developed for a fixed purpose, but instead designed for undertaking various processing requests. This leads to the concept of multimode embedded systems, in which the number and nature of active tasks change during runtime. Under dynamic situations, providing high performance along with various design concerns becomes a really difficult problem. Therefore, we propose a novel power-aware secure embedded systems design framework that efficiently solves the problem of runtime quality optimization with security and power constraints. The efficiency of our proposed techniques are evaluated in extensive experiments. © 2016 ACM.",Embedded systems; Multimode systems; Multiobjective optimization; Power-aware design; Security; System design and optimization,Design; Efficiency; Embedded software; Hardware security; Multiobjective optimization; Power management; Quality of service; Systems analysis; Design and optimization; Functional correctness; Modern embedded systems; Multi-mode embedded systems; Multimode system; Power-aware design; Quality optimization; Security; Embedded systems
Design of a high-performance cdma-based broadcast-free photonic multi-core network on chip,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964893617&doi=10.1145%2f2839301&partnerID=40&md5=c20da090db32a69798c8a4c5143c570a,"Present-day focus on multicore research has not only increased computing power but also power- and bandwidth-efficient communication among cores. On-chip communication networks have become popular today because of their low energy use and modular structure compared to bus-based interconnects. Silicon photonics has further boosted the performance of on-chip interconnection networks with its low energy-delay product and high reliability. In current multicore Network-on-Chip (NoC) architectures, photonics is playing an important role in transferring large volumes of data both on- and off-chip. The problem addressed in this work is the issue of broadcast traffic arising due to invalidation requests from on-chip cache memories. Although such traffic is typically less than 1% of total traffic, it can easily present a high load on network resources, creating congestion and degrading performance. In this article, we propose a CDMA-based, secure, scalable, and energy-efficient technique to eliminate broadcast invalidations and increase overall performance. Experimental results indicate a performance boost up to 22.2% over a competing Photonic NoC and up to 57.4% over Electrical Mesh-based NoC when the proposed technique is used. Additional hardware deployed has an area overhead of less than 1%, whereas total energy consumed is at par with other state-of-the-art techniques. © 2016 ACM.",Code division multiple access; Multicore architecture; Optical interconnect; Photonic networks on chip,Buffer storage; Cache memory; Code division multiple access; Computer architecture; Energy efficiency; Low power electronics; Network architecture; Network-on-chip; Photonics; Servers; Software architecture; Traffic congestion; VLSI circuits; Bandwidth Efficient Communications; Energy-efficient techniques; Multicore architectures; Network-on-chip architectures; On-chip communication networks; On-chip interconnection network; Optical interconnect; Photonic network; Integrated circuit design
Toward smart embedded systems: A self-aware system-on-chip (SoC) perspective,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964771626&doi=10.1145%2f2872936&partnerID=40&md5=d9f257946fdd3e5656c020ec08a7f803,"Embedded systems must address a multitude of potentially conflicting design constraints such as resiliency, energy, heat, cost, performance, security, etc., all in the face of highly dynamic operational behaviors and environmental conditions. By incorporating elements of intelligence, the hope is that the resulting ""smart"" embedded systems will function correctly and within desired constraints in spite of highly dynamic changes in the applications and the environment, as well as in the underlying software/hardware platforms. Since terms related to ""smartness"" (e.g., self-awareness, self-adaptivity, and autonomy) have been used loosely in many software and hardware computing contexts, we first present a taxonomy of ""self-x"" terms and use this taxonomy to relate major ""smart"" software and hardware computing efforts. A major attribute for smart embedded systems is the notion of self-awareness that enables an embedded system to monitor its own state and behavior, as well as the external environment, so as to adapt intelligently. Toward this end, we use a System-on-Chip perspective to show how the CyberPhysical System-on-Chip (CPSoC) exemplar platform achieves self-awareness through a combination of cross-layer sensing, actuation, self-aware adaptations, and online learning. We conclude with some thoughts on open challenges and research directions. © 2016 ACM.",Dynamic power management; Dynamic reliability management; Prediction; Reliability modeling; Resilience estimation; Resilience sensor; Variability,Application programs; Application specific integrated circuits; Forecasting; Hardware; Programmable logic controllers; System-on-chip; Taxonomies; Cyber physical systems (CPSs); Dynamic power management; Dynamic reliability managements; Environmental conditions; External environments; Reliability model; Software and hardwares; Variability; Embedded systems
A logic-based benders decomposition approach for mapping applications on heterogeneous multicore platforms,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964882973&doi=10.1145%2f2838733&partnerID=40&md5=c8ffaa018aecaaef95c62893b26aec45,"The development of efficient methods for mapping applications on heterogeneous multicore platforms is a key issue in the field of embedded systems. In this article, a novel approach based on the Logic-Based Benders decomposition principle is introduced for mapping complex applications on these platforms, aiming at optimizing their execution time. To provide optimal solutions for this problem in a short time, a new hybrid model that combines Integer Linear Programming (ILP) and Constraint Programming (CP) models is introduced. Also, to reduce the complexity of the model and its solution time, a set of novel techniques for generating additional constraints called Benders cuts is proposed. An extensive set of experiments has been performed in which synthetic applications described by Directed Acyclic Graphs (DAGs) were mapped to a number of heterogeneous multicore platforms. Moreover, experiments with DAGs that correspond to two real-life applications have also been performed. Based on the experimental results, it is proven that the proposed approach outperforms the pure ILP model in terms of the solution time and quality of the solution. Specifically, the proposed approach is able to find an optimal solution within a time limit of 2 hours in the vast majority of performed experiments, while the pure ILP model fails. Also, for the cases where both methods fail to find an optimal solution within the time limit, the solution of the proposed approach is systematically better than the solution of the ILP model. © 2016 ACM.",Benders decomposition; Constraint programming; Integer linear programming; Multicore embedded platforms; Task scheduling,Computer circuits; Computer programming; Constraint theory; Directed graphs; Embedded systems; Inductive logic programming (ILP); Integer programming; Mapping; Optimal systems; Reconfigurable hardware; Stochastic programming; Benders decomposition; Constraint programming; Embedded platforms; Integer Linear Programming; Task-scheduling; Multicore programming
Designing parameterizable hardware IPs in a model-based design environment for high-level synthesis,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964702277&doi=10.1145%2f2871737&partnerID=40&md5=a7ee9da10140b67b76b69c3fa824d4e2,"Model-based hardware design allows one to map a single model to multiple hardware and/or software architectures, essentially eliminating one of the major limitations of manual coding in C or RTL. Modelbased design for hardware implementation has traditionally offered a limited set of microarchitectures, which are typically suitable only for some application scenarios. In this article we illustrate how digital signal processing (DSP) algorithms can be modeled as flexible intellectual property blocks to be used within the popular Simulink model-based design environment. These blocks are written in C and are designed for both functional simulation and hardware implementation, including architectural design space exploration and hardware implementation through high-level synthesis. A key advantage of our modeling approach is that the very same bit-accurate model is used for simulation and high-level synthesis. To prove the feasibility of our proposed approach, we modeled a fast Fourier transform (FFT) algorithm and synthesized it for different DSP applications with very different performance and cost requirements.We also implemented a high-levelsynthesis (HLS) intellectual property (IP) generator that can generate flexible FFT HLS-IP blocks that can be mapped to multiple micro-/macroarchitectures, to enable design space exploration as well as being used for functional simulation in the Simulink environment. © 2016 ACM.",Audio detector; C/C++ hardware IP description; Design reuse; FFT; GPS acquisition; IP generator; Model-based design; Model-based high-level synthesis; Parameterized IPs; Simulink modeling,C (programming language); Design; Digital signal processing; Fast Fourier transforms; Hardware; Intellectual property; Reconfigurable hardware; Signal processing; Audio detectors; Design reuse; Hardware IP; IP generator; Model- based designs; Model-based OPC; Parameterized; Simulink modeling; High level synthesis
Stable greedy: Adaptive garbage collection for durable page-mapping multichannel SSDs,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964825915&doi=10.1145%2f2820613&partnerID=40&md5=d03e9dd33256a30722f9dbfe6862b3e0,"Commodity solid state drives (SSDs) have recently begun involving the adoption of powerful controllers for multichannelflash managementat the page level. However, many of these models still use primitive garbage-collection algorithms, because previous approaches are subject to poor scalability with high-capacity flash memory. This study presents Stable Greedy for garbage collection in page-mapping multichannel SSDs. Stable Greedy identifies page-accurate data hotness using block-level information, and jointly considers block space utilization and block stability for victim selection. Its design considers flash wear leveling for SSD lifetime enhancement at the block level as well as at the channel level. Stable Greedy runs at a constant time, and requires limited RAM space. The simulation results revealed that Stable Greedy outperformed previous methods considerably under various workloads and multichannel architectures. Stable Greedy was successfully implemented on the OpenSSD platform, and the actual performance measurements were consistent with the simulation results. © 2016 ACM.",Flash translation layers; Garbage collection; Solid state disks,Digital storage; Flash memory; Mapping; Random access storage; Refuse collection; Flash translation layer; Garbage collection; Garbage collection algorithm; Lifetime enhancement; Multichannel architecture; Performance measurements; Solid state disks; Solid state drives; Flash-based SSDs
Feasibility of fork-join real-time task graph models: Hardness and algorithms,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964869778&doi=10.1145%2f2809780&partnerID=40&md5=55a627d426b5576bea42f1516b947366,"In the formal analysis of real-time systems, modeling of branching codes and modeling of intratask parallelism structures are two of the most important research topics. These two real-time properties are combined, resulting in the fork-join real-time task (FJRT) model, which extends the digraph-based task model with forking and joining semantics. We prove that the EDF schedulability problem on a preemptive uniprocessor for the FJRT model is coNP-hard in the strong sense, even if the utilization of the task system is bounded by a constant strictly less than 1. Then, we show that the problem becomes tractable with some slight structural restrictions on parallel sections, for which we propose an exact schedulability test with pseudo-polynomial time complexity. Our results thus establish a borderline between the tractable and intractable FJRT models. © 2016 ACM.",Digraph-based task; EDF-schedulability; Fork-join; Tractability,Algorithms; Directed graphs; Graph theory; Interactive computer systems; Polynomial approximation; Semantics; Digraph-based task; Pseudo-polynomial time complexity; Real-time properties; Real-time tasks; Schedulability; Schedulability test; Structural restrictions; Tractability; Real time systems
CUDA leaks: A detailed hack for CUDA and a (Partial) fix,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964816013&doi=10.1145%2f2801153&partnerID=40&md5=ae7d416b726518868dea53afe51b0201,"Graphics processing units (GPUs) are increasingly common on desktops, servers, and embedded platforms. In this article, we report on new security issues related to CUDA, which is the most widespread platform for GPU computing. In particular, details and proofs-of-concept are provided about novel vulnerabilities to which CUDA architectures are subject. We show how such vulnerabilities can be exploited to cause severe information leakage. As a case study, we experimentally show how to exploit one of these vulnerabilities on a GPU implementation of the AES encryption algorithm. Finally, we also suggest software patches and alternative approaches to tackle the presented vulnerabilities. © 2016 ACM.",GPGPU; GPU; Information leakage; Registers,Computer graphics; Program processors; Embedded platforms; GPGPU; GPU implementation; Graphics processing units; Information leakage; Registers; Security issues; Software patches; Cryptography
Software-based selective validation techniques for Robust CGRAs against soft errors,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964893862&doi=10.1145%2f2843943&partnerID=40&md5=e17e915307302abb85f5acb81fb6b687,"Coarse-Grained Reconfigurable Architectures (CGRAs) are drawing significant attention since they promise both performances with parallelism and flexibility with reconfiguration. Soft errors (or transient faults) are becoming a serious design concern in embedded systems including CGRAs since the soft error rate is increasing exponentially as technology is scaling. A recently proposed software-based technique with TMR (Triple Modular Redundancy) implemented on CGRAs incurs extreme overheads in terms of runtime and energy consumption mainly due to expensive voting mechanisms for the outputs from the triplication ofevery operation. In this article, we propose selective validation mechanisms for efficient modular redundancy techniques in the datapaths on CGRAs. Our techniques selectively validate the results at synchronous operations rather than every operation in order to reduce the expensive performance overhead from the validation mechanism. We also present an optimization technique to further improve the runtime and the energy consumption by minimizing synchronous operations where a validating mechanism needs to be applied. Our experimental results demonstrate that our selective validation-based TMR technique with our optimization on CGRAs can improve the runtime by 41.0% and the energy consumption by 26.2% on average over benchmarks as compared to the recently proposed software-based TMR technique with the full validation. © 2016 ACM.",CGRA; DMR; Reconfigurable architecture; Selective validation; Soft error; TMR,Embedded systems; Energy utilization; Error correction; Errors; Fault tolerant computer systems; Radiation hardening; Reconfigurable hardware; Redundancy; CGRA; Coarse grained reconfigurable architecture (CGRAs); Optimization techniques; Selective validation; Soft error; Software-based techniques; Synchronous operations; Triple modular redundancy; Reconfigurable architectures
Structure design of wireless sensor nodes with energy and cost awareness for multichannel signal measurement,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964862103&doi=10.1145%2f2790300&partnerID=40&md5=bd2aa79357c011360206fbffc7e8b04c,"This article aims to develop a design pattern of a wireless sensor node working in multichannel signal measurement for effectively lowering energy consumption and cost. The proposed design pattern enables the architecture of a wireless sensor node to adapt to application requirements, thus to significantly reduce system redundancy. Two multisensor structures are parameterized regarding frequency response, power consumption, and cost. The system design pattern provides flexibility through three proposed interface circuits that bridge between multisensor structures and the microprocessors inside sensor nodes. It also allows adjusting time the delay parameter that can enlarge the selection range of main electronic components, and thereby increases the robustness of the model for practical implementations. A virtual case study is provided to demonstrate how to apply this model into an application design. © 2016 ACM.",Multichannel signal measurement; Node architecture; Node cost; Power consumption; System redundance; Wireless sensor nodes,Computer architecture; Costs; Design; Electric power utilization; Energy utilization; Frequency response; Sensor nodes; Application requirements; Electronic component; Energy consumption and cost; Interface circuits; Multichannel signals; Node architectures; System redundance; Wireless sensor node; Structural design
Energy-aware scheduling for real-time systems: A survey,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964823054&doi=10.1145%2f2808231&partnerID=40&md5=15a746cf2a085805101b0a092c56c9e4,"This article presents a survey of energy-aware scheduling algorithms proposed for real-time systems. The analysis presents the main results starting from the middle 1990s until today, showing how the proposed solutions evolved to address the evolution of the platform's features and needs. The survey first presents a taxonomy to classify the existing approaches for uniprocessor systems, distinguishing them according to the technology exploited for reducing energy consumption, that is, Dynamic Voltage and Frequency Scaling (DVFS), Dynamic Power Management (DPM), or both. Then, the survey discusses the approaches proposed in the literature to deal with the additional problems related to the evolution of computing platforms toward multicore architectures. © 2016 ACM.",Dynamic power management; Dynamic voltage and frequency scaling; Energy; Idle; Low power; Multicore; Power; Real-time scheduling; Single core; Sleep,Algorithms; Computer architecture; Dynamic frequency scaling; Energy management; Energy utilization; Interactive computer systems; Power management; Scheduling; Scheduling algorithms; Software architecture; Surveys; Voltage scaling; Dynamic power management; Dynamic voltage and frequency scaling; Energy; Idle; Low Power; Multi core; Power; Real - time scheduling; Single core; Sleep; Real time systems
Beyond cross-section: Spatio-temporal reliability analysis,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964801046&doi=10.1145%2f2794148&partnerID=40&md5=0d584761282feccc5f0c98cc689b3549,"A computational system employed in safety-critical applications typically has reliability as a primary concern. Thus, the designer focuses on minimizing the device radiation-sensitive area, often leading to performance degradation. In this article, we present a mathematical model to evaluate system reliability in spatial (i.e., radiation-sensitive area) and temporal (i.e., performance) terms and prove that minimizing radiation-sensitive area does not necessarily maximize application reliability. To support our claim, we present an empirical counterexample where application reliability is improved even if the radiation-sensitive area of the device is increased. An extensive radiation test campaign using a 28nm commercial-off-the-shelf ARM-based SoC was conducted, and experimental results demonstrate that, while executing the considered application at military aircraft altitude, the probability of executing a two-year mission workload without failures is increased by 5.85% if L1 caches are enabled (thus increasing the radiation-sensitive area) when compared to no cache level being enabled. However, if both L1 and L2 caches are enabled, the probability is decreased by 31.59%. © 2015 ACM.",Cache memories; Embedded systems; Performance; Reliability,Buffer storage; Cache memory; Embedded systems; Radiation; Reliability; System-on-chip; Application reliabilities; Commercial off the shelves; Computational system; Performance; Performance degradation; Safety critical applications; Spatio temporal; System reliability; Reliability analysis
A cache-based flash translation layer for TLC-based multimedia storage devices,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964862743&doi=10.1145%2f2820614&partnerID=40&md5=295d0307f0d325ca063a33e4ea0e4bab,"Current triple-level cell (TLC)-based solids-tate drives used in multimedia storage devices support multichannel access to increase capacity and throughput. Unfortunately, current state-of-the-art FTL algorithms must employ selective caching for inquiring about the address mapping information, which causes low space utilization, a large flash memory requirement, and performance degradation. In this article, the Cache-based Flash Translation Layer (Cab-FTL) is proposed for TLC-based multimedia storage devices. Cab-FTL enhances the read and write performances by achieving high space utilization while reducing the size of the mapping tables to 1.68% compared to DFTL. Despite a caching of the mapping tables in DRAM, Cab-FTL achieves a fast system boot using its fast wake-up mechanism. © 2016 ACM.",Firmware; Flash memory; Solid-state drive,Dynamic random access storage; Firmware; Mapping; Virtual storage; Address mappings; Flash translation layer; High space utilizations; Memory requirements; Multimedia storage; Performance degradation; Solid state drives; Space utilization; Flash memory
Theory and application of delay constraints in Arbiter PUF,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964811420&doi=10.1145%2f2815621&partnerID=40&md5=741f69ded9f4ff7146f4262aabe007f1,"Physically Unclonable Function (PUF) circuits are often vulnerable to mathematical model-building attacks. We theoretically quantify the advantage provided to an adversary by any training dataset expansion technique along the lines of security analysis of cryptographic hash functions. We present an algorithm to enumerate certain sets of delay constraints for the widely studied Arbiter PUF (APUF) circuit, then demonstrate how these delay constraints can be utilized to expand the set of known Challenge-Response Pairs (CRPs), thus facilitating model-building attacks. We provide experimental results for Field Programmable Gate Array (FPGA)-based APUF to establish the effectiveness of the proposed attack. © 2016 ACM.",Arbiter PUF; Delay constraints; Hardware-intrinsic security; Security analysis,Cryptography; Delay circuits; Field programmable gate arrays (FPGA); Functions; Hash functions; Model buildings; Reconfigurable hardware; Security systems; Arbiter PUF; Challenge-response pair; Cryptographic hash functions; Delay constraints; Physically unclonable functions; Security analysis; Training dataset; Asynchronous sequential logic
A real-time FPGA-based accelerator for ECG analysis and diagnosis using association-rule mining,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964780712&doi=10.1145%2f2821508&partnerID=40&md5=e629d275a63833f41f285f551ebe3d0b,"Telemedicine provides health care services at a distance using information and communication technologies, which intends to be a solution to the challenges faced by current health care systems with growing numbers of population, increased demands from patients, and shortages in human resources. Recent advances in telemedicine, especially in wearable electrocardiogram (ECG) monitors, call for more intelligent and efficient automatic ECG analysis and diagnostic systems. We present a streaming architecture implemented on Field-Programmable Gate Arrays (FPGAS) to accelerate real-time ECG signal analysis and diagnosis in a pipelining and parallel way. Association-rule mining is employed to generate early diagnostic results by matching features of ECG with generated association rules. To improve performance of the processing, we propose a hardware-oriented data-mining algorithm named Bit-Q-Apriori. The corresponding hardware implementation indicates a good scalability and outperforms other hardware designs in terms of performance, throughput, and hardware cost. © 2016 ACM.",Association-rule mining; ECG; FPGA; Frequent itemset mining,Algorithms; Association rules; Data mining; Diagnosis; Electrocardiography; Hardware; Health care; Reconfigurable hardware; Telemedicine; Data mining algorithm; Diagnostic systems; Frequent itemset mining; Hardware implementations; Healthcare services; Improve performance; Information and Communication Technologies; Streaming architecture; Field programmable gate arrays (FPGA)
Real-time reachability for verified simplex design,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964788955&doi=10.1145%2f2723871&partnerID=40&md5=574271430b840d4bca5d485ac3affe1f,"The Simplex architecture ensures the safe use of an unverifiable complex/smart controller by using it in conjunction with a verified safety controller and verified supervisory controller (switching logic). This architecture enables the safe use of smart, high-performance, untrusted, and complex control algorithms to enable autonomy without requiring the smart controllers to be formally verified or certified. Simplex incorporates a supervisory controller that will take over control from the unverified complex/smart controller if it misbehaves and use a safety controller. The supervisory controller should (1) guarantee that the system never enters an unsafe state (safety), but should also (2) use the complex/smart controller asmuch as possible (minimize conservatism). The problem of precisely and correctly defining the switching logic of the supervisory controller has previously been considered either using a control-theoretic optimization approach or through an offline hybrid-systems reachability computation. In this work, we show that a combined online/offline approach that uses aspects of the two earlier methods, along with a real-time reachability computation, also maintains safety, but with significantly less conservatism, allowing the complex controller to be used more frequently.We demonstrate the advantages of this unified approach on a saturated inverted pendulum system, inwhich the verifiable region of attraction is over twice as large compared to the earlier approach. Additionally, to validate the claims that the real-time reachability approach may be implemented on embedded platforms, we have ported and conducted embedded hardware studies using both ARM processors and Atmel AVR microcontrollers. This is the first ever demonstration of a hybrid-systems reachability computation in real time on actual embedded platforms, which required addressing significant technical challenges. © 2016 ACM.",Cyber-physical systems; Formal verification; Hybrid systems,Algorithms; Computation theory; Computer circuits; Control theory; Embedded systems; Formal verification; Hybrid systems; Pendulums; Real time systems; Reconfigurable hardware; Atmel avr microcontrollers; Complex control algorithms; Cyber physical systems (CPSs); Inverted pendulum system; Optimization approach; Region of attraction; Supervisory controllers; Technical challenges; Controllers
Cross-layer opportunistic scheduling for device-to-device video multicast services,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964746092&doi=10.1145%2f2856034&partnerID=40&md5=dceb6a76a3c454594388879359751f64,"In this article, we address the problem of how to make the wireless device-to-device (D2D) video multicast systems have better quality provision with consideration of internet-of-things (IoT) applications.We propose an opportunistic transmission and fair resource allocation framework, including joint application-layer and physical-layer transmission and optimization. First, we use a parallel subchannels structure by concatenating the Fountain codes and diversity-embedded space-time block codes to provide reliable and flexible transmission in heterogeneous circumstances. Second, we exploit the quality of heterogeneous user experience (quality of experience) metric under D2D video multicast systems, with consideration of various channel states, device capability, video content urgency, and the number of demanding users. Third, we formulate reliable multiple video streams broadcasting to heterogeneous devices as an aggregate maximum utility achieving problem, and we use opportunistic scheduling to select suitable users in each transmission interval to improve the broadcasting utility. Fourth, we use the utility fair scheme to guide rate allocation among multicontent video multicast. Extensive performance comparison and analysis are presented to demonstrate efficiency of the proposed solution. © 2016 ACM.",Device-to-device; Scheduling; Video multicast,Block codes; Multicasting; Network layers; Quality of service; Scheduling; Space-time block coding (STBC); Video streaming; Device-to-device; Fair resource allocation; Internet of Things (IOT); Opportunistic scheduling; Opportunistic transmission; Quality of experience (QoE); Transmission intervals; Video multicast; Space time codes
Error detector placement for soft computing applications,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964805531&doi=10.1145%2f2801154&partnerID=40&md5=84f3c24bd78be1ed57a8867539790cb4,"The scaling of Silicon devices has exacerbated the unreliability of modern computer systems, and power constraints have necessitated the involvement of software in hardware error detection. At the same time, emerging workloads in the form of soft computing applications (e.g., multimedia applications) can tolerate most hardware errors as long as the erroneous outputs do not deviate significantly from error-free outcomes. We term outcomes that deviate significantly from the error-free outcomes as Egregious Data Corruptions (EDCs). In this study, we propose a technique to place detectors for selectively detecting EDC-causing errors in an application. We performed an initial study to formulate heuristics that identify EDC-causing data. Based on these heuristics, we developed an algorithm that identifies program locations for placing high coverage detectors for EDCs using static analysis. Our technique achieves an average EDC coverage of 82%, under performance overheads of 10%, while detecting 10% of the Non-EDC and benign faults. We also evaluate the error resilience of these applications under the 14 compiler optimizations. © 2016 ACM.",Detector placement; EDCs; Hardware fault detection; Static analysis,Errors; Fault detection; Hardware; Program compilers; Reconfigurable hardware; Soft computing; Static analysis; Compiler optimizations; Detector placement; EDCs; Hardware faults; Modern computer systems; Multimedia applications; Power constraints; Soft computing applications; Computer hardware
Preaveraging and carry propagate approaches to side-channel analysis of HMAC-SHA256,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964915989&doi=10.1145%2f2794093&partnerID=40&md5=fcd3d4756157b192eff4683f6ceea15f,"Although HMAC-SHA has been standardized for over a decade, few published attacks on the single-cycle round implementation exist. In this research, new attack techniques are provided, for the first time, (1) to help to discriminate between values of secret intermediate variables within HMAC and (2) to reduce the large word size complexity. Preaveraging and carry propagate techniques are proposed using chosen plaintexts and shown to significantly reduce the complexity and runtimes for side-channel analysis of an Altera FPGA platform. This research is important for advancing side channel analysis of complex embedded ASICs and ensuring secure implementations in future embedded ubiquitous devices. © 2016 ACM.",Cache attack; EM attack; Side channel analysis,Field programmable gate arrays (FPGA); Advancing side; Cache attack; Chosen plaintexts; EM attack; Secure implementation; Side-channel analysis; Size complexity; Ubiquitous devices; Side channel attack
Design and implementation of warbler family of lightweight pseudorandom number generators for smart devices,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964909369&doi=10.1145%2f2808230&partnerID=40&md5=02f251ac86fb4c7857cd68899c83544c,"With the advent of ubiquitous computing and the Internet of Things (IoT), the security and privacy issues for various smart devices such as radio-frequency identification (RFID) tags and wireless sensor nodes are receiving increased attention from academia and industry. A number of lightweight cryptographic primitives have been proposed to provide security services for resource-constrained smart devices. As one of the core primitives, a cryptographically secure pseudorandom number generator (PRNG) plays an important role for lightweight embedded applications. The most existing PRNGs proposed for smart devices employ true random number generators as a component, which generally incur significant power consumption and gate count in hardware. In this article, we present Warbler family, a new pseudorandom number generator family based on nonlinear feedback shift registers (NLFSRs) with desirable randomness properties. The design of the Warbler family is based on the combination of modified de Bruijn blocks together with a nonlinear feedback Welch-Gong (WG) sequence generator, which enables us to precisely characterize the randomness properties and to flexibly adjust the security level of the resulting PRNG. Some criteria for selecting parameters of the Warbler family are proposed to offer the maximum level of security. Two instances of the Warbler family are also described, which feature two different security levels and are dedicated to EPC C1 Gen2 RFID tags and wireless sensor nodes, respectively. The security analysis shows that the proposed instances not only can pass the cryptographic statistical tests recommended by the EPC C1 Gen2 standard and NIST but also are resistant to the cryptanalytic attacks such as algebraic attacks, cube attacks, time-memory-data tradeoff attacks, Mihaljević et al.'s attacks, and weak internal state and fault injection attacks. Our ASIC implementations using a 65nm CMOS process demonstrate that the proposed two lightweight instances of the Warbler family can achieve good performance in terms of speed and area and provide ideal solutions for securing low-cost smart devices. © 2016 ACM.",Hardware implementation; NLFSRs; PRNG; RFID; Security; Stream ciphers,CMOS integrated circuits; Computer hardware description languages; Cryptography; Hardware; Integrated circuit design; Network security; Nonlinear feedback; Number theory; Radio frequency identification (RFID); Random processes; Reconfigurable hardware; Sensor nodes; Shift registers; Ubiquitous computing; Hardware implementations; NLFSRs; PRNG; Security; Stream Ciphers; Random number generation
Contract-based requirement modularization via synthesis of correct decompositions,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964863086&doi=10.1145%2f2885752&partnerID=40&md5=526a5d0af9744d5612f803c16700b283,"In distributed development of modern systems, contracts play a vital role in ensuring interoperability of components and adherence to specifications. It is therefore often desirable to verify the satisfaction of an overall property represented as a contract, given the satisfaction of smaller properties also represented as contracts. When the verification result is negative, designers must face the issue of refining the subproperties and components. This is an instance of the classical synthesis problems: ""can we construct a model that satisfies some given specification?"" In this work, we propose two strategies enabling designers to synthesize or refine a set of contracts so that their composition satisfies a given contract.We develop a generic algebraic method and show how it can be applied in different contract models to support top-down component-based development of distributed systems. © 2016 ACM.",Contract; Decomposition; Distributed systems; Requirement; Synthesis,Algebra; Contracts; Decomposition; Modular construction; Specifications; Synthesis (chemical); Algebraic method; Component-Based Development; Distributed development; Distributed systems; Overall properties; Requirement; Synthesis problems; Verification results; Interoperability
TBES: Template-based exploration and synthesis of heterogeneous multiprocessor architectures on FPGA,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964868775&doi=10.1145%2f2816817&partnerID=40&md5=d37887e1728ba03ee81dfed6e46487b6,"This article describes TBES, a software end-to-end environment for synthesizing multitask applications on FPGAs. The implementation follows a template-based approach for creating heterogeneous multiprocessor architectures. Heterogeneity stems from the use of general-purpose processors along with custom accelerators. Experimental results demonstrate substantial speedup for several classes of applications. Furthermore, this work allows for reducing development costs and saving development time for the software architect, the domain expert, and the optimization expert. This work provides a framework to bring together various existing tools and optimisation algorithms. The advantages are manifold: modularity and flexibility, easy customization for best-fit algorithm selection, durability and evolution over time, and legacy preservation including domain experts' know-how. In addition to the use of architecture templates for the overall system, a second contribution lies in using high-level synthesis for promoting exploration of hardware IPs. The domain expert, who best knows which tasks are good candidates for hardware implementation, selects parts of the initial application to be potentially synthesized as dedicated accelerators. As a consequence, the HLS general problem turns into a constrained and more tractable issue, and automation capabilities eliminate the need for tedious and error-prone manual processes during domain space exploration. The automation only takes place once the application has been broken down into concurrent tasks by the designer, who can then drive the synthesis process with a set of parameters provided by TBES to balance tradeoffs between optimization efforts and quality of results. The approach is demonstrated step by step up to FPGA implementations and executions with an MJPEG benchmark and a complex Viola-Jones face detection application. We show that TBES allows one to achieve results with up to 10 times speedup to reduce development times and to widen design space exploration. © 2016 ACM.",Electronic system level; High-level synthesis; Multiprocessor; System-on-chip,Algorithms; Application programs; Application specific integrated circuits; Benchmarking; Face recognition; Field programmable gate arrays (FPGA); General purpose computers; Hardware; Multiprocessing systems; Optimization; Reconfigurable hardware; Software architecture; Space research; System theory; System-on-chip; Systems analysis; Technology transfer; Design space exploration; Electronic system level; FPGA implementations; General purpose processors; Hardware implementations; Heterogeneous multiprocessors; Multiprocessor; Template-based approaches; High level synthesis
Guest editorial: Special issue on models and methodologies for system design,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964343977&doi=10.1145%2f2885503&partnerID=40&md5=35399d0c000e9c869bc1f6d89c920de2,"This special issue is based on innovative ideas presented and discussed during the 12th ACM/IEEE International Conference on Formal Methods and Models for System Design (MEMOCODE'14) held at EPFL in Lausanne, Switzerland, on October 19-21, 2014. Selected papers from the conference were invited for this special issue together with an open call soliciting novel contributions on the topics of this conference. Rigorous reviews of 12 submissions led to the selection of four articles for this special issue. In this editorial statement, we outline the premise and the context of this special issue, give a short introduction to the theme under consideration, and briefly introduce the articles selected.",,Formal methods; Innovative ideas; Switzerland; Systems analysis
Persistent clocks for batteryless sensing devices,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027253206&doi=10.1145%2f2903140&partnerID=40&md5=4aa1e5a652db00d3b07b7c69d9e0836a,"Sensing platforms are becoming batteryless to enable the vision of the Internet of Things, where trillions of devices collect data, interact with each other, and interact with people. However, these batteryless sensing platforms-that rely purely on energy harvesting-are rarely able to maintain a sense of time after a power failure. Thismakes working with sensor data that is time sensitive especially difficult.We propose two novel, zero-power timekeepers that use remanence decay to measure the time elapsed between power failures. Our approaches compute the elapsed time from the amount of decay of a capacitive device, either on-chip Static Random-Access Memory (SRAM) or a dedicated capacitor. This enables hourglass-like timers that give intermittently powered sensing devices a persistent sense of time. Our evaluation shows that applications using either timekeeper can keep time accurately through power failures as long as 45s with low overhead. © 2016 ACM.",Batteryless; Clocks; CRFID; Remanence timekeepers; RTC; SRAM,Clocks; Energy harvesting; Outages; Random access storage; Remanence; Battery-less; Capacitive devices; CRFID; Sense of time; Sensing devices; Sensing platforms; Static random access memory; Through power; Static random access storage
FE-SViT: A SViT-based fuzzy extractor framework,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006985174&doi=10.1145%2f2930669&partnerID=40&md5=0362f11b5ee25a2cd53b4af162731c28,"As a promising bio-cryptographic technique, the fuzzy extractor seamlessly binds biometrics and cryptography for template protection and key generation. However, most existing methods hardly solve the following issues simultaneously: (1) Fingerprint registration, (2) Verification accuracy, (3) Security strength, and (4) Computational efficiency. In this article, we introduce a bio-crypto-oriented fingerprint verification scheme - Selective Vertex-indexed Triangulation (SViT) which maps minutia global topology to local triangulation with minimum information loss. Then, a SViT-based fuzzy extractor framework (FE-SViT) is proposed and high verification accuracy is achieved. The FE-SViT is highly parallelizable and efficient which makes it suitable for embedded devices. © 2016 ACM.",Biometric template protection; Fingerprint security; Fuzzy extractor; Key generation,Biometrics; Cryptography; Surveying; Triangulation; Biometric template protections; Cryptographic techniques; Fingerprint registration; Fingerprint security; Fingerprint verification; Fuzzy extractors; Key generation; Minimum information loss; Computational efficiency
Learning hardware-friendly classifiers through algorithmic stability,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957083866&doi=10.1145%2f2836165&partnerID=40&md5=38963dd4efb518a08e068774611c42b0,"Most state-of-the-art machine-learning (ML) algorithms do not consider the computational constraints of implementing the learned model on embedded devices. These constraints are, for example, the limited depth of the arithmetic unit, the memory availability, or the battery capacity. We propose a new learning framework, the Algorithmic Risk Minimization (ARM), which relies on Algorithmic-Stability, and includes these constraints inside the learning process itself. ARM allows one to train advanced resource-sparing ML models and to efficiently deploy them on smart embedded systems. Finally, we show the advantages of our proposal on a smartphone-based Human Activity Recognition application by comparing it to a conventional ML approach. © 2016 ACM.",Algorithmic stability; Bit-based classifiers; Generalization performances; Hardware-friendly classifiers; Rademacher complexity; Support vector machines,Artificial intelligence; Embedded systems; Hardware; Learning algorithms; Learning systems; Parallel processing systems; Reconfigurable hardware; Smartphones; Support vector machines; Algorithmic stability; Computational constraints; Generalization performance; Human activity recognition; Learning frameworks; Learning process; Rademacher complexity; Risk minimization; Algorithms
Adaptive and hierarchical runtime manager for energy-aware thermal management of embedded systems,2016,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957058268&doi=10.1145%2f2834120&partnerID=40&md5=34c494d777c89fe0e4235f5fc0b6166e,"Modern embedded systems execute applications, which interact with the operating system and hardware differently depending on the type of workload. These cross-layer interactions result in wide variations of the chip-wide thermal profile. In this article, a reinforcement learning-based runtime manager is proposed that guarantees application-specific performance requirements and controls the POSIX thread allocation and voltage/frequency scaling for energy-efficient thermal management. This controls three thermal aspects: peak temperature, average temperature, and thermal cycling. Contrary to existing learning-based runtime approaches that optimize energy and temperature individually, the proposed runtime manager is the first approach to combine the two objectives, simultaneously addressing all three thermal aspects. However, determining thread allocation and core frequencies to optimize energy and temperature is an NP-hard problem. This leads to exponential growth in the learning table (significant memory overhead) and a corresponding increase in the exploration time to learn the most appropriate thread allocation and core frequency for a particular application workload. To confine the learning space and to minimize the learning cost, the proposed runtime manager is implemented in a two-stage hierarchy: a heuristic-based thread allocation at a longer time interval to improve thermal cycling, followed by a learning-based hardware frequency selection at a much finer interval to improve average temperature, peak temperature, and energy consumption. This enables finer control on temperature in an energy-efficient manner while simultaneously addressing scalability, which is a crucial aspect for multi-/many-core embedded systems. The proposed hierarchical runtime manager is implemented for Linux running on nVidia's Tegra SoC, featuring four ARM Cortex-A15 cores. Experiments conducted with a range of embedded and cpu-intensive applications demonstrate that the proposed runtime manager not only reduces energy consumption by an average 15% with respect to Linux but also improves all the thermal aspects-average temperature by 14°C, peak temperature by 16°C, and thermal cycling by 54%. © 2016 ACM.",Embedded systems; Linux operating system,Computational complexity; Computer operating systems; Dynamic frequency scaling; Energy efficiency; Energy utilization; Hardware; Hierarchical systems; Linux; Managers; Optimization; Power management; Programmable logic controllers; Reconfigurable hardware; Reinforcement learning; System-on-chip; Temperature control; Thermal cycling; Thermal variables control; Voltage scaling; Application specific; Cross-layer interaction; Exponential growth; Frequency selection; LINUX- operating system; Modern embedded systems; Peak temperatures; Performance requirements; Embedded systems
SMaLL: Software for Rapidly Instantiating Machine Learning Libraries,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193482794&doi=10.1145%2f3607870&partnerID=40&md5=cbdeff8a49da1284d3ab44281a571971,"Interest in deploying deep neural network (DNN) inference on edge devices has resulted in an explosion of the number and types of hardware platforms that machine learning (ML) libraries must support. High-level programming interfaces, such as TensorFlow, can be readily ported across different devices; however, maintaining performance when porting the low-level implementation is more nuanced. High-performance inference implementations require an effective mapping of the high-level interface to the target hardware platform. Commonly, this mapping may use optimizing compilers to generate code at compile time or high-performance vendor libraries that have been specialized to the target platform. Both approaches rely on expert knowledge across levels to produce an efficient mapping. This makes supporting new architectures difficult and time-consuming. In this work, we present a DNN library framework, SMaLL, that is easily extensible to new architectures. The framework uses a unified loop structure and shared, cache-friendly data format across all intermediate layers, eliminating the time and memory overheads incurred by data transformation between layers. Each layer is implemented by specifying its dimensions and a kernel, the key computing operation of that layer. The unified loop structure and kernel abstraction allows the reuse of code across layers and computing platforms. New architectures only require a few hundred lines in the kernel to be redesigned. To show the benefits of our approach, we have developed software that supports a range of layer types and computing platforms; this software is easily extensible for rapidly instantiating high-performance DNN libraries. An evaluation of the portability of our framework is shown by instantiating end-to-end networks from the MLPerf:tiny benchmark suite on five ARM platforms and one x86 platform (an AMD Zen 2). We also show that the end-to-end performance is comparable to or better than ML frameworks such as TensorFlow, TVM, and LibTorch. © 2024 Copyright held by the owner/author(s).",embedded systems; high-performance; machine learning libraries; Mathematical software; portability,Codes (symbols); Computer software portability; Deep neural networks; Learning systems; Libraries; Mapping; Metadata; Network architecture; Computing platform; Embedded-system; Hardware platform; High-performance; Loop structure; Machine learning library; Machine-learning; Mathematical software; Performance; Portability; Embedded systems
SensorGAN: A Novel Data Recovery Approach for Wearable Human Activity Recognition,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194199940&doi=10.1145%2f3609425&partnerID=40&md5=3d40ece91034db9af730081a79fa4ae3,"Human activity recognition (HAR) and, more broadly, activities of daily life recognition using wearable devices have the potential to transform a number of applications, including mobile healthcare, smart homes, and fitness monitoring. Recent approaches for HAR use multiple sensors on various locations on the body to achieve higher accuracy for complex activities. While multiple sensors increase the accuracy, they are also susceptible to reliability issues when one or more sensors are unable to provide data to the application due to sensor malfunction, user error, or energy limitations. Training multiple activity classifiers that use a subset of sensors is not desirable, since it may lead to reduced accuracy for applications. To handle these limitations, we propose a novel generative approach that recovers the missing data of sensors using data available from other sensors. The recovered data are then used to seamlessly classify activities. Experiments using three publicly available activity datasets show that with data missing from one sensor, the proposed approach achieves accuracy that is within 10% of the accuracy with no missing data. Moreover, implementation on a wearable device prototype shows that the proposed approach takes about 1.5 ms for recovering data in the w-HAR dataset, which results in an energy consumption of 606 μJ. The low-energy consumption ensures that SensorGAN is suitable for effectively recovering data in tinyML applications on energy-constrained devices. © 2024 Copyright held by the owner/author(s).",clustering; data imputation; health monitoring; Human activity recognition; missing data detection; wearable electronics,Energy utilization; Intelligent buildings; mHealth; Pattern recognition; Recovery; Wearable sensors; Clusterings; Data imputation; Data recovery; Data-detection; Health monitoring; Human activity recognition; Missing data; Missing data detection; Multiple sensors; Wearable devices; Automation
XimSwap: Many-to-Many Face Swapping for TinyML,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193261950&doi=10.1145%2f3603173&partnerID=40&md5=4ea251a753a0d66889d3d8a27e60b2d0,"The unprecedented development of deep learning approaches for video processing has caused growing privacy concerns. To ensure data analysis while maintaining privacy, it is essential to address how to protect individuals’ identities. One solution is to anonymize data at the source, avoiding the transmission or storage of information that could lead to identification. This study introduces XimSwap, a novel deep learning technique for real-time video anonymization, which can remove facial identification features directly on edge devices with minimal computational resources. Our approach offers a comprehensive solution that guarantees privacy by design. This novel method for implementing face-swapping ensures that the pose and expression of a target face remain unchanged and can be used on embedded devices with very limited computational resources. By incorporating style transfer layers into convolutional ones and optimizing the network’s operation, we achieved a reduction of over 98% in the required operations and parameters compared with state-of-the-art architectures. Our approach also significantly reduces RAM usage, making it possible to implement the anonymization process on tiny edge devices, including microcontrollers, such as the STM32H743. © 2024 Copyright held by the owner/author(s).",edge AI; Faceswap; neural networks; tiny ML,Deep learning; Edge computing; Learning systems; Video signal processing; Anonymization; Computational resources; Edge AI; Face swapping; Faceswap; Learning approach; Many to many; Neural-networks; Tiny ML; Video processing; Random access storage
TyBox: An Automatic Design and Code Generation Toolbox for TinyML Incremental On-Device Learning,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193526909&doi=10.1145%2f3604566&partnerID=40&md5=bee3fd89cd464eb45f672ca7915604c3,"Incremental on-device learning is one of the most relevant and interesting challenges in the field of Tiny Machine Learning (TinyML). Indeed, differently from traditional TinyML solutions where the training is typically carried out on the Cloud and inference only occurs on the tiny devices (e.g., embedded systems or Internet-of-Things units), incremental on-device TinyML allows both the inference and the training of TinyML models directly on tiny devices. This ability paves the way for TinyML-enabled intelligent devices that can learn directly on the field and adapt to evolving environments, different working conditions, or specific users. The literature in this field is quite limited with very few solutions focusing only on the incremental fine-tuning of machine learning models, whereas a general solution encompassing algorithms and code generation for incremental on-device TinyML is still perceived as missing. The aim of this article is to introduce, to the best of our knowledge for the first time in the literature, a toolbox called TyBox for the automatic design and code generation of incremental on-device TinyML classification models. In more detail, starting from a “static” TinyML model, TyBox is able to (i) automatically design the “incremental” on-device version of the TinyML model that has been suitably designed to take into account the technological constraint on the RAM memory of the target tiny device, and (ii) autonomously provide the C++ codes and libraries to support the inference and learning of the incremental on-device TinyML model directly on the tiny devices. TyBox has been extensively compared with a state-of-the-art incremental learning solution for TinyML and tested on an off-the-shelf tiny device (i.e., the Arduino Nano 33 BLE) in three relevant TinyML application tasks and scenarios: binary image classification, multi-class image classification, and ultra-wide-band human activity recognition. In addition, TyBox is released to the scientific community as a public repository. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",incremental learning; interpreters and code generation for tiny systems; mapping; Tiny Machine Learning; TinyML software generation,Binary images; C++ (programming language); Codes (symbols); Embedded systems; Image classification; Object oriented programming; Random access storage; Automatic design; Codegeneration; Incremental learning; Interpreter and code generation for tiny system; Machine learning models; Machine learning software; Machine-learning; Software generation; Tiny machine learning; Tiny machine learning software generation; Machine learning
Constrained Tiny Machine Learning for Predicting Gas Concentration with I4.0 Low-cost Sensors,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193499214&doi=10.1145%2f3590956&partnerID=40&md5=eed793301eade30507ad354c5bb888c5,"Low-cost gas sensors (LCS) often produce inaccurate measurements due to varying environmental conditions that are not consistent with laboratory settings, leading to inadequate productivity levels compared to high-quality sensors. To address this issue, we propose the use of Machine Learning (ML) to predict accurate concentrations of pollutant gases acquired by LCS integrated into an embedded Internet of Things platform. However, a key challenge is to optimize an accurate ML design under low memory and computation power constraints of microcontrollers (MCUs) while maintaining accurate ML scores. After data analysis and pre-processing, we assess and analyze the performance of five ML algorithms to predict the concentration of pollutants gases from multiple specifications (weather, presence of other gases, etc.). To support the experiments, datasets from three sources are used: (1) VOCSens, (2) Belgian Interregional Environment Agency cell, and (3) Visual-Crossing. Once the best model was optimized and validated, multiple hard constraints were added to the selected ML structure to satisfy material and expert requirements. Trained models were ported to be implemented locally in a MCU after comparing several porting libraries. The assembled code obtained is evaluated based on two metrics: storage memory consumption and inference time, relative to the highest attainable capacities. The improved random forest is the best ML model for the used dataset with an R2 score meeting of 0.72 and Root Means Square Error of 0.0028 ppm. The best generated Tiny-ML model needs 3% of RAM and 98% of Flash storage. The empirical results prove that the developed ML algorithm applied to LCS provides high accuracy to predict pollutant gases. This algorithm can also be used to adjust the LCS systems to provide calibrated data in real time, even if the platform being used is not particularly advanced or powerful. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",calibration; Constrained machine learning; gas concentration; low-cost sensors; random forest; regression; TinyML,Costs; Forestry; Gases; Machine learning; Microcontrollers; Random access storage; Constrained machine learning; Gas concentration; Gas-sensors; Low-cost sensors; Low-costs; Machine-learning; Pollutant gas; Random forests; Regression; Tinyml; Forecasting
TinyNS: Platform-aware Neurosymbolic Auto Tiny Machine Learning,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194167573&doi=10.1145%2f3603171&partnerID=40&md5=915332eac8785aff4c74ef3e93fd1b9b,"Machine learning at the extreme edge has enabled a plethora of intelligent, time-critical, and remote applications. However, deploying interpretable artificial intelligence systems that can perform high-level symbolic reasoning and satisfy the underlying system rules and physics within the tight platform resource constraints is challenging. In this article, we introduce TinyNS, the first platform-aware neurosymbolic architecture search framework for joint optimization of symbolic and neural operators. TinyNS provides recipes and parsers to automatically write microcontroller code for five types of neurosymbolic models, combining the context awareness and integrity of symbolic techniques with the robustness and performance of machine learning models. TinyNS uses a fast, gradient-free, black-box Bayesian optimizer over discontinuous, conditional, numeric, and categorical search spaces to find the best synergy of symbolic code and neural networks within the hardware resource budget. To guarantee deployability, TinyNS talks to the target hardware during the optimization process. We showcase the utility of TinyNS by deploying microcontroller-class neurosymbolic models through several case studies. In all use cases, TinyNS outperforms purely neural or purely symbolic approaches while guaranteeing execution on real hardware. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",AutoML; bayesian; neural architecture search; Neurosymbolic; platform-aware; TinyML,Bayesian networks; Budget control; Machine learning; Microcontrollers; Network architecture; Automl; Bayesian; Machine-learning; Neural architecture search; Neural architectures; Neurosymbolic; Platform-aware; Remote applications; Time-critical applications; Tinyml; Codes (symbols)
Characterizing Parameter Scaling with Quantization for Deployment of CNNs on Real-Time Systems,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193501546&doi=10.1145%2f3654799&partnerID=40&md5=e96126a0857bd88f14241513fa280fe3,"Modern deep-learning models tend to include billions of parameters, reducing real-time performance. Embedded systems are compute-constrained while frequently used to deploy these models for real-time systems given size, weight, and power requirements. Tools like parameter-scaling methods help to shrink models to ease deployment. This research compares two scaling methods for convolutional neural networks, uniform scaling and NeuralScale, and analyzes their impact on inference latency, memory utilization, and power. Uniform scaling scales the number of filters evenly across a network. NeuralScale adaptively scales the model to theoretically achieve the highest accuracy for a target parameter count. In this study, VGG-11, MobileNetV2, and ResNet-50 models were scaled to four ratios: 0.25×, 0.50×, 0.75×, 1.00×. These models were benchmarked on an ARM Cortex-A72 CPU, an NVIDIA Jetson AGX Xavier GPU, and a Xilinx ZCU104 FPGA. Additionally, quantization was applied to meet real-time objectives. The CIFAR-10 and tinyImageNet datasets were studied. On CIFAR-10, NeuralScale creates more computationally intensive models than uniform scaling for the same parameter count, with relative speeds of 41% on the CPU, 72% on the GPU, and 96% on the FPGA. The additional computational complexity is a tradeoff for accuracy improvements in VGG-11 and MobileNetV2 NeuralScale models but reduced ResNet-50 NeuralScale accuracy. Furthermore, quantization alone achieves similar or better performance on the CPU and GPU devices when compared with models scaled to 0.50×, despite slight reductions in accuracy. On the GPU, quantization reduces latency by 2.7× and memory consumption by 4.3×. Uniform-scaling models are 1.8× faster and use 2.8× less memory. NeuralScale reduces latency by 1.3× and dropped memory by 1.1×. We find quantization to be a practical first tool for improved performance. Uniform scaling can easily be applied for additional improvements. NeuralScale may improve accuracy but tends to negatively impact performance, so more care must be taken with it. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",benchmarking; Computer vision; convolutional neural networks; machine learning; parameter scaling,Computer vision; Convolution; Convolutional neural networks; Deep learning; Embedded systems; Field programmable gate arrays (FPGA); Graphics processing unit; Interactive computer systems; Learning systems; Convolutional neural network; Learning models; Machine-learning; Parameter scaling; Performance; Quantisation; Real - Time system; Real time performance; Scaling method; Scalings; Real time systems
Enhancing the Energy Efficiency and Robustness of tinyML Computer Vision Using Coarsely-quantized Log-gradient Input Images,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193510119&doi=10.1145%2f3591466&partnerID=40&md5=aabef0d388128bb544720a3f4fb465d3,"This article studies the merits of applying log-gradient input images to convolutional neural networks (CNNs) for tinyML computer vision (CV). We show that log gradients enable: (i) aggressive 1-bit quantization of first-layer inputs, (ii) potential CNN resource reductions, (iii) inherent insensitivity to illumination changes (1.7% accuracy loss across 2−5 . . . 23 brightness variation vs. up to 10% for JPEG), and (iv) robustness to adversarial attacks (>10% higher accuracy than JPEG-trained models). We establish these results using the PASCAL RAW image dataset and through a combination of experiments using quantization threshold search, neural architecture search, and a fixed three-layer network. The latter reveals that training on log-gradient images leads to higher filter similarity, making the CNN more prunable. The combined benefits of aggressive first-layer quantization, CNN resource reductions, and operation without tight exposure control and image signal processing (ISP) are helpful for pushing tinyML CV toward its ultimate efficiency limits. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Adversarial examples; computer vision pipeline; filter similarity; illumination invariance; image signal processing; log gradients; neural network quantization; quantization threshold search; sensor datasets,Convolutional neural networks; Energy efficiency; Image enhancement; Multilayer neural networks; Network layers; Pipelines; Quantization (signal); Adversarial example; Computer vision pipeline; Filter similarity; Illumination invariance; Image signal processing; Log gradient; Neural network quantization; Neural-networks; Quantisation; Quantization threshold search; Sensor dataset; Threshold searches; Computer vision
An Approach to the Systematic Characterization of Multitask Accelerated CNN Inference in Edge MPSoCs,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193480316&doi=10.1145%2f3611015&partnerID=40&md5=54c24c48e486cd14061a6c871368cd7a,"Deep Learning is ubiquitous today and is increasingly moving from the cloud down to the edge of networked infrastructures, where it enables embedded applications to perform complex inference tasks close to the data sources, reducing long-distance data movement and alleviating the need for a powerful cloud infrastructure. Edge-class multi-processor system on chip (MPSoC) devices featuring an on-chip FPGA fabric offer key advantages for Deep Learning inference tasks, especially for complex applications where multiple models may be run concurrently in the same platform. In this work, we propose an approach and a practical framework for the systematic characterization of multithreaded Deep Learning inference on edge FPGA MPSoCs. We instantiate the framework into a real-world MPSoC platform, targeting Xilinx Vitis-AI as a representative example of a commercial Deep Learning acceleration toolkit for edge environments. We design a comprehensive experimental campaign and apply it to the platform for several convolutional neural networks, each trained on three different datasets. We show that our approach can be used for both hardware- and software-level analysis of a target system. Among other findings, the analysis revealed a suboptimal behavior of the underlying toolkit runtime, involving the utilization of the accelerator cores and the uneven software latency of the support library, influenced by the shapes of the input tensors. © 2024 Copyright held by the owner/author(s).",deep learning; Edge computing; FPGA; MPSoC; quantization,Complex networks; Convolutional neural networks; Deep learning; Field programmable gate arrays (FPGA); Learning systems; Multiprocessing systems; System-on-chip; Cloud infrastructures; Data movements; Data-source; Deep learning; Distance datum; Edge computing; Embedded application; Multi processor system on chips; Quantisation; Systems-on-chip devices; Edge computing
Scalable Binary Neural Network Applications in Oblivious Inference,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193905621&doi=10.1145%2f3607192&partnerID=40&md5=8229cb39840ee69b37721687d3700f40,"Binary neural network (BNN) delivers increased compute intensity and reduces memory/data requirements for computation. Scalable BNN enables inference in a limited time due to different constraints. This paper explores the application of Scalable BNN in oblivious inference, a service provided by a server to mistrusting clients. Using this service, a client can obtain the inference result on his/her data by a trained model held by the server without disclosing the data or learning the model parameters. Two contributions of this paper are: (1) we devise lightweight cryptographic protocols explicitly designed to exploit the unique characteristics of BNNs. (2) we present an advanced dynamic exploration of the runtime-accuracy tradeoff of scalable BNNs in a single-shot training process. While previous works trained multiple BNNs with different computational complexities (which is cumbersome due to the slow convergence of BNNs), we train a single BNN that can perform inference under various computational budgets. Compared to CryptFlow2, the state-of-the-art technique in the oblivious inference of non-binary DNNs, our approach reaches 3× faster inference while keeping the same accuracy. Compared to XONN, the state-of-the-art technique in the oblivious inference of binary networks, we achieve 2× to 12× faster inference while obtaining higher accuracy. © 2024 Copyright held by the owner/author(s).",oblivious inference; Scalable neural network; secure computing,Binary neural networks; Data requirements; Fast inference; Modeling parameters; Neural network application; Neural-networks; Oblivious inference; Scalable neural network; Secure computing; State-of-the-art techniques; Budget control
An Investigation on Hardware-Aware Vision Transformer Scaling,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193472395&doi=10.1145%2f3611387&partnerID=40&md5=ce36a5534c263b0acec304e9653f16d1,"Vision Transformer (ViT) has demonstrated promising performance in various computer vision tasks, and recently attracted a lot of research attention. Many recent works have focused on proposing new architectures to improve ViT and deploying it into real-world applications. However, little effort has been made to analyze and understand ViT’s architecture design space and its implication for hardware costs on different devices. In this work, by simply scaling ViT’s depth, width, input size, and other basic configurations, we show that a scaled vanilla ViT model without bells and whistles can achieve comparable or superior accuracy-efficiency trade-off than most of the latest ViT variants. Specifically, compared with DeiT-Tiny, our scaled model achieves a ↑ 1.9% higher ImageNet top-1 accuracy under the same FLOPs and a ↑ 3.7% better ImageNet top-1 accuracy under the same latency on an NVIDIA Edge GPU TX2. Motivated by this, we further investigate the extracted scaling strategies from the following two aspects: (1) can these scaling strategies be transferred across different real hardware devices? and (2) can these scaling strategies be transferred to different ViT variants and tasks?. For (1), our exploration, based on various devices with different resource budgets, indicates that the transferability effectiveness depends on the underlying device together with its corresponding deployment tool. For (2), we validate the effective transferability of the aforementioned scaling strategies obtained from a vanilla ViT model on top of an image classification task to the PiT model, a strong ViT variant targeting efficiency as well as object detection and video classification tasks. In particular, when transferred to PiT, our scaling strategies lead to a boosted ImageNet top-1 accuracy of from 74.6% to 76.7% (↑ 2.1%) under the same 0.7G FLOPs. When transferred to the COCO object detection task, the average precision is boosted by ↑ 0.7% under a similar throughput on a V100 GPU. © 2024 Copyright held by the owner/author(s).",Deep neural network scaling; vision transformer,Budget control; Computer hardware; Computer vision; Deep neural networks; Economic and social effects; Efficiency; Image classification; Network architecture; Object recognition; Architecture designs; Classification tasks; Deep neural network scaling; Design spaces; Objects detection; Performance; Real-world; Scalings; Transformer modeling; Vision transformer; Object detection
TinyM2Net-V2: A Compact Low-power Software Hardware Architecture for Multimodal Deep Neural Networks,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193511954&doi=10.1145%2f3595633&partnerID=40&md5=1aea3565eb0bc15f63585f1c9f3610f8,"With the evaluation of Artificial Intelligence (AI), there has been a resurgence of interest in how to use AI algorithms on low-power embedded systems to broaden potential use cases of the Internet of Things (IoT). To mimic multimodal human perception, multimodal deep neural networks (M-DNN) have recently become very popular with the classification task due to their impressive performance for computer vision and audio processing tasks. This article presents TinyM2Net-V2—a compact low-power software hardware architecture for multimodal deep neural networks for resource-constrained tiny devices. To compress the models to implement on tiny devices, cyclicly sparsification and hybrid quantization (4-bits weights and 8-bits activations) methods are used. Although model compression techniques are an active research area, we are the first to demonstrate their efficacy for multimodal deep neural networks, using cyclicly sparsification and hybrid quantization of weights/activations. TinyM2Net-V2 shows that even a tiny multimodal deep neural network model can improve the classification accuracy more than that of any unimodal counterparts. Parameterized M-DNN model architecture was designed to be evaluated in two different case-studies: vehicle detection from multimodal images and audios and COVID-19 detection from multimodal audio recordings. The most compressed TinyM2Net-V2 achieves 92.5% COVID-19 detection accuracy (6.8% improvement from the unimodal full precision model) and 90.6% vehicle classification accuracy (7.7% improvement from the unimodal full precision model). A parameterized and flexible FPGA hardware accelerator was designed as well for TinyM2Net-V2 models. To the best of our knowledge, this is the first work accelerating multimodal deep neural network models on low-power Artix-7 FPGA hardware. We achieved energy efficiency of 9.04 GOP/s/W and 15.38 GOP/s/W for case-study 1 and case-study 2, respectively, which is comparable to the state-of-the-art results. Finally, we compared our tiny FPGA hardware implementation results with off-the-shelf resource-constrained devices and showed our implementation is faster and consumed less power compared to the off-the-shelf resource-constrained devices. © 2024 Copyright held by the owner/author(s).",FPGA; model compression; multimodal deep neural networks; tinyML,Computer hardware; Embedded systems; Energy efficiency; Field programmable gate arrays (FPGA); Internet of things; Low power electronics; Network architecture; Neural network models; Case-studies; Low Power; Model compression; Multi-modal; Multimodal deep neural network; Neural network model; Power softwares; Software/hardware; Tinyml; Unimodal; Deep neural networks
Reg-Tune: A Regression-Focused Fine-Tuning Approach for Profiling Low Energy Consumption and Latency,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193901890&doi=10.1145%2f3623380&partnerID=40&md5=7fcf03a200a5daa1b2e4e7fdc4142e19,"Fine-tuning deep neural networks is pivotal for creating inference modules that can be suitably imported to edge or field-programmable gate array (FPGA) platforms. Traditionally, exploration of different parameters throughout the layers of deep neural networks has been done using grid search and other brute force techniques. Although these methods lead to the optimal choice of network parameters, the search process can be very time consuming and may not consider deployment constraints across different target platforms. This work addresses this problem by proposing Reg-Tune, a regression-based profiling approach to quickly determine the trend of different metrics in relation to hardware deployment of neural networks on tinyML platforms like FPGAs and edge devices. We start by training a handful of configurations belonging to different combinations of NN〈q(quantization), s(scalinд)〉 or NN〈r(resolution), s〉 workloads to generate the accuracy values respectively for their corresponding application. Next, we deploy these configurations on the target device to generate energy/latency values. According to our hypothesis, the most energy-efficient configuration suitable for deployment on the target device is a function of the variables q, r, and s. Finally, these trained and deployed configurations and their related results are used as data points for polynomial regression with the variables q, r, and s to realize the trend for accuracy/energy/latency on the target device. Our setup allows us to choose the near-optimal energy-consuming or latency-driven configuration for the desired accuracy from the contour profiles of energy/latency across different tinyML device platforms. To this extent, we demonstrate the profiling process for three different case studies and across two platforms for energy and latency fine-tuning. Our approach results in at least 5.7× better energy efficiency when compared to recent implementations for human activity recognition on FPGA and 74.6% reduction in latency for semantic segmentation of aerial imagery on edge devices compared to baseline deployments. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep neural networks; hardware accelerators; human activity recognition; image segmentation; neural architecture search; regression; tinyML,Antennas; Energy efficiency; Energy utilization; Field programmable gate arrays (FPGA); Multilayer neural networks; Pattern recognition; Semantic Segmentation; Semantics; Energy; Fine tuning; Hardware accelerators; Human activity recognition; Images segmentations; Low energy consumption; Neural architecture search; Neural architectures; Regression; Tinyml; Deep neural networks
Online Processing of Vehicular Data on the Edge Through an Unsupervised TinyML Regression Technique,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193922255&doi=10.1145%2f3591356&partnerID=40&md5=da88dee13693583e2725260a387773ba,"The Internet of Things (IoT) has made it possible to include everyday objects in a connected network, allowing them to intelligently process data and respond to their environment. Thus, it is expected that those objects will gain an intelligent understanding of their environment and be able to process data more efficiently than before. Particularly, such edge computing paradigm has allowed the execution of inference methods on resource-constrained devices such as microcontrollers, significantly changing the way IoT applications have evolved in recent years. However, although this scenario has supported the development of Tiny Machine Learning (TinyML) approaches on such devices, there are still some challenges that require further investigation when optimizing data streaming on the edge. Therefore, this article proposes a new unsupervised TinyML regression technique based on the typicality and eccentricity of the samples to be processed. Moreover, the proposed technique also exploits a Recursive Least Squares (RLS) filter approach. Combining all these features, the proposed method uses similarities between samples to identify patterns when processing data streams, predicting outcomes based on these patterns. The results obtained through the extensive experimentation utilizing vehicular data streams were highly encouraging. The proposed algorithm was meticulously compared with the RLS algorithm and Convolutional Neural Networks (CNN). It exhibited significantly superior performance, with mean squared errors that were 4.68 and 12.02 times lower, respectively, compared to the aforementioned techniques. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Eccentricity; regression; TinyML; typicality; unsupervised learning; vehicular networks,Convolutional neural networks; Data streams; E-learning; Learning algorithms; Machine learning; Mean square error; Regression analysis; Data stream; Eccentricity; Machine-learning; Online processing; Process data; Regression; Regression techniques; Tiny machine learning; Typicality; Vehicular networks; Internet of things
Introduction to the Special Issue on tinyML,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194067200&doi=10.1145%2f3658375&partnerID=40&md5=db7fe6b7c5b22aa9af0593d00c879dfb,[No abstract available],,
NAVIDRO: A CARES Architectural Style for Configuring Drone Co-Simulation,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193903617&doi=10.1145%2f3651889&partnerID=40&md5=b8d10ca678b75d067ae8c4f185a17414,"One primary objective of drone simulation is to evaluate diverse drone configurations and contexts aligned with specific user objectives. The initial challenge for simulator designers involves managing the heterogeneity of drone components, encompassing both software and hardware systems, as well as the drone’s behavior. To facilitate the integration of these diverse models, the Functional Mock-Up Interface (FMI) for co-simulation proposes a generic data-oriented interface. However, an additional challenge lies in simplifying the configuration of co-simulation, necessitating an approach to guide the modeling of parametric features and operational conditions such as failures or environment changes. The article addresses this challenge by introducing CARES, a model-driven engineering and component-based approach for designing drone simulators, integrating the FMI for co-simulation. The proposed models incorporate concepts from component-based software engineering and FMI. The NAVIDRO architectural style is presented for designing and configuring drone co-simulation. CARES utilizes a code generator to produce structural glue code (Java or C++), facilitating the integration of FMI-based domain-specific code. The approach is evaluated through the development of a simulator for navigation functions in an autonomous underwater vehicle, demonstrating its effectiveness in assessing various autonomous underwater vehicle configurations and contexts. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",co-simulation; Component-based design; cyber-physical system; drone; model-driven engineering,Autonomous underwater vehicles; Autonomous vehicles; C++ (programming language); Computer software; Drones; Embedded systems; Software architecture; Architectural style; Autonomous underwater vehicles]; Component based design; Cosimulation; Cybe-physical systems; Cyber-physical systems; Mock up; Model-driven Engineering; Primary objective; Software and hardwares; Cyber Physical System
"Energy Management for Fault-tolerant (m,k)-constrained Real-time Systems That Use Standby-Sparing",2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193901076&doi=10.1145%2f3648365&partnerID=40&md5=c0680301f0e916a380bff3fda1438f36,"Fault tolerance, energy management, and quality of service (QoS) are essential aspects for the design of real-time embedded systems. In this work, we focus on exploring methods that can simultaneously address the above three critical issues under standby-sparing. The standby-sparing mechanism adopts a dual-processor architecture in which each processor plays the role of the backup for the other one dynamically. In this way, it can provide fault tolerance subject to both permanent and transient faults. Due to its duplicate executions of the real-time jobs/tasks, the energy consumption of a standby-sparing system could be quite high. With the purpose of reducing energy under standby-sparing, we proposed three novel scheduling schemes: The first one is for (1, 1)-constrained tasks, and the second one and the third one (which can be combined into an integrated approach to maximize the overall energy reduction) are for general (m, k)-constrained tasks that require that among any k consecutive jobs of a task no more than (k − m) out of them could miss their deadlines. Through extensive evaluations and performance analysis, our results demonstrate that compared with the existing research, the proposed techniques can reduce energy by up to 11% for (1, 1)-constrained tasks and 25% for general (m, k)-constrained tasks while assuring (m, k)-constraints and fault tolerance as well as providing better user perceived QoS levels under standby-sparing. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Energy efficiency; fault tolerance; QoS; real-time systems; standby-sparing,Embedded systems; Energy management; Energy utilization; Fault tolerance; Interactive computer systems; Quality of service; Real time systems; Constrained tasks; Critical issues; Dual processors; Energy; Energy quality; Fault-tolerant; Quality-of-service; Real - Time system; Real-time embedded systems; Standby-sparing; Energy efficiency
Toward Energy-efficient STT-MRAM-based Near Memory Computing Architecture for Embedded Systems,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194171590&doi=10.1145%2f3650729&partnerID=40&md5=49676d79de695de12fc27ed238d7b985,"Convolutional Neural Networks (CNNs) have significantly impacted embedded system applications across various domains. However, this exacerbates the real-time processing and hardware resource-constrained challenges of embedded systems. To tackle these issues, we propose spin-transfer torque magnetic random-access memory (STT-MRAM)-based near memory computing (NMC) design for embedded systems. We optimize this design from three aspects: Fast-pipelined STT-MRAM readout scheme provides higher memory bandwidth for NMC design, enhancing real-time processing capability with a non-trivial area overhead. Direct index compression format in conjunction with digital sparse matrix-vector multiplication (SpMV) accelerator supports various matrices of practical applications that alleviate computing resource requirements. Custom NMC instructions and stream converter for NMC systems dynamically adjust available hardware resources for better utilization. Experimental results demonstrate that the memory bandwidth of STT-MRAM achieves 26.7 GB/s. Energy consumption and latency improvement of digital SpMV accelerator are up to 64× and 1,120× across sparsity matrices spanning from 10% to 99.8%. Single-precision and double-precision elements transmission increased up to 8× and 9.6×, respectively. Furthermore, our design achieves a throughput of up to 15.9× over state-of-the-art designs. © 2024 Copyright held by the owner/author(s).",compressed format; computing resources; digital accelerator; Real-time processing; throughput,Bandwidth; Embedded systems; Energy efficiency; Green computing; Magnetic recording; Matrix algebra; Memory architecture; MRAM devices; Network architecture; Pipeline processing systems; Real time systems; Compressed format; Computing resource; Digital accelerators; Embedded-system; Hardware resources; Magnetic random access memory; Realtime processing; Sparse matrix-vector multiplication; Sparse-Matrix Vector multiplications; Spin transfer torque; Energy utilization
MemFHE: End-to-end Computing with Fully Homomorphic Encryption in Memory,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189432437&doi=10.1145%2f3569955&partnerID=40&md5=9416c15a49322edefbed87ccdb87a98c,"The increasing amount of data and the growing complexity of problems have resulted in an ever-growing reliance on cloud computing. However, many applications, most notably in healthcare, finance, or defense, demand security and privacy, which today's solutions cannot fully address. Fully homomorphic encryption (FHE) elevates the bar of today's solutions by adding confidentiality of data during processing. It allows computation on fully encrypted data without the need for decryption, thus fully preserving privacy. To enable processing encrypted data at usable levels of classic security, e.g., 128-bit, the encryption procedure introduces noticeable data size expansion - the ciphertext is much bigger than the native aggregate of native data types. In this article, we present MemFHE, which is the first accelerator of both client and server for the latest Ring-GSW (Gentry et al. [17])-based homomorphic encryption schemes using Processing in Memory (PIM). PIM alleviates the data movement issues with large FHE encrypted data while providing in situ execution and extensive parallelism needed for FHE's polynomial operations. While the client-PIM can homomorphically encrypt and decrypt data, the server-PIM can process homomorphically encrypted data without decryption. MemFHE's server-PIM is pipelined and is designed to provide flexible bootstrapping, allowing two encryption techniques and various FHE security levels based on the application requirements. We evaluate MemFHE for various security levels and compare it with state-of-the-art CPU implementations for Ring-GSW-based FHE. MemFHE is up to 20k× (265×) faster than CPU (GPU) for FHE arithmetic operations and provides on average 2,007× higher throughput than [36] while implementing neural networks with FHE.  © 2024 Association for Computing Machinery.",Additional Key Words and PhrasesMemFHE; fully homomorphic encryption,Additional key word and phrasesmemfhe; Cloud-computing; Encrypted data; End to end; Fully homomorphic encryption; Key words; Processing-in-memory; Security and privacy; Security level; Server processing; Cryptography
A Configurable CRYSTALS-Kyber Hardware Implementation with Side-Channel Protection,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189241840&doi=10.1145%2f3587037&partnerID=40&md5=a924354d4867620f49d9565b1ac40050,"In this work, we present a configurable and side channel resistant implementation of the post-quantum keyexchange algorithm CRYSTALS-Kyber. The implemented design can be configured for different performance and area requirements leading to different trade-offs for different applications. A low area implementation can be achieved in 5,269 LUTs and 2,422 FFs, whereas a high performance implementation required 7,151 LUTs and 3,730 FFs. Due to a deeply pipelined architecture, a high operating speed of more than 250 MHz could be achieved on 28nm Xilinx FPGAs. The side channel resistance is implemented using a carefully chosen set of novel and known techniques such as Fault Detection Hashes, Instruction Randomization, FSM Protection and so on. resulting in a low overhead of less than 5% while being highly configurable. To the best of our knowledge, this work presents the first side-channel and fault attack protected configurable accelerator for CRYSTALS-Kyber. Using TVLA (test vector leakage assessment), we validate the implemented protection techniques and demonstrate that the design does not leak information even after 200 K traces. Furthermore, one of the configuration choices results in the smallest hardware implementation of CRYSTALS-Kyber known in the literature.  © 2024 Copyright held by the owner/author(s).",Cryptography; cryptoprocessor; fault-resistance; key-exchange; Kyber; post-quantum; SCA,Chromium compounds; Economic and social effects; Fault detection; Channel protection; Crypto-processor; Fault resistances; Hardware implementations; Key-exchange; Kyber; Performance requirements; Post quantum; SCA; Side-channel; Side channel attack
Toward Next Generation Quantum-Safe eIDs and eMRTDs: A Survey,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189238299&doi=10.1145%2f3585517&partnerID=40&md5=dcca734572d2302cd266e26ff90ebc31,"Security mechanisms of Electronic Personal Documents (eCards) depend on (asymmetric) cryptography that is and always has been subject to the threat of compromise, be it from conventional attacks or quantum computers. With Post-Quantum Cryptography (PQC), we now have alternative building blocks at hand that can be leveraged to protect against both kind of attacks. Thus, PQC should be incorporated into eCard ecosystems, yet it is not clear how this is done best. In the work at hand, we review the state of currently used cryptosystems for eCard security, as well as their possible quantum-secure replacements. Further, we identify and categorize respective challenges that need to be addressed, present and assess existing approaches for their solution, and formulate research questions for open issues. By providing an overview of the situation, we help unraveling the issue and pave the way toward quantum-safe electronic Identity Documents and electronic Machine-Readable Travel Documents.  © 2024 Copyright held by the owner/author(s).",eCards; electronic identification documents (eID); electronic machine readable travel documents (MRTD); Post-quantum cryptography (PQC),Quantum computers; Quantum cryptography; Asymmetric cryptography; Ecard; Electronic identification document; Electronic machine readable travel document; Machine readable travel documents; Personal documents; Post quantum cryptography; Post-quantum cryptography; Quanta computers; Security mechanism; Quantum theory
Cryptographic Engineering a Fast and Efficient SIKE in FPGA,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189452802&doi=10.1145%2f3584919&partnerID=40&md5=1dadbb153cb6d6b3500dfd640df8202d,"Recent attacks have shown that SIKE is not secure and should not be used in its current state. However, this work was completed before these attacks were discovered and might be beneficial to other cryptosystems such as SQISign. The primary downside of SIKE is its performance. However, this work achieves new SIKE speed records even using less resources than the state-of-the-art. Our approach entails designing and optimizing a new field multiplier, SIKE-optimized Keccak unit, and high-level controller. On a Xilinx Virtex-7 FPGA, this architecture performs the NIST Level 1 SIKE scheme key encapsulation and key decapsulation functions in 2.23 and 2.39 ms, respectively. The combined key encapsulation and decapsulation time is 4.62 ms, which outperforms the next best Virtex-7 implementation by nearly 2 ms. Our implementation achieves speed records for the NIST Level 1, 2, and 3 parameter sets. Only our NIST Level 5 parameter set was beat by an all-out performance implementation. Our implementations also efficiently utilize the FPGA resources, achieving new records in area-time product metrics for all parameter sets. Overall, this work continues to push the bar for accelerating SIKE computations to make a stronger case for SIKE standardization.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesIsogeny-based cryptography; Montgomery multiplication; post-quantum cryptography; RISC-V; SIKE,Quantum cryptography; Additional key word and phrasesisogeny-based cryptography; CryptoGraphics; De-capsulation; Key words; Level-1; Montgomery multiplication; Parameter set; Post quantum cryptography; RISC-V; SIKE; Field programmable gate arrays (FPGA)
Analysis of EM Fault Injection on Bit-sliced Number Theoretic Transform Software in Dilithium,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188361533&doi=10.1145%2f3583757&partnerID=40&md5=fad0b629ca60420ff65879ba2678eadb,"Bitslicing is a software implementation technique that treats an N-bit processor datapath as N parallel single-bit datapaths. Bitslicing is particularly useful to implement data-parallel algorithms, algorithms that apply the same operation sequence to every element of a vector. Indeed, a bit-wise processor instruction applies the same logical operation to every single-bit slice. A second benefit of bitsliced execution is that the natural spatial redundancy of bitsliced software can support countermeasures against fault attacks. A k-redundant program on an N-bit processor then runs as N/k parallel redundant slices. In this contribution, we combine these two benefits of bitslicing to implement a fault countermeasure for the number-theoretic transform (NTT). The NTT efficiently implements a polynomial multiplication. The internal symmetry of the NTT algorithm lends itself to a data-parallel implementation, and hence it is a good candidate for the redundantly bitsliced implementation. We implement a redundantly bitsliced NTT on an advanced 667MHz ARM Cortex-A9 processor, and study the fault coverage for the protected NTT under optimized electromagnetic fault injection (EMFI). Our work brings two major contributions. First, we show for the first time how to develop a redundantly bitsliced version of the NTT. We integrate the protected NTT into a full Dilithium signature sequence. Second, we demonstrate an EMFI analysis on a prototype implementation of the Dilithium signature sequence on ARM Cortex-M9. We perform a detailed EM fault-injection parameter search to optimize the location, intensity and timing of injected EM pulses. We demonstrate that, under optimized fault injection parameters, about 10% of the injected faults become potentially exploitable. However, the redundantly bitsliced NTT design is able to catch the majority of these potentially exploitable faults, even when the remainder of the Dilithium algorithm as well as the control flow is left unprotected. To our knowledge, this is the first demonstration of a bitslice-redundant design of the NTT that offers distributed fault detection throughout the execution of the algorithm.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesDilithium; ARM Cortex-A9; bit-slicing; electromagnetic fault injection; fault attack countermeasure; intra-instruction redundancy,ARM processors; Fault detection; Integrated circuit design; Program processors; Side channel attack; Software testing; Additional key word and phrasesdilithium; ARM cortex-a9; Bit-slicing; Cortexes; Dilithium; Electromagnetic Fault injections; Fault attack countermeasures; Intra-instruction redundancy; Key words; Number-theoretic transforms; Redundancy
"Side-channel and Fault-injection attacks over Lattice-based Post-quantum Schemes (Kyber, Dilithium): Survey and New Results",2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189465851&doi=10.1145%2f3603170&partnerID=40&md5=dd7a9baef5d2170969470419b0388516,"In this work, we present a systematic study of Side-Channel Attacks (SCA) and Fault Injection Attacks (FIA) on structured lattice-based schemes, with main focus on Kyber Key Encapsulation Mechanism (KEM) and Dilithium signature scheme, which are leading candidates in the NIST standardization process for Post Quantum Cryptography (PQC). Through our study, we attempt to understand the underlying similarities and differences between the existing attacks while classifying them into different categories. Given the wide variety of reported attacks, simultaneous protection against all the attacks requires to implement customized protections/countermeasures for both Kyber and Dilithium. We therefore present a range of customized countermeasures, capable of providing defenses/mitigations against existing SCA/FIA, and incorporate several SCA and FIA countermeasures within a single design of Kyber and Dilithium. Among the several countermeasures discussed in this work, we present novel countermeasures that offer simultaneous protection against several SCA- and FIA-based chosen-ciphertext attacks for Kyber KEM. We implement the presented countermeasures within two well-known public software libraries for PQC: (1) pqm4 library for the ARM Cortex-M4-based microcontroller and (2) liboqs library for the Raspberry Pi 3 Model B Plus based on the ARM Cortex-A53 processor. Our performance evaluation reveals that the presented custom countermeasures incur reasonable performance overheads on both the evaluated embedded platforms. We therefore believe our work argues for usage of custom countermeasures within real-world implementations of lattice-based schemes, either in a standalone manner or as reinforcements to generic countermeasures such as masking. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,ARM processors; Hardware security; Network security; Public key cryptography; Quantum cryptography; Software testing; Cortexes; Dilithium; Fault injection attacks; Key encapsulation mechanisms; Lattice-based; New results; Post quantum; Post quantum cryptography; Side-channel; Side-channel attacks; Side channel attack
Post-Quantum Signatures on RISC-V with Hardware Acceleration,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189448006&doi=10.1145%2f3579092&partnerID=40&md5=3009d2d397a87c9a67aa1f998c6f2b13,"CRYSTALS-Dilithium and Falcon are digital signature algorithms based on cryptographic lattices, which are considered secure even if large-scale quantum computers will be able to break conventional public-key cryp- tography. Both schemes have been selected for standardization in the NIST Post-Quantum competition. In this work, we present a RISC-V HW/SW codesign that aims to combine the advantages of software and hardware implementations, i.e., flexibility and performance. It shows the use of flexible hardware accelerators, which have been previously used for Public-Key Encryption (PKE) and Key-Encapsulation Mechanism (KEM), for Post-Quantum signatures. It is optimized for Dilithium as a generic signature scheme but also accelerates applications that require fast verification of Falcon's compact signatures. We provide a comparison with pre- vious works showing that for Dilithium and Falcon, cycle counts are significantly reduced, such that our design is faster than previous software implementations or other HW/SW codesigns. In addition to that, we present a compact Globalfoundries 22 nm ASIC design that runs at 800 M Hz . By using hardware accelera- tion, energy consumption for Dilithium is reduced by up to 92 . 2%, and up to 67 . 5% for Falcon's signature verification.  © 2024 Copyright held by the owner/author(s).",CRYSTALS-Dilithium; digital signatures; Falcon; HW/SW codesign; NIST PQC; Post-Quantum; RISC-V,Chromium compounds; Computer hardware; Electronic document identification systems; Energy utilization; Hardware-software codesign; Public key cryptography; Quantum computers; Quantum cryptography; CRYSTALS-dilithium; Dilithium; Falcon; Hardware acceleration; HW/SW Codesign; NIST PQC; Post quantum; Quantum signature; RISC-V; Software implementation; Authentication
Side-channel Analysis of Lattice-based Post-quantum Cryptography: Exploiting Polynomial Multiplication,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189246029&doi=10.1145%2f3569420&partnerID=40&md5=32a186ed52994c2984293bd007d408ea,"Polynomial multiplication algorithms such as Toom-Cook and the Number Theoretic Transform are fundamental building blocks for lattice-based post-quantum cryptography. In this work we present correlation power-analysis-based side-channel analysis methodologies targeting every polynomial multiplication strategy for all lattice-based post-quantum key encapsulation mechanisms in the final round of the NIST postquantum standardization procedure. We perform practical experiments on real side-channel measurements, demonstrating that ourmethod allows to extract the secret key fromall lattice-based post-quantum key encapsulation mechanisms. Our analysis shows that the used polynomial multiplication strategy can significantly impact the time complexity of the attack.  © 2023 Association for Computing Machinery.",number-theoretic transform; Post-quantum cryptography; side-channel analysis; Toom-Cook multiplication,Quantum cryptography; Side channel attack; Key encapsulation mechanisms; Lattice-based; Number-theoretic transforms; Polynomial multiplication; Post quantum; Post quantum cryptography; Quantum key; Side-channel analysis; Toom-Cook; Toom-cook multiplication; Polynomials
Agile Acceleration of Stateful Hash-based Signatures in Hardware,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189492550&doi=10.1145%2f3567426&partnerID=40&md5=a0078b2c33347737d9ef21daa114fe87,"With the development of large-scale quantum computers, the current landscape of asymmetric cryptographic algorithms will change dramatically. Today's standards like RSA, DSA, and ElGamal will no longer provide sufficient security against quantum attackers and need to be replaced with novel algorithms. In the face of these developments, NIST has already started a standardization process for new Key Encapsulation Mechanisms (KEMs) and Digital Signatures (DSs). Moreover, NIST has recommended the two stateful Hash-Based Signatures (HBSs) schemes XMSS and LMS for use in devices with a long expected lifetime and limited capabilities for maintenance. Both schemes are also standardized by the IETF. In this work, we present the first agile hardware implementation that supports both LMS and XMSS. Our design can instantiate either LMS, XMSS, or both schemes using a simple configuration setting. Leveraging the vast similarities of the two schemes, the hardware utilization of the agile design increases by 20% in LUTs and only 3% in Flip Flops (FFs) over a standalone XMSS implementation. Furthermore, our approach can easily be configured with an arbitrary number of hash cores and accelerators for the one-time signatures for different application scenarios. We evaluate our implementation on the Xilinx Artix-7 FPGA platform, which is the recommended target for PQC implementations by NIST. We explore potential tradeoffs in the design space and compare our results to previous work in this field.  © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesXMSS; hardware implementation; LMS; post-quantum cryptography,Computer hardware; Hardware security; Network security; Public key cryptography; Quantum computers; Quantum cryptography; Quantum theory; 'current; Additional key word and phrasesxmss; Cryptographic algorithms; ElGamal; Hardware implementations; Key words; Large scale quantum computers; LMS; Novel algorithm; Post quantum cryptography; Flip flop circuits
LL-GNN: Low Latency Graph Neural Networks on FPGAs for High Energy Physics,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189295845&doi=10.1145%2f3640464&partnerID=40&md5=0b25a5b099f92ad2850d273cedac1f53,"This work presents a novel reconfigurable architecture for Low Latency Graph Neural Network (LL-GNN) designs for particle detectors, delivering unprecedented low latency performance. Incorporating FPGA-based GNNs into particle detectors presents a unique challenge since it requires sub-microsecond latency to deploy the networks for online event selection with a data rate of hundreds of terabytes per second in the Level-1 triggers at the CERN Large Hadron Collider experiments. This article proposes a novel outer-product based matrix multiplication approach, which is enhanced by exploiting the structured adjacency matrix and a column-major data layout. In addition, we propose a custom code transformation for the matrix multiplication operations, which leverages the structured sparsity patterns and binary features of adjacency matrices to reduce latency and improve hardware efficiency. Moreover, a fusion step is introduced to further reduce the end-to-end design latency by eliminating unnecessary boundaries. Furthermore, a GNN-specific algorithmhardware co-design approach is presented which not only finds a design with a much better latency but also finds a high accuracy design under given latency constraints. To facilitate this, a customizable template for this low latency GNN hardware architecture has been designed and open-sourced, which enables the generation of low-latency FPGA designs with efficient resource utilization using a high-level synthesis tool. Evaluation results show that our FPGA implementation is up to 9.0 times faster and achieves up to 13.1 times higher power efficiency than a GPU implementation. Compared to the previous FPGA implementations, this work achieves 6.51 to 16.7 times lower latency. Moreover, the latency of our FPGA design is sufficiently low to enable deployment of GNNs in a sub-microsecond, real-time collider trigger system, enabling it to benefit from improved accuracy. The proposed LL-GNN design advances the next generation of trigger systems by enabling sophisticated algorithms to process experimental data efficiently. © 2024 Copyright held by the owner/author(s).",Domain specific hardware architecture; graph neural network; low latency,Cosmology; High energy physics; Matrix algebra; Network architecture; Reconfigurable architectures; Adjacency matrix; Domain specific; Domain specific hardware architecture; FPGA design; Graph neural networks; Hardware architecture; Low latency; Neural network designs; Specific hardware; Sub-microsecond; Field programmable gate arrays (FPGA)
Stash: Flexible Energy Storage for Intermittent Sensors,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189428066&doi=10.1145%2f3641511&partnerID=40&md5=7654e4a586de6ce27dce57c8e6dcfdf3,"Batteryless sensors promise a sustainable future for sensing, but they face significant challenges when storing and using environmental energy. Incoming energy can fluctuate unpredictably between periods of scarcity and abundance, and device performance depends on both incoming energy and how much a device can store. Existing batteryless devices have used fixed or run-time selectable front-end capacitor banks to meet the energy needs of different tasks. Neither approach adapts well to rapidly changing energy harvesting conditions, nor does it allow devices to store excess energy during times of abundance without sacrificing performance. This article presents Stash, a hardware back-end energy storage technique that allows batteryless devices to charge quickly and store excess energy when it is abundant, extending their operating time and carrying out additional tasks without compromising the main ones. Stash performs like a small capacitor device when small capacitors excel and like a large capacitor device when large capacitors excel, with no additional software complexity and negligible power overhead. We evaluate Stash using two applicationsâĂŤtemperature sensing and wearable activity monitoringâĂŤunder both synthetic solar energy and recorded solar and thermal traces from various human activities. Our results show that Stash increased sensor coverage by up to 15% under variable energy-harvesting conditions when compared to competitor configurations that used fixed small, large, and reconfigurable front-end energy storage. © 2024 Copyright held by the owner/author(s).",Batteryless; energy harvesting; intermittent computing,Energy storage; Solar energy; Battery-less; Condition; Device performance; Environmental energy; Excel; Excess energy; Fixed time; Front end; Incoming energy; Intermittent computing; Energy harvesting
Compact Instruction Set Extensions for Dilithium,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189478679&doi=10.1145%2f3643826&partnerID=40&md5=8f06be87c494c00e8dd195fde3e96302,"Post-quantum cryptography is considered to provide security against both traditional and quantum computer attacks. Dilithium is a digital signature algorithm that derives its security from the challenge of finding short vectors in lattices. It has been selected as one of the standardizations in the NIST post-quantum cryptography project. Hardware-software co-design is a commonly adopted implementation strategy to address various implementation challenges, including limited resources, high performance, and flexibility requirements. In this study, we investigate using compact instruction set extensions (ISEs) for Dilithium, aiming to improve software efficiency with low hardware overheads. To begin with, we propose tightly coupled accelerators that are deeply integrated into the RISC-V processor. These accelerators target the most computationally demanding components in resource-constrained processors, such as polynomial generation, Number Theoretic Transform (NTT), and modular arithmetic. Next, we design a set of custom instructions that seamlessly integrate with the RISC-V base instruction formats, completing the accelerators in a compact manner. Subsequently, we implement our ISEs in a chip design for the Hummingbird E203 core and conduct performance benchmarks for Dilithium utilizing these ISEs. Additionally, we evaluate the resource consumption of the ISEs on FPGA and ASIC technologies. Compared to the reference software implementation on the RISC-V core, our co-design demonstrates a remarkable speedup factor ranging from 6.95 to 9.96. This significant improvement in performance is achieved by incorporating additional hardware resources, specifically, a 35% increase in LUTs, a 14% increase in FFs, 7 additional DSPs, and no additional RAM. Furthermore, compared to the state-of-the-art approach, our work achieves faster speed performance with a reduced circuit cost. Specifically, the usage of additional LUTs, FFs, and RAMs is reduced by 47.53%, 50.43%, and 100%, respectively. On ASIC technology, our approach demonstrates 12, 412 cell counts. Our co-design provides a better tradeoff implementation on speed performance and circuit overheads.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesPost-quantum cryptography; Dilithium; instruction set extensions; RISC-V,Benchmarking; Computer hardware; Hardware security; Integrated circuit design; Quantum cryptography; Additional key word and phrasespost-quantum cryptography; ASIC technologies; Co-designs; Dilithium; Instruction set extension; Key words; Performance; Post quantum cryptography; RISC-V; Speed performance; Hardware-software codesign
PolyARBerNN: A Neural Network Guided Solver and Optimizer for Bounded Polynomial Inequalities,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189463913&doi=10.1145%2f3632970&partnerID=40&md5=4ec7768e7a98b61b34c834a9ec6113e8,"Constraints solvers play a significant role in the analysis, synthesis, and formal verification of complex cyberphysical systems. In this article, we study the problem of designing a scalable constraints solver for an important class of constraints named polynomial constraint inequalities (also known as nonlinear real arithmetic theory). In this article, we introduce a solver named PolyARBerNN that uses convex polynomials as abstractions for highly nonlinears polynomials. Such abstractions were previously shown to be powerful to prune the search space and restrict the usage of sound and complete solvers to small search spaces. Compared with the previous efforts on using convex abstractions, PolyARBerNN provides three main contributions namely (i) a neural network guided abstraction refinement procedure that helps selecting the right abstraction out of a set of pre-defined abstractions, (ii) a Bernstein polynomial-based search space pruning mechanism that can be used to compute tight estimates of the polynomial maximum and minimum values which can be used as an additional abstraction of the polynomials, and (iii) an optimizer that transforms polynomial objective functions into polynomial constraints (on the gradient of the objective function) whose solutions are guaranteed to be close to the global optima. These enhancements together allowed the PolyARBerNN solver to solve complex instances and scales more favorably compared to the state-of-the-art nonlinear real arithmetic solvers while maintaining the soundness and completeness of the resulting solver. In particular, our test benches show that PolyARBerNN achieved 100X speedup compared with Z3 8.9, Yices 2.6, and PVS (a solver that uses Bernstein expansion to solve multivariate polynomial constraints) on a variety of standard test benches. Finally, we implemented an optimizer called PolyAROpt that uses PolyARBerNN to solve constrained polynomial optimization problems. Numerical results show that PolyAROpt is able to solve high-dimensional and high order polynomial optimization problems with higher speed compared to the built-in optimizer in the Z3 8.9 solver. © 2024 Copyright held by the owner/author(s).",abstraction refinement; bernstein polynomials; Neural networks,Abstracting; Complex networks; Constrained optimization; Embedded systems; Abstraction-refinement; Bernstein polynomial; Constraint solvers; Neural-networks; Objective functions; Optimizers; Polynomial optimization problem; Real arithmetic; Search spaces; Test-bench; Polynomials
Intelligent Caching for Vehicular Dew Computing in Poor Network Connectivity Environments,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189450897&doi=10.1145%2f3643038&partnerID=40&md5=1676225dedd0b87abd5b42b70a8e7ffb,"In vehicular networks, some edge servers may not function properly due to the time-varying load condition and the uneven computing resource distribution, resulting in a low quality of caching services. To overcome this challenge, we develop a Vehicular dew computing (VDC) architecture for the first time by combining dew computing with vehicular networks, which can achieve wireless communication between vehicles in a resource-constrained environment. Consequently, it is crucial to develop an adaptive caching scheme that empowers vehicles to form efficient cooperation in VDC. In this paper, we propose an intelligent caching scheme based on VDC architecture, which includes two parts. First, to meet the dynamic nature of VDC, a spatiotemporal vehicle clustering algorithm is proposed to establish adaptive cooperation to assist content caching for vehicles. Second, the multi-armed bandit algorithm is employed to select suitable content for caching in vehicles based on real-time file popularity, and a model is established to dynamically update each vehicle's request preferences. Extensive experiments are conducted to demonstrate that the proposed scheme has excellent performance in terms of cluster head stability and cache hit rate.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesVehicular dew computing; edge caching; reinforcement learning; spatiotemporal vehicle cluster,Cluster computing; Clustering algorithms; Computer architecture; Network architecture; Vehicle to vehicle communications; Vehicles; Additional key word and phrasesvehicular dew computing; Caching scheme; Computing architecture; Edge caching; Edge server; Key words; Network connectivity; Reinforcement learnings; Spatiotemporal vehicle cluster; Vehicular networks; Reinforcement learning
Flexible Updating of Internet of Things Computing Functions through Optimizing Dynamic Partial Reconfiguration,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189453980&doi=10.1145%2f3643825&partnerID=40&md5=22675327c6499392b379405fc2198faf,"With applications to become increasingly compute- and data-intensive, requiring more processing power, many Internet of Things (IoT) platforms in robots, drones, and autonomous vehicles that implement neural network inference, cryptographic functions or signal processing (e.g., multimedia, communication), employ field programmable gate arrays (FPGAs). At the same time, dynamic partial reconfiguration (DPR) in modern FPGAs enable changing the function of a part of the FPGA by dynamically loading new bitstreams to the logic regions without affecting the function of other parts of the FPGA. This is especially useful to update functions of IoT devices while in operation for bug fixing or functionality adjustments and, more importantly, when these IoT devices integrate low-cost FPGAs that can hardly realize many hard accelerators. To deal with one of the major limitations of using partial reconfiguration in IoT devices, this work introduces techniques to flexibly use DPR, namely, FLEXDPR, by sharing reconfigurable partitions among different accelerator functions and by supporting virtual relocation of these functions. Experimental results on the Xilinx Zynq-7000 platform reveal energy and latency efficiency improvements of about 20%, on average. Overall, the suggested approach can reduce partial reconfiguration overhead while easing the scheduler's decisions for the deployment of hardware functions throughout time and space in a performance-conscious manner.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSharing reconfigurable partitions; event-driven simulator on FPGA; partial reconfiguration scheduler; relocatable accelerators,Computation theory; Dynamic loads; Field programmable gate arrays (FPGA); Signal processing; Vehicle to vehicle communications; Additional key word and phrasessharing reconfigurable partition; Event-driven simulator on field programmable gate array; Event-driven simulators; Field programmables; Key words; Partial reconfiguration; Partial reconfiguration scheduler; Programmable gate array; Reconfigurable; Relocatable accelerator; Internet of things
A Space-Grained Cleaning Method to Reduce Long-Tail Latency of DM-SMR Disks,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189487196&doi=10.1145%2f3643827&partnerID=40&md5=5f4f9e09a4c39a0ea312d646d44c1fa4,"DM-SMR (device-managed shingled magnetic recording) disks allocate a portion of disk space as the persistent cache (PC) to address the issue of overlapping tracks during data updates. When the PC space becomes insufficient, a space cleaning is triggered to reclaim its invalid space. However, the space cleaning is time-consuming and contributes to the long-tail latency of DM-SMR disks. In the article, we will propose a space-grained cleaning method that leverages various idle periods to effectively reduce the long-tail latency of DM-SMR disks. The objective is to perform a proper space-grained cleaning for a suitable space region at an appropriate time period, thereby preventing delays in subsequent I/O requests and reducing the long-tail latency associated with DM-SMR disks. The experimental results demonstrate a substantial reduction in the long-tail latency of DM-SMR disks through the proposed method.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesShingled magnetic recording (SMR) disks; long-tail latency; space-grained cleaning,Magnetic recording; Additional key word and phrasesshingled magnetic recording (SMR) disk; Cleaning methods; Data update; Disc space; Key words; Long tail; Long-tail latency; Magnetic recording disks; Shingled magnetic recordings; Space-grained cleaning; Cleaning
Adversarial Transferability in Embedded Sensor Systems: An Activity Recognition Perspective,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189439766&doi=10.1145%2f3641861&partnerID=40&md5=66947cb804922824aad4af8d04ad124b,"Machine learning algorithms are increasingly used for inference and decision-making in embedded systems. Data from sensors are used to train machine learning models for various smart functions of embedded and cyber-physical systems ranging from applications in healthcare, autonomous vehicles, and national security. However, recent studies have shown that machine learning models can be fooled by adding adversarial noise to their inputs. The perturbed inputs are called adversarial examples. Furthermore, adversarial examples designed to fool one machine learning system are also often effective against another system. This property of adversarial examples is called adversarial transferability and has not been explored in wearable systems to date. In this work, we take the first stride in studying adversarial transferability in wearable sensor systems from four viewpoints: (1) transferability between machine learning models; (2) transferability across users/subjects of the embedded system; (3) transferability across sensor body locations; and (4) transferability across datasets used for model training. We present a set of carefully designed experiments to investigate these transferability scenarios. We also propose a threat model describing the interactions of an adversary with the source and target sensor systems in different transferability settings. In most cases, we found high untargeted transferability, whereas targeted transferability success scores varied from 0% to 80%. The transferability of adversarial examples depends on many factors such as the inclusion of data from all subjects, sensor body position, number of samples in the dataset, type of learning algorithm, and the distribution of source and target system dataset. The transferability of adversarial examples decreased sharply when the data distribution of the source and target system became more distinct. We also provide guidelines and suggestions for the community for designing robust sensor systems. Code and dataset used in our analysis is publicly available here.1  © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesSensor systems; adversarial machine learning; adversarial transferability; human activity recognition,Automobile bodies; Cyber Physical System; Decision making; Inference engines; Learning algorithms; Machine learning; National security; Pattern recognition; Wearable sensors; Additional key word and phrasessensor system; Adversarial machine learning; Adversarial transferability; Embedded-system; Human activity recognition; Key words; Machine learning models; Machine-learning; Sensor body; Sensor systems; Embedded systems
STDF: Spatio-Temporal Deformable Fusion for Video Quality Enhancement on Embedded Platforms,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189445766&doi=10.1145%2f3645113&partnerID=40&md5=35f8a29e3b3e10d94f1f55e4fdc28c39,"With the development of embedded systems and deep learning, it is feasible to combine them for offering various and convenient human-centered services, which is based on high-quality (HQ) videos. However, due to the limit of video traffic load and unavoidable noise, the visual quality of an image from an edge camera may degrade significantly, influencing the overall video and service quality. To maintain video stability, video quality enhancement (QE), aiming at recovering HQ videos from their distorted low-quality (LQ) sources, has aroused increasing attention in recent years. The key challenge for video QE lies in how to effectively aggregate complementary information from multiple frames (i.e., temporal fusion). To handle diverse motion in videos, existing methods commonly apply motion compensation before the temporal fusion. However, the motion field estimated from the distorted LQ video tends to be inaccurate and unreliable, thereby resulting in ineffective fusion and restoration. In addition, motion estimation for consecutive frames is generally conducted in a pairwise manner, which leads to expensive and inefficient computation. In this article, we propose a fast yet effective temporal fusion scheme for video QE by incorporating a novel Spatio-Temporal Deformable Convolution (STDC) to simultaneously compensate motion and aggregate temporal information. Specifically, the proposed temporal fusion scheme takes a target frame along with its adjacent reference frames as input to jointly estimate an offset field to deform the spatio-temporal sampling positions of convolution. As a result, complementary information from multiple frames can be fused within the STDC operation in one forward pass. Extensive experimental results on three benchmark datasets show that our method performs favorably to the state of the art in terms of accuracy and efficiency.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesVideo restoration; deep neural network; embedded systems; quality enhancement,Aggregates; Convolution; Deep neural networks; Embedded systems; Image reconstruction; Motion compensation; Motion estimation; Additional key word and phrasesvideo restoration; Embedded platforms; Embedded-system; High-quality videos; Key words; Low qualities; Multiple-frame; Quality enhancement; Spatio-temporal; Video quality enhancements; Restoration
Special Issue on Post-Quantum Cryptography for Embedded Systems,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189438010&doi=10.1145%2f3641852&partnerID=40&md5=cb78ed706d66db7a78b252da3cd329c7,[No abstract available],,
Robust Embedded Autonomous Driving Positioning System Fusing LiDAR and Inertial Sensors,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184781671&doi=10.1145%2f3626098&partnerID=40&md5=45e6d11812976699d990d41aafdc06bf,"Autonomous driving emphasizes precise multi-sensor fusion positioning on limit resource embedded systems. LiDAR-centered sensor fusion system serves as a mainstream navigation system due to its insensitivity to illumination and viewpoint change. However, these types of systems suffer from handling large-scale sequential LiDAR data using limited resources on board, leading LiDAR-centralized sensor fusion unpractical. As a result, hand-crafted features such as plane and edge are leveraged in majority mainstream positioning methods to alleviate this unsatisfaction, triggering a new cornerstone in LiDAR Inertial sensor fusion. However, such super light weight feature extraction, although it achieves real-time constraint in LiDAR-centered sensor fusion, encounters severe vulnerability under high speed rotational or translational perturbation. In this paper, we propose a sparse tensor based LiDAR Inertial fusion method for autonomous driving embedded system. Leveraging the power of sparse tensor, the global geometrical feature is fetched so that the point cloud sparsity defect is alleviated. Inertial sensor is deployed to conquer the time-consuming step caused by the coarse level point-wise inlier matching. We construct our experiments on both representative dataset benchmarks and realistic scenes. The evaluation results show the robustness and accuracy of our proposed solution compared to classical methods. © 2024 Copyright held by the owner/author(s).",autonomous driving; embedded system; fault-tolerance algorithms; Sensor fusion,Autonomous vehicles; Fault tolerance; Inertial navigation systems; Optical radar; Sensor data fusion; Tensors; Autonomous driving; Embedded-system; Fault-tolerance algorithms; Inertial sensor; Large-scales; Multi-sensor fusion; Positioning system; Sensor fusion; Sensor fusion systems; Sparse tensors; Embedded systems
Multi-Compression Scale DNN Inference Acceleration based on Cloud-Edge-End Collaboration,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184747915&doi=10.1145%2f3634704&partnerID=40&md5=cf453da534aa3b7cdee255f8ccfb3a1b,"Edge intelligence has emerged as a promising paradigm to accelerate DNN inference by model partitioning, which is particularly useful for intelligent scenarios that demand high accuracy and low latency. However, the dynamic nature of the edge environment and the diversity of end devices pose a significant challenge for DNN model partitioning strategies. Meanwhile, limited resources of the edge server make it difficult to manage resource allocation efficiently among multiple devices. In addition, most of the existing studies disregard the different service requirements of the DNN inference tasks, such as its high accuracy-sensitive or high latency-sensitive. To address these challenges, we propose a Multi-Compression Scale DNN Inference Acceleration (MCIA) based on cloud-edge-end collaboration. We model this problem as a mixed-integer multi-dimensional optimization problem, jointly optimizing the DNN model version choice, the partitioning choice, and the allocation of computational and bandwidth resources to maximize the tradeoff between inference accuracy and latency depending on the property of the tasks. Initially, we train multiple versions of DNN inference models with different compression scales in the cloud, and deploy them to end devices and edge server. Next, a deep reinforcement learning-based algorithm is developed for joint decision making of adaptive collaborative inference and resource allocation based on the current multi-compression scale models and the task property. Experimental results show that MCIA can adapt to heterogeneous devices and dynamic networks, and has superior performance compared with other methods. © 2024 Copyright held by the owner/author(s).",deep reinforcement learning; DNN inference; Edge intelligence; model partitioning; multi-compression scale,Decision making; Deep learning; Inference engines; Integer programming; Learning systems; Reinforcement learning; Deep reinforcement learning; DNN inference; Edge intelligence; Edge server; End-devices; High-accuracy; Model partitioning; Multi-compression; Multi-compression scale; Reinforcement learnings; Resource allocation
Hierarchical Resource Orchestration Framework for Real-time Containers,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177811974&doi=10.1145%2f3592856&partnerID=40&md5=8a9bff9bcaab03b7723fa188ae631716,"Container-based virtualization is a promising deployment model in fog and edge computing applications, because it allows a seamless co-existence of virtualized applications in a heterogeneous environment without introducing significant overhead. Certain application domains (e.g., industrial automation, automotive, or aerospace) mandate that applications exhibit a certain degree of temporal predictability. Container-based virtualization cannot be easily used for such applications, since the technology is not designed to support real-time properties and handle temporal disturbances. This article proposes a framework consisting of a static offline and a dynamic online phase for resource allocation and adaptive re-dimensioning of real-time containers. In the offline phase, the optimal initial deployment and dimensioning of containers are decided based on ideal system models. Additionally, to adapt to dynamic variations caused by changing workloads or interferences, the online phase adapts the CPU usage and limits of real-time containers at runtime to improve the real-time behavior of the real-time containerized applications while optimizing resource usage. We implement the framework in a real Linux-based system and show through a series of experiments that the proposed framework is able to adjust and re-distribute computing resources between containers to improve the real-time behavior of containerized applications in the presence of temporal disturbances while optimizing resource usage. © 2024 Copyright held by the owner/author(s).",real-time; Real-time container-based virtualization; real-time docker,Computer operating systems; Containers; Virtual reality; Hierarchical resource; Offline; Real time behavior; Real- time; Real-time container-based virtualization; Real-time docker; Resource usage; Virtualizations; Virtualization
Criticality-aware Monitoring and Orchestration for Containerized Industry 4.0 Environments,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184771638&doi=10.1145%2f3604567&partnerID=40&md5=c193d9bbbcca12926307c934352455c8,"The evolution of industrial environments makes the reconfigurability and flexibility key requirements to rapidly adapt to changeable market needs. Computing paradigms like Edge/Fog computing are able to provide the required flexibility and scalability while guaranteeing low latencies and response times. Orchestration systems play a key role in these environments, enforcing automatic management of resources and workloads’ lifecycle, and drastically reducing the need for manual interventions. However, they do not currently meet industrial non-functional requirements, such as real-timeliness, determinism, reliability, and support for mixed-criticality workloads. In this article, we present k4.0s, an orchestration system for Industry 4.0 (I4.0) environments, which enables the support for real-time and mixed-criticality workloads. We highlight through experiments the need for novel monitoring approaches and propose a workflow for selecting monitoring metrics, which depends on both workload requirements and hosting node guarantees. We introduce new abstractions for the components of a cluster in order to enable criticality-aware monitoring and orchestration of real-time industrial workloads. Finally, we design an orchestration system architecture that reflects the proposed model, introducing new components and prototyping a Kubernetes-based implementation, taking the first steps towards a fully I4.0-enabled orchestration system. © 2024 Copyright held by the owner/author(s).",containers; Industry 4.0; mixed criticality; orchestration; Real time,Criticality (nuclear fission); Industry 4.0; Life cycle; Automatic management; Computing paradigm; Industrial environments; Low latency; Manual intervention; Market needs; Mixed criticalities; Orchestration; Real- time; Reconfigurability; Containers
Multi-criteria Optimization of Real-time DAGs on Heterogeneous Platforms under P-EDF,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184773160&doi=10.1145%2f3592609&partnerID=40&md5=ce32164f79d5a13804d65fa76ef33fa0,"This article tackles the problem of optimal placement of complex real-time embedded applications on heterogeneous platforms. Applications are composed of directed acyclic graphs of tasks, with each directed-acyclic-graph (DAG) having a minimum inter-arrival period for its activation requests and an end-to-end deadline within which all of the computations need to terminate since each activation. The platforms of interest are heterogeneous power-aware multi-core platforms with Dynamic Voltage and Frequency Scaling (DVFS) capabilities, including big.LITTLE Arm architectures and platforms with GPU or Field Programmable Gate Array (FPGA) hardware accelerators with Dynamic Partial Reconfiguration capabilities. Tasks can be deployed on CPUs using partitioned Earliest Deadline First (EDF)-based scheduling. Additionally, some of the tasks may have an alternate implementation available for one of the accelerators on the target platform, which are assumed to serve requests in non-preemptive FIFO order. The system can be optimized by minimizing power consumption, respecting precise timing constraints, maximizing the applications’ slack, respecting given power consumption constraints, or even a combination of these, in a multi-objective formulation. We propose an off-line optimization of the mentioned problem based on mixed-integer quadratic constraint programming (MIQCP). The optimization provides the DVFS configuration of all the CPUs (or accelerators) capable of frequency switching and the placement to be followed by each task in the DAGs, including the software-vs.-hardware implementation choice for tasks that can be hardware accelerated. For relatively big problems, we developed heuristic solvers capable of providing suboptimal solutions in a significantly reduced time compared to the MIQCP strategy, thus widening the applicability of the proposed framework. We validate the approach by running a set of randomly generated DAGs on Linux under SCHED_DEADLINE, deployed onto two real boards, one with Arm big.LITTLE architecture, the other with FPGA acceleration, verifying that the experimental runs meet the theoretical expectations in terms of timing and power optimization goals. © 2024 Copyright held by the owner/author(s).",DVFS; End-to-end timing requirements; energy efficiency; heuristic optimization; Linux kernel; mixed-integer quadratic constraint programming; SCHED_DEADLINE; software-to-hardware mapping,Chemical activation; Clocks; Constraint programming; Constraint theory; Directed graphs; Dynamic frequency scaling; Electric power utilization; Embedded systems; Field programmable gate arrays (FPGA); Integer programming; Memory architecture; Multiobjective optimization; Program processors; Real time systems; Scheduling algorithms; System-on-chip; Timing circuits; Voltage scaling; Constraint programming; Dynamic voltage and frequency scaling; End to end; End-to-end timing requirement; Heuristic optimization; Linux kernel; Mixed integer; Mixed-integer quadratic constraint programming; Quadratic constraint; SCHED_DEADLINE; Software-to-hardware mapping; Timing requirements; Energy efficiency
Secure and Lightweight Blockchain-based Truthful Data Trading for Real-Time Vehicular Crowdsensing,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184741872&doi=10.1145%2f3582008&partnerID=40&md5=950655d55c69040ec93a5969e841f0e5,"As the number of smart cars grows rapidly, vehicular crowdsensing (VCS) is gradually becoming popular. In a VCS infrastructure, sensing devices and computing units hold on smart cars as well as cloud servers form an IoT-edge-cloud continuum to perform real-time sensing tasks. In order to encourage the smart cars to participate in the real-time VCS process, blockchain technology can be combined with VCS to provide an automated incentive for VCS data trading without relying on trusted third parties. However, directly using blockchain to enforce the VCS data trading process incurs expensive service fees and participants still can conduct various misbehavior. In this article, we propose a secure blockchain-based data trading system for VCS named BTT system to address the above issues. In particular, we first integrate the blockchain-based data trading process with a lightweight privacy-preserving truth discovery algorithm to ensure the accuracy of sensing data while preserving data privacy. We then propose a gas-aware optimization mechanism to minimize the gas consumption of the data trading process. Finally, we carefully design a distributed judgment mechanism to regulate all participants to behave correctly in the data trading process. To demonstrate the practicability of our design, we implement a prototype of the BTT system deployed on an Ethereum test network and conduct extensive simulations. © 2024 Copyright held by the owner/author(s).",auction; blockchain; edge computing; privacy-preserving truth discovery; real-time sensing tasks; Vehicle crowdsensing,Edge computing; Privacy-preserving techniques; Auction; Block-chain; Edge computing; Privacy preserving; Privacy-preserving truth discovery; Real time sensing; Real-time sensing task; Sensing tasks; Smart car; Vehicle crowdsensing; Blockchain
Modeling and Analysis of ETC Control System with Colored Petri Net and Dynamic Slicing,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184785012&doi=10.1145%2f3633450&partnerID=40&md5=108d466f3468d87839008713df9a345d,"Nowadays, Electronic Toll Collection (ETC) control systems have been widely adopted to smoothen traffic flow on highways. However, as it is a complex business interaction system, there are inevitably flaws in its control logic process, such as the problem of vehicle fee evasion. We find that there is more than one way for vehicles to evade fees. This shows that it is difficult to ensure the completeness of its design. Therefore, it is necessary to adopt a novel formal method to model and analyze its design, detect flaws, and modify it. In this article, a Colored Petri net (CPN) is introduced to establish its model. To analyze and modify the system model more efficiently, a dynamic slicing method of CPN is proposed. First, a static slice is obtained from the static slicing criterion by backtracking. Second, considering all binding elements that can be enabled under the initial marking, a forward slice is obtained from the dynamic slicing criterion by traversing. Third, the dynamic slicing of CPN is obtained by taking the intersection of both slices. The proposed dynamic slicing method of CPN can be used to formalize and verify the behavior properties of an ETC control system, and the flaws can be detected effectively. As a case study, the flaw about a vehicle that has not completed the payment following the previous vehicle to pass the railing is detected by the proposed method. © 2024 Copyright held by the owner/author(s).",Colored Petri net; dynamic slicing; ETC control system; formalized analysis,Control systems; Petri nets; Toll highways; Vehicles; Business interactions; Colored Petri Nets; Dynamic slicing; Electronic toll collection; Electronic toll collection control system; Formalized analyse; Modelling and analysis; Slicing criterion; Slicing methods; Traffic flow; Formal methods
PArtNNer: Platform-Agnostic Adaptive Edge-Cloud DNN Partitioning for Minimizing End-to-End Latency,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184771693&doi=10.1145%2f3630266&partnerID=40&md5=3193f24385f29afba7ed3c6f91a3c56d,"The last decade has seen the emergence of Deep Neural Networks (DNNs) as the de facto algorithm for various computer vision applications. In intelligent edge devices, sensor data streams acquired by the device are processed by a DNN application running on either the edge device itself or in the cloud. However, “edge-only” and “cloud-only” execution of State-of-the-Art DNNs may not meet an application’s latency requirements due to the limited compute, memory, and energy resources in edge devices, dynamically varying bandwidth of edge-cloud connectivity networks, and temporal variations in the computational load of cloud servers. This work investigates distributed (partitioned) inference across edge devices (mobile/end device) and cloud servers to minimize end-to-end DNN inference latency. We study the impact of temporally varying operating conditions and the underlying compute and communication architecture on the decision of whether to run the inference solely on the edge, entirely in the cloud, or by partitioning the DNN model execution among the two. Leveraging the insights gained from this study and the wide variation in the capabilities of various edge platforms that run DNN inference, we propose PArtNNer, a platform-agnostic adaptive DNN partitioning algorithm that finds the optimal partitioning point in DNNs to minimize inference latency. PArtNNer can adapt to dynamic variations in communication bandwidth and cloud server load without requiring pre-characterization of underlying platforms. Experimental results for six image classification and object detection DNNs on a set of five commercial off-the-shelf compute platforms and three communication standards indicate that PArtNNer results in 10.2× and 3.2× (on average) and up to 21.1× and 6.7× improvements in end-to-end inference latency compared to execution of the DNN entirely on the edge device or entirely on a cloud server, respectively. Compared to pre-characterization-based partitioning approaches, PArtNNer converges to the optimal partitioning point 17.6× faster. © 2024 Copyright held by the owner/author(s).",collaborative AI; Deep learning; edge inference; real-time computing,Bandwidth; Cloud computing; Energy resources; Inference engines; Object detection; Cloud servers; Collaborative AI; Deep learning; Edge clouds; Edge inference; End to end; Network inference; Network partitioning; Optimal partitioning; Real-time computing; Deep neural networks
Minimal-Overlap Centrality for Multi-Gateway Designation in Real-Time TSCH Networks,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184742850&doi=10.1145%2f3610583&partnerID=40&md5=fbccdbfda50c12f9e33612676015ddf2,"This article presents a novel centrality-driven gateway designation framework for the improved real-time performance of low-power wireless sensor networks (WSNs) at system design time. We target time-synchronized channel hopping (TSCH) WSNs with centralized network management and multiple gateways with the objective of enhancing traffic schedulability by design. To this aim, we propose a novel network centrality metric termed minimal-overlap centrality that characterizes the overall number of path overlaps between all the active flows in the network when a given node is selected as gateway. The metric is used as a gateway designation criterion to elect as a gateway the node leading to the minimal number of overlaps. The method is then extended to multiple gateways with the aid of the unsupervised learning method of spectral clustering. Concretely, after a given number of clusters are identified, we use the new metric at each cluster to designate as cluster gateway the node with the least overall number of overlaps. Extensive simulations with random topologies under centralized earliest-deadline-first (EDF) scheduling and shortest-path routing suggest our approach is dominant over traditional centrality metrics from social network analysis, namely, eigenvector, closeness, betweenness, and degree. Notably, our approach reduces by up to 40% the worst-case end-to-end deadline misses achieved by classical centrality-driven gateway designation methods. © 2024 Copyright held by the owner/author(s).",Earliest-deadline-first (EDF); gateway selection; time-synchronized channel hopping (TSCH),Learning systems; Real time systems; Response time (computer systems); Sensor nodes; Unsupervised learning; Channel hopping; Earliest deadline first; Early-deadline-first; Gateway selection; Hopping networks; Low power wireless sensor networks; Multiple gateways; Real time performance; Real- time; Time-synchronized channel hopping; Clustering algorithms
COBRRA: COntention-aware cache Bypass with Request-Response Arbitration,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184749482&doi=10.1145%2f3632748&partnerID=40&md5=46b3dc3aecffba0a79c0e0cce40356dd,"In modern multi-processor systems-on-chip (MPSoCs), requests from different processor cores, accelerators, and their responses from the lower-level memory contend for the shared cache bandwidth, making it a critical performance bottleneck. Prior research on shared cache management has considered requests from cores but has ignored crucial contributions from their responses. Prior cache bypass techniques focused on data reuse and neglected the system-level implications of shared cache contention. We propose COBRRA, a novel shared cache controller policy that mitigates the contention by aggressively bypassing selected responses from the lower-level memory and scheduling the remaining requests and responses to the cache efficiently. COBRRA is able to improve the average performance of a set of 15 SPEC workloads by 49% and 33% compared to the no-bypass baseline and the best-performing state-of-the-art bypass solution, respectively. Furthermore, COBRRA reduces the overall cache energy consumption by 38% and 31% compared to the no-bypass baseline and the most energy-efficient state-of-the-art bypass solution, respectively. © 2024 Copyright held by the owner/author(s).",Cache contention; Request; Request response arbitration; Response; Response bypass; Shared caches,Cache memory; Energy utilization; Multiprocessing systems; System-on-chip; Cache contention; Contention-aware; Multi processor system on chips; Processor cores; Request; Request response arbitration; Response; Response bypass; Shared cache; State of the art; Energy efficiency
Introduction to the Special Issue on Real-Time Computing in the IoT-to-Edge-to-Cloud Continuum,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184762325&doi=10.1145%2f3605180&partnerID=40&md5=e0325e7c6ef7bf2563f52d34b7853bd4,[No abstract available],,
Virtual Environment Model Generation for CPS Goal Verification using Imitation Learning,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184775945&doi=10.1145%2f3633804&partnerID=40&md5=dd38d0584a506a1238112a35edf1e5e4,"Cyber-Physical Systems (CPS) continuously interact with their physical environments through embedded software controllers that observe the environments and determine actions. Field Operational Tests (FOT) are essential to verify to what extent the CPS under analysis can achieve certain CPS goals, such as satisfying the safety and performance requirements, while interacting with the real operational environment. However, performing many FOTs to obtain statistically significant verification results is challenging due to its high cost and risk in practice. Simulation-based verification can be an alternative to address the challenge, but it still requires an accurate virtual environment model that can replace the real environment interacting with the CPS in a closed loop. In this article, we propose ENVI (ENVironment Imitation), a novel approach to automatically generate an accurate virtual environment model, enabling efficient and accurate simulation-based CPS goal verification in practice. To do this, we first formally define the problem of the virtual environment model generation and solve it by leveraging Imitation Learning (IL), which has been actively studied in machine learning to learn complex behaviors from expert demonstrations. The key idea behind the model generation is to leverage IL for training a model that imitates the interactions between the CPS controller and its real environment as recorded in (possibly very small) FOT logs. We then statistically verify the goal achievement of the CPS by simulating it with the generated model. We empirically evaluate ENVI by applying it to the verification of two popular autonomous driving assistant systems. The results show that ENVI can reduce the cost of CPS goal verification while maintaining its accuracy by generating accurate environment models from only a few FOT logs. The use of IL in virtual environment model generation opens new research directions, further discussed at the end of the article. © 2024 Copyright held by the owner/author(s).",Cyber-physical system (CPS); environment modeling; imitation learning (IL); simulation-based CPS goal verification; software controller,Controllers; Embedded systems; Learning systems; Verification; Virtual addresses; Virtual reality; Cybe-physical system; Cybe-physical systems; Cyber-physical systems; Environment models; Imitation learning; Simulation-based cybe-physical system goal verification; Software controllers; Cyber Physical System
Deadline-Aware Task Offloading for Vehicular Edge Computing Networks Using Traffic Light Data,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184740575&doi=10.1145%2f3594541&partnerID=40&md5=7e963bd77ab3269428679655d5a5e229,"As vehicles have become increasingly automated, novel vehicular applications have emerged to enhance the safety and security of the vehicles and improve user experience. This brings ever-increasing data and resource requirements for timely computation by the vehicle’s on-board computing systems. To meet these demands, prior work proposes deploying vehicular edge computing (VEC) resources in road-side units (RSUs) in the traffic infrastructure with which the vehicles can communicate and offload compute-intensive tasks. Due to the limited communication range of these RSUs, the communication link between the vehicles and the RSUs — and, therefore, the response times of the offloaded applications — are significantly impacted by vehicle mobility through road traffic. Existing task offloading strategies do not consider the influence of traffic lights on vehicular mobility while offloading workloads onto the RSUs. This causes deadline misses and quality-of-service (QoS) reduction for the offloaded tasks. In this article, we present a novel task model that captures time and location-specific requirements for vehicular applications. We then present a deadline-based strategy that incorporates traffic light data to opportunistically offload tasks. Our approach allows up to 33% more tasks to be offloaded onto RSUs compared with existing work without causing deadline misses, maximizing the resource utilization of RSUs. © 2024 Copyright held by the owner/author(s).",connected traffic infrastructure; Edge computing; task offloading,Computation offloading; Quality of service; Roads and streets; Vehicle to vehicle communications; Connected traffic infrastructure; Deadline-aware; Edge computing; Road sides; Safety and securities; Task offloading; Traffic infrastructure; Traffic light; Users' experiences; Vehicular applications; Vehicles
Energy-Aware Adaptive Mixed-Criticality Scheduling with Semi-Clairvoyance and Graceful Degradation,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184778531&doi=10.1145%2f3632749&partnerID=40&md5=d70a4c8e14a7296abcc48d177195aac2,"The classic Mixed-Criticality System (MCS) task model is a non-clairvoyance model in which the change of the system behavior is based on the completion of high-criticality tasks while dropping low-criticality tasks in high-criticality mode. In this paper, we simultaneously consider graceful degradation and semi-clairvoyance in MCS. We first propose the analysis for adaptive mixed-criticality with semi-clairvoyance denoted as C-AMC-sem. The so-called semi-clairvoyance refers to the system’s behavior change being revealed at the time that jobs are released. Moreover, we propose a new algorithm based on C-AMC-sem to reduce energy consumption. Finally, we verify the performance of the proposed algorithms via experiments upon synthetically generated tasksets. The experimental results indicate that the proposed algorithms significantly outperform the existing algorithms. © 2024 Copyright held by the owner/author(s).",energy-aware scheduling; graceful degradation; mixed-criticality systems; Real-time scheduling; semi-clairvoyance,Criticality (nuclear fission); Power management; Real time systems; Behaviour changes; Energy aware; Energy-aware scheduling; Graceful degradation; Mixed criticalities; Mixed-criticality systems; Real time scheduling; Semi-clairvoyance; System behaviors; Task modelling; Energy utilization
Distributed Task Offloading and Resource Purchasing in NOMA-Enabled Mobile Edge Computing: Hierarchical Game Theoretical Approaches,2024,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184747135&doi=10.1145%2f3597023&partnerID=40&md5=8f65ff67d9a867366b85e4e8699d15e2,"As the computing resources and the battery capacity of mobile devices are usually limited, it is a feasible solution to offload the computation-intensive tasks generated by mobile devices to edge servers (ESs) in mobile edge computing (MEC). In this article, we study the multi-user multi-server task offloading problem in MEC systems, where all users compete for the limited communication resources and computing resources. We formulate the offloading problem with the goal of minimizing the cost of the users and maximizing the profits of the ESs. We propose a hierarchical EETORP (Economic and Efficient Task Offloading and Resource Purchasing) framework that includes a two-stage joint optimization process. Then we prove that the problem is NP-complete. For the first stage, we formulate the offloading problem as a multi-channel access game (MCA-Game) and prove theoretically the existence of at least one Nash equilibrium strategy in MCA-Game. Next, we propose a game-based multi-channel access (GMCA) algorithm to obtain the Nash equilibrium strategy and analyze the performance guarantee of the obtained offloading strategy in the worst case. For the second stage, we model the computing resource allocation between the users and ESs by Stackelberg game theory, and reformulate the problem as a resource pricing and purchasing game (PAP-Game). We prove theoretically the property of incentive compatibility and the existence of Stackelberg equilibrium. A game-based pricing and purchasing (GPAP) algorithm is proposed. Finally, a series of both parameter analysis and comparison experiments are carried out, which validate the convergence and effectiveness of the GMCA algorithm and GPAP algorithm. © 2024 Copyright held by the owner/author(s).",game theory; MEC; resource pricing; resource purchasing; Task offloading,Computation offloading; Computation theory; Computer games; Costs; Economics; Mobile edge computing; Sales; Channel access; Channel access algorithms; Computing resource; Edge server; Game-Based; Multi channel; Nash equilibrium strategies; Resource pricing; Resource purchasing; Task offloading; Game theory
SG-Float: Achieving Memory Access and Computing Power Reduction Using Self-Gating Float in CNNs,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177991705&doi=10.1145%2f3624582&partnerID=40&md5=1f08c77838269e153395fb65398ce164,"Convolutional neural networks (CNNs) are essential for advancing the field of artificial intelligence. However, since these networks are highly demanding in terms of memory and computation, implementing CNNs can be challenging. To make CNNs more accessible to energy-constrained devices, researchers are exploring new algorithmic techniques and hardware designs that can reduce memory and computation requirements.In this work, we present self-gating float (SG-Float), algorithm hardware co-design of a novel binary number format, which can significantly reduce memory access and computing power requirements in CNNs. SG-Float is a self-gating format that uses the exponent to self-gate the mantissa to zero, exploiting the characteristic of floating-point that the exponent determines the magnitude of a floating-point value and the error tolerance property of CNNs. SG-Float represents relatively small values using only the exponent, which increases the proportion of ineffective mantissas, corresponding to reducing mantissa multiplications of floating-point numbers. To minimize the accuracy loss caused by the approximation error introduced by SG-Float, we propose a fine-tuning process to determine the exponent thresholds of SG-Float and reclaim the accuracy loss. We also develop a hardware optimization technique, called the SG-Float buffering strategy, to best match SG-Float with CNN accelerators and further reduce memory access. We apply the SG-Float buffering strategy to vector-vector multiplication processing elements (PEs), which NVDLA adopts, in TSMC 40nm technology. Our evaluation results demonstrate that SG-Float can achieve up to 35% reduction in memory access power and up to 54% reduction in computing power compared with AdaptivFloat, a state-of-the-art format, with negligible power and area overhead. Additionally, we show that SG-Float can be combined with neural network pruning methods to further reduce memory access and mantissa multiplications in pruned CNN models. Overall, our work shows that SG-Float is a promising solution to the problem of CNN memory access and computing power.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Accelerator; Approximate Computing; Floating Point; Neural Network,Convolutional neural networks; Digital arithmetic; Memory architecture; % reductions; Accuracy loss; Approximate computing; Buffering strategy; Computing power; Convolutional neural network; Floating points; Memory access; Neural-networks; Selfgating; Computing power
SensiX++: Bringing MLOps and Multi-tenant Model Serving to Sensory Edge Devices,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177987177&doi=10.1145%2f3617507&partnerID=40&md5=cf3f01a84d52588f8daa6c997da1e2ae,"We present SensiX++, a multi-tenant runtime for adaptive model execution with integrated MLOps on edge devices, e.g., a camera, a microphone, or IoT sensors. SensiX++ operates on two fundamental principles: highly modular componentisation to externalise data operations with clear abstractions and document-centric manifestation for system-wide orchestration. First, a data coordinator manages the lifecycle of sensors and serves models with correct data through automated transformations. Next, a resource-aware model server executes multiple models in isolation through model abstraction, pipeline automation, and feature sharing. An adaptive scheduler then orchestrates the best-effort executions of multiple models across heterogeneous accelerators, balancing latency and throughput. Finally, microservices with REST APIs serve synthesised model predictions, system statistics, and continuous deployment. Collectively, these components enable SensiX++ to serve multiple models efficiently with fine-grained control on edge devices while minimising data operation redundancy, managing data and device heterogeneity, and reducing resource contention. We benchmark SensiX++ with 10 different vision and acoustics models across various multi-tenant configurations on different edge accelerators (Jetson AGX and Coral TPU) designed for sensory devices. We report on the overall throughput and quantified benefits of various automation components of SensiX++ and demonstrate its efficacy in significantly reducing operational complexity and lowering the effort to deploy, upgrade, reconfigure, and serve embedded models on edge devices.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",edge; MLOps; model serving; multi-tenancy,Abstracting; Information management; Life cycle; Adaptive models; Data operations; Edge; MLOp; Model executions; Model serving; Multi tenancies; Multi tenants; Multiple-modeling; Runtimes; Automation
Enabling Binary Neural Network Training on the Edge,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177987339&doi=10.1145%2f3626100&partnerID=40&md5=5a00dd458032624f4e97d9cfa5a8b0e7,"The ever-growing computational demands of increasingly complex machine learning models frequently necessitate the use of powerful cloud-based infrastructure for their training. Binary neural networks are known to be promising candidates for on-device inference due to their extreme compute and memory savings over higher-precision alternatives. However, their existing training methods require the concurrent storage of high-precision activations for all layers, generally making learning on memory-constrained devices infeasible. In this article, we demonstrate that the backward propagation operations needed for binary neural network training are strongly robust to quantization, thereby making on-the-edge learning with modern models a practical proposition. We introduce a low-cost binary neural network training strategy exhibiting sizable memory footprint reductions while inducing little to no accuracy loss vs Courbariaux & Bengio's standard approach. These decreases are primarily enabled through the retention of activations exclusively in binary format. Against the latter algorithm, our drop-in replacement sees memory requirement reductions of 3-5×, while reaching similar test accuracy (± 2 pp) in comparable time, across a range of small-scale models trained to classify popular datasets. We also demonstrate from-scratch ImageNet training of binarized ResNet-18, achieving a 3.78× memory reduction. Our work is open-source, and includes the Raspberry Pi-targeted prototype we used to verify our modeled memory decreases and capture the associated energy drops. Such savings will allow for unnecessary cloud offloading to be avoided, reducing latency, increasing energy efficiency, and safeguarding end-user privacy.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",binary neural network; Deep neural network; edge devices; embedded systems; memory reduction; training,Backpropagation; Chemical activation; Classification (of information); Computation offloading; Drops; Embedded systems; Energy efficiency; Learning systems; % reductions; Binary neural networks; Complex machines; Computational demands; Edge device; Embedded-system; High-precision; Machine learning models; Memory reduction; Neural networks trainings; Deep neural networks
Energy-Efficient Communications for Improving Timely Progress of Intermittent-Powered BLE Devices,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177995011&doi=10.1145%2f3626197&partnerID=40&md5=f85e52a8e760de45b2923eadc0fde8e6,"Battery-less devices offer potential solutions for maintaining sustainable Internet of Things (IoT) networks. However, limited energy harvesting capacity can lead to power failures, limiting the system's quality of service (QoS). To improve timely task progress, we present ETIME, a scheduling framework that enables energy-efficient communication for intermittent-powered IoT devices. To maximize energy efficiency while meeting the timely requirements of intermittent systems, we first model the relationship between insufficient harvesting energy and task behavior time. We then propose a method for predicting response times for battery-less devices. Considering both delays from multiple task interference and insufficient system energy, we introduce a dynamic wake-up strategy to improve timely task progress. Additionally, to minimize power consumption from connection components, we propose a dynamic connection interval adjustment to provide energy-efficient communication. The proposed algorithms are implemented in a lightweight operating system on real devices. Experimental results show that our approach can significantly improve progress for timely applications while maintaining task progress.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",battery-less; Energy harvesting; Internet of Things; scheduling,Electric batteries; Energy efficiency; Energy harvesting; Quality of service; Battery-less; Energy efficient communications; Harvesting energies; Intermittent systems; Limited energies; Multiple tasks; Power failure; Quality-of-service; Scheduling frameworks; System quality; Internet of things
RegKey: A Register-based Implementation of ECC Signature Algorithms Against One-shot Memory Disclosure,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177983731&doi=10.1145%2f3604805&partnerID=40&md5=d061c28f22f539d4bb9f225f539c648d,"To ensure the security of cryptographic algorithm implementations, several cryptographic key protection schemes have been proposed to prevent various memory disclosure attacks. Among them, the register-based solutions do not rely on special hardware features and offer better applicability. However, due to the size limitation of register resources, the performance of register-based solutions is much worse than conventional cryptosystem implementations without security enhancements. This paper presents RegKey, an efficient register-based implementation of ECC (elliptic curve cryptography) signature algorithms. Different from other schemes that protect the whole cryptographic operations, RegKey only uses CPU registers to execute simple but critical operations, significantly reducing the usage of register resources and performance overheads. To achieve this goal, RegKey splits the ECC signing into two parts, (1) complex elliptic curve group operations on non-sensitive data in main memory as normal implementations, and (2) simple prime field operations on sensitive data inside CPU registers. RegKey guarantees the plaintext private key and random number used for signing only appear in registers to effectively resist one-shot memory disclosure attacks such as cold-boot attacks and warm-boot attacks, which are usually launched by physically accessing the victim machine to acquire partial or even entire memory data but only once. Compared with existing cryptographic key protection schemes, the performance of RegKey is greatly improved. Regkey is applicable to different platforms because it does not rely on special CPU hardware features. Since RegKey focuses on one-shot memory disclosure instead of persistent software-based attacks, it works as a choice suitable for embedded devices or offline machines where physical attacks are the main threat.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cryptographic key protection; ECC signature algorithms; One-shot memory disclosure; registers,Geometry; Hardware security; Public key cryptography; Cryptographic key; Cryptographic key protection; Curve cryptography; Elliptic curve; Elliptic curve cryptography signature algorithm; Key protections; One-shot memory disclosure; Performance; Register; Signature algorithms; Sensitive data
Scheduling Dynamic Software Updates in Mobile Robots,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177975270&doi=10.1145%2f3623676&partnerID=40&md5=518d58552307f2d2afb174da9bd13a45,"We present NeRTA (Next Release Time Analysis), a technique to enable dynamic software updates for low-level control software of mobile robots. Dynamic software updates enable software correction and evolution during system operation. In mobile robotics, they are crucial to resolve software defects without interrupting system operation or to enable on-the-fly extensions. Low-level control software for mobile robots, however, is time sensitive and runs on resource-constrained hardware with no operating system support. To minimize the impact of the update process, NeRTA safely schedules updates during times when the computing unit would otherwise be idle. It does so by utilizing information from the existing scheduling algorithm without impacting its operation. As such, NeRTA works orthogonal to the existing scheduler, retaining the existing platform-specific optimizations and fine-tuning, and may simply operate as a plug-in component. To enable larger dynamic updates, we further conceive an additional mechanism called bounded reactive control and apply mixed-criticality concepts. The former cautiously reduces the overall control frequency, whereas the latter excludes less critical tasks from NeRTA processing. Their use increases the available idle times. We combine real-world experiments on embedded hardware with simulations to evaluate NeRTA. Our experimental evaluation shows that the difference between NeRTA's estimated idle times and the measured idle times is less than 15% in more than three-quarters of the samples. The combined effect of bounded reactive control and mixed-criticality concepts results in a 150+% increase in available idle times. We also show that the processing overhead of NeRTA and of the additional mechanisms is essentially negligible.  © 2023 Copyright held by the owner/author(s).",aerial drones; Dynamic software updates; mobile robotics; safety-critical systems,Criticality (nuclear fission); Drones; Dynamics; Mobile robots; Safety engineering; Scheduling algorithms; Aerial drone; Control software; Dynamic software update; Idle time; Mobile robotic; Reactive control; Release time; Safety critical systems; Systems operation; Time analysis; Antennas
On the RTL Implementation of FINN Matrix Vector Unit,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162198021&doi=10.1145%2f3547141&partnerID=40&md5=f934eb6ef7de58b8d69483ab3ecf40d7,"Field-programmable gate array (FPGA)-based accelerators are becoming increasingly popular for deep neural network (DNN) inference due to their ability to scale performance with increasing degrees of specialization with dataflow architectures or custom data type precision. In order to reduce the barrier for software engineers and data scientists to adopt FPGAs, C and OpenCL-based design entries with high-level synthesis (HLS) have been introduced. They provide higher abstraction compared with register-transfer level (RTL)-based design. HLS offers faster development time, better maintainability, and more flexibility in code exploration when evaluating several options for multi-dimension tensors, convolutional layers, or different degrees of parallelism. For this reason, HLS has been adopted by DNN accelerator generation frameworks such as FINN and hls4ml.In this article, we present an alternative backend library for FINN, leveraging RTL. We investigate and evaluate, across a spectrum of design dimensions, the pros and cons of an RTL-based implementation versus the original HLS variant. We show that for smaller design parameters, RTL produces significantly smaller circuits as compared with HLS. For larger circuits, however, the look-up table (LUT) count of RTL-based design is slightly higher, up to around 15-. On the other hand, HLS consistently requires more flip-flops (FFs+with an orders-of-magnitude difference for smaller designs) and block RAMs (BRAMs×times+ADs- more). This also impacts the critical path delay, with RTL producing significantly faster circuits, up to around 80+ACU-. RTL also benefits from at least a 10× reduction in synthesis time. Finally, the results were validated in practice using two real-world use cases, one of a multi-layer perceptron (MLP) used in network intrusion detection and the other a convolution network called ResNet, used in image recognition. Overall, since HLS frameworks code-generate the hardware design, the benefits of the ease in the design entry is less important. As such, the gained benefits in synthesis time together with some design-dependent resource benefits make the RTL abstraction an attractive alternative.  © Association for Computing Machinery.",convolutional neural network; FINN; FPGA; HLS; RTL,Convolution; Convolutional codes; Deep neural networks; Delay circuits; Digital arithmetic; Flip flop circuits; High level synthesis; Image recognition; Integrated circuit design; Intrusion detection; Logic gates; Table lookup; Convolutional neural network; Field programmables; Field-programmable gate array; FINN; High-level synthesis; Matrix-vector; Programmable gate array; Register-transfer level; Synthesis time; Vector units; Field programmable gate arrays (FPGA)
An Intermediate-Centric Dataflow for Transposed Convolution Acceleration on FPGA,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177975935&doi=10.1145%2f3561053&partnerID=40&md5=538714f5cfb789ac47876d87e2f662c4,"Transposed convolution has been prevailing in convolutional neural networks (CNNs), playing an important role in multiple scenarios such as image segmentation and back-propagation process of training CNNs. This mainly benefits from the ability to up-sample the input feature maps by interpolating new information from the input feature pixels. However, the backward-stencil computation constrains its performance and hindered its wide application in diverse platforms. Moreover, in contrast to the efforts on accelerating the convolution, there is a rare investigation on the acceleration of transposed convolution that is identically compute-intensive as the former.For acceleration of transposed convolution, we propose an intermediate-centric dataflow scheme, in which we decouple the generation of the intermediate patch from its further process, aim at efficiently performing the backward-stencil computation. The intermediate-centric dataflow breaks the transposed convolution into several phases/stages, achieving feeding the input feature maps and performing the backward-stencil computation in a pipelining manner. It also provides four-degree computation parallelism and efficient data reuse of input feature maps/weights. Furthermore, we also theoretically analyze the irregular data dependence leveraging the polyhedral model, which constrains the parallel computing of transposed convolution. Additionally, we devise an optimization problem to explore the design space and automatically generate the optimal design configurations for different transposed convolutional layers and hardware platforms. By selecting the representative transposed convolutional layers from DCGAN, FSRCNN, and FCN, we generate the corresponding accelerator arrays of intermediate-centric dataflow on the Xilinx Alveo U200 platform and reach the performance of 3.92 TOPS, 2.72 TOPS, and 4.76 TOPS, respectively.  © 2023 Copyright held by the owner/author(s).",automatic framework; dataflow; FPGA; Transposed convolution,Backpropagation; Data flow analysis; Field programmable gate arrays (FPGA); Image segmentation; Integrated circuit design; Neural networks; Space platforms; Automatic framework; Convolutional neural network; Dataflow; Feature map; Images segmentations; Input features; Performance; Segmentation propagation; Stencil computations; Transposed convolution; Convolution
TH-iSSD: Design and Implementation of a Generic and Reconfigurable Near-Data Processing Framework,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177987775&doi=10.1145%2f3563456&partnerID=40&md5=1e53f42a80f45c0363730d2fcc9ab994,"We present the design and implementation of TH-iSSD, a near-data processing framework to address the data movement problem. TH-iSSD does not pose any restriction to the hardware selection and is highly reconfigurable - its core components, such as the on-device compute unit (e.g., FPGA, embedded CPUs) and data collectors (e.g., camera, sensors), can be easily replaced to adapt to different use cases. TH-iSSD achieves this goal by incorporating highly flexible computation and data paths. In the data path, TH-iSSD adopts an efficient device-level data switch that exchanges data with both host CPUs and peripheral sensors; it also enables direct accesses between the sensing, computation, and storage hardware components, which completely eliminates the redundant data movement overhead, and thus delivers both high performance and energy efficiency. In the computation path, TH-iSSD provides an abstraction of filestream for developers, which abstracts a collection of data along with the related computation task as a file. Since existing applications are familiar with POSIX-like interfaces, they can be ported on top of our platform with minimal code modification. Moreover, TH-iSSD also introduces mechanisms including pipelined near-data processing and priority-aware I/O scheduling to make TH-iSSD perform more effectively. We deploy TH-iSSD to accelerate two types of applications: the content-based information retrieval system and the edge zero-streaming system. Our experimental results show that TH-iSSD achieves up to 1.6× higher throughput and 36% lower latency than compute-centric designs.  © 2023 Copyright held by the owner/author(s).",deep learning; information retrieval; Near data processing; storage architecture,Abstracting; Data handling; Deep learning; Digital storage; Energy efficiency; Field programmable gate arrays (FPGA); Information retrieval; Pipeline processing systems; Reconfigurable architectures; Reconfigurable hardware; Search engines; Computation paths; Core components; Data movements; Data paths; Deep learning; Design and implementations; Hardware selection; Near data processing; Reconfigurable; Storage architectures; Program processors
Design and Analysis of High Performance Heterogeneous Block-based Approximate Adders,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177983270&doi=10.1145%2f3625686&partnerID=40&md5=6453e07538f65d7d2c34adb945460b14,"Approximate computing is an emerging paradigm to improve the power and performance efficiency of error-resilient applications. As adders are one of the key components in almost all processing systems, a significant amount of research has been carried out toward designing approximate adders that can offer better efficiency than conventional designs; however, at the cost of some accuracy loss. In this article, we highlight a new class of energy-efficient approximate adders, namely, Heterogeneous Block-based Approximate Adders (HBAAs), and propose a generic configurable adder model that can be configured to represent a particular HBAA configuration. An HBAA, in general, is composed of heterogeneous sub-adder blocks of equal length, where each sub-adder can be an approximate sub-adder and have a different configuration. The sub-adders are mainly approximated through inexact logic and carry truncation. Compared to the existing design space, HBAAs provide additional design points that fall on the Pareto-front and offer a better quality-efficiency tradeoff in certain scenarios. Furthermore, to enable efficient design space exploration based on user-defined constraints, we propose an analytical model to efficiently evaluate the Probability Mass Function (PMF) of approximation error and other error metrics, such as Mean Error Distance (MED), Normalized Mean Error Distance (NMED), and Error Rate (ER) of HBAAs. The results show that HBAA configurations can provide around 15% reduction in area and up to 17% reduction in energy compared to state-of-the-art approximate adders.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",accuracy; approximate adders; Approximate computing; efficiency; error analysis; low latency; low power; performance estimation; quality; tradeoff,Computation theory; Computing power; Energy efficiency; Errors; Quality control; Accuracy; Approximate adder; Approximate computing; Block based; Low latency; Low Power; Mean error distances; Performance estimation; Quality; Tradeoff; Adders
Dynamic Thermal Management of 3D Memory through Rotating Low Power States and Partial Channel Closure,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177982976&doi=10.1145%2f3624581&partnerID=40&md5=f007c700d5879e409284f4c1556b2b61,"Modern high-performance and high-bandwidth three-dimensional (3D) memories are characterized by frequent heating. Prior art suggests turning off hot channels and migrating data to the background DDR memory, incurring significant performance and energy overheads. We propose three Dynamic Thermal Management (DTM) approaches for 3D memories, reducing these overheads. The first approach, Rotating-channel Low-power-state-based DTM (RL-DTM), minimizes the energy overheads by avoiding data migration. RL-DTM places 3D memory channels into low power states instead of turning them off. Since data accesses are disallowed during low power state, RL-DTM balances each channel's low-power-state duration. The second approach, Masked rotating-channel Low-power-state-based DTM (ML-DTM), is a fine-grained policy that minimizes the energy-delay product (EDP) and improves the performance of RL-DTM by considering the channel access rate. The third strategy, Partial channel closure and ML-DTM, minimizes performance overheads of existing channel-level turn-off-based policies by closing a channel only partially and integrating ML-DTM, reducing the number of channels being turned off. We evaluate the proposed DTM policies using various mixes of SPEC benchmarks and multi-threaded workloads and observe them to significantly improve performance, energy, and EDP over state-of-the-art approaches for different 3D memory architectures.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",3D memory; Dynamic Thermal Management; partial channel closure,Benchmarking; Cost reduction; Temperature control; 3D memory; Dynamic thermal management; Energy delay product; Energy overheads; Low power state; Modern high performance; Partial channel closure; Performance; Rotating channels; State based; Memory architecture
FD-CNN: A Frequency-Domain FPGA Acceleration Scheme for CNN-Based Image-Processing Applications,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177978591&doi=10.1145%2f3559105&partnerID=40&md5=b5549e698a4bb3f884728c1c813ad4e7,"In the emerging edge-computing scenarios, FPGAs have been widely adopted to accelerate convolutional neural network (CNN)-based image-processing applications, such as image classification, object detection, and image segmentation, and so on. A standard image-processing pipeline first decodes the collected compressed images from Internet of Things (IoTs) to RGB data, then feeds them into CNN engines to compute the results. Previous works mainly focus on optimizing the CNN inference parts. However, we notice that on the popular ZYNQ FPGA platforms, image decoding can also become the bottleneck due to the poor performance of embedded ARM CPUs. Even with a hardware accelerator, the decoding operations still incur considerable latency. Moreover, conventional RGB-based CNNs have too few input channels at the first layer, which can hardly utilize the high parallelism of CNN engines and greatly slows down the network inference. To overcome these problems, in this article, we propose FD-CNN, a novel CNN accelerator leveraging the partial-decoding technique to accelerate CNNs directly in the frequency domain. Specifically, we omit the most time-consuming IDCT (Inverse Discrete Cosine Transform) operations of image decoding and directly feed the DCT coefficients (i.e., the frequency data) into CNNs. By this means, the image decoder can be greatly simplified. Moreover, compared to the RGB data, frequency data has a narrower input resolution but has 64× more channels. Such an input shape is more hardware friendly than RGB data and can substantially reduce the CNN inference time. We then systematically discuss the algorithm, architecture, and command set design of FD-CNN. To deal with the irregularity of different CNN applications, we propose an image-decoding-aware design-space exploration (DSE) workflow to optimize the pipeline. We further propose an early stopping strategy to tackle the time-consuming progressive JPEG decoding. Comprehensive experiments demonstrate that FD-CNN achieves, on average, 3.24×, 4.29× throughput improvement, 2.55×, 2.54× energy reduction and 2.38×, 2.58× lower latency on ZC-706 and ZCU-102 platforms, respectively, compared to the baseline image-processing pipelines.  © 2023 Association for Computing Machinery.",Convolutional Neural Network; Edge computing; FPGA; image decoding,Convolution; Convolutional neural networks; Discrete cosine transforms; Edge detection; Engines; Field programmable gate arrays (FPGA); Finite difference method; Frequency domain analysis; Image coding; Image segmentation; Integrated circuit design; Object detection; Pipeline processing systems; Pipelines; Program processors; Convolutional neural network; Edge computing; Frequency data; Frequency domains; Image decoding; Image processing applications; Image processing pipeline; Network engines; Network inference; Network-based; Decoding
Accelerating Attention Mechanism on FPGAs based on Efficient Reconfigurable Systolic Array,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177993148&doi=10.1145%2f3549937&partnerID=40&md5=f6cee141b141574d2258075bea57f70b,"Transformer model architectures have recently received great interest in natural language, machine translation, and computer vision, where attention mechanisms are their building blocks. However, the attention mechanism is expensive because of its intensive matrix computations and complicated data flow. The existing hardware architecture has some disadvantages for the computing structure of attention, such as inflexibility and low efficiency. Most of the existing papers accelerate attention by reducing the amount of computation through various pruning algorithms, which will affect the results to a certain extent with different sparsity. This paper proposes the hardware accelerator for the multi-head attention (MHA) on field-programmable gate arrays (FPGAs) with reconfigurable architecture, efficient systolic array, and hardware-friendly radix-2 softmax. We propose a novel method called Four inputs Processing Element (FPE) to double the computation rate of the data-aware systolic array (SA) and make it efficient and load balance. Especially, the computation framework is well designed to ensure the utilization of SA efficiently. Our design is evaluated on a Xilinx Alveo U250 card, and the proposed architecture achieves 51.3×, 17.3× improvement in latency, and 54.4×, 17.9× energy savings compared to CPU and GPU.  © 2023 Association for Computing Machinery.",Accelerator; attention; FPGA; reconfigurable systolic array; softmax; Transformer,Energy conservation; Field programmable gate arrays (FPGA); Matrix algebra; Reconfigurable architectures; Reconfigurable hardware; Attention; Attention mechanisms; Field programmables; Field-programmable gate array; Modeling architecture; Programmable gate array; Reconfigurable systolic arrays; Softmax; Transformer; Transformer modeling; Systolic arrays
ACDSE: A Design Space Exploration Method for CNN Accelerator based on Adaptive Compression Mechanism,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177979719&doi=10.1145%2f3545177&partnerID=40&md5=121a2a3a21402769b33dea8c37c6f52e,"Customized accelerators for Convolutional Neural Network (CNN) can achieve better energy efficiency than general computing platforms. However, the design of a high-performance accelerator should take into account a variety of parameters and physical constraints. The increasing parameters and tighter constraints gradually complicate the design space, which poses new challenges to the capacity and efficiency of design space exploration methods. In this paper, we provide a novel design space exploration method named ACDSE for optimizing the design process of CNN accelerators. ACDSE implements the adaptive compression mechanism to dynamically adjust the search range and prune low-value design points according to the exploration states. As a result, it can focus on valuable subspace while also improving exploration capacity and efficiency. Additionally, we implement ACDSE to address the problem of CNN accelerator latency optimization. The experiment indicates that, compared to former DSE methods, ACDSE can reduce latency and increase efficiency by 1.39x-5.07x and 2.07x-43.87x, respectively, under the most stringent constraint conditions, demonstrating its superior adaptability to the complicated design space.  © 2023 Association for Computing Machinery.",CNN accelerator; derivative-free optimization; design space exploration; Reinforcement Learning,Computer aided design; Convolutional neural networks; Deep learning; Energy efficiency; Optimization; Adaptive compression; Compression mechanism; Computing platform; Convolutional neural network; Convolutional neural network accelerator; Derivative-free optimization; Design space exploration; Design spaces; Exploration methods; Reinforcement learnings; Reinforcement learning
Online Distributed Schedule Randomization to Mitigate Timing Attacks in Industrial Control Systems,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177989174&doi=10.1145%2f3624584&partnerID=40&md5=7b2b4b73c934392309e8f01438669900,"Industrial control systems (ICSs) consist of a large number of control applications that are associated with periodic real-time flows with hard deadlines. To facilitate large-scale integration, remote control, and co-ordination, wireless sensor and actuator networks form the main communication framework in most ICSs. Among the existing wireless sensor and actuator network protocols, WirelessHART is the most suitable protocol for real-time applications in ICSs. The communications in a WirelessHART network are time-division multiple access based. To satisfy the hard deadlines of the real-time flows, the schedule in a WirelessHART network is pre-computed. The same schedule is repeated over every hyperperiod (i.e., lowest common multiple of the periods of the flows). However, a malicious attacker can exploit the repetitive behavior of the flow schedules to launch timing attacks (e.g., selective jamming attacks). To mitigate timing attacks, we propose an online distributed schedule randomization strategy that randomizes the time-slots in the schedules at each network device without violating the flow deadlines, while ensuring the closed-loop control stability. To increase the extent of randomization in the schedules further, and to reduce the energy consumption of the system, we incorporate a period adaptation strategy that adjusts the transmission periods of the flows depending on the stability of the control loops at runtime. We use Kullback-Leibler divergence and prediction probability of slots as two metrics to evaluate the performance of our proposed strategy. We compare our strategy with an offline centralized schedule randomization strategy. Experimental results show that the schedules generated by our strategy are 10% to 15% more diverse and 5% to 10% less predictable on average compared to the offline strategy when the number of base schedules and keys vary between 4 and 6 and 12 and 32, respectively, under all slot utilization (number of occupied slots in a hyperperiod). On incorporating period adaptation, the divergence in the schedules reduceat each period increase with 46% less power consumption on average.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",industrial control systems; period adaptation; Real-time networks; schedule randomization; security; timing attacks; WirelessHART,Energy utilization; Online systems; Random processes; Real time systems; Remote control; Time division multiple access; Wireless sensor networks; Hyper periods; Industrial control systems; Period adaptation; Randomisation; Real time network; Realtime flows; Schedule randomization; Security; Wireless sensors and actuator networks; WirelessHART; Timing circuits
A Comprehensive Model for Efficient Design Space Exploration of Imprecise Computational Blocks,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177987716&doi=10.1145%2f3625555&partnerID=40&md5=24e3bed234a2f65db0298b6f0fde6c17,"After almost a decade of research, development of more efficient imprecise computational blocks is still a major concern in imprecise computing domain. There are many instances of the introduced imprecise components of different types, while their main difference is that they propose different precision-cost-performance trade-offs. In this paper, a novel comprehensive model for the imprecise components is introduced, which can be exploited to cover a wide range of precision-cost-performance trade-offs, for different types of imprecise components. The model helps to find the suitable imprecise component based on any desired error criterion. Therefore, the most significant advantage of the proposed model is that it can be simply exploited for design space exploration of different imprecise components to extract the suitable components, with the desired precision-cost-performance trade-off for any specific application. To demonstrate the efficiency of the proposed model, two novel families of Lowest-cost Imprecise Adders (LIAs) and Lowest-cost Imprecise Multipliers (LIMs) are introduced in the paper, which are systematically extracted based on exploration of the design space provided by the proposed model. A wide range of simulation and synthesis results are also presented in the paper to prove the comparable efficiency of the systematically extracted LIA/LIM structures with respect to the most efficient existing human-made imprecise components both individually and in a Multiply-Accumulate application.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Adder; approximate computing; design space exploration; imprecise computational blocks; imprecise computing; multiplier,Economic and social effects; Efficiency; Approximate computing; Comprehensive modeling; Cost performance; Design space exploration; Efficient design space explorations; Imprecise computational block; Imprecise computing; Low-costs; Multiplier; Performance tradeoff; Adders
High-performance Reconfigurable DNN Accelerator on a Bandwidth-limited Embedded System,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142798380&doi=10.1145%2f3530818&partnerID=40&md5=6a245438803daa138db8f083e503b5b6,"Deep convolutional neural networks (DNNs) have been widely used in many applications, particularly in machine vision. It is challenging to accelerate DNNs on embedded systems because real-world machine vision applications should reserve a lot of external memory bandwidth for other tasks, such as video capture and display, while leaving little bandwidth for accelerating DNNs. In order to solve this issue, in this study, we propose a high-throughput accelerator, called reconfigurable tiny neural network accelerator (ReTiNNA), for the bandwidth-limited system and present a real-time object detection system for the high-resolution video image. We first present a dedicated computation engine that takes different data mapping methods for various filter types to improve data reuse and reduce hardware resources. We then propose an adaptive layer-wise tiling strategy that tiles the feature maps into strips to reduce the control complexity of data transmission dramatically and to improve the efficiency of data transmission. Finally, a design space exploration (DSE) approach is presented to explore design space more accurately in the case of insufficient bandwidth to improve the performance of the low-bandwidth accelerator. With a low bandwidth of 2.23 GB/s and a low hardware consumption of 90.261K LUTs and 448 DSPs, ReTiNNA can still achieve a high performance of 155.86 GOPS on VGG16 and 68.20 GOPS on ResNet50, which is better than other state-of-the-art designs implemented on FPGA devices. Furthermore, the real-time object detection system can achieve a high object detection speed of 19 fps for high-resolution video.  © 2023 Association for Computing Machinery.",accelerator; Convolutional neural networks; design space exploration; real-time object detection system; reconfigurable,Bandwidth; Computer hardware; Computer vision; Convolution; Convolutional neural networks; Data communication systems; Data transfer; Deep neural networks; Network architecture; Object detection; Object recognition; Bandwidth limiteds; Convolutional neural network; Design space exploration; Embedded-system; Machine-vision; Object detection systems; Performance; Real- time; Real-time object detection system; Reconfigurable; Embedded systems
Power Side-channel Attack Resistant Circuit Designs of ARX Ciphers Using High-level Synthesis,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173279877&doi=10.1145%2f3609507&partnerID=40&md5=4223b7bbdf77dd73a0c53a55861af433,"In the Internet of Things (IoT) era, edge devices have been considerably diversified and are often designed using high-level synthesis (HLS) for improved design productivity. However, HLS tools were originally developed in a security-unaware manner, resulting in vulnerabilities to power side-channel attacks (PSCAs), which are a serious threat to IoT systems. Currently, the impact and applicability of existing methods to PSCA-resistant designs using HLS are limited. In this article, we propose an effective HLS-based design method for PSCA-resistant ciphers implemented in hardware. In particular, we focus on lightweight block ciphers composed of addition/rotation/XOR (ARX)-based permutations to study the effects of the threshold implementation (which is one of the provably secure countermeasures against PSCAs) to the behavioral descriptions of ciphers along with the changes in HLS scheduling. The results obtained using Welch's t-test demonstrate that our proposed method can successfully improve the resistance against PSCAs for all ARX-based ciphers used as benchmarks.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",high-level synthesis; Power side-channel attack,High level synthesis; Internet of things; Timing circuits; Attack resistants; Circuit designs; Design method; Design productivity; High-level synthesis; Improved designs; Power; Power side-channel attack; Side-channel attacks; Synthesis tool; Side channel attack
Fine-grained Hardware Acceleration for Efficient Batteryless Intermittent Inference on the Edge,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173265299&doi=10.1145%2f3608475&partnerID=40&md5=b8a8b59a9d64759dee6fb2f971537cc8,"Backing up the intermediate results of hardware-accelerated deep inference is crucial to ensure the progress of execution on batteryless computing platforms. However, hardware accelerators in low-power AI platforms only support the one-shot atomic execution of one neural network inference without any backups. This article introduces a new toolchain for MAX78000, which is a brand-new microcontroller with a hardware-based convolutional neural network (CNN) accelerator. Our toolchain converts any MAX78000-compatible neural network into an intermittently executable form. The toolchain enables finer checkpoint granularity on the MAX78000 CNN accelerator, allowing for backups of any intermediate neural network layer output. Based on the layer-by-layer CNN execution, we propose a new backup technique that performs only necessary (urgent) checkpoints. The method involves the batteryless system switching to ultra-low-power mode while charging, saving intermediate results only when input power is lower than ultra-low-power mode energy consumption. By avoiding unnecessary memory transfer, the proposed solution increases the inference throughput by 1.9× for simulation and by 1.2× for real-world setup compared to the coarse-grained baseline execution.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",checkpointing; convolutional neural networks; edge computing; energy harvesting; hardware accelerator; Intermittent computing,Acceleration; Computing power; Convolution; Edge computing; Energy utilization; Low power electronics; Multilayer neural networks; Network layers; Battery-less; Check pointing; Convolutional neural network; Edge computing; Hardware accelerators; Intermediate results; Intermittent computing; Low power modes; Neural-networks; Ultra-low power; Energy harvesting
BISDU: A Bit-Serial Dot-Product Unit for Microcontrollers,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173630135&doi=10.1145%2f3608447&partnerID=40&md5=b04d4dcb1e5815518d310e5df9229704,"Low-precision quantized neural networks (QNNs) reduce the required memory space, bandwidth, and computational power, and hence are suitable for deployment in applications such as IoT edge devices. Mixed-precision QNNs, where weights commonly have lower precision than activations or different precision is used for different layers, can limit the accuracy loss caused by low-bit quantization, while still benefiting from reduced memory footprint and faster execution. Previous multiple-precision functional units supporting 8-bit, 4-bit, and 2-bit SIMD instructions have limitations, such as large area overhead, under-utilization of multipliers, and wasted memory space for low and mixed bit-width operations.This article introduces BISDU, a bit-serial dot-product unit to support and accelerate execution of mixed-precision low-bit QNNs on resource-constrained microcontrollers. BISDU is a multiplier-less dot-product unit, with frugal hardware requirements (a population count unit and 2:1 multiplexers). The proposed bit-serial dot-product unit leverages the conventional logical operations of a microcontroller to perform multiplications, which enables efficient software implementations of binary (Xnor), ternary (Xor), and mixed-precision [W×A] (And) dot-product operations.The experimental results show that BISDU achieves competitive performance compared to two state-of-the-art units, XpulpNN and Dustin, when executing low-bit-width CNNs. We demonstrate the advantage that bit-serial execution provides by enabling trading accuracy against weight footprint and execution time. BISDU increases the area of the ALU by 68% and the ALU power consumption by 42% compared to a baseline 32-bit RISC-V (RV32IC) microcontroller core. In comparison, XpulpNN and Dustin increase the area by 6.9× and 11.1× and the power consumption by 3.8× and 5.97×, respectively. The bit-serial state-of-the-art, based on a conventional popcount instruction, increases the area by 42% and power by 32%, with BISDU providing a 37% speedup over it.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesBit-serial; Dot-product; ISA extension; Low power; Microcontroller; Quantized neural networks,Controllers; Electric power utilization; Low power electronics; Additional key word and phrasesbit-serial; Bit-serial; Dot-product; ISA extension; Key words; Low Power; Mixed precision; Neural-networks; Product-unit; Quantized neural network; Microcontrollers
Faster Implementation of Ideal Lattice-Based Cryptography Using AVX512,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173216937&doi=10.1145%2f3609223&partnerID=40&md5=51337385cecaf0b2b94e944fae109502,"With the development of quantum computing, the existing cryptography schemes based on classical cryptographic primitives will no longer be secure. Hence, cryptographers are designing post-quantum cryptographic (PQC) schemes, and ideal lattice-based cryptography has emerged as a prime candidate. Today, as ideal lattice-based cryptography becomes more mature, its performance becomes an important optimization goal. In ideal lattice-based cryptography, polynomial arithmetic and polynomial sampling are the most time-consuming operations and therefore need to be accelerated. In this article, taking advantage of the parallelism of new 512-bit advanced vector instructions (AVX512), we present parallel implementations of polynomial arithmetic and polynomial sampling, thus comprehensively improving their performance. We conduct experiments with the Dilithium scheme(one scheme of NIST PQC Standardization Process Round-4). Our implementation gets a nice performance boost compared to its pure C language and 256-bit advanced vector instructions (AVX2) implementation.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Lattice-based cryptography(LBC); polynomial arithmetic; polynomial sampling,C (programming language); Quantum computers; Quantum cryptography; Sampling; Cryptographic primitives; Cryptographic schemes; Fast implementation; Lattice-based cryptography; Optimization goals; Performance; Polynomial arithmetic; Polynomial sampling; Post quantum; Quantum Computing; Polynomials
Regular Composite Resource Partitioning and Reconfiguration in Open Systems,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173254267&doi=10.1145%2f3609424&partnerID=40&md5=b4fc7452b4b62740da3c36a3119b3a74,"We consider the problem of resource provisioning for real-time cyber-physical applications in an open system environment where there does not exist a global resource scheduler that has complete knowledge of the real-time performance requirements of each individual application that shares the resources with the other applications. Regularity-based Resource Partition (RRP) model is an effective strategy to hierarchically partition and assign various resource slices among such applications. However, previous work on RRP model only discusses uniform resource environment, where resources are implicitly assumed to be synchronized and clocked at the same frequency. The challenge is that a task utilizing multiple resources may experience unexpected delays in non-uniform environments, where resources are clocked at different frequencies. This paper extends the RRP model to non-uniform multi-resource open system environments to tackle this problem. It first introduces a novel composite resource partition abstraction and then proposes algorithms to construct and reconfigure the composite resource partitions. Specifically, the Acyclic Regular Composite Resource Partition Scheduling (ARCRP-S) algorithm constructs regular composite resource partitions and the Acyclic Regular Composite Resource Partition Dynamic Reconfiguration (ARCRP-DR) algorithm reconfigures the composite resource partitions in the run time upon requests of partition configuration changes. Our experimental results show that compared with state-of-the-art methods, ARCRP-S can prevent unexpected resource supply shortfall and improve the schedulability up to 50%. On the other hand, ARCRP-DR can guarantee the resource supply during the reconfiguration with moderate computational overhead.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",composite resource; dynamic reconfiguration; open systems; Regularity-based resource partition,Clocks; Open systems; Composite resource; Cyber physicals; Dynamic re-configuration; Partition model; Physical application; Real- time; Regularity-based resource partition; Resource partitioning; Resource reconfigurations; System environment; Dynamic models
Consistency Constraints for Mapping Dataflow Graphs to Hybrid Dataflow/von Neumann Architectures,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173617273&doi=10.1145%2f3607869&partnerID=40&md5=6d45c00f4522305bbec0bacf13daec75,"Dataflow process networks (DPNs) provide a convenient model of computation that is often used to model system behavior in model-based designs. With fixed sets of nodes, they are also used as dataflow graphs as an intermediate program representation by compilers to uncover instruction-level parallelism of sequential programs. Many recent processor architectures, which are still von Neumann architectures, also use dataflow computing to increase their exploitation of instruction-level parallelism by exposing their datapaths so that the compiler can take care of the allocation of processing units (PUs), the execution schedules of instructions on the PUs, and the communication of intermediate values between PUs. If the communication paths are buffered, these architectures can be abstracted into a DPN architecture whose PUs and interconnection network are DPN nodes.In this article, we introduce a DPN abstraction of hybrid dataflow/von Neumann architectures and consider the mapping of the nodes of a given dataflow graph to the PUs of such a DPN architecture such that there are no conflicts due to the mapping of different nodes to the same PU. We express the allocation and scheduling constraints in terms of propositional logic for the original dataflow graph and for a modified version of the dataflow graph that simplifies the constraints by introducing levels using copy nodes, such that all nodes receive inputs only from nodes of the previous level. We also formulate equisatisfiable SMT constraints using integer variables to reason directly about the parallel runtime. On this basis, we further present alternative SAT constraints that explicitly encode concurrency, and discuss variants of the constraints for a better understanding of the same.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesDataflow graphs; code generation; exposed datapath architectures; linear graph layouts; queue layouts,Abstracting; Data flow analysis; Embedded systems; Flow graphs; Interconnection networks (circuit switching); Network architecture; Program compilers; Additional key word and phrasesdataflow graph; Codegeneration; Data-path architecture; Exposed datapath architecture; Graph layout; Key words; Linear graph; Linear graph layout; Processing units; Queue layout; Mapping
Interruptible Remote Attestation of Low-end IoT Microcontrollers via Performance Counters,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173227610&doi=10.1145%2f3611674&partnerID=40&md5=0625a63a8b17dc769f1c0fa88a62c40b,"Remote attestation is a method used in distributed systems to detect integrity violations on a target device (prover) through a challenge-response protocol initiated by a verifier device. The prover calculates a hash of its memory, which is compared to a known good state hash by the verifier. We propose a novel technique, called Counters Help Against Roving Malware (CHARM), which uses hardware performance counters on the prover's side and machine learning on the verifier's side to make interruptible remote attestation feasible, even for constrained microcontrollers. We will demonstrate the effectiveness of various machine learning tools and data manipulation techniques on prediction accuracy in a variety of scenarios.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",integrity; interrupts; machine learning; microcontrollers; performance counters; remote attestation; Security,Controllers; Internet of things; Machine learning; Malware; Network security; Challenge response protocols; Distributed systems; Integrity; Interrupt; Machine-learning; Malwares; Novel techniques; Performance counters; Remote attestation; Security; Microcontrollers
A Secure and Efficient Framework for Outsourcing Large-scale Matrix Determinant and Linear Equations,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173227960&doi=10.1145%2f3611014&partnerID=40&md5=8196a077e3e490c17538a294678ff19d,"Large-scale matrix determinants and linear equations are two basic computational tools in science and engineering fields. However, it is difficult for a resource-constrained client to solve large-scale computational tasks. Cloud computing service provides additional computing resources for resource-constrained clients. To solve the problem of large-scale computation, in this article, a secure and efficient framework is proposed to outsource large-scale matrix determinants and linear equations to a cloud. Specifically, the proposed framework contains two protocols, which solve large-scale matrix determinant and linear equations, respectively. In the outsourcing protocols of large-scale matrix determinants and linear equations, the task matrix is encrypted and sent to the cloud by the client. The encrypted task matrix is directly computed by using LU factorization in the cloud. The computed result is returned and verified by the cloud and the client, respectively. The computed result is decrypted if it passes the verification. Otherwise, it is returned to the cloud for recalculation. The framework can protect the input privacy and output privacy of the client. The framework also can guarantee the correctness of the result and reduce the local computational complexity. Furthermore, the experimental results show that the framework can save more than 70% of computing resources after outsourcing computing. Thus, this article provides a secure and efficient alternative for solving large-scale computational tasks.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Cloud computing; linear equations; lu factorization; matrix determinant; secure outsourcing,Cloud computing; Cryptography; Factorization; Lower-upper decomposition; Outsourcing; Cloud-computing; Computational task; Computational tools; Computing resource; Large-scales; LU factorization; matrix; Matrix determinant; Science and engineering; Secure outsourcing; Linear equations
Real-Time Guarantees in Routerless Networks-on-Chip,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173586735&doi=10.1145%2f3616539&partnerID=40&md5=788610d0595d3093174bdefa23422b85,"This article considers the use of routerless networks-on-chip as an alternative on-chip interconnect for multi-processor systems requiring hard real-time guarantees for inter-processor communication. It presents a novel analytical framework that can provide latency upper bounds to real-time packet flows sent over routerless networks-on-chip, and it uses that framework to evaluate the ability of such networks to provide real-time guarantees. Extensive comparative analysis is provided, considering different architectures for routerless networks and a state-of-the-art wormhole network based on priority-preemptive routers as a baseline. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",real-time networks; Response time analysis,Network architecture; Real time systems; Routers; Hard real-time; Hard-real-time; Inter processor communication; Multi processor systems; Networks on chips; On-chip interconnects; Real time guarantees; Real time network; Response-time analysis; Routerless networks; Network-on-chip
Look-up the Rainbow: Table-based Implementation of Rainbow Signature on 64-bit ARMv8 Processors,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173577594&doi=10.1145%2f3607140&partnerID=40&md5=ca4c709e4a7f7659405a68657b87a011,"The Rainbow Signature Scheme is one of the finalists in the National Institute of Standards and Technology (NIST) Post-Quantum Cryptography (PQC) standardization competition, but failed to win because it has lack of stability in the parameter selection. It is the only signature candidate based on a multivariate quadratic equation. Rainbow signatures have smaller signature sizes compared with other post-quantum cryptography candidates. However, they require expensive tower-field based polynomial multiplications. In this article, we propose an efficient implementation of Rainbow signatures using a look-up table-based multiplication method. The polynomial multiplications in Rainbow signatures are performed on the 16 field, which is divided into sub-fields 4 and 2 under the tower-field method. To accelerate the multiplication process on target processors, we propose a look-up table-based tower-field multiplication technique. In 16, all values are expressed in 4-bit data format and can be implemented using a 256-byte look-up table access. The implementation uses the TBL and TBX instructions of the 64-bit ARMv8 target processor. For Rainbow III and Rainbow V, they are computed on the 256 field using an additional 16-byte table instead of creating a new look-up table. The proposed technique uses the vector registers of 64-bit ARMv8 processors and can calculate 16 result values with a single instruction. We also proposed implementations that are resistant to timing attacks. There are two types of implementations. The first one is the cache side-attack resistant implementation, which utilizes the 128-byte cache lines of the M1 processor. In this implementation, cache misses do not occur, and cache hits always occur. The second type is the constant-time implementation. This method takes a step-by-step approach to finding the required look-up table value and ensures that the same number of accesses is made regardless of which look-up table value is called. This implementation is designed to be constant-time, meaning it does not leak timing information. Our experiments on modern Apple M1 processors showed up to 428.73× and 114.16× better performance for finite field multiplications and Rainbow signatures schemes, respectively, compared with previous reference implementations. To the best of our knowledge, this proposed Rainbow implementation is the first optimized Rainbow implementation for 64-bit ARMv8 processors. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",64-bit ARMv8 processors; Post-quantum cryptography; rainbow signature; software implementations,ARM processors; Authentication; Cache memory; Network security; Quantum cryptography; Side channel attack; 64-bit ARMv8 processor; Constant time; Lookup tables (LUTs); Polynomial multiplication; Post quantum cryptography; Rainbow signature; Rainbow tables; Signature Scheme; Software implementation; Tower fields; Table lookup
Hephaestus: Codesigning and Automating 3D Image Registration on Reconfigurable Architectures,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171775737&doi=10.1145%2f3607928&partnerID=40&md5=a9e9fa2d3e117a91b1002a58f0a065c9,"Healthcare is a pivotal research field, and medical imaging is crucial in many applications. Therefore finding new architectural and algorithmic solutions would benefit highly repetitive image processing procedures. One of the most complex tasks in this sense is image registration, which finds the optimal geometric alignment among 3D image stacks and is widely employed in healthcare and robotics. Given the high computational demand of such a procedure, hardware accelerators are promising real-time and energy-efficient solutions, but they are complex to design and integrate within software pipelines. Therefore, this work presents an automation framework called Hephaestus that generates efficient 3D image registration pipelines combined with reconfigurable accelerators. Moreover, to alleviate the burden from the software, we codesign software-programmable accelerators that can adapt at run-time to the image volume dimensions. Hephaestus features a cross-platform abstraction layer that enables transparently high-performance and embedded systems deployment. However, given the computational complexity of 3D image registration, the embedded devices become a relevant and complex setting being constrained in memory; thus, they require further attention and tailoring of the accelerators and registration application to reach satisfactory results. Therefore, with Hephaestus, we also propose an approximation mechanism that enables such devices to perform the 3D image registration and even achieve, in some cases, the accuracy of the high-performance ones. Overall, Hephaestus demonstrates 1.85× of maximum speedup, 2.35× of efficiency improvement with respect to the State of the Art, a maximum speedup of 2.51× and 2.76× efficiency improvements against our software, while attaining state-of-the-art accuracy on 3D registrations.  © 2023 Copyright held by the owner/author(s).",3D image registration; design automation; FPGA,Abstracting; Computer aided design; Computer hardware; Embedded systems; Energy efficiency; Health care; Image registration; Integrated circuit design; Medical imaging; Pipelines; Reconfigurable architectures; System-on-chip; 3D image registration; Algorithmic solutions; Architectural solutions; Co-designing; Design automations; Efficiency improvement; Images processing; Processing procedures; Research fields; State of the art; Field programmable gate arrays (FPGA)
VADF: Versatile Approximate Data Formats for Energy-Efficient Computing,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171797641&doi=10.1145%2f3609106&partnerID=40&md5=2090ae3d162099ede7777214f3a294a0,"Approximate computing (AC) techniques provide overall performance gains in terms of power and energy savings at the cost of minor loss in application accuracy. For this reason, AC has emerged as a viable method for efficiently supporting several compute-intensive applications, e.g., machine learning, deep learning, and image processing, that can tolerate bounded errors in computations. However, most prior techniques do not consider the possibility of soft errors or malicious bit-flips in AC systems. These errors may interact with approximation-introduced errors in unforeseen ways, leading to disastrous consequences, such as the failure of computing systems. A recent research effort, FTApprox (DATE'21) proposes an error-resilient approximate data format. FTApprox stores two blocks, starting from the one containing the most significant valid (MSV) bit. It also stores location of the MSV block and protects them using error-correcting bits (ECBs). However, FTApprox has crucial limitations such as lack of flexibility, redundantly storing zeros in the MSV, etc.In this paper, we propose a novel storage format named Versatile Approximate Data Format (VADF) for storing approximate integer numbers while providing resilience to soft errors. VADF prescribes rules for storing, for example, a 32-bit number in either 8-bit, 12-bit or 16-bit numbers. VADF identifies the MSV bit and stores a certain number of bits following the MSV bit. It also stores the location of the MSV bit and protects it by ECBs. VADF does not explicitly store the MSB bit itself and this prevents VADF from accruing significant errors. VADF incurs lower error than both truncation methodologies and FTApprox. We further evaluate five image-processing and machine-learning applications and confirm that VADF provides higher application quality than FTApprox in the presence and absence of soft errors. Finally, VADF allows the use of narrow arithmetic units. For example, instead of using a 32-bit multiplier/adder, one can first use VADF (or FTApprox) to compress the data and then use a 8-bit multiplier/adder. Through this approach, VADF facilitates 95.97% and 79.3% energy savings in multiplication and addition, respectively. However, the subsequent re-conversion of the 8-bit output data to 32-bit data using Inv-VADF(16,3,32) diminishes the energy savings by 9.6% for addition and 0.56% for multiplication operation, respectively. The code is available at https://github.com/CandleLabAI/VADF-ApproximateDataFormat-TECS.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Approximate computing; approximate data formats; soft-error resilience,Deep learning; Energy efficiency; Error correction; Image processing; Quality control; Radiation hardening; Approximate computing; Approximate data format; Computing system; Energy  savings; Energy-savings; Error resilience; Error-correcting bits; Images processing; Soft error; Soft-error resilience; Digital storage
Protection Window Based Security-Aware Scheduling against Schedule-Based Attacks,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171787685&doi=10.1145%2f3609098&partnerID=40&md5=569f9608d461ab567ed30f4cb9d7c8e4,"With widespread use of common-off-the-shelf components and the drive towards connection with external environments, the real-time systems are facing more and more security problems. In particular, the real-time systems are vulnerable to the schedule-based attacks because of their predictable and deterministic nature in operation. In this paper, we present a security-aware real-time scheduling scheme to counteract the schedule-based attacks by preventing the untrusted tasks from executing during the attack effective window (AEW). In order to minimize the AEW untrusted coverage ratio for the system with uncertain AEW size, we introduce the protection window to characterize the system protection capability limit due to the system schedulability constraint. To increase the opportunity of the priority inversion for the security-aware scheduling, we design an online feasibility test method based on the busy interval analysis. In addition, to reduce the run-time overhead of the online feasibility test, we also propose an efficient online feasibility test method based on the priority inversion budget analysis to avoid online iterative calculation through the offline maximum slack analysis. Owing to the protection window and the online feasibility test, our proposed approach can efficiently provide best-effort protection to mitigate the schedule-based attack vulnerability while ensuring system schedulability. Experiments show the significant security capability improvement of our proposed approach over the state-of-the-art coverage oriented scheduling algorithm.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",attack effective window (AEW); online feasibility test; Schedule-based attacks; security-aware scheduling,Budget control; Interactive computer systems; Iterative methods; Scheduling algorithms; Attack effective window; Feasibility tests; Online feasibility test; Priority inversion; Real - Time system; Schedulability; Schedule-based attack; Security-aware; Security-aware scheduling; Test method; Real time systems
Consistency vs. Availability in Distributed Cyber-Physical Systems,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171785541&doi=10.1145%2f3609119&partnerID=40&md5=0ac11ece5a467f2d257b26bceb750cf7,"In distributed applications, Brewer's CAP theorem tells us that when networks become partitioned (P), one must give up either consistency (C) or availability (A). Consistency is agreement on the values of shared variables; availability is the ability to respond to reads and writes accessing those shared variables. Availability is a real-time property whereas consistency is a logical property. We extend consistency and availability to refer to cyber-physical properties such as the state of the physical system and delays in actuation. We have further extended the CAP theorem to relate quantitative measures of these two properties to quantitative measures of communication and computation latency (L), obtaining a relation called the CAL theorem that is linear in a max-plus algebra. This paper shows how to use the CAL theorem in various ways to help design cyber-physical systems. We develop a methodology for systematically trading off availability and consistency in application-specific ways and to guide the system designer when putting functionality in end devices, in edge computers, or in the cloud. We build on the Lingua Franca coordination language to provide system designers with concrete analysis and design tools to make the required tradeoffs in deployable embedded software.  © 2023 Copyright held by the owner/author(s).",availability; concurrency; consistency; Coordination,Commerce; Computation theory; Embedded systems; Systems analysis; Concurrency; Consistency; Coordination; Cybe-physical systems; Cyber-physical systems; Distributed applications; Quantitative measures; Real-time properties; Shared variables; System designers; Cyber Physical System
Improving Worst-case TSN Communication Times of Large Sensor Data Samples by Exploiting Synchronization,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171757112&doi=10.1145%2f3609120&partnerID=40&md5=ff9865f91509fe435eead3bdad8a1e7c,"Higher levels of automated driving also require a more sophisticated environmental perception. Therefore, an increasing number of sensors transmit their data samples as frame bursts to other applications for further processing. As a vehicle has to react to its environment in time, such data is subject to safety-critical latency constraints. To keep up with the resulting data rates, there is an ongoing transition to a Time-Sensitive Networking (TSN)-based communication backbone. However, the use of TSN-related industry standards does not match the automotive requirements of large timely sensor data transmission, nor it offers benefits on time-critical transmissions of single control data packets. By using the full data rate of prioritized IEEE 802.1Q Ethernet, giving time guarantees on large data samples is possible, but with strongly degraded results due to data collision. Resolving such collisions with time-aware shaping comes with significant overhead. Hence, rather than optimizing the parameters of the existing protocol, we propose a system design that synchronizes the transmission times of sensor data samples. This limits network protocol complexity and hardware requirements by avoiding tight time synchronization and time-aware shaping. We demonstrate that individual sensor data samples are transmitted without significant interference, exclusively at full Ethernet data rate. We provide a synchronous event model together with a straightforward response time analysis for synchronous multi-frame sample transmissions. The results show that worst-case latencies of such sample communication, in contrast to non-synchronized approaches, are close to their theoretical minimum as well as to simulative results while keeping the overall network utilization high.  © 2023 Copyright held by the owner/author(s).",automated driving; Ethernet; real-time; Safety; TSN; verification,Complex networks; Network protocols; Safety engineering; Synchronization; Automated driving; Communication time; Data sample; Data-rate; Environmental perceptions; Latency constraints; Real- time; Sensors data; Time-sensitive networking; Worst-case time; Ethernet
CIM: A Novel Clustering-based Energy-Efficient Data Imputation Method for Human Activity Recognition,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171783242&doi=10.1145%2f3609111&partnerID=40&md5=f067beb7d2e6f6c0c5cd21d571e1ba41,"Human activity recognition (HAR) is an important component in a number of health applications, including rehabilitation, Parkinson's disease, daily activity monitoring, and fitness monitoring. State-of-the-art HAR approaches use multiple sensors on the body to accurately identify activities at runtime. These approaches typically assume that data from all sensors are available for runtime activity recognition. However, data from one or more sensors may be unavailable due to malfunction, energy constraints, or communication challenges between the sensors. Missing data can lead to significant degradation in the accuracy, thus affecting quality of service to users. A common approach for handling missing data is to train classifiers or sensor data recovery algorithms for each combination of missing sensors. However, this results in significant memory and energy overhead on resource-constrained wearable devices. In strong contrast to prior approaches, this paper presents a clustering-based approach (CIM) to impute missing data at runtime. We first define a set of possible clusters and representative data patterns for each sensor in HAR. Then, we create and store a mapping between clusters across sensors. At runtime, when data from a sensor are missing, we utilize the stored mapping table to obtain most likely cluster for the missing sensor. The representative window for the identified cluster is then used as imputation to perform activity classification. We also provide a method to obtain imputation-aware activity prediction sets to handle uncertainty in data when using imputation. Experiments on three HAR datasets show that CIM achieves accuracy within 10% of a baseline without missing data for one missing sensor when providing single activity labels. The accuracy gap drops to less than 1% with imputation-aware classification. Measurements on a low-power processor show that CIM achieves close to 100% energy savings compared to state-of-the-art generative approaches.  © 2023 Copyright held by the owner/author(s).",clustering; data imputation; health monitoring; Human activity recognition; missing data detection; wearable electronics,Classification (of information); Data handling; Energy efficiency; Pattern recognition; Quality of service; Wearable sensors; Clusterings; Data imputation; Data-detection; Health monitoring; Human activity recognition; Missing data; Missing data detection; Novel clustering; Runtimes; State of the art; Mapping
Energy-efficient Personalized Federated Search with Graph for Edge Computing,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171772195&doi=10.1145%2f3609435&partnerID=40&md5=a1bd6f29ac56250389003cd0340a9c63,"Federated Learning (FL) is a popular method for privacy-preserving machine learning on edge devices. However, the heterogeneity of edge devices, including differences in system architecture, data, and co-running applications, can significantly impact the energy efficiency of FL. To address these issues, we propose an energy-efficient personalized federated search framework. This framework has three key components. Firstly, we search for partial models with high inference efficiency to reduce training energy consumption and the occurrence of stragglers in each round. Secondly, we build lightweight search controllers that control the model sampling and respond to runtime variances, mitigating new straggler issues caused by co-running applications. Finally, we design an adaptive search update strategy based on graph aggregation to improve personalized training convergence. Our framework reduces the energy consumption of the training process by lowering the training overhead of each round and speeding up the training convergence rate. Experimental results show that our approach achieves up to 5.02% accuracy and 3.45× energy efficiency improvements. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",energy-efficient; federated search; graph-based aggregation; Personalized FL,Edge computing; Energy efficiency; Graphic methods; Learning systems; Privacy-preserving techniques; Edge computing; Energy efficient; Energy-consumption; Federated search; Graph-based; Graph-based aggregation; Machine-learning; Personalized federated learning; Privacy preserving; Running applications; Energy utilization
SpinBayes: Algorithm-Hardware Co-Design for Uncertainty Estimation Using Bayesian In-Memory Approximation on Spintronic-Based Architectures,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171777124&doi=10.1145%2f3609116&partnerID=40&md5=83f0f19f3d1e995eb9c4f46569aa9209,"Recent development in neural networks (NNs) has led to their widespread use in critical and automated decision-making systems, where uncertainty estimation is essential for trustworthiness. Although conventional NNs can solve many problems accurately, they do not capture the uncertainty of the data or the model during optimization. In contrast, Bayesian neural networks (BNNs), which learn probabilistic distributions for their parameters, offer a sound theoretical framework for estimating uncertainty. However, traditional hardware implementations of BNNs are expensive in terms of computational and memory resources, as they (i) are realized with inefficient von Neumann architectures, (ii) use a significantly large number of random number generators (RNGs) to implement the distributions of BNNs, and (iii) have a substantially greater number of parameters than conventional NNs. Computing-in-memory (CiM) architectures with emerging resistive non-volatile memories (NVMs) are promising candidates for accelerating classical NNs. In particular, spintronic technology, which is distinguished by its low latency and high endurance, aligns very well with these requirements. In the specific context of Bayesian neural networks (BNNs), spintronics technologies are very valuable, thanks to their inherent potential to act as stochastic or as deterministic devices. Consequently, BNNs mapped on spintronic-based CiM architectures could be a highly efficient implementation strategy. However, the direct implementation on CiM hardware of the learned probabilistic distributions of BNN may not be feasible and can incur high overhead. In this work, we propose a new Bayesian neural network topology, named SpinBayes, that is able to perform efficient sampling during the Bayesian inference process. Moreover, a Bayesian approximation method, called in-memory approximation, is proposed that approximates the original probabilistic distributions of BNN with a distribution that can be efficiently mapped to spintronic-based CiM architectures. Compared to state-of-the-art methods, the memory overhead is reduced by 8× and the energy consumption by 80×. Our method has been evaluated on several classification and semantic segmentation tasks and can detect up to 100% of various types of out-of-distribution data, highlighting the robustness of our approach, without any performance sacrifice.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Bayesian in-memory approximation.; Bayesian neural network; low-cost uncertainty estimation; SpinBayes; Spintronic; Uncertainty estimation,Approximation algorithms; Decision making; Energy utilization; Memory architecture; Network architecture; Neural networks; Number theory; Random number generation; Semantics; Stochastic systems; Uncertainty analysis; Bayesian; Bayesian in-memory approximation.; Bayesian neural networks; Cost uncertainty; Low-cost uncertainty estimation; Low-costs; Neural-networks; Spinbaye; Uncertainty estimation; Inference engines
Thermal Management for 3D-Stacked Systems via Unified Core-Memory Power Regulation,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171804020&doi=10.1145%2f3608040&partnerID=40&md5=e29607c60961649f5e01691032bcc3a6,"3D-stacked processor-memory systems stack memory (DRAM banks) directly on top of logic (CPU cores) using chiplet-on-chiplet packaging technology to provide the next-level computing performance in embedded platforms. Stacking, however, severely increases the system's power density without any accompanying increase in the heat dissipation capacity. Consequently, 3D-stacked processor-memory systems suffer more severe thermal issues than their non-stacked counterparts. Nevertheless, 3D-stacked processor-memory systems do inherit power (thermal) management knobs from their non-stacked predecessors - namely Dynamic Voltage and Frequency Scaling (DVFS) for cores and Low Power Mode (LPM) for memory banks. In the context of 3D-stacked processor-memory systems, DVFS and LPM are performance- and power-wise deeply intertwined. Their non-unified independent use on 3D-stacked processor-memory systems results in sub-optimal thermal management. The unified use of DVFS and LPM for thermal management for 3D-stacked processor-memory systems remains unexplored. The lack of implementation of LPM in thermal simulators for 3D-stacked processor-memory systems hinders real-world representative evaluation for a unified approach.We extend the state-of-the-art interval thermal simulator for 3D-stacked processor-memory systems CoMeT with an LPM power management knob for memory banks. We also propose a learning-based thermal management technique for 3D-stacked processor-memory systems that employ DVFS and LPM in a unified manner. Detailed interval thermal simulations with the extended CoMeT framework show a 10.15% average response time improvement with the PARSEC and SPLASH-2 benchmark suites, along with widely-used Deep Neural Network (DNN) workloads against a state-of-the-art thermal management technique for 2.5D processor-memory systems (ported directly to 3D-stacked processor-memory systems) that also proposes unified use of DVFS and LPM.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",3D-stacked processors; AI for systems; chiplet-on-chiplet stacking; embedded systems; Low power design; resource-constrained edge computing,Computation theory; Computing power; Deep neural networks; Dynamic frequency scaling; Dynamic random access storage; Embedded systems; Energy policy; Gradient methods; Low power electronics; Temperature control; Thermal management (electronics); Three dimensional integrated circuits; Voltage scaling; 3d-stacked processor; AI for system; Chiplet-on-chiplet stacking; Edge computing; Embedded-system; Low-power design; Memory systems; Processor memory; Resource-constrained edge computing; Stackings; Edge computing
LaDy: Enabling Locality-aware Deduplication Technology on Shingled Magnetic Recording Drives,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171785091&doi=10.1145%2f3607921&partnerID=40&md5=9ff69e1fc2901df7afd15794bf9ffb44,"The continuous increase in data volume has led to the adoption of shingled-magnetic recording (SMR) as the primary technology for modern storage drives. This technology offers high storage density and low unit cost but introduces significant performance overheads due to the read-update-write operation and garbage collection (GC) process. To reduce these overheads, data deduplication has been identified as an effective solution as it reduces the amount of written data to an SMR-based storage device. However, deduplication can result in poor data locality, leading to decreased read performance. To tackle this problem, this study proposes a data locality-aware deduplication technology, LaDy, that considers both the overheads of writing duplicate data and the impact on data locality to determine whether the duplicate data should be written. LaDy integrates with DiskSim, an open-source project, and modifies it to simulate an SMR-based drive. The experimental results demonstrate that LaDy can significantly reduce the response time in the best-case scenario by 87.3% compared with CAFTL on the SMR drive. LaDy achieves this by selectively writing duplicate data, which preserves data locality, resulting in improved read performance. The proposed solution provides an effective and efficient method for mitigating the performance overheads associated with data deduplication in SMR-based storage devices.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",data deduplication; disk technology; locality; Shingled magnetic recording; SMR,Magnetic recording; Magnetic storage; Data de duplications; Data locality; Deduplication; Disk technology; Locality; Locality aware; Performance; Read performance; Shingled magnetic recordings; Virtual storage
CRIMP: Compact & Reliable DNN Inference on In-Memory Processing via Crossbar-Aligned Compression and Non-ideality Adaptation,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171805904&doi=10.1145%2f3609115&partnerID=40&md5=55f1e4ca46cdbddc1263315729fbc853,"Crossbar-based In-Memory Processing (IMP) accelerators have been widely adopted to achieve high-speed and low-power computing, especially for deep neural network (DNN) models with numerous weights and high computational complexity. However, the floating-point (FP) arithmetic is not compatible with crossbar architectures. Also, redundant weights of current DNN models occupy too many crossbars, limiting the efficiency of crossbar accelerators. Meanwhile, due to the inherent non-ideal behavior of crossbar devices, like write variations, pre-trained DNN models suffer from accuracy degradation when it is deployed on a crossbar-based IMP accelerator for inference. Although some approaches are proposed to address these issues, they often fail to consider the interaction among these issues, and introduce significant hardware overhead for solving each issue. To deploy complex models on IMP accelerators, we should compact the model and mitigate the influence of device non-ideal behaviors without introducing significant overhead from each technique.In this paper, we first propose to reuse bit-shift units in crossbars for approximately multiplying scaling factors in our quantization scheme to avoid using FP processors. Second, we propose to apply kernel-group pruning and crossbar pruning to eliminate the hardware units for data aligning. We also design a zerorize-recover training process for our pruning method to achieve higher accuracy. Third, we adopt the runtime-aware non-ideality adaptation with a self-compensation scheme to relieve the impact of non-ideality by exploiting the feature of crossbars. Finally, we integrate these three optimization procedures into one training process to form a comprehensive learning framework for co-optimization, which can achieve higher accuracy. The experimental results indicate that our comprehensive learning framework can obtain significant improvements over the original model when inferring on the crossbar-based IMP accelerator, with an average reduction of computing power and computing area by 100.02× and 17.37×, respectively. Furthermore, we can obtain totally integer-only, pruned, and reliable VGG-16 and ResNet-56 models for the Cifar-10 dataset on IMP accelerators, with accuracy drops of only 2.19% and 1.26%, respectively, without any hardware overhead.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",In-memory processing; pruning; quantization; ReRAM non-ideality,Complex networks; Computing power; Digital arithmetic; Low power electronics; Neural network models; RRAM; Hardware overheads; Ideal behavior; In-memory processing; Neural network model; Nonideal; Nonideality; Pruning; Quantisation; ReRAM non-ideality; Training process; Deep neural networks
ViT4Mal: Lightweight Vision Transformer for Malware Detection on Edge Devices,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171795604&doi=10.1145%2f3609112&partnerID=40&md5=29aabef941e433d374ae259dc47fe376,"There has been a tremendous growth of edge devices connected to the network in recent years. Although these devices make our life simpler and smarter, they need to perform computations under severe resource and energy constraints, while being vulnerable to malware attacks. Once compromised, these devices are further exploited as attack vectors targeting critical infrastructure. Most existing malware detection solutions are resource and compute-intensive and hence perform poorly in protecting edge devices. In this paper, we propose a novel approach ViT4Mal that utilizes a lightweight vision transformer (ViT) for malware detection on an edge device. ViT4Mal first converts executable byte-code into images to learn malware features and later uses a customized lightweight ViT to detect malware with high accuracy. We have performed extensive experiments to compare our model with state-of-the-art CNNs in the malware detection domain. Experimental results corroborate that ViTs don't demand deeper networks to achieve comparable accuracy of around 97% corresponding to heavily structured CNN models. We have also performed hardware deployment of our proposed lightweight ViT4Mal model on the Xilinx PYNQ Z1 FPGA board by applying specialized hardware optimizations such as quantization, loop pipelining, and array partitioning. ViT4Mal achieved an accuracy of ∼94% and a 41x speedup compared to the original ViT model.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",FPGA; hardware optimization; inference latency; IoT; malware; matrix multiplication; resource-constrained; vision transformer (ViT),Constrained optimization; Internet of things; Malware; Hardware optimization; Inference latency; IoT; Malware detection; Malwares; MAtrix multiplication; Resource Constraint; Resource-constrained; Simple++; Vision transformer; Field programmable gate arrays (FPGA)
Probabilistic Reaction Time Analysis,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171793940&doi=10.1145%2f3609390&partnerID=40&md5=c5b0cbb900f90eb255c7a23153d2c954,"In many embedded systems, for instance, in the automotive, avionic, or robotics domain, critical functionalities are implemented via chains of communicating recurrent tasks. To ensure safety and correctness of such systems, guarantees on the reaction time, that is, the delay between a cause (e.g., an external activity or reading of a sensor) and the corresponding effect, must be provided.Current approaches focus on the maximum reaction time, considering the worst-case system behavior. However, in many scenarios, probabilistic guarantees on the reaction time are sufficient. That is, it is sufficient to provide a guarantee that the reaction does not exceed a certain threshold with (at least) a certain probability.This work provides such probabilistic guarantees on the reaction time, considering two types of randomness: response time randomness and failure probabilities. To the best of our knowledge, this is the first work that defines and analyzes probabilistic reaction time for cause-effect chains based on sporadic tasks. © 2023 Copyright held by the owner/author(s).",end-to-end; probability; Reaction time; sporadic,Random processes; 'current; Automotives; Embedded-system; End to end; Probabilistic guarantees; Probabilistic reaction; Recurrent tasks; Sporadics; Time analysis; Via chain; Embedded systems
PReFeR : Physically Related Function based Remote Attestation Protocol,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171795486&doi=10.1145%2f3609104&partnerID=40&md5=01ed5b5cc2fede29a23ac75d4de8925e,"Remote attestation is a request-response based security service that permits a trusted entity (verifier) to check the current state of an untrusted remote device (prover). The verifier initiates the attestation process by sending an attestation challenge to the prover; the prover responds with its current state, which establishes its trustworthiness. Physically Unclonable Function (PUF) offers an attractive choice for hybrid attestation schemes owing to its low overhead security guarantees. However, this comes with the limitation of secure storage of the PUF model or large challenge-response database on the verifier end. To address these issues, in this work, we propose a hybrid attestation framework, named PReFeR, that leverages a new class of hardware primitive known as Physically Related Function (PReF) to remotely attest low-end devices without the requirement of secure storage or heavy cryptographic operations. It comprises a static attestation scheme that validates the memory state of the remote device prior to code execution, followed by a dynamic run-time attestation scheme that asserts the correct code execution by evaluating the content of special registers present in embedded systems, known as hardware performance counters (HPC). The use of HPCs in the dynamic attestation scheme mitigates the popular class of attack known as the time-of-check-time-of-use (TOCTOU) attack, which has broken several state-of-the-art hybrid attestation schemes. We demonstrate our protocol and present our experimental results using a prototype implementation on Digilent Cora Z7 board, a low-cost embedded platform, specially designed for IoT applications.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",dynamic attestation; fuzzy extractor; hardware performance counters; Remote attestation,Codes (symbols); Cryptography; Hardware security; Network security; 'current; Code execution; Dynamic attestation; Fuzzy extractors; Hardware performance counters; Physically unclonable functions; Related functions; Remote attestation; Remote devices; Secure storage; Embedded systems
A Constructive State-based Semantics and Interpreter for a Synchronous Data-flow Language with State Machines,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171803552&doi=10.1145%2f3609131&partnerID=40&md5=0e0d1b9298f0807386ae5dd8c557b1cd,"Scade is a domain-specific synchronous functional language used to implement safety-critical real-time software for more than twenty years. Two main approaches have been considered for its semantics: (i) an indirect collapsing semantics based on a source-to-source translation of high-level constructs into a data-flow core language whose semantics is precisely specified and is the entry for code generation; a relational synchronous semantics, akin to Esterel, that applies directly to the source. It defines what is a valid synchronous reaction but hides, on purpose, if a semantics exists, is unique and can be computed; hence, it is not executable.This paper presents, for the first time, an executable, state-based semantics for a language that has the key constructs of Scade all together, in particular the arbitrary combination of data-flow equations and hierarchical state machines. It can apply directly to the source language before static checks and compilation steps. It is constructive in the sense that the language in which the semantics is defined is a statically typed functional language with call-by-value and strong normalization, e.g., it is expressible in a proof-assistant where all functions terminate. It leads to a reference, purely functional, interpreter. This semantics is modular and can account for possible errors, allowing to establish what property is ensured by each static verification performed by the compiler. It also clarifies how causality is treated in Scade compared with Esterel.This semantics can serve as an oracle for compiler testing and validation; to prototype novel language constructs before they are implemented, to execute possibly unfinished models or that are correct but rejected by the compiler; to prove the correctness of compilation steps.The semantics given in the paper is implemented as an interpreter in a purely functional style, in OCaml. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",dynamic semantics; embedded software; Programming language; synchronous programming,Ada (programming language); Data transfer; Equations of state; Esters; Program compilers; Program interpreters; Safety engineering; Dataflow; Dynamic semantic; Esterel; Executables; Functional languages; Purely functional; State based; State-machine; Synchronous data-flow languages; Synchronous programming; Semantics
iAware: Interaction Aware Task Scheduling for Reducing Resource Contention in Mobile Systems,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171788796&doi=10.1145%2f3609391&partnerID=40&md5=6a5e3b169a86a8f044fe06d77a1a2a26,"To ensure the user experience of mobile systems, the foreground application can be differentiated to minimize the impact of background applications. However, this article observes that system services in the kernel and framework layer, instead of background applications, are now the major resource competitors. Specifically, these service tasks tend to be quiet when people rarely interact with the foreground application and active when interactions become frequent, and this high overlap of busy times leads to contention for resources. This article proposes iAware, an interaction-aware task scheduling framework in mobile systems. The key insight is to make use of the previously ignored idle period and schedule service tasks to run at that period. iAware quantify the interaction characteristic based on the screen touch event, and successfully stagger the periods of frequent user interactions. With iAware, service tasks tend to run when few interactions occur, for example, when the device's screen is turned off, instead of when the user is frequently interacting with it. iAware is implemented on real smartphones. Experimental results show that the user experience is significantly improved with iAware. Compared to the state-of-the-art, the application launching speed and frame rate are enhanced by 38.89% and 7.97% separately, with no more than 1% additional battery consumption. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",mobile system; Task scheduling; user experience,Multitasking; Scheduling algorithms; User interfaces; Idle periods; Interaction characteristics; Mobile systems; Resource contention; Scheduling frameworks; Smart phones; System services; Tasks scheduling; User interaction; Users' experiences; Touch screens
Let Coarse-Grained Resources Be Shared: Mapping Entire Neural Networks on FPGAs,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172939993&doi=10.1145%2f3609109&partnerID=40&md5=fee7d6570995cd6119505ce2e7efed43,"Traditional HLS (High-Level Synthesis) provides rapid prototyping of hardware accelerators without coding with HDLs (Hardware Description Languages). However, such an approach does not well support allocating large applications like entire deep neural networks on a single FPGA (Field Programmable Gate Array) device. The approach leads to designs that are inefficient or do not fit into FPGAs due to resource constraints. This work proposes to shrink generated designs by coarse-grained resource control based on function sharing in functional IRs (Intermediate Representations). The proposed compiler passes and rewrite system aim at producing valid design points and removing redundant hardware. Such optimizations make fitting entire neural networks on FPGAs feasible and produce competitive performance compared to running specialized kernels for each layer. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",functional IRs; High-level synthesis; neural networks; rewrite rules,Computer hardware description languages; Deep neural networks; High level languages; High level synthesis; Logic Synthesis; Multilayer neural networks; Coarse-grained; Field programmables; Functional intermediate representation; Hardware accelerators; High-level synthesis; Intermediate representations; Neural-networks; Programmable gate array; Rapid-prototyping; Rewrite rules; Field programmable gate arrays (FPGA)
FedHIL: Heterogeneity Resilient Federated Learning for Robust Indoor Localization with Mobile Devices,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171788665&doi=10.1145%2f3607919&partnerID=40&md5=56cefe4f3221984c98a5bf5468a02b22,"Indoor localization plays a vital role in applications such as emergency response, warehouse management, and augmented reality experiences. By deploying machine learning (ML) based indoor localization frameworks on their mobile devices, users can localize themselves in a variety of indoor and subterranean environments. However, achieving accurate indoor localization can be challenging due to heterogeneity in the hardware and software stacks of mobile devices, which can result in inconsistent and inaccurate location estimates. Traditional ML models also heavily rely on initial training data, making them vulnerable to degradation in performance with dynamic changes across indoor environments. To address the challenges due to device heterogeneity and lack of adaptivity, we propose a novel embedded ML framework called FedHIL. Our framework combines indoor localization and federated learning (FL) to improve indoor localization accuracy in device-heterogeneous environments while also preserving user data privacy. FedHIL integrates a domain-specific selective weight adjustment approach to preserve the ML model's performance for indoor localization during FL, even in the presence of extremely noisy data. Experimental evaluations in diverse real-world indoor environments and with heterogeneous mobile devices show that FedHIL outperforms state-of-the-art FL and non-FL indoor localization frameworks. FedHIL is able to achieve 1.62 × better localization accuracy on average than the best performing FL-based indoor localization framework from prior work.  © 2023 Copyright held by the owner/author(s).",data privacy; Federated learning; neural networks; noise resilient; Wi-Fi fingerprinting,Augmented reality; Data privacy; Indoor positioning systems; Learning systems; Smartphones; Warehouses; Wireless local area networks (WLAN); Emergency response; Federated learning; Indoor environment; Indoor localization; Localization accuracy; Machine learning models; Machine-learning; Neural-networks; Noise resilient; Wi-fi fingerprinting; Wi-Fi
B-AWARE: Blockage Aware RSU Scheduling for 5G Enabled Autonomous Vehicles,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171764631&doi=10.1145%2f3609133&partnerID=40&md5=5af614f848a126ec868c5a91b5a4f5aa,"5G Millimeter Wave (mmWave) technology holds great promise for Connected Autonomous Vehicles (CAVs) due to its ability to achieve data rates in the Gbps range. However, mmWave suffers from a high beamforming overhead and requirement of line of sight (LOS) to maintain a strong connection. For Vehicle-to-Infrastructure (V2I) scenarios, where CAVs connect to roadside units (RSUs), these drawbacks become apparent. Because vehicles are dynamic, there is a large potential for link blockages. These blockages are detrimental to the connected applications running on the vehicle, such as cooperative perception and remote driver takeover. Existing RSU selection schemes base their decisions on signal strength and vehicle trajectory alone, which is not enough to prevent the blockage of links. Many modern CAVs motion planning algorithms routinely use other vehicle's near-future path plans, either by explicit communication among vehicles, or by prediction. In this paper, we make use of the knowledge of other vehicle's near future path plans to further improve the RSU association mechanism for CAVs. We solve the RSU association algorithm by converting it to a shortest path problem with the objective to maximize the total communication bandwidth. We evaluate our approach, titled B-AWARE, in simulation using Simulation of Urban Mobility (SUMO) and Digital twin for self-dRiving Intelligent VEhicles (DRIVE) on 12 highway and city street scenarios with varying traffic density and RSU placements. Simulations show B-AWARE results in a 1.05× improvement of the potential datarate in the average case and 1.28× in the best case vs. the state-of-the-art. But more impressively, B-AWARE reduces the time spent with no connection by 42% in the average case and 60% in the best case as compared to the state-of-the-art methods. This is a result of B-AWARE reducing nearly 100% of blockage occurrences. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",5G; autonomous vehicles; CAV; mmWave; RSU; selection; user association; V2I; vehicular networks,5G mobile communication systems; Digital storage; Graph theory; Millimeter waves; Motion planning; Vehicle to vehicle communications; 5g; Autonomous Vehicles; Connected autonomous vehicle; Data-rate; Path plan; Roadside units; Selection; User associations; Vehicular networks; Autonomous vehicles
EMS-i: An Efficient Memory System Design with Specialized Caching Mechanism for Recommendation Inference,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171791826&doi=10.1145%2f3609384&partnerID=40&md5=e9b47a3362981f6d895db45603b5719b,"Recommendation systems have been widely embedded into many Internet services. For example, Meta's deep learning recommendation model (DLRM) shows high prefictive accuracy of click-through rate in processing large-scale embedding tables. The SparseLengthSum (SLS) kernel of the DLRM dominates the inference time of the DLRM due to intensive irregular memory accesses to the embedding vectors. Some prior works directly adopt near data processing (NDP) solutions to obtain higher memory bandwidth to accelerate SLS. However, their inferior memory hierarchy induces low performance-cost ratio and fails to fully exploit the data locality. Although some software-managed cache policies were proposed to improve the cache hit rate, the incurred cache miss penalty is unacceptable considering the high overheads of executing the corresponding programs and the communication between the host and the accelerator. To address the issues aforementioned, we propose EMS-i, an efficient memory system design that integrates Solide State Drive (SSD) into the memory hierarchy using Compute Express Link (CXL) for recommendation system inference. We specialize the caching mechanism according to the characteristics of various DLRM workloads and propose a novel prefetching mechanism to further improve the performance. In addition, we delicately design the inference kernel and develop a customized mapping scheme for SLS operation, considering the multi-level parallelism in SLS and the data locality within a batch of queries. Compared to the state-of-the-art NDP solutions, EMS-i achieves up to 10.9× speedup over RecSSD and the performance comparable to RecNMP with 72% energy savings. EMS-i also saves up to 8.7× and 6.6 × memory cost w.r.t. RecSSD and RecNMP, respectively. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",compute express link; Recommendation system,Cyber Physical System; Data handling; Deep learning; Embedded systems; Embeddings; Energy efficiency; Memory architecture; Recommender systems; Caching mechanism; Clickthrough rates (CTR); Compute express link; Data locality; Embeddings; Internet-services; Memory hierarchy; Memory systems; Performance; Processing solutions; Systems analysis
Neural Abstraction-Based Controller Synthesis and Deployment,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171799944&doi=10.1145%2f3608104&partnerID=40&md5=c1602e5246acbfacd7610eb6aedd7f2e,"Abstraction-based techniques are an attractive approach for synthesizing correct-by-construction controllers to satisfy high-level temporal requirements. A main bottleneck for successful application of these techniques is the memory requirement, both during controller synthesis (to store the abstract transition relation) and in controller deployment (to store the control map).We propose memory-efficient methods for mitigating the high memory demands of the abstraction-based techniques using neural network representations. To perform synthesis for reach-avoid specifications, we propose an on-the-fly algorithm that relies on compressed neural network representations of the forward and backward dynamics of the system. In contrast to usual applications of neural representations, our technique maintains soundness of the end-to-end process. To ensure this, we correct the output of the trained neural network such that the corrected output representations are sound with respect to the finite abstraction. For deployment, we provide a novel training algorithm to find a neural network representation of the synthesized controller and experimentally show that the controller can be correctly represented as a combination of a neural network and a look-up table that requires a substantially smaller memory.We demonstrate experimentally that our approach significantly reduces the memory requirements of abstraction-based methods. We compare the performance of our approach with the standard abstraction-based synthesis on several models. For the selected benchmarks, our approach reduces the memory requirements respectively for the synthesis and deployment by a factor of 1.31× 105 and 7.13× 103 on average, and up to 7.54× 105 and 3.18× 104. Although this reduction is at the cost of increased off-line computations to train the neural networks, all the steps of our approach are parallelizable and can be implemented on machines with higher number of processing units to reduce the required computational time.  © 2023 Copyright held by the owner/author(s).",Abstraction-based control; compact representations; formal synthesis; neural networks,Abstracting; Table lookup; Abstraction-based control; Compact representation; Control maps; Controller synthesis; Correct-by-construction; Formal synthesis; Memory requirements; Network representation; Neural-networks; Transition relations; Controllers
Methods to Realize Preemption in Phased Execution Models,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171772641&doi=10.1145%2f3609132&partnerID=40&md5=cd859793a6f977938b36f64546fa54ea,"Phased execution models are a good solution to tame the increased complexity and contention of commercial off-the-shelf (COTS) multi-core platforms, e.g., Acquisition-Execution-Restitution (AER) model, PRedictable Execution Model (PREM). Such models separate execution from access to shared resources on the platform to minimize contention. All data and instructions needed during an execution phase are copied into the local memory of the core before starting to execute. Phased execution models are generally used with non-preemptive scheduling to increase predictability. However, the blocking time in non-preemptive systems can reduce schedulability. Therefore, an investigation of preemption methods for phased execution models is warranted. Although, preemption for phased execution models must be carefully designed to retain its execution semantics, i.e., the handling of local memory during preemption becomes non-trivial.This paper investigates different methods to realize preemption in phased execution models while preserving their semantics. To the best of our knowledge, this is the first paper to explore different approaches to implement preemption in phased execution models from the perspective of data management. We introduce two strategies to realize preemption of execution phases based on different methods of handling local data of the preempted task. Heuristics are used to create time-triggered schedules for task sets that follow the proposed preemption methods. Additionally, a schedulability-aware preemption heuristic is proposed to reduce the number of preemptions by allowing preemption only when it is beneficial in terms of schedulability. Evaluations on a large number of synthetic task sets are performed to compare the proposed preemption models against each other and against a non-preemptive version. Furthermore, our schedulability-aware preemption heuristic has higher schedulability with a clear margin in all our experiments compared to the non-preemptive and fully-preemptive versions. © 2023 Copyright held by the owner/author(s).",Phased execution model; preemptive scheduling,Computer architecture; Data handling; Heuristic methods; Semantics; Commercial off the shelves; Commercial off-the shelves; Commercial-off-the-shelf; Execution modeling; Execution phasis; Local memory; Non-preemptive; Phased execution model; Pre-emptive scheduling; Schedulability; Information management
BASS: Safe Deep Tissue Optical Sensing for Wearable Embedded Systems,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171797563&doi=10.1145%2f3607916&partnerID=40&md5=905843b33e216ca1eabc9c942c86eb46,"In wearable optical sensing applications whose target tissue is not superficial, such as deep tissue oximetry, the task of embedded system design has to strike a balance between two competing factors. On one hand, the sensing task is assisted by increasing the radiated energy into the body, which in turn, improves the signal-to-noise ratio (SNR) of the deep tissue at the sensor. On the other hand, patient safety consideration imposes a constraint on the amount of radiated energy into the body. In this paper, we study the trade-offs between the two factors by exploring the design space of the light source activation pulse.Furthermore, we propose BASS, an algorithm that leverages the activation pulse design space exploration, which further optimizes deep tissue SNR via spectral averaging, while ensuring the radiated energy into the body meets a safe upper bound. The effectiveness of the proposed technique is demonstrated via analytical derivations, simulations, and in vivo measurements in both pregnant sheep models and human subjects.  © 2023 Copyright held by the owner/author(s).",deep tissue optical sensing; design space exploration; internet of medical things; Medical cyber-physical systems; multi-objective optimization; wearable embedded systems,Chemical activation; Cyber Physical System; Economic and social effects; Embedded systems; Light sources; Multiobjective optimization; Signal to noise ratio; Activation pulse; Deep tissue optical sensing; Design space exploration; Embedded-system; Internet of medical thing; Medical cyber physical systems; Multi-objectives optimization; Optical sensing; Radiated energies; Wearable embedded system; Tissue
MaGNAS: A Mapping-Aware Graph Neural Architecture Search Framework for Heterogeneous MPSoC Deployment,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171793482&doi=10.1145%2f3609386&partnerID=40&md5=52a539f482f9a11ff35575a932031dae,"Graph Neural Networks (GNNs) are becoming increasingly popular for vision-based applications due to their intrinsic capacity in modeling structural and contextual relations between various parts of an image frame. On another front, the rising popularity of deep vision-based applications at the edge has been facilitated by the recent advancements in heterogeneous multi-processor Systems on Chips (MPSoCs) that enable inference under real-time, stringent execution requirements. By extension, GNNs employed for vision-based applications must adhere to the same execution requirements. Yet contrary to typical deep neural networks, the irregular flow of graph learning operations poses a challenge to running GNNs on such heterogeneous MPSoC platforms. In this paper, we propose a novel unified design-mapping approach for efficient processing of vision GNN workloads on heterogeneous MPSoC platforms. Particularly, we develop MaGNAS, a mapping-aware Graph Neural Architecture Search framework. MaGNAS proposes a GNN architectural design space coupled with prospective mapping options on a heterogeneous SoC to identify model architectures that maximize on-device resource efficiency. To achieve this, MaGNAS employs a two-tier evolutionary search to identify optimal GNNs and mapping pairings that yield the best performance trade-offs. Through designing a supernet derived from the recent Vision GNN (ViG) architecture, we conducted experiments on four (04) state-of-the-art vision datasets using both (i) a real hardware SoC platform (NVIDIA Xavier AGX) and (ii) a performance/cost model simulator for DNN accelerators. Our experimental results demonstrate that MaGNAS is able to provide 1.57× latency speedup and is 3.38× more energy-efficient for several vision datasets executed on the Xavier MPSoC vs. the GPU-only deployment while sustaining an average 0.11% accuracy reduction from the baseline. © 2023 Copyright held by the owner/author(s).",edge computing; Graph neural networks; HW-SW codesign; MPSoCs,Economic and social effects; Edge computing; Energy efficiency; Flow graphs; Graph neural networks; Integrated circuit design; Multiprocessing systems; Network architecture; Network coding; Real time systems; System-on-chip; Edge computing; Graph neural networks; HW/SW Codesign; Image frames; Intrinsic capacity; Multi processor system on chips; Neural architectures; Real- time; System-on-chip platforms; Vision-based applications; Mapping
BitSET: Bit-Serial Early Termination for Computation Reduction in Convolutional Neural Networks,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171781141&doi=10.1145%2f3609093&partnerID=40&md5=29b8bafc1eb052b88bd6b78b088993df,"Convolutional Neural Networks (CNNs) have demonstrated remarkable performance across a wide range of machine learning tasks. However, the high accuracy usually comes at the cost of substantial computation and energy consumption, making it difficult to be deployed on mobile and embedded devices. In CNNs, the compute-intensive convolutional layers are usually followed by a ReLU activation layer, which clamps negative outputs to zeros, resulting in large activation sparsity. By exploiting such sparsity in CNN models, we propose a software-hardware co-design BitSET, that aggressively saves energy during CNN inference. The bit-serial BitSET accelerator adopts a prediction-based bit-level early termination technique that terminates the ineffectual computation of negative outputs early. To assist the algorithm, we propose a novel weight encoding that allows more accurate predictions with fewer bits. BitSET leverages the bit-level computation reduction both in the predictive early termination algorithm and in the non-predictive, energy-efficient bit-serial architecture. Compared to UNPU, an energy-efficient bit-serial CNN accelerator, BitSET yields an average 1.5× speedup and 1.4× energy efficiency improvement with no accuracy loss due to a 48% reduction in bit-level computations. Relaxing the allowed accuracy loss to 1% increases the gains to an average of 1.6× speedup and 1.4× energy efficiency improvement.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",accelerator; approximate computing; CNN; Convolutional neural networks; deep neural networks; DNN; early termination; software-hardware co-design,Chemical activation; Computational efficiency; Convolution; Convolutional codes; Convolutional neural networks; Energy efficiency; Energy utilization; Hardware-software codesign; Approximate computing; Bit level; Bit-serial; Computation reduction; Convolutional neural network; DNN; Early termination; Negative outputs; Software/hardware co designs; Deep neural networks
IOSR: Improving I/O Efficiency for Memory Swapping on Mobile Devices Via Scheduling and Reshaping,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171765468&doi=10.1145%2f3607923&partnerID=40&md5=00fd96ab54e8b455aef00cc6191394c5,"Mobile systems and applications are becoming increasingly feature-rich and powerful, which constantly suffer from memory pressure, especially for devices equipped with limited DRAM. Swapping inactive DRAM pages to the storage device is a promising solution to extend the physical memory. However, existing mobile devices usually adopt flash memory as the storage device, where swapping DRAM pages to flash memory may introduce significant performance overhead. In this paper, we first conduct an in-depth analysis of the I/O characteristics of the flash-based memory swapping, including the I/O interference and swap I/O randomness in swap subsystem. Then an I/O efficiency optimization framework for memory swapping (IOSR) is proposed to enhance the performance of flash-based memory swapping for mobile devices. IOSR consists of two methods: swap I/O scheduling (SIOS) and swap I/O pattern reshaping (SIOR). SIOS is designed to schedule the swap I/O to reduce interference with other processes I/Os. SIOR is designed to reshape the swap I/O pattern with process-oriented swap slot allocation and adaptive granularity swap read-ahead. IOSR is implemented on Google Pixel 4. Experimental results show that IOSR reduces the application switching time by 31.7% and improves the swap-in bandwidth by 35.5% on average compared to the state-of-the-art.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",memory swapping; Mobile device; swap I/O management; user experience,Air traffic control; Dynamic random access storage; Flash memory; Virtual storage; Efficiency optimization; In-depth analysis; Memory pressure; Memory swapping; Mobile applications; Mobile systems; Performance; Physical memory; Swap I/O management; Users' experiences; Efficiency
Formal Synthesis of Neural Barrier Certificates for Continuous Systems via Counterexample Guided Learning,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171769824&doi=10.1145%2f3609125&partnerID=40&md5=1e2263a41f6a5581675bc4b33f6a02ff,"This paper presents a novel approach to safety verification based on neural barrier certificates synthesis for continuous dynamical systems. We construct the synthesis framework as an inductive loop between a Learner and a Verifier based on barrier certificate learning and counterexample guidance. Compared with the counterexample-guided verification method based on the SMT solver, we design and learn neural barrier functions with special structure, and use the special form to convert the counterexample generation into a polynomial optimization problem for obtaining the optimal counterexample. In the verification phase, the task of identifying the real barrier certificate can be tackled by solving the Linear Matrix Inequalities (LMI) feasibility problem, which is efficient and makes the proposed method formally sound. The experimental results demonstrate that our approach is more effective and practical than the traditional SOS-based barrier certificates synthesis and the state-of-the-art neural barrier certificates learning approach.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",barrier certificate; continuous dynamical systems; counterexample guidance; Formal verification,Dynamical systems; Learning systems; Linear matrix inequalities; Barrier certificates; Continuous dynamical system; Continuous system; Counterexample guidance; Formal synthesis; Inductive loops; Learn+; Safety verification; Verification method; Verifier-based; Formal verification
Predictable GPU Wavefront Splitting for Safety-Critical Systems,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171787910&doi=10.1145%2f3609102&partnerID=40&md5=2ccde1c82f7a64a2db350c77ba6807bc,"We present a predictable wavefront splitting (PWS) technique for graphics processing units (GPUs). PWS improves the performance of GPU applications by reducing the impact of branch divergence while ensuring that worst-case execution time (WCET) estimates can be computed. This makes PWS an appropriate technique to use in safety-critical applications, such as autonomous driving systems, avionics, and space, that require strict temporal guarantees. In developing PWS on an AMD-based GPU, we propose microarchitectural enhancements to the GPU, and a compiler pass that eliminates branch serializations to reduce the WCET of a wavefront. Our analysis of PWS exhibits a performance improvement of 11% over existing architectures with a lower WCET than prior works in wavefront splitting.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",GPU; safety-critical systems,Computer graphics; Computer graphics equipment; Program processors; Safety engineering; Security systems; Wavefronts; Appropriate techniques; Autonomous driving; Driving systems; Existing architectures; Performance; Safety critical applications; Safety critical systems; Splitting techniques; Wavefront splitting; Worst-case execution time; Graphics processing unit
Probabilistic Black-Box Checking via Active MDP Learning,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171777155&doi=10.1145%2f3609127&partnerID=40&md5=e27e52e7e2c3b56ca699eee833d4643a,"We introduce a novel methodology for testing stochastic black-box systems, frequently encountered in embedded systems. Our approach enhances the established black-box checking (BBC) technique to address stochastic behavior. Traditional BBC primarily involves iteratively identifying an input that breaches the system's specifications by executing the following three phases: the learning phase to construct an automaton approximating the black box's behavior, the synthesis phase to identify a candidate counterexample from the learned automaton, and the validation phase to validate the obtained candidate counterexample and the learned automaton against the original black-box system. Our method, ProbBBC, refines the conventional BBC approach by (1) employing an active Markov Decision Process (MDP) learning method during the learning phase, (2) incorporating probabilistic model checking in the synthesis phase, and (3) applying statistical hypothesis testing in the validation phase. ProbBBC uniquely integrates these techniques rather than merely substituting each method in the traditional BBC; for instance, the statistical hypothesis testing and the MDP learning procedure exchange information regarding the black-box system's observation with one another. The experiment results suggest that ProbBBC outperforms an existing method, especially for systems with limited observation.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",automata learning; probabilistic model checking; stochastic systems; Testing,Black-box testing; Embedded systems; Iterative methods; Learning systems; Markov processes; Model checking; Stochastic models; Automaton learning; Black box system; Black boxes; Learning phasis; Markov Decision Processes; Probabilistic model checking; Probabilistic model-checking; Process learning; Synthesis phase; Validation phase; Stochastic systems
Towards Building Verifiable CPS using Lingua Franca,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171787934&doi=10.1145%2f3609134&partnerID=40&md5=63cbe1502058a6d3793328ca6f9117f3,"Formal verification of cyber-physical systems (CPS) is challenging because it has to consider real-time and concurrency aspects that are often absent in ordinary software. Moreover, the software in CPS is often complex and low-level, making it hard to assure that a formal model of the system used for verification is a faithful representation of the actual implementation, which can undermine the value of a verification result. To address this problem, we propose a methodology for building verifiable CPS based on the principle that a formal model of the software can be derived automatically from its implementation. Our approach requires that the system implementation is specified in Lingua Franca (LF), a polyglot coordination language tailored for real-time, concurrent CPS, which we made amenable to the specification of safety properties via annotations in the code. The program structure and the deterministic semantics of LF enable automatic construction of formal axiomatic models directly from LF programs. The generated models are automatically checked using Bounded Model Checking (BMC) by the verification engine Uclid5 using the Z3 SMT solver. The proposed technique enables checking a well-defined fragment of Safety Metric Temporal Logic (Safety MTL) formulas. To ensure the completeness of BMC, we present a method to derive an upper bound on the completeness threshold of an axiomatic model based on the semantics of LF. We implement our approach in the LF Verifier and evaluate it using a benchmark suite with 22 programs sampled from real-life applications and benchmarks for Erlang, Lustre, actor-oriented languages, and RTOSes. The LF Verifier correctly checks 21 out of 22 programs automatically. © 2023 Copyright held by the owner/author(s).",automated verification; axiomatic modeling; concurrency; Cyber-physical systems; model-based design; safety MTL,Application programs; Benchmarking; Embedded systems; Formal verification; Linearization; Model checking; Real time systems; Semantics; Automated verification; Axiomatic models; Bounded model checking; Concurrency; Cybe-physical systems; Cyber-physical systems; Formal modeling; Model-based design; Real- time; Safety MTL; Cyber Physical System
Equation-Directed Axiomatization of Lustre Semantics to Enable Optimized Code Validation,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171784551&doi=10.1145%2f3609393&partnerID=40&md5=86ee850c5a80b8eeff269c712c577f29,"Model-based design tools like SCADE Suite and Simulink are often used to design safety-critical embedded software. Consequently, generating correct code from such models is crucial. We tackle this challenge on Lustre, a dataflow synchronous language that embodies the concepts that base such tools. Instead of proving correct a whole code generator, we turn an existing compiler into a certifying compiler from Lustre to C, following a translation validation approach.We propose a solution that generates both C code and an attached specification expressing a correctness result for the generated and optionally optimized code. The specification yields proof obligations that are discharged by external solvers through the Frama-C platform. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",ACSL; Frama-C; Lustre,C (programming language); Program compilers; Safety engineering; Semantics; Simulink; ACSL; Axiomati-sation; Code validation; Correct code; Design safety; Design tool; Frama-C; Luster; Model-based design; Simulink; Specifications
Kryptonite: Worst-Case Program Interference Estimation on Multi-Core Embedded Systems,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171791914&doi=10.1145%2f3609128&partnerID=40&md5=8c268e4c6ebac9d56c3ab544d858447d,"Due to the low costs and energy needed, cyber-physical systems are adopting multi-core processors for their embedded computing requirements. In order to guarantee safety when the application has real-time constraints, a critical requirement is to estimate the worst-case interference from other executing programs. However, the complexity of multi-core hardware inhibits precisely determining the Worst-Case Program Interference. Existing solutions are either prone to overestimate the interference or are not scalable to different hardware sizes and designs.In this paper we present Kryptonite, an automated framework to synthesize Worst-Case Program Interference (WCPI) environments for multi-core systems. Fundamental to Kryptoniteis a set of tiny hardware-specific code gadgets that are crafted to maximize interference locally. The gadgets are arranged using a greedy approach and then molded using a Reinforcement Learning algorithm to create the WCPI environment. We demonstrate Kryptoniteon the automotive grade Infineon AURIX TC399 processor with a wide range of programs that includes a commercial real-time automotive application. We show that, while being easily scalable and tunable, Kryptonitecreates WCPI environments increasing the runtime by up to 58% for benchmark applications and 26% for the automotive application.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Program interference; worst-case analysis,Application programs; Embedded systems; Learning algorithms; Reinforcement learning; Automotive applications; Cybe-physical systems; Cyber-physical systems; Interference environments; Interference estimation; Low-costs; Lower energies; Multicore embedded system; Program interference; Worst-case analysis; Benchmarking
CrossTalk: Making Low-Latency Fault Tolerance Cheap by Exploiting Redundant Networks,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171781707&doi=10.1145%2f3609436&partnerID=40&md5=1c77f14b6bf2630fd5ecb1cd296b1748,"Real-time embedded systems perform many important functions in the modern world. A standard way to tolerate faults in these systems is with Byzantine fault-tolerant (BFT) state machine replication (SMR), in which multiple replicas execute the same software and their outputs are compared by the actuators. Unfortunately, traditional BFT SMR protocols are slow, requiring replicas to exchange sensor data back and forth over multiple rounds in order to reach agreement before each execution. The state of the art in reducing the latency of BFT SMR is eager execution, in which replicas execute on data from different sensors simultaneously on different processor cores. However, this technique results in 3-5× higher computation overheads compared to traditional BFT SMR systems, significantly limiting schedulability.We present CrossTalk, a new BFT SMR protocol that leverages the prevalence of redundant switched networks in embedded systems to reduce latency without added computation. The key idea is to use specific algorithms to move messages between redundant network planes (which many systems already possess) as the messages travel from the sensors to the replicas. As a result, CrossTalk can ensure agreement automatically in the network, avoiding the need for any communication between replicas. Our evaluation shows that CrossTalk improves schedulability by 2.13-4.24× over the state of the art. Moreover, in a NASA simulation of a real spaceflight mission, CrossTalk tolerates more faults than the state of the art while using nearly 3× less processor time.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Byzantine fault tolerance; distributed systems; real-time systems; state machine replication,Crosstalk; Distributed computer systems; Embedded systems; Fault tolerance; Fault tolerant computer systems; Interactive computer systems; NASA; Byzantine fault; Byzantine fault tolerance; Distributed systems; Fault-tolerant state; Low latency; Real - Time system; Replication protocol; Schedulability; State machine replication; State of the art; Real time systems
Verified Compilation of Synchronous Dataflow with State Machines,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171746172&doi=10.1145%2f3608102&partnerID=40&md5=f861f85cbb85494895cb149c08746434,"Safety-critical embedded software is routinely programmed in block-diagram languages. Recent work in the Vélus project specifies such a language and its compiler in the Coq proof assistant. It builds on the CompCert verified C compiler to give an end-to-end proof linking the dataflow semantics of source programs to traces of the generated assembly code. We extend this work with switched blocks, shared variables, reset blocks, and state machines; define a relational semantics to integrate these block- and mode-based constructions into the existing stream-based model; adapt the standard source-to-source rewriting scheme to compile the new constructions; and reestablish the correctness theorem.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",interactive theorem proving; Stream languages; verified compilation,C (programming language); Data flow analysis; Program compilers; Semantics; Theorem proving; Block diagrams; C compilers; Coq proof assistant; Diagram languages; End to end; Interactive theorem proving; State-machine; Stream languages; Synchronous Dataflow; Verified compilation; Safety engineering
ZPP: A Dynamic Technique to Eliminate Cache Pollution in NoC based MPSoCs,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171758168&doi=10.1145%2f3609113&partnerID=40&md5=a112392abff0dd82b1b9bed8efd3c994,"Data prefetching efficiently reduces the memory access latency in NUCA architectures as the Last Level Cache (LLC) is shared and distributed across multiple cores. But cache pollution generated by prefetcher reduces its efficiency by causing contention for shared resources such as LLC and the underlying network. The paper proposes Zero Pollution Prefetcher (ZPP) that eliminates cache pollution for NUCA architecture. For this purpose, ZPP uses L1 prefetcher and places the prefetched blocks in the data locations of LLC where modified blocks are stored. Since modified blocks in LLC are stale and request for such blocks are served from the exclusively owned private cache, their space unnecessary consumes power to maintain such stale data in the cache. The benefits of ZPP are (a) Eliminates cache pollution in L1 and LLC by storing prefetched blocks in LLC locations where stale blocks are stored. (b) Insufficient cache space is solved by placing prefetched blocks in LLC as LLCs are larger in size than L1 cache. This helps in prefetching more cache blocks, thereby increasing prefetch aggressiveness. (c) Increasing prefetch aggressiveness increases its coverage. (d) It also maintains an equivalent lookup latency to L1 cache for prefetched blocks. Experimentally it has been found that ZPP increases weighted speedup by 2.19x as compared to a system with no prefetching while prefetch coverage and prefetch accuracy increases by 50%, and 12%, respectively compared to the baseline.1 © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cache pollution; last level cache; network on chip; Prefetching; tiled chip MultiProcessor,Cache memory; Memory architecture; Network architecture; Cache pollution; Chip Multiprocessor; Data-prefetching; Dynamic techniques; L1 caches; Last-level caches; Networks on chips; Prefetches; Prefetching; Tiled chip multiprocessor; Network-on-chip
AxOTreeS: A Tree Search Approach to Synthesizing FPGA-based Approximate Operators,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171737454&doi=10.1145%2f3609096&partnerID=40&md5=72562512a2eea4d7c3189bd7b62343c1,"Approximate computing (AxC) provides the scope for achieving disproportionate gains in a system's power, performance, and area (PPA) metrics by leveraging an application's inherent error-resilient behavior (BEHAV). Trading computational accuracy for performance gains makes AxC an attractive proposition for implementing computationally complex AI/ML-based applications on resource-constrained embedded systems. The growing diversity of application domains using AI/ML has also led to the increasing usage of FPGA-based embedded systems. However, implementing AxC for FPGAs has primarily been limited to the post-processing of ASIC-optimized approximate operators (AxOs). This approach usually involves selecting from a set of AxOs that have been optimized for a gate-based implementation in an ASIC. While such an approach does allow leveraging existing knowledge of ASIC-based AxO design, it limits the scope for considering the challenges and opportunities associated with FPGA's LUT-based computation structures. Similarly, the few works considering the LUT-based computing for AxO design use generic optimization approaches that do not allow integrating problem-specific prior knowledge - empirical and/or statistical. To this end, we propose a novel tree search-based approach to AxO synthesis for FPGAs. Specifically, we present a design methodology using Monte Carlo Tree Search (MCTS)-based search tree traversal that allows the designer to integrate statistical data, such as correlation, into the AxOs optimization. With the proposed methods, we report improvements over standard MCTS algorithm-based results as well as improved hypervolume for both operator-level and application-specific DSE, compared to state-of-the-art design methodologies.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",AI-based exploration; Approximate computing; arithmetic operator design; automated hardware design; circuit synthesis; computer arithmetic; Monte Carlo Tree Search,Application specific integrated circuits; Artificial intelligence; Computer hardware; Embedded systems; Integrated circuit design; Knowledge management; Monte Carlo methods; Timing circuits; Trees (mathematics); AI-based exploration; Approximate computing; Arithmetic operator design; Automated hardware design; Circuit synthesis; Computer arithmetic; Hardware design; Monte carlo tree search; Search-based; Tree-search; Field programmable gate arrays (FPGA)
Mining Hyperproperties using Temporal Logics,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171751823&doi=10.1145%2f3609394&partnerID=40&md5=d74b4bdaa99b7b57d8171c7d7279f327,"Formal specifications are essential to express precisely systems, but they are often difficult to define or unavailable. Specification mining aims to automatically infer specifications from system executions. The existing literature mainly focuses on learning properties defined on single system executions. However, many system characteristics, such as security policies and robustness, require relating two or more executions, and hence cannot be captured by properties. Hyperproperties address this limitation by allowing simultaneous reasoning about multiple executions with quantification over system traces.In this paper, we propose an effective approach for mining Hyper Signal Temporal Logic (HyperSTL) specifications. Our approach is based on the syntax-guided synthesis framework and allows users to control the amount of prior knowledge embedded in the mining procedure. To the best of our knowledge, this is the first mining method for hyperproperties that does not require a pre-defined template as input and allows for quantifier alternation. We implemented our approach and demonstrated its applicability and versatility in several case studies where we showed that we can use the same method to mine specifications both with and without templates, but also to infer subsets of HyperSTL, including STL, HyperLTL, LTL and non-temporal specifications. © 2023 Copyright held by the owner/author(s).",Cyber-Physical Systems; Hyperproperties; Specification Mining,Data mining; Embedded systems; Formal specification; Temporal logic; Cybe-physical systems; Cyber-physical systems; Effective approaches; Hyperproperty; Prior-knowledge; Property; Security policy; Specification mining; System characteristics; Temporal logic specifications; Cyber Physical System
GHOST: A Graph Neural Network Accelerator using Silicon Photonics,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171740054&doi=10.1145%2f3609097&partnerID=40&md5=2e9ec9c82d4e9628760080192b38559e,"Graph neural networks (GNNs) have emerged as a powerful approach for modelling and learning from graph-structured data. Multiple fields have since benefitted enormously from the capabilities of GNNs, such as recommendation systems, social network analysis, drug discovery, and robotics. However, accelerating and efficiently processing GNNs require a unique approach that goes beyond conventional artificial neural network accelerators, due to the substantial computational and memory requirements of GNNs. The slowdown of scaling in CMOS platforms also motivates a search for alternative implementation substrates. In this paper, we present GHOST, the first silicon-photonic hardware accelerator for GNNs. GHOST efficiently alleviates the costs associated with both vertex-centric and edge-centric operations. It implements separately the three main stages involved in running GNNs in the optical domain, allowing it to be used for the inference of various widely used GNN models and architectures, such as graph convolution networks and graph attention networks. Our simulation studies indicate that GHOST exhibits at least 10.2 × better throughput and 3.8 × better energy efficiency when compared to GPU, TPU, CPU and multiple state-of-the-art GNN hardware accelerators.  © 2023 Copyright held by the owner/author(s).",Graph Neural Networks; optical computing; silicon photonics,Energy efficiency; Graph neural networks; Optical data processing; Photonic devices; Computational requirements; Drug discovery; Graph neural networks; Graph structured data; Hardware accelerators; Memory requirements; Processing graphs; Scalings; Silicon photonics; Social Network Analysis; Silicon photonics
Stochastic Analysis of Control Systems Subject to Communication and Computation Faults,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171742469&doi=10.1145%2f3609123&partnerID=40&md5=f962fa9bdc148340c4231c9d5e75c286,"Control theory allows one to design controllers that are robust to external disturbances, model simplification, and modelling inaccuracy. Researchers have investigated whether the robustness carries on to the controller's digital implementation, mostly looking at how the controller reacts to either communication or computational problems. Communication problems are typically modelled using random variables (i.e., estimating the probability that a fault will occur during a transmission), while computational problems are modelled using deterministic guarantees on the number of deadlines that the control task has to meet. These fault models allow the engineer to both design robust controllers and assess the controllers' behaviour in the presence of isolated faults. Despite being very relevant for the real-world implementations of control system, the question of what happens when these faults occur simultaneously does not yet have a proper answer. In this paper, we answer this question in the stochastic setting, using the theory of Markov Jump Linear Systems to provide stability contracts with almost sure guarantees of convergence. For linear time-invariant Markov jump linear systems, mean square stability implies almost sure convergence - a property that is central to our investigation. Our research primarily emphasises the validation of this property for closed-loop systems that are subject to packet losses and computational overruns, potentially occurring simultaneously. We apply our method to two case studies from the recent literature and show their robustness to a comprehensive set of faults. We employ closed-loop system simulations to empirically derive performance metrics that elucidate the quality of the controller implementation, such as the system settling time and the integral absolute error.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Computational problems; deadline misses; packet dropouts;,Closed loop systems; Computation theory; Linear control systems; Markov processes; Packet loss; Robustness (control systems); Stochastic systems; System stability; Closed-loop system; Communication problems; Computational problem; Deadline miss; Design controllers; Markov jump linear systems; Packet dropout;; Packet dropouts; Property; Stochastic analysis; Controllers
Rectifying Skewed Kernel Page Reclamation in Mobile Devices for Improving User-Perceivable Latency,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171757260&doi=10.1145%2f3607937&partnerID=40&md5=5967a32749c131fd13de1a6d7e8200df,"A crucial design factor for users of smart mobile devices is the latency of graphical interface interaction. Switching a background app to foreground is a frequent operation on mobile devices and the latency of this process is highly perceivable to users. Based on an Android smartphone, through analysis of memory reference generated during the app-switching process, we observe that file (virtual) pages and anonymous pages are both heavily involved. However, to our surprise, the amounts of the two types of pages in the main memory are highly imbalanced, and frequent I/O operations on file pages noticeably slows down the app-switching process. In this study, we advocate to improve the app-switching latency by rectifying the skewed kernel page reclaiming. Our approach involves two parts: proactive identification of unused anonymous pages and adaptive balance between file pages and anonymous pages. As mobile apps are found inflating their anonymous pages, we propose identifying unused anonymous pages in sync with the app-switching events. In addition, Android devices replaces the swap device with RAM-based zram, and swapping on zram is much faster than file accessing on flash storage. Without causing thrashing, we propose swapping out as many anonymous pages to zram as possible for caching more file pages. We conduct experiments on a Google Pixel phone with realistic user workloads, and results confirm that our method is adaptive to different memory requirements and greatly improves the app-switching latency by up to 43% compared with the original kernel.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Android; embedded systems; operating systems; Virtual memory,Embedded systems; Flash memory; Random access storage; Android; Android smartphone; Design factors; Embedded-system; Graphical interface; Interface interaction; Memory references; Operating system; Switching process; Virtual memory; Android (operating system)
SpikeHard: Efficiency-Driven Neuromorphic Hardware for Heterogeneous Systems-on-Chip,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171738087&doi=10.1145%2f3609101&partnerID=40&md5=07b3c8e30754cad3bf2ba660ab02137b,"Neuromorphic computing is an emerging field with the potential to offer performance and energy-efficiency gains over traditional machine learning approaches. Most neuromorphic hardware, however, has been designed with limited concerns to the problem of integrating it with other components in a heterogeneous System-on-Chip (SoC). Building on a state-of-the-art reconfigurable neuromorphic architecture, we present the design of a neuromorphic hardware accelerator equipped with a programmable interface that simplifies both the integration into an SoC and communication with the processor present on the SoC. To optimize the allocation of on-chip resources, we develop an optimizer to restructure existing neuromorphic models for a given hardware architecture, and perform design-space exploration to find highly efficient implementations. We conduct experiments with various FPGA-based prototypes of many-accelerator SoCs, where Linux-based applications running on a RISC-V processor invoke Pareto-optimal implementations of our accelerator alongside third-party accelerators. These experiments demonstrate that our neuromorphic hardware, which is up to 89× faster and 170× more energy efficient after applying our optimizer, can be used in synergy with other accelerators for different application purposes.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",machine learning; Neuromorphic computing; spiking neural network,Computer hardware; Computer operating systems; Energy efficiency; Integrated circuit design; Interface states; Machine learning; Network architecture; Neural networks; Pareto principle; Programmable logic controllers; Reconfigurable architectures; Systems analysis; Efficiency gain; Heterogeneous systems; Machine-learning; Neural-networks; Neuromorphic computing; Neuromorphic hardwares; Optimizers; Performance; Spiking neural network; Systems-on-Chip; System-on-chip
A Self-Sustained CPS Design for Reliable Wildfire Monitoring,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171735531&doi=10.1145%2f3608100&partnerID=40&md5=1e2a26a48be5f06b0cf46e820074f5fd,"Continuous monitoring of areas nearby the electric grid is critical for preventing and early detection of devastating wildfires. Existing wildfire monitoring systems are intermittent and oblivious to local ambient risk factors, resulting in poor wildfire awareness. Ambient sensor suites deployed near the gridlines can increase the monitoring granularity and detection accuracy. However, these sensors must address two challenging and competing objectives at the same time. First, they must remain powered for years without manual maintenance due to their remote locations. Second, they must provide and transmit reliable information if and when a wildfire starts. The first objective requires aggressive energy savings and ambient energy harvesting, while the second requires continuous operation of a range of sensors. To the best of our knowledge, this paper presents the first self-sustained cyber-physical system that dynamically co-optimizes the wildfire detection accuracy and active time of sensors. The proposed approach employs reinforcement learning to train a policy that controls the sensor operations as a function of the environment (i.e., current sensor readings), harvested energy, and battery level. The proposed cyber-physical system is evaluated extensively using real-life temperature, wind, and solar energy harvesting datasets and an open-source wildfire simulator. In long-term (5 years) evaluations, the proposed framework achieves 89% uptime, which is 46% higher than a carefully tuned heuristic approach. At the same time, it averages a 2-minute initial response time, which is at least 2.5× faster than the same heuristic approach. Furthermore, the policy network consumes 0.6 mJ per day on the TI CC2652R microcontroller using TensorFlow Lite for Micro, which is negligible compared to the daily sensor suite energy consumption.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",decision making; edge device; energy harvesting; IoT; resource management; self-sustainable; sensing; Wildfire monitoring,Cyber Physical System; Embedded systems; Energy conservation; Energy harvesting; Energy utilization; Heuristic methods; Internet of things; Open systems; Reinforcement learning; Solar energy; Cybe-physical systems; Decisions makings; Detection accuracy; Edge device; IoT; Resource management; Self-sustainable; Sensing; Sensor suite; Wildfire monitoring; Decision making
Keep in Balance: Runtime-reconfigurable Intermittent Deep Inference,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171734441&doi=10.1145%2f3607918&partnerID=40&md5=89d798df226ec5de70264048a6b65d2f,"Intermittent deep neural network (DNN) inference is a promising technique to enable intelligent applications on tiny devices powered by ambient energy sources. Nonetheless, intermittent execution presents inherent challenges, primarily involving accumulating progress across power cycles and having to refetch volatile data lost due to power loss in each power cycle. Existing approaches typically optimize the inference configuration to maximize data reuse. However, we observe that such a fixed configuration may be significantly inefficient due to the fluctuating balance point between data reuse and data refetch caused by the dynamic nature of ambient energy.This work proposes DynBal, an approach to dynamically reconfigure the inference engine at runtime. DynBal is realized as a middleware plugin that improves inference performance by exploring the interplay between data reuse and data refetch to maintain their balance with respect to the changing level of intermittency. An indirect metric is developed to easily evaluate an inference configuration considering the variability in intermittency, and a lightweight reconfiguration algorithm is employed to efficiently optimize the configuration at runtime. We evaluate the improvement brought by integrating DynBal into a recent intermittent inference approach that uses a fixed configuration. Evaluations were conducted on a Texas Instruments device with various network models and under varied intermittent power strengths. Our experimental results demonstrate that DynBal can speed up intermittent inference by 3.26 times, achieving a greater improvement for a large network under high intermittency and a large gap between memory and computation performance.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep neural networks; energy harvesting; intermittent systems; runtime reconfiguration; tinyML,Energy harvesting; Inference engines; Middleware; Ambients; Data reuse; Deep inference; Intermittency; Intermittent systems; Power cycle; Run-time reconfigurable; Run-time reconfiguration; Runtimes; Tinyml; Deep neural networks
DASS: Differentiable Architecture Search for Sparse Neural Networks,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171744110&doi=10.1145%2f3609385&partnerID=40&md5=19774116678c50a7b7d24c4feb419c63,"The deployment of Deep Neural Networks (DNNs) on edge devices is hindered by the substantial gap between performance requirements and available computational power. While recent research has made significant strides in developing pruning methods to build a sparse network for reducing the computing overhead of DNNs, there remains considerable accuracy loss, especially at high pruning ratios. We find that the architectures designed for dense networks by differentiable architecture search methods are ineffective when pruning mechanisms are applied to them. The main reason is that the current methods do not support sparse architectures in their search space and use a search objective that is made for dense networks and does not focus on sparsity.This paper proposes a new method to search for sparsity-friendly neural architectures. It is done by adding two new sparse operations to the search space and modifying the search objective. We propose two novel parametric SparseConv and SparseLinear operations in order to expand the search space to include sparse operations. In particular, these operations make a flexible search space due to using sparse parametric versions of linear and convolution operations. The proposed search objective lets us train the architecture based on the sparsity of the search space operations. Quantitative analyses demonstrate that architectures found through DASS outperform those used in the state-of-the-art sparse networks on the CIFAR-10 and ImageNet datasets. In terms of performance and hardware effectiveness, DASS increases the accuracy of the sparse version of MobileNet-v2 from 73.44% to 81.35% (+7.91% improvement) with a 3.87× faster inference time. © 2023 Copyright held by the owner/author(s).",image classification; network sparsification; Neural architecture search; optimization,Deep neural networks; Network architecture; Dense network; Images classification; Network sparsification; Neural architecture search; Neural architectures; Optimisations; Search spaces; Sparse network; Sparse neural networks; Sparsification; Image classification
DaCapo: An On-Device Learning Scheme for Memory-Constrained Embedded Systems,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171761766&doi=10.1145%2f3609121&partnerID=40&md5=e49ba5d3982022e3b8b798c664f8ec6a,"The use of deep neural network (DNN) applications in microcontroller unit (MCU) embedded systems is getting popular. However, the DNN models in such systems frequently suffer from accuracy loss due to the dataset shift problem. On-device learning resolves this problem by updating the model parameters on-site with the real-world data, thus localizing the model to its surroundings. However, the backpropagation step during on-device learning requires the output of every layer computed during the forward pass to be stored in memory. This is usually infeasible in MCU devices as they are equipped only with a few KBs of SRAM. Given their energy limitation and the timeliness requirements, using flash memory to store the output of every layer is not practical either. Although there have been proposed a few research results to enable on-device learning under stringent memory conditions, they require the modification of the target models or the use of non-conventional gradient computation strategies. This paper proposes DaCapo, a backpropagation scheme that enables on-device learning in memory-constrained embedded systems. DaCapo stores only the output of certain layers, known as checkpoints, in SRAM, and discards the others. The discarded outputs are recomputed during backpropagation from the nearest checkpoint in front of them. In order to minimize the recomputation occurrences, DaCapo optimally plans the checkpoints to be stored in the SRAM area at a particular phase of the backpropagation and thus replaces the checkpoints stored in memory as the backpropagation progresses. We implemented the proposed scheme in an STM32F429ZI board and evaluated it with five representative DNN models. Our evaluation showed that DaCapo improved backpropagation time by up to 22% and saved energy consumption by up to 28% in comparison to AIfES, a machine learning platform optimized for MCU devices. In addition, our proposed approach enabled the training of MobileNet, which the MCU device had been previously unable to train.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",backpropagation; embedded systems; Internet-of-Things; machine learning; On-device learning,Backpropagation; Deep neural networks; Embedded systems; Energy utilization; Flash memory; Microcontrollers; Static random access storage; Accuracy loss; Dataset shifts; Embedded-system; Learning schemes; Machine-learning; Microcontroller unit; Modeling parameters; Neural network application; Neural network model; On-device learning; Internet of things
ANV-PUF: Machine-Learning-Resilient NVM-Based Arbiter PUF,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171729472&doi=10.1145%2f3609388&partnerID=40&md5=0168c514251d9e4b7fa259028d4def37,"Physical Unclonable Functions (PUFs) have been widely considered an attractive security primitive. They use the deviations in the fabrication process to have unique responses from each device. Due to their nature, they serve as a DNA-like identity of the device. But PUFs have also been targeted for attacks. It has been proven that machine learning (ML) can be used to effectively model a PUF design and predict its behavior, leading to leakage of the internal secrets.To combat such attacks, several designs have been proposed to make it harder to model PUFs. One design direction is to use Non-Volatile Memory (NVM) as the building block of the PUF. NVM typically are multi-level cells, i.e, they have several internal states, which makes it harder to model them. However, the current state of the art of NVM-based PUFs is limited to 'weak PUFs', i.e., the number of outputs grows only linearly with the number of inputs, which limits the number of possible secret values that can be stored using the PUF. To overcome this limitation, in this work we design the Arbiter Non-Volatile PUF (ANV-PUF) that is exponential in the number of inputs and that is resilient against ML-based modeling. The concept is based on the famous delay-based Arbiter PUF (which is not resilient against modeling attacks) while using NVM as a building block instead of switches. Hence, we replace the switch delays (which are easy to model via ML) with the multi-level property of NVM (which is hard to model via ML). Consequently, our design has the exponential output characteristics of the Arbiter PUF and the resilience against attacks from the NVM-based PUFs. Our results show that the resilience to ML modeling, uniqueness, and uniformity are all in the ideal range of 50%. Thus, in contrast to the state-of-the-art, ANV-PUF is able to be resilient to attacks, while having an exponential number of outputs. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",NVM; PUF; Security,Cryptography; Digital storage; Hardware security; Structural design; Building blockes; Exponentials; Fabrication process; Machine-learning; Multilevels; Non-volatile memory; Nonvolatile; Security; Security primitives; State of the art; Machine learning
DTRL: Decision Tree-based Multi-Objective Reinforcement Learning for Runtime Task Scheduling in Domain-Specific System-on-Chips,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171745557&doi=10.1145%2f3609108&partnerID=40&md5=cf3db186a80bd85191469123dd0bddc4,"Domain-specific systems-on-chip (DSSoCs) combine general-purpose processors and specialized hardware accelerators to improve performance and energy efficiency for a specific domain. The optimal allocation of tasks to processing elements (PEs) with minimal runtime overheads is crucial to achieving this potential. However, this problem remains challenging as prior approaches suffer from non-optimal scheduling decisions or significant runtime overheads. Moreover, existing techniques focus on a single optimization objective, such as maximizing performance. This work proposes DTRL, a decision-tree-based multi-objective reinforcement learning technique for runtime task scheduling in DSSoCs. DTRL trains a single global differentiable decision tree (DDT) policy that covers the entire objective space quantified by a preference vector. Our extensive experimental evaluations using our novel reinforcement learning environment demonstrate that DTRL captures the trade-off between execution time and power consumption, thereby generating a Pareto set of solutions using a single policy. Furthermore, comparison with state-of-the-art heuristic-, optimization-, and machine learning-based schedulers shows that DTRL achieves up to 9× higher performance and up to 3.08× reduction in energy consumption. The trained DDT policy achieves 120 ns inference latency on Xilinx Zynq ZCU102 FPGA at 1.2 GHz, resulting in negligible runtime overheads. Evaluation on the same hardware shows that DTRL achieves up to 16% higher performance than a state-of-the-art heuristic scheduler.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",decision trees; Domain-specific system-on-chip; multi-objective optimization; reinforcement learning; resource management; task scheduling,Application specific integrated circuits; Computer aided instruction; Economic and social effects; Energy efficiency; Energy utilization; General purpose computers; Learning systems; Multiobjective optimization; Multitasking; Reinforcement learning; Scheduling algorithms; System-on-chip; Domain specific; Domain-specific system-on-chip; Multi-objectives optimization; Performance; Reinforcement learnings; Resource management; Runtime overheads; Systems-on-Chip; Tasks scheduling; Tree-based; Decision trees
WARM-tree: Making Quadtrees Write-efficient and Space-economic on Persistent Memories,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171748384&doi=10.1145%2f3608033&partnerID=40&md5=be82d69e262baeca4ba8e4eb0ca64b83,"Recently, the value of data has been widely recognized, which highlights the significance of data-centric computing in diversified application scenarios. In many cases, the data are multidimensional, and the management of multidimensional data often confronts greater challenges in supporting efficient data access operations and guaranteeing the space utilization. On the other hand, while many existing index data structures have been proposed for multidimensional data management, however, their designs are not fully optimized for modern nonvolatile memories, in particular the byte-addressable persistent memories. As a result, they might undergo serious access performance degradation or fail to guarantee space utilization. This observation motivates the redesigning of index data structures for multidimensional point data on modern persistent memories, such as the phase-change memory. In this work, we present the WARM-tree, a multidimensional tree for reducing the write amplification effect, for multidimensional point data. In our evaluation studies, as compared to the bucket PR quadtree and R∗-tree, the WARM-tree can provide any worst-case space utilization guarantees in the form of (m, Currency+) and effectively reduces the write traffic of key insertions by up to 48.10% and 85.86%, respectively, at the price of degraded average space utilization and prolonged latency of query operations. This suggests that the WARM-tree is a potential multidimensional index structure for insert-intensive workloads.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Persistent memory; space utilization; write amplification,Data structures; Information management; Trees (mathematics); Application scenario; Data access; Data centric; Index data structures; Multidimensional data; Persistent memory; Point data; Quad trees; Space utilization; Write amplifications; Phase change memory
Overflow-free Compute Memories for Edge AI Acceleration,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171761781&doi=10.1145%2f3609387&partnerID=40&md5=cc3acb9c430229834c081bc863c23c1a,"Compute memories are memory arrays augmented with dedicated logic to support arithmetic. They support the efficient execution of data-centric computing patterns, such as those characterizing Artificial Intelligence (AI) algorithms. These architectures can provide computing capabilities as part of the memory array structures (In-Memory Computing, IMC) or at their immediate periphery (Near-Memory Computing, NMC). By bringing the processing elements inside (or very close to) storage, compute memories minimize the cost of data access. Moreover, highly parallel (and, hence, high-performance) computations are enabled by exploiting the regular structure of memory arrays. However, the regular layout of memory elements also constrains the data range of inputs and outputs, since the bitwidths of operands and results stored at each address cannot be freely varied. Addressing this challenge, we herein propose a HW/SW co-design methodology combining careful per-layer quantization and inter-layer scaling with lightweight hardware support for overflow-free computation of dot-vector operations. We demonstrate their use to implement the convolutional and fully connected layers of AI models. We embody our strategy in two implementations, based on IMC and NMC, respectively. Experimental results highlight that an area overhead of only 10.5% (for IMC) and 12.9% (for NMC) is required when interfacing with a 2KB subarray. Furthermore, inferences on benchmark CNNs show negligible accuracy degradation due to quantization for equivalent floating-point implementations. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",convolutional neural networks; edge machine learning; In-memory computing; near-memory computing; quantization,Computation theory; Convolution; Digital arithmetic; Digital storage; Memory architecture; Vector quantization; Artificial intelligence algorithms; Computing capability; Convolutional neural network; Data centric; Edge machine learning; In-memory computing; Machine-learning; Memory array; Near-memory computing; Quantisation; Machine learning
STADIA: Photonic Stochastic Gradient Descent for Neural Network Accelerators,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171743257&doi=10.1145%2f3607920&partnerID=40&md5=b859845bf0dca033ffc51aff52a83a93,"Deep Neural Networks (DNNs) have demonstrated great success in many fields such as image recognition and text analysis. However, the ever-increasing sizes of both DNN models and training datasets make deep leaning extremely computation- and memory-intensive. Recently, photonic computing has emerged as a promising technology for accelerating DNNs. While the design of photonic accelerators for DNN inference and forward propagation of DNN training has been widely investigated, the architectural acceleration for equally important backpropagation of DNN training has not been well studied. In this paper, we propose a novel silicon photonic-based backpropagation accelerator for high performance DNN training. Specifically, a general-purpose photonic gradient descent unit named STADIA is designed to implement the multiplication, accumulation, and subtraction operations required for computing gradients using mature optical devices including Mach-Zehnder Interferometer (MZI) and Mircoring Resonator (MRR), which can significantly reduce the training latency and improve the energy efficiency of backpropagation. To demonstrate efficient parallel computing, we propose a STADIA-based backpropagation acceleration architecture and design a dataflow by using wavelength-division multiplexing (WDM). We analyze the precision of STADIA by quantifying the precision limitations imposed by losses and noises. Furthermore, we evaluate STADIA with different element sizes by analyzing the power, area and time delay for photonic accelerators based on DNN models such as AlexNet, VGG19 and ResNet. Simulation results show that the proposed architecture STADIA can achieve significant improvement by 9.7× in time efficiency and 147.2× in energy efficiency, compared with the most advanced optical-memristor based backpropagation accelerator.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",neural networks accelerator; optical computing; Stochastic gradient descent,Backpropagation; Character recognition; Energy efficiency; Finite element method; Gradient methods; Image recognition; Network architecture; Optical data processing; Silicon photonics; Stochastic systems; Forward propagation; Network inference; Neural network accelerator; Neural network model; Neural networks trainings; Neural-networks; Silicon photonics; Stochastic gradient descent; Text analysis; Training dataset; Deep neural networks
Sound Mixed Fixed-Point Quantization of Neural Networks,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171731556&doi=10.1145%2f3609118&partnerID=40&md5=5afda394f80e980b6e10a8e7de0e685c,"Neural networks are increasingly being used as components in safety-critical applications, for instance, as controllers in embedded systems. Their formal safety verification has made significant progress but typically considers only idealized real-valued networks. For practical applications, such neural networks have to be quantized, i.e., implemented in finite-precision arithmetic, which inevitably introduces roundoff errors. Choosing a suitable precision that is both guaranteed to satisfy a roundoff error bound to ensure safety and that is as small as possible to not waste resources is highly nontrivial to do manually. This task is especially challenging when quantizing a neural network in fixed-point arithmetic, where one can choose among a large number of precisions and has to ensure overflow-freedom explicitly.This paper presents the first sound and fully automated mixed-precision quantization approach that specifically targets deep feed-forward neural networks. Our quantization is based on mixed-integer linear programming (MILP) and leverages the unique structure of neural networks and effective over-approximations to make MILP optimization feasible. Our approach efficiently optimizes the number of bits needed to implement a network while guaranteeing a provided error bound. Our evaluation on existing embedded neural controller benchmarks shows that our optimization translates into precision assignments that mostly use fewer machine cycles when compiled to an FPGA with a commercial HLS compiler than code generated by (sound) state-of-the-art. Furthermore, our approach handles significantly more benchmarks substantially faster, especially for larger networks.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Fixed-point arithmetic; mixed integer linear programming; mixed precision,Embedded systems; Errors; Feedforward neural networks; Integer programming; Safety engineering; Error bound; Fixed points; Fixed-point arithmetics; Integer Linear Programming; Mixed integer linear; Mixed integer linear programming; Mixed precision; Neural-networks; Quantisation; Round-off errors; Fixed point arithmetic
Optimal Synthesis of Robust IDK Classifier Cascades,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171748381&doi=10.1145%2f3609129&partnerID=40&md5=6eb9f7432c9ee8f51571f3180660d350,"An IDK classifier is a computing component that categorizes inputs into one of a number of classes, if it is able to do so with the required level of confidence, otherwise it returns ""I Don't Know""(IDK). IDK classifier cascades have been proposed as a way of balancing the needs for fast response and high accuracy in classification-based machine perception. Efficient algorithms for the synthesis of IDK classifier cascades have been derived; however, the responsiveness of these cascades is highly dependent on the accuracy of predictions regarding the run-time behavior of the classifiers from which they are built. Accurate predictions of such run-time behavior is difficult to obtain for many of the classifiers used for perception. By applying the algorithms using predictions framework, we propose efficient algorithms for the synthesis of IDK classifier cascades that are robust to inaccurate predictions in the following sense: the IDK classifier cascades synthesized by our algorithms have short expected execution durations when the predictions are accurate, and these expected durations increase only within specified bounds when the predictions are inaccurate.  © 2023 Copyright held by the owner/author(s).",Algorithms using predictions; classification; on-line scheduling; robust scheduling,Balancing; Forecasting; Algorithm using prediction; Fast response; High-accuracy; Machine perception; Number of class; Online scheduling; Optimal synthesis; Robust scheduling; Runtimes; Time behavior; Classification (of information)
Computationally Efficient DNN Mapping Search Heuristic using Deep Reinforcement Learning,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171749506&doi=10.1145%2f3609110&partnerID=40&md5=3b8788de9bfdd94f5fe3718f0df080dc,"In this work, we present a computationally efficient Reinforcement Learning mapping search heuristic for finding high quality mappings for N-dimensional convolution loops that uses a computationally inexpensive reward function based on potential data reuse of operands to guide the search process. We also present a RL state representation generalizable to N-dimensional convolution loops, and a state representation parsing strategy ensuring that only valid mappings are evaluated for quality. Our RL search heuristic is applicable to multi-core systems with a memory hierarchy. We show that our RL based search heuristic for a range of 3D convolution layers, at significantly lower computational expense than random search, generally yields mappings with lower Energy-Delay Product (EDP) for an architecture with multiple processing elements with shared memory connected to DRAM. Our evaluation results demonstrated across 19 3D convolution layers, shows that our RL method performed only an average 11.24% of the operations of that of Timeloop's random search for assessing same number of valid mappings. The mappings found using Timeloop had an average 12.51% higher EDP compared to lowest EDP mapping found using our RL method. Further, the lowest EDP mappings found using our method had an average only 4.69× higher EDP than the theoretical lower bound EDP, with the best case being only 1.29× higher.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",convolution; DNN; mapping search; Reinforcement learning,Computational efficiency; Computer aided design; Convolution; Deep learning; Dynamic random access storage; Heuristic algorithms; Memory architecture; Quality control; Reinforcement learning; Computationally efficient; DNN; Energy delay product; High quality; Lower energies; Mapping search; Random searches; Reinforcement learnings; Search heuristics; State representation; Mapping
ObNoCs: Protecting Network-on-Chip Fabrics Against Reverse-Engineering Attacks,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171734654&doi=10.1145%2f3609107&partnerID=40&md5=53c7da07855298d1150b306eece3199e,"Modern System-on-Chip designs typically use Network-on-Chip (NoC) fabrics to implement coordination among integrated hardware blocks. An important class of security vulnerabilities involves a rogue foundry reverse-engineering the NoC topology and routing logic. In this paper, we develop an infrastructure, ObNoCs, for protecting NoC fabrics against such attacks. ObNoCs systematically replaces router connections with switches that can be programmed after fabrication to induce the desired topology. Our approach provides provable redaction of NoC functionality: switch configurations induce a large number of legal topologies, only one of which corresponds to the intended topology. We implement the ObNoCs methodology on Intel Quartus™ Platform, and experimental results on realistic SoC designs show that the architecture incurs minimal overhead in power, resource utilization, and system latency.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",communication fabrics; Hardware security; obfuscation; supply-chain attacks,Crosslinking; Data obfuscation; Hardware security; Network security; Programmable logic controllers; Reverse engineering; Routers; Servers; Supply chains; Topology; Communication fabric; Hardware blocks; Networks on chips; Obfuscation; On-chip functionalities; Routings; Security vulnerabilities; Supply-chain attack; Switch configuration; System on chips design; Network-on-chip
Modular DFR: Digital Delayed Feedback Reservoir Model for Enhancing Design Flexibility,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171764154&doi=10.1145%2f3609105&partnerID=40&md5=115a17fc30868c040a790d968ca38123,"A delayed feedback reservoir (DFR) is a type of reservoir computing system well-suited for hardware implementations owing to its simple structure. Most existing DFR implementations use analog circuits that require both digital-to-analog and analog-to-digital converters for interfacing. However, digital DFRs emulate analog nonlinear components in the digital domain, resulting in a lack of design flexibility and higher power consumption. In this paper, we propose a novel modular DFR model that is suitable for fully digital implementations. The proposed model reduces the number of hyperparameters and allows flexibility in the selection of the nonlinear function, which improves the accuracy while reducing the power consumption. We further present two DFR realizations with different nonlinear functions, achieving 10× power reduction and 5.3× throughput improvement while maintaining equal or better accuracy.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",delayed feedback reservoir (DFR); edge computing; Reservoir computing,Analog to digital conversion; Computing power; Digital to analog conversion; Electric power utilization; Computing system; Delayed feedback; Delayed feedback reservoir; Design flexibility; Edge computing; Hardware implementations; Modulars; Nonlinear functions; Reservoir Computing; Reservoir models; Edge computing
CABARRE: Request Response Arbitration for Shared Cache Management,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171729065&doi=10.1145%2f3608096&partnerID=40&md5=23b1b64e07daef451e19411ed9720676,"Modern multi-processor systems-on-chip (MPSoCs) are characterized by caches shared by multiple cores. These shared caches receive requests issued by the processor cores. Requests that are subject to cache misses may result in the generation of responses. These responses are received from the lower level of the memory hierarchy and written to the cache. The outstanding requests and responses contend for the shared cache bandwidth. To mitigate the impact of the cache bandwidth contention on the overall system performance, an efficient request and response arbitration policy is needed.Research on shared cache management has neglected the additional cache contention caused by responses, which are written to the cache. We propose CABARRE, a novel request and response arbitration policy at shared caches, so as to improve the overall system performance. CABARRE shows a performance improvement of 23% on average across a set of SPEC workloads compared to straightforward adaptations of state-of-the-art solutions.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",arbitration; cache bandwidth; multi-core systems; requests; responses; Shared caches,Cache memory; Multiprocessing systems; System-on-chip; Arbitration; Cache bandwidth; Multi processor system on chips; Multi-core systems; Processor cores; Request; Response; Shared cache; Shared cache managements; Systems performance; Bandwidth
Proactive Stripe Reconstruction to Improve Cache Use Efficiency of SSD-Based RAID Systems,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171729062&doi=10.1145%2f3609099&partnerID=40&md5=8d11d1215135fedd79fe7d973e86331a,"Solid-State Drives (SSDs) exhibit different failure characteristics compared to conventional hard disk drives. In particular, the Bit Error Rate (BER) of an SSD increases as it bears more writes. Then, Parity-based Redundant Array of Inexpensive Disks (RAID) arrays composed from SSDs are introduced to address correlated failures. In the RAID-5 implementation, specifically, the process of parity generation (or update) associating with a data stripe, consists of read and write operations to the SSDs. Whenever a new update request comes to the RAID system, the related parity must be also updated and flushed onto the RAID component of SSD. Such frequent parity updates result in poor RAID performance and shorten the life-time of the SSDs. Consequently, a DRAM cache is commonly equipped accompanying with the RAID controller, called the parity cache, and used to buffer the parity chunks that are most frequently updated data, for boosting I/O performance.To better improve the use efficiency of the parity cache, this paper proposes a stripe reconstruction approach to minimize the number of parity updates on SSDs, thus boosting I/O performance of the SSD RAID system. When the currently updated stripe has both cold and hot updated data chunks, it will proactively carry out stripe reconstruction if we can find another matched stripe that also includes cold and hot update data chunks on the complementary RAID components. In the reconstruction process, we first group the cold data chunks of two matched stripes, to build a new stripe and flush the parity chunk on the RAID component. After that, the hot data chunks are organized as a new stripe as well, and its parity chunk is buffered in the parity cache. This results in better cache use efficiency, as it can reduce the number of parity updates on RAID components of SSDs, as well as proactively free up cache space for quickly absorbing subsequent write requests. In addition, the proposed method adjusts the target SSD of write requests based on stripe reconstructions through considering the I/O workload balance of all SSDs. Experimental results show that our proposal can reduce the number of parity chunk updates in SSDs by 2.3% and overall I/O latency by 12.2% on average, compared to state-of-the-art parity cache management techniques.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cache use efficiency; modeling; performance; SSD-based RAID; stripe reconstruction,Bit error rate; Hard disk storage; Cache use efficiency; Data chunks; Disk systems; Modeling; Performance; Redundant arrays of inexpensive disk; Solid-state drive-based redundant array of inexpensive disk; Stripe reconstruction; Updated data; Use efficiency; Efficiency
FSIMR: File-system-aware Data Management for Interlaced Magnetic Recording,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171757863&doi=10.1145%2f3607922&partnerID=40&md5=b040c88ea76f733260650f866188b19b,"Interlaced Magnetic Recording (IMR) is an emerging recording technology for hard-disk drives (HDDs) that provides larger storage capacity at a lower cost. By partially overlapping (interlacing) each bottom track with two adjacent top tracks, IMR-based HDDs successfully increase the data density while incurring some hardware write constraints. To update each bottom track, the data on two adjacent top tracks must be read and rewritten to avoid losing their valid data, resulting in additional overhead for performing read-modify-write (RMW) operations. Therefore, researchers have proposed various data management schemes to mitigate such overhead in recent years, aiming at improving the write performance. However, these designs have not taken into account the data characteristics of the file system, which is a crucial layer of operating systems for storing/retrieving data into/from HDDs. Consequently, the write performance improvement is limited due to the unawareness of spatial locality and hotness of data. This paper proposes a file-system-aware data management scheme called FSIMR to improve system write performance. Noticing that data of the same directory may have higher spatial locality and are mostly updated at the same time, FSIMR logically partitions the IMR-based HDD into fixed-sized zones; data belonging to the same directory will be arranged to one zone to reduce the time of seeking to-be-updated data (seek time). Furthermore, cold data within a zone are arranged to bottom tracks and updated in an out-of-place manner to eliminate RMW operations. Our experimental results show that the proposed FSIMR could reduce the seek time by up to 14% without introducing additional RMW operations, compared to existing designs.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",data allocation; directory; file system; Interlaced Magnetic Recording; read-modify-write,File organization; Hard disk storage; Magnetic recording; Bottom tracks; Data allocation; Directory; Filesystem; Hard Disk Drive; Interlaced magnetic recording; Performance; Read-modify writes; System-Aware; Write operations; Information management
Toward Optimal Softcore Carry-aware Approximate Multipliers on Xilinx FPGAs,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168758214&doi=10.1145%2f3564243&partnerID=40&md5=0de22cfd0d5db096af7f85a162ef6f3e,"Domain-specific accelerators for signal processing, image processing, and machine learning are increasingly being implemented on SRAM-based field-programmable gate arrays (FPGAs). Owing to the inherent error tolerance of such applications, approximate arithmetic operations, in particular, the design of approximate multipliers, have become an important research problem. Truncation of lower bits is a widely used approximation approach; however, analyzing and limiting the effects of carry-propagation due to this approximation has not been explored in detail yet. In this article, an optimized carry-aware approximate radix-4 Booth multiplier design is presented that leverages the built-in slice look-up tables (LUTs) and carry-chain resources in a novel configuration. The proposed multiplier simplifies the computation of the upper and lower bits and provides significant benefits in terms of FPGA resource usage (LUTs saving 38.5%-42.9%), Power Delay Product (PDP saving 49.4%-53%), performance metric (LUTs × critical path delay (CPD) × PDP saving 68.9%-73.1%) and errors (70% improvement in mean relative error distance) compared to the latest state-of-the-art designs. Therefore, the proposed designs are an attractive choice to implement multiplication on FPGA-based accelerators.  © 2023 Association for Computing Machinery.",Neural Network,Backpropagation; Errors; Field programmable gate arrays (FPGA); Image processing; Integrated circuit design; Domain specific; Field programmables; Images processing; Inherent errors; Lookup tables (LUTs); Machine-learning; Neural-networks; Programmable gate array; Signal-processing; Softcore; Table lookup
Runtime Resource Management with Multiple-Step-Ahead Workload Prediction,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170645262&doi=10.1145%2f3605213&partnerID=40&md5=1d8dee88f96c35ea4c15406556e63ba7,"Modern embedded platforms need sophisticated resource managers to utilize their heterogeneous computational resources efficiently. Furthermore, such platforms are subject to fluctuating workloads that are unforeseeable at design time. Predicting the incoming workload could enhance the efficiency of resource management in this situation. But is that the case? And, if so, how substantial is this improvement? Does multiple-step-ahead prediction of the workload contribute to this improvement? How precise must the prediction be to improve decisions rather than cause harm? By proposing a prediction-based resource manager that aims at meeting task deadlines while minimizing energy usage, and by conducting extensive tests, we attempt to provide answers to the aforementioned questions.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesHeterogeneous architecture; multiple-step-ahead prediction; resource management,Natural resources management; Resource allocation; Additional key word and phrasesheterogeneous architecture; Computational resources; Design time; Embedded platforms; Key words; Multiple-step-ahead prediction; Resource management; Resource managers; Runtimes; Workload predictions; Forecasting
Automatic Generation of Resource and Accuracy Configurable Processing Elements,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168767741&doi=10.1145%2f3594540&partnerID=40&md5=23d7548f06c3867e315d067a78778dd7,"Low-power consumption and scarce computational resources limit the computation at the edge. Besides, the approximate computing paradigm reports promising techniques for designing accelerators to deal with inherent limitations of the edge, and high-level synthesis with C++ opens the opportunity to use meta-programming for specialisable generic design. This work proposes a framework for automatically generating synthesis-time configurable processing elements (PEs) for matrix multiplication-addition (GEMMA) and convolution. To evaluate our work, we perform a design exploration after varying data bit-width, operand sizes, and kernel sizes. Our analyses include resource consumption scaling, clocks-to-solution, design efficiency, and error distribution, presenting a comprehensive view of how the parameters affect the properties of our generic implementations. The GEMMA presented a trade-off between granularity vs efficiency, where large PEs with short data widths are favoured by the design efficiency, achieving, theoretically, up to 75 GMAC/s on a Xilinx XC7Z020 @ 100 MHz with an efficiency of 27%. For design efficiency, we propose a figure of merit to evaluate operations per second and resource utilisation with respect to the maximum achievable by the FPGA. Regarding the convolution PEs, we implemented two algorithms: a window-based spatial convolution and Winograd. The former is the best in terms of performance with 150 GMAC/s, reaching up to 47% of efficiency. Winograd also outperformed numerically using a 3× 3 kernel filter, presenting a mean error of 11.01% in 4-bits operands with a PSNR=16.28 dB, compared to the spatial convolution with 38.2% of mean error and PSNR=5.89 dB. Finally, we discuss how the error is mostly dependent on the PE's parameters. In the GEMMA, the error depends on the matrix size, causing limitations in the PE scaling but still applicable to accelerators. The PEs developed during this research will lead to further granular approximate accelerator research.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Approximate computing; deep neural networks; edge computing; hardware acceleration; multi-layer neural network; neural network hardware,C++ (programming language); Convolution; Economic and social effects; Edge computing; Energy efficiency; High level synthesis; Matrix algebra; Multilayer neural networks; Network layers; Approximate computing; Design efficiency; Edge computing; Hardware acceleration; Multi-layer neural networks; Neural network hardware; Processing elements; Scalings; Spatial convolution; Winograd; Deep neural networks
REPAIRS: Gaussian Mixture Model-based Completion and Optimization of Partially Specified Systems,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168769833&doi=10.1145%2f3605147&partnerID=40&md5=3ce9cd0b927f05000b7ce9c2cabd3786,"Most system optimization techniques focus on finding the values of the system components to achieve the best performance. Searching over all component values gives the search methodology the freedom to explore the entire design space to determine the best system configuration. However, real-world systems often require searching in a restricted space over only a subset of component values while freezing some of the components to fixed values. Rather than optimizing from scratch to search over the subset of components, incorporating the past simulation logs (search performed when all components were allowed to vary) enables the optimization mechanism to utilize knowledge from past system behavior. In addition, when the system gives the same response over different combinations of input values, the designer may prefer one combination over another. Furthermore, real-world data often contain errors. To avoid catastrophic consequences of making decisions based on incorrect data points, we need a mechanism to identify and correct the resulting error. We propose REPAIRS, a methodology to complete/optimize partially specified systems. It also performs data integrity checks and identifies/corrects errors after detecting an anomaly in the data. We use a Gaussian mixture model to learn the joint distribution of the system inputs and the corresponding output response (objectives/constraints). We use the learned model to complete a partially specified system where only a subset of the component values and/or the system response is specified. When the system response exhibits multiple modes (e.g., same response for different combinations of input values), REPAIRS determines the combinations of input values that correspond to the several modes. Using past simulation logs, it searches over various subsets of system inputs to improve the performance of the reference solution. We also present a framework for verifying the integrity of a given data instance. When the integrity check fails, we provide a mechanism to identify the error location and correct the error. REPAIRS provides an explanation for the decision it makes for the different use cases described in this article. We provide results of REPAIRS in the context of completion, partial optimization, and data integrity check of real-world systems. REPAIRS achieves a hypervolume that is better than that obtained using a baseline method by up to 50%. It successfully identifies the error location and predicts the correct value of the erroneous feature with an error less than 0.2%. It detects error locations with a mean accuracy of up to 95% even when three feature values have an error.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Active learning; constrained multi-objective optimization; data integrity; Gaussian mixture model; inverse design,Constrained optimization; Gaussian distribution; Image segmentation; Inverse problems; Location; Multiobjective optimization; Repair; Set theory; Active Learning; Constrained multi-objective optimizations; Data integrity; Error location; Gaussian Mixture Model; Input values; Integrity check; Inverse designs; Optimisations; Performance; Errors
Real-Time USB Networking and Device I/O,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168762392&doi=10.1145%2f3604429&partnerID=40&md5=8c4a6dad7eabcfdc055f0189cddec823,"Multicore PC-class embedded systems present an opportunity to consolidate separate microcontrollers as software-defined functions. For instance, an automotive system with more than 100 electronic control units (ECUs) could be replaced with one or, at most, several multicore PCs running software tasks for chassis, body, powertrain, infotainment, and advanced driver assistance system (ADAS) services. However, a key challenge is how to handle real-time device input and output (I/O) and host-level networking as part of sensor data processing and control. A traditional microcontroller would commonly feature one or more Controller Area Network (CAN) buses for real-time I/O. CAN buses are usually absent in PCs, which instead feature higher bandwidth Universal Serial Bus (USB) interfaces. This article shows how to achieve real-time device I/O and host-to-host communication over USB, using suitably written device drivers and a time-aware POSIX-like ""tuned pipe""abstraction. This allows developers to establish task pipelines spanning one or more hosts, with end-to-end latency and throughput guarantees for sensor data processing, control, and actuation.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",extensible Host Controller Interface (xHCI); real-time host-to-host communication; real-time input/output; Universal Serial Bus (USB),Advanced driver assistance systems; Automobile drivers; Control system synthesis; Data handling; Embedded systems; Microcomputers; Microcontrollers; Real time systems; System buses; Extensible host controller interface (xHCI); Host controller interfaces; Input and outputs; Input-output; Real- time; Real-time host-to-host communication; Real-time input/output; Universal serial bus; Controllers
Energy-Efficient Approximate Edge Inference Systems,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168774351&doi=10.1145%2f3589766&partnerID=40&md5=37336ffd1ce608dcbde09bc52bf6fb6f,"The rapid proliferation of the Internet of Things and the dramatic resurgence of artificial intelligence based application workloads have led to immense interest in performing inference on energy-constrained edge devices. Approximate computing (a design paradigm that trades off a small degradation in application quality for disproportionate energy savings) is a promising technique to enable energy-efficient inference at the edge. This article introduces the concept of an approximate edge inference system (AxIS) and proposes a systematic methodology to perform joint approximations between different subsystems in a deep neural network (DNN)-based edge inference system, leading to significant energy benefits compared to approximating individual subsystems in isolation. We use a smart camera system that executes various DNN-based image classification and object detection applications to illustrate how the sensor, memory, compute, and communication subsystems can all be approximated synergistically. We demonstrate our proposed methodology using two variants of a smart camera system: (a) CamEdge, where the DNN is executed locally on the edge device, and (b) CamCloud, where the edge device sends the captured image to a remote cloud server that executes the DNN. We have prototyped such an approximate inference system using an Intel Stratix IV GX-based Terasic TR4-230 FPGA development board. Experimental results obtained using six large DNNs and four compact DNNs running image classification applications demonstrate significant energy savings (≈ 1.6× -4.7× for large DNNs and ≈ 1.5× -3.6× for small DNNs), for minimal (<1%) loss in application-level quality. Furthermore, results using four object detection DNNs exhibit energy savings of ≈ 1.5× -5.2× for similar quality loss. Compared to approximating a single subsystem in isolation, AxIS achieves 1.05× -3.25× gains in energy savings for image classification and 1.35× -4.2× gains for object detection on average, for minimal (<1%) application-level quality loss.  © 2023 Copyright held by the owner/author(s).",Approximate computing; approximate systems; deep learning; DRAM; edge AI; edge-to-cloud computing; energy efficiency; quality-aware pruning; quality-energy tradeoff,Cameras; Cloud computing; Dynamic random access storage; Energy efficiency; Green computing; Image classification; Object detection; Object recognition; Approximate computing; Approximate system; Cloud-computing; Deep learning; Edge AI; Edge-to-cloud computing; Energy tradeoff; Inference systems; Quality-aware pruning; Quality-energy tradeoff; Deep neural networks
Compositional Timing Analysis of Asynchronized Distributed Cause-effect Chains,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168762660&doi=10.1145%2f3587036&partnerID=40&md5=fc557ff309627ec2e5e8f32a5fbb8990,"Real-time systems require the formal guarantee of timing constraints, not only for the individual tasks but also for the end-to-end latency of data flows. The data flow among multiple tasks, e.g., from sensors to actuators, is described by a cause-effect chain, independent from the priority order of the tasks. In this article, we provide an end-to-end timing-analysis for cause-effect chains on asynchronized distributed systems with periodic task activations, considering the maximum reaction time (MRT) (i.e., the duration of data processing) and the maximum data age (MDA) (i.e., the worst-case data freshness). We first provide an analysis of the end-to-end latency on one local electronic control unit (ECU) that has to consider only the jobs in a bounded time interval. We extend our analysis to globally asynchronized systems by exploiting a compositional property to combine the local results. Throughout synthesized data based on an automotive benchmark as well as on randomized parameters, we show that our analytical results improve the state-of-the-art.  © 2023 Copyright held by the owner/author(s).",Cause-effect chains; compositional end-to-end analysis; maximum data age; maximum reaction time,Control systems; Data handling; Data transfer; Interactive computer systems; Timing circuits; Cause-effect; Cause-effect chain; Compositional end-to-end analyse; Dataflow; End to end latencies; End-to-end analysis; Maximum data age; Maximum reaction time; Real - Time system; Timing Analysis; Real time systems
Efficient Table-based Function Approximation on FPGAs Using Interval Splitting and BRAM Instantiation,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168771896&doi=10.1145%2f3580737&partnerID=40&md5=2d147414bbf861ff66578664304c81da,"This article proposes a novel approach for the generation of memory-efficient table-based function approximation circuits for edge devices in general and FPGAs in particular. Given a function f(x) to be approximated in a given interval [x0, x0+a) and a maximum approximation error Ea, the goal is to determine a function table implementation with a minimized memory footprint, i.e., number of entries that need to be stored. Rather than state-of-the-art work performing an equidistant sampling of the given interval by so-called breakpoints and using linear interpolation between two adjacent breakpoints to determine f(x) at the maximum error bound, we propose and compare three algorithms for splitting the given interval into sub-intervals to reduce the required memory footprint drastically based on the observation that in sub-intervals of low gradient, a coarser sampling grid may be assumed while guaranteeing the maximum interpolation error bound Ea. Experiments on elementary mathematical functions show that a large fraction in memory footprint may be saved. Second, a hardware architecture implementing the sub-interval selection, breakpoint lookup, and interpolation at a latency of just 9 clock cycles is introduced. Third, for each generated circuit design, BRAMs are automatically instantiated rather than synthesizing the reduced footprint function table using LUT primitives, providing an additional degree of resource efficiency. The approach presented here for FPGAs can equally be applied to other circuit technologies for fast and, at the same time, memory-optimized function approximation at the edge.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",approximate computing; BRAM; FPGA; function approximation,Errors; Integrated circuit manufacture; Interpolation; Approximate computing; BRAM; Breakpoint; Error bound; Function tables; Functions approximations; Memory efficient; Memory footprint; Splittings; Subintervals; Field programmable gate arrays (FPGA)
A Methodology for Fault-tolerant Pareto-optimal Approximate Designs of FPGA-based Accelerators,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168769854&doi=10.1145%2f3568021&partnerID=40&md5=8571c08db49b41695f54ae4b97a326df,"Approximate Computing Techniques (ACTs) take advantage of resilience computing applications to trade off among output precision, area, power, and performance. ACTs can lead to significant gains at affordable costs when efficiently implemented on Field Programmable Gate Array- (FPGA) based accelerators. Although several novel ACTs works have been proposed for FPGA accelerators, their applicability to high-assurance systems has not been explored as much. ACTs are becoming necessary in many critical Edge computing systems, such as self-driving cars and Earth observation satellites, to increase computational efficiency. However, an important question comes to mind when targeting critical systems: Does ACT optimization negatively affect the reliability of the system and how can one find optimal design architectures that blend classic mitigation techniques like Triple Modular Redundancy with approximation- and precise-based arithmetic hardware units to achieve the best possible computational efficiency without compromising dependability? This work aims to solve this research problem by introducing a Design Space Exploration (DSE) methodology that employs ACTs in arithmetic units of the design and identifies Pareto-optimal microarchitectures that balance all relevant gains of ACTs, such as area, speed, power, failure rate, and precision, by inserting the correct amount of approximation in the design. In a nutshell, our DSE methodology has formulated the DSE with a Multi-Objective Optimization Problem (MOP). Each Pareto-optimal solution of our tool finds which arithmetic units of the design to implement with precise and approximate circuits and which units to selectively triplicate to remove single points of failure that compromise system reliability below acceptable thresholds. We also suggest another formulation of the DSE into a Single-Objective constraint Optimization Problem (ScOP) producing a single optimal point, and that the user may demand, as a less time-consuming alternative to the MOP if a complete Pareto-front is not needed. Our methodology generates fault-tolerant versions of the Pareto-optimal approximate designs (or simple optimized approximate designs if the ScOP choice is picked) by selectively applying mitigation techniques in a way that the overheads of redundant resources for fault-tolerance do not negate the gains of approximation in comparison to the fault-tolerant versions of the precise design. We evaluate our method on two FPGA-based accelerators: a JPEG encoder and an H.264/Advanced Video Coding decoder. Our experimental results show significant gains in area, frequency, and power consumption without compromising output quality and system reliability compared to classic solutions that replicate all or a part of the resources of the precise design to increase dependability metrics.  © 2023 Association for Computing Machinery.",approximate adders; Approximate Computing Techniques; DCT; fault tolerance; FPGAs; JPEG; Pareto-optimal; selective-TMR,Adders; Computational efficiency; Computer architecture; Computer hardware; Constrained optimization; Economic and social effects; Failure analysis; Fault tolerance; Fault tolerant computer systems; Integrated circuit design; Multiobjective optimization; Optimal systems; Pareto principle; Redundancy; Video signal processing; Approximate adder; Approximate computing technique; Computing techniques; DCT; Design space exploration; Field programmables; JPEG; Pareto-optimal; Programmable gate array; Selective-TMR; Field programmable gate arrays (FPGA)
HMT: A Hardware-centric Hybrid Bonsai Merkle Tree Algorithm for High-performance Authentication,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168765372&doi=10.1145%2f3595179&partnerID=40&md5=c8d956b68284b5ca6fcdc8da8cbb8f25,"The Bonsai Merkle tree (BMT) is a widely used tree structure for authentication of metadata such as encryption counters in a secure computing system. Common BMT algorithms were designed for traditional Von Neumann architectures with a software-centric implementation in mind and as such, they are predominantly recursive and sequential in nature. However, the modern heterogeneous computing platforms employing Field-Programmable Gate Array (FPGA) devices require concurrency-focused algorithms to fully utilize the versatility and parallel nature of such systems. The recursive nature of traditional BMT algorithms makes them challenging to implement in such hardware-based setups. Our goal for this work is to introduce HMT, a hardware-friendly BMT algorithm that enables the verification and update processes to function independently and provides the benefits of relaxed update while being comparable to the eager update in terms of update complexity. The methodology of HMT contributes both novel algorithmic revisions and innovative hardware techniques to implementing BMT. We mathematically demonstrate the challenges of potentially unbounded recursions in relaxed BMT updates. To solve this problem, we use a partitioned BMT caching scheme that allocates a separate write-back cache for each BMT level - thus allowing for low and fixed upper bounds for dirty evictions compared to the traditional BMT caches. Then we introduce the aforementioned hybrid BMT algorithm that is hardware-targeted, parallel, and relaxes the update depending on BMT cache hit but makes the update conditions more flexible compared to lazy update to save additional write-backs. Deploying this new algorithm, we have designed a new BMT controller with a dataflow architecture including speculative buffers and parallel write-back engines to facilitate performance-enhancing mechanisms (like multiple concurrent authentication and independent updates) that were not possible with the conventional lazy algorithm. Our empirical performance measurements on a Xilinx U200 accelerator FPGA have demonstrated that HMT can achieve up to 7× improvement in bandwidth and 4.5× reduction in latency over lazy-update BMT baseline and up to 14% faster execution in standard benchmarks compared to a state-of-the-art, eager-update BMT solution.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",dataflow; FPGA; Integrity; merkle tree,Concurrency control; Cryptography; Field programmable gate arrays (FPGA); Parallel processing systems; Trees (mathematics); Dataflow; Field programmables; Field-programmable gate array; Integrity; Merkle trees; Performance; Programmable gate array; Tree algorithms; Tree structures; Write-back; Authentication
"Model-Based Diagnosis of Real-Time Systems: Robustness Against Varying Latency, Clock Drift, and Out-of-Order Observations",2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168771095&doi=10.1145%2f3597209&partnerID=40&md5=a02413cd6118a03ef7f5c4c124bf868c,"Online fault diagnosis techniques are a key enabler of effective failure mitigation. For real-time systems, the problem of identifying faults is aggravated by timing imprecisions such as varying latency between events and their observation. This paper tackles the challenge of diagnosing faults based on partial observations which are subject to timing imprecisions and potentially made out-of-order due to latency. In this paper, we develop a theory of robust real-time diagnosis importing well-established notions from timed automata theory and the diagnosis of discrete event systems. The theory itself enables a foundational understanding and investigation of the problem and its intricacies. Based on this theory, we further devise an online diagnosis algorithm consuming observations incrementally as they are made and enabling diagnosis, whenever possible, within a bounded worst-case delay. We prove the correctness of the algorithm and its properties with respect to the theory. Aiming at practical feasibility, we also show how to obtain sound but not necessarily complete diagnosis results with space and time requirements bounded by the size of the system model and independent of the number of observations. Finally, using a prototypical implementation, we report on first empirical results obtained by simulation of a small excerpt of an industrial automation example.  © 2023 Copyright held by the owner/author(s).",embedded real-time systems; Online fault diagnosis; timed automata,Automata theory; Discrete event simulation; Embedded systems; Failure analysis; Fault detection; Interactive computer systems; Online systems; Clock drift; Embedded real time systems; Failure mitigation; Fault diagnosis technique; Model-based diagnosis; On-line fault diagnosis; Out of order; Real - Time system; System robustness; Timed Automata; Real time systems
Optimal Checkpointing Strategy for Real-time Systems with Both Logical and Timing Correctness,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168772512&doi=10.1145%2f3603172&partnerID=40&md5=440ba180d187628d73bbc4e6c11405dd,"Real-time systems are susceptible to adversarial factors such as faults and attacks, leading to severe consequences. This paper presents an optimal checkpoint scheme to bolster fault resilience in real-time systems, addressing both logical consistency and timing correctness. First, we partition message-passing processes into a directed acyclic graph (DAG) based on their dependencies, ensuring checkpoint logical consistency. Then, we identify the DAG's critical path, representing the longest sequential path, and analyze the optimal checkpoint strategy along this path to minimize overall execution time, including checkpointing overhead. Upon fault detection, the system rolls back to the nearest valid checkpoints for recovery. Our algorithm derives the optimal checkpoint count and intervals, and we evaluate its performance through extensive simulations and a case study. Results show a 99.97% and 67.86% reduction in execution time compared to checkpoint-free systems in simulations and the case study, respectively. Moreover, our proposed strategy outperforms prior work and baseline methods, increasing deadline achievement rates by 31.41% and 2.92% for small-scale tasks and 78.53% and 4.15% for large-scale tasks.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",checkpointing; fault resilience; logical consistency; Real-time systems; timing correctness,Directed graphs; Fault detection; Fault tolerance; Interactive computer systems; Message passing; Timing circuits; Acyclic graphs; Case-studies; Check pointing; Fault resilience; Graph-based; Logical consistency; Message passing process; Optimal checkpointing; Real - Time system; Timing correctness; Real time systems
AQuA: A New Image Quality Metric for Optimizing Video Analytics Systems,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168761157&doi=10.1145%2f3568423&partnerID=40&md5=359b9b3332cb4f74c878d4fded13a614,"Millions of cameras at the edge are being deployed to power a variety of different deep learning applications. However, the frames captured by these cameras are not always pristine - they can be distorted due to lighting issues, sensor noise, compression etc. Such distortions not only deteriorate visual quality, they impact the accuracy of deep learning applications that process such video streams. In this work, we introduce AQuA, to protect application accuracy against such distorted frames by scoring the level of distortion in the frames. It takes into account the analytical quality of frames, not the visual quality, by learning a novel metric, classifier opinion score, and uses a lightweight, CNN-based, object-independent feature extractor. AQuA accurately scores distortion levels of frames and generalizes to multiple different deep learning applications. When used for filtering poor-quality frames at edge, it reduces high-confidence errors for analytics applications by 17%. Through filtering, and due to its low overhead (14 ms), AQuA can also reduce computation time and average bandwidth usage by 25%. Finally, we discuss numerous new avenues of optimizations of video analytics pipelines enabled by AQuA.  © 2023 Association for Computing Machinery.",image quality; machine learning systems; Video analytics,Deep learning; Image quality; Learning systems; Analytics systems; Feature extractor; Image-quality metrics; Machine learning systems; Metric classifiers; Noise compression; Power; Sensors noise; Video analytics; Visual qualities; Cameras
A Multi-tenant Key-value SSD with Secondary Index for Search Query Processing and Analysis,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168774663&doi=10.1145%2f3590153&partnerID=40&md5=b31499216179a0f41fca78be53420fbe,"Key-value SSDs (KVSSDs) introduced so far are limited in their use as an alternative to the key-value store running on the host due to the following technical limitations. First, they were designed only for a single tenant, limiting the use of multiple tenants. Second, they mainly focused on designing indexes for primary key-based searches, without supporting various queries using a combination of primary key and non-primary attribute-based searches. This article proposes Cerberus, a Log Structured Merged (LSM) tree-based KVSSD armed with (1) namespace and performance isolation for multiple tenants in a multi-tenant environment and (2) capability for processing non-primary attribute-based search queries. Specifically, Cerberus identifies the tenant's namespace and splits a single large LSM-tree into namespace-specific LSM-tree indexes for tenants. Cerberus also manages secondary LSM-tree indexes to enable non-primary attribute-based data access and fast search query processing. With the SSD-internal CPU/DRAM resources, Cerberus supports non-primary attribute-based search queries and handles complex queries that are combined with search and computing operations. We prototyped Cerberus on the Cosmos+ OpenSSD platform. When there are multiple tenants, Cerberus exhibits up to 2.9× higher read throughput and negligible write overhead compared to existing KVSSD. Cerberus also shows lower latency by up to 9.31× for non-primary attribute-based queries.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Key-value solid-state drive; NoSQL database storage engine,Trees (mathematics); Virtual storage; Attribute-based; Database storage; Key values; Key-value solid-state drive; Log structured; Multi tenants; Namespaces; NoSQL database storage engine; Search queries; Storage engines; Query processing
"Special Issue: ""Approximation at the Edge""",2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170716216&doi=10.1145%2f3605757&partnerID=40&md5=7b9985f34b630d1930f21d4fdad84b37,[No abstract available],Additional Key Words and PhrasesApproximate Computing; edge computing; energy efficiency,
An Ultra-low-power Embedded AI Fire Detection and Crowd Counting System for Indoor Areas,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166045784&doi=10.1145%2f3582433&partnerID=40&md5=d5b7124fb37b0e10cd7c302549e6d084,"Fire incidents in residential and industrial areas are often the cause of human casualties and property damage. Although there are existing systems that detect fire and monitor the presence of people in indoor areas, research on their implementation in embedded platforms is limited. This article introduces an ultra-low-power embedded system for fire detection and crowd counting using efficient deep learning methods. For the prediction of fire occurrences, environmental and gas sensor along with multilayer perceptron nodes are used. For crowd counting, a custom lightweight version of YOLOv5 is introduced, using an architecture based on ShuffleNetV2, resulting in a model with low memory requirements, high accuracy predictions, and fast inference on an embedded platform. The accuracy, power consumption, and memory requirements of the proposed system are evaluated using public datasets and datasets acquired by the environmental and image sensors, and its performance is compared to that of existing approaches.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",computer vision; crowd counting; embedded system; Fire detection; LW-YOLOv5; machine learning for low-power systems; Multilayer perceptron (MLP); ultra-low-power system; YOLOv5,Computer vision; Deep learning; Fire detectors; Fires; Learning systems; Multilayer neural networks; Multilayers; Crowd counting; Embedded-system; Fire detection; Low-power systems; LW-YOLOv5; Machine learning for low-power system; Machine-learning; Multilayer perceptron; Multilayers perceptrons; Ultra-low power systems; YOLOv5; Embedded systems
A Granularity-Based Clustering Method for Reducing Write Amplification in Solid-State Drives,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170642461&doi=10.1145%2f3605779&partnerID=40&md5=f09a56c246363e4eb3eea207e72be588,"In recent years, solid-state drives (SSDs) that adopt NAND flash memory have been widely used as the main storage devices. In particular, NAND flash memory has a special feature of ""out-of-place""updates to write the up-to-date data to a free page, and the corresponding old page will become invalid. When the number of free pages in SSDs is insufficient, garbage collection (GC) will be executed to reclaim the invalid pages in a block by erasing the block. Many studies have shown that a good hot/cold data separation (i.e., clustering) can greatly reduce the overhead of GC so as to improve the SSD performance. However, previous clustering methods usually use a static number of clusters or a fixed size of granularity (i.e., a fine-grained or a coarse-grained granularity), so they may not always perform well for different kinds of workloads. Therefore, we propose a granularity-based clustering method to adaptively adjust the size of granularity groups for an appropriate number of clusters at runtime according to the update distances of logical addresses. According to the experimental results, we can improve SSD performance by reducing the overhead of GC and decrease the write amplification. Furthermore, we can show that the proposed method can utilize a fine-grained granularity to retain the record accuracy of update distances and also utilize a coarse-grained granularity to reduce the space required to record update distances.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesGranularity; clustering method; NAND flash memory; solid-state drives; write amplification,Cluster analysis; Cost reduction; Flash-based SSDs; Memory architecture; NAND circuits; Additional key word and phrasesgranularity; Based clustering; Clustering methods; Garbage collection; Key words; NAND flash memory; Number of clusters; Performance; Solid-state drive; Write amplifications; Virtual storage
Tailor-made Virtualization Monitor Design for CPU Virtualization on LEON Processors,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168775245&doi=10.1145%2f3584702&partnerID=40&md5=7779c96984ae4f4a371373ec04c72b24,"In recent decades, mixed-criticality systems have been widely adopted to reduce the complexity and development times of real-time critical applications. In these systems, applications run on a separation kernel hypervisor, a software element that controls the execution of the different operating systems, providing a virtualized environment and ensuring the necessary spatial and temporal isolation. The guest code can run unmodified and unaware of the hypervisor or be explicitly modified to have a tight coupling with the hypervisor. The former is known as full virtualization, while the latter is known as para-virtualization. Full virtualization offers better compatibility and flexibility than para-virtualization at the cost of a performance penalty.LEON is a processor family that implements the SPARC V8 architecture and whose use is widespread in the field of space systems. To the best of our knowledge, all separation kernel hypervisors designed to support the development of mixed-criticality systems for LEON employ para-virtualization, which hinders the adaptation of real-time operating systems.This article presents the design of a Virtualization Monitor that allows guest real-time operating systems to run virtualized on LEON-based systems without needing to modify their source code. It is designed as a stand-alone component within a hypervisor and incorporates a set of techniques such as static binary rewriting, automatic code generation, and the use of operating system profiles. To validate the proposed solution, tests and benchmarks have been implemented for three guest systems, RTEMS, FreeRTOS, and Zephyr, analyzing the overhead introduced in certain situations characteristic of real-time applications. Finally, the same benchmarks have been run on AIR, one of the hypervisors that uses para-virtualization. The results obtained show that the use of the proposed techniques allows us to obtain similar results to those obtained using para-virtualization without the need to modify the source code of the guest real-time operating systems.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",binary instrumentation; Full virtualization; LEON processor; on-board Software,Application programs; Automatic programming; Benchmarking; Codes (symbols); Computer operating systems; Criticality (nuclear fission); Integrated circuit design; Real time systems; Virtualization; Binary instrumentations; Full virtualization; Hypervisors; LEON processor; Mixed-criticality systems; On board softwares; Para-virtualization; Real.time operating system; Source codes; Virtualizations; Virtual reality
XploreNAS: Explore Adversarially Robust and Hardware-efficient Neural Architectures for Non-ideal Xbars,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168765453&doi=10.1145%2f3593045&partnerID=40&md5=d6bf0d13304069991386cfd86cadd212,"Compute In-Memory platforms such as memristive crossbars are gaining focus as they facilitate acceleration of Deep Neural Networks (DNNs) with high area and compute efficiencies. However, the intrinsic non-idealities associated with the analog nature of computing in crossbars limits the performance of the deployed DNNs. Furthermore, DNNs are shown to be vulnerable to adversarial attacks leading to severe security threats in their large-scale deployment. Thus, finding adversarially robust DNN architectures for non-ideal crossbars is critical to the safe and secure deployment of DNNs on the edge. This work proposes a two-phase algorithm-hardware co-optimization approach called XploreNAS that searches for hardware efficient and adversarially robust neural architectures for non-ideal crossbar platforms. We use the one-shot Neural Architecture Search approach to train a large Supernet with crossbar-awareness and sample adversarially robust Subnets therefrom, maintaining competitive hardware efficiency. Our experiments on crossbars with benchmark datasets (SVHN, CIFAR10, CIFAR100) show up to ∼8-16% improvement in the adversarial robustness of the searched Subnets against a baseline ResNet-18 model subjected to crossbar-aware adversarial training. We benchmark our robust Subnets for Energy-Delay-Area-Products (EDAPs) using the Neurosim tool and find that with additional hardware efficiency-driven optimizations, the Subnets attain ∼1.5-1.6× lower EDAPs than ResNet-18 baseline.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adversarial robustness; EDAP; memristive crossbars; Neural architecture search; non-idealities,Network architecture; Adversarial robustness; Energy delay; Energy-delay-area-product; Hardware efficiency; Memristive crossbar; Neural architecture search; Neural architectures; Nonideal; Nonideality; Subnets; Deep neural networks
ACM TECS Special Issue on Embedded System Security Tutorials,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164516716&doi=10.1145%2f3594872&partnerID=40&md5=d55073a1ead1ec98cf60af495c0c8479,[No abstract available],,
Lazy Load Scheduling for Mixed-criticality Applications in Heterogeneous MPSoCs,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164296375&doi=10.1145%2f3587694&partnerID=40&md5=6f2df50f81e2d9c1ea5f93a03d8a99bd,"Newly emerging multiprocessor system-on-a-chip (MPSoC) platforms provide hard processing cores with programmable logic (PL) for high-performance computing applications. In this article, we take a deep look into these commercially available heterogeneous platforms and show how to design mixed-criticality applications such that different processing components can be isolated to avoid contention on the shared resources such as last-level cache and main memory.Our approach involves software/hardware co-design to achieve isolation between the different criticality domains. At the hardware level, we use a scratchpad memory (SPM) with dedicated interfaces inside the PL to avoid conflicts in the main memory. At the software level, we employ a hypervisor to support cache-coloring such that conflicts at the shared L2 cache can be avoided. In order to move the tasks in/out of the SPM memory, we rely on a DMA engine and propose a new CPU-DMA co-scheduling policy, called Lazy Load, for which we also derive the response time analysis. The results of a case study on image processing demonstrate that the contention on the shared memory subsystem can be avoided when running with our proposed architecture. Moreover, comprehensive schedulability evaluations show that the newly proposed Lazy Load policy outperforms the existing CPU-DMA scheduling approaches and is effective in mitigating the main memory interference in our proposed architecture.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",heterogeneous multiprocessor systems-on-chip; Mixed-criticality real-time systems; schedulability analysis,Cache memory; Computation theory; Criticality (nuclear fission); Image processing; Integrated circuit design; Interactive computer systems; Memory architecture; Multiprocessing systems; System-on-chip; Heterogeneous multiprocessor system-on-chip; Heterogeneous multiprocessor systems; Main-memory; Mixed criticalities; Mixed-criticality real-time system; Multiprocessor system on chips; Multiprocessor systems-on-chips; Programmable logic; Real - Time system; Schedulability analysis; Real time systems
Neural Network Compression for Noisy Storage Devices,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164278336&doi=10.1145%2f3588436&partnerID=40&md5=d2c3ce95c0a66984f4cae6645c741085,"Compression and efficient storage of neural network (NN) parameters is critical for applications that run on resource-constrained devices. Despite the significant progress in NN model compression, there has been considerably less investigation in the actual physical storage of NN parameters. Conventionally, model compression and physical storage are decoupled, as digital storage media with error-correcting codes (ECCs) provide robust error-free storage. However, this decoupled approach is inefficient as it ignores the overparameterization present in most NNs and forces the memory device to allocate the same amount of resources to every bit of information regardless of its importance. In this work, we investigate analog memory devices as an alternative to digital media - one that naturally provides a way to add more protection for significant bits unlike its counterpart, but is noisy and may compromise the stored model's performance if used naively. We develop a variety of robust coding strategies for NN weight storage on analog devices, and propose an approach to jointly optimize model compression and memory resource allocation. We then demonstrate the efficacy of our approach on models trained on MNIST, CIFAR-10, and ImageNet datasets for existing compression techniques. Compared to conventional error-free digital storage, our method reduces the memory footprint by up to one order of magnitude, without significantly compromising the stored model's accuracy.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",analog storage; compression; Neural networks; PCM; robustness,Codes (symbols); Errors; Neural networks; Pulse code modulation; Virtual storage; Compression; Digital storage media; Error correcting code; Model compression; Network compression; Neural network model; Neural network parameters; Neural-networks; Resourceconstrained devices; Robustness; Analog storage
Robust Cause-Effect Chains with Bounded Execution Time and System-Level Logical Execution Time,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164291019&doi=10.1145%2f3573388&partnerID=40&md5=a2fc2e95d4b30277016d4760ce31f537,"In automotive and industrial real-time software systems, the primary timing constraints relate to cause-effect chains. A cause-effect chain is a sequence of linked tasks and it typically implements the process of reading sensor data, computing algorithms, and driving actuators. The classic timing analysis computes the maximum end-to-end latency of a given cause-effect chain to verify that its end-to-end deadline can be satisfied in all cases. This information is useful but not sufficient in practice: Software is usually evolving and updates may always alter the maximum end-to-end latency. It would be desirable to judge the quality of a software design a priori by quantifying how robust the timing of a given cause-effect chain will be in the presence of software updates. In this article, we derive robustness margins which guarantee that if software extensions stay within certain bounds, then the end-to-end deadline of a cause-effect chain can still be satisfied. Robustness margins are also useful to know if the system model has uncertain parameters. A robust system design can tolerate bounded deviations from the nominal system model without violating timing constraints. The results are applicable to both the bounded execution time programming model and the (system-level) logical execution time programming model. In this article, we study both an industrial use case from the automotive industry and analyze synthetically generated experiments with our open-source tool TORO.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",(system-level) logical execution time; bounded execution time; Cause-effect chains; extensibility; robustness,Automotive industry; Open source software; Open systems; Real time systems; Uncertainty analysis; (system-level) logical execution time; Bounded execution time; Cause-effect; Cause-effect chain; End to end latencies; Extensibility; Logical execution time; Robustness; System levels; Timing constraints; Software design
CODEBench: A Neural Architecture and Hardware Accelerator Co-Design Framework,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164275810&doi=10.1145%2f3575798&partnerID=40&md5=3d99196a815138d155d30ad5ebeae179,"Recently, automated co-design of machine learning (ML) models and accelerator architectures has attracted significant attention from both the industry and academia. However, most co-design frameworks either explore a limited search space or employ suboptimal exploration techniques for simultaneous design decision investigations of the ML model and the accelerator. Furthermore, training the ML model and simulating the accelerator performance is computationally expensive. To address these limitations, this work proposes a novel neural architecture and hardware accelerator co-design framework, called CODEBench. It comprises two new benchmarking sub-frameworks, CNNBench and AccelBench, which explore expanded design spaces of convolutional neural networks (CNNs) and CNN accelerators. CNNBench leverages an advanced search technique, Bayesian Optimization using Second-order Gradients and Heteroscedastic Surrogate Model for Neural Architecture Search, to efficiently train a neural heteroscedastic surrogate model to converge to an optimal CNN architecture by employing second-order gradients. AccelBench performs cycle-accurate simulations for diverse accelerator architectures in a vast design space. With the proposed co-design method, called Bayesian Optimization using Second-order Gradients and Heteroscedastic Surrogate Model for Co-Design of CNNs and Accelerators, our best CNN-accelerator pair achieves 1.4% higher accuracy on the CIFAR-10 dataset compared to the state-of-the-art pair while enabling 59.1% lower latency and 60.8% lower energy consumption. On the ImageNet dataset, it achieves 3.7% higher Top1 accuracy at 43.8% lower latency and 11.2% lower energy consumption. CODEBench outperforms the state-of-the-art framework, i.e., Auto-NBA, by achieving 1.5% higher accuracy and 34.7× higher throughput while enabling 11.0× lower energy-delay product and 4.0× lower chip area on CIFAR-10.  © 2023 Association for Computing Machinery.",Active learning; application-specific integrated circuits; hardware-software co-design; machine learning; Neural Architecture Search; neural network accelerators,Application programs; Computer hardware; Convolution; Convolutional neural networks; Energy utilization; Hardware-software codesign; Integrated circuit design; Integrated circuits; Learning systems; Network architecture; Timing circuits; Active Learning; Application-specific integrated circuits; Co-designs; Convolutional neural network; Hardware/software codesign; Machine-learning; Neural architecture search; Neural architectures; Neural network accelerator; Neural-networks; Machine learning
Edge-AI-Driven Framework with Efficient Mobile Network Design for Facial Expression Recognition,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164280960&doi=10.1145%2f3587038&partnerID=40&md5=726764bbf575207538885fbf178f264e,"Facial Expression Recognition (FER) in the wild poses significant challenges due to realistic occlusions, illumination, scale, and head pose variations of the facial images. In this article, we propose an Edge-AI-driven framework for FER. On the algorithms aspect, we propose two attention modules, Arbitrary-oriented Spatial Pooling (ASP) and Scalable Frequency Pooling (SFP), for effective feature extraction to improve classification accuracy. On the systems aspect, we propose an edge-cloud joint inference architecture for FER to achieve low-latency inference, consisting of a lightweight backbone network running on the edge device, and two optional attention modules partially offloaded to the cloud. Performance evaluation demonstrates that our approach achieves a good balance between classification accuracy and inference latency.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cloud offloading; Deep learning; edge computing; Facial Expression Recognition,Deep learning; Face recognition; Classification accuracy; Cloud offloading; Deep learning; Edge computing; Facial expression recognition; Facial images; Features extraction; Head pose; Network design; Pose variation; Edge computing
TCX: A RISC Style Tensor Computing Extension and a Programmable Tensor Processor,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164289928&doi=10.1145%2f3568310&partnerID=40&md5=55e58f806d769644a4224a79d16289b0,"Neural network processors and accelerators are domain-specific architectures deployed to solve the high computational requirements of deep learning algorithms. This article proposes a new instruction set extension for tensor computing, TCX, using Reduced Instruction Set Computer (RISC) instructions enhanced with variable length tensor extensions. It features a multi-dimensional register file, dimension registers, and fully generic tensor instructions. It can be seamlessly integrated into existing RISC Instruction Set Architectures and provides software compatibility for scalable hardware implementations. We present a tensor accelerator implementation of the tensor extensions using an out-of-order RISC microarchitecture. The tensor accelerator is scalable in computation units from several hundred to tens of thousands. An optimized register renaming mechanism is described that allows for many physical tensor registers without requiring architectural support for large tensor register names. We describe new tensor load and store instructions that reduce bandwidth requirements using tensor dimension registers. Implementations may balance data bandwidth and computation utilization for different types of tensor computations such as element-wise, depthwise, and matrix-multiplication. We characterize the computation precision of tensor operations to balance area, generality, and accuracy loss for several well-known neural networks. The TCX processor runs at 1 GHz and sustains 8.2 Tera operations per second using a 4,096 multiply-accumulate compute unit. It consumes 12.8 mm2 while dissipating 0.46W/TOPs in TSMC 28-nm technology.  © 2023 Association for Computing Machinery.",ASIC design; convolutional neural network; Neural network accelerator; Tensor Processor,Application specific integrated circuits; Bandwidth; Computer architecture; Convolutional neural networks; Deep learning; Integrated circuit design; Network architecture; ASIC design; Computational requirements; Computer instructions; Convolutional neural network; Domain specific architectures; Network processor; Neural network accelerator; Neural-networks; Reduced instruction set computers; Tensor processor; Tensors
Trireme: Exploration of Hierarchical Multi-level Parallelism for Hardware Acceleration,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163771610&doi=10.1145%2f3580394&partnerID=40&md5=88a1f7dfb4686211ad0883c652b6eab6,"The design of heterogeneous systems that include domain specific accelerators is a challenging and time-consuming process. While taking into account area constraints, designers must decide which parts of an application to accelerate in hardware and which to leave in software. Moreover, applications in domains such as Extended Reality (XR) offer opportunities for various forms of parallel execution, including loop level, task level, and pipeline parallelism. To assist the design process and expose every possible level of parallelism, we present Trireme, a fully automated tool-chain that explores multiple levels of parallelism and produces domain-specific accelerator designs and configurations that maximize performance, given an area budget. FPGA SoCs were used as target platforms, and Catapult HLS [7] was used to synthesize RTL using a commercial 12 nm FinFET technology. Experiments on demanding benchmarks from the XR domain revealed a speedup of up to 20×, as well as a speedup of up to 37× for smaller applications, compared to software-only implementations.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Accelerators; ASICs; compiler techniques and optimizations; design tools; heterogeneous systems parallelism,Application programs; Application specific integrated circuits; Benchmarking; Hierarchical systems; Integrated circuit design; Compiler optimizations; Compiler techniques; Design tool; Domain specific; Hardware acceleration; Heterogeneous system parallelism; Heterogeneous systems; Loop level; Multi-level parallelism; Parallel executions; Budget control
CNN-based Robust Sound Source Localization with SRP-PHAT for the Extreme Edge,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164300679&doi=10.1145%2f3586996&partnerID=40&md5=04ea41c684edd32981ea5a735e7ad245,"Robust sound source localization for environments with noise and reverberation are increasingly exploiting deep neural networks fed with various acoustic features. Yet, state-of-the-art research mainly focuses on optimizing algorithmic accuracy, resulting in huge models preventing edge-device deployment. The edge, however, urges for real-time low-footprint acoustic reasoning for applications such as hearing aids and robot interactions. Hence, we set off from a robust CNN-based model using SRP-PHAT features, Cross3D [16], to pursue an efficient yet compact model architecture for the extreme edge. For both the SRP feature representation and neural network, we propose respectively our scalable LC-SRP-Edge and Cross3D-Edge algorithms which are optimized towards lower hardware overhead. LC-SRP-Edge halves the complexity and on-chip memory overhead for the sinc interpolation compared to the original LC-SRP [19]. Over multiple SRP resolution cases, Cross3D-Edge saves 10.32%∼73.71% computational complexity and 59.77%∼94.66% neural network weights against the Cross3D baseline. In terms of the accuracy-efficiency tradeoff, the most balanced version (EM) requires only 127.1 MFLOPS computation, 3.71 MByte/s bandwidth, and 0.821 MByte on-chip memory in total, while still retaining competitiveness in state-of-the-art accuracy comparisons. It achieves 8.59 ms/frame end-to-end latency on a Rasberry Pi 4B, which is 7.26× faster than the corresponding baseline.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep neural network; hardware efficiency; Sound source localization; SRP-PHAT,Acoustic generators; Audition; Complex networks; Efficiency; Hearing aids; Acoustic features; Algorithmics; Art research; Hardware efficiency; Neural-networks; On-chip-memory; Real- time; Sound source localization; SRP-PHAT; State of the art; Deep neural networks
Challenges and Opportunities of Security-Aware EDA,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164284739&doi=10.1145%2f3576199&partnerID=40&md5=51f99ef05a68ef22ff1d59287b0c32b4,"The foundation of every digital system is based on hardware in which security, as a core service of many applications, should be deeply embedded. Unfortunately, the knowledge of system security and efficient hardware design is spread over different communities and, due to the complex and ever-evolving nature of hardware-based system security, state-of-the-art security is not always implemented in state-of-the-art hardware. However, automated security-aware hardware design seems to be a promising solution to bridge the gap between the different communities. In this work, we systematize state-of-the-art research with respect to security-aware Electronic Design Automation (EDA) and identify a modern security-aware EDA framework. As part of this work, we consider threats in the form of information flow, timing and power side channels, and fault injection, which are the fundamental building blocks of more complex hardware-based attacks. Based on the existing research, we provide important observations and research questions to guide future research in support of modern, holistic, and security-aware hardware design infrastructures.  © 2023 Copyright held by the owner/author(s).",computer aided design; electronic design automation; Fault Injection Analysis; Hardware design; Information Flow Analysis; Side-Channel Analysis,Automation; Bridges; Computer aided analysis; Electronic design automation; Side channel attack; Software testing; Computer-aided design; Electronics design automation; Fault injection; Fault injection analyse; Hardware design; Information flow analysis; Security-aware; Side-channel analysis; State of the art; System security; Computer hardware
Multi-bit Data Flow Error Detection Method Based on SDC Vulnerability Analysis,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164298963&doi=10.1145%2f3572838&partnerID=40&md5=f5d68352770bc9c128f44682c8b81ff3,"One of the most difficult data flow errors to detect caused by single-event upsets in space radiation is the Silent Data Corruption (SDC). To solve the problem of multi-bit upsets causing program SDC, an instruction multi-bit SDC vulnerability prediction model based on one-class support vector machine classification is built using SDC vulnerability analysis, which has more accurate vulnerability instruction identification capabilities. By hardening the program with selective instruction redundancy, we propose a multi-bit data flow error detection method for detecting SDC error (SDCVA-OCSVM), aiming to protect the data in the memory or register used by the program. We have also verified the effectiveness of the method through comparative experiments. The method has been verified to have a higher error detection rate and lower code size and time overhead.  © 2023 Association for Computing Machinery.",Error detection; multi-bit SDC vulnerability; support vector machine,Bit error rate; Data flow analysis; Data transfer; Error detection; Radiation hardening; Dataflow; Detection methods; Multi-bit silent data corruption vulnerability; Multi-bit upset; Multi-bits; Silent data corruptions; Single event upsets; Space radiations; Support vectors machine; Vulnerability analysis; Support vector machines
High-Level Approaches to Hardware Security: A Tutorial,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164302446&doi=10.1145%2f3577200&partnerID=40&md5=4011a374261ac14cbdd91e0cddd17533,"Designers use third-party intellectual property (IP) cores and outsource various steps in the integrated circuit (IC) design and manufacturing flow. As a result, security vulnerabilities have been rising. This is forcing IC designers and end users to re-evaluate their trust in ICs. If attackers get hold of an unprotected IC, they can reverse engineer the IC and pirate the IP. Similarly, if attackers get hold of a design, they can insert malicious circuits or take advantage of ""backdoors""in a design. Unintended design bugs can also result in security weaknesses. This tutorial paper provides an introduction to the domain of hardware security through two pedagogical examples of hardware security problems. The first is a walk-through of the scan chain-based side channel attack. The second is a walk-through of logic locking of digital designs. The tutorial material is accompanied by open access digital resources that are linked in this article.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cybersecurity; Hardware; logic locking; scan chain,Computer circuits; Cybersecurity; Hardware security; Integrated circuit design; Locks (fasteners); Side channel attack; Cyber security; Design flows; Forcings; Hardware; High-level approach; Logic locking; Manufacturing flow; Scan chain; Security vulnerabilities; Third parties; Integrated circuits
Risk of Stochastic Systems for Temporal Logic Specifications,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164293164&doi=10.1145%2f3580490&partnerID=40&md5=f33616ea8e4fb8b2c778ba0a50eed0e9,"The wide availability of data coupled with the computational advances in artificial intelligence and machine learning promise to enable many future technologies such as autonomous driving. While there has been a variety of successful demonstrations of these technologies, critical system failures have repeatedly been reported. Even if rare, such system failures pose a serious barrier to adoption without a rigorous risk assessment. This article presents a framework for the systematic and rigorous risk verification of systems. We consider a wide range of system specifications formulated in signal temporal logic (STL) and model the system as a stochastic process, permitting discrete-time and continuous-time stochastic processes. We then define the STL robustness risk as the risk of lacking robustness against failure. This definition is motivated as system failures are often caused by missing robustness to modeling errors, system disturbances, and distribution shifts in the underlying data generating process. Within the definition, we permit general classes of risk measures and focus on tail risk measures such as the value-at-risk and the conditional value-at-risk. While the STL robustness risk is in general hard to compute, we propose the approximate STL robustness risk as a more tractable notion that upper bounds the STL robustness risk. We show how the approximate STL robustness risk can accurately be estimated from system trajectory data. For discrete-time stochastic processes, we show under which conditions the approximate STL robustness risk can even be computed exactly. We illustrate our verification algorithm in the autonomous driving simulator CARLA and show how a least risky controller can be selected among four neural network lane-keeping controllers for five meaningful system specifications.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Risk-aware decision making; signal temporal logic; stochastic robustness; stochastic system verification,Autonomous vehicles; Computer circuits; Continuous time systems; Random processes; Risk assessment; Risk perception; Specifications; Stochastic models; Temporal logic; Autonomous driving; Decisions makings; Risk aware; Risk-aware decision making; Signal temporal logic; Stochastic robustness; Stochastic system verification; System failures; System verifications; Systems specification; Stochastic systems
Reliability Assessment and Safety Arguments for Machine Learning Components in System Assurance,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164282663&doi=10.1145%2f3570918&partnerID=40&md5=1c2be8b061c8fec6469934583381545c,"The increasing use of Machine Learning (ML) components embedded in autonomous systems - so-called Learning-Enabled Systems (LESs) - has resulted in the pressing need to assure their functional safety. As for traditional functional safety, the emerging consensus within both, industry and academia, is to use assurance cases for this purpose. Typically assurance cases support claims of reliability in support of safety, and can be viewed as a structured way of organising arguments and evidence generated from safety analysis and reliability modelling activities. While such assurance activities are traditionally guided by consensus-based standards developed from vast engineering experience, LESs pose new challenges in safety-critical application due to the characteristics and design of ML models. In this article, we first present an overall assurance framework for LESs with an emphasis on quantitative aspects, e.g., breaking down system-level safety targets to component-level requirements and supporting claims stated in reliability metrics. We then introduce a novel model-agnostic Reliability Assessment Model (RAM) for ML classifiers that utilises the operational profile and robustness verification evidence. We discuss the model assumptions and the inherent challenges of assessing ML reliability uncovered by our RAM and propose solutions to practical use. Probabilistic safety argument templates at the lower ML component-level are also developed based on the RAM. Finally, to evaluate and demonstrate our methods, we not only conduct experiments on synthetic/benchmark datasets but also scope our methods with case studies on simulated Autonomous Underwater Vehicles and physical Unmanned Ground Vehicles.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",assurance cases; Learning-Enabled Systems; operational profile; probabilistic claims; Robotics and Autonomous Systems; robustness verification; safe AI; safety arguments; safety regulation; safety-critical systems; Software reliability; statistical testing,Accident prevention; Autonomous underwater vehicles; Autonomous vehicles; Embedded systems; Ground vehicles; Learning systems; Machine components; Machine learning; Reliability analysis; Safety factor; Safety testing; Software testing; Verification; Assurance case; Learning-enabled system; Operational profile; Probabilistic claim; Probabilistics; Robotic and autonomous system; Robustness verification; Safe AI; Safety arguments; Safety critical systems; Safety regulations; Software-Reliability; Statistical testing; Software reliability
Hardware Trojan Detection Using Machine Learning: A Tutorial,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163179050&doi=10.1145%2f3579823&partnerID=40&md5=5541fd0a39840f50b5e21c91044a8320,"With the growth and globalization of IC design and development, there is an increase in the number of Designers and Design houses. As setting up a fabrication facility may easily cost upwards of $20 billion, costs for advanced nodes may be even greater. IC design houses that cannot produce their chips in-house have no option but to use external foundries that are often in other countries. Establishing trust with these external foundries can be a challenge, and these foundries are assumed to be untrusted. The use of these untrusted foundries in the global semiconductor supply chain has raised concerns about the security of the fabricated ICs targeted for sensitive applications. One of these security threats is the adversarial infestation of fabricated ICs with a Hardware Trojan (HT). An HT can be broadly described as a malicious modification to a circuit to control, modify, disable, or monitor its logic. Conventional VLSI manufacturing tests and verification methods fail to detect HT due to the different and un-modeled nature of these malicious modifications. Current state-of-the-art HT detection methods utilize statistical analysis of various side-channel information collected from ICs, such as power analysis, power supply transient analysis, regional supply current analysis, temperature analysis, wireless transmission power analysis, and delay analysis. To detect HTs, most methods require a Trojan-free reference golden IC. A signature from these golden ICs is extracted and used to detect ICs with HTs. However, access to a golden IC is not always feasible. Thus, a mechanism for HT detection is sought that does not require the golden IC. Machine Learning (ML) approaches have emerged to be extremely useful in helping eliminate the need for a golden IC. Recent works on utilizing ML for HT detection have been shown to be promising in achieving this goal. Thus, in this tutorial, we will explain utilizing ML as a solution to the challenge of HT detection. Additionally, we will describe the Electronic Design Automation (EDA) tool flow for automating ML-assisted HT detection. Moreover, to further discuss the benefits of ML-assisted HT detection solutions, we will demonstrate a Neural Network (NN)-assisted timing profiling method for HT detection. Finally, we will discuss the shortcomings and open challenges of ML-assisted HT detection methods.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Hardware security; hardware Trojan detection; machine learning; neural networks; side-channel analysis,Computer aided design; Foundries; Integrated circuit design; Machine learning; Malware; Side channel attack; Supply chains; Transient analysis; Design and Development; Design house; Detection methods; Globalisation; Hardware Trojan detection; IC design house; Machine-learning; Neural-networks; Power analysis; Side-channel analysis; Hardware security
EASYR: Energy-Efficient Adaptive System Reconfiguration for Dynamic Deadlines in Autonomous Driving on Multicore Processors,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164278398&doi=10.1145%2f3570503&partnerID=40&md5=f38db2c40a97eedbfe133afcc7a2d9fb,"The increasing computing demands of autonomous driving applications have driven the adoption of multicore processors in real-time systems, which in turn renders energy optimizations critical for reducing battery capacity and vehicle weight. A typical energy optimization method targeting traditional real-time systems finds a critical speed under a static deadline, resulting in conservative energy savings that are unable to exploit dynamic changes in the system and environment. We capture emerging dynamic deadlines arising from the vehicle's change in velocity and driving context for an additional energy optimization opportunity. In this article, we extend the preliminary work for uniprocessors [66] to multicore processors, which introduces several challenges. We use the state-of-the-art real-time gang scheduling [5] to mitigate some of the challenges. However, it entails an NP-hard combinatorial problem in that tasks need to be grouped into gangs of tasks, gang formation, which could significantly affect the energy saving result. As such, we present EASYR, an adaptive system optimization and reconfiguration approach that generates gangs of tasks from a given directed acyclic graph for multicore processors and dynamically adapts the scheduling parameters and processor speeds to satisfy dynamic deadlines while consuming as little energy as possible. The timing constraints are also satisfied between system reconfigurations through our proposed safe mode change protocol. Our extensive experiments with randomly generated task graphs show that our gang formation heuristic performs 32% better than the state-of-the-art one. Using an autonomous driving task set from Bosch and real-world driving data, our experiments show that EASYR achieves energy reductions of up to 30.3% on average in typical driving scenarios compared with a conventional energy optimization method with the current state-of-the-art gang formation heuristic in real-time systems, demonstrating great potential for dynamic energy optimization gains by exploiting dynamic deadlines.  © 2023 Copyright held by the owner/author(s).",Directed acyclic graph (DAG); dynamic voltage and frequency scaling (DVFS); earliest deadline first (EDF); energy-efficient; real-time; voltage-frequency island (VFI),Adaptive systems; Autonomous vehicles; Directed graphs; Dynamic frequency scaling; Energy efficiency; Fault tolerance; Heuristic methods; Interactive computer systems; Optimization; Response time (computer systems); Scheduling algorithms; Voltage scaling; Acyclic graphs; Directed acyclic graph; Dynamic voltage and frequency scaling; Earliest deadline first; Early deadline first; Energy efficient; Real- time; Voltage-frequency island; Voltage-frequency islands; Real time systems
Tutorial: Toward Robust Deep Learning against Poisoning Attacks,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152244136&doi=10.1145%2f3574159&partnerID=40&md5=489104c717c65400539a52bf1e6c2a34,"Deep Learning (DL) has been increasingly deployed in various real-world applications due to its unprecedented performance and automated capability of learning hidden representations. While DL can achieve high task performance, the training process of a DL model is both time- and resource-consuming. Therefore, current supply chains of the DL models assume the customers obtain pre-trained Deep Neural Networks (DNNs) from the third-party providers that have sufficient computing power. In the centralized setting, the model designer trains the DL model using the local dataset. However, the collected training data may contain erroneous or poisoned data points. The model designer might craft malicious training samples and inject a backdoor in the DL model distributed to the users. As a result, the user's model will malfunction. In the federated learning setting, the cloud server aggregates local models trained on individual local datasets and updates the global model. In this scenario, the local client could poison the local training set and/or arbitrarily manipulate the local update. If the cloud server incorporates the malicious local gradients in model aggregation, the resulting global model will have degraded performance or backdoor behaviors. In this article, we present a comprehensive overview of contemporary data poisoning and model poisoning attacks against DL models in both centralized and federated learning scenarios. In addition, we review existing detection and defense techniques against various poisoning attacks.  © 2023 Association for Computing Machinery.",federated learning; Neural networks; poisoning attacks; robustness,Cloud computing; Computing power; Deep neural networks; Learning systems; Backdoors; Centralised; Cloud servers; Federated learning; Global models; Learning models; Neural-networks; Poisoning attacks; Real-world; Robustness; Supply chains
Trustworthy Autonomous System Development,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146609991&doi=10.1145%2f3545178&partnerID=40&md5=83ba73391b0217b7c159c154adae8b07,"Autonomous systems emerge from the need to progressively replace human operators by autonomous agents in a wide variety of application areas. We offer an analysis of the state of the art in developing autonomous systems, focusing on design and validation and showing that the multi-faceted challenges involved go well beyond the limits of weak AI. We argue that traditional model-based techniques are defeated by the complexity of the problem, while solutions based on end-to-end machine learning fail to provide the necessary trustworthiness. We advocate a hybrid design approach, which combines the two, adopting the best of each, and seeks tradeoffs between trustworthiness and performance. We claim that traditional risk analysis and mitigation techniques fail to scale and discuss the trend of moving away from correctness at design time and toward reliance on runtime assurance techniques. We argue that simulation and testing remain the only realistic approach for global validation and show how current methods can be adapted to autonomous systems. We conclude by discussing the factors that will play a decisive role in the acceptance of autonomous systems and by highlighting the urgent need for new theoretical foundations.  © 2023 Association for Computing Machinery.",Autonomous system; critical systems engineering; dependability; machine learning; requirement and scenario specification; simulation and testing; trustworthy system development; validation,Autonomous agents; Risk analysis; Risk assessment; Autonomous system; Critical system engineering; Critical systems; Dependability; Machine-learning; Requirements specifications; Scenario specification; Simulation and testing; System development; Trustworthy system development; Trustworthy systems; Validation; Machine learning
Does SoC Hardware Development Become Agile by Saying So: A Literature Review and Mapping Study,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164292134&doi=10.1145%2f3578554&partnerID=40&md5=e70517ef35919e20320bad7dec67c39b,"The success of agile development methods in software development has raised interest in System-on-Chip (SoC) design, which involves high architectural and development process complexity under time and project management pressure. This article discovers the current state of agile hardware development with the questions (1) how well literature covers the SoC development process, (2) what agile methods and practices are applied or (3) what proposals are made to increase the agility, and (4) what is the impact for the SoC community. To answer the questions, a mapping study and literature review were performed. Seven hundred thirty papers were first studied, and eventually, after a rigorous filtering process, 25 papers were thoroughly analyzed. The results show that the popular agile SW development methods are applied in 5 cases, ideas adapted from the agile Hardware manifesto in 9 cases, and 11 cases do not define the Agile HW development method. Most of the papers address shorter development time by better methodologies and tools that indirectly shape the SoC development toward agility. The focus of agile hardware development is mostly on the SoC artifacts and methodological improvements have not been quantified. However, the literature indicates a significant impact on many academic chip prototypes. The challenges are better understood and the interest in agile methods is clearly increasing. The methodological gaps in the prevalent situation encourage further research and more accurate reporting of the development in addition to the SoC artifacts.  © 2023 Copyright held by the owner/author(s).",agile development; literature review; mapping study; methodology development; System-on-Chip,Application specific integrated circuits; Mapping; Programmable logic controllers; Project management; Software design; Agile development; Agile development methods; Agile methods; Development method; Development process; Hardware development; Literature reviews; Mapping studies; Methodology development; Systems-on-Chip; System-on-chip
DNN Is Not All You Need: Parallelizing Non-neural ML Algorithms on Ultra-low-power IoT Processors,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164280291&doi=10.1145%2f3571133&partnerID=40&md5=669da9265d5e430958cfbd19388e8996,"Machine Learning (ML) functions are becoming ubiquitous in latency- and privacy-sensitive IoT applications, prompting a shift toward near-sensor processing at the extreme edge and the consequent increasing adoption of Parallel Ultra-low-power (PULP) IoT processors. These compute- and memory-constrained parallel architectures need to run efficiently a wide range of algorithms, including key Non-neural ML kernels that compete favorably with Deep Neural Networks in terms of accuracy under severe resource constraints. In this article, we focus on enabling efficient parallel execution of Non-neural ML algorithms on two RISCV-based PULP platforms, namely, GAP8, a commercial chip, and PULP-OPEN, a research platform running on an FPGA emulator. We optimized the parallel algorithms through a fine-grained analysis and intensive optimization to maximize the speedup, considering two alternative Floating-point (FP) emulation libraries on GAP8 and the native FPU support on PULP-OPEN. Experimental results show that a target-optimized emulation library can lead to an average 1.61× runtime improvement and 37% energy reduction compared to a standard emulation library, while the native FPU support reaches up to 32.09× and 99%, respectively. In terms of parallel speedup, our design improves the sequential execution by 7.04× on average on the targeted octa-core platforms leading to energy and latency decrease up to 87%. Last, we present a comparison with the ARM Cortex-M4 microcontroller, a widely adopted commercial solution for edge deployments, which is 12.87× slower than PULP-OPEN.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",edge; Machine learning; MCUs; parallel ultra-low-power platforms,Deep neural networks; Digital arithmetic; Internet of things; Learning algorithms; Learning systems; Parallel architectures; Edge; Learning functions; Learning kernels; Machine learning algorithms; Machine-learning; MCU; Parallel ultra-low-power platform; Parallelizing; Sensor processing; Ultra-low power; Microcontrollers
Reconfigurable System-on-Chip Architectures for Robust Visual SLAM on Humanoid Robots,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149168993&doi=10.1145%2f3570210&partnerID=40&md5=2e9c4392a07b1c450e2fe3948a9f73fe,"Visual Simultaneous Localization and Mapping (vSLAM) is the method of employing an optical sensor to map the robot's observable surroundings while also identifying the robot's pose in relation to that map. The accuracy and speed of vSLAM calculations can have a very significant impact on the performance and effectiveness of subsequent tasks that need to be executed by the robot, making it a key building component for current robotic designs. The application of vSLAM in the area of humanoid robotics is particularly difficult due to the robot's unsteady locomotion. This paper introduces a pose graph optimization module based on RGB (ORB) features, as an extension of the KinectFusion pipeline (a well-known vSLAM algorithm), to assist in recovering the robot's stance during unstable gait patterns when the KinectFusion tracking system fails. We develop and test a wide range of embedded MPSoC FPGA designs, and we investigate numerous architectural improvements, both precise and approximation, to study their impact on performance and accuracy. Extensive design space exploration reveals that properly designed approximations, which exploit domain knowledge and efficient management of CPU and FPGA fabric resources, enable real-time vSLAM at more than 30 fps in humanoid robots with high energy-efficiency and without compromising robot tracking and map construction. This is the first FPGA design to achieve robust, real-time dense SLAM operation targeting specifically humanoid robots. An open source release of our implementations and data can be found in [1].  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesSimultaneous localization and mapping; approximate computing; FPGA; humanoid robots,Anthropomorphic robots; Energy efficiency; Integrated circuit design; Machine design; Mapping; Reconfigurable architectures; System-on-chip; Additional key word and phrasessimultaneous localization and mapping; Approximate computing; FPGA design; Humanoid robot; Key words; KinectFusion; Localization and mappings; Performance; Real- time; Visual simultaneous localization and mappings; Field programmable gate arrays (FPGA)
FARSI: An Early-stage Design Space Exploration Framework to Tame the Domain-specific System-on-chip Complexity,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149182006&doi=10.1145%2f3544016&partnerID=40&md5=0ee1a05f4f7ce344b4249d01d95270ed,"Domain-specific SoCs (DSSoCs) are an attractive solution for domains with extremely stringent power, performance, and area constraints. However, DSSoCs suffer from two fundamental complexities. On the one hand, their many specialized hardware blocks result in complex systems and thus high development effort. On the other hand, their many system knobs expand the complexity of design space, making the search for the optimal design difficult. Thus to reach prevalence, taming such complexities is necessary. To address these challenges, in this work, we identify the necessary features of an early-stage design space exploration framework that targets the complex design space of DSSoCs and provide an instance of one such framework that we refer to as FARSI. FARSI provides an agile system-level simulator with speed up and accuracy of 8,400× and 98.5% compared to Synopsys Platform Architect. FARSI also provides an efficient exploration heuristic and achieves up to 62× and 35× improvement in convergence time compared to the classic simulated annealing (SA) and modern Multi-Objective Optimistic Search. This is done by augmenting SA with architectural reasoning such as locality exploitation and bottleneck relaxation. Furthermore, we embed various co-design capabilities and show that, on average, they have a 32% impact on the convergence rate. Finally, we demonstrate that using development-cost-aware policies can lower the system complexity, both in terms of the component count and variation by as much as 60% and 82% (e.g., for Network-on-a-Chip subsystem), respectively.  © 2023 Association for Computing Machinery.",augmented reality; design space exploration; Domain specific systems; simulation,Application specific integrated circuits; Augmented reality; Complex networks; Computer aided design; Integrated circuit design; Network-on-chip; Servers; Attractive solutions; Chip complexity; Design space exploration; Design spaces; Domain specific; Domain specific system; Early stage designs; Simulation; Stringents; Systems-on-Chip; Simulated annealing
ETAP: Energy-aware Timing Analysis of Intermittent Programs,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149178411&doi=10.1145%2f3563216&partnerID=40&md5=91e71d1dd86eadc4e8462a683a062e82,"Energy harvesting battery-free embedded devices rely only on ambient energy harvesting that enables stand-alone and sustainable IoT applications. These devices execute programs when the harvested ambient energy in their energy reservoir is sufficient to operate and stop execution abruptly (and start charging) otherwise. These intermittent programs have varying timing behavior under different energy conditions, hardware configurations, and program structures. This article presents Energy-aware Timing Analysis of intermittent Programs (ETAP), a probabilistic symbolic execution approach that analyzes the timing and energy behavior of intermittent programs at compile time. ETAP symbolically executes the given program while taking time and energy cost models for ambient energy and dynamic energy consumption into account. We evaluate ETAP by comparing the compile-time analysis results of our benchmark codes and real-world application with the results of their executions on real hardware. Our evaluation shows that ETAP's prediction error rate is between 0.0076% and 10.8%, and it speeds up the timing analysis by at least two orders of magnitude compared to manual testing.  © 2023 Association for Computing Machinery.",energy harvesting; Intermittent computing; symbolic execution; timing analysis,Benchmarking; Energy utilization; Model checking; Power management; Timing circuits; Ambients; Battery-free; Compile time; Embedded device; Energy; Energy aware; Intermittent computing; Stand -alone; Symbolic execution; Timing Analysis; Energy harvesting
Vector Extensions in COTS Processors to Increase Guaranteed Performance in Real-Time Systems,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149186057&doi=10.1145%2f3561054&partnerID=40&md5=23be0e6e993d65cbd3cefebe96087c5b,"The need for increased application performance in high-integrity systems such as those in avionics is on the rise as software continues to implement more complex functionalities. The prevalent computing solution for future high-integrity embedded products is multi-processor systems-on-chip (MPSoC) processors. MPSoCs include central processing unit (CPU) multicores that enable improving performance via thread-level parallelism. MPSoCs also include generic accelerators (graphics processing units [GPUs]) and application-specific accelerators. However, the data processing approach (DPA) required to exploit each of these underlying parallel hardware blocks carries several open challenges to enable the safe deployment in high-integrity domains. The main challenges include the qualification of its associated runtime system and the difficulties in analyzing programs deploying the DPA with out-of-the-box timing analysis and code coverage tools. In this work, we perform a thorough analysis of vector extensions (VExts) in current commercial off-the-shelf (COTS) processors for high-integrity systems. We show that VExts prevent many of the challenges arising with parallel programming models and GPUs. Unlike other DPAs, VExts require no runtime support, prevent design race conditions that might arise with parallel programming models, and have minimum impact on the software ecosystem, enabling the use of existing code coverage and timing analysis tools. We develop vectorized versions of neural network kernels and show that the NVIDIA Xavier VExts provide a reasonable increase in guaranteed application performance of up to 2.7x. Our analysis contends that VExts are the DPA approach with arguably the fastest path for adoption in high-integrity systems.  © 2023 Association for Computing Machinery.",embedded systems; real-time; Vector extensions,Application programs; Codes (symbols); Computer graphics; Data handling; Graphics processing unit; Interactive computer systems; Multicore programming; Parallel processing systems; Parallel programming; Program processors; Real time systems; System-on-chip; Application performance; Commercial off the shelves; Commercial off-the shelves; Commercial-off-the-shelf; Embedded-system; High-integrity systems; Processing approach; Real- time; Timing Analysis; Vector extension; Embedded systems
QUIDAM: A Framework for Quantization-aware DNN Accelerator and Model Co-Exploration,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149180435&doi=10.1145%2f3555807&partnerID=40&md5=0c10612513dd0f08cb5aeb6b3e56ec0c,"As the machine learning and systems communities strive to achieve higher energy efficiency through custom deep neural network (DNN) accelerators, varied precision or quantization levels, and model compression techniques, there is a need for design space exploration frameworks that incorporate quantization-aware processing elements into the accelerator design space while having accurate and fast power, performance, and area models. In this work, we present QUIDAM, a highly parameterized quantization-aware DNN accelerator and model co-exploration framework. Our framework can facilitate future research on design space exploration of DNN accelerators for various design choices such as bit precision, processing element type, scratchpad sizes of processing elements, global buffer size, number of total processing elements, and DNN configurations. Our results show that different bit precisions and processing element types lead to significant differences in terms of performance per area and energy. Specifically, our framework identifies a wide range of design points where performance per area and energy varies more than 5× and 35×, respectively. With the proposed framework, we show that lightweight processing elements achieve on par accuracy results and up to 5.7× more performance per area and energy improvement when compared to the best 16-bit integer quantization-based implementation. Finally, due to the efficiency of the pre-characterized power, performance, and area models, QUIDAM can speed up the design exploration process by three to four orders of magnitude as it removes the need for expensive synthesis and characterization of each design.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep neural networks; design space exploration; hardware accelerators; model compression; quantization,Computer aided design; Energy efficiency; Bit precision; Design space exploration; Element type; Energy; Hardware accelerators; Model compression; Performance; Power performance; Processing elements; Quantisation; Deep neural networks
"SHARP: An Adaptable, Energy-Efficient Accelerator for Recurrent Neural Networks",2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149169401&doi=10.1145%2f3552513&partnerID=40&md5=21d9c528551c74fb0a6bb8b085465bf3,"The effectiveness of Recurrent Neural Networks (RNNs) for tasks such as Automatic Speech Recognition has fostered interest in RNN inference acceleration. Due to the recurrent nature and data dependencies of RNN computations, prior work has designed customized architectures specifically tailored to the computation pattern of RNN, getting high computation efficiency for certain chosen model sizes. However, given that the dimensionality of RNNs varies a lot for different tasks, it is crucial to generalize this efficiency to diverse configurations.In this work, we identify adaptiveness as a key feature that is missing from today's RNN accelerators. In particular, we first show the problem of low resource utilization and low adaptiveness for the state-of-the-art RNN implementations on GPU, FPGA, and ASIC architectures. To solve these issues, we propose an intelligent tiled-based dispatching mechanism for increasing the adaptiveness of RNN computation, in order to efficiently handle the data dependencies. To do so, we propose Sharp as a hardware accelerator, which pipelines RNN computation using an effective scheduling scheme to hide most of the dependent serialization. Furthermore, Sharp employs dynamic reconfigurable architecture to adapt to the model's characteristics.Sharp achieves 2×, 2.8×, and 82× speedups on average, considering different RNN models and resource budgets, compared to the state-of-the-art ASIC, FPGA, and GPU implementations, respectively. Furthermore, we provide significant energy reduction with respect to the previous solutions, due to the low power dissipation of Sharp (321 GFLOPS/Watt).  © 2023 Association for Computing Machinery.",accelerator; Long-Short-Term Memory (LSTM); low power; reconfigurability; Recurrent Neural Network (RNN); scheduling,Budget control; Computing power; Energy efficiency; Field programmable gate arrays (FPGA); Graphics processing unit; Low power electronics; Memory architecture; Network architecture; Reconfigurable architectures; Speech recognition; Adaptiveness; Automatic speech recognition; Data dependencies; Energy efficient; Long-short-term memory; Low Power; Network computations; Reconfigurability; Recurrent neural network; State of the art; Long short-term memory
Application-centric Network Management-Addressing Safety and Real-time in V2X Applications,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149178753&doi=10.1145%2f3528411&partnerID=40&md5=cc6fced068ad93ac58e0341cbe84828e,"The current roadmaps and surveys for future wireless networking typically focus on communication and networking technologies and use representative applications to derive future network requirements. Such a benchmarking approach, however, does not cover the application integration challenge that arises from the many distributed applications sharing a network infrastructure, each with their individual topology and data structure. The paper addresses V2X networks as an important example. Crucial end-to-end application constraints including real-time and safety encourage a closer look at application interference and systematic integration. This perspective paper proposes a two-layer resource management that divides the problem into an application integration and a network management task. Valet parking with high-resolution infrastructure camera support is elaborated as a use case that overarches vehicle network and wireless network management. Experiments demonstrate the benefits of complementing the current network-centric management by an application-centric integration.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Application-aware resource manager; network; RAN; resource manager; V2X,Benchmarking; Integration; Wireless networks; 'current; Application-aware resource manager; Application-centric; Network; Networks management; RAN; Real- time; Resource managers; V2X; Network management
Early DSE and Automatic Generation of Coarse-grained Merged Accelerators,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149175873&doi=10.1145%2f3546070&partnerID=40&md5=d55734de2fecc9f9eb7189d49f0469df,"Post-Moore's law area-constrained systems rely on accelerators to deliver performance enhancements. Coarse-grained accelerators can offer substantial domain acceleration, but manual, ad hoc identification of code to accelerate is prohibitively expensive. Because cycle-accurate simulators and high-level synthesis (HLS) flows are so time-consuming, the manual creation of high-utilization accelerators that exploit control and data flow patterns at optimal granularities is rarely successful. To address these challenges, we present AccelMerger, the first automated methodology to create coarse-grained, control- A nd data-flow-rich merged accelerators. AccelMerger uses sequence alignment matching to recognize similar function call-graphs and loops, and neural networks to quickly evaluate their post-HLS characteristics. It accurately identifies which functions to accelerate, and it merges accelerators to respect an area budget and to accommodate system communication characteristics like latency and bandwidth. Merging two accelerators can save as much as 99% of the area of one. The space saved is used by a globally optimal integer linear program to allocate more accelerators for increased performance. We demonstrate AccelMerger's effectiveness using HLS flows without any manual effort to fine-tune the resulting designs. On FPGA-based systems, AccelMerger yields application performance improvements of up to 16.7× over software implementations, and 1.91× on average with respect to state-of-the-art early-stage design space exploration tools.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",datapath optimization; Hardware-software codesign; neural networks,Acceleration; Application programs; Budget control; Data flow analysis; Data flow graphs; Data transfer; Hardware-software codesign; Integer programming; Network coding; Automatic Generation; Coarse-grained; Data paths; Dataflow; Datapath optimization; Hardware/software codesign; High-level synthesis; Moore Law; Neural-networks; Optimisations; High level synthesis
High-Performance Implementation of the Identity-Based Signature Scheme in IEEE P1363 on GPU,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149171552&doi=10.1145%2f3564784&partnerID=40&md5=a6f4461683080c3515cc3c01a8bf16f6,"Identity-based cryptography is proposed to solve the complicated certificate management of traditional public-key cryptography. The pairing computation and high-level tower extension field arithmetic turn out to be the performance bottleneck of pairing-based signature schemes. Graphics processing units have been increasingly popular for general-purpose computing in recent years. They have shown a lot of promise in speeding up cryptographic schemes such as AES, RSA, and ECDSA. However, to our knowledge, the research on parallel implementation of pairings and identity-based cryptographic schemes on graphics processing units is somewhat outdated. Therefore, in this article, we implement the identity-based signature scheme in the IEEE P1363 Standard on a modern NVIDIA RTX 3060 card. We convert the pairing computation in signature verification into a product of pairings with fixed arguments and therefore avoid the scalar multiplication in 2. Then we employ the precomputation technique to improve the elliptic curve scalar multiplication, exponentiation in and the pairing computation. We also apply PTX ISA to multiple-precision arithmetic. Experiments demonstrate that our implementation can perform 43,856/46,753/39,798 pairings/sec for the Optimal Ate pairing, the pairing with a fixed argument, and two pairings with fixed arguments, respectively. Peak throughputs of signature generation and verification can achieve 322.6 and 40.6 kops/sec over the BN254 curve.  © 2023 Association for Computing Machinery.",CUDA; graphics processing units; Identity-based signatures,Authentication; Computer graphics; IEEE Standards; Program processors; Public key cryptography; Certificate management; Cryptographic schemes; CUDA; Extension field; High performance implementations; Identity based cryptography; Identity based signature; Identity-based signature scheme; Scalar multiplication; Signature verification; Graphics processing unit
Experimental Demonstration of STT-MRAM-based Nonvolatile Instantly On/Off System for IoT Applications: Case Studies,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148486729&doi=10.1145%2f3546193&partnerID=40&md5=8fb839e9342c2dedb3d8aba4c3b9db96,"Energy consumption has been a big challenge for electronic devices, particularly for battery-powered Internet of Things (IoT) equipment. To address such a challenge, on the one hand, low-power electronic design methodologies and novel power management techniques have been proposed, such as nonvolatile memories and instantly on/off systems; on the other hand, the energy harvesting technology by collecting signals from human activity or the environment has attracted widespread attention in the IoT area. However, the system with self-powered energy harvesting may suffer frequent energy failures or fluctuating energy conditions, which degrade system reliability and user experience. Therefore, how to make the system under unreliable power inputs operate correctly and efficiently is one of the most critical issues for energy harvesting technology. In this article, we built an instantly on/off system based on nonvolatile STT-MRAM for IoT applications, which can instantly power on/off under different conditions of the harvested energy. The system powers on and operates normally when the harvested energy is enough (over the preset threshold); otherwise, the system powers off and stores the operational data back to the nonvolatile STT-MRAM. We described implementations of the hardware/software co-designed architecture (with image acquisition as an example) based on the commercialized 32 MB STT-MRAM, and we experimentally demonstrated the system functionality and efficiency under five typical energy harvesting scenarios, including radio frequency, thermal, solar, piezoelectric, and WIFI. Our experimental results show that the power consumption and data restore time were reduced by 15.1% and 714 times, respectively, in comparison with the DRAM-based counterpart.  © 2023 Association for Computing Machinery.",energy harvesting; instantly on/off system; IoT applications; Non-volatile memory; STT-MRAM,Dynamic random access storage; Energy utilization; Internet of things; Magnetic recording; MRAM devices; On-off control systems; Power management; Case-studies; Energy; Energy-consumption; Experimental demonstrations; Instantly on/off system; Internet of thing application; Non-volatile memory; Nonvolatile; Power; STT-MRAM; Energy harvesting
Domain-Specific Architectures: Research Problems and Promising Approaches,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149186244&doi=10.1145%2f3563946&partnerID=40&md5=8bf5e5d21d3e163e0a9a98f40a67187e,"Process technology-driven performance and energy efficiency improvements have slowed down as we approach physical design limits. General-purpose manycore architectures attempt to circumvent this challenge, but they have a significant performance and energy-efficient gap compared to special-purpose solutions. Domain-specific architectures (DSAs), an instance of heterogeneous architectures, efficiently combine general-purpose cores and specialized hardware accelerators to boost energy efficiency and provide programming flexibility. Indeed, the hardware, software, and systems aspects in DSAs are highly tailored to maximize the energy efficiency of applications in a target domain. As DSAs and their conceptualization advance rapidly, there is a strong need to understand the research problems that need immediate attention. This article discusses the primary research directions in the design and runtime management of DSAs. Then, it surveys some promising approaches and highlights the outstanding research needs.  © 2023 Association for Computing Machinery.",Domain-specific architectures; domain-specific system-on-chip; DSA runtime resource management; emerging systems; hardware architectures; runtime frameworks,Application programs; Application specific integrated circuits; Energy efficiency; Integrated circuit design; Domain specific; Domain specific architectures; Domain-specific architecture runtime resource management; Domain-specific system-on-chip; Emerging system; Hardware architecture; Resource management; Runtime frameworks; Runtimes; Systems-on-Chip; System-on-chip
SAT-Reach: A Bounded Model Checker for Affine Hybrid Systems,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148890213&doi=10.1145%2f3567425&partnerID=40&md5=519b81dc5faec831976ecfd559a46bb7,"Bounded model checking (BMC) is well-known to be undecidable even for simple hybrid systems. Existing work targeted for a wide class of non-linear hybrid systems reduces the BMC problem to the satisfiability problem of an satisfiability modulo theory formula encoding the hybrid system dynamics. Consequently, the satisfiability of the formula is deduced with a δ-decision procedure. However, the encoded formula can be complex for large automaton and for deep exploration causing the decision procedure to be inefficient. Additionally, a generalized decision procedure can be inefficient for hybrid systems with simple dynamics. In this article, we propose a BMC algorithm built upon the foundation of the counter example guided abstraction refinement (CEGAR) technique and targeted for hybrid systems with piecewise affine dynamics, modeled as a hybrid automaton. In particular, our algorithm begins by searching an abstract counterexample in the discrete state-space of the automaton. We check whether a discovered abstract counterexample is spurious or real by a two-tier refinement of the state-space guided by the abstract counterexample. The primary refinement is through symbolic reachability analysis and the following refinement is via a search of a real counterexample by the trajectory splicing method, guided in turn by the outcome of reachability analysis. We show that our algorithm reaps the benefits of the CEGAR technique by directing the exploration in the regions of interest and pruning search space that is irrelevant to the property under consideration. In addition, an optimization by memoizing the computed symbolic states during reachability analysis has been proposed for efficiency. The proposed algorithm is implemented in the tool SAT-Reach, and we compare its performance with dReach, XSpeed, Flow∗, SpaceEx, and a pattern database heuristic-guided search algorithm. Experiments demonstrate the efficacy of our algorithm.  © 2023 Association for Computing Machinery.",affine hybrid systems; Bounded model checking; CEGAR; counterexample generation; reachability analysis; SAT-solving,Abstracting; Linear systems; Model checking; Abstraction-refinement; Affine hybrid systems; Bounded model checking; Counter example guided abstraction refinement; Counter examples; Counterexample generation; Decision procedure; Reachability analysis; SAT-solving; Simple++; Hybrid systems
A Predictable QoS-aware Memory Request Scheduler for Soft Real-time Systems,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149183264&doi=10.1145%2f3561052&partnerID=40&md5=27b93d0a0a5b804e81dab0744c770ef5,"A memory controller manages the flow of data to and from attached memory devices. The order in which a set of contending memory requests from different tasks are serviced significantly influences the rate of progress and completion times of these tasks. This in turn may affect the Quality-of-Service (QoS) delivered by these tasks. In this article, we focus towards the design of a QoS-aware memory controller targeted towards soft real-time systems. The proposed memory controller tries to generate an urgency-based schedule for the contending memory requests based on the allowable response time latencies associated with each request. The objective is to improve task-level response time predictability while maximizing acquired QoS. Exhaustive experiments carried out using real memory traces and standard simulation tools exhibit the practical efficacy of the proposed memory controller design.  © 2023 Association for Computing Machinery.",Memory request scheduling; predictable memory controller; QoS-aware memory request scheduling; soft real-time systems,Controllers; Interactive computer systems; Real time systems; Response time (computer systems); Flow of data; Memory controller; Memory request scheduling; Predictable memory controller; Quality-of-service; Quality-of-service-aware memory request scheduling; Request scheduling; Service-aware; Soft real-time systems; Quality of service
A Contrastive Plan Explanation Framework for Hybrid System Models,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149182176&doi=10.1145%2f3561532&partnerID=40&md5=4b647868fea641c3da0f85e7210beb7b,"In artificial intelligence planning, having an explanation of a plan given by a planner is often desirable. The ability to explain various aspects of a synthesized plan to an end user not only brings in trust on the planner but also reveals insights of the planning domain and the planning process. Contrastive questions such as ""Why action A instead of action B?""can be answered with a contrastive explanation that compares properties of the original plan containing A against the contrastive plan containing B. In this article, we explore a set of contrastive questions that a user of a planning tool may raise and propose a re-model and re-plan framework to provide explanations to such questions. Earlier work has reported this framework on planning instances for discrete problem domains described in the Planning Domain Definition Language (PDDL) and its variants. In this article, we propose an extension for planning instances described by PDDL+ for hybrid systems that portray a mix of discrete-continuous dynamics. Specifically, given a mixed discrete-continuous system model in PDDL+ and a plan describing the set of desirable actions on the same to achieve a destined goal, we present a framework that can integrate contrastive questions in PDDL+ and synthesize alternate plans. We present a detailed case study on our approach and propose a comparison metric to compare the original plan with the alternate ones.  © 2023 Association for Computing Machinery.",Artificial intelligence planning; bounded reachability; explainable planning; hybrid automata,Artificial intelligence; Discrete time control systems; Artificial intelligence planning; Bounded reachability; End-users; Explainable planning; Hybrid automatons; Hybrid system models; Planning domain definition language; Planning domains; Reachability; Synthesised; Hybrid systems
HLS-based High-throughput and Work-efficient Synthesizable Graph Processing Template Pipeline,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149167838&doi=10.1145%2f3529256&partnerID=40&md5=8e6eb142938cfe5616a084ee2874f454,"Hardware systems composed of diverse execution resources are being deployed to cope with the complexity and performance requirements of Artificial Intelligence (AI) and Machine Learning (ML) applications. With the emergence of new hardware platforms, system-wide programming support has become much more important. While this is true for various devices ranging from CPUs to GPUs, it is especially critical for specific neural network accelerators implemented on FPGAs. For example, Intel's recent HARP platform encompasses a Xeon CPU and an FPGA, which requires an intense software stack to be used effectively. Programming such a hybrid system will be a challenge for most of the non-expert users. High-level language solutions such as Intel OpenCL for FPGA try to address the problem. However, as the abstraction level increases, the efficiency of implementation decreases, depicting two opposing requirements. In this work, we propose a framework to generate HLS-based, FPGA-accelerated, high-throughput/work-efficient, synthesizable, and template-based graph-processing pipeline. While a fixed and clock-wise precisely designed deep-pipeline architecture, written in SystemC, is responsible for processing graph vertices, the user implements the intended iterative graph algorithm by implementing/modifying only a single module in C/C++. This way, efficiency and high performance can be achieved with better programmability and productivity. With similar programming efforts, it is shown that the proposed template outperforms a high-throughput OpenCL baseline by up to 50% in terms of edge throughput. Furthermore, the novel work-efficient design significantly improves execution time and power consumption by up to 100×.  © 2023 Association for Computing Machinery.",deep pipeline; Graph processing; hardware accelerator; OpenCL; SystemC; Xeon+FPGA,Artificial intelligence; C++ (programming language); Energy efficiency; Graphic methods; Hybrid systems; Iterative methods; Pipeline processing systems; Pipelines; Program processors; Artificial intelligence learning; Deep pipelines; Graph processing; Hardware accelerators; Hardware system; High-throughput; Opencl; Performance requirements; SystemC; Xeon+FPGA; Field programmable gate arrays (FPGA)
Introduction to the Special Issue on Domain-Specific System-on-Chip Architectures and Run-Time Management Techniques,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149238547&doi=10.1145%2f3567834&partnerID=40&md5=0b4b479a38311cc5d1a631e7728c6079,[No abstract available],,
AdaTest: Reinforcement Learning and Adaptive Sampling for On-chip Hardware Trojan Detection,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149176015&doi=10.1145%2f3544015&partnerID=40&md5=9d5dc6e830e01b1aeb175cdd9ba6d709,"This paper proposes AdaTest, a novel adaptive test pattern generation framework for efficient and reliable Hardware Trojan (HT) detection. HT is a backdoor attack that tampers with the design of victim integrated circuits (ICs). AdaTest improves the existing HT detection techniques in terms of scalability and accuracy of detecting smaller Trojans in the presence of noise and variations. To achieve high trigger coverage, AdaTest leverages Reinforcement Learning (RL) to produce a diverse set of test inputs. Particularly, we progressively generate test vectors with high 'reward' values in an iterative manner. In each iteration, the test set is evaluated and adaptively expanded as needed. Furthermore, AdaTest integrates adaptive sampling to prioritize test samples that provide more information for HT detection, thus reducing the number of samples while improving the samples' quality for faster exploration. We develop AdaTest with a Software/Hardware co-design principle and provide an optimized on-chip architecture solution. AdaTest's architecture minimizes the hardware overhead in two ways: (i) Deploying circuit emulation on programmable hardware to accelerate reward evaluation of the test input; (ii) Pipelining each computation stage in AdaTest by automatically constructing auxiliary circuit for test input generation, reward evaluation, and adaptive sampling. We evaluate AdaTest's performance on various HT benchmarks and compare it with two prior works that use logic testing for HT detection. Experimental results show that AdaTest engenders up to two orders of test generation speedup and two orders of test set size reduction compared to the prior works while achieving the same level or higher Trojan detection rate.  © 2023 Association for Computing Machinery.",Hardware trojan detection; logic testing; software/hardware co-design,Benchmarking; Computation theory; Computer circuits; Design for testability; Field programmable gate arrays (FPGA); Hardware security; Hardware-software codesign; Integrated circuit design; Integrated circuits; Iterative methods; Malware; Software testing; Adaptive sampling; Adaptive tests; Hardware Trojan detection; Logic testing; On chips; Reinforcement learnings; Reward-evaluation; Software/hardware co designs; Test inputs; Test sets; Reinforcement learning
"CEDR: A Compiler-integrated, Extensible DSSoC Runtime",2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149169393&doi=10.1145%2f3529257&partnerID=40&md5=584e7a3f449ee613e75ca2f721c9b501,"In this work, we present a Compiler-integrated, Extensible Domain Specific System on Chip Runtime (CEDR) ecosystem to facilitate research toward addressing the challenges of architecture, system software, and application development with distinct plug-and-play integration points in a unified compile time and runtime workflow. We demonstrate the utility of CEDR on the Xilinx Zynq MPSoC-ZCU102 for evaluating performance of pre-silicon hardware in the trade space of SoC configuration, scheduling policy and workload complexity based on dynamically arriving workload scenarios composed of real-life signal processing applications scaling to thousands of application instances with Fast Fourier Transform and matrix multiply accelerators. We provide insights into the tradeoffs present in this design space through a number of distinct case studies. CEDR is portable and has been deployed and validated on Odroid-XU3, X86, and Nvidia Jetson Xavier-based SoC platforms. Taken together, CEDR is a capable environment for enabling research in exploring the boundaries of productive application development, resource management heuristic development, and hardware configuration analysis for heterogeneous architectures.  © 2023 Copyright held by the owner/author(s).",Domain-specific SoCs; heterogeneous application runtimes,Application programs; Commerce; Fast Fourier transforms; Program compilers; Programmable logic controllers; Signal processing; Application development; Domain specific; Domain-specific soc; Heterogeneous application runtime; Integration points; Plug and play integration; Runtimes; System applications; System softwares; Systems-on-Chip; System-on-chip
Virtualizing a Post-Moore's Law Analog Mesh Processor: The Case of a Photonic PDE Accelerator,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149186719&doi=10.1145%2f3544971&partnerID=40&md5=63997ca55fc238e9d139999903390cb3,"Innovative processor architectures aim to play a critical role in future sustainment of performance improvements under severe limitations imposed by the end of Moore's Law. The Reconfigurable Optical Computer (ROC) is one such innovative, Post-Moore's Law processor. ROC is designed to solve partial differential equations in one shot as opposed to existing solutions, which are based on costly iterative computations. This is achieved by leveraging physical properties of a mesh of optical components that behave analogously to lumped electrical components. However, virtualization is required to combat shortfalls of the accelerator hardware. Namely, (1) the infeasibility of building large photonic arrays to accommodate arbitrarily large problems and (2) underutilization brought about by mismatches in problem and accelerator mesh sizes due to future advances in manufacturing technology. In this work, we introduce an architecture and methodology for lightweight virtualization of ROC that exploits advantages borne from optical computing technology. Specifically, we apply temporal and spatial virtualization to ROC and then extend the accelerator scheduling tradespace with the introduction of spectral virtualization. Additionally, we investigate multiple resource scheduling strategies for a system-on-chip (SoC)-based PDE acceleration architecture and show that virtual configuration management offers a speedup of approximately 2×. Finally, we show that overhead from virtualization is minimal, and our experimental results show two orders of magnitude increased speed as compared to microprocessor execution while keeping errors due to virtualization under 10%.  © 2023 Association for Computing Machinery.",domain-specific accelerators; parallel processing; scheduling; scientific computing accelerators; Virtualization,Computer architecture; Iterative methods; Mesh generation; Optical data processing; Programmable logic controllers; System-on-chip; Virtual reality; Domain specific; Domain-specific accelerator; Moore Law; Optical computers; Parallel processing; Performance; Processor architectures; Reconfigurable; Scientific computing accelerator; Virtualizations; Virtualization
AHA: An Agile Approach to the Design of Coarse-Grained Reconfigurable Accelerators and Compilers,2023,ACM Transactions on Embedded Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149178266&doi=10.1145%2f3534933&partnerID=40&md5=2a6d391e661a9bf0e0b20d0efb6eca97,"With the slowing of Moore's law, computer architects have turned to domain-specific hardware specialization to continue improving the performance and efficiency of computing systems. However, specialization typically entails significant modifications to the software stack to properly leverage the updated hardware. The lack of a structured approach for updating the compiler and the accelerator in tandem has impeded many attempts to systematize this procedure. We propose a new approach to enable flexible and evolvable domain-specific hardware specialization based on coarse-grained reconfigurable arrays (CGRAs). Our agile methodology employs a combination of new programming languages and formal methods to automatically generate the accelerator hardware and its compiler from a single source of truth. This enables the creation of design-space exploration frameworks that automatically generate accelerator architectures that approach the efficiencies of hand-designed accelerators, with a significantly lower design effort for both hardware and compiler generation. Our current system accelerates dense linear algebra applications but is modular and can be extended to support other domains. Our methodology has the potential to significantly improve the productivity of hardware-software engineering teams and enable quicker customization and deployment of complex accelerator-rich computing systems.  © 2023 Association for Computing Machinery.",coarse-grained reconfigurable arrays; domain-specific languages; Hardware accelerators; image processing,Computer hardware; Efficiency; Formal methods; Linear algebra; Problem oriented languages; Program compilers; Agile approaches; Coarse-grained reconfigurable; Coarse-grained reconfigurable arrays; Computing system; Domain specific; Domains specific languages; Hardware accelerators; Images processing; Specialisation; Specific hardware; Image processing
