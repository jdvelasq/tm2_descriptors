Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
Iteration interleaving-based SIMD lane partition,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954181026&doi=10.1145%2f2847253&partnerID=40&md5=e82069098457b58998f8d267e024efd3,"The efficacy of single instruction, multiple data (SIMD) architectures is limited when handling divergent control flows. This circumstance results in SIMD fragments using only a subset of the available lanes. We propose an iteration interleaving-based SIMD lane partition (IISLP) architecture that interleaves the execution of consecutive iterations and dynamically partitions SIMD lanes into branch paths with comparable execution time. The benefits are twofold: SIMD fragments under divergent branches can execute in parallel, and the pathology of fragment starvation can also be well eliminated. Our experiments show that IISLP doubles the performance of a baseline mechanism and provides a speedup of 28% versus instruction shuffle. Copyright © 2015 ACM.",Instruction shuffle; Iteration interleaving; SIMD; SIMD lane partition; Vector iteration,"Hardware; Branch paths; Control flows; Instruction shuffle; Iteration interleaving; SIMD; Single instruction , multiple datum; Software engineering"
Optimizing control transfer and memory virtualization in full system emulators,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954122373&doi=10.1145%2f2837027&partnerID=40&md5=cf6b3414d181d74b833290e5dec14a36,"Full system emulators provide virtual platforms for several important applications, such as kernel and system software development, co-verification with cycle accurate CPU simulators, or application development for hardware still in development. Full system emulators usually use dynamic binary translation to obtain reasonable performance. This paper focuses on optimizing the performance of full system emulators. First, we optimize performance by enabling classic control transfer optimizations of dynamic binary translation in full system emulation, such as indirect branch target caching and block chaining. Second, we improve the performance of memory virtualization of cross-ISA virtual machines by improving the efficiency of the software translation lookaside buffer (software TLB). We implement our optimizations on QEMU, an industrial-strength full system emulator, along with the Android emulator. Experimental results show that our optimizations achieve an average speedup of 1.98X for ARM-to-X86-64 QEMU running SPEC CINT2006 benchmarks with train inputs. Our optimizations also achieve an average speedup of 1.44X and 1.40X for IA32-to-X86-64 QEMU and AArch64-to-X86-64 QEMU on SPEC CINT2006. We use a set of real applications downloaded from Google Play as benchmarks for the Android emulator. Experimental results show that our optimizations achieve an average speedup of 1.43X for the Android emulator running these applications. Copyright © 2015 ACM.",Control transfer optimizations; Memory virtualization optimizations,Android (operating system); Application programs; Bins; Buffer storage; Embedded systems; Java programming language; Software design; Verification; Virtual reality; Dynamic binary translation; Industrial strength; Memory virtualization; Optimizing control; Real applications; System emulation; Translation lookaside buffer; Virtual machines; Benchmarking
A framework for application-guided task management on heterogeneous embedded systems,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954138399&doi=10.1145%2f2835177&partnerID=40&md5=5c03e24a63eaf7185c6ad83a4198feda,"In this article, we propose a general framework for fine-grain application-aware task management in heterogeneous embedded platforms, which allows integration of different mechanisms for an efficient resource utilization, frequency scaling, and task migration. The proposed framework incorporates several components for accurate runtime monitoring by relying on the OS facilities and performance self-reporting for parallel and iterative applications. The framework efficiency is experimentally evaluated on a real hardware platform, where significant power and energy savings are attained for SPEC CPU2006 and PARSEC benchmarks, by guiding frequency scaling and intercluster migrations according to the runtime application behavior and predefined performance targets. Copyright © 2015 ACM.",Big.LITTLE; Dynamic voltage and frequency control; Embedded systems; Heterogeneous multiprocessor; Quality of service; Scheduling; Task migration,Benchmarking; Dynamic frequency scaling; Energy conservation; Quality control; Quality of service; Scheduling; Application behaviors; Big.LITTLE; Different mechanisms; Dynamic voltage; Heterogeneous embedded system; Heterogeneous multiprocessors; Resource utilizations; Task migration; Embedded systems
Integer linear programming-based scheduling for transport triggered architectures,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954319266&doi=10.1145%2f2845082&partnerID=40&md5=d37f6aafb66066caa5ed9f662719a181,"Static multi-issue machines, such as traditional Very Long Instructional Word (VLIW) architectures, move complexity from the hardware to the compiler. This is motivated by the ability to support high degrees of instruction-level parallelism without requiring complicated scheduling logic in the processor hardware. The simpler-control hardware results in reduced area and power consumption, but leads to a challenge of engineering a compiler with good code-generation quality. Transport triggered architectures (TTA), and other so-called exposed datapath architectures, take the compiler-oriented philosophy even further by pushing more details of the datapath under software control. Themain benefit of this is the reduced register file pressure, with a drawback of adding even more complexity to the compiler side. In this article, we propose an Integer Linear Programming (ILP)-based instruction scheduling model for TTAs. The model describes the architecture characteristics, the particular processor resource constraints, and the operation dependencies of the scheduled program. The model is validated andmeasured by compiling application kernels to various TTAs with a different number of datapath components and connectivity. In the best case, the cycle count is reduced to 52% when compared to a heuristic scheduler. In addition to producing shorter schedules, the number of register accesses in the compiled programs is generally notably less than those with the heuristic scheduler; in the best case, the ILP scheduler reduced the number of register file reads to 33% of the heuristic results and register file writes to 18%. On the other hand, as expected, the ILP-based scheduler uses distinctly more time to produce a schedule than the heuristic scheduler, but the compilation time is within tolerable limits for production-code generation. Copyright © 2015 ACM.",Code generation; Exposed datapath; Instruction-level parallelism; Integer linear programming; Transport triggered architectures,Codes (symbols); Hardware; Inductive logic programming (ILP); Network components; Parallel processing systems; Program compilers; Program processors; Quality control; Reconfigurable hardware; Scheduling; Very long instruction word architecture; Code Generation; Data paths; Instruction level parallelism; Integer Linear Programming; Transport triggered architecture; Integer programming
DASH: Deadline-aware high-performance memory scheduler for heterogeneous systems with hardware accelerators,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954161826&doi=10.1145%2f2847255&partnerID=40&md5=7e988a9d46329c4138f7b22e90e88837,"Modern SoCs integrate multiple CPU cores and hardware accelerators (HWAs) that share the same main memory system, causing interference among memory requests from different agents. The result of this interference, if it is not controlledwell, is missed deadlines for HWAs and low CPU performance. Fewprevious works have tackled this problem. State-of-the-art mechanisms designed for CPU-GPU systems strive to meet a target frame rate for GPUs by prioritizing the GPU close to the time when it has to complete a frame. We observe two major problems when such an approach is adapted to a heterogeneous CPU-HWA system. First, HWAs miss deadlines because they are prioritized only when close to their deadlines. Second, such an approach does not consider the diverse memory access characteristics of different applications running on CPUs and HWAs, leading to low performance for latency-sensitive CPU applications and deadline misses for some HWAs, including GPUs. In this article, we propose a Deadline-Awarememory Scheduler for Heterogeneous systems (DASH), which overcomes these problems using three key ideas, with the goal of meeting HWAs' deadlines while providing high CPU performance. First, DASH prioritizes an HWA when it is not on track to meet its deadline any time during a deadline period, instead of prioritizing it only when close to a deadline. Second, DASH prioritizes HWAs over memory-intensive CPU applications based on the observation that memory-intensive applications' performance is not sensitive to memory latency. Third, DASH treats short-deadline HWAs differently as they are more likely to miss their deadlines and schedules their requests based on worst-case memory access time estimates. Extensive evaluations across a wide variety of different workloads and systems show that DASH achieves significantly better CPU performance than the best previous scheduler while always meeting the deadlines for all HWAs, including GPUs, thereby largely improving frame rates. Copyright © 2015 ACM.",Deadline; GPUs; Hardware accelerators; Heterogeneous systems; Main memory; Memory controller; Multicore; Performance; Real-time systems; Scheduling; System-on-a-chip,Application specific integrated circuits; Computer hardware; Hardware; Interactive computer systems; Memory architecture; Program processors; Reconfigurable hardware; Scheduling; System-on-chip; Deadline; GPUs; Hardware accelerators; Heterogeneous systems; Main memory; Memory controller; Multi core; Performance; System on a chip; Real time systems
A filtering mechanism to reduce network bandwidth utilization of transaction execution,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954187913&doi=10.1145%2f2837028&partnerID=40&md5=b8068504e86ba0ba8b70dfac7c4f6485,"Hardware Transactional Memory (HTM) relies heavily on the on-chip network for intertransaction communication. However, the network bandwidth utilization of transactions has been largely neglected in HTM designs. In this work, we propose a cost model to analyze network bandwidth in transaction execution. The cost model identifies a set of key factors that can be optimized through system design to reduce the communication cost of HTM. Based on themodel and network traffic characterization of a representative HTM design, we identify a huge source of superfluous traffic due to failed requests in transaction conflicts. As observed in a spectrum of workloads, 39% of the transactional requests fail due to conflicts, which renders 58% of the transactional network traffic futile. To combat this pathology, a novel in-network filtering mechanism is proposed. The on-chip router is augmented to predict conflicts among transactions and proactively filter out those requests that have a high probability to fail. Experimental results show the proposed mechanism reduces total network traffic by 24% on average for a set of high-contention TM applications, thereby reducing energy consumption by an average of 24%. Meanwhile, the contention in the coherence directory is reduced by 68%, on average. These improvements are achieved with only 5% area added to a conventional on-chip router design. © 2015 ACM.",Communication cost modeling; Energy efficiency; Network traffic; On-chip network; Transactional memory,Costs; Design; Energy efficiency; Energy utilization; Storage allocation (computer); Communication cost; Hardware transactional memory; Network bandwidth utilization; Network traffic; Network traffic characterizations; On-chip networks; Reducing energy consumption; Transactional memory; Bandwidth
PARSECSs: Evaluating the impact of task parallelism in the PARSEC benchmark suite,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954189607&doi=10.1145%2f2829952&partnerID=40&md5=c1d00e2e716bb381a3eec58914a4c960,"In this work, we show how parallel applications can be implemented efficiently using task parallelism. We also evaluate the benefits of such parallel paradigm with respect to other approaches. We use the PARSEC benchmark suite as our test bed, which includes applications representative of a wide range of domains from HPC to desktop and server applications. We adopt different parallelization techniques, tailored to the needs of each application, to fully exploit the task-based model. Our evaluation shows that task parallelism achieves better performance than thread-based parallelization models, such as Pthreads. Our experimental results show that we can obtain scalability improvements up to 42% on a 16-core system and code size reductions up to 81%. Such reductions are achieved by removing from the source code application specific schedulers or thread pooling systems and transferring these responsibilities to the runtime system software. © 2015 ACM.",Concurrency; Parallel applications; Parallel architectures; Parallel benchmarks; Parallel runtime systems; Scalable applications; Synchronization; Task-based programming models,Application programs; Benchmarking; Parallel architectures; Synchronization; Concurrency; Parallel application; Parallel benchmarks; Programming models; Runtime systems; Computer systems programming
Automatic vectorization of interleaved data revisited,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954119867&doi=10.1145%2f2838735&partnerID=40&md5=7280abfaf3d4fcbd298c7d6f3278e333,"Automatically exploiting short vector instructions sets (SSE, AVX, NEON) is a critically important task for optimizing compilers. Vector instructions typically work best on data that is contiguous in memory, and operating on non-contiguous data requires additional work to gather and scatter the data. There are several varieties of non-contiguous access, including interleaved data access. An existing approach used by GCC generates extremely efficient code for loops with power-of-2 interleaving factors (strides). In this paper we propose a generalization of this approach that produces similar code for any compile-time constant interleaving factor. In addition, we propose several novel program transformations, which were made possible by our generalized representation of the problem. Experiments show that our approach achieves significant speedups for both power-of-2 and non-power-of-2 interleaving factors. Our vectorization approach results in mean speedups over scalar code of 1.77x on Intel SSE and 2.53x on Intel AVX2 in real-world benchmarking on a selection of BLAS Level 1 routines. On the same benchmark programs, GCC 5.0 achieves mean improvements of 1.43x on Intel SSE and 1.30x on Intel AVX2. In synthetic benchmarking on Intel SSE, our maximum improvement on data movement is over 4x for gathering operations and over 6x for scattering operations versus scalar code. © 2015 ACM.",Data; Interleaving; SIMD; Vectorization,Benchmarking; Automatic vectorization; Benchmark programs; Data; Interleaving; Optimizing compilers; Program transformations; SIMD; Vectorization; Codes (symbols)
The polyhedral model of nonlinear loops,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954113617&doi=10.1145%2f2838734&partnerID=40&md5=705a0a69d0cd081be922c54363c6062c,"Runtime code optimization and speculative execution are becoming increasingly prominent to leverage performance in the current multi- and many-core era. However, a wider and more efficient use of such techniques is mainly hampered by the prohibitive time overhead induced by centralized data race detection, dynamic code behavior modeling, and code generation. Most of the existing Thread Level Speculation (TLS) systems rely on naively slicing the target loops into chunks and trying to execute the chunks in parallel with the help of a centralized performance-penalizing verification module that takes care of data races. Due to the lack of a data dependence model, these speculative systems are not capable of doing advanced transformations, and, more importantly, the chances of rollback are high. The polyhedral model is a wellknown mathematical model to analyze and optimize loop nests. The current state-of-art tools limit the application of the polyhedral model to static control codes. Thus, none of these tools can generally handle codes withwhile loops, indirect memory accesses, or pointers. Apollo (Automatic POLyhedral Loop Optimizer) is a framework that goes one step beyond and applies the polyhedral model dynamically by using TLS. Apollo can predict, at runtime, whether the codes are behaving linearly or not, and it applies polyhedral transformations on-the-fly. This article presents a novel system that enables Apollo to handle codes whose memory accesses and loop bounds are not necessarily linear. More generally, this approach expands the applicability of the polyhedral model at runtime to a wider class of codes. Plugging together both linear and nonlinear accesses to the dependence prediction model enables the application of polyhedral loop optimizing transformations even for nonlinear code kernels while also allowing a low-cost speculation verification. Copyright © 2015 ACM.",Nonlinear memory references; Polyhedral model; Speculative and dynamic loop parallelization,Codes (symbols); Embedded systems; Linear transformations; Data race detection; Dependence prediction; Dynamic loops; Nonlinear memory; Polyhedral modeling; Polyhedral transformations; Speculative execution; Thread level speculation; Mathematical transformations
JavaScript parallelizing compiler for exploiting parallelism from data-parallel HTML5 applications,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954190307&doi=10.1145%2f2846098&partnerID=40&md5=8b1815cb3a6fcb7e758305b2aed51303,"With the advent of the HTML5 standard, JavaScript is increasingly processing computationally intensive, data-parallel workloads. Thus, the enhancement of JavaScript performance has been emphasized because the performance gap between JavaScript and native applications is still substantial. Despite this urgency, conventional JavaScript compilers do not exploit much of parallelism even from data-parallel JavaScript applications, despite contemporary mobile devices being equipped with expensive parallel hardware platforms, such as multicore processors and GPGPUs. In this article, we propose an automatically parallelizing JavaScript compiler that targets emerging, dataparallel HTML5 applications by leveraging the mature affine loop analysis of conventional static compilers. We identify that themost critical issues when parallelizing JavaScript with a conventional static analysis are ensuring correct parallelization, minimizing compilation overhead, and conducting low-cost recovery when there is a speculation failure during parallel execution. We propose a mechanism for safely handling the failure at a low cost, based on compiler techniques and the property of idempotence. Our experiment shows that the proposed JavaScript parallelizing compiler detects most affine parallel loops. Also, we achieved a maximum speedup of 3.22 times on a quad-core system, while incurring negligible compilation and recovery overheads with various sets of data-parallel HTML5 applications. Copyright © 2015 ACM.",HTML5; JavaScript; Javascript compilers; Javascript engines; Javascriptcore; JIT; Loop parallelization,Cost benefit analysis; Depreciation; High level languages; HTML; Mobile devices; Program processors; HTML5; Javascript; Javascriptcore; JIT; Loop parallelization; Program compilers
On how to accelerate iterative stencil loops: A scalable streaming-based approach,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954195321&doi=10.1145%2f2842615&partnerID=40&md5=8f82d46623d30f49eb53361122fc5826,"In high-performance systems, stencil computations play a crucial role as they appear in a variety of different fields of application, ranging from partial differential equation solving, to computer simulation of particles' interaction, to image processing and computer vision. The computationally intensive nature of those algorithms created the need for solutions to efficiently implement them in order to save both execution time and energy. This, in combination with their regular structure, has justified their widespread study and the proposal of largely different approaches to their optimization. However, most of these works are focused on aggressive compile time optimization, cache locality optimization, and parallelism extraction for the multicore/multiprocessor domain, while fewer works are focused on the exploitation of custom architectures to further exploit the regular structure of Iterative Stencil Loops (ISLs), specifically with the goal of improving power efficiency. This work introduces amethodology to systematically design power-efficient hardware accelerators for the optimal execution of ISL algorithms on Field-programmable Gate Arrays (FPGAs). As part of the methodology, we introduce the notion of Streaming Stencil Time-step (SST), a streaming-based architecture capable of achieving both low resource usage and efficient data reuse thanks to an optimal data buffering strategy, and we introduce a technique called SSTs queuing that is capable of delivering a pseudolinear execution time speedup with constant bandwidth. The methodology has been validated on significant benchmarks on a Virtex-7 FPGA using the Xilinx Vivado suite. Results demonstrate how the efficient usage of the on-chipmemory resources realized by an SST allows one to treat problem sizes whose implementation would otherwise not be possible via direct synthesis of the original, unmanipulated code via High-Level Synthesis (HLS). We also show how the SSTs queuing effectively ensures a pseudolinear throughput speedup while consuming constant off-chip bandwidth. © 2015 ACM.",FPGAs; Power efficiency; Streaming architectures,Bandwidth; Computer vision; High level synthesis; Image processing; Iterative methods; Optimization; Reconfigurable hardware; Hardware accelerators; High performance systems; Image processing and computer vision; Iterative stencil loops; Parallelism extraction; Power efficiency; Stencil computations; Streaming architecture; Field programmable gate arrays (FPGA)
Adaptive correction of sampling bias in dynamic call graphs,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954098060&doi=10.1145%2f2840806&partnerID=40&md5=0f5f333ad5497fff4fd73d66b5942609,"This article introduces a practical low-overhead adaptive technique of correcting sampling bias in profiling dynamic call graphs. Timer-based sampling keeps the overhead low but sampling bias lowers the accuracy when either observable call events or sampling actions are not equally spaced in time. To mitigate sampling bias, our adaptive correction techniqueweights each sample bymonitoring time-varying spacing of call events and sampling actions. We implemented and evaluated our adaptive correction technique in Jikes RVM, a high-performance virtual machine. In our empirical evaluation, our technique significantly improved the sampling accuracy without measurable overhead and resulted in effective feedback directed inlining. Copyright © 2015 ACM.",Correction; Dynamic call graph; Sampling,Electric power factor correction; Hardware; Sampling; Software engineering; Adaptive corrections; Adaptive technique; Call graphs; Empirical evaluations; Feedback directed; Sampling accuracies; Sampling bias; Virtual machines; XML
Sensible energy accounting with abstract metering for multicore systems,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954178841&doi=10.1145%2f2842616&partnerID=40&md5=e010b85b486caf812f5fc936ca0f6e38,"Chip multicore processors (CMPs) are the preferred processing platform across different domains such as data centers, real-time systems, and mobile devices. In all those domains, energy is arguably the most expensive resource in a computing system. Accurately quantifying energy usage in a multicore environment presents a challenge as well as an opportunity for optimization. Standard metering approaches are not capable of delivering consistent results with shared resources, since the same task with the same inputs may have different energy consumption based on the mix of co-running tasks. However, it is reasonable for data-center operators to charge on the basis of estimated energy usage rather than time since energy is more correlated with their actual cost. This article introduces the concept of Sensible Energy Accounting (SEA). For a task running in a multicore system, SEA accurately estimates the energy the task would have consumed running in isolation with a given fraction of the CMP shared resources. We explain the potential benefits of SEA in different domains and describe two hardware techniques to implement it for a shared last-level cache and on-core resources in SMT processors. Moreover, with SEA, an energy-aware scheduler can find a highly efficient on-chip resource assignment, reducing by up to 39% the total processor energy for a 4-core system. Copyright © 2015 ACM.",Chip multiprocessors; Energy accounting; Modeling and estimation; Power modeling; Resource allocation; Simultaneous multithreaded,Abstracting; Distributed computer systems; Energy utilization; Interactive computer systems; Microprocessor chips; Mobile devices; Power management; Resource allocation; Chip multicore processors; Chip Multiprocessor; Energy accounting; Multicore environments; Multithreaded; Power model; Resource assignment; Total processor energy; Real time systems
GP-SIMD processing-in-memory,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921291648&doi=10.1145%2f2686875&partnerID=40&md5=b5c02d03ca36e517f510a1778122951c,"GP-SIMD, a novel hybrid general-purpose SIMD computer architecture, resolves the issue of data synchronization by in-memory computing through combining data storageand massively parallel processing. GP-SIMD employs a two-dimensional access memory with modified SRAM storage cells and a bit-serial processing unit per each memory row. An analytic performance model of the GP-SIMD architecture is presented, comparing it to associative processor and to conventional SIMD architectures. Cycle-accurate simulation of four workloads supports the analytical comparison. Assuming a moderate die area, GP-SIMD architecture outperforms both the associative processor and conventional SIMD coprocessor architectures by almost an order of magnitude while consuming less power. © 2014 ACM.",Associative processor; Multicore; PIM; Processing in memory; SIMD,Associative processing; Digital storage; Static random access storage; Associative processor; Multi core; PIM; Processing in memory; SIMD; Computer architecture
Improving multibank memory access parallelism with lattice-based partitioning,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921317470&doi=10.1145%2f2675359&partnerID=40&md5=d5bf031b309a26624c2d3cd580ebde29,"Emerging architectures, such as reconfigurable hardware platforms, provide the unprecedented opportunity of customizing the memory infrastructure based on application access patterns. This work addresses the problem of automated memory partitioning for such architectures, taking into account potentially parallel data accesses to physically independent banks. Targeted at affine static control parts (SCoPs), the technique relies on the Z-polyhedral model for program analysis and adopts a partitioning scheme based on integer lattices. The approach enables the definition of a solution space including previous works as particular cases. The problem of minimizing the total amount of memory required across the partitioned banks, referred to as storage minimization throughout the article, is tackled by an optimal approach yielding asymptotically zero memory waste or, as an alternative, an efficient approach ensuring arbitrarily small waste. The article also presents a prototype toolchain and a detailed step-by-step case study demonstrating the impact of the proposed technique along with extensive comparisons with alternative approaches in the literature. © 2015 ACM.",Field-programmable gate arrays; Fine-grained distributed shared memory; Memory partitioning; Polyhedral model,Digital storage; Field programmable gate arrays (FPGA); Integer programming; Reconfigurable architectures; Reconfigurable hardware; Distributed shared memory; Emerging architectures; Memory Partitioning; Multi-bank memory; Optimal approaches; Parallel data access; Polyhedral modeling; Reconfigurable hardware platform; Memory architecture
Low-power high-efficiency video decoding using general-purpose processors,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921288076&doi=10.1145%2f2685551&partnerID=40&md5=d80c008d912dedfda8883b45e020dcc2,"In this article, we investigate how code optimization techniques and low-power states of general-purpose processors improve the power efficiency of HEVC decoding. The power and performance efficiency of the use of SIMD instructions, multicore architectures, and low-power active and idle states are analyzed in detail for offline video decoding. In addition, the power efficiency of techniques such as ""race to idle"" and ""exploiting slack"" with DVFS are evaluated for real-time video decoding. Results show that ""exploiting slack"" is more power efficient than ""race to idle"" for all evaluated platforms representing smartphone, tablet, laptop, and desktop computing systems. © 2014 ACM.",Low-power computing; Parallel processing; Video decoding,Decoding; Efficiency; Embedded systems; General purpose computers; Software architecture; Code optimization technique; General purpose processors; Low-power computing; Multicore architectures; Parallel processing; Performance efficiency; SIMD instructions; Video decoding; Video signal processing
Managing mismatches in voltage stacking with CoreUnfolding,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954120417&doi=10.1145%2f2835178&partnerID=40&md5=0f076a59522ce52b00d268f51ff05364,"Five percent to 25% of power could bewasted before it is delivered to the computational resources on a die, due to inefficiencies of voltage regulators and resistive loss. The power delivery could benefit if, at the same power, the delivered voltage increases and the current decreases. This article presents CoreUnfolding, a technique that leverages voltage Stacking to improve power delivery efficiency. Our experiments show that about 10% system-wide power can be saved, the voltage regulator area can be reduced by 30%, di/dt improves 49%, and the power pin count is reduced by 40% (≊20% reduction in packaging costs), with negligible performance degradation. Copyright © 2015 ACM.",Microarchitecture; Power delivery network; Voltage stacking,Voltage regulators; Computational resources; Micro architectures; Packaging costs; Performance degradation; Power delivery efficiencies; Power delivery network; Resistive loss; Voltage increase; Electric power transmission
Falcon: A graph manipulation language for heterogeneous systems,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954163280&doi=10.1145%2f2842618&partnerID=40&md5=e1b7c6552debcc191644981aa3ced4f5,"Graph algorithms have been shown to possess enough parallelism to keep several computing resources Busy - even hundreds of cores on a GPU. Unfortunately, tuning their implementation for efficient execution on a particular hardware configuration of heterogeneous systems consisting of multicore CPUs and GPUs is challenging, time consuming, and error prone. To address these issues,we propose a domain-specific language (DSL), Falcon, for implementing graph algorithms that (i) abstracts the hardware, (ii) provides constructs to write explicitly parallel programs at a higher level, and (iii) can work with general algorithms that may change the graph structure (morph algorithms). We illustrate the usage of our DSL to implement local computation algorithms (that do not change the graph structure) and morph algorithms such as Delaunay mesh refinement, survey propagation, and dynamic SSSP on GPU and multicore CPUs. Using a set of benchmark graphs, we illustrate that the generated code performs close to the state-of-the-art hand-tuned implementations. © 2015 ACM.",CUDA; Domain specific languages; GPU; Graph manipulation languages; Local computation algorithms; Morph algorithms; Multi-core CPU; OpenMP,Application programming interfaces (API); Computational linguistics; Computer hardware; Computer programming languages; Graphic methods; Graphical user interfaces; Hardware; Problem oriented languages; Program processors; Reconfigurable hardware; CUDA; Domain specific languages; GPU; Graph manipulation; Local computation; Multi-core cpus; OpenMP; Algorithms
A compile-time optimization method for WCET reduction in real-time embedded systems through block formation,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954307068&doi=10.1145%2f2845083&partnerID=40&md5=f0bb3bab081a1c69d23e93c2239335df,"Compile-time optimizations play an important role in the efficient design of real-time embedded systems. Usually, compile-time optimizations are designed to reduce average-case execution time (ACET). While ACET is a main concern in high-performance computing systems, in real-time embedded systems, concerns are different and worst-case execution time (WCET) is much more important than ACET. Therefore, WCET reduction is more desirable than ACET reduction in many real-time embedded systems. In this article, we propose a compile-time optimization method aimed at reducing WCET in real-time embedded systems. In the proposed method, based on the predicated execution capability of embedded processors, program code blocks that are in the worst-case paths of the program are merged to increase instruction-level parallelism and opportunity for WCET reduction. The use of predicated execution enables merging code blocks from different worst-case paths that can be very effective inWCET reduction. The experimental results show that the proposed method can reduce WCET by up to 45% as compared to previous compile-time block formation methods. It is noteworthy that compared to previous works, while the proposed method usually achieves more WCET reduction, it has considerably less negative impact on ACET and code size. © 2015 ACM.",Compile-time optimization; Hyperblock; WCET,Codes (symbols); Program processors; Real time systems; Compile time; High performance computing systems; Hyperblock; Instruction level parallelism; Predicated execution; Real-time embedded systems; WCET; Worst-case execution time; Embedded systems
Rethinking memory permissions for protection against cross-layer attacks,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954212815&doi=10.1145%2f2842621&partnerID=40&md5=55b5612772a41598cdb1aad47be421ef,"The inclusive permissions structure (e.g., the Intel ring model) of modern commodity CPUs provides privileged system software layers with arbitrary permissions to access andmodify client processes, allowing them to manage these clients and the system resources efficiently. Unfortunately, these inclusive permissions allow a compromised high-privileged software layer to perform arbitrary malicious activities. In this article, our goal is to prevent attacks that cross system layers while maintaining the abilities of system software to manage the system and allocate resources. In particular, we present a hardware-supported page permission framework for physical pages that is based on the concept of noninclusive sets of memory permissions for different layers of system software (such as hypervisors, operating systems, and user-level applications). Instead of viewing privilege levels as an ordered hierarchy with each successive level being more privileged, we view them as distinct levels each with its own set of permissions. In order to enable system software to manage client processes, we define a set of legal permission transitions that support resource allocation but preserve security. We show that the model prevents a range of recent attacks. We also show that it can be implemented with negligible performance overhead (both at load time and at runtime), low hardware complexity, and minimal changes to the commodity OS and hypervisor code. © 2015 ACM.",Architecture; Security,Architecture; Computer operating systems; Computer software; Hardware; Program processors; Reconfigurable hardware; Cross layer; Cross systems; Different layers; Hardware complexity; Malicious activities; Security; System resources; System softwares; Application programs
Scalable energy efficiency with resilience for high performance computing systems: A quantitative methodology,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954206149&doi=10.1145%2f2822893&partnerID=40&md5=0c6e621a88fed35b718602883462432c,"Ever-growing performance of supercomputers nowadays brings demanding requirements of energy efficiency and resilience, due to rapidly expanding size and duration in use of the large-scale computing systems. Many application/architecture-dependent parameters that determine energy efficiency and resilience individually have causal effects with each other, which directly affect the trade-offs among performance, energy efficiency and resilience at scale. To enable high-efficiency management for large-scale High-Performance Computing (HPC) systems nowadays, quantitatively understanding the entangled effects among performance, energy efficiency, and resilience is thus required. While previous work focuses on exploring energysaving and resilience-enhancing opportunities separately, little has been done to theoretically and empirically investigate the interplay between energy efficiency and resilience at scale. In this article, by extending the Amdahl's Law and the Karp-Flatt Metric, taking resilience into consideration, we quantitatively model the integrated energy efficiency in terms of performance perWatt and showcase the trade-offs among typical HPC parameters, such as number of cores, frequency/voltage, and failure rates. Experimental results for a wide spectrum of HPC benchmarks on two HPC systems show that the proposed models are accurate in extrapolating resilience-aware performance and energy efficiency, and capable of capturing the interplay among various energy-saving and resilience factors. Moreover, the models can help find the optimal HPC configuration for the highest integrated energy efficiency, in the presence of failures and applied resilience techniques. © 2015 ACM.",Checkpoint and restart; DVFS; Energy; Failures; HPC; Performance; Power; Resilience; Scalability; Undervolting,Benchmarking; Commerce; Economic and social effects; Energy conservation; Failure (mechanical); Scalability; Supercomputers; Checkpoint-and-restart; DVFS; Energy; HPC; Performance; Power; Resilience; Undervolting; Energy efficiency
FluidCheck: A redundant threading-based approach for reliable execution in manycore processors,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954315984&doi=10.1145%2f2842620&partnerID=40&md5=935989a6c5b104c7557ab23328969c1b,"Soft errors have become a serious cause of concern with reducing feature sizes. The ability to accommodate complex, Simultaneous Multithreading (SMT) cores on a single chip presents a unique opportunity to achieve reliable execution, safe from soft errors, with low performance penalties. In this context, we present FluidCheck, a checker architecture that allows highly flexible assignment and migration of checking duties across cores. In this article, we present a mechanism to dynamically use the resources of SMT cores for checking the results of other threads, and propose a variety of heuristics for migration of such checker threads across cores. Secondly, to make the process of checking more efficient, we propose a set of architectural enhancements that reduce power consumption, decrease the length of the critical path, and reduce the load on the Network-on-Chip (NoC). Based on our observations, we design a 16 core system for running SPEC2006 based bag-of-tasks applications. Our experiments demonstrate that fully reliable execution can be attained with a mere 27% slowdown, surpassing traditional redundant threading based techniques by roughly 42%. Copyright © 2015 ACM.",Checker architectures; Redundant multithreading; Reliability,Complex networks; Distributed computer systems; Energy efficiency; Fault tolerant computer systems; Microprocessor chips; Multitasking; Network architecture; Program processors; Radiation hardening; Reconfigurable hardware; Reliability; VLSI circuits; Architectural enhancement; Bag-of-Tasks applications; Many-core processors; Network-on-chip(NoC); Performance penalties; Redundant multithreading; Reliable execution; Simultaneous multi-threading; Network-on-chip
Two-level hybrid sampled simulation of multithreaded applications,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954167515&doi=10.1145%2f2818353&partnerID=40&md5=cec7d79541b02696fe4bfdfe6299b991,"Sampled microarchitectural simulation of single-threaded applications is mature technology for over a decade now. Sampling multithreaded applications, on the other hand, is much more complicated. Not until very recently have researchers proposed solutions for sampled simulation of multithreaded applications. Time-Based Sampling (TBS) samples multithreaded application execution based on time-not instructions as is typically done for single-threaded applications-yielding estimates for a multithreaded application's execution time. In this article, we revisit and analyze previously proposed TBS approaches (periodic and cantor fractal based sampling), and we obtain a number of novel and surprising insights, such as (i) accurately estimating fast-forwarding IPC, that is, performance in-between sampling units, is more important than accurately estimating sample IPC, that is, performance within the sampling units; (ii) fast-forwarding IPC estimation accuracy is determined by both the sampling unit distribution and how to use the sampling units to predict fast-forwarding IPC; and (iii) cantor sampling is more accurate at small sampling unit sizes, whereas periodic is more accurate at large sampling unit sizes. These insights lead to the development of Two-levelHybrid Sampling (THS), a novel samplingmethodology formultithreaded applications that combines periodic sampling's accuracy at large time scales (i.e., uniformly selecting coarse-grain sampling units across the entire program execution) with cantor sampling's accuracy at small time scales (i.e., the ability to accurately predict fast-forwarding IPC in-between small sampling units). The clustered occurrence of small sampling units under cantor sampling also enables shortened warmup and thus enhanced simulation speed. Overall, THS achieves an average absolute execution time prediction error of 4% while yielding an average simulation speedup of 40× compared to detailed simulation, which is both more accurate and faster than the current state-of-the-art. Case studies illustrate THS' ability to accurately predict relative performance differences across the design space. © 2015 ACM.",Microarchitecture simulation; Multicore processor; Multithreaded workloads; Sampling,Application programs; Forecasting; Microprocessor chips; Multicore programming; Program processors; Sampling; Execution time predictions; Micro architectures; Microarchitectural simulation; Multi-core processor; Multi-threaded application; Multithreaded; Relative performance; Time-based samplings; Importance sampling
Fence placement for legacy data-race-free programs via synchronization read detection,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954223670&doi=10.1145%2f2835179&partnerID=40&md5=a9386785f8429ce1be1314a8d7b9efa4,"Shared-memory programmers traditionally assumed Sequential Consistency (SC), butmodern systems have relaxedmemory consistency. Here, the trend in languages is towardData-Race-Free (DRF) models, where, assuming annotated synchronizations and the program being well-synchronized by those synchronizations, the hardware and compiler guarantee SC. However, legacy programs lack annotations, so even well-synchronized (legacy DRF) programs aren't recognized. For legacy DRF programs, we can significantly prune the set of memory orderings determined by automated fence placement by automatically identifying synchronization reads. We prove our rules for identifying them conservatively, implement them within LLVM, and observe a 30% average performance improvement over previous techniques. © 2015 ACM.",Fence placement; Relaxed memory models,Fences; Synchronization; Legacy data; Memory ordering; Relaxed memory models; Sequential consistency; Shared memory; Program compilers
Four metrics to evaluate heterogeneous multicores,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954209029&doi=10.1145%2f2829950&partnerID=40&md5=50dc1c3757d326d510199918a9b9858d,"Semiconductor device scaling has made single-ISA heterogeneous processors a reality. Heterogeneous processors contain a number of different CPU cores that all implement the same Instruction Set Architecture (ISA). This enables greater flexibility and specialization, as runtime constraints and workload characteristics can influence which core a given workload is run on. A major roadblock to the further development of heterogeneous processors is the lack of appropriate evaluation metrics. Existing metrics can be used to evaluate individual cores, but to evaluate a heterogeneous processor, the cores must be considered as a collective. Without appropriate metrics, it is impossible to establish design goals for processors, and it is difficult to accurately compare two different heterogeneous processors. We present four new metrics to evaluate user-oriented aspects of sets of heterogeneous cores: localized nonuniformity, gap overhead, set overhead, and generality. The metrics consider sets rather than individual cores. We use examples to demonstrate each metric, and show that the metrics can be used to quantify intuitions about heterogeneous cores. © 2015 ACM.",Effective speed; Gap overhead; Generality; Localized nonuniformity; Set overhead; Single-ISA,Semiconductor devices; Gap overhead; Generality; Nonuniformity; Set overhead; Single-ISA; Computer architecture
SPCM: The striped phase change memory,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954311993&doi=10.1145%2f2829951&partnerID=40&md5=e5ca25525fdb3687bea332cfa294d895,"Phase Change Memory (PCM) devices are one of the known promising technologies to take the place of DRAM devices with the aim of overcoming the obstacles of reducing feature size and stopping ever growing amounts of leakage power. In exchange for providing high capacity, high density, and nonvolatility, PCM Multilevel Cells (MLCs) impose high write energy and long latency. Many techniques have been proposed to resolve these side effects. However, read performance issues are usually left behind the great importance of write latency, energy, and lifetime. In this article, we focus on read performance and improve the critical path latency of the main memory system. To this end, we exploit striping scheme by which multiple lines are grouped and lie on a single MLC line array. In order to achieve more performance gain, an adaptive ordering mechanism is used to sort lines in a group based on their read frequency. This scheme imposes large energy and lifetime overheads due to its intensive demand for higher write bandwidth. Thus, we equipped our design with a grouping/pairing write queue to synchronize write-back requests such that all updates to an MLC array occur at once. The design is also augmented by a directional write scheme that takes benefits of the uniformity of accesses to the PCM device-caused by the large DRAM cache-to determine the writing mode (striped or nonstriped). This adaptation to write operations relaxes the energy and lifetime overheads. We improve the read latency of a 2-bit MLC PCM memory by more than 24% (and Instructions Per Cycle (IPC) by about 9%) and energy-delay product by about 20% for a small lifetime degradation of 8%, on average. Copyright © 2015 ACM.",Line striping; Multilevel cell memory; Phase change memory,Dynamic random access storage; Integrated circuit design; Energy delay product; Instructions per cycles; Lifetime degradation; Line striping; Multilevel cell; Phase change memory (pcm); Read performance; Write operations; Phase change memory
MINIME-GPU: Multicore benchmark synthesizer for GPUs,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954107474&doi=10.1145%2f2818693&partnerID=40&md5=9a2eae234bd7f4f65fd4ce746b2ecfe1,"We introduce MINIME-GPU, a novel automated benchmark synthesis framework for graphics processing units (GPUs) that serves to speed up architectural simulation of modern GPU architectures. Our framework captures important characteristics of original GPU applications and generates synthetic GPU benchmarks using the Open Computing Language (OpenCL) library from those applications. To the best of our knowledge, this is the first time synthetic OpenCL benchmarks for GPUs are generated from existing applications. We use several characteristics, including instruction throughput, compute unit occupancy, and memory efficiency, to compare the similarity of original applications and their corresponding synthetic benchmarks. The experimental results show that our synthetic benchmark generation framework is capable of generating synthetic benchmarks that have similar characteristics with the original applications from which they are generated. On average, the similarity (accuracy) is 96% and the speedup is 541×. In addition, our synthetic benchmarks use the OpenCL library, which allows us to obtain portable human readable benchmarks as opposed to using assembly-level code, and they are faster and smaller than the original applications from which they are generated. We experimentally validated that our synthetic benchmarks preserve the characteristics of the original applications across different architectures. Copyright © 2015 ACM.",Benchmark characterization; Benchmark generation; GPU; OpenCL; Synthetic benchmarks,Computer graphics; Program processors; Architectural simulation; Assembly levels; GPU; Graphics processing units; Human-readable; Memory efficiency; OpenCL; Synthetic benchmark; Benchmarking
Integrated mapping and synthesis techniques for network-on-chip topologies with express channels,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954241382&doi=10.1145%2f2831233&partnerID=40&md5=56c4f7014e254f763b69e364de1ae04d,"The addition of express channels to a traditional mesh network-on-chip (NoC) has emerged as a viable solution to solve the problem of high latency. In this article, we address the problem of integrated mapping and synthesis for express channel-based mesh NoC topologies. An integer linear programming-based formulation has been presented for the mapping problem followed by a constructive heuristic for simultaneous application mapping and synthesis for an express channel-based NoC. The static and dynamic simulation results indicate that the obtained mappings lead to significant reduction in both average packet delay and network energy consumption. The obtained synthesized topologies were also found to be much more power efficient compared to conventional express channel topologies. © 2015 ACM.",Constructive heuristic; Integer linear programming,Energy utilization; Heuristic programming; Integer programming; Mapping; Mesh generation; Problem solving; Servers; Topology; VLSI circuits; Application mapping; Average packet delays; Channel topology; Constructive heuristic; Integer Linear Programming; Network energy consumption; Network-on-chip(NoC); Synthesis techniques; Network-on-chip
Tumbler: An effective load-balancing technique for multi-CPU multicore systems,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954205523&doi=10.1145%2f2827698&partnerID=40&md5=a048ae3d7dc16ebc83a99f4be56c15bf,"Schedulers used by modern OSs (e.g., Oracle Solaris 11TM and GNU/Linux) balance load by balancing the number of threads in run queues of different cores. While this approach is effective for a single CPU multicore system, we show that it can lead to a significant load imbalance across CPUs of a multi-CPU multicore system. Because different threads of a multithreaded application often exhibit different levels of CPU utilization, load cannot be measured in terms of the number of threads alone. We propose Tumbler that migrates the threads of a multithreaded program across multiple CPUs to balance the load across the CPUs. While Tumbler distributes the threads equally across the CPUs, its assignment of threads to CPUs is aimed at minimizing the variation in utilization of different CPUs to achieve load balance. We evaluated Tumbler using a wide variety of 35 multithreaded applications, and our experimental results show that Tumbler outperforms both Oracle Solaris 11TM and GNU/Linux. © 2015 ACM.",Load balancing; Multicore; Operating systems,Computer operating systems; Distributed computer systems; Network management; Open source software; Resource allocation; CPU utilization; Load balancing technique; Load imbalance; Multi core; Multi-core systems; Multi-threaded application; Multi-threaded programs; Number of threads; Program processors
"FaultSim: A fast, configurable memory-reliability simulator for conventional and 3d-stacked systems",2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954229605&doi=10.1145%2f2831234&partnerID=40&md5=ebd5720d518eb2acc66bc7530eb4b02f,"As memory systems scale, maintaining their Reliability Availability and Serviceability (RAS) is becoming more complex. To make matters worse, recent studies of DRAM failures in data centers and supercomputer environments have highlighted that large-granularity failures are common in DRAM chips. Furthermore, themove toward 3D-stacked memories canmake the system vulnerable to newer failure modes, such as those occurring from faults in Through-Silicon Vias (TSVs). To architect future systems and to use emerging technology, system designers will need to employ strong error correction and repair techniques. Unfortunately, evaluating the relative effectiveness of these reliability mechanisms is often difficult and is traditionally done with analytical models, which are both error prone and time-consuming to develop. To this end, this article proposes FAULTSIM, a fast configurable memory-reliability simulation tool for 2D and 3D-stacked memory systems. FaultSim employs Monte Carlo simulations, which are driven by real-world failure statistics. We discuss the novel algorithms and data structures used in FaultSim to accelerate the evaluation of different resilience schemes. We implement BCH-1 (SECDED) and ChipKill codes using FaultSim and validate against an analytical model. FaultSim implements BCH-1 and ChipKill codes with a deviation of only 0.032% and 8.41% from the analytical model. FaultSim can simulate 1 million Monte Carlo trials (each for a period of 7 years) of BCH-1 and ChipKill codes in only 34 seconds and 33 seconds, respectively. © 2015 ACM.",Error correcting codes; Monte carlo simulation; Reliability; Stacked memory; Through silicon vias,Algorithms; Analytical models; Codes (symbols); Dynamic random access storage; Electronics packaging; Error correction; Errors; Integrated circuit interconnects; Intelligent systems; Monte Carlo methods; Reliability; Supercomputers; Availability and serviceability; Emerging technologies; Error correcting code; Failure statistics; Memory reliability; Monte Carlo trials; Stacked memory; Through silicon vias; Three dimensional integrated circuits
Enabling PGAS productivity with hardware support for shared address mapping: A UPC case study,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954145143&doi=10.1145%2f2842686&partnerID=40&md5=b45afe16dc4882ca134d3a4ba5529fd6,"Due to its rich memory model, the partitioned global address space (PGAS) parallel programming model strikes a balance between locality-awareness and the ease of use of the global address space model. Although locality-awareness can lead to high performance, supporting the PGAS memory model is associated with penalties that can hinder PGAS's potential for scalability and speed of execution. This is becausemapping the PGAS memory model to the underlying system requires a mapping process that is done in software, thereby introducing substantial overhead for shared accesses even when they are local. Compiler optimizations have not been sufficient to offset this overhead. On the other hand, manual code optimizations can help, but this eliminates the productivity edge of PGAS. This article proposes a processor microarchitecture extension that can perform such address mapping in hardware with nearly no performance overhead. These extensions are then availed to compilers through extensions to the processor instructions. Thus, the need for manual optimizations is eliminated and the productivity of PGAS languages is unleashed. Using Unified Parallel C (UPC), a PGAS language, we present a case study of a prototype compiler and architecture support. Two different implementations of the system were realized. The first uses a full-system simulator, gem5, which evaluates the overall performance gain of the new hardware support. The second uses an FPGA Leon3 soft-core processor to verify implementation feasibility and to parameterize the cost of the new hardware. The new instructions show promising results on all tested codes, including the NAS Parallel Benchmark kernels in UPC. Performance improvements of up to 5.5× for unmodified codes, sometimes surpassing handoptimized performance, were demonstrated. We also show that our four-core FPGA prototype requires less than 2.4% of the overall chip's area. © 2015 ACM.",Address translation; Hardware support; PGAS; Productivity; UPC,Codes (symbols); Computational linguistics; Distributed computer systems; Field programmable gate arrays (FPGA); Hardware; Mapping; MATLAB; Microprocessor chips; Parallel processing systems; Parallel programming; Productivity; Program compilers; Reconfigurable hardware; Address translation; Compiler optimizations; Hardware supports; NAS parallel benchmarks; Parallel programming model; Partitioned Global Address Space; PGAS; UPC; C (programming language)
Symmetry-agnostic coordinated management of the memory hierarchy in multicore systems,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954324136&doi=10.1145%2f2847254&partnerID=40&md5=3e2c1440937d7fff8c19f3cca214a9d9,"In a multicore system, many applications share the last-level cache (LLC) and memory bandwidth. These resources need to be carefully managed in a coordinated way to maximize performance. DRAM is still the technology of choice in most systems. However, as traditional DRAM technology faces energy, reliability, and scalability challenges, nonvolatile memory (NVM) technologies are gaining traction. While DRAM is read/write symmetric (a read operation has comparable latency and energy consumption as a write operation), many NVM technologies (such as Phase-Change Memory, PCM) experience read/write asymmetry: write operations are typically much slower and more power hungry than read operations. Whether the memory's characteristics are symmetric or asymmetric influences the way shared resources are managed. We propose two symmetry-agnostic schemes to manage a shared LLC through way partitioning and memory through bandwidth allocation. The proposals work well for both symmetric and asymmetric memory. First, an exhaustive search is proposed to find the best combination of a cache way partition and bandwidth allocation. Second, an approximate scheme, derived from a theoretical model, is proposed without the overhead of exhaustive search. Simulation results show that the approximate scheme improves weighted speedup by at least 14% on average (regardless of the memory symmetry) over a state-of-the-art way partitioning and memory bandwidth allocation. Simulation results also show that the approximate scheme achieves comparable weighted speedup as a state-of-the-art multiple resource management scheme, XChange, for symmetric memory, and outperforms it by an average of 10% for asymmetric memory. Copyright © 2015 ACM.",Cache partitioning; Memory bandwidth partitioning; Nonvolatile memory; Phase change memory; Shared resource,Bandwidth; Dynamic random access storage; Energy utilization; Nonvolatile storage; Phase change memory; Cache partitioning; Lastlevel caches (LLC); Memory bandwidths; Multi-core systems; Non-volatile memory; Non-volatile memory technology; Shared resources; Theoretical modeling; Cache memory
Reuse distance-based probabilistic cache replacement,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946572419&doi=10.1145%2f2818374&partnerID=40&md5=c833703d5095b035bb201e4aac04efcc,"This article proposes Probabilistic Replacement Policy (PRP), a novel replacement policy that evicts the line with minimum estimated hit probability under optimal replacement instead of the line with maximum expected reuse distance. The latter is optimal under the independent reference model of programs, which does not hold for last-level caches (LLC). PRP requires 7% and 2% metadata overheads in the cache and DRAM respectively. Using a sampling scheme makes DRAM overhead negligible, with minimal performance impact. Including detailed overhead modeling and equal cache areas, PRP outperforms SHiP, a state-of-the-art LLC replacement algorithm, by 4% for memory-intensive SPEC-CPU2006 benchmarks. © 2015 ACM.",Cache replacement policy; Probabilistic replacement; Reuse distance,Dynamic random access storage; Cache replacement policy; Independent reference models; Lastlevel caches (LLC); Performance impact; Probabilistic replacement; Replacement algorithm; Replacement policy; Reuse distance; Cache memory
Buri: Scaling big-memory computing with hardware-based memory expansion,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946118701&doi=10.1145%2f2808233&partnerID=40&md5=cc058525fc2208e6139a2cd855dbb98e,"Motivated by the challenges of scaling up memory capacity and fully exploiting the benefits of memory compression, we propose Buri, a hardware-based memory compression scheme, which simultaneously achieves cost efficiency, high performance, and ease of adoption. Buri combines (1) a self-contained, ready-to-adopt hardware compression module, which manages metadata compression and memory allocation/relocation operations; (2) a set of hardware optimization mechanisms, which reduce the area and performance overheads in accommodating the address indirection required by memory compression; and (3) lightweight BIOS/OS extensions used to handle exceptions. Our evaluation with large memory workload traces shows that Buri can increase capacity by 70%, in addition to the compression ratio already provided by database software. © 2015 ACM 1544-3566/2015/10-ART31 $15.00.",Big data; Compression; Memory; Performance; Scalability,Big data; Compaction; Data storage equipment; Hardware; Scalability; Compression modules; Cost efficiency; Database software; Hardware optimization; Memory capacity; Memory compression; Performance; Scaling-up; Data compression
Spatiotemporal SIMT and scalarization for improving GPU efficiency,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941337826&doi=10.1145%2f2811402&partnerID=40&md5=ade36d3f43e5ce4cc930c3f344126689,"Temporal SIMT (TSIMT) has been suggested as an alternative to conventional (spatial) SIMT for improving GPU performance on branch-intensive code. Although TSIMT has been briefly mentioned before, it was not evaluated. We present a complete design and evaluation of TSIMT GPUs, along with the inclusion of scalarization and a combination of temporal and spatial SIMT, named Spatiotemporal SIMT (STSIMT). Simulations show that TSIMT alone results in a performance reduction, but a combination of scalarization and STSIMT yields a mean performance enhancement of 19.6% and improves the energy-delay product by 26.2% compared to SIMT. © 2015 ACM.",Branch divergence; GPUS; Scalarization; Temporal SIMT,Hardware; Software engineering; Branch divergence; Design and evaluations; Energy delay product; GPUS; Performance enhancements; Scalarization; Temporal and spatial; Temporal SIMT; Program processors
The effects of granularity and adaptivity on private/shared classification for coherence,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940883461&doi=10.1145%2f2790301&partnerID=40&md5=33855ae4e984813df0a46318d2fce6cb,"Classification of data into private and shared has proven to be a catalyst for techniques to reduce coherence cost, since private data can be taken out of coherence and resources can be concentrated on providing coherence for shared data. In this article, we examine how granularity - page-level versus cache-line level - and adaptivity - going from shared to private - affect the outcome of classification and its final impact on coherence. We create a classification technique, called Generational Classification, and a coherence protocol called Generational Coherence, which treats data as private or shared based on cache-line generations. We compare two coherence protocols based on self-invalidation/self-downgrade with respect to data classification. Our findings are enlightening: (i) Some programs benefit from finer granularity, some benefit further from adaptivity, but some do not benefit from either. (ii) Reducing the amount of shared data has no perceptible impact on coherence misses caused by self-invalidation of shared data, hence no impact on performance. (iii) In contrast, classifying more data as private has implications for protocols that employ write-through as a means of self-downgrade, resulting in network traffic reduction - up to 30% - by reducing write-through traffic. © 2015 ACM.",C.1.2. [Processor Architectures]: Multiple Data Stream Architectures (Multiprocessors); Cache coherence; Design; Memory hierarchy; Multicore; Performance,Cache memory; Classification (of information); Data streams; Design; Memory architecture; Multicore programming; Network architecture; Cache Coherence; Memory hierarchy; Multi core; Multiple data stream architectures (multiprocessors); Performance; Data Sharing
DPCS: Dynamic power/capacity scaling for SRAM caches in the nanoscale era,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940841263&doi=10.1145%2f2792982&partnerID=40&md5=6f8866e067844fee6176b4e19d888db2,"Fault-Tolerant Voltage-Scalable (FTVS) SRAM cache architectures are a promising approach to improve energy efficiency of memories in the presence of nanoscale process variation. Complex FTVS schemes are commonly proposed to achieve very low minimum supply voltages, but these can suffer from high overheads and thus do not always offer the best power/capacity trade-offs. We observe on our 45nm test chips that the ""fault inclusion property"" can enable lightweight fault maps that support multiple runtime supply voltages. Based on this observation, we propose a simple and low-overhead FTVS cache architecture for power/capacity scaling. Our mechanism combines multilevel voltage scaling with optional architectural support for power gating of blocks as they become faulty at low voltages. A static (SPCS) policy sets the runtime cache VDD once such that a only a few cache blocks may be faulty in order to minimize the impact on performance. We describe a Static Power/Capacity Scaling (SPCS) policy and two alternate Dynamic Power/Capacity Scaling (DPCS) policies that opportunistically reduce the cache voltage even further for more energy savings. This architecture achieves lower static power for all effective cache capacities than a recent more complex FTVS scheme. This is due to significantly lower overheads, despite the inability of our approach to match the min-VDD of the competing work at a fixed target yield. Over a set of SPEC CPU2006 benchmarks on two system configurations, the average total cache (system) energy saved by SPCS is 62% (22%), while the two DPCS policies achieve roughly similar energy reduction, around 79% (26%). On average, the DPCS approaches incur 2.24% performance and 6% area penalties. © 2015 ACM.","B.3.1 [memory structures]: Design styles- Cache memories; B.3.2 [memory structures]: Semiconductor memories- Static memory (sram); B.8.1 [performance and reliability]: Reliability, testing, and fault tolerance; Block disable; Design; Energy proportionality; Fault-tolerant voltage scaling; Low power; Nanoscale technology; Performance; Process variation; Reliability; Resizable cache; Variability-aware",Design; Economic and social effects; Energy conservation; Energy efficiency; Fault tolerance; Memory architecture; Nanotechnology; Reliability; Semiconductor storage; Static random access storage; Voltage scaling; Block disable; Design styles; Energy proportionalities; Fault-tolerant; Low Power; Nanoscale technologies; Performance; Performance and reliabilities; Process Variation; Resizable cache; Semiconductor memory; Variability-Aware; Cache memory
Revisiting clustered microarchitecture for future superscalar cores: A case for wide issue clusters,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940851256&doi=10.1145%2f2800787&partnerID=40&md5=749fdf2db709642e478ac507106860db,"During the past 10 years, the clock frequency of high-end superscalar processors has not increased. Performance keeps growing mainly by integrating more cores on the same chip and by introducing new instruction set extensions. However, this benefits only some applications and requires rewriting and/or recompiling these applications. A more general way to accelerate applications is to increase the IPC, the number of instructions executed per cycle. Although the focus of academic microarchitecture research moved away from IPC techniques, the IPC of commercial processors was continuously improved during these years. We argue that some of the benefits of technology scaling should be used to raise the IPC of future superscalar cores further. Starting from microarchitecture parameters similar to recent commercial high-end cores, we show that an effective way to increase the IPC is to allow the out-of-order engine to issue more micro-ops per cycle. But this must be done without impacting the clock cycle. We propose combining two techniques: clustering and register write specialization. Past research on clustered microarchitectures focused on narrow issue clusters, as the emphasis at that time was on allowing high clock frequencies. Instead, in this study, we consider wide issue clusters, with the goal of increasing the IPC under a constant clock frequency. We show that on a wide issue dual cluster, a very simple steering policy that sends 64 consecutive instructions to the same cluster, the next 64 instructions to the other cluster, and so forth, permits tolerating an intercluster delay of three cycles. We also propose a method for decreasing the energy cost of sending results from one cluster to the other cluster. © 2015 ACM.",Clustered microarchitecture; Instruction-level parallelism; Steering policy; Superscalar core,Clocks; Computer architecture; Clock frequency; Clustered microarchitectures; Instruction level parallelism; Instruction set extension; Micro architectures; Superscalar; Superscalar Processor; Technology scaling; Program processors
Leveraging transactional execution for memory consistency model emulation,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940841005&doi=10.1145%2f2786980&partnerID=40&md5=9e1797dd090f6e83a7fde83bec609948,"System emulation is widely used in today's computer systems. This technology opens new opportunities for resource sharing as well as enhancing system security and reliability. System emulation across different instruction set architectures (ISA) can enable further opportunities. For example, cross-ISA emulation can enable workload consolidation over a wide range of microprocessors and potentially facilitate the seamless deployment of new processor architectures. As multicore and manycore processors become pervasive, it is important to address the challenges toward supporting system emulation on these platforms. A key challenge in cross-ISA emulation on multicore systems is ensuring the correctness of emulation when the guest and the host memory consistency models differ. Many existing cross-ISA system emulators are sequential, thus they are able to avoid this problem at the cost of significant performance degradation. Recently proposed parallel emulators are able to address the performance limitation; however, they provide limited support for memory consistency model emulation. When the host system has a weaker memory consistency model compared to the guest system, the emulator can insert memory fences at appropriate locations in the translated code to enforce the guest memory ordering constraints. These memory fences can significantly degrade the performance of the translated code. Transactional execution support available on certain recent microprocessors provides an alternative approach. Transactional execution of the translated code enforces sequential consistency (SC) at the coarse-grained transaction level, which in turn ensures that all memory accesses made on the host machine conform to SC. Enforcing SC on the host machine guarantees that the emulated execution will be correct for any guest memory model. In this article, we compare and evaluate the overheads associated with using transactions and fences for memory consistency model emulation on the Intel Haswell processor. Our experience of implementing these two approaches on a state-of-the-art parallel emulator, COREMU, demonstrates that memory consistency model emulation using transactions performs better when the transaction sizes are large enough to amortize the transaction overhead and the transaction conflict rate is low, whereas inserting memory fences is better for applications in which the transaction overhead is high. A hybrid implementation that dynamically determines which approach to invoke can outperform both approaches. Our results, based on the SPLASH-2 and the PARSEC benchmark suites, demonstrate that the proposed hybrid approach is able to outperform the fence insertion mechanism by 4.9% and the transactional execution approach by 24.9% for two-thread applications, and outperform them by 4.5% and 44.7%, respectively, for four-threaded execution. © 2015 ACM.",Memory consistency models; Parallel emulators; Transactional memory,Benchmarking; Fences; Network security; Parallel processing systems; Instruction set architecture; Memory consistency models; Multicore and manycore processors; Parallel emulators; Performance degradation; Performance limitations; Processor architectures; Transactional memory; Memory architecture
CAFFEINE: A utility-driven prefetcher aggressiveness engine for multicores,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940854378&doi=10.1145%2f2806891&partnerID=40&md5=e3b170ecc2987f6c7ac60d4af8d93115,"Aggressive prefetching improves system performance by hiding and tolerating off-chip memory latency. However, on a multicore system, prefetchers of different cores contend for shared resources and aggressive prefetching can degrade the overall system performance. The role of a prefetcher aggressiveness engine is to select appropriate aggressiveness levels for each prefetcher such that shared resource contention caused by prefetchers is reduced, thereby improving system performance. State-of-the-art prefetcher aggressiveness engines monitor metrics such as prefetch accuracy, bandwidth consumption, and last-level cache pollution. They use carefully tuned thresholds for these metrics, and when the thresholds are crossed, they trigger aggressiveness control measures. These engines have three major shortcomings: (1) thresholds are dependent on the system configuration (cache size, DRAM scheduling policy, and cache replacement policy) and have to be tuned appropriately, (2) there is no single threshold that works well across all the workloads, and (3) thresholds are oblivious to the phase change of applications. To overcome these shortcomings, we propose CAFFEINE, a model-based approach that analyzes the effectiveness of a prefetcher and uses a metric called net utility to control the aggressiveness. Our metric provides net processor cycles saved because of prefetching by approximating the cycles saved across the memory subsystem, from last-level cache to DRAM. We evaluate CAFFEINE across a wide range of workloads and compare it with the state-of-the-art prefetcher aggressiveness engine. Experimental results demonstrate that, on average (geomean), CAFFEINE achieves 9.5% (as much as 38.29%) and 11% (as much as 20.7%) better performance than the best-performing aggressiveness engine for four-core and eight-core systems, respectively. © 2015 ACM.",Cache pollution; Intercore interference; Memory systems; Multicore; Prefetching,Caffeine; Dynamic random access storage; Engines; Pollution; Bandwidth consumption; Cache pollution; Cache replacement policy; Memory systems; Multi core; Prefetching; Shared resource contentions; System configurations; Cache memory
Snippets: Taking the high road to a low level,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937035688&doi=10.1145%2f2764907&partnerID=40&md5=747b383484c87d80c113df59e2f7fad5,"When building a compiler for a high-level language, certain intrinsic features of the language must be expressed in terms of the resulting low-level operations. Complex features are often expressed by explicitly weaving together bits of low-level IR, a process that is tedious, error prone, difficult to read, difficult to reason about, and machine dependent. In the Graal compiler for Java, we take a different approach: we use snippets of Java code to express semantics in a high-level, architecture-independent way. Two important restrictions make snippets feasible in practice: they are compiler specific, and they are explicitly prepared and specialized. Snippets make Graal simpler and more portable while still capable of generating machine code that can compete with other compilers of the Java HotSpot VM.",Compiler; Dynamic compilation; Graal; Java; Just-in-time compilation; Snippet,High level languages; Java programming language; Semantics; Compiler; Dynamic compilation; Graal; Java; Just-in-time compilation; Snippet; Program compilers
SECRET: A selective error correction framework for refresh energy reduction in DRAMs,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937032888&doi=10.1145%2f2747876&partnerID=40&md5=322e6a072f0a8271ff249a542b445caa,"DRAMs are used as the main memory in most computing systems today. Studies show that DRAMs contribute to a significant part of overall system power consumption. One of the main challenges in low-power DRAM design is the inevitable refresh process. Due to process variation, memory cells exhibit retention time variations. Current DRAMs use a single refresh period determined by the cell with the largest leakage. Since prolonging refresh intervals introduces retention errors, a set of previous works adopt conventional error-correcting code (ECC) to correct retention errors. However, these approaches introduce significant area and energy overheads. In this article, we propose a novel error correction framework for retention errors in DRAMs, called SECRET (selective error correction for refresh energy reduction). The key observations we make are that retention errors are hard errors rather than soft errors, and only few DRAM cells have large leakage. Therefore, instead of equipping error correction capability for all memory cells as existing ECC schemes, we only allocate error correction information to leaky cells under a refresh interval. Our SECRET framework contains two parts: an offline phase to identify memory cells with retention errors given a target error rate and a low-overhead error correction mechanism. The experimental results show that among all test cases performed, the proposed SECRET framework can reduce refresh power by 87.2% and overall DRAM power up to 18.57% with negligible area and performance overheads. © 2015 ACM.",DRAM-based main memory; Error control codes; Refresh power reduction; Retention errors,Cells; Dynamic random access storage; Error correction; Radiation hardening; Semiconductor storage; Correction mechanism; Error control code; Error correcting code; Error correction capability; Main memory; Power reductions; Process Variation; Target error rate; Errors
Intercepting functions for memoization: A case study using transcendental functions,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937041351&doi=10.1145%2f2751559&partnerID=40&md5=5cfd40bb02db4f2b4ffc31c1cd7e5f94,"Memoization is the technique of saving the results of executions so that future executions can be omitted when the input set repeats. Memoization has been proposed in previous literature at the instruction, basic block, and function levels using hardware, as well as pure software-level approaches including changes to programming language. In this article, we focus on software memoization for procedural languages such as C and Fortran at the granularity of a function. We propose a simple linker-based technique for enabling software memoization of any dynamically linked pure function by function interception and illustrate our framework using a set of computationally expensive pure functions - the transcendental functions. Transcendental functions are those that cannot be expressed in terms of a finite sequence of algebraic operations (trigonometric functions, exponential functions, etc.) and hence are computationally expensive. Our technique does not need the availability of source code and thus can even be applied to commercial applications, as well as applications with legacy codes. As far as users are concerned, enabling memoization is as simple as setting an environment variable. Our framework does not make any specific assumptions about the underlying architecture or compiler toolchains and can work with a variety of current architectures. We present experimental results for a x86-64 platform using both gcc and icc compiler toolchains, and an ARM Cortex-A9 platform using gcc. Our experiments include a mix of real-world programs and standard benchmark suites: SPEC and Splash2x. On standard benchmark applications that extensively call the transcendental functions, we report memoization benefits of up to 50% on Intel Ivy Bridge and up to 10% on ARM Cortex-A9. Memoization was able to regain a performance loss of 76% in bwaves due to a known performance bug in the GNU implementation of the pow function. The same benchmark on ARM Cortex-A9 benefited by more than 200%. © 2015 ACM.",Function interception; Link time optimization,ARM processors; Benchmarking; C (programming language); Computational linguistics; Open source software; Program compilers; Algebraic operations; Benchmark applications; Commercial applications; Link-time optimization; Procedural languages; Real world projects; Transcendental functions; Trigonometric functions; Exponential functions
Section-based program analysis to reduce overhead of detecting unsynchronized thread communication,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937036390&doi=10.1145%2f2766451&partnerID=40&md5=eb3c9faa91ee8cf29a87d6c7bb059f6e,"Most systems that test and verify parallel programs, such as deterministic execution engines, data race detectors, and software transactional memory systems, require instrumenting loads and stores in an application. This can cause a very significant runtime and memory overhead compared to executing uninstrumented code. Multithreaded programming typically allows any thread to perform loads and stores to any location in the process's address space independently, and such tools monitor all these memory accesses. However, many of the addresses in these unsynchronized memory accesses are only used by a single thread and do not affect other executing threads. We propose Section-Based Program Analysis (SBPA), a novel way to decompose the program into disjoint code sections to identify and eliminate instrumenting such loads and stores during program compilation so that the program runtime overhead is significantly reduced. Our analysis includes improvements to pointer analysis and uses a few user directives to increase the effectiveness of SBPA further. We implemented SBPA foradeterministic execution runtime environment and were abletoeliminate 51% ofdynamic memory access instrumentations. When combined with directives, such reduction increased to 63%. We also integrated SBPA with ThreadSanitizer, a state-of-the-art dynamic race detector, and achieved a speedup of 2.43 (2.74 with directives) on a geometric mean basis. © 2015 ACM.",Compilers; Multiprocessing/multiprogramming/multitasking; Parallel processing; Program analysis; Runtime environments; Threads,Cost reduction; Java programming language; Multiprocessing programs; Multitasking; Program compilers; Software testing; Multiprocessing/multiprogramming/multitasking; Parallel processing; Program analysis; Runtime environments; Threads; Application programs
Contech: Efficiently generating dynamic task graphs for arbitrary parallel programs,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937033418&doi=10.1145%2f2776893&partnerID=40&md5=ff6f2778a527748cc8b82d4f4be4f64e,"Parallel programs can be characterized by task graphs encoding instructions, memory accesses, and the parallel work's dependencies, while representing any threading library and architecture. This article presents Contech, a high performance framework for generating dynamic task graphs from arbitrary parallel programs, and a novel representation enabling programmers and compiler optimizations to understand and exploit program aspects. The Contech framework supports a variety of languages (including C, C++, and Fortran), parallelization libraries, and ISAs (including ×86 and ARM). Running natively for collection speed and minimizing program perturbation, the instrumentation shows 4× improvement over a Pin-based implementation on PARSEC and NAS benchmarks. © 2015 ACM.",Instrumentation; Parallel program modeling; Task graph,ARM processors; C++ (programming language); Parallel programming; Compiler optimizations; Instrumentation; Memory access; NAS benchmarks; Parallel program; Parallelizations; Performance frameworks; Task graph; Program compilers
EECache: A comprehensive study on the architectural design for energy-efficient last-level caches in chip multiprocessors,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937045161&doi=10.1145%2f2756552&partnerID=40&md5=52edea0b85902f7ee40822a9dda26279,"Power management for large last-level caches (LLCs) is important in chip multiprocessors (CMPs), as the leakage power of LLCs accounts for a significant fraction of the limited on-chip power budget. Since not all workloads running on CMPs need the entire cache, portions of a large, shared LLC can be disabled to save energy. In this article, we explore different design choices, from circuit-level cache organization to microarchitectural management policies, topropose a low-overhead runtime mechanism for energy reduction in the large, shared LLC. We first introduce a slice-based cache organization that can shut down parts of the shared LLC with minimal circuit overhead. Based on this slice-based organization, part of the shared LLC can be turned off according to the spatial and temporal cache access behavior captured by low-overhead sampling-based hardware. In order to eliminate the performance penalties caused by flushing data before powering off a cache slice, we propose data migration policies to prevent the loss of useful data in the LLC. Results show that our energy-efficient cache design (EECache) provides 14.1% energy savings at only 1.2% performance degradation and consumes negligible hardware overhead compared to prior work. © 2015 ACM.",Cache; Energy efficiency; Power management,Budget control; Energy conservation; Energy management; Hardware; Microprocessor chips; Multiprocessing systems; Power management; Cache; Cache organization; Energy-efficient caches; Hardware overheads; Last-level caches; Management policy; Performance degradation; Performance penalties; Energy efficiency
Locality-aware work stealing based on online profiling and auto-tuning for multisocket multicore architectures,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937031972&doi=10.1145%2f2766450&partnerID=40&md5=5562d3e487a44f98ec9633ae6d152852,"Modern mainstream powerful computers adopt multisocket multicore CPU architecture and NUMA-based memory architecture. While traditional work-stealing schedulers are designed for single-socket architectures, they incur severe shared cache misses and remote memory accesses in these computers. To solve the problem, we propose a locality-aware work-stealing (LAWS) scheduler, which better utilizes both the shared cache and the memory system. In LAWS, a load-balanced task allocator is used to evenly split and store the dataset of a program to all the memory nodes and allocate a task to the socket where the local memory node stores its data for reducing remote memory accesses. Then, an adaptive DAG packer adopts an auto-tuning approach to optimally pack an execution DAG into cache-friendly subtrees. After cache-friendly subtrees are created, every socket executes cache-friendly subtrees sequentially for optimizing shared cache usage. Meanwhile, a triple-level work-stealing scheduler is applied to schedule the subtrees and the tasks in each subtree. Through theoretical analysis, we show that LAWS has comparable time and space bounds compared with traditional work-stealing schedulers. Experimental results show that LAWS can improve the performance of memory-bound programs up to 54.2% on AMD-based experimental platforms and up to 48.6% on Intel-based experimental platforms compared with traditional work-stealing schedulers. © 2015 ACM.",History-based auto-tuning; Memory subsystem; Task scheduling,Computer architecture; Memory architecture; Scheduling; Software architecture; Autotuning; Experimental platform; Memory subsystems; Multi-core cpu architectures; Multicore architectures; Online profiling; Remote memory access; Task-scheduling; Cache memory
Enabling GPGPU low-level hardware explorations with MIAOW: An open-source RTL implementation of a GPGPU,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937032823&doi=10.1145%2f2764908&partnerID=40&md5=c5c2b4bb3f8c51fb971f7ca079d5fe71,"Graphic processing unit (GPU)-based general-purpose computing is developing as a viable alternative to CPU-based computing in many domains. Today's tools for GPU analysis include simulators like GPGPU-Sim, Multi2Sim, and Barra. While useful for modeling first-order effects, these tools do not provide a detailed view of GPU microarchitecture and physical design. Further, as GPGPU research evolves, design ideas and modifications demand detailed estimates of impact on overall area and power. Fueled by this need, we introduce MIAOW (Many-core Integrated Accelerator Of Wisconsin), an open-source RTL implementation of the AMD Southern Islands GPGPU ISA, capable of running unmodified OpenCL-based applications. We present our design motivated by our goals to create a realistic, flexible, OpenCL-compatible GPGPU, capable of emulating a full system. We first explore if MIAOW is realistic and then use four case studies to show that MIAOW enables the following: physical design perspective to ""traditional"" microarchitecture, new types of research exploration, and validation/calibration of simulator-based characterization of hardware. The findings and ideas are contributions in their own right, in addition to MIAOW's utility as a tool for others' research. © 2015 ACM.",GPU; Manycore; SIMD,Computer architecture; Design; Hardware; Program processors; First order effect; General-purpose computing; GPU; Graphic processing unit(GPU); Many-core; Micro architectures; Physical design; SIMD; Open systems
Aging-aware compilation for GP-GPUs,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937030351&doi=10.1145%2f2778984&partnerID=40&md5=7909ed92df1dfb8e19bff6cceb63abec,"General-purpose graphic processing units (GP-GPUs) offer high computational throughput using thousands of integrated processing elements (PEs). These PEs are stressed during workload execution, and negative bias temperature instability (NBTI) adversely affects their reliability by introducing new delay-induced faults. However, the effect of these delay variations is not uniformly spread across the PEs: some are affected more - hence less reliable - than others. This variation causes significant reductionin the lifetime of GP-GPU parts. In this article, we address the problem of ""wear leveling"" across processing units to mitigate lifetime uncertainty in GP-GPUs. We propose innovations in the static compiled code that can improve healing in PEs and stream cores (SCs) based on their degradation status. PE healing is a fine-grained very long instruction word (VLIW) slot assignment scheme that balances the stress of instructions across the PEs within an SC. SC healing is a coarse-grained workload allocation scheme that distributes workload across SCs in GP-GPUs. Both schemes share a common property: they adaptively shift workload from less reliable units to more reliable units, either spatially or temporally. These software schemes are based on online calibration with NBTI monitoring that equalizes the expected lifetime of PEs and SCs by regenerating adaptive compiled codes to respond to the specific health state of the GP-GPUs. We evaluate the effectiveness of the proposed schemes for various OpenCL kernels from the AMD APP SDK on Evergreen and Southern Island GPU architectures. The aging-aware healthy kernels generated by the PE (or SC) healing scheme reduce NBTI-induced voltage threshold shift by 30% (77% in the case of SCs), with no (moderate) performance penalty compared to the naive kernels. © 2015 ACM.",Adaptive kernel; Aging-aware compilation; GP-GPUs; NBTI; VLIW,Integrated circuits; Program processors; Adaptive kernels; General purpose graphic processing units; GP-GPUs; NBTI; Negative bias temperature instability; Performance penalties; Very long instruction words; VLIW; Very long instruction word architecture
Reliable integrity checking in multicore processors,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930176372&doi=10.1145%2f2738052&partnerID=40&md5=39ab11fee840d08f51c8383a3076d526,"Security and reliability have become important concerns in the design of computer systems. On one hand, microarchitectural enhancements for security (such as for dynamic integrity checking of code at runtime) have been proposed. On the other hand, independently, microarchitectural enhancements for reliability to detect and tolerate natural faults have also been proposed. A fault in these security enhancements due to alpha particles or aging might potentially pass off maliciously modified instructions as safe, rendering the security enhancements useless. Deliberate fault attacks by attackers can be launched to disable the security enhancements and then launch the well-known security attacks that would otherwise have been detected by these enhancements. We report an integrated microarchitecture support for security and reliability in multicore processors. Specifically, we add integrity checkers to protect the code running on the multiple cores in a multicore processor. We then adapt these checkers to check one another periodically to ensure reliable operation. These checkers naturally can check the other parts of the core. The average performance, power, and area costs for these security-reliability enhancements are 6.42%, 0.73%, and 0.53%, respectively. © 2015 ACM 1544-3566/2015/05-ART10 15.00.",Hardware security; Microarchitecture support for security and reliability,Computer architecture; Hardware security; Reliability; Dynamic Integrity Checking; Integrity checking; Microarchitecture supports; Multi-core processor; Reliability enhancement; Reliable operation; Security and reliabilities; Security enhancements; Network security
Dynamic shared SPM reuse for real-time multicore embedded systems,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930176454&doi=10.1145%2f2738051&partnerID=40&md5=3ebbd764a26b231b71f8a1b0734d0a50,"Allocating the scratchpad memory (SPM) space to tasks is a challenging problem in real-time multicore embedded systems that use shared SPM. Proper SPM space allocation is important, as it considerably influences the application worst-case execution time (WCET), which is of great importance in real-time applications. To address this problem, in this article we present a dynamic SPM reuse scheme, where SPM space can be reused by other tasks during runtime without requiring any static SPM partitioning. Although the proposed scheme is applied dynamically at runtime, the required decision making is fairly complex and hence cannot be performed at runtime. We have developed techniques to perform the decision making offline at design time in the form of optimization problems combined with task scheduling/mapping. The proposed work is unlike previous works that either exploit static schemes for SPM space allocation or perform task scheduling/mapping and SPM space allocation incoherently. The experimental results show that our dynamic SPM reuse scheme can reduce WCET by up to 55% as compared to recent previous works on SPM allocation in real-time multicore embedded systems. © 2015 ACM 1544-3566/2015/05-ART12 15.00.",Embedded real-time systems; Multicore processors; Scheduling; Scratchpad memory; Shared memory,Decision making; Embedded systems; Interactive computer systems; Memory architecture; Multitasking; Optimization; Scheduling; Scheduling algorithms; Embedded real time systems; Multi-core embedded systems; Multi-core processor; Optimization problems; Real-time application; Scratch pad memory; Shared memory; Worst-case execution time; Real time systems
An optimizing code generator for a class of Lattice-Boltzmann computations,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930932956&doi=10.1145%2f2739047&partnerID=40&md5=7554cf2c18bba1f27bd30df81e2727e4,"The Lattice-Boltzmann method (LBM), a promising new particle-based simulation technique for complex and multiscale fluid flows, has seen tremendous adoption in recent years in computational fluid dynamics. Even with a state-of-the-art LBM solver such as Palabos, a user has to still manually write the program using library-supplied primitives. We propose an automated code generator for a class of LBM computations with the objective to achieve high performance on modern architectures. Few studies have looked at time tiling for LBM codes. We exploit a key similarity between stencils and LBM to enable polyhedral optimizations and in turn time tiling for LBM. We also characterize the performance of LBM with the Roofline performance model. Experimental results for standard LBM simulations like Lid Driven Cavity, Flow Past Cylinder, and Poiseuille Flow show that our scheme consistently outperforms Palabos - on average by up to 3× while running on 16 cores of an Intel Xeon (Sandybridge). We also obtain an improvement of 2.47× on the SPEC LBM benchmark. © 2015, Association for Computing Machinery. All rights reserved.",Lattice-Boltzmann method; Performance modeling; Polyhedral framework; Time tiling,Codes (symbols); Computational fluid dynamics; Kinetic theory; Network components; Two phase flow; Lattice Boltzmann method; Lattice boltzmann methods (LBM); Modern architectures; Particle-based simulation; Performance Model; Polyhedral framework; Polyhedral optimizations; Time tiling; Flow of fluids
A joint SW/HW approach for reducing register file vulnerability,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930168474&doi=10.1145%2f2733378&partnerID=40&md5=8dc0f5e643459febd54232f63f4419fc,"The Register File (RF) is a particularly vulnerable component within processor core and at the same time a hotspot with high power density. To reduce RF vulnerability, conventional HW-only approaches such as Error Correction Codes (ECCs) or modular redundancies are not suitable due to their significant power overhead. Conversely, SW-only approaches either have limited improvement on RF reliability or require considerable performance overhead. As a result, new approaches are needed that reduce RF vulnerability with minimal power and performance overhead. This article introduces Application-guided Reliability-enhanced Register file Architecture (ARRA), a novel approach to reduce RF vulnerability of embedded processors. Taking advantage of uneven register utilization, ARRA mirrors, guided by a SW instrumentation, frequently used active registers into passive registers. ARRA is particularly suitable for control applications, as they have a high reliability demand with fairly low (uneven) RF utilization. ARRA is a cross-layer joint HW/SW approach based on an ARRA-extended RF microarchitecture, an ISA extension, as well as static binary analysis and instrumentation. We evaluate ARRA benefits using an ARRA-enhanced Blackfin processor executing a set of DSPBench and MiBench benchmarks. We quantify the benefits using RF Vulnerability Factor (RFVF) and Mean Work To Failure (MWTF). ARRA significantly reduces RFVF from 35% to 6.9% in cost of 0.5% performance lost for control applications. With ARRA's register mirroring, it can also correct Multiple Bit Upsets (MBUs) errors, achieving an 8x increase in MWTF. Compared to a partially ECC-protected RF approach, ARRA demonstrates higher efficiency by achieving comparable vulnerability reduction at much lower power consumption. © 2015 ACM 1544-3566/2015/05-ART9 15.00.",Binary instrumentation; Register File (RF); Register mirroring; Vulnerability Factor,Bins; Computer architecture; Energy efficiency; Error correction; Program processors; Reliability; Binary instrumentations; Error correction codes (ECCs); Lower-power consumption; Register files; Register mirroring; Static binary analysis; Vulnerability factors; Vulnerability reductions; Cost reduction
A new memory-disk integrated system with HW optimizer,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930162062&doi=10.1145%2f2738053&partnerID=40&md5=bb21bfd6410d4a5671b236d7e93e73ce,"Current high-performance computer systems utilize a memory hierarchy of on-chip cache, main memory, and secondary storage due to differences in device characteristics. Limiting the amount of main memory causes page swap operations and duplicates data between the main memory and the storage device. The characteristics of next-generation memory, such as nonvolatility, byte addressability, and scaling to greater capacity, can be used to solve these problems. Simple replacement of secondary storage with new forms of nonvolatile memory in a traditional memory hierarchy still causes typical problems, such as memory bottleneck, page swaps, and write overhead. Thus, we suggest a single architecture that merges the main memory and secondary storage into a system called a Memory-Disk Integrated System (MDIS). The MDIS architecture is composed of a virtually decoupled NVRAM and a nonvolatile memory performance optimizer combining hardware and software to support this system. The virtually decoupled NVRAM module can support conventional main memory and disk storage operations logically without data duplication and can reduce write operations to the NVRAM. To increase the lifetime and optimize the performance of this NVRAM, another hardware module called a Nonvolatile Performance Optimizer (NVPO) is used that is composed of four small buffers. The NVPO exploits spatial and temporal characteristics of static/dynamic data based on program execution characteristics. Enhanced virtual memory management and address translation modules in the operating system can support these hardware components to achieve a seamless memory-storage environment. Our experimental results show that the proposed architecture can improve execution time by about 89% over a conventional DRAM main memory/HDD storage system, and 77% over a state-of-the-art PRAM main memory/HDD disk system with DRAM buffer. Also, the lifetime of the virtually decoupled NVRAM is estimated to be 40% longer than that of a traditional hierarchy based on the same device technology. © 2015 ACM 1544-3566/2015/05-ART11 15.00.",Emerging technologies; Mass storage; Memory control and access; Virtual memory,Access control; Buffer storage; Computer architecture; Computer hardware; Digital storage; Dynamic random access storage; Hardware; Integrated control; Memory architecture; Nonvolatile storage; Virtual addresses; Virtual storage; Emerging technologies; High performance computers; Mass storage; Memory controls; Proposed architectures; Temporal characteristics; Virtual memory; Virtual memory management; Cache memory
Practical iterative optimization for the data center,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930151761&doi=10.1145%2f2739048&partnerID=40&md5=ad9a1373c6cf9f6121e5b45b91b94cf1,"Iterative optimization is a simple but powerful approach that searches the best possible combination of compiler optimizations for a given workload. However, iterative optimization is plagued by several practical issues that prevent it from being widely used in practice: a large number of runs are required to find the best combination, the optimum combination is dataset dependent, and the exploration process incurs significant overhead that needs to be compensated for by performance benefits. Therefore, although iterative optimization has been shown to have a significant performance potential, it seldom is used in production compilers. In this article, we propose iterative optimization for the data center (IODC): we show that the data center offers a context in which all of the preceding hurdles can be overcome. The basic idea is to spawn different combinations across workers and recollect performance statistics at the master, which then evolves to the optimum combination of compiler optimizations. IODC carefully manages costs and benefits, and it is transparent to the end user. To bring IODC to practice, we evaluate it in the presence of co-runners to better reflect real-life data center operation with multiple applications co-running per server. We enhance IODC with the capability to find compatible co-runners along with a mechanism to dynamically adjust the level of aggressiveness to improve its robustness in the presence of co-running applications. We evaluate IODC using both MapReduce and compute-intensive throughput server applications. To reflect the large number of users interacting with the system, we gather a very large collection of datasets (up to hundreds of millions of unique datasets per program), for a total storage of 16.4TB and 850 days of CPU time. We report an average performance improvement of 1.48× and up to 2.08× for five MapReduce applications, and 1.12× and up to 1.39× for nine server applications. Furthermore, our experiments demonstrate that IODC is effective in the presence of co-runners, improving performance by greater than 13% compared to the worst possible co-runner schedule.",Co-run; Compiler; Data center; Iterative optimization; MapReduce; Server,Large dataset; Program compilers; Servers; Compiler; Compiler optimizations; Data centers; Improving performance; Iterative Optimization; Map-reduce; Performance potentials; Performance statistics; Digital storage
GPU performance and power tuning using regression trees,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930165583&doi=10.1145%2f2736287&partnerID=40&md5=e771f2ea375e9c93278803f842449717,"GPU performance and power tuning is difficult, requiring extensive user expertise and time-consuming trial and error. To accelerate design tuning, statistical design space exploration methods have been proposed. This article presents Starchart, a novel design space partitioning tool that uses regression trees to approach GPU tuning problems. Improving on prior work, Starchart offers more automation in identifying key design trade-offs and models design subspaces with distinctly different behaviors. Starchart achieves good model accuracy using very few random samples: less than 0.3% of a given design space; iterative sampling can more quickly target subspaces of interest. © 2015 ACM 1544-3566/2015/05-ART13 15.00.",Decision tree; Design space exploration; GPGPU; Statistical modeling,Decision trees; Economic and social effects; Iterative methods; Program processors; Design space exploration; Design tradeoff; GPGPU; Regression trees; Statistical design; Statistical modeling; Target subspaces; Tuning problems; Design
Buddy SM: Sharing pipeline front-end for improved energy efficiency in GPGPUs,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930159438&doi=10.1145%2f2744202&partnerID=40&md5=069796a27320a01341e65c85d4825592,"A modern general-purpose graphics processing unit (GPGPU) usually consists of multiple streaming multiprocessors (SMs), each having a pipeline that incorporatesagroupof threads executingacommon instruction flow. Although SMs are designed to work independently, we observe that they tend to exhibit very similar behavior for many workloads. If multiple SMs can be grouped and work in the lock-step manner, it is possible to save energy by sharing the front-end units among multiple SMs, including the instruction fetch, decode, and schedule components. However, such sharing brings architectural challenges and sometime causes performance degradation. In this article, we show our design, implementation, and evaluation for such an architecture, which we call Buddy SM. Specifically, multiple SMs can be opportunistically grouped into a buddy cluster. One SM becomes the master, and the rest become the slaves. The front-end unit of the master works actively for itself as well as for the slaves, whereas the front-end logics of the slaves are power gated. For efficient flow control and program correctness, the proposed architecture can identify unfavorable conditions and ungroup the buddy cluster when necessary. We analyze various techniques to improve the performance and energy efficiency of Buddy SM. Detailed experiments manifest that 37.2% front-end and 7.5% total GPU energy reduction can be achieved. © 2015 ACM 1544-3566/2015/05-ART16 15.00.",Energy efficiency; Front-end; GPU,Cluster computing; Computer graphics; Computer graphics equipment; Graphics processing unit; Pipelines; Program processors; Energy reduction; Front end; General purpose graphics processing unit (GPGPU); Instruction fetch; Performance degradation; Program correctness; Proposed architectures; Streaming multiprocessors; Energy efficiency
HRF-relaxed: Adapting HRF to the complexities of industrial heterogeneous memory models,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929177859&doi=10.1145%2f2701618&partnerID=40&md5=33b002e9096ca5001720097ae74a300a,"Memory consistency models, or memory models, allow both programmers and program language imple-menters to reason about concurrent accesses to one or more memory locations. Memory model specifications balance the often conflicting needs for precise semantics, implementation flexibility, and ease of understanding. Toward that end, popular programming languages like Java, C, and C++ have adopted memory models built on the conceptual foundation of Sequential Consistency for Data-Race-Free programs (SC for DRF). These SC for DRF languages were created with general-purpose homogeneous CPU systems in mind, and all assume a single, global memory address space. Such a uniform address space is usually power and performance prohibitive in heterogeneous Systems on Chips (SoCs), and for that reason most heterogeneous languages have adopted split address spaces and operations with nonglobal visibility. There have recently been two attempts to bridge the disconnect between the CPU-centric assumptions of the SC for DRF framework and the realities of heterogeneous SoC architectures. Hower et al. proposed a class of Heterogeneous-Race-Free (HRF) memory models that provide a foundation for understanding many of the issues in heterogeneous memory models. At the same time, the Khronos Group developed the OpenCL 2.0 memory model that builds on the C++ memory model. The OpenCL 2.0 model includes features not addressed by HRF: primarily support for relaxed atomics and a property referred to as scope inclusion. In this article, we generalize HRF to allow formalization of and reasoning about more complicated models using OpenCL 2.0 as a point of reference. With that generalization, we (1) make the OpenCL 2.0 memory model more accessible by introducing a platform for feature comparisons to other models, (2) consider a number of shortcomings in the current OpenCL 2.0 model, and (3) propose changes that could be adopted by future OpenCL 2.0 revisions or by other, related, models. © 2015 ACM.",Computer architecture; Formal models; Memory models; Programming languages,C++ (programming language); Computational linguistics; Computer architecture; Computer programming; Computer programming languages; Computer software; High level languages; Java programming language; Microprocessor chips; Semantics; System-on-chip; Conceptual foundations; Formal model; Heterogeneous memory; Heterogeneous systems; Memory address space; Memory consistency models; Memory models; Sequential consistency; Memory architecture
CERE: LLVM-based codelet extractor and replayer for piecewise benchmarking and optimization,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929148896&doi=10.1145%2f2724717&partnerID=40&md5=8a6a686c00a806708d8c3dbc98282d91,"This article presents Codelet Extractor and REplayer (CERE), an open-source framework for code isolation. CERE finds and extracts the hotspots of an application as isolated fragments of code, called codelets. Codelets can be modified, compiled, run, and measured independently from the original application. Code isolation reduces benchmarking cost and allows piecewise optimization of an application. Unlike previous approaches, CERE isolates codes at the compiler Intermediate Representation (IR) level. Therefore CERE is language agnostic and supports many input languages such as C, C++, Fortran, and D. CERE automatically detects codelets invocations that have the same performance behavior. Then, it selects a reduced set of representative codelets and invocations, much faster to replay, which still captures accurately the original application. In addition, CERE supports recompiling and retargeting the extracted codelets. Therefore, CERE can be used for cross-architecture performance prediction or piecewise code optimization. On the SPEC 2006 FP benchmarks, CERE codelets cover 90.9% and accurately replay 66.3% of the execution time. We use CERE codelets in a realistic study to evaluate three different architectures on the NAS benchmarks. CERE accurately estimates each architecture performance and is 7.3× to 46.6× cheaper than running the full benchmark. © 2015 ACM.",Checkpoint restart; Iterative optimization; Performance prediction; Program replay,Architecture; Benchmarking; Codes (symbols); Computational linguistics; Open systems; Checkpoint restart; Code optimization; Intermediate representations; Iterative Optimization; NAS benchmarks; Open source frameworks; Performance prediction; Program replay; C++ (programming language)
MAGIC: Malicious aging in Circuits/Cores,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929178449&doi=10.1145%2f2724718&partnerID=40&md5=841198138f1b9d7e4f3a4c7a741546f1,"The performance of an IC degrades over its lifetime, ultimately resulting in IC failure. In this article, we present a hardware attack (called MAGIC) to maliciously accelerate NBTI aging effects in cores. In this attack, we identify the input patterns that maliciously age the pipestages of a core. We then craft a program that generates these patterns at the inputs of the targeted pipestage. We demonstrate the MAGIC-based attack on the OpenSPARC processor. Executing this program dramatically accelerates the aging process and degrades the processor's performance by 10.92% in 1 month, bypassing existing aging mitigation and timing-error correction schemes. We also present two low-cost techniques to thwart the proposed attack. © 2015 ACM.",Hardware security; Malicious aging acceleration; NBTI aging,Error correction; Hardware; Program processors; Aging effects; Aging process; Hardware attack; Input patterns; Low costs; Timing errors; Hardware security
Dynamic MIPS rate stabilization for complex processors,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929193109&doi=10.1145%2f2714575&partnerID=40&md5=cb008b10bcf8a066b9f57b63afa1e5f0,"Modern microprocessor cores reach their high performance levels with the help of high clock rates, parallel and speculative execution of a large number of instructions, and vast cache hierarchies. Modern cores also have adaptive features to regulate power and temperature and avoid thermal emergencies. All of these features contribute to highly unpredictable execution times. In this article, we demonstrate that the execution time of in-order (IO), out-of-order (OoO), and OoO simultaneous multithreaded processors can be stable and predictable by stabilizing their mega instructions executed per second (MIPS) rate via a proportional, integral, and differential (PID) gain feedback controller and dynamic voltage and frequency scaling (DVFS). Processor cores in idle cycles are continuously consuming power, which is highly undesirable in systems, especially in real-time systems. In addition to meeting deadlines in real-time systems, our MIPS rate stabilization framework can be applied on top of it to reduce power and energy by avoiding idle cycles. If processors are equipped with MIPS rate stabilization, the execution time can be predicted. Because the MIPS rate remains steady, a stabilized processor meets deadlines on time in real-time systems or in systems with quality-of-service execution latency requirements at the lowest possible frequency. To demonstrate and evaluate this capability, we have selected a subset of the MiBench benchmarks with the widest execution rate variations. We stabilize their MIPS rate on a 1GHz Pentium III-like OoO single-thread microarchitecture, a 1.32GHz StrongARM-like IO microarchitecture, and the 1GHz OoO processor augmented with two-way and four-way simultaneous multithreading. Both IO and OoO cores can take advantage of the stabilization framework, but the energy per instruction of the stabilized OoO core is less because it runs at a lower frequency to meet the same deadlines. The MIPS rate stabilization of complex processors using a PID feedback control loop is a general technique applicable to environments in which lower power or energy coupled with steady, predictable performance are desirable, although we target more specifically real-time systems in this article. © 2015 ACM.",Energy; EPI; Power,Computer architecture; Concurrency control; Dynamic frequency scaling; Embedded systems; Feedback control; Interactive computer systems; Multitasking; Program processors; Proportional control systems; Quality of service; Stabilization; Voltage scaling; Dynamic voltage and frequency scaling; Energy; EPI; Power; Simultaneous multi-threaded processors; Simultaneous multi-threading; Speculative execution; Unpredictable execution time; Real time systems
Generalized task parallelism,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929156089&doi=10.1145%2f2723164&partnerID=40&md5=27dc6ba42d1056b9f773f40e8751be10,"Existing approaches to automatic parallelization produce good results in specific domains. Yet, it is unclear how to integrate their individual strengths to match the demands and opportunities of complex software. This lack of integration has both practical reasons, as integrating those largely differing approaches into one compiler would impose an engineering hell, as well as theoretical reasons, as no joint cost model exists that would drive the choice between parallelization methods. By reducing the problem of generating parallel code from a program dependence graph to integer linear programming, generalized task parallelization integrates central aspects of existing parallelization approaches intoa single unified framework. Implemented ontop of LLVM, the framework seamlessly integrates enabling technologies such as speculation, privatization, and the realization of reductions. Evaluating our implementation on various C programs from different domains, we demonstrate the effectiveness and generality of generalized task parallelization. On a quad-core machine with hyperthreading we achieve speedups of up to 4.6×. © 2015 ACM.",Automatic parallelization; Integer linear programming; Just-in-time compilation,Cost engineering; Integer programming; Privatization; Program compilers; Automatic Parallelization; Different domains; Enabling technologies; Individual strength; Integer Linear Programming; Just-in-time compilation; Program dependence graph; Unified framework; C (programming language)
NoCMsg: A scalable message-passing abstraction for network-on-chips,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924911297&doi=10.1145%2f2701426&partnerID=40&md5=e5f1a4ec2f4f7f5cdac67e6cbeda0177,"The number of cores of contemporary processors is constantly increasing and thus continues to deliver ever higher peak performance (following Moore's transistor law). Yet high core counts present a challenge to hardware and software alike. Following this trend, the network-on-chip (NoC) topology has changed from buses over rings and fully connected meshes to 2D meshes. This work contributes NoCMsg, a low-level message-passing abstraction over NoCs which is specifically designed for large core counts in 2D meshes. NoCMsg ensures deadlock-free messaging for wormhole Manhattan-path routing over the NoC via a polling-based message abstraction and non-flow-controlled communication for selective communication patterns. Experimental results on the TilePro hardware platform show that NoCMsg can significantly reduce communication times by up to 86% for single packet messages and up to 40% for larger messages compared to other NoC-based message approaches. On the TilePro platform, NoCMsg outperforms shared memory abstractions by up to 93% as core counts and interprocess communication increase. Results for fully pipelined double-precision numerical codes show speedups of up to 64% for message passing over shared memory at 32 cores. Overall, we observe that shared memory scales up to about 16 cores on this platform, whereas message passing performs well beyond that threshold. These results generalize to similar NoC-based platforms. © 2015 ACM.",,Abstracting; Fault tolerant computer systems; Memory architecture; Message passing; Communication pattern; Double precision; Fully pipelined; Hardware and software; Hardware platform; Interprocess communication; Network-on-chip(NoC); Peak performance; Network-on-chip
Performance-energy considerations for shared cache management in a heterogeneous multicore processor,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924855916&doi=10.1145%2f2710019&partnerID=40&md5=55a006391c18f6cd1a5dd2a924a5042e,"Heterogeneous multicore processors that integrate CPU cores and data-parallel accelerators such as graphic processing unit (GPU) cores onto the same die raise several new issues for sharing various on-chip resources. The shared last-level cache (LLC) is one of the most important shared resources due to its impact on performance. Accesses to the shared LLC in heterogeneous multicore processors can be dominated by the GPU due to the significantly higher number of concurrent threads supported by the architecture. Under current cache management policies, the CPU applications' share of the LLC can be significantly reduced in the presence of competing GPU applications. For many CPU applications, a reduced share of the LLC could lead to significant performance degradation. On the contrary, GPU applications can tolerate increase in memory access latency when there is sufficient thread-level parallelism (TLP). In addition to the performance challenge, introduction of diverse cores onto the same die changes the energy consumption profile and, in turn, affects the energy efficiency of the processor. In this work, we propose heterogeneous LLC management (HeLM), a novel shared LLC management policy that takes advantage of the GPU's tolerance for memory access latency. HeLM is able to throttle GPU LLC accesses and yield LLC space to cache-sensitive CPU applications. This throttling is achieved by allowing GPU accesses to bypass the LLC when an increase in memory access latency can be tolerated. The latency tolerance of a GPU application is determined by the availability of TLP, which is measured at runtime as the average number of threads that are available for issuing. For a baseline configuration with two CPU cores and four GPU cores, modeled after existing heterogeneous processor designs, HeLM outperforms least recently used (LRU) policy by 10.4%. Additionally, HeLM also outperforms competing policies. Our evaluations show that HeLM is able to sustain performance with varying core mix. In addition to the performance benefit, bypassing also reduces total accesses to the LLC, leading to a reduction in the energy consumption of the LLC module. However, LLC bypassing has the potential to increase off-chip bandwidth utilization and DRAM energy consumption. Our experiments show that HeLM exhibits better energy efficiency by reducing the ED2 by 18% over LRU while impacting only a 7% increase in off-chip bandwidth utilization. © 2015 ACM.",,Bandwidth; Dynamic random access storage; Energy efficiency; Energy utilization; Memory architecture; Parallel processing systems; Cache management policies; Graphic processing unit(GPU); Heterogeneous multicore processors; Heterogeneous processors; Least-recently-used policies; Performance degradation; Shared cache managements; Thread-level parallelism; Graphics processing unit
Accelerating divergent applications on simd architectures using neural networks,2015,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924901738&doi=10.1145%2f2717311&partnerID=40&md5=02ed132db620624db846401e82e6cf3a,"The purpose of this research is to find a neural-network-based solution to the well-known problem of branch divergence in Single Instruction Multiple Data (SIMD) architectures. Our approach differs from existing techniques that handle branch (or control-flow) divergence, which use costly hardware modifications, low-utilization masking techniques, or static prediction methods. As we examine divergent applications, we characterize the degree of data-dependent control flow seen in each and isolate the code regions (or ""kernels"") that cause the most performance degradation due to branch divergence. We then train neural networks (NNs) offline to approximate these kernels and inject the NN computations directly into the applications as substitutes for the kernels they approximate. This essentially translates control flow into nondivergent computation, trading off precision for performance. As our methodology manipulates application source code directly, it is inherently platform agnostic and can be adopted as a general means for accelerating divergent applications on data-parallel architectures. In this article, we present the Neuralizer, an automated software flow for kernel identification, NN training, and NN integration, as well as supplementary user-controlled optimization techniques. Evaluating our approach on a variety of divergent applications run on a Graphics Processing Unit (GPU), we on average achieve performance gains of 13.6× and energy savings of 14.8× with 96% accuracy. © 2015 ACM.",,Codes (symbols); Computer graphics; Computer graphics equipment; Energy conservation; Graphics processing unit; Network architecture; Parallel architectures; Program processors; Data-parallel architectures; Hardware modifications; Network-based solutions; Neural networks (NNS); Optimization techniques; Performance degradation; Prediction methods; Single-instruction multiple-data architectures; Neural networks
Optimizing memory translation emulation in full system emulators,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921280325&doi=10.1145%2f2686034&partnerID=40&md5=b3f03a0dc72d51b808a5650a5233b58f,"The emulation speed of a full system emulator (FSE) determines its usefulness. This work quantitatively measures where time is spent in QEMU [Bellard 2005], an industrial-strength FSE. The analysis finds that memory emulation is one of the most heavily exercised emulator components. For workloads studied, 38.1% of the emulation time is spent in memory emulation on average, even though QEMU implements a software translation lookaside buffer (STLB) to accelerate dynamic address translation. Despite the amount of time spent in memory emulation, there has been no study on how to further improve its speed. This work analyzes where time is spent in memory emulation and studies the performance impact of a number of STLB optimizations. Although there are several performance optimization techniques for hardware TLBs, this work finds that the trade-offs with an STLB are quite different compared to those with hardware TLBs. As a result, not all hardware TLB performance optimization techniques are applicable to STLBs and vice versa. The evaluated STLB optimizations target STLB lookups, as well as refills, and result in an average emulator performance improvement of 24.4% over the baseline. © 2014 ACM.",Dynamic address translation; Full system emulator; TLB,Distributed computer systems; Economic and social effects; Hardware; Java programming language; Dynamic address translation; Full system emulator; Industrial strength; Performance impact; Performance optimizations; Time spent; TLB; Translation lookaside buffer; Buffer storage
Making the most of SMT in HPC: System- and application-level perspectives,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921307581&doi=10.1145%2f2687651&partnerID=40&md5=3bd43a052736272057a4dbe07a0d8a14,"This work presents an end-to-end methodology for quantifying the performance and power benefits of simultaneous multithreading (SMT) for HPC centers and applies this methodology to a production system and workload. Ultimately, SMT's value system-wide depends on whether users effectively employ SMT at the application level. However, predicting SMT's benefit for HPC applications is challenging; by doubling the number of threads, the application's characteristics may change. This work proposes statistical modeling techniques to predict the speedup SMT confers to HPC applications. This approach, accurate to within 8%, uses only lightweight, transparent performance monitors collected during a single run of the application. © 2014 ACM.",Energy evaluation; High-performance computing; Performance evaluation; Simultaneous multithreading,Program processors; Application level; Energy evaluation; High performance computing; Performance evaluation; Performance monitors; Production system; Simultaneous multi-threading; Statistical modeling; Multitasking
Perfect reconstructability of control flow from demand dependence graphs,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921321194&partnerID=40&md5=3d4f68b4c9f5c741a85588291c411425,"Demand-based dependence graphs (DDGs), such as the (Regionalized) Value State Dependence Graph ((R)VSDG), are intermediate representations (IRs) well suited for a wide range of program transformations. They explicitly model the flow of data and state, and only implicitly represent a restricted form of control flow. These features make DDGs especially suitable for automatic parallelization and vectorization, but cannot be leveraged by practical compilers without efficient construction and destruction algorithms. Construction algorithms remodel the arbitrarily complex control flow of a procedure to make it amenable to DDG representation, whereas destruction algorithms reestablish control flow for generating efficient object code. Existing literature presents solutions to both problems, but these impose structural constraints on the generatable control flow, and omit qualitative evaluation. The key contribution of this article is to show that there is no intrinsic structural limitation in the control flow directly extractable from RVSDGs. This fundamental result originates from an interpretation of loop repetition and decision predicates as computed continuations, leading to the introduction of the predicate continuation normal form.We provide an algorithm for constructing RVSDGs in predicate continuation form, and propose a novel destruction algorithm for RVSDGs in this form. Our destruction algorithm can generate arbitrarily complex control flow; we show this by proving that the original CFG an RVSDG was derived from can, apart from overspecific detail, be reconstructed perfectly. Additionally, we prove termination and correctness of these algorithms. Furthermore, we empirically evaluate the performance, the representational overhead at compile time, and the reduction in branch instructions compared to existing solutions. In contrast to previous work, our algorithms impose no additional overhead on the control flow of the produced object code. To our knowledge, this is the first scheme that allows the original control flow of a procedure to be recovered from a DDG representation.",Control flow; Demand-dependence; Intermediate representations; Value state dependence graph,Algorithms; Functional programming; Program compilers; Program processors; Automatic Parallelization; Construction algorithms; Control flows; Demand-dependence; Intermediate representations; Program transformations; Qualitative evaluations; State dependence; F.3.3 [logics and meanings of programs]: studies of program constructs - control primitives; F.3.3 [logics and meanings of programs]: studies of program constructs - functional constructs; Theory; Flow graphs
Mechanistic analytical modeling of superscalar in-order processor performance,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921293471&doi=10.1145%2f2678277&partnerID=40&md5=9be08bb28960ba233c57fbb4e89716b1,"Superscalar in-order processors form an interesting alternative to out-of-order processors because of their energy efficiency and lower design complexity. However, despite the reduced design complexity, it is nontrivial to get performance estimates or insight in the application-microarchitecture interaction without running slow, detailed cycle-level simulations, because performance highly depends on the order of instructions within the application's dynamic instruction stream, as in-order processors stall on interinstruction dependences and functional unit contention. To limit the number of detailed cycle-level simulations needed during design space exploration, we propose a mechanistic analytical performance model that is built from understanding the internal mechanisms of the processor. The mechanistic performance model for superscalar in-order processors is shown to be accurate with an average performance prediction error of 3.2% compared to detailed cycle-accurate simulation using gem5. We also validate the model against hardware, using the ARM Cortex-A8 processor and show that it is accurate within 10% on average. We further demonstrate the usefulness of the model through three case studies: (1) design space exploration, identifying the optimum number of functional units for achieving a given performance target; (2) program-machine interactions, providing insight into microarchitecture bottlenecks; and (3) compiler-architecture interactions, visualizing the impact of compiler optimizations on performance. © 2014 ACM.",Cycle stacks; Functional units; Inter-instruction dependences; Performance modeling; Processor design space exploration; Superscalar in-order processors,Computer architecture; Energy efficiency; Program compilers; Cycle stacks; Functional units; Inter-instruction dependences; Performance Model; Processor design; Superscalar; Integrated circuit design
Architectural support for data-driven execution,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921296904&doi=10.1145%2f2686874&partnerID=40&md5=a5837bb41752411ca1d27dc6a0cc2077,"The exponential growth of sequential processors has come to an end, and thus, parallel processing is probably the only way to achieve performance growth. We propose the development of parallel architectures based on data-driven scheduling. Data-driven scheduling enforces only a partial ordering as dictated by the true data dependencies, which is the minimum synchronization possible. This is very beneficial for parallel processing because it enables it to exploit the maximum possible parallelism. We provide architectural support for data-driven execution for the Data-Driven Multithreading (DDM) model. In the past, DDM has been evaluated mostly inthe form of virtual machines. The main contribution of this workis the development of a highly efficient hardware support for data-driven execution and its integration into a multicore system with eight cores on a Virtex-6 FPGA. The DDM semantics make barriers and cache coherence unnecessary, which reduces the synchronization latencies significantly and makes the cache simpler. The performance evaluation has shown that the support for data-driven execution is very efficient with negligible overheads. Our prototype can support very small problem sizes (matrix 16×16) and ultra-lightweight threads (block of 4×4) that achieve speedups close to linear. Such results cannot be achieved by software-based systems. © 2014 ACM.",Data-driven multithreading; FPGA,Field programmable gate arrays (FPGA); Multitasking; Semantics; Architectural support; Data-driven multithreading; Exponential growth; Hardware supports; Multi-core systems; Parallel processing; Sequential processors; Ultra lightweights; Scheduling
Efficient data encoding for convolutional neural network application,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921287138&doi=10.1145%2f2685394&partnerID=40&md5=4ae6f3a89e58804a3951613c05b9c1a2,"This article presents an approximate data encoding scheme called Significant Position Encoding (SPE). The encoding allows efficient implementation of the recall phase (forward propagation pass) of Convolutional Neural Networks (CNN)-a typical Feed-Forward Neural Network.This implementation uses only7bits data representation and achieves almost the same classification performance compared with the initial network: on MNIST handwriting recognition task, using this data encoding scheme losses only 0.03% in terms of recognition rate (99.27% vs. 99.3%). In terms of storage, we achieve a 12.5% gain compared with an 8 bits fixed-point implementation of the same CNN. Moreover, this data encoding allows efficient implementation of processing unit thanks to the simplicity of scalar product operation-the principal operation in a Feed-Forward Neural Network. © 2014 ACM.",Canonical signed digit; Convolutional neural network; Significant Position Encoding,Character recognition; Convolution; Digital storage; Feedforward neural networks; Neural networks; Canonical signed digits; Classification performance; Convolutional neural network; Data representations; Efficient implementation; Fixed-point implementation; Forward propagation; Handwriting recognition; Encoding (symbols)
Using template matching to infer parallel design patterns,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921266049&doi=10.1145%2f2688905&partnerID=40&md5=ae163c1d253c39768ab4299df98dd9d4,"The triumphant spread of multicore processors over the past decade increases the pressure on software developers to exploit the growing amount of parallelism available in the hardware. However, writing parallel programs is generally challenging. For sequential programs, the formulation of design patterns marked a turning point in software development, boosting programmer productivity and leading to more reusable and maintainable code. While the literature is now also reporting a rising number of parallel design patterns, programmers confronted with the task of parallelizing an existing sequential program still struggle with the question of which parallel pattern to apply where in their code. In this article, we show how template matching, a technique traditionally used in the discovery of sequential design patterns, can also be used to support parallelization decisions. After looking for matches in a previously extracted dynamic dependence graph, we classify code blocks of the input program according to the structure of the parallel patterns we find. Based on this information, the programmer can easily implement the detected pattern and create a parallel version of his or her program. We tested our approach with six programs, in which we successfully detected pipeline and do-all patterns. © 2014 ACM.",Do-all detection; Parallel pattern detection; Parallelism; Pipeline detection,Computer software reusability; Multicore programming; Pipelines; Software design; Dynamic dependence graphs; Multi-core processor; Parallel design patterns; Parallel patterns; Parallelism; Pipeline detection; Programmer productivity; Sequential programs; Template matching
Transactional read-modify-write without aborts,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921321444&doi=10.1145%2f2688904&partnerID=40&md5=b478f1f2ae9dd4b16e599abc6bb011a9,"Language-level transactions are said to provide ""atomicity,"" implying that the order of operations within a transaction should be invisible to concurrent transactions and thus that independent operations within a transaction should be safe to execute in any order. In this article, we present a mechanism for dynamically reordering memory operations within a transaction so that read-modify-write operations on highly contended locations can be delayed until the very end of the transaction. When integrated with traditional transactional conflict detection mechanisms, our approach reduces aborts on hot memory locations, such as statistics counters, thereby improving throughput and reducing wasted work. We present three algorithms for delaying highly contended read-modify-write operations within transactions, and we evaluate their impact on throughput for eager and lazy transactional systems across multiple workloads. We also discuss complications that arise from the interaction between our mechanism and the need for strong language-level semantics, and we propose algorithmic extensions that prevent errors from occurring when accesses are aggressively reordered in a transactional memory implementation with weak semantics. © 2015 ACM.",Conflict detection; Language-level semantics; Transactional memory,Semantics; Storage allocation (computer); Concurrent transactions; Conflict detection; Language levels; Memory locations; Reordering memory; Transactional memory; Transactional systems; Write operations; Algorithmic languages
Compiler/runtime framework for dynamic dataflow parallelization of tiled programs,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921271401&doi=10.1145%2f2687652&partnerID=40&md5=ecce895731821bcbd1d2fe8c78725b01,"Task-parallel languages are increasingly popular.Many of them provide expressive mechanisms for intertask synchronization. For example, OpenMP 4.0 will integrate data-driven execution semantics derived from the StarSs research language. Compared to the more restrictive data-parallel and fork-join concurrency models, the advanced features being introduced into task-parallelmodels in turn enable improved scalability through load balancing,memory latency hiding,mitigation of the pressure on memory bandwidth, and, as a side effect, reduced power consumption. In this article, we develop a systematic approach to compile loop nests into concurrent, dynamically constructed graphs of dependent tasks. We propose a simple and effective heuristic that selects the most profitable parallelization idiom for every dependence type and communication pattern. This heuristic enables the extraction of interband parallelism (cross-barrier parallelism) in a number of numerical computations that range from linear algebra to structured grids and image processing. The proposed static analysis and code generation alleviates the burden of a full-blown dependence resolver to track the readiness of tasks at runtime. We evaluate our approach and algorithms in the PPCG compiler, targeting OpenStream, a representative dataflow task-parallel language with explicit intertask dependences and a lightweight runtime. Experimental results demonstrate the effectiveness of the approach. © 2014 ACM.",Auto-parallelization; Dataflow; Dependence partitioning; Dynamic wavefront; Point-to-point synchronization; Polyhedral compiler; Polyhedral framework; Tile dependences; Tiling,Application programming interfaces (API); Image processing; Linear algebra; Semantics; Static analysis; Auto-parallelization; Dataflow; Dependence partitioning; Point to point; Polyhedral compiler; Polyhedral framework; Tiling; Program compilers
Efficient correction of anomalies in snapshot isolation transactions,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921266907&doi=10.1145%2f2693260&partnerID=40&md5=bde73f2e51f349ca0865719e1b46e2e1,"Transactional memory systems providing snapshot isolation enable concurrent access to shared data without incurring aborts on read-write conflicts. Reducing aborts is extremely relevant as it leads to higher concurrency, greater performance, and better predictability. Unfortunately, snapshot isolation does not provide serializability as it allows certain anomalies that can lead to subtle consistency violations. While some mechanisms have been proposed to verify the correctness of a program utilizing snapshot isolation transactions, it remains difficult to repair incorrect applications. To reduce the programmer's burden in this case, we present a technique based on dynamic code and graph dependency analysis that automatically corrects existing snapshot isolation anomalies in transactional memory programs. Our evaluation shows that corrected applications retain the performance benefits characteristic of snapshot isolation over conventional transactional memory systems. © 2014 ACM.",Concurrency; Consistency; Snapshot isolation; Transactional memory,Data Sharing; Storage allocation (computer); Concurrency; Concurrent access; Consistency; Dependency analysis; Performance benefits; Serializability; Snapshot isolation; Transactional memory; Application programs
Compiler-directed power management for superscalars,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921316914&doi=10.1145%2f2685393&partnerID=40&md5=999f8ee71b39868668b9cf7925ff0cf2,"Modern superscalar CPUs contain large complex structures and diverse execution units, consuming wide dynamic power range. Building a power delivery network for the worst-case power consumption is not energy efficient and often is impossible to fit in small systems. Instantaneous power excursions can cause voltage droops. Power management algorithms are too slow to respond to instantaneous events. In this article, we propose a novel compiler-directed framework to address this problem. The framework is validated on a 4th Generation Intel® Core™ processor and with simulator on output trace. Up to 16% performance speedup is measured over baseline for the SPEC CPU2006 benchmarks. © 2014 ACM.",Compiler assisted; Energy; Power management; Power modeling,Benchmarking; Electric power transmission; Embedded systems; Energy efficiency; Program compilers; Compiler-assisted; Complex structure; Energy; Energy efficient; Instantaneous power; Power delivery network; Power management algorithms; Power model; Power management
The effects of parameter tuning in software thread-level speculation in JavaScript engines,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921280932&doi=10.1145%2f2686036&partnerID=40&md5=8b87f27e7717fc4117cba31eb9426f97,"JavaScript is a sequential programming language that has a large potential for parallel execution in Web applications. Thread-level speculation can take advantage of this, but it has a large memory overhead. In this article, we evaluate the effects of adjusting various parameters for thread-level speculation. Our results clearly show that thread-level speculation is a useful technique for taking advantage of multicore architectures for JavaScript in Web applications, that nested speculation is required in thread-level speculation, and that the execution characteristics of Web applications significantly reduce the needed memory, the number of threads, and the depth of our speculation. © 2014 ACM.",Dynamic parallelization; Multithreading; Speculative execution; Virtual machines; Web applications,Concurrency control; High level languages; Social networking (online); Software architecture; World Wide Web; Multi-threading; Parallelizations; Speculative execution; Virtual machines; WEB application; Java programming language
Measuring microarchitectural details of multi- and many-core memory systems through microbenchmarking,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921300908&doi=10.1145%2f2687356&partnerID=40&md5=96ca483677670dfd1a7c9d9525c06926,"As multicore and many-core architectures evolve, their memory systems are becoming increasingly more complex. To bridge the latency and bandwidth gap between the processor and memory, they often use a mix of multilevel private/shared caches that are either blocking or nonblocking and are connected by high-speed network-on-chip. Moreover, they also incorporate hardware and software prefetching and simultaneous multithreading (SMT) to hide memory latency. On such multi- and many-core systems, to incorporate various memory optimization schemes using compiler optimizations and performance tuning techniques, it is crucial to have microarchitectural details of the target memory system. Unfortunately, such details are often unavailable from vendors, especially for newly released processors. In this article, we propose a novel microbenchmarking methodology based on short elapsed-time events (SETEs) to obtain comprehensive memory microarchitectural details in multi- and many-core processors. This approach requires detailed analysis of potential interfering factors that could affect the intended behavior of such memory systems. We lay out effective guidelines to control and mitigate those interfering factors. Taking the impact of SMT into consideration, our proposed methodology not only can measure traditional cache/memory latency and off-chip bandwidth but also can uncover the details of software and hardware prefetching units not attempted in previous studies. Using the newly released Intel Xeon Phi many-core processor (with in-order cores) as an example, we show how we can use a set of microbenchmarks to determine various microarchitectural features of its memory system (many are undocumented from vendors). To demonstrate the portability and validate the correctness of such a methodology, we use the welldocumented Intel Sandy Bridge multicore processor (with out-of-order cores) as another example, where most data are available and can be validated. Moreover, to illustrate the usefulness of the measured data, we do a multistage coordinated data prefetching case study on both Xeon Phi and Sandy Bridge and show that by using the measured data, we can achieve 1.3X and 1.08X performance speedup, respectively, compared to the state-of-the-art Intel ICC compiler. We believe that these measurements also provide useful insights into memory optimization, analysis, and modeling of such multicore and many-core architectures. © 2014 ACM.",Many-core; Memory microarchitecture; Microbenchmarking; Multicore; Prefetching,Bandwidth; Complex networks; Computer architecture; Distributed computer systems; Hardware; HIgh speed networks; Microprocessor chips; Multitasking; Network architecture; Network-on-chip; Program compilers; Program processors; VLSI circuits; Many core; Micro architectures; Micro-benchmarking; Multi core; Prefetching; Cache memory
The impact of the SIMD width on control-flow and memory divergence,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921292862&doi=10.1145%2f2687355&partnerID=40&md5=8d5925bbf868e515713a909b45339917,"Power consumption is a prevalent issue in current and future computing systems. SIMD processors amortize the power consumption of managing the instruction stream by executing the same instruction in parallel on multiple data. Therefore, in the past years, the SIMD width has steadily increased, and it is not unlikely that it will continue to do so. In this article, we experimentally study the influence of the SIMD width to the execution of data-parallel programs. We investigate how an increasing SIMD width (up to 1024) influences control-flow divergence and memory-access divergence, and how well techniques to mitigate them will work on larger SIMD widths. We perform our study on 76 OpenCL applications and show that a group of programs scales well up to SIMD width 1024, whereas another group of programs increasingly suffers from controlflow divergence. For those programs, thread regrouping techniques may become increasingly important for larger SIMD widths. We show what average speedups can be expected when increasing the SIMD width. For example, when switching from scalar execution to SIMD width 64, one can expect a speedup of 60.11, which increases to 62.46 when using thread regrouping. We also analyze the frequency of regular (uniform, consecutive) memory access patterns and observe a monotonic decrease of regular memory accesses from 82.6% at SIMD width 4 to 43.1% at SIMD width 1024. © 2014 ACM.",Data parallelism; Divergent control flow; GPGPU; Memory access patterns; OpenCL performance analysis; SIMD; Vectorization,Electric power utilization; Green computing; Memory architecture; Program processors; Control flows; Data parallelism; GPGPU; Memory access patterns; Performance analysis; SIMD; Vectorization; Application programs
On using the roofline model with lower bounds on data movement,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921320722&doi=10.1145%2f2693656&partnerID=40&md5=143d0d6036beefc038e9b235fc219793,"The roofline model is a popular approach for ""bound and bottleneck"" performance analysis. It focuses on the limits to the performance of processors because of limited bandwidth to off-chip memory. It models upper bounds on performance as a function of operational intensity, the ratio of computational operations per byte of data moved from/to memory. While operational intensity can be directly measured for a specific implementation of an algorithm on a particular target platform, it is of interest to obtain broader insights on bottlenecks, where various semantically equivalent implementations of an algorithm are considered, along with analysis for variations in architectural parameters. This is currently very cumbersome and requires performance modeling and analysis of many variants. In this article, we address this problem by using the roofline model in conjunction with upper bounds on the operational intensity of computations as a function of cache capacity, derived from lower bounds on data movement. This enables bottleneck analysis that holds across all dependence-preserving semantically equivalent implementations of an algorithm. We demonstrate the utility of the approach in assessing fundamental limits to performance and energy efficiency for several benchmark algorithms across a design space of architectural variations. © 2015 ACM.",Algorithm-architecture codesign; Architecture design space exploration; I/O lower bounds; Operational intensity upper bounds,Energy efficiency; Memory architecture; Architectural parameters; Architecture designs; Co-designs; Computational operations; Implementation of an algorithm; Lower bounds; Performance modeling and analysis; Upper Bound; Benchmarking
Mitigating prefetcher-caused pollution using informed caching policies for prefetched blocks,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921267973&doi=10.1145%2f2677956&partnerID=40&md5=66344bd0dc50745bad0a9d648788047a,"Many modern high-performance processors prefetch blocks into the on-chip cache. Prefetched blocks can potentially pollute the cache by evicting more useful blocks. In this work, we observe that both accurate and inaccurate prefetches lead to cache pollution, and propose a comprehensive mechanism to mitigate prefetcher-caused cache pollution. First, we observe that over 95% of useful prefetches in a wide variety of applications are not reused after the first demand hit (in secondary caches). Based on this observation, our first mechanism simply demotes a prefetched block to the lowest priority on a demand hit. Second, to address pollution caused by inaccurate prefetches, we propose a self-tuning prefetch accuracy predictor to predict if a prefetch is accurate or inaccurate. Only predicted-accurate prefetches are inserted into the cache with a high priority. Evaluations show that our final mechanism, which combines these two ideas, significantly improves performance compared to both the baseline LRU policy and two state-of-the-art approaches to mitigating prefetcher-caused cache pollution (up to 49%, and 6% on average for 157 two-core multiprogrammed workloads). The performance improvement is consistent across a wide variety of system configurations. We thank the anonymous reviewers for their feedback. We acknowledge members of the SAFARI and LBA groups for their feedback and for the stimulating research environment they provide. We specifically thank Lavanya Subramanian and Nandita Vijaykumar for their feedback and comments on early versions of this paper. We acknowledge the generous support of Intel, Qualcomm, and Samsung. This work is supported in part by NSF grants 0953246, 1212962, 1320531, the Intel Science and Technology Center for Cloud Computing, and the Semiconductor Research Corporation. © 2014 ACM.",Cache insertion/promotion policy; Cache pollution; Caches; Prefetching,Microprocessor chips; Multiprogramming; Pollution; Cache pollution; Caches; Caching policy; Modern high performance; Prefetching; Research environment; Science and technology centers; System configurations; Buffer storage
Fast crown scheduling heuristics for energy-efficient mapping and scaling of moldable streaming tasks on manycore systems,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921270863&doi=10.1145%2f2687653&partnerID=40&md5=2518cfd099d0c20c4d0879a2f2a53f96,"Exploiting effectively massively parallel architectures is a major challenge that stream programming can help facilitate. We investigate the problem of generating energy-optimal code for a collection of streaming tasks that include parallelizable or moldable tasks on a generic manycore processor with dynamic discrete frequency scaling. Streaming task collections differ from classical task sets in that all tasks are running concurrently, so that cores typically run several tasks that are scheduled round-robin at user level in a data-driven way. A stream of data flows through the tasks and intermediate results may be forwarded to other tasks, as in a pipelined task graph. In this article, we consider crown scheduling, a novel technique for the combined optimization of resource allocation, mapping, and discrete voltage/frequency scaling for moldable streaming task collections in order to optimize energy efficiency given a throughput constraint. We first present optimal offline algorithms for separate and integrated crown scheduling based on integer linear programming (ILP). We make no restricting assumption about speedup behavior. We introduce the fast heuristic Longest Task, Lowest Group (LTLG) as a generalization of the Longest Processing Time (LPT) algorithm to achieve a load-balanced mapping of parallel tasks, and the Height heuristic for crown frequency scaling.We use them in feedback loop heuristics based on binary search and simulated annealing to optimize crown allocation. Our experimental evaluation of the ILP models for a generic manycore architecture shows that at least for small and medium-sized streaming task collections even the integrated variant of crown scheduling can be solved to optimality by a state-of-the-art ILP solver within a few seconds. Our heuristics produce makespan and energy consumption close to optimality within the limits of the phase-separated crown scheduling technique and the crown structure. Their optimization time is longer than the one of other algorithms we test, but our heuristics consistently produce better solutions. © 2014 ACM.",Frequency scaling; Manycore; Mapping; Multicore; Parallel energy; Scheduling; Streaming,Acoustic streaming; Data flow analysis; Data streams; Dynamic frequency scaling; Energy efficiency; Energy utilization; Flow graphs; Mapping; Optimal systems; Parallel architectures; Scheduling; Simulated annealing; Structural optimization; Voltage scaling; Experimental evaluation; Frequency-scaling; Integer Linear Programming; Longest processing time algorithms; Many-core; Multi core; Parallel energy; Throughput constraints; Integer programming
Cross-loop optimization of arithmetic intensity for finite element local assembly,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921293987&doi=10.1145%2f2687415&partnerID=40&md5=f8cddd332fc6c44d65da4f435e6fe978,"We study and systematically evaluate a class of composable code transformations that improve arithmetic intensity in local assembly operations, which represent a significant fraction of the execution time in finite element methods. Their performance optimization is indeed a challenging issue. Even though affine loop nests are generally present, the short trip counts and the complexity of mathematical expressions, which vary among different problems, make it hard to determine an optimal sequence of successful transformations. Our investigation has resulted in the implementation of a compiler (called COFFEE) for local assembly kernels, fully integrated with a framework for developing finite element methods. The compiler manipulates abstract syntax trees generated from a domain-specific language by introducing domain-aware optimizations for instruction-level parallelism and register locality. Eventually, it produces C code including vector SIMD intrinsics. Experiments using a range of real-world finite element problems of increasing complexity show that significant performance improvement is achieved. The generality of the approach and the applicability of the proposed code transformations to other domains is also discussed. © 2014 ACM.",Compilers; Finite element integration; Local assembly; Optimizations; SIMD vectorization,Codes (symbols); Cosine transforms; Finite element method; Optimization; Problem oriented languages; Program compilers; Trees (mathematics); Domain specific languages; Finite element problems; Instruction level parallelism; Local assembly; Mathematical expressions; Performance improvements; Performance optimizations; Vectorization; C (programming language)
Studying optimal spilling in the light of SSA,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921298506&doi=10.1145%2f2685392&partnerID=40&md5=0f6537d7e2de1c6f613992339c8949bb,"Recent developments in register allocation, mostly linked to static single assignment (SSA) form, have shown the benefits of decoupling the problem in two phases: a first spilling phase places load and store instructions so that the register pressure at all program points is small enough, and a second assignment and coalescing phase maps the variables to physical registers and reduces the number of move instructions among registers. This article focuses on the first phase, for which many open questions remain: in particular, we study the notion of optimal spilling (what can be expressed?) and the impact of SSA form (does it help?). To identify the important features for optimal spilling on load-store architectures, we develop a new integer linear programming formulation, more accurate and expressive than previous approaches. Among other features, we can express SSA φ-functions, memory-to-memory copies, and the fact that a value can be stored simultaneously inaregister andinmemory. Based onthis formulation, we present a thorough analysis of the results obtained for the SPECINT 2000 and EEMBC 1.1 benchmarks, from which we draw, among others, the following conclusions: (1) rematerialization is extremely important; (2) SSA complicates the formulation of optimal spilling, especially because of memory coalescing when the code is not in conventional SSA (CSSA); (3) microarchitectural features are significant and thus have to be accounted for; and (4) significant savings can be obtained in terms of static spill costs, cache miss rates, and dynamic instruction counts. © 2014 ACM.",Performance models; Register allocation; Spilling; Static single assignment form,Flocculation; Integer programming; Optimal systems; Dynamic instructions; Integer linear programming formulation; Performance Model; Physical registers; Register allocation; Spilling; Static single assignment form; Static single assignments; Cache memory
Optimal parallelogram selection for hierarchical tiling,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921275369&doi=10.1145%2f2687414&partnerID=40&md5=f21fed7ab613ea7b25e3a4dc3a4892ad,"Loop tiling is an effective optimization to improve performance of multiply nested loops, which are the most time-consuming parts in many programs. Most massively parallel systems today are organized hierarchically, and different levels of the hierarchy differ in the organization of parallelism and the memory models they adopt. To make better use of these machines, it is clear that loop nests should be tiled hierarchically to fit the hierarchical organization of the machine; however, it is not so clear what should be the exact form of these hierarchical tiles. In particular, tile shape selection is of critical importance to expose parallelism of the tiled loop nests. Although loop tiling is a well-known optimization, not much is known about tile shape selection. In this article, we study tile shape selection when the shapes are any type of parallelograms and introduce a model to relate the tile shape of the hierarchy to the execution time. Using this model, we implement a system that automatically finds the tile shapes that minimize the execution time in a hierarchical system. Our experimental results show that in several cases, the tiles automatically selected by our system outperform the most intuitive tiling schemes usually adopted by programmers because of their simplicity. © 2014 ACM.",Compiler optimizations; Hierarchical tiling; Tile shape,Hierarchical systems; Compiler optimizations; Hierarchical organizations; Hierarchical tiling; Improve performance; Massively parallel systems; Memory models; Nested Loops; Time-consuming parts; Cache memory
Efficient out-of-order execution of guarded ISAs,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84917733512&doi=10.1145%2f2677037&partnerID=40&md5=3b3c00a89ccd5db268deb76b523a73a5,"ARM ISA-based processors are no longer low-cost, low-power processors. Nowadays, ARM ISA-based processor manufacturers are striving to implement medium-end to high-end processor cores, which implies implementing a state-of-the-art out-of-order execution engine. Unfortunately, providing efficient out-of-order execution on legacy ARM codes may be quite challenging due to guarded instructions. Predicting the guarded instructions addresses the main serialization impact associated with guarded instructions execution and the multiple definition problem. Moreover, guard prediction allows one to use a global branch-and-guard history predictor to predict both branches and guards, often improving branch prediction accuracy. Unfortunately, such a global branch-and-guard history predictor requires the systematic use of guard predictions. In that case, poor guard prediction accuracy would lead to poor overall performance on some applications. Building on top of recent advances in branch prediction and confidence estimation, we propose a hybrid branch-and-guard predictor, combining a global branch history component and global branch-andguard history component. The potential gain or loss due to the systematic use of guard prediction is dynamically evaluated at runtime. Two computing modes are enabled: systematic guard prediction use and high-confidence-only guard prediction use. Our experiments show that on most applications, an overwhelming majority of guarded instructions are predicted. Therefore, a simple but relatively inefficient hardware solution can be used to execute the few unpredicted guarded instructions. Significant performance benefits are observed on most applications, while applications with poorly predictable guards do not suffer from performance loss.",Branch prediction; Predicated execution; Superscalar processors,ARM processors; Branch prediction; Confidence estimation; Global branch history; Low power processors; Out-of-order execution; Performance benefits; Predicated execution; Superscalar Processor; Forecasting
Topological characterization of hamming and dragonfly networks and its implications on routing,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84917699146&doi=10.1145%2f2677038&partnerID=40&md5=cf1f4aaa854b5978d8d4d3269acfc3e9,"Current High-Performance Computing (HPC) and data center networks rely on large-radix routers. Hamming graphs (Cartesian products of complete graphs) and dragonflies (two-level direct networks with nodes organized in groups) are some direct topologies proposed for such networks. The original definition of the dragonfly topology is very loose, with several degrees of freedom, such as the inter- and intragroup topology, the specific global connectivity, and the number of parallel links between groups (or trunking level). This work provides a comprehensive analysis of the topological properties of the dragonfly network, providing balancing conditions for network dimensioning, as well as introducing and classifying several alternatives for the global connectivity and trunking level. From a topological study of the network, it is noted that a Hamming graph can be seen as a canonical dragonfly topology with a high level of trunking. Based on this observation and by carefully selecting the global connectivity, the Dimension Order Routing (DOR) mechanism safely used in Hamming graphs is adapted to dragonfly networks with trunking. The resulting routing algorithms approximate the performance of minimal, nonminimal, and adaptive routings typically used in dragonflies but without requiring virtual channels to avoid packet deadlock, thus allowing for lower cost router implementations. This is obtained by properly selecting the link to route between groups based on a graph coloring of network routers. Evaluations show that the proposed mechanisms are competitive with traditional solutions when using the same number of virtual channels and enable for simpler implementations with lower cost. Finally, multilevel dragonflies are discussed, considering how the proposed mechanisms could be adapted to them.",Deadlock-freedom; Dragonfly network; Hamming graph; Routing; Topology,Degrees of freedom (mechanics); Graph theory; Network routing; Routers; Cartesian products of complete graphs; Comprehensive analysis; Data center networks; Deadlock freedom; Hamming graphs; High performance computing (HPC); Routing; Topological properties; Topology
Building and optimizing MRAM-based commodity memories,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84917729858&doi=10.1145%2f2667105&partnerID=40&md5=8159bad9db84aa9d5bf36a66fd94119d,"Emerging non-volatile memory technologies such as MRAM are promising design solutions for energyefficient memory architecture, especially formobile systems. However, building commodityMRAMby reusing DRAM designs is not straightforward. The existing memory interfaces are incompatible with MRAM small page size, and they fail to leverage MRAM unique properties, causing unnecessary performance and energy overhead. In this article, we propose four techniques to enable and optimize an LPDDRx-compatible MRAM solution: ComboAS to solve the pin incompatibility; DynLat to avoid unnecessary access latencies; and EarlyPA and BufW to further improve performance by exploiting the MRAM unique features of nondestructive read and independent write path. Combining all these techniques together, we boost the MRAM performance by 17% and provide a DRAM-compatible MRAM solution consuming 21% less energy.",Energy; LPDDR2; LPDDR3; Main memory; MRAM; Nonvolatile memory; Performance; Spin-transfer torque,Dynamic random access storage; Magnetic recording; Magnetic storage; Memory architecture; Nonvolatile storage; Energy; LPDDR2; LPDDR3; Main memory; MRAM; Non-volatile memory; Performance; Spin transfer torque; MRAM devices
Volatile STT-RAM scratchpad design and data allocation for low energy,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84917685948&doi=10.1145%2f2669556&partnerID=40&md5=8126318cf414d1dd09ca9bb778fd3195,"On-chip power consumption is one of the fundamental challenges of current technology scaling. Cache memories consume a sizable part of this power, particularly due to leakage energy. STT-RAM is one of several new memory technologies that have been proposed in order to improve power while preserving performance. It features high density and low leakage, but at the expense of write energy and performance. This article explores the use of STT-RAM-based scratchpad memories that trade nonvolatility in exchange for faster and less energetically expensive accesses, making them feasible for on-chip implementation in embedded systems. A novel multiretention scratchpad partitioning is proposed, featuring multiple storage spaces with different retention, energy, and performance characteristics. A customized compiler-based allocation algorithm suitable for use with such a scratchpad organization is described. Our experiments indicate that a multiretention STT-RAM scratchpad can provide energy savings of 53% with respect to an iso-area, hardware-managed SRAM cache.",Relaxed-retention; Scratchpad; STT-RAM,Embedded systems; Energy conservation; Memory architecture; Static random access storage; Allocation algorithm; Current technology; On-chip implementations; Performance characteristics; Relaxed-retention; Scratch pad memory; Scratchpad; Stt rams; Cache memory
Bones: An automatic skeleton-based C-to-CUDA compiler for GPUs,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84917682540&doi=10.1145%2f2665079&partnerID=40&md5=12dfe504d370648e4b1b019471e5b20a,"The shift toward parallel processor architectures has made programming and code generation increasingly challenging. To address this programmability challenge, this article presents a technique to fully automatically generate efficient and readable code for parallel processors (with a focus on GPUs). This is made possible by combining algorithmic skeletons, traditional compilation, and ""algorithmic species,"" a classification of program code. Compilation starts by automatically annotating C code with class information (the algorithmic species). This code is then fed into the skeleton-based source-to-source compiler BONES to generate CUDA code. To generate efficient code, BONES also performs optimizations including host-accelerator transfer optimization and kernel fusion. This results in a unique approach, integrating a skeleton-based compiler for the first time into an automated flow. The benefits are demonstrated experimentally for PolyBench GPU kernels, showing geometric mean speed-ups of 1.4× and 2.4× compared to PPCG and PAR4ALL, and for five Rodinia GPU benchmarks, showing a gap of only 1.2× compared to hand-optimized code.",Algorithmic skeletons; Compiler; CUDA; GPU; Parallel programming,Bone; Codes (symbols); Graphics processing unit; Musculoskeletal system; Parallel processing systems; Parallel programming; Program compilers; Program processors; Algorithmic skeleton; Class information; Code Generation; Compiler; CUDA; Geometric mean; Parallel processor; Programmability; C (programming language)
Revisiting the complexity of hardware cache coherence and some implications,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84917683190&doi=10.1145%2f2663345&partnerID=40&md5=cdceef5e67734eacfa7976b18e56ee3d,"Cache coherence is an integral part of shared-memory systems but is also widely considered to be one of the most complex parts of such systems. Much prior work has addressed this complexity and the verification techniques to prove the correctness of hardware coherence. Given the new multicore era with increasing number of cores, there is a renewed debate about whether the complexity of hardware coherence has been tamed or whether it should be abandoned in favor of software coherence. This article revisits the complexity of hardware cache coherence by verifying a publicly available, state-of-the-art implementation of the widely used MESI protocol, using the Murφ model checking tool. To our surprise, we found six bugs in this protocol, most of which were hard to analyze and took several days to fix. To compare the complexity, we also verified the recently proposed DeNovo protocol, which exploits disciplined software programming models. We found three relatively easy to fix bugs in this less mature protocol. After fixing these bugs, our verification experiments showed that, compared to DeNovo, MESI had 15X more reachable states leading to a 20X increase in verification (model checking) time. Although we were eventually successful in verifying the protocols, the tool required making several simplifying assumptions (e.g., two cores, one address). Our results have several implications: (1) they indicate that hardware coherence protocols remain complex; (2) they reinforce the need for protocol designers to embrace formal verification tools to demonstrate correctness of new protocols and extensions; (3) they reinforce the need for formal verification tools that are both scalable and usable by non-expert; and (4) they show that a system based on hardware-software co-design can offer a simpler approach for cache coherence, thus reducing the overall verification effort and allowing verification of more detailed models and protocol extensions that are otherwise limited by computing resources.",Cache coherence; Model checking; Multicore; Protocol verification; Shared memory; Software-hardware co-design,Formal verification; Hardware-software codesign; Memory architecture; Model checking; Cache Coherence; Co-designs; Multi core; Protocol verification; Shared memory; Cache memory
Improving hybrid FTL by fully exploiting internal SSD parallelism with virtual blocks,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84917686856&doi=10.1145%2f2677160&partnerID=40&md5=641e2394b9d29f89f25e697df4af86a1,"Compared with either block or page-mapping Flash Translation Layer (FTL), hybrid-mapping FTL for flash Solid State Disks (SSDs), such as Fully Associative Section Translation (FAST), has relatively high space efficiency because of its smaller mapping table than the latter and higher flexibility than the former. As a result, hybrid-mapping FTL has become the most commonly used scheme in SSDs. But the hybrid-mapping FTL incurs a large number of costly full-merge operations. Thus, a critical challenge to hybrid-mapping FTL is how to reduce the cost of full-merge operations and improve partial merge operations and switch operations. In this article, we propose a novel FTL scheme, called Virtual Block-based Parallel FAST (VBPFAST), that divides flash area into Virtual Blocks (VBlocks) and Physical Blocks (PBlocks) where VBlocks are used to fully exploit channel-level, die-level, and plane-level parallelism of flash. Leveraging these three levels of parallelism, the cost of full merge in VBP-FAST is significantly reduced from that of FAST. In the meantime, VBP-FAST uses PBlocks to retain the advantages of partial merge and switch operations. Our extensive trace-driven simulation results show that VBP-FAST speeds up FAST by a factor of 5.3-8.4 for random workloads and of 1.7 for sequential workloads with channel-level, die-level, and plane-level parallelism of 8, 2, and 2 (i.e., eight channels, two dies, and two planes).",FTL; NAND flash; Parallelism; SSD,Cost reduction; Flash memory; Mapping; Mergers and acquisitions; Critical challenges; Flash translation layer; NAND Flash; Parallelism; Solid state disks; Space efficiencies; Switch operation; Trace driven simulation; Flash-based SSDs
MAPS: Optimizing massively parallel applications using device-level memory abstraction,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84917736767&doi=10.1145%2f2680544&partnerID=40&md5=90c6371b33d51891879df58df4d35039,"GPUs play an increasingly important role in high-performance computing. While developing naive code is straightforward, optimizing massively parallel applications requires deep understanding of the underlying architecture. The developer must struggle with complex index calculations and manual memory transfers. This article classifies memory access patterns used in most parallel algorithms, based on Berkeley's Parallel ""Dwarfs."" It then proposes the MAPS framework, a device-level memory abstraction that facilitates memory access on GPUs, alleviating complex indexing using on-device containers and iterators. This article presents an implementation of MAPS and shows that its performance is comparable to carefully optimized implementations of real-world applications.",GPGPU; Heterogeneous computing architectures; Memory abstraction; Memory access patterns,Abstracting; Computer architecture; Optical projectors; Program processors; Complex indices; GPGPU; Heterogeneous computing; High performance computing; Massively parallels; Memory access; Memory access patterns; Optimized implementation; Memory architecture
Automatic and portable mapping of data parallel programs to OpenCL for GPU-based heterogeneous systems,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84917674918&doi=10.1145%2f2677036&partnerID=40&md5=bbc89b097faaf3358c210bc58de2d56f,"Machine-learning mapping General-purpose GPU-based systems are highly attractive, as they give potentially massive performance at little cost. Realizing such potential is challenging due to the complexity of programming. This article presents a compiler-based approach to automatically generate optimized OpenCL code from data parallel OpenMP programs for GPUs. A key feature of our scheme is that it leverages existing transformations, especially data transformations, to improve performance on GPU architectures and uses automatic machine learning to build a predictive model to determine if it is worthwhile running the OpenCL code on the GPU or OpenMP code on themulticore host.We applied our approach to the entireNAS parallel benchmark suite and evaluated it on distinct GPU-based systems.We achieved average (up to) speedups of 4.51× and 4.20× (143× and 67×) on Core i7/NVIDIA GeForce GTX580 and Core i7/AMD Radeon 7970 platforms, respectively, over a sequential baseline. Our approach achieves, on average, greater than 10× speedups over two state-of-the-art automatic GPU code generators.",GPU; OpenCL,Application programming interfaces (API); Artificial intelligence; Benchmarking; Codes (symbols); Graphics processing unit; Learning systems; Mapping; Program processors; Data parallel programs; Data transformation; General purpose gpu; Heterogeneous systems; Improve performance; OpenCL; Parallel benchmarks; Predictive modeling; Program compilers
Efficient data mapping and buffering techniques for multilevel cell phase-change memories,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84917729568&doi=10.1145%2f2669365&partnerID=40&md5=b792eab56aff1d341a411f5bc34e5412,"New phase-change memory (PCM) devices have low-access latencies (like DRAM) and high capacities (i.e., low cost per bit, like Flash). In addition to being able to scale to smaller cell sizes than DRAM, a PCM cell can also store multiple bits per cell (referred to as multilevel cell, or MLC), enabling even greater capacity per bit. However, reading and writing the different bits of data from and to an MLC PCM cell requires different amounts of time: one bit is read or written first, followed by another. Due to this asymmetric access process, the bits in an MLC PCM cell have different access latency and energy depending on which bit in the cell is being read or written. We leverage this observation to design a new way to store and buffer data in MLC PCM devices. While traditional devices couple the bits in each cell next to one another in the address space, our key idea is to logically decouple the bits in each cell into two separate regions depending on their read/write characteristics: fast-read/slow-write bits and slow-read/fast-write bits. We propose a low-overhead hardware/software technique to predict and map data that would benefit from being in each region at runtime. In addition, we show how MLC bit decoupling provides more flexibility in the way data is buffered in the device, enabling more efficient use of existing device buffer space. Our evaluations for a multicore system show that MLC bit decoupling improves system performance by 19.2%, memory energy efficiency by 14.4%, and thread fairness by 19.3% over a state-of-the-art MLC PCM system that couples the bits in its cells.We show that our results are consistent across a variety of workloads and system configurations.",Data buffering; Data mapping; Energy; Main memory; Multilevel cell; Performance; Phase-change memory,Cells; Cytology; Dynamic random access storage; Energy efficiency; Flash memory; Mapping; Data buffering; Data mappings; Energy; Main memory; Multilevel cell; Performance; Phase change memory
Faster fully compressed pattern matching by recompression,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921469943&doi=10.1145%2f2631920&partnerID=40&md5=02d71666f843c3fc7c75111c58bddf10,"In this article, a fully compressed pattern matching problem is studied. The compression is represented by straight-line programs (SLPs)-that is, context-free grammars generating exactly one string; the term fully means that both the pattern and the text are given in the compressed form. The problem is approached using a recently developed technique of local recompression: the SLPs are refactored so that substrings of the pattern and text are encoded in both SLPs in the same way. To this end, the SLPs are locally decompressed and then recompressed in a uniform way. This technique yields an O((n+ m) log M) algorithm for compressed pattern matching, assuming that M fits in O(1) machine words, where n (m) is the size of the compressed representation of the text (pattern, respectively), and M is the size of the decompressed pattern. If only m+ n fits in O(1) machine words, the running time increases to O((n+m) log Mlog(n+m)). The previous best algorithm due to Lifshits has O(n2m) running time. © 2015 ACM.",Algorithms; Languages; Theory,Algorithms; Context free grammars; Query languages; Compressed pattern matching; Recompression; Running time; Straight line program; Sub-strings; Theory; Pattern matching
Automated fine-grained CPU provisioning for virtual machines,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910136624&doi=10.1145%2f2637480&partnerID=40&md5=22686c82fa9bd068783768676037a66c,"Ideally, the pay-as-you-go model of Infrastructure as a Service (IaaS) clouds should enable users to rent just enough resources (e.g., CPU or memory bandwidth) to fulfill their service level objectives (SLOs). Achieving this goal is hard on current IaaS offers, which require users to explicitly specify the amount of resources to reserve; this requirement is nontrivial for users, because estimating the amount of resources needed to attain application-level SLOs is often complex, especially when resources are virtualized and the service provider colocates virtual machines (VMs) on host nodes. For this reason, users who deploy VMs subject to SLOs are usually prone to overprovisioning resources, thus resulting in inflated business costs. This article tackles this issue with AutoPro: a runtime system that enhances IaaS clouds with automated and fine-grained resource provisioning based on performance SLOs. Our main contribution with AutoPro is filling the gap between application-level performance SLOs and allocation of a contended resource, without requiring explicit reservations from users. In this article, we focus on CPU bandwidth allocation to throughput-driven, compute-intensive multithreaded applications colocated on a multicore processor; we show that a theoretically sound, yet simple, control strategy can enable automated fine-grained allocation of this contended resource, without the need for offline profiling. Additionally, AutoPro helps service providers optimize infrastructure utilization by provisioning idle resources to best-effort workloads, so as to maximize node-level utilization. Our extensive experimental evaluation confirms that AutoPro is able to automatically determine and enforce allocations to meet performance SLOs while maximizing node-level utilization by supporting batch workloads on a best-effort basis. © 2014 ACM.",Cloud computing; Performance management; Resource allocation; Virtualization,Automation; Bandwidth; Cloud computing; Network security; Resource allocation; Virtual machine; Virtualization; Application-level performance; Control strategies; Experimental evaluation; Memory bandwidths; Multi-core processor; Multi-threaded application; Performance management; Service level objective; Infrastructure as a service (IaaS)
Interval deletion is fixed-parameter tractable,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921490678&doi=10.1145%2f2629595&partnerID=40&md5=b42f4a8147ef3f4cae913f2c521a16c1,"We study the minimum interval deletion problem, which asks for the removal of a set of at most κ vertices to make a graph of n vertices into an interval graph. We present a parameterized algorithm of runtime 10κ · nO(1) for this problem-that is, we show that the problem is fixed-parameter tractable. © 2015 ACM.",Algorithms,Algorithms; Software engineering; Interval graph; Parameterized algorithm; Runtimes; Graph structures
Minimum makespan multi-vehicle dial-a-ride,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921471565&doi=10.1145%2f2629653&partnerID=40&md5=2875bd9ecaea49d6526b3d700e57a560,"Dial-a-Ride problems consist of a set V of n vertices in a metric space (denoting travel time between vertices) and a set of m objects represented as source-destination pairs {(si, ti )}i=1m, where each object requires to be moved from its source to destination vertex. In the multi-vehicle Dial-a-Ride problem, there are q vehicles, each having capacity κ and where each vehicle j ε [q] has its own depot-vertex rj ε V. A feasible schedule consists of a capacitated route for each vehicle (where vehicle j originates and ends at its depot rj) that together move all objects from their sources to destinations. The objective is to find a feasible schedule that minimizes the maximum completion time (i.e., makespan) of vehicles, where the completion time of vehicle j is the time when it returns to its depot rj at the end of its route. We study the preemptive version of multivehicle Dial-a-Ride, in which an object may be left at intermediate vertices and transported by more than one vehicle, while being moved from source to destination. Our main results are an O(log3 n)-approximation algorithm for preemptive multi-vehicle Dial-a-Ride, and an improved O(log t)-approximation for its special case when there is no capacity constraint (here t ≤ n is the number of distinct depot-vertices). There is an Ω(log1/4-ε n) hardness of approximation known even for single vehicle capacitated Dial-a-Ride [Gørtz 2006]. For uncapacitated multi-vehicle Dial-a-Ride, we show that there are instances when natural lower bounds (used in our algorithm) areΩ(log t) factor away from the optimum. We also consider the special class of metrics induced by graphs excluding any fixed minor (e.g., planar metrics). In this case, we obtain improved guarantees of O(log2 n) for capacitated multi-vehicle Dial-a-Ride, and O(1) for the uncapacitated problem. © 2015 ACM.",Algorithms,Algorithms; Approximation algorithms; Travel time; Capacity constraints; Completion time; Dial-a-ride problem; Feasible schedule; Hardness of approximation; Minimum makespan; Natural lower bounds; Source-destination pairs; Vehicles
Min st-cut oracle for planar graphs with near-linear preprocessing time,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921523415&doi=10.1145%2f2684068&partnerID=40&md5=7b02aea3a5b558e23efaa2476076e680,"For an undirected n-vertex planar graph G with nonnegative edge weights, we consider the following type of query: given two vertices s and t in G, what is the weight of a min st-cut in G? We show how to answer such queries in constant time with O(n log4 n) preprocessing time and O(n log n) space. We use a Gomory-Hu tree to represent all the pairwise min cuts implicitly. Previously, no subquadratic time algorithm was known for this problem. Since all-pairs min cut and the minimum-cycle basis are dual problems in planar graphs, we also obtain an implicit representation of a minimum-cycle basis in O(n log4 n) time and O(n log n) space. Additionally, an explicit representation can be obtained in O(C) time and space where C is the size of the basis. These results require that shortest paths are unique. This can be guaranteed either by using randomization without overhead or deterministically with an additional log2 n factor in the preprocessing times. © 2015 ACM.",Algorithms,Algorithms; Graphic methods; Query processing; Constant time; Explicit representation; Implicit representation; Minimum cycle basis; Non negatives; Preprocessing time; Shortest path; Time algorithms; Graph theory
Hardware fault recovery for I/O intensive applications,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910153509&doi=10.1145%2f2656342&partnerID=40&md5=43264e0e2312d335debd7672797c2503,"With continued process scaling, the rate of hardware failures in commodity systems is increasing. Because these commodity systems are highly sensitive to cost, traditional solutions that employ heavy redundancy to handle such failures are no longer acceptable owing to their high associated costs. Detecting such faults by identifying anomalous software execution and recovering through checkpoint-and-replay is emerging as a viable low-cost alternative for future commodity systems. An important but commonly ignored aspect of such solutions is ensuring that external outputs to the system are fault-free. The outputs must be delayed until the detectors guarantee this, influencing fault-free performance. The overheads for resiliency must thus be evaluated while taking these delays into consideration; prior work has largely ignored this relationship. This article concerns recovery for I/O intensive applications from in-core faults. We present a strategy to buffer external outputs using dedicated hardware and show that checkpoint intervals previously considered as acceptable incur exorbitant overheads when hardware buffering is considered. We then present two techniques to reduce the checkpoint interval and demonstrate a practical solution that provides high resiliency while incurring low overheads. © 2014 ACM.",Fault tolerance; Hardware reliability; I/O recovery,Electric fault currents; Fault tolerance; Associated costs; Checkpoint intervals; Commodity systems; Dedicated hardware; Hardware failures; Hardware reliability; Practical solutions; Software execution; Recovery
Preventing STT-RAM Last-Level Caches from Port Obstruction,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910129352&doi=10.1145%2f2633046&partnerID=40&md5=2b0f2795173000d5c6598b0a24a57bd2,"Many new nonvolatile memory (NVM) technologies have been heavily studied to replace the power-hungry SRAM/DRAM-based memory hierarchy in today's computers. Among various emerging NVM technologies, Spin-Transfer Torque RAM (STT-RAM) has many benefits, such as fast read latency, low leakage power, and high density, making it a promising candidate for last-level caches (LLCs).1 However, STT-RAM write operation is expensive. In particular, a long STT-RAM cache write operation might obstruct other cache accesses and result in severe performance degradation. Consequently, how to mitigate STT-RAM write overhead is critical to the success of STT-RAM adoption. In this article, we propose an obstruction-aware cache management policy called OAP. OAP monitors cache traffic, detects LLC-obstructive processes, and differentiates the cache accesses from different processes. Our experiment on a four-core architecture with an 8MB STT-RAM L3 cache shows a 14% performance improvement and 64% energy reduction. © 2014 ACM.",Last-level caches; Nonvolatile memory; Performance improvement; Port obstruction; STT-RAM,Memory architecture; Nonvolatile storage; Static random access storage; Last-level caches; Non-volatile memory; Performance improvement; Port obstruction; Stt rams; Cache memory
Efficient power gating of SIMD accelerators through dynamic selective devectorization in an HW/SW codesigned environment,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910140436&doi=10.1145%2f2629681&partnerID=40&md5=42ee637b4807d1fddd09b9029c248ddb,"Leakage energy is a growing concern in current and future microprocessors. Functional units of microprocessors are responsible for a major fraction of this energy. Therefore, reducing functional unit leakage has received much attention in recent years. Power gating is one of the most widely used techniques to minimize leakage energy. Power gating turns off the functional units during the idle periods to reduce the leakage. Therefore, the amount of leakage energy savings is directly proportional to the idle time duration. This article focuses on increasing the idle interval for the higher SIMD lanes. The applications are profiled dynamically, in a hardware/software codesigned environment, to find the higher SIMD lanes' usage pattern. If the higher lanes need to be turned on for small time periods, the corresponding portion of the code is devectorized to keep the higher lanes off. The devectorized code is executed on the lowest SIMD lane. Our experimental results show that the average energy savings of the proposed mechanism are 15%, 12%, and 71% greater than power gating for SPECFP2006, Physicsbench, and Eigen benchmark suites, respectively. Moreover, the slowdown caused by devectorization is negligible. © 2014 ACM.",Devectorization; Hardware/software codesigned processors; Leakage; Power gating,Energy conservation; Leakage (fluid); Benchmark suites; Co-designed processors; Devectorization; Efficient power; Functional units; Hardware/software; Leakage energies; Power gatings; Hardware-software codesign
A polynomial-time approximation scheme for euclidean steiner forest,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921515746&doi=10.1145%2f2629654&partnerID=40&md5=0d163a58eaf7649fc7ccb9cd571b02a8,"We give a randomized O(n polylog n)-time approximation scheme for the Steiner forest problem in the Euclidean plane. For every fixed ε > 0 and given n terminals in the plane with connection requests between some pairs of terminals, our scheme finds a (1 + ε) approximation to the minimum-length forest that connects every requested pair of terminals. © 2015 ACM.",Algorithms; Theory,Algorithms; Forests; Problem Solving; Algorithms; Forestry; Geometry; Approximation scheme; Euclidean; Euclidean planes; N-terminals; Polynomial time approximation schemes; Steiner forest problem; Steiner forests; Theory; Polynomial approximation
Average case and distributional analysis of dual-pivot quicksort,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921481981&doi=10.1145%2f2629340&partnerID=40&md5=404ad8685081954a36e298477d4259a8,"In 2009, Oracle replaced the long-serving sorting algorithm in its Java 7 runtime library by a new dual-pivot Quicksort variant due to Vladimir Yaroslavskiy. The decision was based on the strikingly good performance of Yaroslavskiy's implementation in running time experiments. At that time, no precise investigations of the algorithm were available to explain its superior performance - on the contrary: previous theoretical studies of other dual-pivot Quicksort variants even discouraged the use of two pivots. In 2012, two of the authors gave an average case analysis of a simplified version of Yaroslavskiy's algorithm, proving that savings in the number of comparisons are possible. However, Yaroslavskiy's algorithm needs more swaps, which renders the analysis inconclusive. To force the issue, we herein extend our analysis to the fully detailed style of Knuth: we determine the exact number of executed Java Bytecode instructions. Surprisingly, Yaroslavskiy's algorithm needs sightly more Bytecode instructions than a simple implementation of classic Quicksort - contradicting observed running times. As in Oracle's library implementation, we incorporate the use of Insertionsort on small subproblems and show that it indeed speeds up Yaroslavskiy's Quicksort in terms of Bytecodes; but even with optimal Insertionsort thresholds, the new Quicksort variant needs slightly more Bytecode instructions on average. Finally, we show that the (suitably normalized) costs of Yaroslavskiy's algorithm converge to a random variable whose distribution is characterized by a fixed-point equation. From that, we compute variances of costs and show that for large n, costs are concentrated around their mean. © 2015 ACM.",Algorithms; Design; Theory,Algorithms; Design; Software engineering; Average-case analysis; Fixed point equation; Java byte codes; Run-time library; Running time; Sorting algorithm; Theoretical study; Theory; Java programming language
Topology-aware and dependence-aware scheduling and memory allocation for task-parallel languages,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910119210&doi=10.1145%2f2641764&partnerID=40&md5=afd98e4d283603ada0bebb5a04ebfe17,"We present a joint scheduling and memory allocation algorithm for efficient execution of task-parallel programs on non-uniform memory architecture (NUMA) systems. Task and data placement decisions are based on a static description of the memory hierarchy and on runtime information about intertask communication. Existing locality-aware scheduling strategies for fine-grained tasks have strong limitations: they are specific to some class of machines or applications, they do not handle task dependences, they require manual program annotations, or they rely on fragile profiling schemes. By contrast, our solution makes no assumption on the structure of programs or on the layout of data in memory. Experimental results, based on the Open-Stream language, show that locality of accesses to main memory of scientific applications can be increased significantly on a 64-core machine, resulting in a speedup of up to 1.63x compared to a state-of-the-art work-stealing scheduler. © 2014 ACM.",Dataflow programming; Dynamic scheduling; FIFO queue; Kahn process network; Lock-free algorithm; Weak memory model; Work stealing,Application programs; Computer hardware description languages; Data flow analysis; Memory architecture; Scheduling; Dataflow programming; Dynamic scheduling; FIFO queue; Kahn process networks; Lock-free algorithms; Weak memory models; Work stealing; Multitasking
Maximizing the minimum load for random processing times,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921518500&doi=10.1145%2f2651421&partnerID=40&md5=5b29e9e6f19b9206b04dc823983e24ec,"In this article, we consider a stochastic variant of the so-called Santa Claus problem. The Santa Claus problem is equivalent to the problem of scheduling a set of n jobs on m parallel machines without preemption, so as to maximize the minimum load. We consider the identical machine version of this scheduling problem with the additional restriction that the scheduler has only a guess of the processing times; that is, the processing time of a job is a random variable. We show that there is a critical value ρ(n, m) such that if the duration of the jobs is exponentially distributed and the expected values deviate by less than a multiplicative factor of ρ(n, m) from each other, then a greedy algorithm has an expected competitive ratio arbitrarily close to one; that is, it performs in expectation almost as good as an algorithm that knows the actual values in advance. On the other hand, if the expected values deviate by more than a multiplicative factor of ρ(n, m), then the expected performance is arbitrarily bad for all algorithms. © 2015 ACM.",Algorithms; Theory,Algorithms; Stochastic systems; Competitive ratio; Greedy algorithms; Identical machines; Multiplicative factors; Parallel machine; Random processing time; Scheduling problem; Theory; Scheduling
NUCA-L1: A non-uniform access latency level-1 cache architecture for multicores operating at near-threshold voltages,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910132742&doi=10.1145%2f2631918&partnerID=40&md5=44c3f496f2c2417a3418b3edb0392b62,"Research has shown that operating in the near-threshold region is expected to provide up to 10x energy efficiency for future processors. However, reliable operation below a minimum voltage (Vccmin) cannot be guaranteed due to process variations. Because SRAM margins can easily be violated at near-threshold voltages, their bit-cell failure rates are expected to rise steeply. Multicore processors rely on fast private L1 caches to exploit data locality and achieve high performance. In the presence of high bit-cell fault rates, traditionally an L1 cache either sacrifices capacity or incurs additional latency to correct the faults. We observe that L1 cache sensitivity to hit latency offers a design trade-off between capacity and latency. When fault rate is high at extreme Vccmin, it is beneficial to recover L1 cache capacity, even if it comes at the cost of additional latency. However, at low fault rates, the additional constant latency to recover cache capacity degrades performance. With this trade-off in mind, we propose a Non-Uniform Cache Access L1 architecture (NUCA-L1) that avoids additional latency on accesses to fault-free cache lines. To mitigate the capacity bottleneck, it deploys a correction mechanism to recover capacity at the cost of additional latency. Using extensive simulations of a 64-core multicore, we demonstrate that at various bit-cell fault rates, our proposed private NUCA-L1 cache architecture performs better than state-of-the-art schemes, along with a significant reduction in energy consumption. © 2014 ACM.",NTV,Architecture; Cache memory; Economic and social effects; Energy efficiency; Energy utilization; Failure analysis; Recovery; Cache architecture; Correction mechanism; Extensive simulations; Multi-core processor; Non-uniform cache access; Reduction in energy consumption; Reliable operation; State-of-the-art scheme; Threshold voltage
Time complexity of link reversal routing,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921531259&doi=10.1145%2f2644815&partnerID=40&md5=385b02caf444fe9d4b38c55711c6a325,"Link reversal is a versatile algorithm design paradigm, originally proposed by Gafni and Bertsekas in 1981 for routing and subsequently applied to other problems including mutual exclusion, leader election, and resource allocation. Although these algorithms are well known, until now there have been only preliminary results on time complexity, even for the simplest link reversal algorithm for routing, called Full Reversal. In Full Reversal, a sink reverses all its incident links, whereas in other link reversal algorithms (e.g., Partial Reversal), a sink reverses only some of its incident links. Charron-Bost et al. introduced a generalization, called LR, that includes Full and Partial Reversal as special cases. In this article, we present an exact expression for the time complexity of LR. The expression is stated in terms of simple properties of the initial graph. The result specializes to exact formulas for the time complexity of any node in any initial acyclic directed graph for both Full and Partial Reversal. Having the exact formulas provides insight into the behavior of Full and Partial Reversal on specific graph families. Our first technical insight is to describe the behavior of Full Reversal as a dynamical system and to observe that this system is linear in min-plus algebra. Our second technical insight is to overcome the difficulty posed by the fact that LR is not linear by transforming every execution of LR from an initial graph into an execution of Full Reversal from a different initial graph while maintaining the execution's work and time complexity. © 2015 ACM.",Algorithms; Theory,Algorithms; Dynamical systems; Acyclic directed graph; Algorithm design; Exact formulas; Leader election; Min-plus algebra; Mutual exclusions; Theory; Time complexity; Directed graphs
FLARES: An aging aware algorithm to autonomously adapt the error correction capability in NAND flash memories,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910137521&doi=10.1145%2f2631919&partnerID=40&md5=35a9db54bf8391647145d87afca4b3db,"With the advent of solid-state storage systems, NAND flashmemories are becoming a key storage technology. However, they suffer from serious reliability and endurance issues during the operating lifetime that can be handled by the use of appropriate error correction codes (ECCs) in order to reconstruct the information when needed. Adaptable ECCs may provide the flexibility to avoid worst-case reliability design, thus leading to improved performance. However, a way to control such adaptable ECCs' strength is required. This article proposes FLARES, an algorithm able to adapt the ECC correction capability of each page of a flash based on a flash RBER prediction model and on a measurement of the number of errors detected in a given time window. FLARES has been fully implemented within the YAFFS 2 filesystem under the Linux operating system. This allowed us to perform an extensive set of simulations on a set of standard benchmarks that highlighted the benefit of FLARES on the overall storage subsystem performances. © 2014 ACM.",Adaptable ECC; BCH codes; Error-correcting codes; NAND flash memory,Computer operating systems; Error correction; Memory architecture; NAND circuits; Predictive analytics; Adaptable ECC; BCH code; Error correcting code; Error correction capability; Error correction codes (ECCs); LINUX- operating system; NAND flash memory; Solid state storage systems; Flash memory
Multiprogram throughput metrics: A systematic approach,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910129710&doi=10.1145%2f2663346&partnerID=40&md5=108fcf144188e7b652501060478f2113,"Running multiple programs on a processor aims at increasing the throughput of that processor. However, defining meaningful throughput metrics in a simulation environment is not as straightforward as reporting execution time. This has led to an ongoing debate on what forms a meaningful throughput metric for multiprogram workloads. We present a method to construct throughput metrics in a systematic way: we start by expressing assumptions on job size, job distribution, scheduling, and so forth that together define a theoretical throughput experiment. The throughput metric is then the average throughput of this experiment. Different assumptions lead to different metrics, so one should be aware of these assumptions when making conclusions based on results using a specific metric. Throughput metrics should always be defined from explicit assumptions, because this leads to a better understanding of the implications and limits of the results obtained with that metric. We elaborate multiple metrics based on different assumptions. In particular, we identify the assumptions that lead to the commonly used weighted speedup and harmonic mean of speedups. Our study clarifies that they are actual throughput metrics, which was recently questioned. We also propose some new throughput metrics, which cannot always be expressed as a closed formula. We use real experimental data to characterize metrics and show how they relate to each other. © 2014 ACM.",Multi-program workloads; Multicore; Simultaneous multithreading; Throughput metrics,Multicore programming; Actual throughputs; Average throughput; Multi core; Multi-program workloads; Multiple program; Simulation environment; Simultaneous multi-threading; Throughput metrics; Multitasking
Effective transactional memory execution management for improved concurrency,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910129159&doi=10.1145%2f2633048&partnerID=40&md5=46381b70a55e1a4215f329231fcce047,"This article describes a transactional memory execution model intended to exploit maximum parallelism from sequential and multithreaded programs. A program code section is partitioned into chunks that will be mapped onto threads and executed transactionally. These transactions run concurrently and out of order, trying to exploit maximum parallelism but managed by a specific fully distributed commit control to meet data dependencies. To accomplish correct parallel execution, a partial precedence order relation is derived from the program code section and/or defined by the programmer. When a conflict between chunks is eagerly detected, the precedence order relation is used to determine the best policy to solve the conflict that preserves the precedence order while maximizing concurrency. The model defines a new transactional state called executed but not committed. This state allows exploiting concurrency on two levels: intrathread and interthread. Intrathread concurrency is improved by having pending uncommitted transactions while executing a new one in the same thread. The new state improves interthread concurrency because it permits out-of-order transaction commits regarding the precedence order. Our model has been implemented in a lightweight software transactionalmemory system, TinySTM, and has been evaluated on a set of benchmarks obtaining an important performance improvement over the baseline TM system. © 2014 ACM.",Concurrency exploitation; Dependence analysis; Optimistic concurrency; Program parallelization; Transactional memory,Benchmarking; Codes (symbols); Storage allocation (computer); Concurrency exploitation; Dependence analysis; Optimistic concurrency; Program parallelization; Transactional memory; Concurrency control
Exploiting existing comparators for fine-grained low-cost error detection,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910150559&doi=10.1145%2f2656341&partnerID=40&md5=284239933019be3298bca075dc5da895,"Fault tolerance has become a fundamental concern in computer design, in addition to performance and power. Although several error detection schemes have been proposed to discover a faulty core in the system, these proposals could waste the whole core, including many error-free structures in it after error detection. Moreover, many fault-tolerant designs require additional hardware for data replication or for comparing the replicated data. In this study, we provide a low-cost, fine-grained error detection scheme by exploiting already existing comparators and data replications in the several pipeline stages such as issue queue, rename logic, and translation lookaside buffer. We reduce the vulnerability of the source register tags in IQ by 60%, the vulnerability of instruction TLB by 64%, the vulnerability of data TLB by 45%, and the vulnerability of the register tags of rename logic by 20%. © 2014 ACM.",Additional key words and phrases; CAM logic; Reliability,Buffer storage; Comparator circuits; Comparators (optical); Computer circuits; Costs; Fault tolerance; Reliability; Computer designs; Data replication; Detection scheme; Fault tolerant design; Free structures; Key words; Replicated data; Translation lookaside buffer; Error detection
An evaluation of high-level mechanistic core models,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910130274&doi=10.1145%2f2629677&partnerID=40&md5=de5acda3db9b3de7f0ee7762bf30cffb,"Large core counts and complex cache hierarchies are increasing the burden placed on commonly used simulation and modeling techniques. Although analytical models provide fast results, they do not apply to complex, many-core shared-memory systems. In contrast, detailed cycle-level simulation can be accurate but also tends to be slow, which limits the number of configurations that can be evaluated. A middle ground is needed that provides for fast simulation of complex many-core processors while still providing accurate results. In this article, we explore, analyze, and compare the accuracy and simulation speed of high-abstraction core models as a potential solution to slow cycle-level simulation. We describe a number of enhancements to interval simulation to improve its accuracy while maintaining simulation speed. In addition, we introduce the instruction-window centric (IW-centric) core model, a new mechanistic core model that bridges the gap between interval simulation and cycle-accurate simulation by enabling high-speed simulations with higher levels of detail. We also show that using accurate core models like these are important for memory subsystem studies, and that simple, naivemodels, like a one-IPC coremodel, can lead to misleading and incorrect results and conclusions in practical design studies. Validation against real hardware shows good accuracy, with an average single-core error of 11.1% and a maximum of 18.8% for the IW-centric model with a 1.5% slowdown compared to interval simulation. © 2014 ACM.",Design space exploration; Interval model; Interval simulation; Multicore processor; Performance modeling,Software engineering; Design space exploration; Interval models; Interval simulation; Multi-core processor; Performance Model; Memory architecture
EFGR: An enhanced Fine Granularity Refresh feature for high-performance DDR4 DRAM devices,2014,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910119114&doi=10.1145%2f2656340&partnerID=40&md5=44c09b23a1b99706545bf0129d98a304,"High-density DRAM devices spend significant time refreshing the DRAM cells, leading to performance drop. The JEDEC DDR4 standard provides a Fine Granularity Refresh (FGR) feature to tackle refresh. Motivated by the observation that in FGR mode, only a few banks are involved, we propose an Enhanced FGR (EFGR) feature that introduces three optimizations to the basic FGR feature and exposes the bank-level parallelism within the rank even during the refresh. The first optimization decouples the nonrefreshing banks. The second and third optimizations determine the maximum number of nonrefreshing banks that can be active during refresh and selectively precharge the banks before refresh, respectively. Our simulation results show that the EFGR feature is able to recover almost 56.6% of the performance loss incurred due to refresh operations. © 2014 ACM.",Activation energy; DDR4; DRAM architecture; Fine-Granularity Refresh; Partial rank refresh; Precharge; Refresh; Selective precharge,Software engineering; DDR4; Fine granularity; Partial rank refresh; Pre-charge; Refresh; Selective precharge; Activation energy
Reducing Instruction Fetch Energy in Multi-Issue Processors,2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025402752&doi=10.1145%2f2541228.2555318&partnerID=40&md5=0372e3401b584549b41ec98058c6a819,"The need to minimize power while maximizing performance has led to recent developments of powerful superscalar designs targeted at embedded and portable use. Instruction fetch is responsible for a significant fraction of microprocessor power and energy, and is therefore an attractive target for architectural power optimization.We present novel techniques that take advantage of guarantees so that the instruction translation lookaside buffer, branch target buffer, and branch prediction buffer can frequently be disabled, reducing their energy usage, while simultaneously reducing branch predictor contention. These techniques require no changes to the instruction set and can easily be integrated into most single- and multiple-issue processors. © 2014, ACM. All rights reserved.",Branch Prediction Buffer; Branch Target Buffer; Design; Experimentation; Instruction Translation Lookaside Buffer; Measurement,
Accelerating an Application Domain with Specialized Functional Units,2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025411620&doi=10.1145%2f2541228.2555303&partnerID=40&md5=153ee55df6412d32bcbde0de21762194,"Hardware specialization has received renewed interest recently as chips are hitting power limits. Chip designers of traditional processor architectures have primarily focused on general-purpose computing, partially due to time-to-market pressure and simpler design processes. But new power limits require some chip specialization. Although hardware configured for a specific application yields large speedups for low-power dissipation, its design is more complex and less reusable. We instead explore domain-based specialization, a scalable approach that balances hardware's reusability and performance efficiency. We focus on specialization using customized compute units that accelerate particular operations. In this article, we develop automatic techniques to identify code sequences from different applications within a domain that can be targeted to a new custom instruction that will be run inside a configurable specialized functional unit (SFU). We demonstrate that using a canonical representation of computations finds more common code sequences among applications that can be mapped to the same custom instruction, leading to larger speedups while specializing a smaller core area than previous pattern-matching techniques. We also propose new heuristics to narrow the search space of domain-specific custom instructions, finding those that achieve the best performance across applications. We estimate the overall performance achieved with our automatic techniques using hardware models on a set of nine media benchmarks, showing that when limiting the core area devoted to specialization, the SFU customization with the largest speedups includes both application- and domain-specific custom instructions. We demonstrate that exploring domain-specific hardware acceleration is key to continued computing system performance improvements. © 2014, ACM. All rights reserved.",Acceleration; Application-Specific; Canonical Representation; Customization; Design; Domain-Specific; Experimentation; Measurement; Performance; Specialized Functional Unit,
Revisiting Memory Management on Virtualized Environments,2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025420173&doi=10.1145%2f2541228.2555305&partnerID=40&md5=d51f26dc0cf4942350c37e419ab495e6,"With the evolvement of hardware, 64-bit Central Processing Units (CPUs) and 64-bit Operating Systems (OSs) have dominated the market. This article investigates the performance of virtual memory management of Virtual Machines (VMs) with a large virtual address space in 64-bit OSs, which imposes different pressure on memory virtualization than 32-bit systems. Each of the two conventional memory virtualization approaches, Shadowing Paging (SP) and Hardware-Assisted Paging (HAP), causes different overhead for different applications. Our experiments show that 64-bit applications prefer to run in a VM using SP, while 32-bit applications do not have a uniform preference between SP and HAP. In this article, we trace this inconsistency between 32-bit applications and 64-bit applications to its root cause through a systematic empirical study in Linux systems and discover that the major overhead of SP results from memory management in the 32-bit GNU C library (glibc). We propose enhancements to the existing memory management algorithms, which substantially reduce the overhead of SP. Based on the evaluations using SPEC CPU2006, Parsec 2.1, and cloud benchmarks, our results show that SP, with the improved memory allocators, can compete with HAP in almost all cases, in both 64-bit and 32-bit systems. We conclude that without a significant breakthrough in HAP, researchers should pay more attention to SP, which is more flexible and cost effective. © 2014, ACM. All rights reserved.",Algorithms; Design; Hardware-Assisted Paging; Memory Allocation; Performance; Shadowing Paging; Virtualization,
Analysis of Dependence Tracking Algorithms for Task Dataflow Execution,2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025387323&doi=10.1145%2f2541228.2555316&partnerID=40&md5=815a3cb6b2564169574feca2e366f298,"Processor architectures has taken a turn toward many-core processors, which integrate multiple processing cores on a single chip to increase overall performance, and there are no signs that this trend will stop in the near future. Many-core processors are harder to program than multicore and single-core processors due to the need for writing parallel or concurrent programs with high degrees of parallelism. Moreover, many-cores have to operate in a mode of strong scaling because of memory bandwidth constraints. In strong scaling, increasingly finer-grain parallelism must be extracted in order to keep all processing cores busy. Task dataflow programming models have a high potential to simplify parallel programming because they alleviate the programmer from identifying precisely all intertask dependences when writing programs. Instead, the task dataflow runtime system detects and enforces intertask dependences during execution based on the description of memory accessed by each task. The runtime constructs a task dataflow graph that captures all tasks and their dependences. Tasks are scheduled to execute in parallel, taking into account dependences specified in the task graph. Several papers report important overheads for task dataflow systems, which severely limits the scalability and usability of such systems. In this article, we study efficient schemes to manage task graphs and analyze their scalability. We assume a programming model that supports input, output, and in/out annotations on task arguments, as well as commutative in/out and reductions. We analyze the structure of task graphs and identify versions and generations as key concepts for efficient management of task graphs. Then, we present three schemes to manage task graphs building on graph representations, hypergraphs, and lists. We also consider a fourth edgeless scheme that synchronizes tasks using integers. Analysis using microbenchmarks shows that the graph representation is not always scalable and that the edgeless scheme introduces least overhead in nearly all situations. © 2014, ACM. All rights reserved.",Algorithms; Design; Performance; Scheduling; Task Dataflow,
Fast Pattern-Specific Routing for Fat Tree Networks,2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025406453&doi=10.1145%2f2541228.2555293&partnerID=40&md5=9bf04de85afa80788f985c9a54d5e838,"In the context of eXtended Generalized Fat Tree (XGFT) topologies, widely used in HPC and datacenter network designs, we propose a generic method, based on Integer Linear Programming (ILP), to efficiently determine optimal routes for arbitrary workloads. We propose a novel approach that combines ILP with dynamic programming, effectively reducing the time to solution. Specifically, we divide the network into smaller subdomains optimized using a custom ILP formulation that ensures global optimality of local solutions. Local solutions are then combined into an optimal global solution using dynamic programming. Finally, we demonstrate through a series of extensive benchmarks that our approach scales in practice to networks interconnecting several thousands of nodes, using a single-threaded, freely available linear programming solver on commodity hardware, with the potential for higher scalability by means of commercial, parallel solvers. © 2014, ACM. All rights reserved.",Algorithms; Design; Experimentation; Extended generalized fat trees; integer linear programming; optimal routing; Performance; Theory,
"Easy, Fast, and Energy-Efficient Object Detection on Heterogeneous On-Chip Architectures",2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025407940&doi=10.1145%2f2541228.2555302&partnerID=40&md5=21e3ab997c9fee2aaf84c78e27d06f84,"We optimize a visual object detection application (that uses Vision Video Library kernels) and show that OpenCL is a unified programming paradigm that can provide high performance when running on the Ivy Bridge heterogeneous on-chip architecture.We evaluate different mapping techniques and show that running each kernel where it fits the best and using software pipelining can provide 1.91 times higher performance and 42% better energy efficiency. We also show how to trade accuracy for energy at runtime. Overall, our application can perform accurate object detection at 40 frames per second (fps) in an energy-efficient manner. © 2014, ACM. All rights reserved.",Design; Energy Efficiency; Heterogeneous On-Chip Architectures; Languages; Opencl; Performance; Portable (Mobile) Devices; Simd,
Reducing DRAM Row Activations with Eager Read/Write Clustering,2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025397004&doi=10.1145%2f2541228.2555300&partnerID=40&md5=02bdcc75286c472e916983dbda4a2f08,"This article describes and evaluates a new approach to optimizing DRAM performance and energy consumption that is based on eagerly writing dirty cache lines to DRAM. Under this approach, many dirty cache lines are written to DRAM before they are evicted. In particular, dirty cache lines that have not been recently accessed are eagerly written to DRAM when the corresponding row has been activated by an ordinary, noneager access, such as a read. This approach enables clustering of reads and writes that target the same row, resulting in a significant reduction in row activations. Specifically, for a variety of applications, it reduces the number of DRAM row activations by an average of 42% and a maximum of 82%. Moreover, the results from a full-system simulator show compelling performance improvements and energy consumption reductions. Out of 23 applications, 6 have overall performance improvements between 10% and 20%, and 3 have improvements in excess of 20%. Furthermore, 12 consume between 10% and 20% less DRAM energy, and 7 have energy consumption reductions in excess of 20%. © 2014, ACM. All rights reserved.",Design; Dram; Eager Writeback; Energy Consumption; Measurement; Performance; Performance,
Designing a Practical Data Filter Cache to Improve Both Energy Efficiency and Performance,2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025409571&doi=10.1145%2f2541228.2555310&partnerID=40&md5=f37772fff09a544dbf248f911d89a986,"Conventional Data Filter Cache (DFC) designs improve processor energy efficiency, but degrade performance. Furthermore, the single-cycle line transfer suggested in prior studies adversely affects Level-1 Data Cache (L1 DC) area and energy efficiency. We propose a practical DFC that is accessed early in the pipeline and transfers a line over multiple cycles. Our DFC design improves performance and eliminates a substantial fraction of L1 DC accesses for loads, L1 DC tag checks on stores, and data translation lookaside buffer accesses for both loads and stores. Our evaluation shows that the proposed DFC can reduce the data access energy by 42.5% and improve execution time by 4.2%. © 2014, ACM. All rights reserved.",Data Cache Design; Energy Efficiency; Filter Cache; Performance Improvement; Speculation,
Evaluator-Executor Transformation for Efficient Pipelining of Loops with Conditionals,2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025382400&doi=10.1145%2f2541228.2555317&partnerID=40&md5=d23a69431da95c511e12ce8992f5585d,"Control divergence poses many problems in parallelizing loops. While predicated execution is commonlyused to convert control dependence into data dependence, it often incurs high overhead because it allocatesresources equally for both branches of a conditional statement regardless of their execution frequencies. Forthose loops with unbalanced conditionals, we propose a software transformation that divides a loop into twoor three smaller loops so that the condition is evaluated only in the first loop, while the less frequent branchis executed in the second loop in a way that is much more efficient than in the original loop. To reduce theoverhead of extra data transfer caused by the loop fission, we also present a hardware extension for a classof Coarse-Grained Reconfigurable Architectures (CGRAs). Our experiments using MiBench and computervision benchmarks on a CGRA demonstrate that our techniques can improve the performance of loops overpredicated execution by up to 65% (37.5%, on average), when the hardware extension is enabled.Without anyhardware modification, our software-only version can improve performance by up to 64% (33%, on average),while simultaneously reducing the energy consumption of the entire CGRA including configuration and datamemory by 22%, on average. © 2014, ACM. All rights reserved.",Algorithms; Coarse-Grained Reconfigurable Architecture (Cgra); Conditional Statements; Control Divergence; Design; Loop Fission; Performance; Predicated Execution; Software Pipelining,
HPar: A Practical Parallel Parser for HTML'Taming HTML Complexities for Parallel Parsing,2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025408118&doi=10.1145%2f2541228.2555301&partnerID=40&md5=987bf6f554d5da4638b729e00fc135e2,"Parallelizing HTML parsing is challenging due to the complexities of HTML documents and the inherent dependencies in its parsing algorithm. As a result, despite numerous studies in parallel parsing, HTML parsing remains sequential today. It forms one of the final barriers for fully parallelizing browser operations to minimize the browser's response time—an important variable for user experiences, especially on portable devices. This article provides a comprehensive analysis on the special complexities of parallel HTML parsing and presents a systematic exploration in overcoming those difficulties through specially designed speculative parallelizations. This work develops, to the best of our knowledge, the first pipelining and data-level parallel HTMLparsers. The data-level parallel parser, named HPar, achieves up to 2.4×speedup on quadcore devices. This work demonstrates the feasibility of efficient, parallel HTML parsing for the first time and offers a set of novel insights for parallel HTML parsing. © 2014, ACM. All rights reserved.",Algorithms; Design; Html Parsing; Multicore; Parallelization; Performance,
Temporal-Based Multilevel Correlating Inclusive Cache Replacement,2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976236083&doi=10.1145%2f2541228.2555290&partnerID=40&md5=76abb011f8e036a2498f822a7fb671d4,"Inclusive caches have been widely used in Chip Multiprocessors (CMPs) to simplify cache coherence.However, they have poor performance compared with noninclusive caches not only because of the limited capacity of the entire cache hierarchy but also due to ignorance of temporal locality of the Last-Level Cache (LLC). Blocks that are highly referenced (referred to as hot blocks) are always hit in higher-level caches (e.g., L1 cache) and are rarely referenced in the LLC. Therefore, they become replacement victims in the LLC. Due to the inclusion property, blocks evicted from the LLC have to also be invalidated from higher-level caches. Invalidation of hot blocks from the entire cache hierarchy introduces costly off-chip misses that makes the inclusive cache perform poorly. Neither blocks that are highly referenced in the LLC nor blocks that are highly referenced in higherlevel caches should be the LLC replacement victims.We propose temporal-based multilevel correlating cache replacement for inclusive caches to evict blocks in the LLC that are also not hot in higher-level caches using correlated temporal information acquired from all levels of a cache hierarchy with minimal overhead. Invalidation of these blocks does not hurt the performance. By contrast, replacing them as early as possible with useful blocks helps improve cache performance. Based on our experiments, in a dual-core CMP, an inclusive cache with temporal-based multilevel correlating cache replacement significantly outperforms an inclusive cache with traditional LRU replacement by yielding an average speedup of 12.7%, which is comparable to an enhanced noninclusive cache, while requiring less than 1% of storage overhead. © 2014, ACM. All rights reserved.",Chip Multiprocessors; Design; Inclusive Caches; Last-Level Cache; Performance; Replacement Policy,
Techniques to Improve Performance in Requester-Wins Hardware Transactional Memory,2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025394518&doi=10.1145%2f2541228.2555299&partnerID=40&md5=997fae6c487ebbd2264cf41d4904250f,"The simplicity of requester-wins Hardware Transactional Memory (HTM) makes it easy to incorporate in existing chip multiprocessors. Hence, such systems are expected to be widely available in the near future. Unfortunately, these implementations are prone to suffer severe performance degradation due to transient and persistent livelock conditions. This article shows that existing techniques are unable to mitigate this degradation effectively. It then proposes and evaluates four novel techniques'two software-based that employ information provided by the hardware and two that require simple core-local hardware additions' which have the potential to boost the performance of requester-wins HTM designs substantially. © 2014, ACM. All rights reserved.",Algorithms; Conflict Resolution; Contention Management; Design; Hardware Transactional Memory; Performance; Requesterwins,
Selecting Representative Benchmark Inputs for Exploring Microprocessor Design Spaces,2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025404720&doi=10.1145%2f2541228.2555294&partnerID=40&md5=063475651b7f42e9316c93577703951e,"The design process of a microprocessor requires representative workloads to steer the search process toward an optimum design point for the target application domain. However, considering a broad set of workloads to cover the large space of potential workloads is infeasible given how time-consuming design space exploration typically is. Hence, it is crucial to select a small yet representative set of workloads, which leads to a shorter design cycle while yielding a (near) optimal design. Prior work has mostly looked into selecting representative benchmarks; however, limited attention was given to the selection of benchmark inputs and how this affects workload representativeness during design space exploration. Using a set of 1,000 inputs for a number of embedded benchmarks and a design space with around 1,700 design points, we find that selecting a single or three random input(s) per benchmark potentially (in a worst-case scenario) leads to a suboptimal design that is 56% and 33% off, on average, relative to the optimal design in our design space in terms of Energy-Delay Product (EDP). We then propose and evaluate a number of methods for selecting representative inputs and show that we can find the optimum design point with as few as three inputs. © 2014, ACM. All rights reserved.",Design; Experimentation; Input Selection; Measurement; Performance; Processor Design Space Exploration; Workload Selection,
Tile Size Selection Revisited,2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978489408&doi=10.1145%2f2541228.2555292&partnerID=40&md5=a1e9bd29bdf4821016fe26b15a0dad77,"Loop tiling is a widely used loop transformation to enhance data locality and allow data reuse. In the tiled code, however, tiles of different sizes can lead to significant variation in performance. Thus, selection of an optimal tile size is critical to performance of tiled codes. In the past, tile size selection has been attempted using both static analytical and dynamic empirical (auto-tuning) models. Past work using static models assumed a direct-mapped cache for the purpose of analysis and thus proved to be less robust. On the other hand, the auto-tuning models involve an exhaustive search in a large space of tiled codes. In this article, we propose a new analytical model for tile size selection that leverages the high set associativity in modern caches to minimize conflict misses. Our tile size selection model targets data reuse in multiple levels of cache. In addition, it considers the interaction of tiling with the SIMD unit in modern processors in estimating the optimal tile size. We find that these factors, not considered in previous models, are critical in developing a robust model for tile size selection. We implement our tile size selection model in a polyhedral compiler and test it on 12 benchmark kernels using two different problem sizes. Our model outperforms the previous analytical models that are based on reusing data in a single level of cache and achieves an average performance improvement of 9.7% and 20.4%, respectively, over the best square (cubic) tiles for the two problem sizes. In addition, the tile size chosen by our tile size selection algorithm is similar to the best performing size obtained through an extensive search, validating the analytical model underlying the algorithm. © 2014, ACM. All rights reserved.",Algorithms; Loop Tiling; Multilevel Cache; Performance; Tile Size Selection; Vectorization,
"A System Architecture, Processor, and Communication Protocol for Secure Implants",2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025393425&doi=10.1145%2f2541228.2555313&partnerID=40&md5=a06d246ab7e7d61c01276274924fd49e,"Secure and energy-efficient communication between Implantable Medical Devices (IMDs) and authorized external users is attracting increasing attention these days. However, there currently exists no systematic approach to the problem, while solutions from neighboring fields, such as wireless sensor networks, are not directly transferable due to the peculiarities of the IMD domain. This work describes an original, efficient solution for secure IMD communication. A new implant system architecture is proposed, where security and main-implant functionality are made completely decoupled by running the tasks onto two separate cores. Wireless communication goes through a custom security ASIP, called SISC (Smart-Implant Security Core), which runs an energy-efficient security protocol. The security core is powered by RF-harvested energy until it performs external-reader authentication, providing an elegant defense mechanism against battery Denialof- Service (DoS) and other, more common attacks. The system has been evaluated based on a realistic case study involving an artificial pancreas implant. When synthesized for a UMC 90nm CMOS ASIC technology, our system architecture achieves defense against unauthorized accesses having zero energy cost, running entity authentication through harvesting only 7.45μJ of RF energy from the requesting entity. In all other successfully authenticated accesses, our architecture achieves secure data exchange without affecting the performance of the main IMD functionality, adding less than 1%. (1.3mJ) to the daily energy consumption of a typical implant. Compared to a singe-core, secure reference IMD, which would still be more vulnerable to some types of attacks, our secure system on chip (SoC) achieves high security levels at 56% energy savings and at an area overhead of less than 15%. © 2014, ACM. All rights reserved.",Algorithms; Design; Implantable Device; Performance; Security; System On Chip; Ultra-Low Power,
WADE: Writeback-Aware Dynamic Cache Management for NVM-Based Main Memory System,2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025381328&doi=10.1145%2f2541228.2555304&partnerID=40&md5=310a0371b48ef01ea891a7cc388c2746,"Emerging Non-Volatile Memory (NVM) technologies are explored as potential alternatives to traditional SRAM/DRAM-based memory architecture in future microprocessor design. One of the major disadvantages for NVM is the latency and energy overhead associated with write operations. Mitigation techniques to minimize the write overhead for NVM-based main memory architecture have been studied extensively. However, most prior work focuses on optimization techniques for NVM-based main memory itself, with little attention paid to cache management policies for the Last-Level Cache (LLC). In this article, we propose a Writeback-Aware Dynamic CachE (WADE) management technique to help mitigate the write overhead in NVM-based memory.1 The proposal is based on the observation that, when dirty cache blocks are evicted from the LLC and written into NVM-based memory (with PCM as an example), the long latency and high energy associated with write operations to NVM-based memory can cause system performance/power degradation. Thus, reducing the number of writeback requests from the LLC is critical. The proposed WADE cache management technique tries to keep highly reused dirty cache blocks in the LLC. The technique predicts blocks that are frequently written back in the LLC. The LLC sets are dynamically partitioned into a frequent writeback list and a nonfrequent writeback list. It keeps a best size of each list in the LLC. Our evaluation shows that the technique can reduce the number of writeback requests by 16.5% for memory-intensive single-threaded benchmarks and 10.8% for multicore workloads. It yields a geometric mean speedup of 5.1% for single-thread applications and 7.6% for multicore workloads. Due to the reduced number of writeback requests to main memory, the technique reduces the energy consumption by 8.1% for single-thread applications and 7.6% for multicore workloads. © 2014, ACM. All rights reserved.",cache segmentation; Design; Last-level cache; nonvolatile memory; Performance; replacement policy,
PCantorSim: Accelerating Parallel Architecture Simulation through Fractal-Based Sampling,2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025409753&doi=10.1145%2f2541228.2555307&partnerID=40&md5=a6cc155ed46c3093e36dc526e9826833,"Computer architects rely heavily on microarchitecture simulation to evaluate design alternatives. Unfortunately, cycle-accurate simulation is extremely slow, being at least 4 to 6 orders of magnitude slower than real hardware. This longstanding problem is further exacerbated in the multi-/many-core era, because singlethreaded simulation performance has not improved much, while the design space has expanded substantially. Parallel simulation is a promising approach, yet does not completely solve the simulation challenge. Furthermore, existing sampling techniques, which are widely used for single-threaded applications, do not readily apply to multithreaded applications as thread interaction and synchronization must now be taken into account. This work presents PCantorSim, a novel Cantor set (a classic fractal)—based sampling scheme to accelerate parallel simulation of multithreaded applications. Through the use of the proposed methodology, only less than 5% of an application's execution time is simulated in detail. We have implemented our approach in Sniper (a parallel multicore simulator) and evaluated it by running the PARSEC benchmarks on a simulated 8-core system. The results show that PCantorSim increases simulation speed over detailed parallel simulation by a factor of 20×, on average, with an average absolute execution time prediction error of 5.3%. © 2014, ACM. All rights reserved.",Algorithms; Cantor Set; Fractal; Measurement; Microarchitecture Simulation; Parallel Simulation; Performance; Performance Evaluation; Sampled Simulation,
TornadoNoC: A Lightweight and Scalable On-Chip Network Architecture for the Many-Core Era,2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025422291&doi=10.1145%2f2541228.2555312&partnerID=40&md5=2723c90ee26c509ffc18d62986cf0df1,"The rapid emergence of Chip Multi-Processors (CMP) as the de facto microprocessor archetype has highlighted the importance of scalable and efficient on-chip networks. Packet-based Networks-on-Chip (NoC) are gradually cementing themselves as the medium of choice for the multi-/many-core systems of the near future, due to their innate scalability. However, the prominence of the debilitating power wall requires the NoC to also be as energy efficient as possible. To achieve these two antipodal requirements'scalability and energy efficiency'we propose TornadoNoC, an interconnect architecture that employs a novel flow control mechanism. To prevent livelocks and deadlocks, a sequence numbering scheme and a dynamic ring inflation technique are proposed, and their correctness formally proven. The primary objective of TornadoNoC is to achieve substantial gains in (a) scalability to many-core systems and (b) the area/power footprint, as compared to current state-of-the-art router implementations. The new router is demonstrated to provide better scalability to hundreds of cores than an ideal single-cycle wormhole implementation and other scalabilityenhanced low-cost routers. Extensive simulations using both synthetic traffic patterns and real applications running in a full-system simulator corroborate the efficacy of the proposed design. Finally, hardware synthesis analysis using commercial 65nm standard-cell libraries indicates that the area and power budgets of the new router are reduced by up to 53% and 58%, respectively, as compared to existing state-of-the-art low-cost routers. © 2014, ACM. All rights reserved.",Algorithms; Architecture; Deflection-Based Routing; Design; Low-Cost Router; Network-On-Chip; Performance; Ring-Based Interconnection Network,
Fast Modulo Scheduler Utilizing Patternized Routes for Coarse-Grained Reconfigurable Architectures,2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025415624&doi=10.1145%2f2541228.2555314&partnerID=40&md5=ab9a122d442f23868698a49c431fad60,"Coarse-Grained Reconfigurable Architectures (CGRAs) present a potential of high compute throughput with energy efficiency. A CGRA consists of an array of Functional Units (FUs), which communicate with each other through an interconnect network containing transmission nodes and register files. To achieve high performance from the software solutions mapped onto CGRAs, modulo scheduling of loops is generally employed. One of the key challenges in modulo scheduling for CGRAs is to explicitly handle routings of operands from a source to a destination operations through various routing resources. Existing modulo schedulers for CGRAs are slow because finding a valid routing is generally a searching problem over a large space, even with the guidance of well-defined cost metrics. Applications in traditional embedded multimedia domains are regarded as relatively tolerant to a slow compile time in exchange for a high-quality solution. However, many rapidly growing domains of applications, such as 3D graphics, require a fast compilation. Entrances of CGRAs to these domains have been blocked mainly due to their long compile time. We attack this problem by utilizing patternized routes, for which resources and time slots for a success can be estimated in advance when a source operation is placed. By conservatively reserving predefined resources at predefined time slots, future routings originating from the source operation are guaranteed. Experiments on a real-world 3D graphics benchmark suite show that our scheduler improves the compile time up to 6,000 times while achieving an average 70% throughputs of the state-of-the-art CGRA modulo scheduler, the Edge-centric Modulo Scheduler (EMS). © 2014, ACM. All rights reserved.",Architecture; Coarse-grained reconfigurable architecture; Compiler; modulo scheduling; Performance; software pipelining,
"C1C: A Configurable, Compiler-Guided STT-RAM L1 Cache",2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962912859&doi=10.1145%2f2541228.2555308&partnerID=40&md5=a56f4ae7fbba59786a503756fb8e9bfb,"Spin-Transfer Torque RAM (STT-RAM), a promising alternative to SRAM for reducing leakage power consumption, has been widely studied to mitigate the impact of its asymmetrically long write latency. Recently, STT-RAM has been proposed for L1 caches by relaxing the data retention time to improve write performance and dynamic energy. However, as the technology scales down from 65nm to 22nm, the performance of the read operation scales poorly due to reduced sense margins and sense amplifier delays. In this article, we leverage a dual-mode STT memory cell to design a configurable L1 cache architecture termed C1C to mitigate read performance barriers with technology scaling. Guided by application access characteristics discovered through novel compiler analyses, the proposed cache adaptively switches between a high performance and a low-power access mode. Our evaluation demonstrates that the proposed cache with compiler guidance outperforms a state-of-the-art STT-RAM cache design by 9% with high dynamic energy efficiency, leading to significant performance/watt improvements over several competing approaches. © 2014, ACM. All rights reserved.",Algorithms; Cache; Compiler; Configurable; Design; Differential; Performance; Stt-Ram,
ReSense: Mapping Dynamic Workloads of Colocated Multithreaded Applications Using Resource Sensitivity,2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939136403&doi=10.1145%2f2541228.2555298&partnerID=40&md5=17fa870b136aa768a5beab1950a70026,"To utilize the full potential of modern chip multiprocessors and obtain scalable performance improvements, it is critical to mitigate resource contention created by multithreaded workloads. In this article, we describe ReSense, the first runtime system that uses application characteristics to dynamically map multithreaded applications from dynamic workloads—workloads where multithreaded applications arrive, execute, and terminate continuously in unpredictable ways. ReSense mitigates contention for the shared resources in the memory hierarchy by applying a novel thread-mapping algorithm that dynamically adjusts the mapping of threads from dynamic workloads using a precalculated sensitivity score. The sensitivity score quantifies an application's sensitivity to sharing a particular memory resource and is calculated by an efficient characterization process that involves running the multithreaded application by itself on the target platform. To measure ReSense's effectiveness, sensitivity scores were determined for 21 benchmarks from PARSEC-2.1 and NPB-OMP-3.3 for the shared resources in the memory hierarchy on four different platforms. Using three different-sized dynamic workloads composed of randomly selected two, four, and eight corunning benchmarks with randomly selected start times, ReSense was able to improve the average response time of the three workloads by up to 27.03%, 20.89%, and 29.34% and throughput by up to 19.97%, 46.56%, and 29.86%, respectively, over the native OS on real hardware. By estimating and comparing ReSense's effectiveness with the optimal thread mapping for two different workloads, we found that the maximum average difference with the experimentally determined optimal performance was 1.49% for average response time and 2.08% for throughput. © 2014, ACM. All rights reserved.",Algorithms; Coscheduling; Measurement; Memory Hierarchy; Multicore; Multithreaded Applications; Performance; Resource Contention; Thread Mapping,
Profile-Guided Transaction Coalescing—Lowering Transactional Overheads by Merging Transactions,2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025423641&doi=10.1145%2f2541228.2555306&partnerID=40&md5=dbcb2ecf097570505af653c224063469,"Previous studies in software transactional memory mostly focused on reducing the overhead of transactional read and write operations. In this article, we introduce transaction coalescing, a profile-guided compiler optimization technique that attempts to reduce the overheads of starting and committing a transaction by merging two or more small transactions into one large transaction. We develop a profiling tool and a transaction coalescing heuristic to identify candidate transactions suitable for coalescing. We implement a compiler extension to automatically merge the candidate transactions at the compile time. We evaluate the effectiveness of our technique using the hash table micro-benchmark and the STAMP benchmark suite. Transaction coalescing improves the performance of the hash table significantly and the performance of Vacation and SSCA2 benchmarks by 19.4% and 36.4%, respectively, when running with 12 threads. © 2014, ACM. All rights reserved.",Algorithms; Coalescing; Optimization; Performance; Transactional Memory,
Hardware Support for Accurate Per-Task Energy Metering in Multicore Systems,2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982138010&doi=10.1145%2f2541228.2555291&partnerID=40&md5=b2236f893d96eaebe83e904ab3c1cf60,"Accurately determining the energy consumed by each task in a system will become of prominent importance in future multicore-based systems because it offers several benefits, including (i) better application energy/performance optimizations, (ii) improved energy-aware task scheduling, and (iii) energy-aware billing in data centers. Unfortunately, existing methods for energy metering in multicores fail to provide accurate energy estimates for each task when several tasks run simultaneously. This article makes a case for accurate Per-Task Energy Metering (PTEM) based on tracking the resource utilization and occupancy of each task. Different hardware implementationswith different trade-offs between energy prediction accuracy and hardware-implementation complexity are proposed. Our evaluation shows that the energy consumed in a multicore by each task can be accurately measured. For a 32-core, 2-way, simultaneous multithreaded core setup, PTEM reduces the average accuracy error from more than 12% when our hardware support is not used to less than 4% when it is used. The maximum observed error for any task in the workload we used reduces from 58% down to 9% when our hardware support is used. © 2014, ACM. All rights reserved.",Chip Multiprocessors; Design; Hardware Counters; Measurement; Modeling And Estimation; Per-Task Energy Attribution; Performance; Power Modeling; Simultaneous Multithreaded,
Beyond Reuse Distance Analysis: Dynamic Analysis for Characterization of Data Locality Potential,2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025399067&doi=10.1145%2f2541228.2555309&partnerID=40&md5=6ad2d2d3b6c97eb0e5a44d6cd5c5e49b,"Emerging computer architectures will feature drastically decreased flops/byte (ratio of peak processing rate to memory bandwidth) as highlighted by recent studies on Exascale architectural trends. Further, flops are getting cheaper, while the energy cost of data movement is increasingly dominant. The understanding and characterization of data locality properties of computations is critical in order to guide efforts to enhance data locality. Reuse distance analysis of memory address traces is a valuable tool to perform data locality characterization of programs. A single reuse distance analysis can be used to estimate the number of cache misses in a fully associative LRU cache of any size, thereby providing estimates on the minimum bandwidth requirements at different levels of the memory hierarchy to avoid being bandwidth bound. However, such an analysis only holds for the particular execution order that produced the trace. It cannot estimate potential improvement in data locality through dependence-preserving transformations that change the execution schedule of the operations in the computation. In this article, we develop a novel dynamic analysis approach to characterize the inherent locality properties of a computation and thereby assess the potential for data locality enhancement via dependencepreserving transformations. The execution trace of a code is analyzed to extract a Computational-Directed Acyclic Graph (CDAG) of the data dependences. The CDAG is then partitioned into convex subsets, and the convex partitioning is used to reorder the operations in the execution trace to enhance data locality. The approach enables us to go beyond reuse distance analysis of a single specific order of execution of the operations of a computation in characterization of its data locality properties. It can serve a valuable role in identifying promising code regions for manual transformation, as well as assessing the effectiveness of compiler transformations for data locality enhancement. We demonstrate the effectiveness of the approach using a number of benchmarks, including case studies where the potential shown by the analysis is exploited to achieve lower data movement costs and better performance. © 2014, ACM. All rights reserved.",Algorithms; Data locality; dynamic analysis; Measurement; Performance; tiling,
GPU Code Generation for ODE-Based Applications with Phased Shared-Data Access Patterns,2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025403827&doi=10.1145%2f2541228.2555311&partnerID=40&md5=42560086da4495873f541712cd8b821e,"We present a novel code generation scheme for GPUs. Its key feature is the platform-aware generation of a heterogeneous pool of threads. This exposes more data-sharing opportunities among the concurrent threads and reduces the memory requirements that would otherwise exceed the capacity of the on-chip memory. Instead of the conventional strategy of focusing on exposing as much parallelism as possible, our scheme leverages on the phased nature of memory access patterns found in many applications that exhibit massive parallelism. We demonstrate the effectiveness of our code generation strategy on a computational systems biology application. This application consists of computing a Dynamic Bayesian Network (DBN) approximation of the dynamics of signalling pathways described as a system of Ordinary Differential Equations (ODEs). The approximation algorithm involves (i) sampling many (of the order of a few million) times from the set of initial states, (ii) generating trajectories through numerical integration, and (iii) storing the statistical properties of this set of trajectories in Conditional Probability Tables (CPTs) of a DBN via a prespecified discretization of the time and value domains. The trajectories can be computed in parallel. However, the intermediate data needed for computing them, as well as the entries for the CPTs, are too large to be stored locally. Our experiments show that the proposed code generation scheme scales well, achieving significant performance improvements on three realistic signalling pathways models. These results suggest how our scheme could be extended to deal with other applications involving systems of ODEs. © 2014, ACM. All rights reserved.",Code Generation; Design; Experimentation; Gpu; Memory Hierarchy; Performance,
Information Flow Tracking Meets Just-In-Time Compilation,2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025384341&doi=10.1145%2f2541228.2555295&partnerID=40&md5=616c45c309d0e298169e1550eb682b3a,"Web applications are vulnerable to cross-site scripting attacks that enable data thefts. Information flow tracking in web browsers can prevent communication of sensitive data to unintended recipients and thereby stop such data thefts. Unfortunately, existing solutions have focused on incorporating information flow into browsers' JavaScript interpreters, rather than just-in-time compilers, rendering the resulting performance noncompetitive. Few users will switch to a safer browser if it comes at the cost of significantly degrading web application performance. We present the first information flow tracking JavaScript engine that is based on a true just-in-time compiler, and that thereby outperforms all previous interpreter-based information flow tracking JavaScript engines by more than a factor of two. Our JIT-based engine (i) has the same coverage as previous interpreterbased solutions, (ii) requires reasonable implementation effort, and (iii) introduces new optimizations to achieve acceptable performance. When evaluated against three industry-standard JavaScript benchmark suites, there is still an average slowdown of 73% over engines that do not support information flow, but this is now well within the range that many users will find an acceptable price for obtaining substantially increased security. © 2014, ACM. All rights reserved.",Design; Dynamic Language Security; Information Flow; Javascript; Languages; Security,
JIT Technology with C/C++: Feedback-Directed Dynamic Recompilation for Statically Compiled Languages,2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025390117&doi=10.1145%2f2541228.2555315&partnerID=40&md5=fcb352324f5afe035ecd9db4ec140697,"The growing gap between the advanced capabilities of static compilers as reflected in benchmarking results and the actual performance that users experience in real-life scenarios makes client-side dynamic optimization technologies imperative to the domain of static languages. Dynamic optimization of software distributed in the form of a platform-agnostic Intermediate-Representation (IR) has been very successful in the domain of managed languages, greatly improving upon interpreted code, especially when online profiling is used. However, can such feedback-directed IR-based dynamic code generation be viable in the domain of statically compiled, rather than interpreted, languages? We show that fat binaries, which combine the IR together with the statically compiled executable, can provide a practical solution for software vendors, allowing their software to be dynamically optimized without the limitation of binary-level approaches, which lack the highlevel IR of the program, and without the warm-up costs associated with the IR-only software distribution approach.We describe and evaluate the fat-binary-based runtime compilation approach using SPECint2006, demonstrating that the overheads it incurs are low enough to be successfully surmounted by dynamic optimization. Building on Java JIT technologies, our results already improve upon common real-world usage scenarios, including very small workloads. © 2014, ACM. All rights reserved.",Design; Dynamic Optimization; Just-In-Time Compilation; Performance,
Boosting Timestamp-based Transactional Memory by Exploiting Hardware Cycle Counters,2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950334018&doi=10.1145%2f2541228.2555297&partnerID=40&md5=f7f43660d78f7c7eac1b6e4ab0671ae8,"Time-based transactional memories typically rely on a shared memory counter to ensure consistency. Unfortunately, such a counter can become a bottleneck. In this article, we identify properties of hardware cycle counters that allow their use in place of a shared memory counter. We then devise algorithms that exploit the x86 cycle counter to enable bottleneck-free transactional memory runtime systems. We also consider the impact of privatization safety and hardware ordering constraints on the correctness, performance, and generality of our algorithms. © 2014, ACM. All rights reserved.",Algorithms; counters; Design; Performance; privatization; rdtscp; Transactional memory,
Hardware-Assisted Cooperative Integration of Wear-Leveling and Salvaging for Phase Change Memory,2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025424121&doi=10.1145%2f2459316.2459318&partnerID=40&md5=e0e92c0789e78b071a1820f89fa85ae5,"Phase Change Memory (PCM) has recently emerged as a promising memory technology. However, PCM's limited write endurance restricts its immediate use as a replacement for DRAM. To extend the lifetime of PCM chips, wear-leveling and salvaging techniques have been proposed.Wear-leveling balances write operations across different PCM regions while salvaging extends the duty cycle and provides graceful degradation for a nonnegligible number of failures. Current wear-leveling and salvaging schemes have not been designed and integrated to work cooperatively to achieve the best PCM device lifetime. In particular, a noncontiguous PCM space generated from salvaging complicates wear-leveling and incurs large overhead. In this article, we propose LLS, a Line-Level mapping and Salvaging design. By allocating a dynamic portion of total space in a PCM device as backup space, and mapping failed lines to backup PCM, LLS constructs a contiguous PCM space and masks lowerlevel failures from the OS and applications. LLS integrates wear-leveling and salvaging and copes well with modern OSes. Our experimental results show that LLS achieves 31% longer lifetime than the state-of-theart. It has negligible hardware cost and performance overhead. © 2013, ACM. All rights reserved.",Design; Hard Faults; Performance; Phase Change Memory; Reliability; Salvaging; Wear-Leveling,
Time- and Space-Efficient Flow-Sensitive Points-to Analysis,2013,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025423152&doi=10.1145%2f2541228.2555296&partnerID=40&md5=210d692fe5c073885f711a8605f80b05,"Compilation of real-world programs often requires hours. The term nightly build known to industrial researchers is an artifact of long compilation times. Our goal is to reduce the absolute analysis times for large C codes (of the order of millions of lines). Pointer analysis is one of the key analyses performed during compilation. Its scalability is paramount to achieve the efficiency of the overall compilation process and its precision directly affects that of the client analyses. In this work, we design a time- and space-efficient flow-sensitive pointer analysis and parallelize it on graphics processing units. Our analysis proposes to use an extended bloom filter, called multibloom, to store points-to information in an approximate manner and develops an analysis in terms of the operations over the multibloom. Since bloom filter is a probabilistic data structure, we develop ways to gain back the analysis precision. We achieve effective parallelization by achieving memory coalescing, reducing thread divergence, and improving load balance across GPU warps. Compared to a state-of-the-art sequential solution, our parallel version achieves a 7.8% speedup with less than 5% precision loss on a suite of six large programs. Using two client transformations, we show that this loss in precision only minimally affects a client's precision. © 2014, ACM. All rights reserved.",Algorithms; Approximation; Bloom Filters; Compilers; Flow Sensitivity; Gpgpu; Languages; Performance; Points-To Analysis; Program Analysis,
Performance-Aware Thermal Management via Task Scheduling,2010,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025385369&doi=10.1145%2f1736065.1736070&partnerID=40&md5=c0e8f2ecb2065a11b408c40ec5b51d32,"High on-chip temperature impairs the processor's reliability and reduces its lifetime. Hardwarelevel dynamic thermal management (DTM) techniques can effectively constrain the chip temperature, but degrades the performance. We propose an OS-level technique that performs thermalaware job scheduling to reduce DTMs. The algorithm is based on the observation that hot and cool jobs executed in a different order can make a difference in resulting temperature. Real-system implementation in Linux shows that our scheduler can remove 10.5% to 73.6% of the hardware DTMs in a medium thermal environment. The CPU throughput is improved by up to 7.6% (4.1%, on average) in a severe thermal environment. © 2010, ACM. All rights reserved.",Algorithms; Management; Performance; Task Scheduling; Thermal Management,Computer operating systems; Multitasking; Scheduling; Scheduling algorithms; Thermal management (electronics); Dynamic thermal management; Jobs scheduling; Management techniques; On-chip temperature; Performance; Processor reliability; Tasks scheduling; Thermal environment; Thermal-Aware; Temperature control
Reducing Cache Misses Through Programmable Decoders,2008,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-73149096286&doi=10.1145%2f1328195.1328200&partnerID=40&md5=b52da2a07bddbfba61edab5925b50f57,"Level-one caches normally reside on a processor's critical path, which determines clock frequency. Therefore, fast access to level-one cache is important. Direct-mapped caches exhibit faster access time, but poor hit rates, compared with same sized set-associative caches because of nonuniform accesses to the cache sets. The nonuniform accesses generate more cache misses in some sets, while other sets are underutilized. We propose to increase the decoder length and, hence, reduce the accesses to heavily used sets without dynamically detecting the cache set usage information. We increase the access to the underutilized cache sets by incorporating a replacement policy into the cache design using programmable decoders. On average, the proposed techniques achieve as low a miss rate as a traditional 4-way cache on all 26 SPEC2K benchmarks for the instruction and data caches, respectively. This translates into an average IPC improvement of 21.5 and 42.4% for SPEC2K integer and floating-point benchmarks, respectively. The B-Cache consumes 10.5% more power per access, but exhibits a 12% total memory access-related energy savings as a result of the miss rate reductions, and, hence, the reduction to applications' execution time. Compared with previous techniques that aim at reducing the miss rate of direct-mapped caches, our technique requires only one cycle to access all cache hits and has the same access time of a direct-mapped cache. © 2008, ACM. All rights reserved.",Cache; Design; Dynamic Optimization; Experimentation; Low Power; Performance,
Hiding the Misprediction Penalty of a Resource—Efficient High—Performance Processor,2008,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350107440&doi=10.1145%2f1328195.1328201&partnerID=40&md5=eac0da62f61b1a752b820182fbcf333f,"Misprediction is a major obstacle for increasing speculative out-of-order processors performance. Performance degradation depends on both the number of misprediction events and the recovery time associated with each one of them. In recent years a few checkpoint based microarchitectures have been proposed. In comparison with ROB-based processors, checkpoint processors are scalable and highly resource efficient. Unfortunately, in these proposals the misprediction recovery time is proportional to the instruction queue size. In this paper we analyze methods to reduce the misprediction recovery time. We propose a new register file management scheme and techniques to selectively flush the instruction queue and the load store queue, and to isolate deeply pipelined execution units. The result is a novel checkpoint processor with Constant misprediction RollBack time (CRB). We further present a streamlined, cost-efficient solution, which saves complexity at the price of slightly lower performance. © 2008, ACM. All rights reserved.",Checkpoints; Design; Misprediction; Out-Of-Order Execution; Performance; Rollback; Scalable Architecture,
Exploiting Virtual Registers to Reduce Pressure on Real Registers,2008,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009757623&doi=10.1145%2f1328195.1328198&partnerID=40&md5=31abacb60b2fbda2f66a88bba6dc0c10,"It is well known that a large fraction of variables are short-lived. This paper proposes a novel approach to exploiting this fact to reduce the register pressure for pipelined processors with dataforwarding network. The idea is that the compiler can allocate virtual registers (i.e., place holders to identify dependences among instructions) to short-lived variables, which do not need to be stored to physical storage locations. As a result, real registers (i.e., physically existed registers) can be reserved for long-lived variables for mitigating the register pressure and decreasing the register spills, leading to performance improvement. In this paper, we develop the architectural and compiler support for exploiting virtual registers for statically scheduled processors. Our experimental results show that virtual registers are very effective at reducing the register spills, which, in many cases, can achieve the performance close to the processor with twice number of real registers. Our results also indicate that, for some applications, using 24 virtual, in addition to 8 real registers, can attain even higher performance than that of 16 real without any virtual registers. © 2008, ACM. All rights reserved.",Data Forwarding; Design; Experimentation; Performance; Register Allocation; Register File; Short-Lived Variables; Virtual Register,
Efficient Architectural Design Space Exploration via Predictive Modeling,2008,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-66749097272&doi=10.1145%2f1328195.1328196&partnerID=40&md5=9aa374d3127c59182da1e3be3d911ee4,"Efficiently exploring exponential-size architectural design spaces with many interacting parameters remains an open problem: the sheer number of experiments required renders detailed simulation intractable.We attack this via an automated approach that builds accurate predictive models. We simulate sampled points, using results to teach our models the function describing relationships among design parameters. The models can be queried and are very fast, enabling efficient design tradeoff discovery. We validate our approach via two uniprocessor sensitivity studies, predicting IPC with only 1-2% error. In an experimental study using the approach, training on 1% of a 250- K-point CMP design space allows our models to predict performance with only 4-5% error. Our predictive modeling combines well with techniques that reduce the time taken by each simulation experiment, achieving net time savings of three-four orders of magnitude. © 2008, ACM. All rights reserved.",Artificial Neural Networks; Design; Design Space Exploration; Experimentation; Measurement; Performance; Prediction; Sensitivity Studies,
Object Co-location and Memory Reuse for Java Programs,2008,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863015634&doi=10.1145%2f1328195.1328199&partnerID=40&md5=a7b6fdf1c1235449b9ffd5bc6a851df4,"We introduce a new memory management system, STEMA, which can improve the execution time of Java programs. STEMA detects prolific types on-the-fly and co-locates their objects in a special memory space which supports reuse of memory. We argue and show that memory reuse and co-location of prolific objects can result in improved cache locality, reduced memory fragmentation, reduced GC time, and faster object allocation. We evaluate STEMA using 16 benchmarks. Experimental results show that STEMA performs 2.7%, 4.0%, and 8.2% on average better than MarkSweep, CopyMS, and SemiSpace. © 2008, ACM. All rights reserved.",Experimentation; Garbage Collector; Java; Languages; Measurement; Memory Allocator; Memory Reuse; Mutator; Object Colocation; Performance,
Virtual Machine Showdown: Stack Versus Registers,2008,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994515221&doi=10.1145%2f1328195.1328197&partnerID=40&md5=a69937d6569bc023d80db84d61c04d3f,"Virtual machines (VMs) enable the distribution of programs in an architecture-neutral format, which can easily be interpreted or compiled. A long-running question in the design of VMs is whether a stack architecture or register architecture can be implemented more efficiently with an interpreter. We extend existing work on comparing virtual stack and virtual register architectures in three ways. First, our translation from stack to register code and optimization are much more sophisticated. The result is that we eliminate an average of more than 46% of executed VM instructions, with the bytecode size of the register machine being only 26% larger than that of the corresponding stack one. Second, we present a fully functional virtual-register implementation of the Java virtual machine (JVM), which supports Intel, AMD64, PowerPC and Alpha processors. This register VM supports inline-threaded, direct-threaded, token-threaded, and switch dispatch. Third, we present experimental results on a range of additional optimizations such as register allocation and elimination of redundant heap loads. On the AMD64 architecture the register machine using switch dispatch achieves an average speedup of 1.48 over the corresponding stack machine. Even using the more efficient inline-threaded dispatch, the register VM achieves a speedup of 1.15 over the equivalent stack-based VM. © 2008, ACM. All rights reserved.",Interpreter; Language; Performance; Register architecture; Stack architecture; Virtual machine,
Cross-Component Energy Management: Joint Adaptation of Processor and Memory,2007,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013786846&doi=10.1145%2f1275937.1275938&partnerID=40&md5=ad97ff9b1da5b99834dbe4ea36dc0192,"Researchers have proposed the use of adaptation to reduce the energy consumption of different hardware components, such as the processor, memory, disk, and display for general-purpose applications. Previous algorithms to control these adaptations, however, have focused on a single component. This work takes the first step toward developing algorithms that can jointly control adaptations in multiple interacting components for general-purpose applications, with the goal of minimizing the total energy consumed within a specified performance loss. Specifically, we develop a joint-adaptation algorithm for processor and memory adaptations.We identify two properties that enable per-component algorithms to be easily used in a cross-component context—the algorithms’ performance impact must be guaranteed and composable. We then modify a current processor and a memory algorithm to obey these properties. This allows the cross-component problem to be reduced to determine an appropriate (energy-optimal) allocation of the target performance loss (slack) between the two components. We develop such an optimal slack allocation algorithm that exploits the above properties. The result is an efficient cross-component adaptation framework that minimizes the total energy of the processor and memory without exceeding the target performance loss, while substantially leveraging current per-component algorithms. Our experiments show that joint processor and memory adaptation provides significantly more energy savings than adapting either component alone; intelligent slack distribution is specifically effective for highly computeor memory-intensive applications; and the performance slowdown never exceeds the specification. © 2007, ACM. All rights reserved.",adaptive systems; Algorithms; control algorithms; Design; Energy management; low-power design; memory; Performance; performance guarantee; processor,
Unified Control Flow and Data Dependence Traces,2007,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-53049097726&doi=10.1145%2f1275937.1275943&partnerID=40&md5=3e2ed8075d28096f118103a6ef7cda87,"We describe the design, generation, and compression of the extended whole program path (eWPP), representation that not only captures the control flow history of a program execution but also its data dependence history. This representation is motivated by the observation that, typically, a significant fraction of data dependence history can be recovered from the control flow trace. To capture the remainder of the data dependence history, we introduce disambiguation checks in the program whose control flow signatures capture the results of the checks. The resulting extended control flow trace enables the recovery of otherwise irrecoverable data dependences. The code for the checks is designed to minimize the increase in program execution time and the extended control flow trace size when compared to directly collecting control flow and address traces. Our experiments show that compressed eWPPs are only one-quarter of the size of combined compressed control flow and address traces. However, their collection incurs a 5× increase in runtime overhead relative to the overhead required for directly collecting the control flow and address traces, respectively. © 2007, ACM. All rights reserved.",address trace; Algorithms; Control flow trace; dynamic data dependence trace; Measurement; Performance; profiling,
Online Diagnosis of Hard Faults in Microprocessors,2007,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962054592&doi=10.1145%2f1250727.1250728&partnerID=40&md5=a9be5981a1d68bddd983d5b243a6042a,"We develop a microprocessor design that tolerates hard faults, including fabrication defects and in-field faults, by leveraging existing microprocessor redundancy. To do this, we must: detect and correct errors, diagnose hard faults at the field deconfigurable unit (FDU) granularity, and deconfigure FDUs with hard faults. In our reliable microprocessor design, we use DIVA dynamic verification to detect and correct errors. Our new scheme for diagnosing hard faults tracks instructions’ core structure occupancy from decode until commit. If a DIVA checker detects an error in an instruction, it increments a small saturating error counter for every FDU used by that instruction, including that DIVA checker. A hard fault in an FDU quickly leads to an above-threshold error counter for that FDU and thus diagnoses the fault. For deconfiguration, we use previously developed schemes for functional units and buffers and present a scheme for deconfiguring DIVA checkers. Experimental results show that our reliable microprocessor quickly and accurately diagnoses each hard fault that is injected and continues to function, albeit with somewhat degraded performance. © 2007, ACM. All rights reserved.",Design; Performance; Reliability,
Snug Set-Associative Caches: Reducing Leakage Power of Instruction and Data Caches with No Performance Penalties,2007,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988366765&doi=10.1145%2f1216544.1216549&partnerID=40&md5=eb07c7d9ee047bcd3e5ab141958277d7,"As transistors keep shrinking and on-chip caches keep growing, static power dissipation resulting from leakage of caches takes an increasing fraction of total power in processors. Several techniques have already been proposed to reduce leakage power by turning off unused cache lines. However, they all have to pay the price of performance degradation. This paper presents a cache architecture, the snug set-associative (SSA) cache, that cuts most of static power dissipation of caches without incuring performance penalties. The SSA cache reduces leakage power by implementing the minimum set-associative scheme, which only activates the minimal numbers of ways in each cache set, while the performance losses caused by this scheme are compensated by the base-offset load/store queues. The rationale of combining these two techniques is locality: as the contents of the cache blocks in the current working set are repeatedly accessed, same addresses would be computed again and again. The SSA cache architecture can be applied to data and instruction caches to reduce leakage power without incurring performance penalties. Experimental results show that SSA can cut static power consumption of the L1 data cache by 93%, on average, for SPECint2000 benchmarks, while the execution times are reduced by 5%. Similarly, SSA can cut leakage dissipation of the L1 instruction cache by 92%, on average, and improve performance over 3%. Furthermore, when SSA is adopted for both L1 data and instruction caches, the normalized leakage of L1 data and instruction caches is lowered to 8%, on average, while still accomplishing a 2% reduction in execution times. © 2007, ACM. All rights reserved.",Algorithms; Cache decay; Caches; Design; Drowsy caches; Experimentation; Leakage power; Performance,
A Study of Thread Migration in Temperature-Constrained Multicores,2007,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350070335&doi=10.1145%2f1250727.1250729&partnerID=40&md5=bf2c173ccd378472e91431f6b7d69589,"Temperature has become an important constraint in high-performance processors, especially multicores. Thread migration will be essential to exploit the full potential of future thermally constrained multicores.We propose and study a thread migration method that maximizes performance under a temperature constraint, while minimizing the number of migrations and ensuring fairness between threads.We show that thread migration brings important performance gains and that it is most effective during the first tens of seconds following a decrease of the number of running threads. © 2007, ACM. All rights reserved.",Management; Multicore processor; Performance; Power density; Reliability; Temperature; Thermal management; Thread migration,
ALP: Efficient Support for All Levels of Parallelism for Complex Media Applications,2007,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-52049099632&doi=10.1145%2f1216544.1216546&partnerID=40&md5=211be0a1bb3aba751fd98755d1860db4,"The real-time execution of contemporary complex media applications requires energy-efficient processing capabilities beyond those of current superscalar processors.We observe that the complexity of contemporary media applications requires support for multiple forms of parallelism, including ILP, TLP, and various forms of DLP, such as subword SIMD, short vectors, and streams. Based on our observations, we propose an architecture, called ALP, that efficiently integrates all of these forms of parallelism with evolutionary changes to the programming model and hardware. The novel part of ALP is a DLP technique called SIMD vectors and streams (SVectors/SStreams), which is integrated within a conventional superscalar-based CMP/SMT architecture with subword SIMD. This technique lies between subword SIMD and vectors, providing significant benefits over the former at a lower cost than the latter. Our evaluations show that each form of parallelism supported by ALP is important. Specifically, SVectors/SStreams are effective, compared to a system with the other enhancements in ALP. They give speedups of 1.1 to 3.4X and energy-delay product improvements of 1.1 to 5.1X for applications with DLP. © 2007, ACM. All rights reserved.",Data-level parallelism; Design; Dlp; Media applications; Multimedia; Parallelism; Performance; Simd; Tlp; Vector,
Fairness Enforcement in Switch on Event Multithreading,2007,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010294164&doi=10.1145%2f1275937.1275939&partnerID=40&md5=b7b8d73801845f0322b8346f38958049,"The need to reduce power and complexity will increase the interest in Switch On Event multithreading (coarse-grained multithreading). Switch On Event multithreading is a low-power and low-complexity mechanism to improve processor throughput by switching threads on execution stalls. Fairness may, however, become a problem in a multithreaded processor. Unless fairness is properly handled, some threads may starve while others consume all of the processor cycles. Heuristics that were devised in order to improve fairness in simultaneous multithreading are not applicable to Switch On Event multithreading. This paper defines the fairness metric using the ratio of the individual threads’ speedups and shows how it can be enforced in Switch On Event multithreading. Fairness is controlled by forcing additional thread switch points. These switch points are determined dynamically by runtime estimation of the single threaded performance of each of the individual threads. We analyze the impact of the fairness enforcement mechanism on aggregate IPC and weighted speedup. We present simulation results of the performance of Switch On Event multithreading. Switch On Event multithreading achieves an average aggregate IPC increase of 26% over single thread and 12% weighted speedup when no fairness is enforced. In this case, a sixth of our runs resulted in poor fairness in which one thread ran extremely slowly (10 to 100 times slower than its single-thread performance), while the other thread’s performance was hardly affected. By using the proposed mechanism, we can guarantee fairness at different levels of strictness and, in most cases, even improve the weighted speedup. © 2007, ACM. All rights reserved.",,
Precise Automatable Analytical Modeling of the Cache Behavior of Codes with Indirections,2007,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76749087837&doi=10.1145%2f1275937.1275940&partnerID=40&md5=0e63d02b660ffe3e93e95168ebf976c6,"The performance of memory hierarchies, in which caches play an essential role, is critical in nowadays general-purpose and embedded computing systems because of the growing memory bottleneck problem. Unfortunately, cache behavior is very unstable and difficult to predict. This is particularly true in the presence of irregular access patterns, which exhibit little locality. Such patterns are very common, for example, in applications in which pointers or compressed sparse matrices give place to indirections. Nevertheless, cache behavior in the presence of irregular access patterns has not been widely studied. In this paper we present an extension of a systematic analytical modeling technique based on PMEs (probabilistic miss equations), previously developed by the authors, that allows the automated analysis of the cache behavior for codes with irregular access patterns resulting from indirections. The model generates very accurate predictions despite the irregularities and has very low computing requirements, being the first model that gathers these desirable characteristics that can automatically analyze this kind of codes. These properties enable this model to help drive compiler optimizations, as we show with an example. © 2007, ACM. All rights reserved.",Analytical modeling; irregular access patterns; Measurement; memory hierarchy; Performance; performance prediction,
Inter-Cluster Communication in Vliw Architectures,2007,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984708191&doi=10.1145%2f1250727.1250731&partnerID=40&md5=786653b497f01cdd01d71bb7e35e59ae,"The traditional VLIW (very long instruction word) architecture with a single register file does not scale up well to address growing performance demands on embedded media processors. However, splitting a VLIW processor in smaller clusters, which are comprised of function units fully connected to local register files, can significantly improve VLSI implementation characteristics of the processor, such as speed, energy consumption, and area. In our paper we reveal that achieving the best characteristics of a clustered VLIW requires a thorough selection of an Intercluster Communication (ICC) model, which is the way clustering is exposed in the Instruction Set Architecture. For our study we, first, define a taxonomy of ICC models including copy operations, dedicated issue slots, extended operands, extended results, and multicast. Evaluation of the execution time of the models requires both the dynamic cycle count and clock period.We developed an advanced instruction scheduler for all the five ICC models in order to quantify the dynamic cycle counts of our multimedia C benchmarks. To assess the clock period of the ICC models we designed and laid out VLIW datapaths using the RTL hardware descriptions derived from a deeply pipelined commercial TriMedia processor. In contrast to prior art, our research shows that fully distributed register file architectures (with eight clusters in our study) often underperform compared to moderately clustered machines with two or four clusters because of explosion of the cycle count overhead in the former. Among the evaluated ICC models, performance of the copy operation model, popular both in academia and industry, is severely limited by the copy operations hampering scheduling of regular operations in high ILP (instruction-level parallelism) code. The dedicated issue slots model combats this limitation by dedicating extra VLIW issue slots purely for ICC, reaching the highest 1.74 execution time speedup relative to the unicluster. Furthermore, our VLSI experiments show that the lowest area and energy consumption of 42 and 57% relative to the unicluster, respectively, are achieved by the extended operands model, which, nevertheless, provides higher performance than the copy operation model. © 2007, ACM. All rights reserved.",,
Java Object Header Elimination for Reduced Memory Consumption in 64-bit Virtual Machines,2007,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879697978&doi=10.1145%2f1275937.1275941&partnerID=40&md5=5a8f2833cd8d3655b6e97e16894ff142,"Memory performance is an important design issue for contemporary computer systems given the huge processor/memory speed gap. This paper proposes a space-efficient Java object model for reducing the memory consumption of 64-bit Java virtual machines. We completely eliminate the object header through typed virtual addressing (TVA) or implicit typing. TVA encodes the object type in the object’s virtual address by allocating all objects of a given type in a contiguous memory segment. This allows for removing the type information as well as the status field from the object header. Whenever type and status information is needed, masking is applied to the object’s virtual address for obtaining an offset into type and status information structures. Unlike previous work on implicit typing, we apply TVA to a selected number of frequently allocated object types, hence, the name selective TVA (STVA); this limits the amount of memory fragmentation. In addition to applying STVA, we also compress the type information block (TIB) pointers for all objects that do not fall under TVA. We implement the space-efficient Java object model in the 64-bit version of the Jikes RVM on an AIX IBM platform and compare its performance against the traditionally used Java object model using a multitude of Java benchmarks. We conclude that the space-efficient Java object model reduces memory consumption by on average 15% (and up to 45% for some benchmarks). About one-half the reduction comes from TIB pointer compression; the other onehalf comes from STVA. In terms of performance, the space-efficient object model generally does not affect performance; however, for some benchmarks we observe statistically significant performance speedups, up to 20%. © 2007, ACM. All rights reserved.",64-bit implementation; Design; Experimentation; Implicit typing; Java object model; Performance; Typed virtual addressing; Virtual machine,
Code Reordering on Limited Branch Offset,2007,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872443575&doi=10.1145%2f1250727.1250730&partnerID=40&md5=47ffb42d5f6e9fb940e7401b9b62acea,"Since the 1980’s code reordering has gained popularity as an important way to improve the spatial locality of programs. While the effect of the processor’s microarchitecture and memory hierarchy on this optimization technique has been investigated, little research has focused on the impact of the instruction set. In this paper, we analyze the effect of limited branch offset of the MIPS-like instruction set [Hwu et al. 2004, 2005] on code reordering, explore two simple methods to handle the exceeded branches, and propose the bidirectional code layout (BCL) algorithm to reduce the number of branches exceeding the offset limit. The BCL algorithm sorts the chains according to the position of related chains, avoids cache conflict misses deliberately and lays out the code bidirectionally. It strikes a balance among the distance of related blocks, the instruction cache miss rate, the memory size required, and the control flow transfer. Experimental results show that BCL can effectively reduce exceeded branches by 50.1%, on average, with up to 100% for some programs. Except for some programs with little spatial locality, the BCL algorithm can achieve the performance, as the case with no branch offset limitation. © 2007, ACM. All rights reserved.",Algorithms; Code reordering; Godson Processor; link-time optimization,
Conserving Network Processor Power Consumption by Exploiting Traffic Variability,2007,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013775152&doi=10.1145%2f1216544.1216547&partnerID=40&md5=15487578d2ee3343a8a26928d79b2384,"Network processors (NPs) have emerged as successful platforms for providing both high performance and flexibility in building powerful routers. Typical NPs incorporate multiprocessing and multithreading to achieve maximum parallel processing capabilities. We observed that under low incoming traffic rates, processing elements (PEs) in an NP are idle for most of the time but still consume dynamic power. This paper develops a low-power technique to reduce the activities of PEs in accordance with the varying traffic volume. We propose to monitor the average number of idle threads in a time window, and gate off the clock signals to unnecessary PEs when a subset of PEs is enough to handle the network traffic. We solve the difficulties arising from clock gating the PEs, such as redirecting network packets, determining the thresholds of turning on/off PEs, and avoiding unnecessary packet loss. Our technique brings significant reduction in power consumption of NPs with no packet loss and little impact on overall throughput. © 2007, ACM. All rights reserved.",clock gating; Design; Experimentation; low power; Measurement; Network processor; Performance; scheduling,
SSA-Based Mobile Code: Implementation and Empirical Evaluation,2007,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350284609&doi=10.1145%2f1250727.1250733&partnerID=40&md5=09857c35ded0f996df7761f27f03b596,"Although one might expect transportation formats based on static single-assignment form (SSA) to yield faster just-in-time compilation times than those based on stack-based virtual machines, this claim has not previously been validated, in practice.We attempt to quantify the effect of using an SSA-based mobile code representation by integrating support for a verifiable SSA-based IR into Jikes RVM. Performance results, measured with various optimizations and on both the IA32 and PowerPC, show improvements in both compilation time and code quality. © 2007, ACM. All rights reserved.",Experimentation; Languages; Measurement; Performance; SafeTSA; static single-assignment form; Virtual machines,
VLIW Instruction Scheduling for Minimal Power Variation,2007,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954072350&doi=10.1145%2f1275937.1275942&partnerID=40&md5=92079e37cad7d85366be37ade33d1daf,"The focus of this paper is on the minimization of the variation in power consumed by a VLIW processor during the execution of a target program through instruction scheduling. The problem is formulated as a mixed-integer program (MIP) and a problem-specific branch-and-bound algorithm has been developed to solve it more efficiently than generic MIP solvers. Simulation results based on the TMS320C6711 VLIW digital signal processor using benchmarks from Mediabench and Trimaran showed that over 40% average reduction in power variation can be achieved without sacrificing execution speed of these benchmarks. Computational requirements and convergence rates of our algorithm are also analyzed. © 2007, ACM. All rights reserved.",Algorithms; Instruction scheduling; power variation reduction; processors; VLIW,
Software-Directed Power-Aware Interconnection Networks,2007,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903783165&doi=10.1145%2f1216544.1216548&partnerID=40&md5=8cdd908c0ad53dd746f778a065fbbd7b,"Interconnection networks have been deployed as the communication fabric in a wide spectrum of parallel computer systems, ranging from chip multiprocessors (CMPs) and embedded multicore systems-on-a-chip (SoCs) to clusters and server blades. Recent technology trends have permitted a rapid growth of chip resources, faster clock rates, and wider communication bandwidths, however, these trends have also led to an increase in power consumption that is becoming a key limiting factor in the design of such scalable interconnected systems. Power-aware networks, therefore, need to become inherent components of single and multi-chip parallel systems. In the hardware arena, recent interconnection network power-management research work has employed limitedscope techniques that mostly focus on reducing the power consumed by the network communication links. As these limited-scope techniques are not tailored to the applications running on the network, power savings and the corresponding impact on network latency vary significantly from one application to the next as we demonstrate in this paper; in many cases, network performance can severely suffer. In the software arena, extensive research on compile-time optimizations has produced parallelizing compilers that can efficiently map an application onto hardware for high performance. However, research into power-aware parallelizing compilers is in its infancy. In this paper, we take the first steps toward tailoring applications’ communication needs at run-time for low power. We propose software techniques that extend the flow of a parallelizing compiler in order to direct run-time network power reduction. We target network links, a significant power consumer in these systems, allowing dynamic voltage scaling (DVS) instructions extracted during static compilation to orchestrate link voltage and frequency transitions for power savings during application run-time. Concurrently, an online hardware mechanism measures network congestion levels and adapts these off-line DVS settings to maximize network performance. Our simulations over three existing parallel systems, ranging from very fine-grained single-chip to coarse-grained multi-chip architectures, show that link power consumption can be reduced by up to 76.3%, with a minor increase in latency, ranging from 0.18 to 6.78% across a number of benchmark suites. © 2007, ACM. All rights reserved.",Communication links; Design; Dynamic voltage scaling; Interconnection networks; Management; On-chip networks; Performance; Simulation; Software-directed power reduction,
Architecting a Reliable CMP Switch Architecture,2007,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649634851&doi=10.1145%2f1216544.1216545&partnerID=40&md5=a9365a776f4ad852a9868824c20164e6,"As silicon technologies move into the nanometer regime, transistor reliability is expected to wane as devices become subject to extreme process variation, particle-induced transient errors, and transistor wear-out. Unless these challenges are addressed, computer vendors can expect low yields and short mean-times-to-failure. In this article, we examine the challenges of designing complex computing systems in the presence of transient and permanent faults. We select one small aspect of a typical chip multiprocessor (CMP) system to study in detail, a single CMP router switch. Our goal is to design a BulletProof CMP switch architecture capable of tolerating significant levels of various types of defects. We first assess the vulnerability of the CMP switch to transient faults. To better understand the impact of these faults, we evaluate our CMP switch designs using circuitlevel timing on detailed physical layouts. Our infrastructure represents a new level of fidelity in architectural-level fault analysis, as we can accurately track faults as they occur, noting whether they manifest or not, because of masking in the circuits, logic, or architecture. Our experimental results are quite illuminating. We find that transient faults, because of their fleeting nature, are of little concern for our CMP switch, even within large switch fabrics with fast clocks. Next, we develop a unified model of permanent faults, based on the time-tested bathtub curve. Using this convenient abstraction, we analyze the reliability versus area tradeoff across a wide spectrum of CMP switch designs, ranging from unprotected designs to fully protected designs with on-line repair and recovery capabilities. Protection is considered at multiple levels from the entire system down through arbitrary partitions of the design. We find that designs are attainable that can tolerate a larger number of defects with less overhead than na¨ýve triple-modular redundancy, usingdomain-specific techniques, such as end-to-end error detection, resource sparing, automatic circuit decomposition, and iterative diagnosis and reconfiguration. © 2007, ACM. All rights reserved.",CMP switch; defect-tolerance; Design; Reliability; reliability,
Single-Dimension Software Pipelining for Multidimensional Loops,2007,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006926521&doi=10.1145%2f1216544.1216550&partnerID=40&md5=5ab16fa7f546dcd992efe884caff5b3f,"Traditionally, software pipelining is applied either to the innermost loop of a given loop nest or from the innermost loop to outer loops. This paper proposes a three-step approach, called singledimension software pipelining (SSP), to software pipeline a loop nest at an arbitrary loop level that has a rectangular iteration space and contains no sibling inner loops in it. The first step identifies the most profitable loop level for software pipelining in terms of initiation rate, data reuse potential, or any other optimization criteria. The second step simplifies the multidimensional datadependence graph (DDG) of the selected loop level into a one-dimensional DDG and constructs a one-dimensional (1D) schedule. Based on the one-dimensional schedule, the third step derives a simple mapping function that specifies the schedule time for the operation instances in the multidimensional loop. The classical modulo scheduling is subsumed by SSP as a special case. SSP is also closely related to hyperplane scheduling, and, in fact, extends it to be resource constrained. We prove that SSP schedules are correct and at least as efficient as those schedules generated by traditional modulo scheduling methods.We extend SSP to schedule imperfect loop nests, which are most common at the instruction level. Multiple initiation intervals are naturally allowed to improve execution efficiency. Feasibility and correctness of our approach are verified by a prototype implementation in the ORC compiler for the IA-64 architecture, tested with loop nests from Livermore and SPEC2000 floating-point benchmarks. Preliminary experimental results reveal that, compared to modulo scheduling, software pipelining at an appropriate loop level results in significant performance improvement. Software pipelining is beneficial even with prior loop transformations. © 2007, ACM. All rights reserved.",Algorithms; Languages; Loop transformation; Modulo scheduling; Software pipelining,
A Compiler Cost Model for Speculative Parallelization,2007,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997799602&doi=10.1145%2f1250727.1250732&partnerID=40&md5=dd1df9181f2f2b73459ee336864b5a4a,"Speculative parallelization is a technique that allows code sections that cannot be fully analyzed by the compiler to be aggressively executed in parallel. However, while speculative parallelization can potentially deliver significant speedups, several overheads associated with this technique can limit these speedups in practice. This paper proposes a novel compiler static cost model of speculative multithreaded execution that can be used to predict the resulting performance. This model attempts to predict the expected speedups, or slowdowns, of the candidate speculative sections based on the estimation of the combined runtime effects of various overheads, and taking into account the scheduling restrictions of most speculative execution environments. The model is based on estimating the likely execution duration of threads and considers all the possible permutations of these threads. This model also produces a quantitative estimate of the speedup, which is different from prior heuristics that only qualitatively estimate the benefits of speculative multithreaded execution. In previous work, a limited version of the framework was evaluated on a number of loops from a collection of SPEC benchmarks that suffer mainly from load imbalance and thread dispatch and commit overheads. In this work, an extended framework is also evaluated on loops that may suffer from data-dependence violations. Experimental results show that prediction accuracy is lower when loops with violations are included. Nevertheless, accuracy is still very high for a static model: the framework can identify, on average, 45% of the loops that cause slowdowns and, on average, 96% of the loops that lead to speedups; it predicts the speedups or slowdowns with an error of less than 20% for an average of 28% of the loops across the benchmarks and with an error of less than 50% for an average of 80% of the loops. Overall, the framework often outperforms, by as much as 25%, a naive approach that attempts to speculatively parallelize all the loops considered, and is able to curb the large slowdowns caused in many cases by this naive approach. © 2007, ACM. All rights reserved.",,
Effective Management of Multiple Configurable Units Using Dynamic Optimization,2006,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65249102561&doi=10.1145%2f1187976.1187981&partnerID=40&md5=d6722fb1a72ecc9de0aef78bac0aeafb,"As one of the promising efforts to minimize the surging microprocessor power consumption, adaptive computing environments (ACEs), where microarchitectural resources can be dynamically tuned to match a program’s run-time requirement and characteristics, are becoming increasingly common. In an ACE, efficient management of the configurable units (CUs) is vital for maximizing the benefit of resource adaptation. ACEs usually have multiple configurable hardware units, necessitating exploration of a large number of combinatorial configurations in order to identify the most energy-efficient configuration. In this paper, we propose an ACE management framework for efficient management of multiple CUs, utilizing dynamic optimization systems’ inherent capabilities of detecting and optimizing program hotspots, i.e., dominate code regions.We develop a scheme where hotpot boundaries are used for phase detection and adaptation. The framework achieves good energy reduction on managing multiple CUs with minimal hardware requirements and low implement cost by leveraging the existing infrastructure of a dynamic optimization system. The proposed framework is evaluated by dynamically adapting five CUs with distinct reconfiguration latencies and overheads. Those CUs are issue queue, reorder buffer, level-one data and instruction caches, and level-two cache. Previous research indicates that those five components dominate the energy consumption of a microprocessor. Despite the growing complexity and overhead of adapting five CUs, our technique reduces the energy consumption of those CUs by as much as 45%, while one of the best techniques provided by prior literature achieves less than 15% energy reduction for all CUs. © 2006, ACM. All rights reserved.",Adaptive computing environment (ACE); Design; Dynamic optimization; Experimentation; Hotspots; Performance; Power dissipation,
Implicit Array Bounds Checking on 64-Bit Architectures,2006,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025384050&doi=10.1145%2f1187976.1187982&partnerID=40&md5=4beb7fb9ae36d5215f5c2785e8772b8b,"Several programming languages guarantee that array subscripts are checked to ensure they are within the bounds of the array. While this guarantee improves the correctness and security of arraybased code, it adds overhead to array references. This has been an obstacle to using higher-level languages, such as Java, for high-performance parallel computing, where the language specification requires that all array accesses must be checked to ensure they are within bounds. This is because, in practice, array-bounds checking in scientific applications may increase execution time by more than a factor of 2. Previous research has explored optimizations to statically eliminate bounds checks, but the dynamic nature of many scientific codes makes this difficult or impossible. Our approach is, instead, to create a compiler and operating system infrastructure that does not generate explicit bounds checks. It instead places arrays inside of Index Confinement Regions (ICRs), which are large, isolated, mostly unmapped virtual memory regions. Any array reference outside of its bounds will cause a protection violation; this provides implicit bounds checking. Our results show that when applying this infrastructure to high-performance computing programs written in Java, the overhead of bounds checking relative to a program with no bounds checks is reduced from an average of 63% to an average of 9%. © 2006, ACM. All rights reserved.",64-bit architectures; Array-bounds checking; Measurement; Performance; Virtual memory,
"A Case for a Complexity-Effective, Width-Partitioned Microarchitecture",2006,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46649121102&doi=10.1145%2f1162690.1162693&partnerID=40&md5=bd50e08a3c734c4eb215a2d6276e14be,"The analysis of program executions reveals that most integer and multimedia applications make heavy use of narrow-width operations, i.e., instructions exclusively using narrow-width operands and producing a narrow-width result. Moreover, this usage is relatively well distributed over the application. We observed this program property on the MediaBench and SPEC2000 benchmarks with about 40% of the instructions being narrow-width operations. Current superscalar processors use 64-bit datapaths to execute all the instructions of the applications. In this paper, we suggest the use of a width-partitioned microarchitecture (WPM) to master the hardware complexity of a superscalar processor. For a four-way issue machine, we split the processor in two two-way clusters: the main cluster executing 64-bit operations, load/store, and complex operations and a narrow cluster executing the 16-bit operations. We resort to partitioning to decouple the treatment of the narrowwidth operations from that of the other program instructions. This provides the benefit of greatly simplifying the design of the critical processor components in each cluster (e.g., the register file and the bypass network). The dynamic interleaving of the two instruction types allows maintaining the workload balanced among clusters.WPM also helps to reduce the complexity of the interconnection fabric and of the issue logic. In fact, since the 16-bit cluster can only communicate narrow-width data, the datapath-width of the interconnect fabric can be significantly reduced, yielding a corresponding saving of the interconnect power and area. We explore different possible configurations of WPM, discussing the various implementation tradeoffs. We also examine a speculative steering heuristic to distribute the narrow-width operations among clusters. A detailed analysis of the complexity factors shows using WPM instead of a classical 64-bit two-cluster microarchitecture can save power and silicon area with a minimal impact on the overall performance. © 2006, ACM. All rights reserved.",Clustered Microarchitecture; Complexity-Effective Design; Narrow-Width Operations; Power analysis,
Bit-Split String-Matching Engines for Intrusion Detection and Prevention,2006,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991477170&doi=10.1145%2f1132462.1132464&partnerID=40&md5=8fdb18a801ac237d8db8b11d1f615619,"Network Intrusion Detection and Prevention Systems have emerged as one of the most effective ways of providing security to those connected to the network and at the heart of almost every modern intrusion detection system is a string-matching algorithm. String matching is one of the most critical elements because it allows for the system to make decisions based not just on the headers, but the actual content flowing through the network. Unfortunately, checking every byte of every packet to see if it matches one of a set of thousands of strings becomes a computationally intensive task as network speeds grow into the tens, and eventually hundreds, of gigabits/second. To keep up with these speeds, a specialized device is required, one that can maintain tight bounds on worst-case performance, that can be updated with new rules without interrupting operation, and one that is efficient enough that it could be included on-chip with existing network chips or even into wireless devices. We have developed an approach that relies on a special purpose architecture that executes novel string matching algorithms specially optimized for implementation in our design. We show how the problem can be solved by converting the large database of strings into many tiny state machines, each of which searches for a portion of the rules and a portion of the bits of each rule. Through the careful codesign and optimization of our architecture with a new string-matching algorithm, we show that it is possible to build a system that is 10 times more efficient than the currently best known approaches. © 2006, ACM. All rights reserved.",Algorithms; Design; Performance; Security; Security; State machine splitting; String-matching architecture,
Instruction Packing: Toward Fast and Energy-Efficient Instruction Scheduling,2006,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36749092737&doi=10.1145%2f1138035.1138037&partnerID=40&md5=455e05a5ada2128eec2d4538f0af66aa,"Traditional dynamic scheduler designs use one issue queue entry per instruction, regardless of the actual number of operands actively involved in the wakeup process. We propose Instruction Packing—a novel microarchitectural technique that reduces both delay and power consumption of the issue queue by sharing the associative part of an issue queue entry between two instructions, each with, at most, one nonready register source operand at the time of dispatch. Our results show that this technique results in 40% reduction of the IQ power and 14% reduction in scheduling delay with negligible IPC degradations. © 2006, ACM. All rights reserved.",Instruction packing; Issue queue; Low power; Performance; Power; Superscalar Processors,
Block-Aware Instruction Set Architecture,2006,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38849096758&doi=10.1145%2f1162690.1162694&partnerID=40&md5=1efcff1eeeb2106c786d8ff6c1c5fc42,"Instruction delivery is a critical component for wide-issue, high-frequency processors since its bandwidth and accuracy place an upper limit on performance. The processor front-end accuracy and bandwidth are limited by instruction-cache misses, multicycle instruction-cache accesses, and target or direction mispredictions for control-flow operations. This paper presents a block-aware instruction set (BLISS) that allows software to assist with front-end challenges. BLISS defines basic block descriptors that are stored separately from the actual instructions in a program. We show that BLISS allows for a decoupled front-end that tolerates instruction-cache latency, facilitates instruction prefetching, and leads to higher prediction accuracy. © 2006, ACM. All rights reserved.",Basic block; Branch prediction; Decoupled architecture; Instruction fetch; Instruction set architecture; Software hints,
Evaluating Trace Cache Energy Efficiency,2006,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978290201&doi=10.1145%2f1187976.1187980&partnerID=40&md5=52d4e42f76bc7960bbb3b140dd9dc058,"Future fetch engines need to be energy efficient. Much research has focused on improving fetch bandwidth. In particular, previous research shows that storing concatenated basic blocks to form instruction traces can significantly improve fetch performance. This work evaluates whether this concatenating of basic blocks translates to significant energy-efficiency gains. We compare processor performance and energy efficiency in trace caches compared to instruction caches. We find that, although trace caches modestly outperform instruction cache only alternatives, it is branchprediction accuracy that really determines performance and energy efficiency. When access delay and area restrictions are considered, our results show that sequential trace caches achieve very similar performance and energy efficiency results compared to instruction cache-based fetch engines and show that the trace cache’s failure to significantly outperform the instruction cache-based fetch organizations stems from the poorer implicit branch prediction from the next-trace predictor at smaller areas. Because access delay limits the theoretical performance of the evaluated fetch engines, we also propose a novel ahead-pipelined next-trace predictor. Our results show that an STC fetch organization with a three-stage, ahead-pipelined next-trace predictor can achieve 5–17% IPC and 29% ED2 improvements over conventional, unpipelined organizations. © 2006, ACM. All rights reserved.",Fetch engine energy efficiency; Measurement; Performance; Trace cache,
Analysis of Cache-Coherence Bottlenecks with Hybrid Hardware/Software Techniques,2006,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84993767534&doi=10.1145%2f1187976.1187978&partnerID=40&md5=197c5f93a4bf65358dd05804c9a5ef4d,"Application performance on high-performance shared-memory systems is often limited by sharing patterns resulting in cache-coherence bottlenecks. Current approaches to identify coherence bottlenecks incur considerable run-time overhead and do not scale. We present two novel hardwareassisted coherence-analysis techniques that reduce trace sizes by two orders of magnitude over full traces. First, hardware performance monitoring is combined with capturing stores in software to provide a lossy-trace mechanism, which is an order of magnitude faster than softwareinstrumentation- based full-tracing and retains accuracy. Second, selected long-latency loads are instrumented via binary rewriting, which provides even higher accuracy and control over tracing, but requires additional overhead. © 2006, ACM. All rights reserved.",Cache analysis; Coherence protocols; Dynamic binary rewriting; Hardware performance monitoring; Measurement; Performance; Program instrumentation; Smps,
A Lifetime Optimal Algorithm for Speculative PRE,2006,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991969406&doi=10.1145%2f1138035.1138036&partnerID=40&md5=f2cbfe139a76a5fc3c1b1392073d47bd,"A lifetime optimal algorithm, called MC-PRE, is presented for the first time that performs speculative PRE based on edge profiles. In addition to being computationally optimal in the sense that the total number of dynamic computations for an expression in the transformed code is minimized, MC-PRE is also lifetime optimal since the lifetimes of introduced temporaries are also minimized. The key in achieving lifetime optimality lies not only in finding a unique minimum cut on a transformed graph of a given CFG, but also in performing a data-flow analysis directly on the CFG to avoid making unnecessary code insertions and deletions. The lifetime optimal results are rigorously proved. We evaluate our algorithm in GCC against three previously published PRE algorithms, namely,MC-PREcomp (Qiong and Xue’s computationally optimal version of MC-PRE), LCM (Knoop, R¨ uthing, and Steffen’s lifetime optimal algorithm for performing nonspeculative classic PRE), and CMP-PRE (Bodik, Gupta, and Soffa’s PRE algorithm based on code-motion preventing (CMP) regions, which is speculative but not computationally optimal). We report and analyze our experimental results, obtained from both actual program execution and instrumentation, for all 22 C, C++ and FORTRAN 77 benchmarks from SPECcpu2000 on an Itanium 2 computer system. Our results show that MC-PRE (or MC-PREcomp) is capable of eliminating more partial redundancies than both LCM and CMP-PRE (especially in functions with complex control flow), and, in addition, MC-PRE inserts temporaries with shorter lifetimes than MC-PREcomp. Each of both benefits has contributed to the performance improvements in benchmark programs at the costs of only small compile-time and code-size increases in some benchmarks. © 2006, ACM. All rights reserved.",Algorithms; Classic PRE; Computational optimality; Data-flow analysis; Experimentation; Languages; Lifetime optimality; Partial redundancy elimination; Performance; Speculative PRE,
Efficient Remote Profiling for Resource-Constrained Devices,2006,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960809113&doi=10.1145%2f1132462.1132465&partnerID=40&md5=f65600c3e8fa0133d28ee331a9838882,"The widespread use of ubiquitous, mobile, and continuously connected computing agents has inspired software developers to change the way they test, debug, and optimize software. Users now play an active role in the software evolution cycle by dynamically providing valuable feedback about the execution of a program to developers. Software developers can use this information to isolate bugs in, maintain, and improve the performance of a wide-range of diverse and complex embedded device applications. The collection of such feedback poses a major challenge to systems researchers since it must be performed without degrading a user’s experience with, or consuming the severely restricted resources of the mobile device. At the same time, the resource constraints of embedded devices prohibit the use of extant software profiling solutions. To achieve efficient remote profiling of embedded devices, we couple two efficient hardware/software program monitoring techniques: Hybrid Profiling Support(HPS) and Phase-Aware Sampling. HPS efficiently inserts profiling instructions into an executing program using a novel extension to Dynamic-Instruction Stream Editing(DISE). Phase-aware sampling exploits the recurring behavior of programs to identify key opportunities during execution in order to collect profile information (i.e. sample). Our prior work on phase-aware sampling required code duplication to toggle sampling. By guiding low-overhead, hardware-supported sampling according to program phase behavior via HPS, our system is able to collect highly accurate profiles transparently. We evaluate our system assuming a general purpose configuration as well as a popular handheld device configuration. We measure the accuracy and overhead of our techniques and quantify the overhead in terms of computation, communication, and power consumption. We compare our system to random and periodic sampling for a number of widely used performance profile types. Our results indicate that our system significantly reduces the overhead of sampling while maintaining high accuracy. © 2006, ACM. All rights reserved.",Performance; Phased behavior; Profiling; Resource-constrained devices; Sampling,
Optimal register reassignment for register stack overflow minimization,2006,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864120566&doi=10.1145%2f1132462.1132467&partnerID=40&md5=ce80d54df419946ec1f6746282de360d,"Architectures with a register stack can implement efficient calling conventions. Using the overlapping of callers’ and callees’ registers, callers are able to pass parameters to callees without a memory stack. The most recent instance of a register stack can be found in the Intel Itanium architecture. A hardware component called the register stack engine (RSE) provides an illusion of an infinite-length register stack using a memory-backed process to handle overflow and underflow for a physically limited number of registers. Despite such hardware support, some applications suffer from the overhead required to handle register stack overflow and underflow. The memory latency associated with the overflow and underflow of a register stack can be reduced by generating multiple register allocation instructions within a procedure [Settle et al. 2003]. Live analysis is utilized to find a set of registers that are not required to keep their values across procedure boundaries. However, among those dead registers, only the registers that are consecutively located in a certain part of the register stack frame can be removed. We propose a compiler-supported register reassignment technique that reduces RSE overflow/underflow further. By reassigning registers based on live analysis, our technique forces as many dead registers to be removed as possible. We define the problem of optimal register reassignment, which minimizes interprocedural register stack heights considering multiple call sites within a procedure. We present how this problem is related to a path-finding problem in a graph called a sequence graph. We also propose an efficient heuristic algorithm for the problem. Finally, we present the measurement of effects of the proposed techniques on SPEC CINT2000 benchmark suite and the analysis of the results. The result shows that our approach reduces the RSE cycles by 6.4% and total cpu cycles by 1.7% on average. © 2006, ACM. All rights reserved.",Algorithms; Languages; Performance; Register allocation; Register assignment; Register stack; Sequence graph,
Managing Bounded Code Caches in Dynamic Binary Optimization Systems,2006,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994781115&doi=10.1145%2f1162690.1162692&partnerID=40&md5=e3fb14b9bbb0c9a0b1f86aafb226e6b9,"Dynamic binary optimizers store altered copies of original program instructions in softwaremanaged code caches in order to maximize reuse of transformed code. Code caches store code blocks that may vary in size, reference other code blocks, and carry a high replacement overhead. These unique constraints reduce the effectiveness of conventional cache management policies. Our work directly addresses these unique constraints and presents several contributions to the code-cache management problem. First, we show that evicting more than the minimum number of code blocks from the code cache results in less run-time overhead than the existing alternatives. Such granular evictions reduce overall execution time, as the fixed costs of invoking the eviction mechanism are amortized across multiple cache insertions. Second, a study of the ideal lifetimes of dynamically generated code blocks illustrates the benefit of a replacement algorithm based on a generational heuristic. We describe and evaluate a generational approach to code cache management that makes it easy to identify long-lived code blocks and simultaneously avoid any fragmentation because of the eviction of short-lived blocks. Finally, we present results from an implementation of our generational approach in the DynamoRIO framework and illustrate that, as dynamic optimization systems become more prevalent, effective code cache-management policies will be essential for reliable, scalable performance of modern applications. © 2006, ACM. All rights reserved.",Code caches; Dynamic optimization; Dynamic translation; Just-in-time compilation; Languages; Management; Measurement; Performance,
An Approach Toward Profit-Driven Optimization,2006,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68349085864&doi=10.1145%2f1162690.1162691&partnerID=40&md5=54e38385ca36fc57eb2352f0cdd9e2c0,"Although optimizations have been applied for a number of years to improve the performance of software, problems with respect to the application of optimizations have not been adequately addressed. For example, in certain circumstances, optimizations may degrade performance. However, there is no efficient way to know when a degradation will occur. In this research, we investigate the profitability of optimizations, which is useful for determining the benefit of applying optimizations. We develop a framework that enables us to predict profitability using analytic models. The profitability of an optimization depends on code context, the particular optimization, and machine resources. Thus, our framework has analytic models for each of these components. As part of the framework, there is also a profitability engine that uses models to predict the profit. In this paper, we target scalar optimizations and, in particular, describe the models for partial redundancy elimination (PRE), loop invariant code motion (LICM), and value numbering (VN). We implemented the framework for predicting the profitability of these optimizations. Based on the predictions, we can selectively apply profitable optimizations. We compared the profit-driven approach with an approach that uses a heuristic in deciding when optimizations should be applied. Our experiments demonstrate that the profitability of scalar optimizations can be accurately predicted by using models. That is, without actually applying a scalar optimization, we can determine if an optimization is beneficial and should be applied. © 2006, ACM. All rights reserved.",Design and Experimentation; Performance; Prediction; Profit-driven; Profitability,
Minos: Architectural Support for Protecting Control Data,2006,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017025817&doi=10.1145%2f1187976.1187977&partnerID=40&md5=ff1778dbeadc1c51adda1f50be602981,"We present Minos, a microarchitecture that implements Biba’s low water-mark integrity policy on individual words of data. Minos stops attacks that corrupt control data to hijack program control flow, but is orthogonal to the memory model. Control data is any data that is loaded into the program counter on control-flow transfer, or any data used to calculate such data. The key is that Minos tracks the integrity of all data, but protects control flow by checking this integrity when a program uses the data for control transfer. Existing policies, in contrast, need to differentiate between control and noncontrol data a priori, a task made impossible by coercions between pointers and other data types, such as integers in the C language. Our implementation of Minos for Red Hat Linux 6.2 on a Pentium-based emulator is a stable, usable Linux system on the network on which we are currently running a web server (http://minos.cs.ucdavis.edu). Our emulated Minos systems running Linux andWindows have stopped ten actual attacks. Extensive full-system testing and real-world attacks have given us a unique perspective on the policy tradeoffs that must be made in any system, such as Minos; this paper details and discusses these.We also present a microarchitectural implementation of Minos that achieves negligible impact on cycle time with a small investment in die area, as well as and minor changes to the Linux kernel to handle the tag bits and perform virtual memory swapping. © 2006, ACM. All rights reserved.",Buffer overflow; Control data; Security; Worms,
Efficient Address Remapping in Distributed Shared-Memory Systems,2006,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991225853&doi=10.1145%2f1138035.1138039&partnerID=40&md5=29866ea153e22dadd1f7e733c015851b,"As processor performance continues to improve at a rate much higher than DRAM and network performance, we are approaching a time when large-scale distributed shared memory systems will have remote memory latencies measured in tens of thousands of processor cycles. The Impulse memory system architecture adds an optional level of address indirection at the memory controller. Applications can use this level of indirection to control how data is accessed and cached and thereby improve cache and bus utilization and reduce the number of memory accesses required. Previous Impulse work focuses on uniprocessor systems and relies on software to flush processor caches when necessary to ensure data coherence. In this paper, we investigate an extension of Impulse to multiprocessor systems that extends the coherence protocol to maintain data coherence without requiring software-directed cache flushing. Specifically, the multiprocessor Impulse controller can gather/scatter data across the network while its coherence protocol guarantees that each gather request gets coherent data and each scatter request updates every coherent replica in the system. Our simulation results demonstrate that the proposed system can significantly outperform conventional systems, achieving an average speedup of 9X on four memory-bound benchmarks on a 32-processor system. © 2006, ACM. All rights reserved.",Address remapping; Cache coherence; Design; Multiprocessors; Performance evaluation; Shadow address; Smart memory controller,
Recovery code generation for general speculative optimizations,2006,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969375727&doi=10.1145%2f1132462.1132466&partnerID=40&md5=af2450cd30ef86893c5bc1e56458c872,"A general framework that integrates both control and data speculation using alias profiling and/or compiler heuristic rules has shown to improve CPU2000 performance on Itanium systems. However, speculative optimizations require check instructions and recovery code to ensure correct execution when speculation fails at runtime. How to generate check instructions and their associated recovery code efficiently and effectively is an issue yet to be well studied. It is also, very important that the recovery code generated in the earlier phases integrate gracefully in the later optimization phases. At the very least, it should not hinder later optimizations, thus, ensuring overall performance improvement. This paper proposes a framework that uses an if-block structure to facilitate check instructions and recovery code generation for general speculative optimizations. It allows speculative instructions and their recovery code generated in the early compiler optimization phases to be integrated effectively with the subsequent optimization phases. It also allows multilevel speculation for multilevel pointers and multilevel expression trees to be handled with no additional complexity. The proposed recovery code generation framework has been implemented and evaluated in the Open Research Compiler (ORC). © 2006, ACM. All rights reserved.",Algorithms; Design; Experimentation; Multi-level data speculation; Performance; Recovery code; Speculative SSA form,
Future Execution: A Prefetching Mechanism that Uses Multiple Cores to Speed up Single Threads,2006,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47349095223&doi=10.1145%2f1187976.1187979&partnerID=40&md5=a809443129047b91b7156d296d980c59,"This paper describes future execution (FE), a simple hardware-only technique to accelerate individual program threads running on multicore microprocessors. Our approach uses available idle cores to prefetch important data for the threads executing on the active cores. FE is based on the observation that many cache misses are caused by loads that execute repeatedly and whose address-generating program slices do not change (much) between consecutive executions. To exploit this property, FE dynamically creates a prefetching thread for each active core by simply sending a copy of all committed, register-writing instructions to an otherwise idle core. The key innovation is that on the way to the second core, a value predictor replaces each predictable instruction in the prefetching thread with a load immediate instruction, where the immediate is the predicted result that the instruction is likely to produce during its nth next dynamic execution. Executing this modified instruction stream (i.e., the prefetching thread) on another core allows to compute the future results of the instructions that are not directly predictable, issue prefetches into the shared memory hierarchy, and thus reduce the primary threads’ memory access time.We demonstrate the viability and effectiveness of future execution by performing cycle-accurate simulations of a two-way CMP running the single-threaded SPECcpu2000 benchmark suite. Our mechanism improves program performance by 12%, on average, over a baseline that already includes an optimized hardware stream prefetcher. We further show that FE is complementary to runahead execution and that the combination of these two techniques raises the average speedup to 20% above the performance of the baseline processor with the aggressive stream prefetcher. © 2006, ACM. All rights reserved.",Chip multiprocessors; Design; Future execution; Memory wall; Performance; Prefetching,
CAVA: Using Checkpoint-Assisted Value Prediction to Hide L2 Misses,2006,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006558464&doi=10.1145%2f1138035.1138038&partnerID=40&md5=d57aabbbec07a5f391d4b1230c3acf88,"Modern superscalar processors often suffer long stalls because of load misses in on-chip L2 caches. To address this problem, we propose hiding L2 misses with Checkpoint-Assisted VAlue prediction (CAVA). On an L2 cache miss, a predicted value is returned to the processor. When the missing load finally reaches the head of the ROB, the processor checkpoints its state, retires the load, and speculatively uses the predicted value and continues execution. When the value in memory arrives at the L2 cache, it is compared to the predicted value. If the prediction was correct, speculation has succeeded and execution continues; otherwise, execution is rolled back and restarted from the checkpoint. CAVA uses fast checkpointing, speculative buffering, and a modest-sized value prediction structure that has about 50% accuracy. Compared to an aggressive superscalar processor, CAVA speeds up execution by up to 1.45 for SPECint applications and 1.58 for SPECfp applications, with a geometric mean of 1.14 for SPECint and 1.34 for SPECfp applications. We also evaluate an implementation of Runahead execution—a previously proposed scheme that does not perform value prediction and discards all work done between checkpoint and data reception from memory. Runahead execution speeds up execution by a geometric mean of 1.07 for SPECint and 1.18 for SPECfp applications, compared to the same baseline. © 2006, ACM. All rights reserved.",Checkpointed processor architectures; Design; Memory hierarchies; Multiprocessor; Performance; Value prediction,
Introduction,2006,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025396458&doi=10.1145%2f1132462.1132463&partnerID=40&md5=ef7d4a2736697d3e04c702369e9ac01e,[No abstract available],,
Whole Execution Traces and Their Applications,2005,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997350878&doi=10.1145%2f1089008.1089012&partnerID=40&md5=b76f9bbb8fcecdaeddc7731b287fd92e,"Different types of program profiles (control flow, value, address, and dependence) have been collected and extensively studied by researchers to identify program characteristics that can then be exploited to develop more effective compilers and architectures. Because of the large amounts of profile data produced by realistic program runs, most work has focused on separately collecting and compressing different types of profiles. In this paper, we present a unified representation of profiles called Whole Execution Trace (WET), which includes the complete information contained in each of the above types of traces. Thus, WETs provide a basis for a next-generation software tool that will enable mining of program profiles to identify program characteristics that require understanding of relationships among various types of profiles. The key features of our WET representation are: WET is constructed by labeling a static program representation with profile information such that relevant and related profile information can be directly accessed by analysis algorithms as they traverse the representation; a highly effective two-tier strategy is used to significantly compress the WET; and compression techniques are designed such that they minimally affect the ability to rapidly traverse WET for extracting subsets of information corresponding to individual profile types as well as a combination of profile types. Our experimentation shows that on, an average, execution traces resulting from execution of 647 million statements can be stored in 331 megabytes of storage after compression. The compression factors range from 16 to 83. Moreover the rates at which different types of profiles can be individually or simultaneously extracted are high. We present two applications of WETs, dynamic program slicing and dynamic version matching, which make effective use of multiple kinds of profile information contained in WETs. © 2005, ACM. All rights reserved.",Addresses; Algorithms; Compression; Control flow; Dependences; Measurement; Performance; Profiling; Values,
IATAC: A Smart Predictor to Turn-Off L2 Cache Lines,2005,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016664946&doi=10.1145%2f1061267.1061271&partnerID=40&md5=291f9156c36f7e2035f310cd57c45ed3,"As technology evolves, power dissipation increases and cooling systems become more complex and expensive. There are two main sources of power dissipation in a processor: dynamic power and leakage. Dynamic power has been the most significant factor, but leakage will become increasingly significant in future. It is predicted that leakage will shortly be the most significant cost as it grows at about a 5× rate per generation. Thus, reducing leakage is essential for future processor design. Since large caches occupy most of the area, they are one of the leakiest structures in the chip and hence, a main source of energy consumption for future processors. This paper introduces IATAC (inter-access time per access count), a new hardware technique to reduce cache leakage for L2 caches. IATAC dynamically adapts the cache size to the program requirements turning off cache lines whose content is not likely to be reused. Our evaluation shows that this approach outperforms all previous state-of-the-art techniques. IATAC turns off 65% of the cache lines across different L2 cache configurations with a very small performance degradation of around 2%. © 2005, ACM. All rights reserved.",Cache memories; Design; L2 cache; Low power; Performance; Turning off cache lines,
A Way-Halting Cache for Low-Energy High-Performance Systems,2005,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996712396&doi=10.1145%2f1061267.1061270&partnerID=40&md5=13ac96690c3816c9d2dbb8f48c7dae3c,"Caches contribute to much of a microprocessor system's power and energy consumption. Numerous new cache architectures, such as phased, pseudo-set-associative, way predicting, reactiveassociative, way-shutdown, way-concatenating, and highly-associative, are intended to reduce power and/or energy, but they all impose some performance overhead. We have developed a new cache architecture, called a way-halting cache, that reduces energy further than previously mentioned architectures, while imposing no performance overhead. Our way-halting cache is a four-way set-associative cache that stores the four lowest-order bits of all ways' tags into a fully associative memory, which we call the halt tag array. The lookup in the halt tag array is done in parallel with, and is no slower than, the set-index decoding. The halt tag array predetermines which tags cannot match due to their low-order 4 bits mismatching. Further accesses to ways with known mismatching tags are then halted, thus saving power. Our halt tag array has an additional feature of using static logic only, rather than dynamic logic used in highly associative caches, making our cache simpler to design with existing tools. We provide data from experiments on 29 benchmarks drawn from Powerstone, Mediabench, and Spec 2000, based on our layouts in 0.18 micron CMOS technology. On average, we obtained 55% savings of memory-access related energy over a conventional four-way set-associative cache. We show that savings are greater than previous methods, and nearly twice that of highly associative caches, while imposing no performance overhead and only 2% cache area overhead. © 2005, ACM. All rights reserved.",Cache; Design; Dynamic; Embedded systems; Experimentation; Low energy; Low power; Optimization; Performance,
Introduction,2005,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024267068&doi=10.1145%2f1061267.1061268&partnerID=40&md5=5e228a7d59e2b1d6f7522492d08dc39d,[No abstract available],,
Software-Controlled Fault Tolerance,2005,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019407607&doi=10.1145%2f1113841.1113843&partnerID=40&md5=407ab573b938a9cc9bb289a394ed0dbb,"Traditional fault-tolerance techniques typically utilize resources ineffectively because they cannot adapt to the changing reliability and performance demands of a system. This paper proposes software-controlled fault tolerance, a concept allowing designers and users to tailor their performance and reliability for each situation. Several software-controllable fault-detection techniques are then presented: SWIFT, a software-only technique, and CRAFT, a suite of hybrid hardware/ software techniques. Finally, the paper introduces PROFiT, a technique which adjusts the level of protection and performance at fine granularities through software control. When coupled with software-controllable techniques like SWIFT and CRAFT, PROFiT offers attractive and novel reliability options. © 2005, ACM. All rights reserved.",Fault detection; Reliability; Reliability; Software-controlled fault tolerance,
Accelerated Warmup for Sampled Microarchitecture Simulation,2005,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971355456&doi=10.1145%2f1061267.1061272&partnerID=40&md5=89b0b92ff4ec5a9f37f6df462dd2446d,"To reduce the cost of cycle-accurate software simulation of microarchitectures, many researchers use statistical sampling: by simulating only a small, representative subset of the end-to-end dynamic instruction stream in cycle-accurate detail, simulation results complete in much less time than simulating the cycle-by-cycle progress of an entire benchmark. In order for sampled simulation results to accurately reflect the nature the full dynamic instruction stream, however, state in the simulated cache and branch predictor must match or closely approximate state as it would have appeared had cycle-accurate simulation been used for the entire simulation. Researchers typically address this issue by prefixing a period of warmup—in which cache and branch predictor state are modeled in addition to programmer-visible architected state—to each cluster of contiguous instructions in the sample. One conservative, but slow approach is to always simulate cache and branch predictor state, whether among the cycle-accurate clusters, or among the instructions preceding each cluster. To save time, warmup heuristics have been proposed, but there is no one-size-fits-all heuristic for any benchmark. More rigorous, analytical warmup approaches are necessary in order to balance the requirements of high accuracy and rapidity from sampled simulations. This paper explores this issue and in particular demonstrates the merits of memory reference reuse latency (MRRL). Relative to the IPC measured by modeling all precluster cache and branch predictor activity, MRRL generated an average error in IPC of less than 1% and simultaneously reduced simulation running times by an average of approximately 50% (or 95% of the maximum potential speedup). © 2005, ACM. All rights reserved.",Measurement; Performance; Reuse latency; Sampled simulation; Warmup,
Spectral Prefetcher: An Effective Mechanism for L2 Cache Prefetching,2005,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981522272&doi=10.1145%2f1113841.1113845&partnerID=40&md5=995b9f26e1a2d0cbfcc5e8fdbe91b3c2,"Effective data prefetching requires accurate mechanisms to predict embedded patterns in the miss reference behavior. This paper proposes a novel prefetching mechanism, called the spectral prefetcher (SP), that accurately identifies the pattern by dynamically adjusting to its frequency. The proposed mechanism divides the memory address space into tag concentration zones (TCzones) and detects either the pattern of tags (higher order bits) or the pattern of strides (differences between consecutive tags) within each TCzone. The prefetcher dynamically determines whether the pattern of tags or strides will increase the effectiveness of prefetching and switches accordingly. To measure the performance of our scheme, we use a cycle-accurate aggressive out-of-order simulator that models bus occupancy, bus protocol, and limited bandwidth. Our experimental results show performance improvement of 1.59, on average, and at best 2.10 for the memory-intensive benchmarks we studied. Further, we show that SP outperforms the previously proposed scheme, with twice the size of SP, by 39% and a larger L2 cache, with equivalent storage area by 31%. © 2005, ACM. All rights reserved.",Absolute and differential domain; Adaptive; Autocorrelation; Design; Experimentation; Frequency; L2 cache; Memory; Performance; Prefetch,
Exploring the Limits of Leakage Power Reduction in Caches,2005,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006558078&doi=10.1145%2f1089008.1089009&partnerID=40&md5=4f5d96a29c476c387fa97a83943a667a,"If current technology scaling trends hold, leakage power dissipation will soon become the dominant source of power consumption. Caches, because of the fact that they account for the largest fraction of on-chip transistors in most modern processors, are a primary candidate for attacking the leakage problem. While there has been a flurry of research in this area over the last several years, a major question remains unanswered—What is the total potential of existing architectural and circuit techniques to address this important design concern? In this paper, we explore the limits in which existing circuit and architecture technologies may address this growing problem. We first formally propose a parameterized model that can determine the optimal leakage savings based on the perfect knowledge of the address trace. By carefully applying the sleep and drowsy modes, we find that the total leakage power from the L1 instruction cache, data cache, and a unified L2 cache may be reduced to mere 3.6, 0.9, and 2.3%, respectively, of the unoptimized case. We further study how such a model can be extended to obtain the optimal leakage power savings for different cache configurations. © 2005, ACM. All rights reserved.",Algorithms; Cache intervals; Experimentation; Leakage power; Limits; Performance,
Tradeoffs in Buffering Speculative Memory State for Thread-Level Speculation in Multiprocessors,2005,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149094318&doi=10.1145%2f1089008.1089010&partnerID=40&md5=2235ae0210092dec521ea76c3d5988e6,"Thread-Level Speculation (TLS) provides architectural support to aggressively run hard-to-analyze code in parallel. As speculative tasks run concurrently, they generate unsafe or speculative memory state that needs to be separately buffered and managed in the presence of distributed caches and buffers. Such a state may contain multiple versions of the same variable. In this paper, we introduce a novel taxonomy of approaches to buffer and manage multiversion speculative memory state in multiprocessors. We also present a detailed complexity-benefit tradeoff analysis of the different approaches. Finally, we use numerical applications to evaluate the performance of the approaches under a single architectural framework. Our key insights are that support for buffering the state of multiple speculative tasks and versions per processor is more complexity-effective than support for lazily merging the state of tasks with main memory. Moreover, both supports can be gainfully combined and, in large machines, their effect is nearly fully additive. Finally, the more complex support for storing future state in the main memory can boost performance when buffers are under pressure, but hurts performance when squashes are frequent. © 2005, ACM. All rights reserved.",Caching and buffering support; coherence protocol; Design; memory hierarchies; Performance; shared-memory multiprocessors; thread-level speculation,
Dynamic Memory Interval Test vs. Interprocedural Pointer Analysis in Multimedia Applications,2005,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-38549166028&doi=10.1145%2f1071604.1071608&partnerID=40&md5=010e031b0905864dc56b0fe57fac2248,"Techniques to detect aliasing between access patterns of array elements are quite effective for many numeric applications. However, although multimedia codes usually follow very regular memory access patterns, current commercial compilers remain unsuccessful in disambiguating them due mainly to complex pointer references. The Dynamic Memory Interval Test is a runtime memory disambiguation technique that takes advantage of the specific behavior of multimedia memory access patterns. It evaluates whether or not the full loop is disambiguated by analyzing the region domain of each load or store before each invocation of the loop. This paper provides a detailed evaluation of the approach, compares it against an advanced interprocedural pointer analysis framework, and analyzes the possibility of using both techniques at the same time. Both techniques achieve similar speedups separately (1.25X in average for a 8-issue width architecture). Furthermore, they can be used together to improve performance (reaching an average speed-up of 1.32X). Results also confirm that memory disambiguation is a key optimization to exploit the available parallelism in multimedia codes, especially for wide-issue architectures (1.50X average speed-up when scaling from 4- to 12-issue width in contrast to a low 1.10X for the baseline compiler). © 2005, ACM. All rights reserved.",Languages; Memory disambiguation; Multimedia; Performance; VLIW,
Efficient and Flexible Architectural Support for Dynamic Monitoring,2005,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547197629&doi=10.1145%2f1061267.1061269&partnerID=40&md5=edc2ec3399d5114acb172dd114a34a83,"Recent impressive performance improvements in computer architecture have not led to significant gains in the case of debugging. Software debugging often relies on inserting run-time software checks. In many cases, however, it is hard to find the root cause of a bug. Moreover, program execution typically slows down significantly, often by 10—100 times. To address this problem, this paper introduces the intelligent watcher (iWatcher), a novel architectural scheme to monitor dynamic execution automatically, flexibly, and with minimal overhead. iWatcher associates program-specified monitoring functions with memory locations. When any such location is accessed, the monitoring function is automatically triggered with low overhead. To further reduce overhead and support rollback, iWatcher can optionally leverage thread-level speculation (TLS). The iWatcher architecture can be used to detect various bugs, including buffer overflow, accessing freed locations, memory leaks, stack-smashing and value-invariant violations. To evaluate iWatcher, we use seven applications with various real and injected bugs. Our results show that iWatcher detects many more software bugs than Valgrind, a well-known open-source bug detector. Moreover, iWatcher only induces a 0.1—179% execution overhead, which is orders of magnitude less than Valgrind. Our sensitivity study shows that even with 20% of the dynamic loads monitored in a program, iWatcher adds only 72—182% overhead. Finally, TLS is effective at reducing overheads for programs with substantial monitoring. © 2005, ACM. All rights reserved.",Design; Reliability,
"The Design, Implementation, and Evaluation of Adaptive Code Unloading for Resource-Constrained Devices",2005,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954007874&doi=10.1145%2f1071604.1071606&partnerID=40&md5=0c5039a9a12c7259ee0776900750bfc1,"Java Virtual Machines (JVMs) for resource-constrained devices, e.g., hand-helds and cell phones, commonly employ interpretation for program translation. However, compilers are able to produce significantly better code quality, and, hence, use device resources more efficiently than interpreters, since compilers can consider large sections of code concurrently and exploit optimization opportunities. Moreover, compilation-based systems store code for reuse by future invocations obviating the redundant computation required for reinterpretation of repeatedly executed code. However, code storage required for compilation can increase the memory footprint of the virtual machine (VM) significantly. As a result, for devices with limited memory resources, this additional code storage may preclude some programs from executing, significantly increase memory management overhead, and substantially reduce the amount of memory available for use by the application. To address the limitations of native code storage, we present the design, implementation, and empirical evaluation of a compiled-code management system that can be integrated into any compilation-based JVM. The system unloads compiled code to reduce the memory footprint of the VM. It does so by dynamically identifying and unloading dead or infrequently used code; if the code is later reused, it is recompiled by the system. As such, our system adaptively trades off memory footprint and its associated memory management costs, with recompilation overhead. Our empirical evaluation shows that our code management system significantly reduces the memory requirements of a compile-only JVM, while maintaining the performance benefits enabled by compilation. We investigate a number of implementation alternatives that use dynamic program behavior and system resource availability to determine when to unload as well as what code to unload. From our empirical evaluation of these alternatives, we identify a set of strategies that enable significant reductions in the memory overhead required for application code. Our system reduces code size by 36—62%, on average, which translates into significant execution-time benefits for the benchmarks and JVM configurations that we studied. © 2005, ACM. All rights reserved.",Code unloading; Code-size reduction; JIT; JVM; Languages; Performance; Resourceconstrained devices,
Fast and Efficient Searches for Effective Optimization-Phase Sequences,2005,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961971066&doi=10.1145%2f1071604.1071607&partnerID=40&md5=61b25143d46a3cc05aea0f33d92783e8,"It has long been known that a fixed ordering of optimization phases will not produce the best code for every application. One approach for addressing this phase-ordering problem is to use an evolutionary algorithm to search for a specific sequence of phases for each module or function. While such searches have been shown to produce more efficient code, the approach can be extremely slow because the application is compiled and possibly executed to evaluate each sequence's effectiveness. Consequently, evolutionary or iterative compilation schemes have been promoted for compilation systems targeting embedded applications where meeting strict constraints on execution time, code size, and power consumption is paramount and longer compilation times may be tolerated in the final stage of development, when an application is compiled one last time and embedded in a product. Unfortunately, even for small embedded applications, the search process can take many hours or even days making the approach less attractive to developers. In this paper, we describe two complementary general approaches for achieving faster searches for effective optimization sequences when using a genetic algorithm. The first approach reduces the search time by avoiding unnecessary executions of the application when possible. Results indicate search time reductions of 62%, on average, often reducing searches from hours to minutes. The second approach modifies the search so fewer generations are required to achieve the same results. Measurements show this approach decreases the average number of required generations by 59%. These improvements have the potential for making evolutionary compilation a viable choice for tuning embedded applications. © 2005, ACM. All rights reserved.",Algorithms; Experimentation; Genetic algorithms; Interactive compilation; Measurement; Performance; Phase ordering,
Improving WCET by Applying a WC Code-Positioning Optimization,2005,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962788391&doi=10.1145%2f1113841.1113842&partnerID=40&md5=1c8a92e7e4eff14a8e95fd402d4bbf14,"Applications in embedded systems often need to meet specified timing constraints. It is advantageous to not only calculate the worst-case execution time (WCET) of an application, but to also perform transformation, which reduce the WCET, since an application with a lower WCET will be less likely to violate its timing constraints. Some processors incur a pipeline delay whenever an instruction transfers control to a target that is not the next sequential instruction. Code-positioning optimizations attempt to reduce these delays by positioning the basic blocks to minimize the number of unconditional jumps and taken conditional branches that occur. Traditional code-positioning algorithms use profile data to find the frequently executed edges between basic blocks, then minimize the transfers of control along these edges to reduce the average case execution time (ACET). This paper introduces a WCET code-positioning optimization, driven by the worst-case (WC) path information from a timing analyzer, to reduce the WCET instead of ACET. This WCET optimization changes the layout of the code in memory to reduce the branch penalties along theWCpaths. Unlike the frequency of edges in traditional profile-driven code positioning, the WC path may change after code-positioning decisions are made. Thus, WCET code positioning is inherently more challenging than ACET code positioning. The experimental results show that this optimization typically finds the optimal layout of the basic blocks with the minimal WCET. The results show over a 7% reduction in WCET is achieved after code positioning is performed. © 2005, ACM. All rights reserved.",,
Power-Performance Considerations of Parallel Computing on Chip Multiprocessors,2005,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548142850&doi=10.1145%2f1113841.1113844&partnerID=40&md5=41004166c61dd116332cfade8c36f3a4,"This paper looks at the power-performance implications of running parallel applications on chip multiprocessors (CMPs). First, we develop an analytical model that, for the first time, puts together parallel efficiency, granularity of parallelism, and voltage/frequency scaling, to establish a formal connection with the power consumption and performance of a parallel code running on a CMP. We then conduct detailed simulations of parallel applications running on a detailed powerperformance CMP model to confirm the analytical results and provide further insights. Both analytical and experimental models show that parallel computing can bring significant power savings and still meet a given performance target by choosing granularity and voltage/frequency levels judiciously. The particular choice, however, is dependent on the application’s parallel efficiency curve and the process technology utilized, which our model captures. Likewise, analytical model and experiments show the effect of a limited power budget on the application’s scalability curve. In particular, we show that a limited power budget can cause a rapid performance degradation beyond a number of cores, even in the case of applications with excellent scalability properties. On the other hand, our experiments show that, when a limited power budget is in place, power-thrifty memory-bound applications may actually enjoy better scalability than more computeintensive codes, even if the latter would exhibit higher scalability in a power-unconstrained scenario. © 2005, ACM. All rights reserved.",Chip Multiprocessors; Granularity; Parallel Computation; Parallel efficiency; Performance; Power; Theory; Voltage/frequency scaling,
Adapting Branch-Target Buffer to Improve The Target Predictability of Java Code,2005,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36048997661&doi=10.1145%2f1071604.1071605&partnerID=40&md5=6065894825db36555938a3ff7135d19b,"Java programs are increasing in popularity and prevalence on numerous platforms, including high-performance general-purpose processors. The success of Java technology largely depends on the efficiency in executing the portable Java bytecodes. However, the dynamic characteristics of the Java runtime system present unique performance challenges for several aspects of microarchitecture design. In this work, we focus on the effects of indirect branches on branch-target address prediction performance. Runtime bytecode translation, just-in-time (JIT) compilation, frequent calls to the native interface libraries, and dependence on virtual methods increase the frequency of polymorphic indirect branches. Therefore, accurate target address prediction for indirect branches is very important for Java code. This paper characterizes the indirect branch behavior in Java processing and proposes an adaptive branch-target buffer (BTB) design to enhance the predictability of the targets. Our characterization shows that a traditional BTB will frequently mispredict a few polymorphic indirect branches, significantly deteriorating predictor accuracy in Java processing. Therefore, we propose a rehashable branch-target buffer (R-BTB), which dynamically identifies polymorphic indirect branches and adapts branch-target storage to accommodate multiple targets for a branch. The R-BTB improves the target predictability of indirect branches without sacrificing overall target prediction accuracy. Simulations show that the R-BTB eliminates 61% of the indirect branch mispredictions suffered with a traditional BTB for Java programs running in interpreter mode (46% in JIT mode), which leads to a 57% decrease in overall target address misprediction rate (29% in JIT mode).With an equivalent number of entries, the R-BTB also outperforms the previously proposed target cache scheme for a majority of Java programs by adapting to a greater variety of indirect branch behaviors. © 2005, ACM. All rights reserved.",Branch prediction; Branch-target buffer (BTB); Computer architecture; Design; Java; Performance; Pipelined processor,
Merging Path and Gshare Indexing in Perceptron Branch Prediction,2005,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845255475&doi=10.1145%2f1089008.1089011&partnerID=40&md5=0e1176a3a58831774c56aec8a69cf07b,"We introduce the hashed perceptron predictor, which merges the concepts behind the gshare, path-based and perceptron branch predictors. This predictor can achieve superior accuracy to a path-based and a global perceptron predictor, previously the most accurate dynamic branch predictors known in the literature. We also show how such a predictor can be ahead pipelined to yield one cycle effective latency. On the SPECint2000 set of benchmarks, the hashed perceptron predictor improves accuracy by up to 15.6% over a MAC-RHSP and 27.2% over a path-based neural predictor. © 2005, ACM. All rights reserved.",Branch prediction; Neural networks; Performance; Two-level predictors,
Removing Communications in Clustered Microarchitectures through Instruction Replication,2004,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869345560&doi=10.1145%2f1011528.1011529&partnerID=40&md5=8204f9c365d4379459413060f621f325,"The need to communicate values between clusters can result in a significant performance loss for clustered microarchitectures. In this work, we describe an optimization technique that removes communications by selectively replicating an appropriate set of instructions. Instruction replication is done carefully because it might degrade performance due to the increased contention it can place on processor resources. The proposed scheme is built on top of a previously proposed state-of-the-art modulo-scheduling algorithm. Though this algorithm has been proved to be very effective at reducing communications, results show that the number of communications can be further decreased by around one-third through replication, which results in a significant speedup. IPC is increased by 25% on average for a four-cluster microarchitecture and by as much as 70% for selected programs. We also show that replicating appropriate sets of instructions is more effective than doubling the intercluster connection network bandwidth. © 2004, ACM. All rights reserved.",Algorithms; Experimentation; Ilp; Instruction replication; Modulo-scheduling; Performance Clustered microarchitectures; Statically scheduled processors,
Implementing Branch-Predictor Decay Using Quasi-Static Memory Cells,2004,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-47349094363&doi=10.1145%2f1011528.1011531&partnerID=40&md5=fd19633b2e3aba6a93abf406211d083b,"With semiconductor technology advancing toward deep submicron, leakage energy is of increasing concern, especially for large on-chip array structures such as caches and branch predictors. Recent work has suggested that larger, aggressive branch predictors can and should be used in order to improve microprocessor performance. A further consideration is that more aggressive branch predictors, especially multiported predictors for multiple branch prediction, may be thermal hot spots, thus further increasing leakage. Moreover, as the branch predictor holds state that is transient and predictive, elements can be discarded without adverse effect. For these reasons, it is natural to consider applying decay techniques—already shown to reduce leakage energy for caches—to branch-prediction structures. Due to the structural difference between caches and branch predictors, applying decay techniques to branch predictors is not straightforward. This paper explores the strategies for exploiting spatial and temporal locality to make decay effective for bimodal, gshare, and hybrid predictors, as well as the branch target buffer (BTB). Furthermore, the predictive behavior of branch predictors steers them towards decay based not on state-preserving, static storage cells, but rather quasi-static, dynamic storage cells. This paper will examine the results of implementing. © 2004, ACM. All rights reserved.",Design; Energy aware computing; Performance,
Tolerating Memory Latency through Push Prefetching for Pointer-Intensive Applications,2004,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84999372568&doi=10.1145%2f1044823.1044827&partnerID=40&md5=d3ebfb459f2cd0148865c482842474a6,"Prefetching is often used to overlap memory latency with computation for array-based applications. However, prefetching for pointer-intensive applications remains a challenge because of the irregular memory access pattern and pointer-chasing problem. In this paper, we proposed a cooperative hardware/software prefetching framework, the push architecture, which is designed specifically for linked data structures. The push architecture exploits program structure for future address generation instead of relying on past address history. It identifies the load instructions that traverse a LDS and uses a prefetch engine to execute them ahead of the CPU execution. This allows the prefetch engine to successfully generate future addresses. To overcome the serial nature of LDS address generation, the push architecture employs a novel data movement model. It attaches the prefetch engine to each level of the memory hierarchy and pushes, rather than pulls, data to the CPU. This push model decouples the pointer dereference from the transfer of the current node up to the processor. Thus a series of pointer dereferences becomes a pipelined process rather than a serial process. Simulation results show that the push architecture can reduce up to 100% of memory stall time on a suite of pointer-intensive applications, reducing overall execution time by an average 15%. © 2004, ACM. All rights reserved.",Design; Experimentation; Linked data structures; Memory hierarchy; Performance; Pointer-chasing; Prefetch,
An Analysis of a Resource Efficient Checkpoint Architecture,2004,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904206740&doi=10.1145%2f1044823.1044826&partnerID=40&md5=71334c5fa4d5b5a3b4a7247d3c3fab37,"Large instruction window processors achieve high performance by exposing large amounts of instruction level parallelism. However, accessing large hardware structures typically required to buffer and process such instruction window sizes significantly degrade the cycle time. This paper proposes a novel checkpoint processing and recovery (CPR) microarchitecture, and shows how to implement a large instruction window processor without requiring large structures thus permitting a high clock frequency. We focus on four critical aspects of a microarchitecture: (1) scheduling instructions, (2) recovering from branch mispredicts, (3) buffering a large number of stores and forwarding data from stores to any dependent load, and (4) reclaiming physical registers. While scheduling window size is important, we show the performance of large instruction windows to be more sensitive to the other three design issues. Our CPR proposal incorporates novel microarchitectural schemes for addressing these design issues—a selective checkpoint mechanism for recovering from mispredicts, a hierarchical store queue organization for fast store-load forwarding, and an effective algorithm for aggressive physical register reclamation. Our proposals allow a processor to realize performance gains due to instruction windows of thousands of instructions without requiring large cycle-critical hardware structures. © 2004, ACM. All rights reserved.",Checkpoint architecture; Computer architecture; Computer systems; High-performance computing; Scalable architecture,
Introduction,2004,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024248245&doi=10.1145%2f980152.980153&partnerID=40&md5=633ecc65cad367d9fcad45c55aa69cb4,[No abstract available],,
Toward Kilo-Instruction Processors,2004,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988438049&doi=10.1145%2f1044823.1044825&partnerID=40&md5=d706cf3cd5e38c1c0e3d39c94eeb00f9,"The continuously increasing gap between processor and memory speeds is a serious limitation to the performance achievable by future microprocessors. Currently, processors tolerate long-latency memory operations largely by maintaining a high number of in-flight instructions. In the future, this may require supporting many hundreds, or even thousands, of in-flight instructions. Unfortunately, the traditional approach of scaling up critical processor structures to provide such support is impractical at these levels, due to area, power, and cycle time constraints. In this paper we show that, in order to overcome this resource-scalability problem, the way in which critical processor resources are managed must be changed. Instead of simply upsizing the processor structures, we propose a smarter use of the available resources, supported by a selective checkpointing mechanism. This mechanism allows instructions to commit out of order, and makes a reorder buffer unnecessary. We present a set of techniques such as multilevel instruction queues, late allocation and early release of registers, and early release of load/store queue entries. All together, these techniques constitute what we call a kilo-instruction processor, an architecture that can support thousands of in-flight instructions, and thus may achieve high performance even in the presence of large memory access latencies. © 2004, ACM. All rights reserved.",Design; instruction-level parallelism; Kilo-instruction processors; Memory wall; Multicheckpointing; Performance,
Intraprogram Dynamic Voltage Scaling: Bounding Opportunities with Analytic Modeling,2004,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982859985&doi=10.1145%2f1022969.1022973&partnerID=40&md5=37ccbbed61e34547c7c1c2a037a4787f,"Dynamic voltage scaling (DVS) has become an important dynamic power-management technique to save energy. DVS tunes the power-performance tradeoff to the needs of the application. The goal is to minimize energy consumption while meeting performance needs. Since CPU power consumption is strongly dependent on the supply voltage, DVS exploits the ability to control the power consumption by varying a processor's supply voltage and clock frequency. However, because of the energy and time overhead associated with switching DVS modes, DVS control has been used mainly at the interprogram level. In this paper, we explore the opportunities and limits of intraprogram DVS scheduling. An analytical model is derived to predict the maximum energy savings that can be obtained using intraprogram DVS given a few known program and processor parameters. This model gives insights into scenarios where energy consumption benefits from intraprogram DVS and those where there is no benefit. The model helps us extrapolate the benefits of intraprogram DVS into the future as processor parameters change. We then examine how much of these predicted benefits can actually be achieved through compile-time optimal settings of DVS modes. We extend an existing mixed-integer linear program formulation for this scheduling problem by accurately accounting for DVS energy switching overhead, by providing finer-grained control on settings and by considering multiple data categories in the optimization. Overall, this research provides a comprehensive view of intraprogram compile-time DVS management, providing both practical techniques for its immediate deployment as well theoretical bounds for use into the future. © 2004, ACM. All rights reserved.",Analytical model; Compiler; Design; Dynamic voltage scaling; Experimentation; Low power; Mixed-integer linear programming,
Interaction Cost and Shotgun Profiling,2004,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991726534&doi=10.1145%2f1022969.1022971&partnerID=40&md5=24fed4a796f61acc955bf1bc960e01c2,"We observe that the challenges software optimizers and microarchitects face every day boil down to a single problem: bottleneck analysis. A bottleneck is any event or resource that contributes to execution time, such as a critical cache miss or window stall. Tasks such as tuning processors for energy efficiency and finding the right loads to prefetch all require measuring the performance costs of bottlenecks. In the past, simple event counts were enough to find the important bottlenecks. Today, the parallelism of modern processors makes such analysis much more difficult, rendering traditional performance counters less useful. If two microarchitectural events (such as a fetch stall and a cache miss) occur in the same cycle, which event should we blame for the cycle? What cost should we assign to each event? In this paper, we introduce a new model for understanding event costs to facilitate processor design and optimization. First, we observe that all instructions, hardware structures, and events in a machine can interact in only one of twoways (in parallel or serially).We quantify these interactions by defining interaction cost, which can be zero (independent, no interaction), positive (parallel), or negative (serial). Second, we illustrate the value of using interaction costs in processor design and optimization. In a processor with a long pipeline, we show how to mitigate the negative performance effect of long latency “critical” loops, such as the level-one cache access and issue-wakeup, by optimizing seemingly unrelated resources that interact with them. Finally, we propose shotgun profiling, a class of hardware profiling infrastructures that are parallelism-aware, in contrast to traditional event counters. Our recommended design requires only modest extensions to current hardware counters, while enabling the construction of full-featured dependence graphs of the microexecution. With these dependence graphs, many types of analyses can be performed, including identifying critical instructions, finding slack, as well as computing costs and interaction costs. © 2004, ACM. All rights reserved.",Critical path; Design Performance analysis; Measurement; Modeling; Performance; Profiling,
"TRIPS: A Polymorphous Architecture for Exploiting ILP, TLP, and DLP",2004,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905483003&doi=10.1145%2f980152.980156&partnerID=40&md5=01561dcb17944f7056c03956ca22869f,"This paper describes the polymorphous TRIPS architecture that can be configured for different granularities and types of parallelism. The TRIPS architecture is the first in a class of post-RISC, dataflow-like instruction sets called explicit data-graph execution (EDGE). This EDGE ISA is coupled with hardware mechanisms that enable the processing cores and the on-chip memory system to be configured and combined in different modes for instruction, data, or thread-level parallelism. To adapt to small and large-grain concurrency, the TRIPS architecture prototype contains two outof-order, 16-wide-issue grid processor cores, which can be partitioned when easily extractable finegrained parallelism exists. This approach to polymorphism provides better performance across a wide range of application types than an approach in whichmany small processors are aggregated to run workloads with irregular parallelism. Our results show that high performance can be obtained in each of the three modes—ILP, TLP, and DLP—demonstrating the viability of the polymorphous coarse-grained approach for future microprocessors. © 2004, ACM. All rights reserved.",Computer Architecture; Computer Systems; Configurable Computing; High-Performance Computing; ScAlable,
A Compiler Framework for Speculative Optimizations,2004,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78549259289&doi=10.1145%2f1022969.1022970&partnerID=40&md5=6ba9f9d367a9dd32a75a751261927053,"Speculative execution, such as control speculation or data speculation, is an effective way to improve program performance. Using edge/path profile information or simple heuristic rules, existing compiler frameworks can adequately incorporate and exploit control speculation. However, very little has been done so far to allow existing compiler frameworks to incorporate and exploit data speculation effectively in various program transformations beyond instruction scheduling. This paper proposes a speculative static single assignment form to incorporate information from alias profiling and/or heuristic rules for data speculation, thus allowing existing frameworks to be extended to support both control and data speculation. Such a general framework is very useful for EPIC architectures that provide run-time checking (such as advanced load address table) on data speculation to guarantee the correctness of program execution. We use SSAPRE as one example to illustrate how to incorporate data speculation in partial redundancy elimination, register promotion, and strength reduction. Our extended framework allows both control and data speculations to be performed on top of SSAPRE and, thus, enables more aggressive speculative optimizations. The proposed framework has been implemented on Intel's Open Research Compiler. We present experimental data on some SPEC2000 benchmark programs to demonstrate the usefulness of this framework. © 2004, ACM. All rights reserved.",Algorithms; Data speculation; Design; Experimentation; Partial redundancy elimination; Performance; Register promotion; Speculative SSA form; Speculative weak update,
Datapath and Control for Quantum Wires,2004,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994788961&doi=10.1145%2f980152.980155&partnerID=40&md5=338640515fcd82329dfb9011b7129a3f,"As quantum computing moves closer to reality the need for basic architectural studies becomes more pressing. Quantum wires, which transport quantum data, will be a fundamental component in all anticipated silicon quantum architectures. Since they cannot consist of a stream of electrons, as in the classical case, quantum wires must fundamentally be designed differently. In this paper, we present two quantum wire designs: a swap wire, based on swapping of adjacent qubits, and a teleportation wire, based on the quantum teleportation primitive. We characterize the latency and bandwidth of these two alternatives in a device-independent way. Furthermore, unlike classical wires, quantum wires need control signals in order to operate. We explore the complexity of the control mechanisms and the fundamental tension between the scale of quantum effects and the scale of the classical logic needed to control them. This “pitch-matching” problem imposes constraints on minimum wire lengths and wire intersections, leading us to use a SIMD approach for the control mechanisms. We ultimately show that qubit decoherence imposes a basic limit on the maximum communication distance of the swapping wire, while relatively large overhead imposes a basic limit on the minimum communication distance of the teleportation wire. © 2004, ACM. All rights reserved.",Architecture; Control; Design; Layout; REliability,
Profile-Based Adaptation for Cache Decay,2004,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962348222&doi=10.1145%2f1022969.1022972&partnerID=40&md5=c187aca0f9fa7c605cd22bc8a1b8f4d8,"“Cache decay” is a set of leakage-reduction mechanisms that put cache lines that have not been accessed for a specific duration into a low-leakage standby mode. This duration is called the decay interval, and its optimal value varies across applications. This paper describes an adaptation technique that analytically finds the optimal decay interval through profiling, and shows that the most important variables required for finding the optimal decay interval can be estimated with a reasonable degree of accuracy using profiling. This work explicitly trades off the leakage power saved in putting both the “live” and “dead” lines into standby mode, against its performance and energy costs. It achieves energy savings close to what can be obtained with an omniscient choice of per-benchmark optimal decay interval. © 2004, ACM. All rights reserved.",Adaptation; Algorithms; Cache decay; Design; Interval; Leakage powee; Performance,
Reducing Instruction Cache Energy Consumption Using a Compiler-Based Strategy,2004,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987658048&doi=10.1145%2f980152.980154&partnerID=40&md5=95d64d70a4bf08d9cc00630c3d79897d,"Excessive power consumption is widely considered as a major impediment to designing future microprocessors. With the continued scaling down of threshold voltages, the power consumed due to leaky memory cells in on-chip caches will constitute a significant portion of the processor's power budget. This work focuses on reducing the leakage energy consumed in the instruction cache using a compiler-directed approach. We present and analyze two compiler-based strategies termed as conservative and optimistic. The conservative approach does not put a cache line into a low leakage mode until it is certain that the current instruction in it is dead. On the other hand, the optimistic approach places a cache line in low leakage mode if it detects that the next access to the instruction will occur only after a long gap.We evaluate different optimization alternatives by combining the compiler strategies with state-preserving and state-destroying leakage control mechanisms.We also evaluate the sensitivity of these optimizations to different high-level compiler transformations, energy parameters, and soft errors. © 2004, ACM. All rights reserved.",Cache design; Compiler optimizations; Design; Leakage power; Performance; Reliability,
Temperature-Aware Microarchitecture: Modeling and Implementation,2004,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009352442&doi=10.1145%2f980152.980157&partnerID=40&md5=b524c8d4108ecf57bffb2318da34605a,"With cooling costs rising exponentially, designing cooling solutions for worst-case power dissipation is prohibitively expensive. Chips that can autonomously modify their execution and powerdissipation characteristics permit the use of lower-cost cooling solutions while still guaranteeing safe temperature regulation. Evaluating techniques for this dynamic thermal management (DTM), however, requires a thermal model that is practical for architectural studies. This paper describes HotSpot, an accurate yet fast and practical model based on an equivalent circuit of thermal resistances and capacitances that correspond to microarchitecture blocks and essential aspects of the thermal package. Validationwas performed using finite-element simulation. The paper also introduces several effective methods for DTM: “temperature-tracking” frequency scaling, “migrating computation” to spare hardware units, and a “hybrid” policy that combines fetch gating with dynamic voltage scaling. The latter two achieve their performance advantage by exploiting instruction-level parallelism, showing the importance of microarchitecture research in helping control the growth of cooling costs. Modeling temperature at the microarchitecture level also shows that power metrics are poor predictors of temperature, that sensor imprecision has a substantial impact on the performance of DTM, and that the inclusion of lateral resistances for thermal diffusion is important for accuracy. © 2004, ACM. All rights reserved.",Design; DyNamic compact thermal models; Dynamic thermal management; Dynamic voltage scaling; Feedback control; Fetch gating; Measurement; Performance,
The Optimum Pipeline Depth Considering Both Power and Performance,2004,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247384969&doi=10.1145%2f1044823.1044824&partnerID=40&md5=f585f3948cd45cfdd9165245ed660238,"The impact of pipeline length on both the power and performance of a microprocessor is explored both by theory and by simulation. A theory is presented for a range of power/performance metrics, BIPSm/W. The theory shows that the more important power is to the metric, the shorter the optimum pipeline length that results. For typical parameters neither BIPS/W nor BIPS2/W yield an optimum, i.e., a non-pipelined design is optimal. For BIPS3/W the optimum, averaged over all 55 workloads studied, occurs at a 22.5 FO4 design point, a 7 stage pipeline, but this value is highly dependent on the assumed growth in latch count with pipeline depth. As dynamic power grows, the optimal design point shifts to shorter pipelines. Clock gating pushes the optimum to deeper pipelines. Surprisingly, as leakage power grows, the optimum is also found to shift to deeper pipelines. The optimum pipeline depth varies for different classes of workloads: SPEC95 and SPEC2000 integer applications, traditional (legacy) database and on-line transaction processing applications, modern (e. g. web) applications, and floating point applications. © 2004, ACM. All rights reserved.",Design; Performance; Pipeline Depth; Power and Performance; Simulation; Theory; Workload Specificity,
A Low-Complexity Fetch Architecture for High-Performance Superscalar Processors,2004,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646373633&doi=10.1145%2f1011528.1011532&partnerID=40&md5=25a788f9b260ddfb67395503297933c7,"Fetch engine performance is a key topic in superscalar processors, since it limits the instructionlevel parallelism that can be exploited by the execution core. In the search of high performance, the fetch engine has evolved toward more efficient designs, but its complexity has also increased. In this paper, we present the stream fetch engine, a novel architecture based on the execution of long streams of sequential instructions, taking maximum advantage of code layout optimizations. We describe our design in detail, showing that it achieves high fetch performance, while requiring less complexity than other state-of-the-art fetch architectures. © 2004, ACM. All rights reserved.",Branch prediction; Design; Fetch architecture; High performance; Instruction stream; Low complexity; Performance,
A Low-Power In-Order/Out-Of-Order Issue Queue,2004,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-17644424217&doi=10.1145%2f1011528.1011530&partnerID=40&md5=c85c63a1e6ab4f12ffe75a543843a7fc,"To better address power concerns, a good design strategy should be flexible enough to dynamically reconfigure available resources according to the application's needs such that extra power is dissipated only when it is really needed. In this work, we focus on power-aware solutions for the issue queue (IQ) in an out-of-order superscalar processor. We propose two schemes that partition the IQ into FIFOs such that only the instructions at the head of each FIFO may request to issue. We then monitor the processor and dynamically vary the number and/or size of FIFOs in accordance with utilization. Experimenting with two different distributions in power dissipation, we show up to 69% reduction in power dissipation in the wakeup and arbitration loop, while constraining performance degradation to be no more than 5%. © 2004, ACM. All rights reserved.",Design; Experimentation; High-performance; Instruction issue logic; Low power; Performance,
XEngine: Optimal Tensor Rematerialization for Neural Networks in Heterogeneous Environments,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149030529&doi=10.1145%2f3568956&partnerID=40&md5=57ffbec92ea0de6b3d0e361229115ac0,"Memory efficiency is crucial in training deep learning networks on resource-restricted devices. During backpropagation, forward tensors are used to calculate gradients. Despite the option of keeping those dependencies in memory until they are reused in backpropagation, some forward tensors can be discarded and recomputed later from saved tensors, so-called checkpoints. This allows, in particular, for resource-constrained heterogeneous environments to make use of all available compute devices. Unfortunately, the definition of these checkpoints is a non-Trivial problem and poses a challenge to the programmer-improper or excessive recomputations negate the benefit of checkpointing. In this article, we present XEngine, an approach that schedules network operators to heterogeneous devices in low memory environments by determining checkpoints and recomputations of tensors. Our approach selects suitable resources per timestep and operator and optimizes the end-To-end time for neural networks taking the memory limitation of each device into account. For this, we formulate a mixed-integer quadratic program (MIQP) to schedule operators of deep learning networks on heterogeneous systems. We compare our MIQP solver XEngine against Checkmate [12], a mixed-integer linear programming (MILP) approach that solves recomputation on a single device. Our solver finds solutions that are up to 22.5% faster than the fastest Checkmate schedule in which the network is computed exclusively on a single device. We also find valid schedules for networks making use of both central processing units and graphics processing units if memory limitations do not allow scheduling exclusively to the graphics processing unit. © 2022 Copyright held by the owner/author(s).",heterogeneous computing; integer linear programming; memory management; neural networks; Rematerialization,Backpropagation; Computer graphics; Computer graphics equipment; Deep learning; Graphics processing unit; Integer programming; Neural networks; Program processors; Quadratic programming; Heterogeneous computing; Heterogeneous environments; Integer Linear Programming; Learning network; Memory efficiency; Memory-management; Mixed integer quadratic program; Neural-networks; Recomputation; Rematerialization; Tensors
Puppeteer: A Random Forest Based Manager for Hardware Prefetchers Across the Memory Hierarchy,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149036196&doi=10.1145%2f3570304&partnerID=40&md5=9922f6add06aa4d4a989b75950a28684,"Over the years, processor throughput has steadily increased. However, the memory throughput has not increased at the same rate, which has led to the memory wall problem in turn increasing the gap between effective and theoretical peak processor performance. To cope with this, there has been an abundance of work in the area of data/instruction prefetcher designs. Broadly, prefetchers predict future data/instruction address accesses and proactively fetch data/instructions in the memory hierarchy with the goal of lowering data/instruction access latency. To this end, one or more prefetchers are deployed at each level of the memory hierarchy, but typically, each prefetcher gets designed in isolation without comprehensively accounting for other prefetchers in the system. As a result, individual prefetchers do not always complement each other, and that leads to lower average performance gains and/or many negative outliers. In this work, we propose Puppeteer, which is a hardware prefetcher manager that uses a suite of random forest regressors to determine at runtime which prefetcher should be ON at each level in the memory hierarchy, such that the prefetchers complement each other and we reduce the data/instruction access latency. Compared to a design with no prefetchers, using Puppeteer we improve IPC by 46.0% in 1 one-core, 25.8% in four-core, and 11.9% in eight-core processors on average across traces generated from SPEC2017, SPEC2006, and Cloud suites with ∼11-KB overhead. Moreover, we also reduce the number of negative outliers by more than 89%, and the performance loss of the worst-case negative outlier from 25% to only 5% compared to the state of the art. © 2022 Association for Computing Machinery.",machine learning; Prefetching; runtime management,Memory architecture; Statistics; Access latency; Address access; Lower average; Machine-learning; Memory hierarchy; Memory wall; Prefetching; Processor performance; Random forests; Runtime management; Machine learning
Occam: Optimal Data Reuse for Convolutional Neural Networks,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149183476&doi=10.1145%2f3566052&partnerID=40&md5=c603c148f87124f6ea457564ddfcc461,"Convolutional neural networks (CNNs) are emerging as powerful tools for image processing in important commercial applications. We focus on the important problem of improving the latency of image recognition. While CNNs are highly amenable to prefetching and multithreading to avoid memory latency issues, CNNs' large data - each layer's input, filters, and output - poses a memory bandwidth problem. While previous work captures only some of the enormous data reuse, full reuse implies that the initial input image and filters are read once from off-chip and the final output is written once off-chip without spilling the intermediate layers' data to off-chip. We propose Occam to capture full reuse via four contributions. First, we identify the necessary conditions for full reuse. Second, we identify the dependence closure as the sufficient condition to capture full reuse using the least on-chip memory. Third, because the dependence closure is often too large to fit in on-chip memory, we propose a dynamic programming algorithm that optimally partitions a given CNN to guarantee the least off-chip traffic at the partition boundaries for a given on-chip capacity. While tiling is well-known, our contribution determines the optimal cross-layer tiles. Occam's partitions reside on different chips, forming a pipeline so that a partition's filters and dependence closure remain on-chip as different images pass through (i.e., each partition incurs off-chip traffic only for its inputs and outputs). Finally, because the optimal partitions may result in an unbalanced pipeline, we propose staggered asynchronous pipelines (STAPs) that replicate bottleneck stages to improve throughput by staggering mini-batches across replicas. Importantly, STAPs achieve balanced pipelines without changing Occam's optimal partitioning. Our simulations show that, on average, Occam cuts off-chip transfers by 21× and achieves 2.04× and 1.21× better performance, and 33% better energy than the base case, respectively. Using a field-programmable gate array (FPGA) implementation, Occam performs 6.1× and 1.5× better, on average, than the base case and Layer Fusion, respectively.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",convolutional neural networks; Data reuse; FPGAs; GPUs,Convolution; Convolutional neural networks; Dynamic programming; Image enhancement; Image recognition; Multitasking; Pipeline processing systems; Pipelines; Program processors; Asynchronous pipeline; Condition; Convolutional neural network; Data reuse; Images processing; Input filter; Off-chip; On-chip-memory; Optimal data; Reuse; Field programmable gate arrays (FPGA)
Polyhedral Specification and Code Generation of Sparse Tensor Contraction with Co-iteration,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148998004&doi=10.1145%2f3566054&partnerID=40&md5=47baf192f4db59e97eed49b0330a372d,"This article presents a code generator for sparse tensor contraction computations. It leverages a mathematical representation of loop nest computations in the sparse polyhedral framework (SPF), which extends the polyhedral model to support non-Affine computations, such as those that arise in sparse tensors. SPF is extended to perform layout specification, optimization, and code generation of sparse tensor code: (1) We develop a polyhedral layout specification that decouples iteration spaces for layout and computation; and (2) we develop efficient co-iteration of sparse tensors by combining polyhedra scanning over the layout of one sparse tensor with the synthesis of code to find corresponding elements in other tensors through an SMT solver.We compare the generated code with that produced by a state-of-The-Art tensor compiler, TACO. We achieve on average 1.63× faster parallel performance than TACO on sparse-sparse co-iteration and describe how to improve that to 2.72× average speedup by switching the find algorithms. We also demonstrate that decoupling iteration spaces of layout and computation enables additional layout and computation combinations to be supported. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",code synthesis; Data layout; index array properties; polyhedral compilation; sparse tensor contraction; uninterpreted functions,Codes (symbols); Iterative methods; Program compilers; Specifications; Code synthesis; Data layouts; Index array property; Index arrays; Polyhedral compilation; Property; Sparse tensor contraction; Sparse tensors; Tensor contraction; Uninterpreted Functions; Tensors
SSD-SGD: Communication Sparsification for Distributed Deep Learning Training,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149013664&doi=10.1145%2f3563038&partnerID=40&md5=ad75d0a0c37c087eb24cb00cce2fd1f3,"Intensive communication and synchronization cost for gradients and parameters is the well-known bottleneck of distributed deep learning training. Based on the observations that Synchronous SGD (SSGD) obtains good convergence accuracy while asynchronous SGD (ASGD) delivers a faster raw training speed, we propose Several Steps Delay SGD (SSD-SGD) to combine their merits, aiming at tackling the communication bottleneck via communication sparsification. SSD-SGD explores both global synchronous updates in the parameter servers and asynchronous local updates in the workers in each periodic iteration. The periodic and flexible synchronization makes SSD-SGD achieve good convergence accuracy and fast training speed. To the best of our knowledge, we strike the new balance between synchronization quality and communication sparsification, and improve the tradeoff between accuracy and training speed. Specifically, the core components of SSD-SGD include proper warm-up stage, steps delay stage, and the novel algorithm of global gradient for local update (GLU). GLU is critical for local update operations by using global gradient information to effectively compensate for the delayed local weights. Furthermore, we implement SSD-SGD on MXNet framework and comprehensively evaluate its performance with CIFAR-10 and ImageNet datasets. Experimental results show that SSD-SGD can accelerate distributed training speed under different experimental configurations, by up to 110% (or 2.1× of the original speed), while achieving good convergence accuracy. © 2022 Association for Computing Machinery.",compensation; convergence speed; distributed performance; Steps delay,Deep learning; Iterative methods; Communication and synchronizations; Communication cost; Compensation; Convergence speed; Distributed performance; Global gradients; Sparsification; Step delay; Synchronization cost; Training speed; Synchronization
RegCPython: A Register-based Python Interpreter for Better Performance,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149020193&doi=10.1145%2f3568973&partnerID=40&md5=5514a14f2d62def5134821e68f97cd90,"Interpreters are widely used in the implementation of many programming languages, such as Python, Perl, and Java. Even though various JIT compilers emerge in an endless stream, interpretation efficiency still plays a critical role in program performance. Does a stack-based interpreter or a register-based interpreter perform better? The pros and cons of the pair of architectures have long been discussed. The stack architecture is attractive for its concise model and compact bytecode, but our study finds that the register-based interpreter can also be implemented easily and that its bytecode size only grows by a small margin. Moreover, the latter turns out to be appreciably faster. Specifically, we implemented an open source Python interpreter named RegCPython based on CPython v3.10.1. The former is register based, while the latter is stack based. Without changes in syntax, Application Programming Interface, and Application Binary Interface, RegCPython is excellently compatible with CPython, as it does not break existing syntax or interfaces. It achieves a speedup of 1.287 on the most favorable benchmark and 0.977 even on the most unfavorable benchmark. For all Python-intensive benchmarks, the average speedup reaches 1.120 on x86 and 1.130 on ARM. Our evaluation work, which also serves as an empirical study, provides a detailed performance survey of both interpreters on modern hardware. It points out that the register-based interpreters are more efficient mainly due to the elimination of machine instructions needed, while changes in branch mispredictions and cache misses have a limited impact on performance. Additionally, it confirms that the register-based implementation is also satisfactory in terms of memory footprint, compilation cost, and implementation complexity. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",CPython; interpreter; performance; Python; register architecture; stack architecture,Application programming interfaces (API); Computer hardware description languages; High level languages; Open source software; Program compilers; Syntactics; Bytecodes; Concise model; Cpython; Interpreter; JIT compiler; Open-source; Performance; Program performance; Register architecture; Stack architecture; Python
Solving Sparse Assignment Problems on FPGAs,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146421683&doi=10.1145%2f3546072&partnerID=40&md5=1f3db18e6d1e6c0a5335dccc85a45a9b,"The assignment problem is a fundamental optimization problem and a crucial part of many systems. For example, in multiple object tracking, the assignment problem is used to associate object detections with hypothetical target tracks and solving the assignment problem is one of the most compute-intensive tasks. To enable low-latency real-time implementations, efficient solutions to the assignment problem is required. In this work, we present Sparse and Speculative (SaS) Auction, a novel implementation of the popular Auction algorithm for FPGAs. Two novel optimizations are proposed. First, the pipeline width and depth are reduced by exploiting sparsity in the input problems. Second, dependency speculation is employed to enable a fully pipelined design and increase the throughput. Speedups as high as 50 × are achieved relative to the state-of-the-art implementation for some input distributions. We evaluate the implementation both on randomly generated datasets and realistic datasets from multiple object tracking.  © 2022 Copyright held by the owner/author(s).",Assignment problem; Auction method; FPGA; object tracking,Combinatorial optimization; Object detection; Pipelines; Real time control; Tracking (position); Assignment problems; Auction algorithms; Auction method; Compute-intensive tasks; Low latency; Multiple object tracking; Object Tracking; Objects detection; Optimization problems; Real-time implementations; Field programmable gate arrays (FPGA)
Architecting Optically Controlled Phase Change Memory,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146435463&doi=10.1145%2f3533252&partnerID=40&md5=f4ce4475ccd7b84cd5575e3fce5ac75e,"Phase Change Memory (PCM) is an attractive candidate for main memory, as it offers non-volatility and zero leakage power while providing higher cell densities, longer data retention time, and higher capacity scaling compared to DRAM. In PCM, data is stored in the crystalline or amorphous state of the phase change material. The typical electrically controlled PCM (EPCM), however, suffers from longer write latency and higher write energy compared to DRAM and limited multi-level cell (MLC) capacities. These challenges limit the performance of data-intensive applications running on computing systems with EPCMs.Recently, researchers demonstrated optically controlled PCM (OPCM) cells with support for 5 bits/cell in contrast to 2 bits/cell in EPCM. These OPCM cells can be accessed directly with optical signals that are multiplexed in high-bandwidth-density silicon-photonic links. The higher MLC capacity in OPCM and the direct cell access using optical signals enable an increased read/write throughput and lower energy per access than EPCM. However, due to the direct cell access using optical signals, OPCM systems cannot be designed using conventional memory architecture. We need a complete redesign of the memory architecture that is tailored to the properties of OPCM technology.This article presents the design of a unified network and main memory system called COSMOS that combines OPCM and silicon-photonic links to achieve high memory throughput. COSMOS is composed of a hierarchical multi-banked OPCM array with novel read and write access protocols. COSMOS uses an Electrical-Optical-Electrical (E-O-E) control unit to map standard DRAM read/write commands (sent in electrical domain) from the memory controller on to optical signals that access the OPCM cells. Our evaluation of a 2.5D-integrated system containing a processor and COSMOS demonstrates 2.14 × average speedup across graph and HPC workloads compared to an EPCM system. COSMOS consumes 3.8× lower read energy-per-bit and 5.97× lower write energy-per-bit compared to EPCM. COSMOS is the first non-volatile memory that provides comparable performance and energy consumption as DDR5 in addition to increased bit density, higher area efficiency, and improved scalability. © 2022 Association for Computing Machinery.",2.5D computing system; non-volatile memory; Phase Change Memory; silicon-photonics,Cells; Cytology; Dynamic random access storage; Energy utilization; Green computing; Memory architecture; Network architecture; Nonvolatile storage; Phase change materials; Phase change memory; Photonic devices; 2.5d computing system; Cells capacity; Computing system; Main-memory; Multilevels; Non-volatile memory; Optical signals; Optically controlled; Phase-change memory; Silicon photonics; Silicon photonics
FlexHM: A Practical System for Heterogeneous Memory with Flexible and Efficient Performance Optimizations,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144837377&doi=10.1145%2f3565885&partnerID=40&md5=8af712da8a21fc2704bfea5647bacd45,"With the rapid development of cloud computing, numerous cloud services, containers, and virtual machines have been bringing tremendous demands on high-performance memory resources to modern data centers. Heterogeneous memory, especially the newly released Optane memory, offer appropriate alternatives against DRAM in clouds with the advantages of larger capacity, lower purchase cost, and promising performance. However, cloud services suffer serious implementation inconvenience and performance degradation when using hybrid DRAM and Optane memory. This article proposes FlexHM, a practical system to manage transparent heterogeneous memory resources and flexibly optimize memory access performance for all VMs, containers, and native applications. We present an open-source prototype of FlexHM in Linux with several main contributions. First, FlexHM raises a novel two-level NUMA design to manage DRAM and Optane memory as transparent main memory resources. Second, FlexHM provides flexible and efficient memory management, helping optimize memory access performance or save purchase costs of memory resources for differential cloud services with customized management strategies. Finally, the evaluations show that cloud workloads using 50% Optane slow memory on FlexHM can achieve up to 93% of the performance when using all-DRAM, and FlexHM provides up to 5.8× improvement over the previous heterogeneous memory system solution when workloads use the same ratio of DRAM and Optane memory. © 2022 Association for Computing Machinery.",conservative page migration; heterogeneous memory management; NVM; page hotness; two-level NUMA,Containers; Distributed database systems; Dynamic random access storage; Memory architecture; Web services; Conservative page migration; Heterogeneous memory; Heterogeneous memory management; Memory resources; Memory-management; NVM; Page hotness; Page migration; Performance; Two-level NUMA; Computer operating systems
Lock-Free High-performance Hashing for Persistent Memory via PM-Aware Holistic Optimization,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148957723&doi=10.1145%2f3561651&partnerID=40&md5=e0174900d526febe31cab5bba837b0b4,"Persistent memory (PM) provides large-scale non-volatile memory (NVM) with DRAM-comparable performance. The non-volatility and other unique characteristics of PM architecture bring new opportunities and challenges for the efficient storage system design. For example, some recent crash-consistent and write-friendly hashing schemes are proposed to provide fast queries for PM systems. However, existing PM hashing indexes suffer from the concurrency bottleneck due to the blocking resizing and expensive lock-based concurrency control for queries. Moreover, the lack of PM awareness and systematical design further increases the query latency. To address the concurrency bottleneck of lock contention in PM hashing, we propose clevel hashing, a lock-free concurrent level hashing scheme that provides non-blocking resizing via background threads and lock-free search/insertion/update/deletion using atomic primitives to enable high concurrency for PM hashing. By exploiting the PM characteristics, we present a holistic approach to building clevel hashing for high throughput and low tail latency via the PM-Aware index/allocator co-design. The proposed volatile announcement array with a helping mechanism coordinates lock-free insertions and guarantees a strong consistency model. Our experiments using real-world YCSB workloads on Intel Optane DC PMM show that clevel hashing, respectively, achieves up to 5.7× and 1.6× higher throughput than state-of-The-Art P-CLHT and Dash while guaranteeing low tail latency, e.g., 1.9×-7.2× speedup for the p99 latency with the insert-only workload. © 2022 Association for Computing Machinery.",correctness; hashing; Lock-free index; persistent memory,Dynamic random access storage; Locks (fasteners); Memory architecture; Correctness; Hashing; High-throughput; Holistic optimizations; Large-scales; Lock-free; Lock-free index; Memory aware; Performance; Persistent memory; Concurrency control
BullsEye : Scalable and Accurate Approximation Framework for Cache Miss Calculation,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148999979&doi=10.1145%2f3558003&partnerID=40&md5=37f123d5b6d1111d2d691c1ec0797344,"For Affine Control Programs or Static Control Programs (SCoP), symbolic counting of reuse distances could induce polynomials for each reuse pair. These polynomials along with cache capacity constraints lead to non-Affine (semi-Algebraic) sets; and counting these sets is considered to be a hard problem. The state-of-The-Art methods use various exact enumeration techniques relying on existing cardinality algorithms that can efficiently count affine sets.We propose BullsEye , a novel, scalable, accurate, and problem-size independent approximation framework. It is an analytical cache model for fully associative caches with LRU replacement policy focusing on sampling and linearization of non-Affine stack distance polynomials. First, we propose a simple domain sampling method that can improve the scalability of exact enumeration. Second, we propose linearization techniques relying on Handelman's theorem and Bernstein's representation. To improve the scalability of the Handelman's theorem linearization technique, we propose template (Interval or Octagon) sub-polyhedral approximations.Our methods obtain significant compile-Time improvements with high-Accuracy when compared to HayStack on important polyhedral compilation kernels such as nussinov, cholesky, and adi from PolyBench, and harris, gaussianblur from LLVM-TestSuite. Overall, on PolyBench kernels, our methods show up to 3.31× (geomean) speedup with errors below ≈ 0.08% (geomean) for the octagon sub-polyhedral approximation. © 2022 Association for Computing Machinery.",cache model; performance analysis; Static analysis,Computer software reusability; Geometry; Linearization; Scalability; Static analysis; Bullseye; Cache Miss; Cache modeling; Control program; Exact enumeration; Linearization technique; Performances analysis; Polyhedral approximation; Program control; Static control; Polynomials
Symbolic Analysis for Data Plane Programs Specialization,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148936312&doi=10.1145%2f3557727&partnerID=40&md5=9ffd589829e59fb39d8ae17e00a91950,"Programmable network data planes have extended the capabilities of packet processing in network devices by allowing custom processing pipelines and agnostic packet processing. While a variety of applications can be implemented on current programmable data planes, there are significant constraints due to hardware limitations. One way to meet these constraints is by optimizing data plane programs. Program optimization can be achieved by specializing code that leverages architectural specificity or by compilation passes. In the case of programmable data planes, to respond to the varying requirements of a large set of applications, data plane programs can target different architectures. This leads to difficulties when developers want to reuse the code. One solution to that is to use compiler optimization techniques. We propose performing data plane program specialization to reduce the generated program size. To this end, we propose to specialize in programs written in P4, a Domain Specific Language (DSL) designed for specifying network data planes. The proposed method takes advantage of key aspects of the P4 language to perform a symbolic analysis on a P4 program and then partially evaluate the program to specialize it. The approach we propose is independent of the target architecture. We evaluate the specialization technique by implementing a packet deparser on an FPGA. The results demonstrate that program specialization can reduce the resource usage by a factor of 2 for various packet deparsers. © 2022 Copyright held by the owner/author(s).",Compiler optimization; FPGA; network data plane; P4 language; packet deparsers; program specialization,Application programs; Computer hardware description languages; Network architecture; Packet networks; Pipeline processing systems; Problem oriented languages; Program compilers; Compiler optimizations; Data planes; Data-plane; Network data; Network data plane; P4 language; Packet deparser; Packet processing; Program specialization; Symbolic analysis; Field programmable gate arrays (FPGA)
"TokenSmart: Distributed, Scalable Power Management in the Many-core Era",2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149186532&doi=10.1145%2f3559762&partnerID=40&md5=f6369525caf90b44bcfe4c8efe7428f4,"Centralized power management control systems are hitting a scalability limit. In particular, enforcing a power cap in a many-core system in a performance-friendly manner is quite challenging. Today's on-chip controller reduces the clock speed of compute domains in response to local or global power limit alerts. However, this is opaque to the operating system (OS), which continues to request higher clock frequency based on the workload characteristics acting against the centralized on-chip controller. To address these issues, we introduce TokenSmart, which implements a set of scalable distributed frequency control heuristics within the OS, using a novel token-based mechanism. The number of system-allocated power tokens represents the maximum allowable power consumption; and the OS governor orchestrates a token-passing (or sharing) algorithm between the compute engines. Token allocation count increase (decrease) corresponds to a increase (decrease) of clock frequency. The compute units are connected in a ring-topology allowing minimal meta-data to be passed along with the token value for regulating power budget. We explore different heuristics to assign tokens smartly across the units. This results in efficient power regulation and sustenance of turbo frequencies over a longer duration. Our proposed methodology can be implemented in hardware with multiple on-chip controllers, or in software where each set of cores acts as a compute unit. The methodology is currently implemented within the Linux kernel of a real IBM POWER9 many-core system and experimentally verified on different real world workloads such as Redis, Cassandra, PostgreSQL along with a micro-benchmark such as rt-app. Our experiments indicate the increase in throughput for all the workloads along with the benefit of power savings. For instance, results show a considerable boost of about 4% in throughput of both the PostgreSQL and Redis benchmark with a substantial savings in power consumption (18% and 37%, respectively). If the approach is implemented in hardware, then our experimental analysis speculates the throughput to increase up to 14% in PostgreSQL benchmark.  © 2022 Association for Computing Machinery.",decentralized; Power management; scalable,Budget control; Clocks; Computer operating systems; Controllers; Electric power utilization; Energy policy; Centralised; Clock frequency; Core systems; Decentralised; Management control system; Many-core; On chips; PostgreSQL; Power; Scalable; Power management
Design and Implementation for Nonblocking Execution in GraphBLAS: Tradeoffs and Performance,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148945394&doi=10.1145%2f3561652&partnerID=40&md5=c4f193655c3ab5d7bdfaca70e24eeef8,"GraphBLASis a recent standard that allows the expression of graph algorithms in the language of linear algebra and enables automatic code parallelization and optimization. GraphBLAS operations are memory bound and may benefit from data locality optimizations enabled by nonblocking execution. However, nonblocking execution remains under-evaluated. In this article, we present a novel design and implementation that investigates nonblocking execution in GraphBLAS. Lazy evaluation enables runtime optimizations that improve data locality, and dynamic data dependence analysis identifies operations that may reuse data in cache. The nonblocking execution of an arbitrary number of operations results in dynamic parallelism, and the performance of the nonblocking execution depends on two parameters, which are automatically determined, at run-Time, based on a proposed analytic model. The evaluation confirms the importance of nonblocking execution for various matrices of three algorithms, by showing up to 4.11× speedup over blocking execution as a result of better cache utilization. The proposed analytic model makes the nonblocking execution reach up to 5.13× speedup over the blocking execution. The fully automatic performance is very close to that obtained by using the best manual configuration for both small and large matrices. Finally, the evaluation includes a comparison with other state-of-The-Art frameworks for numerical linear algebra programming that employ parallel execution and similar optimizations to those discussed in this work, and the presented nonblocking execution reaches up to 16.1× speedup over the state of the art. © 2022 Association for Computing Machinery.",analytic performance model; data locality; dynamic data dependence analysis; dynamic parallelism; GraphBLAS; lazy evaluation; loop fusion; loop tiling; nonblocking execution,Analytical models; Analytic performance modelling; Data dependence analysis; Data locality; Dynamic data; Dynamic data dependence analyse; Dynamic parallelism; GraphBLAS; Lazy evaluation; Loop fusion; Loop tiling; Non-blocking; Nonblocking execution; Matrix algebra
PiDRAM: A Holistic End-To-end FPGA-based Framework for Processing-in-DRAM,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138695837&doi=10.1145%2f3563697&partnerID=40&md5=92e80b4d58e4e184829532a6a1884215,"Commodity DRAM-based processing-using-memory (PuM) techniques that are supported by off-The-shelf DRAM chips present an opportunity for alleviating the data movement bottleneck at low cost. However, system integration of these techniques imposes non-Trivial challenges that are yet to be solved. Potential solutions to the integration challenges require appropriate tools to develop any necessary hardware and software components. Unfortunately, current proprietary computing systems, specialized DRAM-Testing platforms, or system simulators do not provide the flexibility and/or the holistic system view that is necessary to properly evaluate and deal with the integration challenges of commodity DRAM-based PuM techniques.We design and develop Processing-in-DRAM (PiDRAM), the first flexible end-To-end framework that enables system integration studies and evaluation of real, commodity DRAM-based PuM techniques. PiDRAM provides software and hardware components to rapidly integrate PuM techniques across the whole system software and hardware stack. We implement PiDRAM on an FPGA-based RISC-V system. To demonstrate the flexibility and ease of use of PiDRAM, we implement and evaluate two state-of-The-Art commodity DRAM-based PuM techniques: (i) in-DRAM copy and initialization (RowClone) and (ii) in-DRAM true random number generation (D-RaNGe). We describe how we solve key integration challenges to make such techniques work and be effective on a real-system prototype, including memory allocation, alignment, and coherence. We observe that end-To-end RowClone speeds up bulk copy and initialization operations by 14.6× and 12.6×, respectively, over conventional CPU copy, even when coherence is supported with inefficient cache flush operations. Over PiDRAM's extensible codebase, integrating both RowClone and D-RaNGe end-To-end on a real RISC-V system prototype takes only 388 lines of Verilog code and 643 lines of C++ code. © 2022 Copyright held by the owner/author(s).",DRAM; FPGA; memory controllers; processing-in-memory; Processing-using-memory; RISC-V,C++ (programming language); Dynamic random access storage; Integration testing; Random number generation; End to end; Memory controller; Processing-in-memory; Processing-using-memory; Random-number generation; RISC-V; Software and hardwares; System integration; System prototype; True randoms; Field programmable gate arrays (FPGA)
Delay-on-Squash: Stopping Microarchitectural Replay Attacks in Their Tracks,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149048135&doi=10.1145%2f3563695&partnerID=40&md5=a86b2e758d7b9ab906c57fe800f471d8,"MicroScope and other similar microarchitectural replay attacks take advantage of the characteristics of speculative execution to trap the execution of the victim application in a loop, enabling the attacker to amplify a side-channel attack by executing it indefinitely. Due to the nature of the replay, it can be used to effectively attack software that are shielded against replay, even under conditions where a side-channel attack would not be possible (e.g., in secure enclaves). At the same time, unlike speculative side-channel attacks, microarchitectural replay attacks can be used to amplify the correct path of execution, rendering many existing speculative side-channel defenses ineffective. In this work, we generalize microarchitectural replay attacks beyond MicroScope and present an efficient defense against them. We make the observation that such attacks rely on repeated squashes of so-called ""replay handles""and that the instructions causing the side-channel must reside in the same reorder buffer window as the handles. We propose Delay-on-Squash, a hardware-only technique for tracking squashed instructions and preventing them from being replayed by speculative replay handles. Our evaluation shows that it is possible to achieve full security against microarchitectural replay attacks with very modest hardware requirements while still maintaining 97% of the insecure baseline performance. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Microarchitecture; replay attacks; security; side-channels,Computer architecture; Base-line performance; Condition; Micro architectures; Re-order buffers; Replay attack; Security; Side-channel; Side-channel attacks; Speculative execution; Side channel attack
As-Is Approximate Computing,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149186712&doi=10.1145%2f3559761&partnerID=40&md5=869942e5c624266921dfb976e9f96a14,"Although approximate computing promises better performance for applications allowing marginal errors, dearth of hardware support and lack of run-Time accuracy guarantees makes it difficult to adopt. We present As-Is, an Anytime Speculative Interruptible System that takes an approximate program and executes it with time-proportional approximations. That is, an approximate version of the program output is generated early and is gradually refined over time, thus providing the run-Time guarantee of eventually reaching 100% accuracy. The novelty of our As-Is architecture is in its ability to conceptually marry approximate computing and speculative computing. We show how existing innovations in speculative architectures can be repurposed for anytime, best-effort approximation, facilitating the design efforts and overheads needed for approximate hardware support. As-Is provides a platform for real-Time constraints and interactive users to interrupt programs early and accept their current approximate results as is. 100% accuracy is always guaranteed if more time can be spared. Our evaluations demonstrate favorable performance-Accuracy tradeoffs for a range of approximate applications.  © 2022 Association for Computing Machinery.",Approximate computing,'current; Approximate computing; Approximate results; Best-effort; Design effort; Hardware supports; IS architecture; Performance; Real time constraints; Runtimes
Practical Software-Based Shadow Stacks on x86-64,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146425937&doi=10.1145%2f3556977&partnerID=40&md5=7548057d5f449a0391470089dec5aad8,"Control-Flow Integrity (CFI) techniques focus often on protecting forward edges and assume that backward edges are protected by shadow stacks. However, software-based shadow stacks that can provide performance, security, and compatibility are still hard to obtain, leaving an important security gap on x86-64. In this article, we introduce a simple, efficient, and effective parallel shadow stack design (based on LLVM), FlashStack, for protecting return addresses in single- and multi-threaded programs running under 64-bit Linux on x86-64, with three distinctive features. First, we introduce a novel dual-prologue approach to enable a protected function to thwart the TOCTTOU attacks, which are constructed by Microsoft's red team and lead to the deprecation of Microsoft's RFG. Second, we design a new mapping mechanism, Segment+Rsp-S, to allow the parallel shadow stack to be accessed efficiently while satisfying the constraints of arch_prctl() and ASLR in 64-bit Linux. Finally, we introduce a lightweight inspection mechanism, SideChannel-K, to harden FlashStack further by detecting entropy-reduction attacks efficiently and protecting the parallel shadow stack effectively with a 10-ms shuffling policy. Our evaluation on SPEC CPU2006, Nginx, and Firefox shows that FlashStack can provide high performance, meaningful security, and reasonable compatibility for server- and client-side programs on x86-64. © 2022 Association for Computing Machinery.",Control-Flow Integrity; runtime re-randomization; Shadow stack,Control-flow integrities; MicroSoft; Performance; Randomisation; Runtime re-randomization; Runtimes; Security gap; Shadow stack; Simple++; Stack designs; Linux
DynamAP: Architectural Support for Dynamic Graph Traversal on the Automata Processor,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146428667&doi=10.1145%2f3556976&partnerID=40&md5=f4c21213ede8a576d82479288894beaa,"Dynamic graph traversals (DGTs) currently are widely used in many important application domains, especially in this big-data era that urgently demands high-performance graph processing and analysis. Unlike static graph traversals, DGTs in real-world application scenarios require not only fast traversal acceleration itself but also, more importantly, a runtime strategy that can effectively accommodate the ever-evolving nature of the graph structure updates followed by a diverse range of graph traversal algorithms. Because of these special features, state-of-the-art designs on conventional compute-centric architectures (e.g., CPU and GPU) struggle to provide sufficient acceleration for DGT processing due to the dominating irregular memory access patterns in graph traversal algorithms and inefficient platform-specific update mechanisms. In this article, we explore the algorithmic features and runtime requirements of real-world DGTs and identify their unique opportunities of acceleration on the recent Micron Automata Processor (AP), an in-situ memory-centric pattern-matching architecture. These features include the natural mapping between traversal algorithms' path exploration pattern to classic non-deterministic finite automata processing, AP's architectural and compilation support for DGTs' evolving traversal operations, and its inherent hardware fitness. However, despite these benefits, enabling highly efficient DGT execution on AP is non-trivial and faces several major challenges. To tackle them, we propose DynamAP, the first AP framework design that enables fast processing for general DGTs. DynamAP is oblivious to periodical traversal algorithm changes and can address the significant overhead caused by frequent graph updates and AP recompilation through our novel hybrid macro designs and associated efficient updating strategies. We evaluate DynamAP against the current DGT designs on a CPU, GPU, and AP with a range of widely adopted DGT algorithms and real-world graphs. For a single update request, our DynamAP achieves an average speedup of 21.3x (up to 39.2x) over the state-of-the-art implementation on host-AP architecture; an average speedup of 9.2x (up to 14.7x) and 1.7x (up to 2.8x) over two highly optimized DGT design frameworks on a 64-GB Intel(R) Xeon CPU and a 32-GB NVIDIA Tesla V100 GPU. DynamAP also maintains high performance and resource utilization for high graph update ratios, and can significantly benefit natural graphs that present a high average vertex degree.  © 2022 Association for Computing Machinery.",Accelerator system design; and reconfigurable systems; CGRAs; FPGAs; near-data computing; systems designed around emerging/partially proven device technologies,Computer hardware; Graphic methods; Graphics processing unit; Integrated circuit design; Memory architecture; Pattern matching; Reconfigurable architectures; Structural design; Systems analysis; Accelerator system; Accelerator system design; And reconfigurable system; CGRA; Device technologies; Dynamic graph; Graph traversals; Near-data computing; Reconfigurable-systems; System designed around emerging/partially proven device technology; Field programmable gate arrays (FPGA)
Reducing Minor Page Fault Overheads through Enhanced Page Walker,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146428214&doi=10.1145%2f3547142&partnerID=40&md5=39ea3b1616778bbe0743eaa3434fc1bc,"Application virtual memory footprints are growing rapidly in all systems from servers down to smartphones. To address this growing demand, system integrators are incorporating ever larger amounts of main memory, warranting rethinking of memory management. In current systems, applications produce page fault exceptions whenever they access virtual memory regions that are not backed by a physical page. As application memory footprints grow, they induce more and more minor page faults. Handling of each minor page fault can take a few thousands of CPU cycles and blocks the application till the OS kernel finds a free physical frame. These page faults can be detrimental to the performance when their frequency of occurrence is high and spread across application runtime. Specifically, lazy allocation-induced minor page faults are increasingly impacting application performance. Our evaluation of several workloads indicates an overhead due to minor page faults as high as 29% of execution time.In this article, we propose to mitigate this problem through a hardware, software co-design approach. Specifically, we first propose to parallelize portions of the kernel page allocation to run ahead of fault time in a separate thread. Then we propose the Minor Fault Offload Engine (MFOE), a per-core hardware accelerator for minor fault handling. MFOE is equipped with a pre-allocated page frame table that it uses to service a page fault. On a page fault, MFOE quickly picks a pre-allocated page frame from this table, makes an entry for it in the TLB, and updates the page table entry to satisfy the page fault. The pre-allocation frame tables are periodically refreshed by a background kernel thread, which also updates the data structures in the kernel to account for the handled page faults. We evaluate this system in the gem5 architectural simulator with a modified Linux kernel running on top of simulated hardware containing the MFOE accelerator. Our results show that MFOE improves the average critical path fault handling latency by 33× and tail critical path latency by 51×. Among the evaluated applications, we observed an improvement of runtime by an average of 6.6%.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Function-as-a-Service (FaaS); Paging; Translate Look-aside Buffer (TLB); virtualization,Buffer storage; Computer operating systems; Hardware-software codesign; Critical Paths; Fault handling; Function-as-a-service; Memory footprint; Paging; Runtimes; Smart phones; Translate look-aside buffer; Virtual memory; Virtualizations; Virtualization
Compiler Support for Sparse Tensor Computations in MLIR,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144308810&doi=10.1145%2f3544559&partnerID=40&md5=1568337f57d87ff460ca5ec48bf56e9f,"Sparse tensors arise in problems in science, engineering, machine learning, and data analytics. Programs that operate on such tensors can exploit sparsity to reduce storage requirements and computational time. Developing and maintaining sparse software by hand, however, is a complex and error-prone task. Therefore, we propose treating sparsity as a property of tensors, not a tedious implementation task, and letting a sparse compiler generate sparse code automatically from a sparsity-agnostic definition of the computation. This article discusses integrating this idea into MLIR. © 2022 Copyright held by the owner/author(s).",Compilers; machine learning; sparse data structures; tensor algebra,Data Analytics; Digital storage; Program compilers; Tensors; Compiler; Computational time; Data analytics; Engineering machines; Machine data; Machine-learning; Sparse data structures; Sparse tensors; Storage requirements; Tensor algebra; Machine learning
COX : Exposing CUDA Warp-level Functions to CPUs,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146425236&doi=10.1145%2f3554736&partnerID=40&md5=d08e1e09f5a2c55d7dfda89a2a7492da,"As CUDA becomes the de facto programming language among data parallel applications such as high-performance computing or machine learning applications, running CUDA on other platforms becomes a compelling option. Although several efforts have attempted to support CUDA on devices other than NVIDIA GPUs, due to extra steps in the translation, the support is always a few years behind CUDA's latest features. In particular, the new CUDA programming model exposes the warp concept in the programming language, which greatly changes the way the CUDA code should be mapped to CPU programs. In this article, hierarchical collapsing that correctly supports CUDA warp-level functions on CPUs is proposed. To verify hierarchical collapsing , we build a framework, COX , that supports executing CUDA source code on the CPU backend. With hierarchical collapsing , 90% of kernels in CUDA SDK samples can be executed on CPUs, much higher than previous works (68%). We also evaluate the performance with benchmarks for real applications and show that hierarchical collapsing can generate CPU programs with comparable or even higher performance than previous projects in general.  © 2022 Copyright held by the owner/author(s).",code migration; compiler transformations; GPU,Application programs; Benchmarking; Codes (symbols); Problem oriented languages; Code migration; Compiler transformations; CUDA Programming; Data-parallel applications; High-performance machines; Machine learning applications; Performance; Performance computing; Programming models; Source codes; Graphics processing unit
Just-In-Time Compilation on ARM - A Closer Look at Call-Site Code Consistency,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146437634&doi=10.1145%2f3546568&partnerID=40&md5=9d686240dbd9277a0b3da74689b5e7d5,"The increase in computational capability of low-power Arm architectures has seen them diversify from their more traditional domain of portable battery powered devices into data center servers, personal computers, and even Supercomputers. Thus, managed languages (Java, Javascript, etc.) that require a managed runtime environment (MRE) need to be ported to the Arm architecture, requiring an understanding of different design tradeoffs. This article studies how the lack of strong hardware support for Self Modifying Code (SMC) in low-power architectures (e.g., absence of cache coherence between instruction cache and data caches), affects Just-In-Time (JIT) compilation and runtime behavior in MREs. Specifically, we focus on the implementation and treatment of call-sites, that must maintain code consistency in the face of concurrent execution and modification to redirect control (patching) by the MRE. The lack of coherence, is compounded with the maximum distance (reach of) a call-site can jump to as the reach is more constrained (smaller distance) in Arm when compared with Intel/AMD. We present four different robust implementations for call-sites and discuss their advantages and disadvantages in the absence of strong hardware support for SMC. Finally, we evaluate each approach using a microbenchmark, further evaluating the best three techniques using three JVM benchmark suites and the open source MaxineVM showcasing performance differences up to 12%. Based on these observations, we propose extending code-cache partitioning strategies for JIT compiled code to encourage more efficient local branching for architectures with limited direct branch ranges.  © 2022 Copyright held by the owner/author(s).",AArch64; JIT compilation; RISC; self modifying code,ARM processors; Benchmarking; Cache memory; Just in time production; Open source software; Open systems; Personal computers; Aarch64; ARM architecture; Battery powered devices; Computational capability; Datacenter; Hardware supports; Just-in-time compilation; Low Power; Runtime environments; Self modifying code; Supercomputers
HAIR: Halving the Area of the Integer Register File with Odd/Even Banking,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146437911&doi=10.1145%2f3544838&partnerID=40&md5=883887447792bb7ee81fed4150b07949,"This article proposes a new microarchitectural scheme for reducing the hardware complexity of the integer register file of a superscalar processor. The register file is split into two banks holding even-numbered and odd-numbered physical registers, respectively. Each bank provides one read port to each two-input integer execution unit. This way, each bank has half the total number of read ports, and the register file area is roughly halved, which reduces the energy dissipated per register access and the register access time. However, a bank conflict occurs when both inputs of a two-input micro-operation lie in the same bank. Bank conflicts hurt performance, and we propose a simple solution to remove most bank conflicts, thus recovering most of the lost performance.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",energy efficiency; integer register file; read ports; Superscalar microarchitecture,Computer architecture; Execution units; Hardwares complexity; Integer register files; Performance; Physical registers; Read port; Register access; Superscalar microarchitecture; Superscalar Processor; Energy efficiency
Energy-efficient In-Memory Address Calculation,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146422935&doi=10.1145%2f3546071&partnerID=40&md5=dc7588f1cba91da86d1a3132446abd89,"Computation-in-Memory (CIM) is an emerging computing paradigm to address memory bottleneck challenges in computer architecture. A CIM unit cannot fully replace a general-purpose processor. Still, it significantly reduces the amount of data transfer between a traditional memory unit and the processor by enriching the transferred information. Data transactions between processor and memory consist of memory access addresses and values. While the main focus in the field of in-memory computing is to apply computations on the content of the memory (values), the importance of CPU-CIM address transactions and calculations for generating the sequence of access addresses for data-dominated applications is generally overlooked. However, the amount of information transactions used for ""address""can easily be even more than half of the total transferred bits in many applications. In this article, we propose a circuit to perform the in-memory Address Calculation Accelerator. Our simulation results showed that calculating address sequences inside the memory (instead of the CPU) can significantly reduce the CPU-CIM address transactions and therefore contribute to considerable energy saving, latency, and bus traffic. For a chosen application of guided image filtering, in-memory address calculation results in almost two orders of magnitude reduction in address transactions over the memory bus.  © 2022 Copyright held by the owner/author(s).",address calculation unit; energy optimization; In-memory processing,Data transfer; General purpose computers; Memory architecture; Address calculation unit; Data transaction; Emerging computing paradigm; Energy efficient; Energy optimization; General purpose processors; In-memory processing; Memory address; Memory bottleneck; Memory units; Energy efficiency
EXPERTISE: An Effective Software-level Redundant Multithreading Scheme against Hardware Faults,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146417875&doi=10.1145%2f3546073&partnerID=40&md5=b54e725889efaa6adfbeb7f920dabe3b,"Error resilience is the primary design concern for safety- and mission-critical applications. Redundant MultiThreading (RMT) is one of the most promising soft and hard error resilience strategies because it does not require additional hardware modification. While the state-of-the-art software RMT scheme can achieve a high degree of error protection, our detailed investigation revealed that it suffers from performance overhead and insufficient fault coverage. This paper proposes EXPERTISE, a compiler-level RMT scheme that can detect the manifestation of hardware faults in all processor components. EXPERTISE transformation generates a checker-thread for the main execution thread. These redundant threads are executed simultaneously on two physically different cores of a multicore processor and perform almost the same computations. After each memory write operation is committed by the main-thread, the checker-thread loads back the written data from the memory and checks it against its own locally computed values. If they match, the execution continues. Otherwise, the error flag is raised. In order to evaluate the effectiveness of the proposed solution, we performed soft and hard error injection experiments on all the different hardware components of an ARM Cortex53-like μ-architecturally simulated microprocessor. Based on statistical fault injection campaigns, we have found that EXPERTISE provides 188× better fault coverage with 27% faster performance as compared to the state-of-the-art scheme.  © 2022 Copyright held by the owner/author(s).",redundant multithreading; Soft error; transient fault,Errors; Radiation hardening; Error resilience; Fault coverages; Hard errors; Hardware faults; Performance; Primary design; Redundant multithreading; Safety critical applications; Soft error; Transient faults; Multitasking
Adaptive Contention Management for Fine-Grained Synchronization on Commodity GPUs,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146422948&doi=10.1145%2f3547301&partnerID=40&md5=0c0a3a8bd25e46654a3007a8f61f6329,"As more emerging applications are moving to GPUs, fine-grained synchronization has become imperative. However, their performance can be severely impaired in case of frequent synchronization failures caused by high data contention. Differently from CPUs, GPUs own thousands of hardware threads and adopt single instruction multiple threads paradigm, making it impractical to deploy the CPU contention management mechanisms directly on GPUs. In this article, we design a Software Warp Controlling Framework (SWCF), which employs producer-consumer execution model and leverages GPU hardware barriers to dynamically control the execution of warps at runtime. On the basis of SWCF, we propose a contention management strategy to decrease frequent synchronization failures while avoiding the over-reducing of parallelism. We evaluate SWCF and the proposed strategy on commodity GPUs using a set of applications with fine-grained synchronization. The results show that on V100 GPU our contention management achieves a 4.7X speedup and outperforms the conventional GPU software backoff solution by 42% on average.  © 2022 Association for Computing Machinery.",backoff; data contention; Fine-grained synchronization; GPUs,Computer hardware; Program processors; Backoffs; Contention managements; Data contention; Emerging applications; Fine grained; Fine-grained synchronization; Hardware threads; Management mechanisms; Multiple threads; Performance; Synchronization
Phronesis: Efficient Performance Modeling for High-dimensional Configuration Tuning,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146423015&doi=10.1145%2f3546868&partnerID=40&md5=76708871ae5b28d319c86bb7f9824028,"We present Phronesis, a learning framework for efficiently modeling the performance of data analytic workloads as a function of their high-dimensional software configuration parameters. Accurate performance models are useful for efficiently optimizing data analytic performance. Phronesis explicitly considers the error decomposition in statistical learning and implications for efficient data acquisition and model growth strategies in performance modeling. We demonstrate Phronesis with three popular machine learning models commonly used in performance tuning: neural network, random forest, and regression spline. We implement and evaluate it for Spark configuration parameters. We show that Phronesis significantly reduces data collection time for training predictive models by up to 57% and 37%, on average, compared to state-of-the-art techniques in building Spark performance models. Furthermore, we construct a configuration autotuning pipeline based on Phronesis. Our results indicate up to 30% gains in performance for Spark workloads over previous, state-of-the-art tuning strategies that use high-dimensional models.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",autotuning; machine learning; performance modeling; Spark,Data acquisition; Autotuning; Configuration parameters; Data analytics; High-dimensional; Higher-dimensional; Learning frameworks; Machine-learning; Performance; Performance Modeling; Software configuration; Machine learning
An Application-oblivious Memory Scheduling System for DNN Accelerators,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146429404&doi=10.1145%2f3535355&partnerID=40&md5=73c079ef5b40fc2cc0c28a0c3024e129,"Deep Neural Networks (DNNs) tend to go deeper and wider, which poses a significant challenge to the training of DNNs, due to the limited memory capacity of DNN accelerators. Existing solutions for memory-efficient DNN training are densely coupled with the application features of DNN workloads, e.g., layer structures or computational graphs of DNNs are necessary for these solutions. This would result in weak versatility for DNNs with sophisticated layer structures or complicated computation graphs. These schemes usually need to be re-implemented or re-adapted due to the new layer structures or the unusual operators in the computational graphs introduced by these DNNs. In this article, we review the memory pressure issues of DNN training from the perspective of runtime systems and model the memory access behaviors of DNN workloads. We identify the iterative, regularity, and extremalization properties of memory access patterns for DNN workloads. Based on these observations, we propose AppObMem, an application-oblivious memory scheduling system. AppObMem automatically traces the memory behaviors of DNN workloads and schedules the memory swapping to reduce the memory pressure of the device accelerators without the perception of high-level information of layer structures or computation graphs. Evaluations on a variety of DNN models show that, AppObMem obtains 40-60% memory savings with acceptable performance loss. AppObMem is also competitive with other open sourced SOTA schemes. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep learning; DNN accelerators; memory scheduling; runtime system,Graphic methods; Memory architecture; Computational graph; Deep learning; Deep neural network accelerator; Layer structures; Limited memory; Memory pressure; Memory scheduling; Neural networks trainings; Run- time systems; Scheduling systems; Deep neural networks
ASA: Accelerating Sparse Accumulation in Column-wise SpGEMM,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146437944&doi=10.1145%2f3543068&partnerID=40&md5=5d423d1599c2583ac27a6d3cd72fa3c3,"Sparse linear algebra is an important kernel in many different applications. Among various sparse general matrix-matrix multiplication (SpGEMM) algorithms, Gustavson's column-wise SpGEMM has good locality when reading input matrix and can be easily parallelized by distributing the computation of different columns of an output matrix to different processors. However, the sparse accumulation (SPA) step in column-wise SpGEMM, which merges partial sums from each of the multiplications by the row indices, is still a performance bottleneck. The state-of-the-art software implementation uses a hash table for partial sum search in the SPA, which makes SPA the largest contributor to the execution time of SpGEMM. There are three reasons that cause the SPA to become the bottleneck: (1) hash probing requires data-dependent branches that are difficult for a branch predictor to predict correctly; (2) the accumulation of partial sum is dependent on the results of the hash probing, which makes it difficult to hide the hash probing latency; and (3) hash collision requires time-consuming linear search and optimizations to reduce these collisions require an accurate estimation of the number of non-zeros in each column of the output matrix.This work proposes ASA architecture to accelerate the SPA. ASA overcomes the challenges of SPA by (1) executing the partial sum search and accumulate with a single instruction through ISA extension to eliminate data-dependent branches in hash probing, (2) using a dedicated on-chip cache to perform the search and accumulation in a pipelined fashion, (3) relying on the parallel search capability of a set-associative cache to reduce search latency, and (4) delaying the merging of overflowed entries. As a result, ASA achieves an average of 2.25× and 5.05× speedup as compared to the state-of-the-art software implementation of a Markov clustering application and its SpGEMM kernel, respectively. As compared to a state-of-the-art hashing accelerator design, ASA achieves an average of 1.95× speedup in the SpGEMM kernel.  © 2022 Copyright held by the owner/author(s).",Markov clustering; sparse accumulation; sparse linear algebra; SpGEMM,Application programs; Data structures; Data dependent; Markov clustering; Matrix-matrix multiplications; Output matrix; Partial sums; Software implementation; Sparse accumulation; Sparse general matrix-matrix multiplication; Sparse linear algebra; State of the art; Matrix algebra
LiteCON: An All-photonic Neuromorphic Accelerator for Energy-efficient Deep Learning,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139203910&doi=10.1145%2f3531226&partnerID=40&md5=d5813461adeaf246fe1932cbd319fa76,"Deep learning is highly pervasive in today's data-intensive era. In particular, convolutional neural networks (CNNs) are being widely adopted in a variety of fields for superior accuracy. However, computing deep CNNs on traditional CPUs and GPUs brings several performance and energy pitfalls. Several novel approaches based on ASIC, FPGA, and resistive-memory devices have been recently demonstrated with promising results. Most of them target only the inference (testing) phase of deep learning. There have been very limited attempts to design a full-fledged deep learning accelerator capable of both training and inference. It is due to the highly compute- and memory-intensive nature of the training phase. In this article, we propose LiteCON, a novel analog photonics CNN accelerator. LiteCON uses silicon microdisk-based convolution, memristor-based memory, and dense-wavelength-division-multiplexing for energy-efficient and ultrafast deep learning. We evaluate LiteCON using a commercial CAD framework (IPKISS) on deep learning benchmark models including LeNet and VGG-Net. Compared to the state of the art, LiteCON improves the CNN throughput, energy efficiency, and computational efficiency by up to 32×, 37×, and 5×, respectively, with trivial accuracy degradation.  © 2022 Association for Computing Machinery.",Deep learning; memristor; on-chip photonics,Computational efficiency; Computer aided design; Convolution; Convolutional neural networks; Deep learning; Energy efficiency; Program processors; Convolutional neural network; Data intensive; Deep learning; Energy; Energy efficient; Memristor; Neuromorphic; On-chip photonics; Performance; Resistive memory; Memristors
PowerMorph: QoS-Aware Server Power Reshaping for Data Center Regulation Service,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139186908&doi=10.1145%2f3524129&partnerID=40&md5=fdf58151c64ab8e604aa5ae9c81ec097,"Adoption of renewable energy in power grids introduces stability challenges in regulating the operation frequency of the electricity grid. Thus, electrical grid operators call for provisioning of frequency regulation services from end-user customers, such as data centers, to help balance the power grid's stability by dynamically adjusting their energy consumption based on the power grid's need. As renewable energy adoption grows, the average reward price of frequency regulation services has become much higher than that of the electricity cost. Therefore, there is a great cost incentive for data centers to provide frequency regulation service. Many existing techniques modulating data center power result in significant performance slowdown or provide a low amount of frequency regulation provision. We present PowerMorph, a tight QoS-aware data center power-reshaping framework, which enables commodity servers to provide practical frequency regulation service. The key behind PowerMorph is using ""complementary workload""as an additional knob to modulate server power, which provides high provision capacity while satisfying tight QoS constraints of latency-critical workloads. We achieve up to 58% improvement to TCO under common conditions, and in certain cases can even completely eliminate the data center electricity bill and provide a net profit. Copyright © 2022 held by the owner/author(s).",co-location; Data center; power management; quality of service; regulation service,Electric power transmission networks; Energy utilization; Green computing; Power management; Colocations; Datacenter; Frequency regulation services; Operation frequency; Power; Power grids; QoS-aware; Quality-of-service; Regulation services; Renewable energies; Quality of service
"CoMeT: An Integrated Interval Thermal Simulation Toolchain for 2D, 2.5D, and 3D Processor-Memory Systems",2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139257618&doi=10.1145%2f3532185&partnerID=40&md5=0168578b9e1e284595b7798dc43434cc,"Processing cores and the accompanying main memory working in tandem enable modern processors. Dissipating heat produced from computation remains a significant problem for processors. Therefore, the thermal management of processors continues to be an active subject of research. Most thermal management research is performed using simulations, given the challenges in measuring temperatures in real processors. Fast yet accurate interval thermal simulation toolchains remain the research tool of choice to study thermal management in processors at the system level. However, the existing toolchains focus on the thermal management of cores in the processors, since they exhibit much higher power densities than memory. The memory bandwidth limitations associated with 2D processors lead to high-density 2.5D and 3D packaging technology: 2.5D packaging technology places cores and memory on the same package; 3D packaging technology takes it further by stacking layers of memory on the top of cores themselves. These new packagings significantly increase the power density of the processors, making them prone to overheating. Therefore, mitigating thermal issues in high-density processors (packaged with stacked memory) becomes even more pressing. However, given the lack of thermal modeling for memories in existing interval thermal simulation toolchains, they are unsuitable for studying thermal management for high-density processors. To address this issue, we present the first integrated Core and Memory interval Thermal (CoMeT) simulation toolchain. CoMeT comprehensively supports thermal simulation of high- and low-density processors corresponding to four different core-memory (integration) configurations - off-chip DDR memory, off-chip 3D memory, 2.5D, and 3D. CoMeT supports several novel features that facilitate overlying system research. CoMeT adds only an additional ∼5% simulation-time overhead compared to an equivalent state-of-the-art core-only toolchain. The source code of CoMeT has been made open for public use under the MIT license.  © 2022 Association for Computing Machinery.",3D memories; stacked architectures; thermal simulation,Chip scale packages; Thermal management (electronics); Three dimensional integrated circuits; 3D memory; 3D packaging; 3D processors; Memory systems; Off-chip; Packaging technologies; Processor memory; Stacked architecture; Thermal; Thermal simulations; Temperature control
A Case for Fine-grain Coherence Specialization in Heterogeneous Systems,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139237014&doi=10.1145%2f3530819&partnerID=40&md5=e208df7d26522cd45b375a025708d28a,"Hardware specialization is becoming a key enabler of energy-efficient performance. Future systems will be increasingly heterogeneous, integrating multiple specialized and programmable accelerators, each with different memory demands. Traditionally, communication between accelerators has been inefficient, typically orchestrated through explicit DMA transfers between different address spaces. More recently, industry has proposed unified coherent memory which enables implicit data movement and more data reuse, but often these interfaces limit the coherence flexibility available to heterogeneous systems. This paper demonstrates the benefits of fine-grained coherence specialization for heterogeneous systems. We propose an architecture that enables low-complexity independent specialization of each individual coherence request in heterogeneous workloads by building upon a simple and flexible baseline coherence interface, Spandex. We then describe how to optimize individual memory requests to improve cache reuse and performance-critical memory latency in emerging heterogeneous workloads. Collectively, our techniques enable significant gains, reducing execution time by up to 61% or network traffic by up to 99% while adding minimal complexity to the Spandex protocol.  Copyright © 2022 held by the owner/author(s).",caches; coherence; GPUs; Shared memory systems,Cache memory; Complex networks; Energy efficiency; Memory architecture; Address space; Cache; DMA transfers; Energy efficient; Finer grains; Heterogeneous systems; Heterogeneous workloads; Performance; Shared memory system; Specialisation; Program processors
Using Barrier Elision to Improve Transactional Code Generation,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139212089&doi=10.1145%2f3533318&partnerID=40&md5=c263e8ec667842a0c88a81e769fda0cb,"With chip manufacturers such as Intel, IBM, and ARM offering native support for transactional memory in their instruction set architectures, memory transactions are on the verge of being considered a genuine application tool rather than just an interesting research topic. Despite this recent increase in popularity on the hardware side of transactional memory (HTM), software support for transactional memory (STM) is still scarce and the only compiler with transactional support currently available, the GNU Compiler Collection (GCC), does not generate code that achieves desirable performance. For hybrid solutions of TM (HyTM), which are frameworks that leverage the best aspects of HTM and STM, the subpar performance of the software side, caused by inefficient compiler generated code, might forbid HyTM to offer optimal results. This article extends previous work focused exclusively on STM implementations by presenting a detailed analysis of transactional code generated by GCC in the context of HybridTM implementations. In particular, it builds on previous research of transactional memory support in the Clang/LLVM compiler framework, which is decoupled from any TM runtime, and presents the following novel contributions: (a) it shows that STM's performance overhead, due to an excessive amount of read and write barriers added by the compiler, also impacts the performance of HyTM systems; and (b) it reveals the importance of the previously proposed annotation mechanism to reduce the performance gap between HTM and STM in phased runtime systems. Furthermore, it shows that, by correctly using the annotations on just a few lines of code, it is possible to reduce the total number of instrumented barriers by 95% and to achieve speed-ups of up to 7× when compared to the original code generated by GCC and the Clang compiler.1  © 2022 Association for Computing Machinery.",debugging; Transactional memory,Memory architecture; Open source software; Program compilers; Storage allocation (computer); Chip manufacturers; Codegeneration; Debugging; GNU compiler collection; Hybrid solution; Instruction set architecture; Memory transactions; Performance; Research topics; Transactional memory; Optimal systems
Online Application Guidance for Heterogeneous Memory Systems,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139257776&doi=10.1145%2f3533855&partnerID=40&md5=46b7ba9efe5947849b498948e51f514c,"As scaling of conventional memory devices has stalled, many high-end computing systems have begun to incorporate alternative memory technologies to meet performance goals. Since these technologies present distinct advantages and tradeoffs compared to conventional DDR∗SDRAM, such as higher bandwidth with lower capacity or vice versa, they are typically packaged alongside conventional SDRAM in a heterogeneous memory architecture. To utilize the different types of memory efficiently, new data management strategies are needed to match application usage to the best available memory technology. However, current proposals for managing heterogeneous memories are limited, because they either (1) do not consider high-level application behavior when assigning data to different types of memory or (2) require separate program execution (with a representative input) to collect information about how the application uses memory resources. This work presents a new data management toolset to address the limitations of existing approaches for managing complex memories. It extends the application runtime layer with automated monitoring and management routines that assign application data to the best tier of memory based on previous usage, without any need for source code modification or a separate profiling run. It evaluates this approach on a state-of-the-art server platform with both conventional DDR4 SDRAM and non-volatile Intel Optane DC memory, using both memory-intensive high-performance computing (HPC) applications as well as standard benchmarks. Overall, the results show that this approach improves program performance significantly compared to a standard unguided approach across a variety of workloads and system configurations. The HPC applications exhibit the largest benefits, with speedups ranging from 1.4× to 7× in the best cases. Additionally, we show that this approach achieves similar performance as a comparable offline profiling-based approach after a short startup period, without requiring separate program execution or offline analysis steps.  © 2022 Association for Computing Machinery.",analysis; heterogeneous memory management; Profiling; runtime systems,Application programs; Dynamic random access storage; Information management; Memory architecture; Online systems; Separation; Analyse; Heterogeneous memory; Heterogeneous memory management; High-performance computing applications; Memory technology; Memory-management; Performance; Profiling; Program execution; Run- time systems; Benchmarking
Preserving Addressability Upon GC-Triggered Data Movements on Non-Volatile Memory,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127791801&doi=10.1145%2f3511706&partnerID=40&md5=c1b1be8ff5abdcab7b5ee79b44b3f55c,"This article points out an important threat that application-level Garbage Collection (GC) creates to the use of non-volatile memory (NVM). Data movements incurred by GC may invalidate the pointers to objects on NVM and, hence, harm the reusability of persistent data across executions. The article proposes the concept of movement-oblivious addressing (MOA), and develops and compares three novel solutions to materialize the concept for solving the addressability problem. It evaluates the designs on five benchmarks and a real-world application. The results demonstrate the promise of the proposed solutions, especially hardware-supported Multi-Level GPointer, in addressing the problem in a space- A nd time-efficient manner. © 2022 Association for Computing Machinery.",garbage collector; memory management; Persistent memory,Benchmarking; Nonvolatile storage; Application level; Data movements; Garbage collection; Garbage collectors; Memory-management; Multilevels; Novel solutions; Persistent memory; Real-world; Time-efficient; Reusability
GiantVM: A Novel Distributed Hypervisor for Resource Aggregation with DSM-aware Optimizations,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127767274&doi=10.1145%2f3505251&partnerID=40&md5=559df094a11201f8f2142e50c5d583ac,"We present GiantVM,1 an open-source distributed hypervisor that provides the many-to-one virtualization to aggregate resources from multiple physical machines. We propose techniques to enable distributed CPU and I/O virtualization and distributed shared memory (DSM) to achieve memory aggregation. GiantVM is implemented based on the state-of-the-art type-II hypervisor QEMU-KVM, and it can currently host conventional OSes such as Linux. (1) We identify the performance bottleneck of GiantVM to be DSM, through a top-down performance analysis. Although GiantVM offers great opportunities for CPU-intensive applications to enjoy the aggregated CPU resources, memory-intensive applications could suffer from cross-node page sharing, which requires frequent DSM involvement and leads to performance collapse. We design the guest-level thread scheduler, DaS (DSM-aware Scheduler), to overcome the bottleneck. When benchmarking with NAS Parallel Benchmarks, the DaS could achieve a performance boost of up to 3.5×, compared to the default Linux kernel scheduler. (2) While evaluating DaS, we observe the advantage of GiantVM as a resource reallocation facility. Thanks to the SSI abstraction of GiantVM, migration could be done by guest-level scheduling. DSM allows standby pages in the migration destination, which need not be transferred through the network. The saved network bandwidth is 68% on average, compared to VM live migration. Resource reallocation with GiantVM increases the overall CPU utilization by 14.3% in a co-location experiment. © 2022 Association for Computing Machinery.",distributed shared memory; false sharing; Resource aggregation; single system image; virtualization,Benchmarking; Linux; Memory architecture; Scheduling; Virtual reality; Distributed shared memory; False sharing; Hypervisors; Memory aware; Optimisations; Performance; Resource aggregation; Resource reallocation; Single system image; Virtualizations; Virtualization
Low-power Near-data Instruction Execution Leveraging Opcode-based Timing Analysis,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127783219&doi=10.1145%2f3504005&partnerID=40&md5=d9a9f2565740f54405b700f44bb98c53,"Traditional processor architectures utilize an external DRAM for data storage, while they also operate under worst-case timing constraints. Such designs are heavily constrained by the delay costs of the data transfer between the core pipeline and the DRAM, and they are incapable of exploiting the timing variations of their pipeline stages. In this work, we focus on a near-data processing methodology combined with a novel timing analysis technique that enables the adaptive frequency scaling of the core clock and boosts the performance of low-power designs. We propose a near-data processing and better-than-worst-case co-design methodology to efficiently move the instruction execution to the DRAM side and, at the same time, to allow the pipeline to operate at higher clock frequencies compared to the worst-case approach. To this end, we develop a timing analysis technique, which evaluates the timing requirements of individual instructions and we dynamically scale the clock frequency, according to the instructions types that currently occupy the pipeline. We evaluate the proposed methodology on six different RISC-V post-layout implementations using an HMC DRAM to enable the processing-in-memory (PIM) process. Results indicate an average speedup factor of 1.96× with a 1.6× reduction in energy consumption compared to a standard RISC-V PIM baseline implementation. © 2022 Association for Computing Machinery.",adaptive clock scaling; better-than-worst case design; hybrid memory cube; Near-data processing; timing analysis,Clocks; Data transfer; Dynamic random access storage; Electric power supplies to apparatus; Energy utilization; Integrated circuit design; Low power electronics; Pipelines; Timing circuits; Adaptive clock scaling; Adaptive clocks; Analysis techniques; Better-than-bad case design; Hybrid memory; Hybrid memory cube; Near-data processing; Scalings; Timing Analysis; Worst case design; Data handling
Register-Pressure-Aware Instruction Scheduling Using Ant Colony Optimization,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127790978&doi=10.1145%2f3505558&partnerID=40&md5=c4600a02e6e2ed89638705af1ddd8210,"This paper describes a new approach to register-pressure-aware instruction scheduling, using Ant Colony Optimization (ACO). ACO is a nature-inspired optimization technique that researchers have successfully applied to NP-hard sequencing problems like the Traveling Salesman Problem (TSP) and its derivatives. In this work, we describe an ACO algorithm for solving the long-standing compiler optimization problem of balancing Instruction-Level Parallelism (ILP) and Register Pressure (RP) in pre-allocation instruction scheduling. Three different cost functions are studied for estimating RP during instruction scheduling. The proposed ACO algorithm is implemented in the LLVM open-source compiler, and its performance is evaluated experimentally on three different machines with three different instruction-set architectures: Intel x86, ARM, and AMD GPU. The proposed ACO algorithm is compared to an exact Branch-and-Bound (B&B) algorithm proposed in previous work. On x86 and ARM, both algorithms are evaluated relative to LLVM's generic scheduler, while on the AMD GPU, the algorithms are evaluated relative to AMD's production scheduler. The experimental results show that using SPECrate 2017 Floating Point, the proposed algorithm gives geometric-mean improvements of 1.13% and 1.25% in execution speed on x86 and ARM, respectively, relative to the LLVM scheduler. Using PlaidML on an AMD GPU, it gives a geometric-mean improvement of 7.14% in execution speed relative to the AMD scheduler. The proposed ACO algorithm gives approximately the same execution-time results as the B&B algorithm, with each algorithm outperforming the other on a substantial number of hard scheduling regions. ACO gives better results than B&B on many large instances that B&B times out on. Both ACO and B&B outperform the LLVM algorithm on the CPU and the AMD algorithm on the GPU. © 2022 Association for Computing Machinery.",Ant Colony Optimization; Compiler optimizations; instruction scheduling; instruction scheduling for the GPU; multi-objective optimization problems; NP-complete problems; register-pressure reduction; spill code minimization,Artificial intelligence; Biomimetics; Computational complexity; Computer architecture; Cost functions; Digital arithmetic; Graphics processing unit; Production control; Program compilers; Scheduling; Signal encoding; Traveling salesman problem; Ant colonies; Ant colony optimization; Colony optimization; Compiler optimizations; Instruction scheduling; Instruction scheduling for the GPU; Minimisation; Multi-objective optimization problem; Pressure reduction; Register pressure; Register-pressure reduction; Spill code; Spill code minimization; Ant colony optimization
A Case for Intra-rack Resource Disaggregation in HPC,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127830207&doi=10.1145%2f3514245&partnerID=40&md5=c57ccf8f4e0fd62c026ba9edddd1bb99,"The expected halt of traditional technology scaling is motivating increased heterogeneity in high-performance computing (HPC) systems with the emergence of numerous specialized accelerators. As heterogeneity increases, so does the risk of underutilizing expensive hardware resources if we preserve today's rigid node configuration and reservation strategies. This has sparked interest in resource disaggregation to enable finer-grain allocation of hardware resources to applications. However, there is currently no data-driven study of what range of disaggregation is appropriate in HPC. To that end, we perform a detailed analysis of key metrics sampled in NERSC's Cori, a production HPC system that executes a diverse open-science HPC workload. In addition, we profile a variety of deep-learning applications to represent an emerging workload. We show that for a rack (cabinet) configuration and applications similar to Cori, a central processing unit with intra-rack disaggregation has a 99.5% probability to find all resources it requires inside its rack. In addition, ideal intra-rack resource disaggregation in Cori could reduce memory and NIC resources by 5.36% to 69.01% and still satisfy the worst-case average rack utilization. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Disaggregation; HPC; LDMS; memory; utilization,Deep learning; Disaggregation; Expensive hardware; Finer grains; Hardware resources; High performance computing systems; High-performance computing; LDMS; Performance computing; Technology scaling; Utilization; Program processors
MemHC: An Optimized GPU Memory Management Framework for Accelerating Many-body Correlation,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127828415&doi=10.1145%2f3506705&partnerID=40&md5=c96de8b0f57e9f49516179a6bfb02580,"The many-body correlation function is a fundamental computation kernel in modern physics computing applications, e.g., Hadron Contractions in Lattice quantum chromodynamics (QCD). This kernel is both computation and memory intensive, involving a series of tensor contractions, and thus usually runs on accelerators like GPUs. Existing optimizations on many-body correlation mainly focus on individual tensor contractions (e.g., cuBLAS libraries and others). In contrast, this work discovers a new optimization dimension for many-body correlation by exploring the optimization opportunities among tensor contractions. More specifically, it targets general GPU architectures (both NVIDIA and AMD) and optimizes many-body correlation's memory management by exploiting a set of memory allocation and communication redundancy elimination opportunities: First, GPU memory allocation redundancy: The intermediate output frequently occurs as input in the subsequent calculations; second, CPU-GPU communication redundancy: Although all tensors are allocated on both CPU and GPU, many of them are used (and reused) on the GPU side only, and thus, many CPU/GPU communications (like that in existing Unified Memory designs) are unnecessary; third, GPU oversubscription: Limited GPU memory size causes oversubscription issues, and existing memory management usually results in near-reuse data eviction, thus incurring extra CPU/GPU memory communications.Targeting these memory optimization opportunities, this article proposes MemHC, an optimized systematic GPU memory management framework that aims to accelerate the calculation of many-body correlation functions utilizing a series of new memory reduction designs. These designs involve optimizations for GPU memory allocation, CPU/GPU memory movement, and GPU memory oversubscription, respectively. More specifically, first, MemHC employs duplication-aware management and lazy release of GPU memories to corresponding host managing for better data reusability. Second, it implements data reorganization and on-demand synchronization to eliminate redundant (or unnecessary) data transfer. Third, MemHC exploits an optimized Least Recently Used (LRU) eviction policy called Pre-Protected LRU to reduce evictions and leverage memory hits. Additionally, MemHC is portable for various platforms including NVIDIA GPUs and AMD GPUs. The evaluation demonstrates that MemHC outperforms unified memory management by to . The proposed Pre-Protected LRU policy outperforms the original LRU policy by up to improvement. © 2022 Copyright held by the owner/author(s).",GPU memory management; many-body correlation function; memory oversubscription; memory redundancy elimination,Data transfer; Graphics processing unit; Program processors; Quantum theory; Redundancy; Reusability; Tensors; GPU memory management; Many-body correlation functions; Many-body correlations; Memory oversubscription; Memory redundancy; Memory redundancy elimination; Memory-management; Optimisations; Redundancy elimination; Tensor contraction; Memory architecture
Cooperative Slack Management: Saving Energy of Multicore Processors by Trading Performance Slack between QoS-Constrained Applications,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127808227&doi=10.1145%2f3505559&partnerID=40&md5=aab62e48ceb143bf43d5c5f106ba4d97,"Processor resources can be adapted at runtime according to the dynamic behavior of applications to reduce the energy consumption of multicore processors without affecting the Quality-of-Service (QoS). To achieve this, an online resource management scheme is needed to control processor configurations such as cache partitioning, dynamic voltage-frequency scaling, and dynamic adaptation of core resources.Prior State-of-the-art has shown the potential for reducing energy without any performance degradation by coordinating the control of different resources. However, in this article, we show that by allowing short-term variations in processing speed (e.g., instructions per second rate), in a controlled fashion, we can enable substantial improvements in energy savings while maintaining QoS. We keep track of such variations in the form of performance slack. Slack can be generated, at some energy cost, by processing faster than the performance target. On the other hand, it can be utilized to save energy by allowing a temporary relaxation in the performance target. Based on this insight, we present Cooperative Slack Management (CSM). During runtime, CSM finds opportunities to generate slack at low energy cost by estimating the performance and energy for different resource configurations using analytical models. This slack is used later when it enables larger energy savings. CSM performs such trade-offs across multiple applications, which means that the slack collected for one application can be used to reduce the energy consumption of another. This cooperative approach significantly increases the opportunities to reduce system energy compared with independent slack management for each application. For example, we show that CSM can potentially save up to 41% of system energy (on average, 25%) in a scenario in which both prior art and an extended version with local slack management for each core are ineffective. © 2022 Association for Computing Machinery.",cache partitioning; DVFS; dynamic core resizing; Multicore processors; performance and energy modeling; QoS,Commerce; Dynamic frequency scaling; Economic and social effects; Energy conservation; Energy utilization; Voltage scaling; Cache partitioning; DVFS; Dynamic core resizing; Dynamic cores; Energy model; Multi-core processor; Performance; Performance Modeling; Quality-of-service; Slack management; Quality of service
MAPPER: Managing Application Performance via Parallel Efficiency Regulation,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127813919&doi=10.1145%2f3501767&partnerID=40&md5=7baf93d667772f8cdf07dd2508924354,"State-of-the-art systems, whether in servers or desktops, provide ample computational and storage resources to allow multiple simultaneously executing potentially parallel applications. However, performance tends to be unpredictable, being a function of algorithmic design, resource allocation choices, and hardware resource limitations.In this article, we introduce MAPPER, a manager of application performance via parallel efficiency regulation. MAPPER uses a privileged daemon to monitor (using hardware performance counters) and coordinate all participating applications by making two coupled decisions: The degree of parallelism to allow each application to improve system efficiency while guaranteeing quality of service (QoS), and which specific CPU cores to schedule applications on. The QoS metric may be chosen by the application and could be in terms of execution time, throughput, or tail latency, relative to the maximum performance achievable on the machine. We demonstrate that using a normalized parallel efficiency metric allows comparison across and cooperation among applications to guarantee their required QoS. While MAPPER may be used without application or runtime modification, use of a simple interface to communicate application-level knowledge improves MAPPER's efficacy. Using a QoS guarantee of 85% of the IPC achieved with a fair share of resources on the machine, MAPPER achieves up to 3.3 speedup relative to unmodified Linux and runtime systems, with an average improvement of 17% in our test cases. At the same time, MAPPER violates QoS for only 2% of the applications (compared to 23% for Linux), while placing much tighter bounds on the worst case. MAPPER relieves hardware bottlenecks via task-to-CPU placement and allocates more CPU contexts to applications that exhibit higher parallel efficiency while guaranteeing QoS, resulting in both individual application performance predictability and overall system efficiency. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Parallel systems; performance counters; resource contention; sharing-aware scheduler,Computer hardware; Efficiency; Linux; Application performance; Parallel efficiency; Parallel system; Performance; Performance counters; Quality-of-service; Resource contention; Sharing-aware scheduler; State-of-the-art system; System efficiency; Quality of service
Accelerating Video Captioning on Heterogeneous System Architectures,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139221481&doi=10.1145%2f3527609&partnerID=40&md5=9ca3524307a1f99b9631cf7ea4f99169,"Video captioning is a core technology to many important applications, such as AI-assisted medical diagnosis, video question answering, storytelling through videos, and lip-reading. Video captioning employs a hybrid CNN + RNN model. Accelerating such a hybrid model on a heterogeneous system is challenging for two reasons. First, CNN and RNN exhibit very different computing behaviors, making the mapping between computation and heterogeneous devices difficult. Second, data dependency exists between the CNN and RNN within a video frame and between adjacent RNNs across video frames. These data dependencies prohibit the full parallelization of the hybrid model. The issues also include the utilization of accelerator resources, which is critical to maximizing the performance. In this work, we propose a fine-grained scheduling scheme for mapping computation and devices within a video frame, and a pipeline scheduling scheme for exploiting maximum parallelism between the execution of the video frames. In addition, we propose two capacity-guided scheduling methods. On the server, the concurrent kernel execution mechanism is exploited for improving GPU utilization. On the edge platform, we rearrange CNN computation among the CPU and EdgeTPUs guided by the EdgeTPU's SRAM capacity so that balanced computation is achieved and off-chip memory overhead is minimized. Experimental results show that our scheduling scheme improves video captioning performance by up to 3.24 with CPU + GPU collaboration over the GPU-only execution. On an edge platform with an ARM CPU and two EdgeTPUs, our CPU + EdgeTPU scheduling exhibits outstanding performance, which achieves up to 54.9 speedup compared to using ARM CPU only and can perform video captioning of 59 frames per second.  © 2022 Association for Computing Machinery.",dynamic programming; heterogeneous system architectures; model scheduling; pipelining; Video captioning,Diagnosis; Graphics processing unit; Mapping; Memory architecture; Scanning; Scheduling; Static random access storage; Heterogeneous system architecture; Heterogeneous systems; Hybrid model; Model scheduling; Performance; Pipelining; Scheduling schemes; Systems architecture; Video captioning; Video frame; Dynamic programming
An Accelerator for Sparse Convolutional Neural Networks Leveraging Systolic General Matrix-matrix Multiplication,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139251882&doi=10.1145%2f3532863&partnerID=40&md5=f6076f24beb12868099d459ed6f21066,"This article proposes a novel hardware accelerator for the inference task with sparse convolutional neural networks (CNNs) by building a hardware unit to perform Image to Column (Im2Col) transformation of the input feature map coupled with a systolic-array-based general matrix-matrix multiplication (GEMM) unit. Our design carefully overlaps the Im2Col transformation with the GEMM computation to maximize parallelism. We propose a novel design for the Im2Col unit that uses a set of distributed local memories connected by a ring network, which improves energy efficiency and latency by streaming the input feature map only once. The systolic-array-based GEMM unit in the accelerator can be dynamically configured as multiple GEMM units with square-shaped systolic arrays or as a single GEMM unit with a tall systolic array. This dynamic reconfigurability enables effective pipelining of Im2Col and GEMM operations and attains high processing element utilization for a wide range of CNNs. Further, our accelerator is sparsity aware, improving performance and energy efficiency by effectively mapping the sparse feature maps and weights to the processing elements, skipping ineffectual operations and unnecessary data movements involving zeros. Our prototype, SPOTS, is on average 2.16, 1.74, and 1.63 faster than Gemmini, Eyeriss, and Sparse-PE, which are prior hardware accelerators for dense and sparse CNNs, respectively. SPOTS is also 78 and 12 more energy-efficient when compared to CPU and GPU implementations, respectively.  Copyright © 2022 held by the owner/author(s).",Convolutional neural networks; GEMM; hardware accelerators; sparse computation; systolic arrays,Acceleration; Convolutional neural networks; Energy efficiency; Linear transformations; Matrix algebra; Systolic arrays; Convolutional neural network; Feature map; General matrix-matrix multiplication; Hardware accelerators; Hardware units; Input features; Matrix-matrix multiplications; Novel hardware; Processing elements; Sparse computation; Convolution
ERASE: Energy Efficient Task Mapping and Resource Management for Work Stealing Runtimes,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127446114&doi=10.1145%2f3510422&partnerID=40&md5=c0198893daeb00d3ec45c4efb759d04a,"Parallel applications often rely on work stealing schedulers in combination with fine-grained tasking to achieve high performance and scalability. However, reducing the total energy consumption in the context of work stealing runtimes is still challenging, particularly when using asymmetric architectures with different types of CPU cores. A common approach for energy savings involves dynamic voltage and frequency scaling (DVFS) wherein throttling is carried out based on factors like task parallelism, stealing relations, and task criticality. This article makes the following observations: (i) leveraging DVFS on a per-task basis is impractical when using fine-grained tasking and in environments with cluster/chip-level DVFS; (ii) task moldability, wherein a single task can execute on multiple threads/cores via work-sharing, can help to reduce energy consumption; and (iii) mismatch between tasks and assigned resources (i.e., core type and number of cores) can detrimentally impact energy consumption. In this article, we propose EneRgy Aware SchedulEr (ERASE), an intra-application task scheduler on top of work stealing runtimes that aims to reduce the total energy consumption of parallel applications. It achieves energy savings by guiding scheduling decisions based on per-task energy consumption predictions of different resource configurations. In addition, ERASE is capable of adapting to both given static frequency settings and externally controlled DVFS. Overall, ERASE achieves up to 31% energy savings and improves performance by 44% on average, compared to the state-of-the-art DVFS-based schedulers. © 2022 Copyright held by the owner/author(s).",Energy; resource management; runtimes; task scheduling; work stealing,Dynamic frequency scaling; Energy efficiency; Energy utilization; Natural resources management; Scheduling; Voltage scaling; Dynamic voltage and frequency scaling; Energy; Energy  savings; Energy aware; Energy-consumption; Energy-savings; Parallel application; Resource management; Runtimes; Work stealing; Resource allocation
Dependence-aware Slice Execution to Boost MLP in Slice-out-of-order Cores,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127761060&doi=10.1145%2f3506704&partnerID=40&md5=96dbe9d741e262827fb6d4d76e5089ec,"Exploiting memory-level parallelism (MLP) is crucial to hide long memory and last-level cache access latencies. While out-of-order (OoO) cores, and techniques building on them, are effective at exploiting MLP, they deliver poor energy efficiency due to their complex and energy-hungry hardware. This work revisits slice-out-of-order (sOoO) cores as an energy-efficient alternative for MLP exploitation. sOoO cores achieve energy efficiency by constructing and executing slices of MLP-generating instructions out-of-order only with respect to the rest of instructions; the slices and the remaining instructions, by themselves, execute in-order. However, we observe that existing sOoO cores miss significant MLP opportunities due to their dependence-oblivious in-order slice execution, which causes dependent slices to frequently block MLP generation. To boost MLP generation, we introduce Freeway, a sOoO core based on a new dependence-aware slice execution policy that tracks dependent slices and keeps them from blocking subsequent independent slices and MLP extraction. The proposed core incurs minimal area and power overheads, yet approaches the MLP benefits of fully OoO cores. Our evaluation shows that Freeway delivers 12% better performance than the state-of-the-art sOoO core and is within 7% of the MLP limits of full OoO execution. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",instruction scheduling; memory level parallelism; Microarchitecture,Cache memory; Access latency; Cache access; Energy; Energy efficient; Instruction scheduling; Last-level caches; Long memory; Memory level parallelisms; Micro architectures; Out of order; Energy efficiency
Building a Fast and Efficient LSM-tree Store by Integrating Local Storage with Cloud Storage,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139240476&doi=10.1145%2f3527452&partnerID=40&md5=2506dbbf12839469f71a200561a0ed32,"The explosive growth of modern web-scale applications has made cost-effectiveness a primary design goal for their underlying databases. As a backbone of modern databases, LSM-tree based key-value stores (LSM store) face limited storage options. They are either designed for local storage that is relatively small, expensive, and fast or for cloud storage that offers larger capacities at reduced costs but slower. Designing an LSM store by integrating local storage with cloud storage services is a promising way to balance the cost and performance. However, such design faces challenges such as data reorganization, metadata overhead, and reliability issues. In this article, we propose RocksMash, a fast and efficient LSM store that uses local storage to store frequently accessed data and metadata while using cloud to hold the rest of the data to achieve cost-effectiveness. To improve metadata space-efficiency and read performance, RocksMash uses an LSM-aware persistent cache that stores metadata in a space-efficient way and stores popular data blocks by using compaction-aware layouts. Moreover, RocksMash uses an extended write-ahead log for fast parallel data recovery. We implemented RocksMash by embedding these designs into RocksDB. The evaluation results show that RocksMash improves the performance by up to 1.7× compared to the state-of-the-art schemes and delivers high reliability, cost-effectiveness, and fast recovery.  Copyright © 2022 held by the owner/author(s).",cache; key-value; Log-structured merge tree; succinct tree,Cloud storage; Metadata; Cache; Cloud storages; Design goal; Explosive growth; Key values; Log structured merge trees; Performance; Primary design; Succinct tree; Tree-based; Cost effectiveness
MetaSys: A Practical Open-source Metadata Management System to Implement and Evaluate Cross-layer Optimizations,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127732110&doi=10.1145%2f3505250&partnerID=40&md5=21f866236349d2f0c27e6c559689f4a7,"This article introduces the first open-source FPGA-based infrastructure, MetaSys, with a prototype in a RISC-V system, to enable the rapid implementation and evaluation of a wide range of cross-layer techniques in real hardware. Hardware-software cooperative techniques are powerful approaches to improving the performance, quality of service, and security of general-purpose processors. They are, however, typically challenging to rapidly implement and evaluate in real hardware as they require full-stack changes to the hardware, system software, and instruction-set architecture (ISA).MetaSys implements a rich hardware-software interface and lightweight metadata support that can be used as a common basis to rapidly implement and evaluate new cross-layer techniques. We demonstrate MetaSys's versatility and ease-of-use by implementing and evaluating three cross-layer techniques for: (i) prefetching in graph analytics; (ii) bounds checking in memory unsafe languages, and (iii) return address protection in stack frames; each technique requiring only ~100 lines of Chisel code over MetaSys.Using MetaSys, we perform the first detailed experimental study to quantify the performance overheads of using a single metadata management system to enable multiple cross-layer optimizations in CPUs. We identify the key sources of bottlenecks and system inefficiency of a general metadata management system. We design MetaSys to minimize these inefficiencies and provide increased versatility compared to previously proposed metadata systems. Using three use cases and a detailed characterization, we demonstrate that a common metadata management system can be used to efficiently support diverse cross-layer techniques in CPUs. MetaSys is completely and freely available at https://github.com/CMU-SAFARI/MetaSys. © 2022 Association for Computing Machinery.",Hardware-software cooperation; memory; metadata; open-source; RISC-V,Computer architecture; Metadata; Open source software; Open systems; Program processors; Quality of service; Cooperative techniques; Cross layer optimization; Cross-layer techniques; Hardware-software cooperation; Hardware/software; Metadata management system; Open-source; Performance quality; Quality-of-Security; RISC-V; General purpose computers
Triangle Dropping: An Occluded-geometry Predictor for Energy-efficient Mobile GPUs,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139188301&doi=10.1145%2f3527861&partnerID=40&md5=ff25695e03c230ba4580c6e0f7712068,"This article proposes a novel micro-architecture approach for mobile GPUs aimed at early removing the occluded geometry in a scene by leveraging frame-to-frame coherence, thus reducing the overall energy consumption. Mobile GPUs commonly implement a Tile-Based Rendering (TBR) architecture that differentiates two main phases: the Geometry Pipeline, where all the geometry of a scene is processed; and the Raster Pipeline, where primitives are rendered in a framebuffer. After the Geometry Pipeline, only non-culled primitives inside the camera's frustum are stored into the Parameter Buffer, a data structure stored in DRAM. However, among the non-culled primitives there is a significant amount that are rendered but non-visible at all, resulting in useless computations. On average, 60% of those primitives are completely occluded in our benchmarks. Despite TBR architectures use on-chip caches for the Parameter Buffer, about 46% of the DRAM traffic still comes from accesses to such buffer. The proposed Triangle Dropping technique leverages the visibility information computed along the Raster Pipeline to predict the primitives' visibility in the next frame to early discard those that will be totally occluded, drastically reducing Parameter Buffer accesses. On average, our approach achieves overall 14.5% energy savings, 28.2% energy-delay product savings, and a speedup of 20.2%.  © 2022 Association for Computing Machinery.",energy efficiency; graphics processors; hidden line/surface removal; Micro-architecture; mobile devices,Computer architecture; Energy utilization; Geometry; Pipeline processing systems; Pipelines; Program processors; Rasterization; Visibility; Energy efficient; Energy-consumption; Frame-to-frame coherence; Framebuffer; Graphic processors; Hidden line/surface removal; Micro architectures; On-chip cache; Surface removal; Tile-based rendering; Energy efficiency
Memory-Aware Functional IR for Higher-Level Synthesis of Accelerators,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127739550&doi=10.1145%2f3501768&partnerID=40&md5=9d0f2d1b5d2d8a3bbaa4fb0df8104476,"Specialized accelerators deliver orders of a magnitude of higher performance than general-purpose processors. The ever-changing nature of modern workloads is pushing the adoption of Field Programmable Gate Arrays (FPGAs) as the substrate of choice. However, FPGAs are hard to program directly using Hardware Description Languages (HDLs). Even modern high-level HDLs, e.g., Spatial and Chisel, still require hardware expertise.This article adopts functional programming concepts to provide a hardware-agnostic higher-level programming abstraction. During synthesis, these abstractions are mechanically lowered into a functional Intermediate Representation (IR) that defines a specific hardware design point. This novel IR expresses different forms of parallelism and standard memory features such as asynchronous off-chip memories or synchronous on-chip buffers. Exposing such features at the IR level is essential for achieving high performance.The viability of this approach is demonstrated on two stencil computations and by exploring the optimization space of matrix-matrix multiplication. Starting from a high-level representation for these algorithms, our compiler produces low-level VHSIC Hardware Description Language (VHDL) code automatically. Several design points are evaluated on an Intel Arria 10 FPGA, demonstrating the ability of the IR to exploit different hardware features. This article also shows that the designs produced are competitive with highly tuned OpenCL implementations and outperform hardware-agnostic OpenCL code. © 2022 Copyright held by the owner/author(s).",accelerators; compilers; functional IR; High-level synthesis,Abstracting; Computer hardware description languages; Functional programming; General purpose computers; High level synthesis; Integrated circuit design; Matrix algebra; Program compilers; Design points; Functional intermediate representation; General purpose processors; High-level programming; High-level synthesis; Intermediate representations; Memory aware; Performance; Programming abstractions; Programming concepts; Field programmable gate arrays (FPGA)
"The Forward Slice Core: A High-Performance, Yet Low-Complexity Microarchitecture",2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127753121&doi=10.1145%2f3499424&partnerID=40&md5=2b8972f0936a2c567f3faaa995542339,"Superscalar out-of-order cores deliver high performance at the cost of increased complexity and power budget. In-order cores, in contrast, are less complex and have a smaller power budget, but offer low performance. A processor architecture should ideally provide high performance in a power- A nd cost-efficient manner. Recently proposed slice-out-of-order (sOoO) cores identify backward slices of memory operations which they execute out-of-order with respect to the rest of the dynamic instruction stream for increased instruction-level and memory-hierarchy parallelism. Unfortunately, constructing backward slices is imprecise and hardware-inefficient, leaving performance on the table.In this article, we propose Forward Slice Core (FSC), a novel core microarchitecture that builds on a stall-on-use in-order core and extracts more instruction-level and memory-hierarchy parallelism than slice-out-of-order cores. FSC does so by identifying and steering forward slices (rather than backward slices) to dedicated in-order FIFO queues. Moreover, FSC puts load-consumers that depend on L1 D-cache misses on the side to enable younger independent load-consumers to execute faster. Finally, FSC eliminates the need for dynamic memory disambiguation by replicating store-address instructions across queues. Considering 3-wide pipeline configurations, we find that FSC improves performance by 27.1%, 21.1%, and 14.6% on average compared to Freeway, the state-of-the-art sOoO core, across SPEC CPU2017, GAP, and DaCapo, respectively, while at the same time incurring reduced hardware complexity. Compared to an OoO core, FSC reduces power consumption by 61.3% and chip area by 47%, providing a microarchitecture with high performance at low complexity. © 2022 Association for Computing Machinery.",dynamic instruction scheduling; slice-out-of-order; Superscalar microarchitecture,Budget control; Dynamic instruction scheduling; Instruction-level; Lower complexity; Micro architectures; Out of order; Performance; Power budgets; Slice-out-of-order; Superscalar; Superscalar microarchitecture; Memory architecture
A Pressure-Aware Policy for Contention Minimization on Multicore Systems,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139257493&doi=10.1145%2f3524616&partnerID=40&md5=5a668cfecc1b889ad286d0051f687132,"Modern Chip Multiprocessors (CMPs) are integrating an increasing amount of cores to address the continually growing demand for high-application performance. The cores of a CMP share several components of the memory hierarchy, such as Last-Level Cache (LLC) and main memory. This allows for considerable gains in multithreaded applications while also helping to maintain architectural simplicity. However, sharing resources can also result in performance bottleneck due to contention among concurrently executing applications. In this work, we formulate a fine-grained application characterization methodology that leverages Performance Monitoring Counters (PMCs) and Cache Monitoring Technology (CMT) in Intel processors. We utilize this characterization methodology to develop two contention-aware scheduling policies, one static and one dynamic, that co-schedule applications based on their resource-interference profiles. Our approach focuses on minimizing contention on both the main-memory bandwidth and the LLC by monitoring the pressure that each application inflicts on these resources. We achieve performance benefits for diverse workloads, outperforming Linux and three state-of-the-art contention-aware schedulers in terms of system throughput and fairness for both single and multithreaded workloads. Compared with Linux, our policy achieves up to 16% greater throughput for single-threaded and up to 40% greater throughput for multithreaded applications. Additionally, the policies increase fairness by up to 65% for single-threaded and up to 130% for multithreaded ones.  © 2022 Association for Computing Machinery.",Chip multiprocessors; contention-aware scheduling; fairness; Linux; pressure minimization; throughput,Cache memory; Linux; Multiprocessing systems; Chip Multiprocessor; Contention-aware; Contention-aware scheduling; Fairness; Last-level caches; Main-memory; Minimisation; Multi- threaded applications; Multithreaded; Pressure minimization; Scheduling
Weaving Synchronous Reactions into the Fabric of SSA-form Compilers,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127744390&doi=10.1145%2f3506706&partnerID=40&md5=3cd410275d7e19272e0340c686a6178d,"We investigate the programming of reactive systems combining closed-loop control with performance-intensive components such as Machine Learning (ML). Reactive control systems are often safety-critical and associated with real-time execution requirements, a domain of predilection for synchronous programming languages. Extending the high levels of assurance found in reactive control systems to computationally intensive code remains an open issue. We tackle it by unifying concepts and algorithms from synchronous languages with abstractions commonly found in general-purpose and ML compilers. This unification across embedded and high-performance computing enables a high degree of reuse of compiler abstractions and code. We first recall commonalities between dataflow synchronous languages and the static single assignment (SSA) form of general-purpose/ML compilers. We highlight the key mechanisms of synchronous languages that SSA does not cover-denotational concepts such as synchronizing computations with an external time base, cyclic and reactive I/O, as well as the operational notions of relaxing control flow dominance and the modeling of absent values. We discover that initialization-related static analyses and code generation aspects can be fully decoupled from other aspects of synchronous semantics such as memory management and causality analysis, the latter being covered by existing dominance-based algorithms of SSA-form compilers. We show how the SSA form can be seamlessly extended to enable all SSA-based transformations and optimizations on reactive programs with synchronous concurrency. We derive a compilation flow suitable for both high-performance and reactive aspects of a control application, by embedding the Lustre dataflow synchronous language into the SSA-based MLIR/LLVM compiler infrastructure. This allows the modeling of signal processing and deep neural network inference in the (closed) loop of feedback-directed control systems. With only minor efforts leveraging the MLIR infrastructure, the generated code matches or outperforms state-of-the-art synchronous language compilers on computationally intensive ML applications. © 2022 Association for Computing Machinery.",lustre; machine learning (ML); MLIR; reactive programming; SSA,Control systems; Deep neural networks; Interactive computer systems; Modeling languages; Real time systems; Safety engineering; Semantics; Signal processing; Static analysis; Luster; Machine learning; Machine learning compilers; MLIR; Performance; Reactive control; Reactive programming; Static single assignment form; Static single assignments; Synchronous languages; Program compilers
Performance and Power Prediction for Concurrent Execution on GPUs,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134329999&doi=10.1145%2f3522712&partnerID=40&md5=ba702297b93807dc4a00c2357ea5eea0,"The unprecedented growth of edge computing and 5G has led to an increased offloading of mobile applications to cloud servers or edge cloudlets.1 The most prominent workloads comprise computer vision applications. Conventional wisdom suggests that computer vision workloads perform significantly well on SIMD/SIMT architectures such as GPUs owing to the dominance of linear algebra kernels in their composition. In this work, we debunk this popular belief by performing a lot of experiments with the concurrent execution of these workloads, which is the most popular pattern in which these workloads are executed on cloud servers. We show that the performance of these applications on GPUs does not scale well with an increase in the number of concurrent applications primarily because of contention at the shared resources and lack of efficient virtualization techniques for GPUs. Hence, there is a need to accurately predict the performance and power of such ensemble workloads on a GPU. Sadly, most of the prior work in the area of performance/power prediction is for only a single application. To the best of our knowledge, we propose the first machine learning-based predictor to predict the performance and power of an ensemble of applications on a GPU. In this article, we show that by using the execution statistics of stand-alone workloads and the fairness of execution when these workloads are executed with three representative microbenchmarks, we can get a reasonably accurate prediction. This is the first such work in the direction of performance and power prediction for concurrent applications that does not rely on the features extracted from concurrent executions or GPU profiling data. Our predictors achieve an accuracy of 91% and 96% in estimating the performance and power of executing two applications concurrently, respectively. We also demonstrate a method to extend our models to four or five concurrently running applications on modern GPUs. © 2022 Association for Computing Machinery.",concurrent execution; GPUs; machine learning; Power and performance prediction,5G mobile communication systems; computation offloading; Computer vision; Computing power; Forecasting; Graphics processing unit; Linear algebra; Program processors; Cloud servers; Computer vision applications; Concurrent execution; Edge computing; Machine-learning; Mobile applications; Performance; Performance prediction; Power; Power predictions; Machine learning
SIMD-Matcher: A SIMD-based Arbitrary Matching Framework,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139262345&doi=10.1145%2f3514246&partnerID=40&md5=9cc6a57501891bfec8d58ad8ad18efd5,"Packet classification methods rely upon matching packet content/header against pre-defined rules, which are generated by network applications and their configurations. With the rapid development of network technology and the fast-growing network applications, users seek more enhanced, secure, and diverse network services. Hence it becomes critical to improve the performance of arbitrary matching operations. This article presents SIMD-Matcher, an efficient Single Instruction Multiple Data (SIMD) and cache-friendly arbitrary matching framework. To further improve the arbitrary matching performance, SIMD-Matcher adopts a trie node with a fixed high fanout and a varying span for each node depending on the data distribution. The trie node layout leverages cache and modern processor features such as SIMD instructions. To support arbitrary matching, we first interpret arbitrary rules into three fields: value, mask, and priority. Second, to support insertion of randomly positioned wildcards to arbitrary rules, we propose the SIMD-Matcher extraction algorithm to process the wildcard bits. Third, we add an array of wildcard entries to the leaf entries, which store the wildcard rules and guarantee the correctness of matching results. Experiments show that SIMD-Matcher outperforms GenMatcher under large-scale ruleset and key set, in terms of search time, insert time, and memory cost. Specifically with 5M rules, our method achieves a 2.7X speedup on search time, and the insertion time takes seconds, gaining a 1.38X speedup; meanwhile, the memory cost reduction is up to 6.17X. Copyright © 2022 held by the owner/author(s). Publication rights licensed to ACM.",Arbitrary matching; performance trade-off; SIMD optimization,Economic and social effects; Arbitrary matching; Data optimization; Matchings; Memory cost; Multiple data; Network applications; Packet classification; Performance tradeoff; Search time; Single instruction multiple data optimization; Cost reduction
An FPGA Overlay for CNN Inference with Fine-grained Flexible Parallelism,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139196119&doi=10.1145%2f3519598&partnerID=40&md5=e7a8d602a1452bace353753f299949ff,"Increasingly, pre-trained convolutional neural networks (CNNs) are being deployed for inference in various computer vision applications, both on the server-side in the data centers and at the edge. CNN inference is a very compute-intensive task. It is a challenge to meet performance metrics such as latency and throughput while optimizing power. Special-purpose ASICs and FPGAs are suitable candidates to meet these power and performance budgets simultaneously. Rapidly evolving CNN architectures involve novel convolution operations such as point convolutions, depth separable convolutions, and so on. This leads to substantial variation in the computational structure across CNNs and layers within a CNN. Because of this, FPGA reconfigurability provides an attractive tradeoff compared to ASICs. FPGA-based hardware designers address the structural variability issue by generating a network-specific accelerator for a single network or a class of networks. However, homogeneous accelerators are network agnostic and often sacrifice throughput and FPGA LUTs for flexibility. In this article, we propose an FPGA overlay for efficient processing of CNNs that can be scaled based on the available compute and memory resources of the FPGA. The overlay is configured on the fly through control words sent by the host on a per-layer basis. Unlike current overlays, our architecture exploits all forms of parallelism inside a convolution operation. A constraint system is employed at the host end to find out the per-layer configuration of the overlay that uses all forms of parallelism in the processing of the layer, resulting in the highest throughput for that layer. We studied the effectiveness of our overlay by using it to process AlexNet, VGG16, YOLO, MobileNet, and ResNet-50 CNNs targeting a Virtex7 and a bigger Ultrascale+VU9P FPGAs. The chosen CNNs have a mix of different types of convolution layers and filter sizes, presenting a good variation in model size and structure. Our accelerator reported a maximum throughput of 1,200 GOps/second on the Virtex7, an improvement of 1.2 to 5 over the recent designs. Also, the reported performance density, measured in giga operations per second per KLUT, is 1.3× to 4× improvement over existing works. Similar speed-up and performance density is also observed for the Ultrascale+VU9P FPGA. © 2022 Association for Computing Machinery.",accelerators; convolutional neural networks; FPGAs,Application specific integrated circuits; Budget control; Convolution; Convolutional neural networks; Network architecture; Compute-intensive tasks; Computer vision applications; Convolutional neural network; Datacenter; Fine grained; Network inference; Performance metrices; Power; Power budgets; Server sides; Field programmable gate arrays (FPGA)
MUA-Router: Maximizing the Utility-of-Allocation for On-chip Pipelining Routers,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139244143&doi=10.1145%2f3519027&partnerID=40&md5=1dd96eb7fc02a188382a15d914c57686,"As an important pipeline stage in the router of Network-on-Chips, switch allocation assigns output ports to input ports and allows flits to transit through the switch without conflicts. Previous work designed efficient switch allocation strategies by maximizing the matching efficiency in time series. However, those works neglected the interaction between different router pipeline stages. In this article, we propose the concept of Utility-of-Allocation (UoA) to indicate the quality of allocation to be practically used in on-chip routers. We demonstrate that router pipelines can interact with each other, and the UoA can be maximized if the interaction between router pipelines is taken into consideration. Based on these observations, a novel class of routers, MUA-Router, is proposed to maximize the UoA through the collaborative design (co-design) between router pipelines. MUA-Router achieves this goal in two ways and accordingly implements two novel instance router architectures. In the first, MUA-Router improves the UoA by mitigating the impact of endpoint congestion in the switch allocation, and thus Eca-Router is proposed. Eca-Router achieves an endpoint-congestion-aware switch allocation through the co-design between routing computation and switch allocation. Based on Eca-Router, CoD-Router is proposed to feed back switch allocation information to routing computation stage to provide switch allocator with more conflict-free requests. Through the co-design between pipelines, MUA-Router significantly improves the efficiency of switch allocation and the performance of the entire network. Evaluation results show that our design can achieve significant performance improvement with moderate overheads. © 2022 Association for Computing Machinery.",co-design; endpoint congestion; Network on Chip; switch allocation,Computer architecture; Efficiency; Network architecture; Pipelines; Routers; Servers; Traffic congestion; Collaborative design; Endpoint congestion; Networks on chips; On chips; On-chip switches; Performance; Pipeline stages; Router pipeline; Routings; Switch allocations; Network-on-chip
An FPGA-based Approach to Evaluate Thermal and Resource Management Strategies of Many-core Processors,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139179207&doi=10.1145%2f3516825&partnerID=40&md5=e91a17c87b2672bacb4321a87fbc9923,"The continuous technology scaling of integrated circuits results in increasingly higher power densities and operating temperatures. Hence, modern many-core processors require sophisticated thermal and resource management strategies to mitigate these undesirable side effects. A simulation-based evaluation of these strategies is limited by the accuracy of the underlying processor model and the simulation speed. Therefore, we present, for the first time, an field-programmable gate array (FPGA)-based evaluation approach to test and compare thermal and resource management strategies using the combination of benchmark generation, FPGA-based application-specific integrated circuit (ASIC) emulation, and run-time monitoring. The proposed benchmark generation method enables an evaluation of run-time management strategies for applications with various run-time characteristics. Furthermore, the ASIC emulation platform features a novel distributed temperature emulator design, whose overhead scales linearly with the number of integrated cores, and a novel dynamic voltage frequency scaling emulator design, which precisely models the timing and energy overhead of voltage and frequency transitions. In our evaluations, we demonstrate the proposed approach for a tiled many-core processor with 80 cores on four Virtex-7 FPGAs. Additionally, we present the suitability of the platform to evaluate state-of-the-art run-time management techniques with a case study. © 2022 Association for Computing Machinery.",ASIC emulation; many-core processor; thermal management,Application specific integrated circuits; Benchmarking; Dynamic frequency scaling; Integrated circuit design; Natural resources management; Resource allocation; Temperature control; Voltage scaling; Application-specific integrated circuit emulation; Application-specific integrated circuits; Circuit emulation; Field programmables; Management strategies; Many-core processors; Programmable gate array; Resource management; Runtime management; Thermal; Field programmable gate arrays (FPGA)
Object Intersection Captures on Interactive Apps to Drive a Crowd-sourced Replay-based Compiler Optimization,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139179049&doi=10.1145%2f3517338&partnerID=40&md5=65628e4d1ca1c050db22096acd38ae57,"Traditional offline optimization frameworks rely on representative hardware, software, and inputs to compare different optimizations on. With application-specific optimization for mobile systems though, the idea of a representative testbench is unrealistic while creating offline inputs is non-trivial. Online approaches partially overcome these problems but they might expose users to suboptimal or even erroneous code. Therefore, our mobile code is poorly optimized, resulting in wasted performance and energy and user frustration. In this article, we introduce a novel compiler optimization approach designed for mobile applications. It requires no developer effort, it tunes applications for the user's device and usage patterns, and it has no negative impact on the user experience. It is based on a lightweight capture and replay mechanism. Our previous work [46] captures the state accessed by any targeted code region during its online stage. By repurposing existing OS capabilities, it keeps the overhead low. In its offline stage, it replays the code region but under different optimization decisions to enable sound comparisons of different optimizations under realistic conditions. In this article, we propose a technique that further decreases the storage sizes without any additional overhead. It captures only the intersection of reachable objects and accessed heap pages. We compare this with another new approach that has minimal runtime overheads at the cost of higher capture sizes. Coupled with a search heuristic for the compiler optimization space, our capture and replay mechanism allows us to discover optimization decisions that improve performance without testing these decisions directly on the user. Finally, with crowd-sourcing we split this offline evaluation effort between several users, allowing us to discover better code in less time. We implemented a prototype system in Android based on LLVM combined with a genetic search engine and a crowd-sourcing architecture. We evaluated it on both benchmarks and real Android applications. Online captures are infrequent and introduce ∼5 ms or 15 ms on average, depending on the approach used. For this negligible effect on user experience, we achieve speedups of 44% on average over the Android compiler and 35% over LLVM -O3. Our collaborative search is just 5% short of that speedup, which is impressive given the acceleration gains. The user with the highest workload concluded the search 7× faster. © 2022 Association for Computing Machinery.",capture; interactive; Iterative compilation; replay,Android (operating system); Benchmarking; Crowdsourcing; Heuristic algorithms; Program compilers; Search engines; Capture; Compiler optimizations; Crowd sourcing; Interactive; Iterative compilation; Off-line optimization; Offline; Optimisations; Replay; Users' experiences; Genetic algorithms
Task-RM: A Resource Manager for Energy Reduction in Task-Parallel Applications under Quality of Service Constraints,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127161515&doi=10.1145%2f3494537&partnerID=40&md5=a8657bdf5d5993907b11e91d1ef7bc6e,"Improving energy efficiency is an important goal of computer system design. This article focuses on a general model of task-parallel applications under quality-of-service requirements on the completion time. Our technique, called Task-RM, exploits the variance in task execution-times and imbalance between tasks to allocate just enough resources in terms of voltage-frequency and core-allocation so that the application completes before the deadline. Moreover, we provide a solution that can harness additional energy savings with the availability of additional processors. We observe that, for the proposed run-time resource manager to allocate resources, it requires specification of the soft deadlines to the tasks. This is accomplished by analyzing the energy-saving scenarios offline and by providing Task-RM with the performance requirements of the tasks. The evaluation shows an energy saving of 33% compared to race-to-idle and 22% compared to dynamic slack allocation (DSA) with an overhead of less than 1%.  © 2022 Association for Computing Machinery.",DVFS; Dynamic resource allocation; Energy efficiency; Heterogeneous multi-core architectures; Precedence constraint task parallel programs; Quality of service; run-time systems,Application programs; Energy efficiency; Managers; Memory architecture; Parallel architectures; Resource allocation; DVFS; Dynamic resource allocations; Heterogeneous multi-core architecture; Heterogeneous multicore; Multicore architectures; Parallel program; Precedence constraint task parallel program; Precedence constraints; Quality-of-service; Run- time systems; Task parallel; Quality of service
CASHT: Contention Analysis in Shared Hierarchies with Thefts,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127101624&doi=10.1145%2f3494538&partnerID=40&md5=0a206c2a2daa60195f5859f4f8388384,"Cache management policies should consider workloads' contention behavior when managing a shared cache. Prior art makes estimates about shared cache behavior by adding extra logic or time to isolate per workload cache statistics. These approaches provide per-workload analysis but do not provide a holistic understanding of the utilization and effectiveness of caches under the ever-growing contention that comes standard with scaling cores. We present Contention Analysis in Shared Hierarchies using Thefts, or CASHT,1 a framework for capturing cache contention information both offline and online. CASHT takes advantage of cache statistics made richer by observing a consequence of cache contention: inter-core evictions, or what we call THEFTS. We use thefts to complement more familiar cache statistics to train a learning model based on Gradient-boosting Trees (GBT) to predict the best ways to partition the last-level cache. GBT achieves 90+% accuracy with trained models as small as 100 B and at least 95% accuracy at 1 kB model size when predicting the best way to partition two workloads. CASHT employs a novel run-time framework for collecting thefts-based metrics despite partition intervention, and enables per-access sampling rather than set sampling that could add overhead but may not capture true workload behavior. Coupling CASHT and GBT for use as a dynamic policy results in a very lightweight and dynamic partitioning scheme that performs within a margin of error of Utility-based Cache Partitioning at a 1/8 the overhead.  © 2022 held by the owner/author(s). Publication rights licensed to ACM.",Cache partitioning; Gradient-boosting trees; Last-level cache,Boosting trees; Cache behavior; Cache management policies; Cache partitioning; Gradient boosting; Gradient-boosting tree; Last-level caches; Prior arts; Shared cache; Workload analysis; Forestry
E-BATCH: Energy-Efficient and High-Throughput RNN Batching,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127089986&doi=10.1145%2f3499757&partnerID=40&md5=aff15e2be6e530453ba6f1ca18a9c6ea,"Recurrent Neural Network (RNN) inference exhibits low hardware utilization due to the strict data dependencies across time-steps. Batching multiple requests can increase throughput. However, RNN batching requires a large amount of padding since the batched input sequences may vastly differ in length. Schemes that dynamically update the batch every few time-steps avoid padding. However, they require executing different RNN layers in a short time span, decreasing energy efficiency. Hence, we propose E-BATCH, a low-latency and energy-efficient batching scheme tailored to RNN accelerators. It consists of a runtime system and effective hardware support. The runtime concatenates multiple sequences to create large batches, resulting in substantial energy savings. Furthermore, the accelerator notifies it when the evaluation of an input sequence is done. Hence, a new input sequence can be immediately added to a batch, thus largely reducing the amount of padding. E-BATCH dynamically controls the number of time-steps evaluated per batch to achieve the best trade-off between latency and energy efficiency for the given hardware platform. We evaluate E-BATCH on top of E-PUR and TPU. E-BATCH improves throughput by 1.8× and energy efficiency by 3.6× in E-PUR, whereas in TPU, it improves throughput by 2.1× and energy efficiency by 1.6×, over the state-of-the-art.  © 2022 Association for Computing Machinery.",Batching; Hardware accelerators; Long short term memory; Recurrent neural network,Computer hardware; Economic and social effects; Energy efficiency; Across time; Batching; Data dependencies; Energy efficient; Hardware accelerators; Hardware utilization; High-throughput; Input sequence; Network inference; Time step; Recurrent neural networks
CARL: Compiler Assigned Reference Leasing,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127167620&doi=10.1145%2f3498730&partnerID=40&md5=daa0e14025eba4290d4cef0b43a9da48,"Data movement is a common performance bottleneck, and its chief remedy is caching. Traditional cache management is transparent to the workload: data that should be kept in cache are determined by the recency information only, while the program information, i.e., future data reuses, is not communicated to the cache. This has changed in a new cache design named Lease Cache. The program control is passed to the lease cache by a compiler technique called Compiler Assigned Reference Lease (CARL). This technique collects the reuse interval distribution for each reference and uses it to compute and assign the lease value to each reference.In this article, we prove that CARL is optimal under certain statistical assumptions. Based on this optimality, we prove miss curve convexity, which is useful for optimizing shared cache, and sub-partitioning monotonicity, which simplifies lease compilation. We evaluate the potential using scientific kernels from PolyBench and show that compiler insertions of up to 34 leases in program code achieve similar or better cache utilization (in variable size cache) than the optimal fixed-size caching policy, which has been unattainable with automatic caching but now within the potential of cache programming for all tested programs and most cache sizes.  © 2022 Association for Computing Machinery.",Cache management; Cache replacement policy; Lease cache; Miss ratio curve; Optimality; Reuse interval distribution,Cache memory; Optimal systems; Software testing; Cache management; Cache replacement policy; Data movements; Lease cache; Management IS; Miss ratio curves; Optimality; Performance bottlenecks; Reuse; Reuse interval distribution; Program compilers
HeapCheck: Low-cost Hardware Support for Memory Safety,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127196408&doi=10.1145%2f3495152&partnerID=40&md5=5046fbb53402552092d9e0f8ffc2982b,"Programs written in C/C++ are vulnerable to memory-safety errors like buffer-overflows and use-after-free. While several mechanisms to detect such errors have been previously proposed, they suffer from a variety of drawbacks, including poor performance, imprecise or probabilistic detection of errors, or requiring invasive changes to the ISA, binary-layout, or source-code that results in compatibility issues. As a result, memory-safety errors continue to be hard to detect and a principal cause of security problems.In this work, we present a minimally invasive and low-cost hardware-based memory-safety checking framework for detecting out-of-bounds accesses and use-after-free errors. The key idea of our mechanism is to re-purpose some of the ""unused bits""in a pointer in 64-bit architectures to store an index into a bounds information table that can be used to catch out-bounds errors and use-after-free errors without any change to the binary layout. Using this memory-safety checking framework, we enable HeapCheck, a design for detecting Out-of-bounds and Use-after-free accesses for heap-objects, that are responsible for the majority of memory-safety errors in the wild. Our evaluations using C/C++ SPEC CPU 2017 workloads on Gem5 show that our solution incurs 1.5% slowdown on average, using an 8 KB on-chip SRAM cache for caching bounds-information. Our mechanism allows detection of out-of-bounds errors in user-code as well as in unmodified shared-library functions. Our mechanism has detected out-of-bounds accesses in 87 lines of code in the SPEC CPU 2017 benchmarks, primarily in Glibc v2.27 functions, that, to our knowledge, have not been previously detected even with popular tools like Address Sanitizer.  © 2022 Association for Computing Machinery.",Hardware bounds checking; Memory safety; Software security,C++ (programming language); Cache memory; Codes (symbols); Costs; Errors; Buffer overflows; Hardware bound checking; Hardware supports; Low cost hardware; Memory safety; Poor performance; Probabilistic detections; Safety errors; Software security; Source codes; Static random access storage
GPU Domain Specialization via Composable On-Package Architecture,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127109006&doi=10.1145%2f3484505&partnerID=40&md5=e76f9fd041374a28f3bd972abea5a5cd,"As GPUs scale their low-precision matrix math throughput to boost deep learning (DL) performance, they upset the balance between math throughput and memory system capabilities. We demonstrate that a converged GPU design trying to address diverging architectural requirements between FP32 (or larger)-based HPC and FP16 (or smaller)-based DL workloads results in sub-optimal configurations for either of the application domains. We argue that a Composable On-PAckage GPU (COPA-GPU) architecture to provide domain-specialized GPU products is the most practical solution to these diverging requirements. A COPA-GPU leverages multi-chip-module disaggregation to support maximal design reuse, along with memory system specialization per application domain. We show how a COPA-GPU enables DL-specialized products by modular augmentation of the baseline GPU architecture with up to 4× higher off-die bandwidth, 32× larger on-package cache, and 2.3× higher DRAM bandwidth and capacity, while conveniently supporting scaled-down HPC-oriented designs. This work explores the microarchitectural design necessary to enable composable GPUs and evaluates the benefits composability can provide to HPC, DL training, and DL inference. We show that when compared to a converged GPU design, a DL-optimized COPA-GPU featuring a combination of 16× larger cache capacity and 1.6× higher DRAM bandwidth scales per-GPU training and inference performance by 31% and 35%, respectively, and reduces the number of GPU instances by 50% in scale-out training scenarios.  © 2021 Association for Computing Machinery.",GPU computing; Multi-chip module,Bandwidth; Deep learning; Dynamic random access storage; Integrated circuit design; Memory architecture; Program processors; Applications domains; Composable; GPU computing; Learning performance; Lower precision; Memory systems; Multi chip modules; Precision matrix; Specialisation; System capabilities; Graphics processing unit
Locality-Aware CTA Scheduling for Gaming Applications,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127128439&doi=10.1145%2f3477497&partnerID=40&md5=8be5eb0a4d22a8678992f24d433d071d,"The compute work rasterizer or the GigaThread Engine of a modern NVIDIA GPU focuses on maximizing compute work occupancy across all streaming multiprocessors in a GPU while retaining design simplicity. In this article, we identify the operational aspects of the GigaThread Engine that help it meet those goals but also lead to less-than-ideal cache locality for texture accesses in 2D compute shaders, which are an important optimization target for gaming applications. We develop three software techniques, namely LargeCTAs, Swizzle, and Agents, to show that it is possible to effectively exploit the texture data working set overlap intrinsic to 2D compute shaders.We evaluate these techniques on gaming applications across two generations of NVIDIA GPUs, RTX 2080 and RTX 3080, and find that they are effective on both GPUs. We find that the bandwidth savings from all our software techniques on RTX 2080 is much higher than the bandwidth savings on baseline execution from inter-generational cache capacity increase going from RTX 2080 to RTX 3080. Our best-performing technique, Agents, records up to a 4.7% average full-frame speedup by reducing bandwidth demand of targeted shaders at the L1-L2 and L2-DRAM interfaces by 23% and 32%, respectively, on the latest generation RTX 3080. These results acutely highlight the sensitivity of cache locality to compute work rasterization order and the importance of locality-aware cooperative thread array scheduling for gaming applications.  © 2021 held by the owner/author(s). Publication rights licensed to ACM.",Cache locality; CTA scheduling; GPUs; Shader programs,Application programs; Bandwidth; Engines; Program processors; Rasterization; Software agents; Textures; Bandwidth savings; Cache locality; CTA scheduling; Gaming applications; Locality aware; Operational aspects; Optimisations; Shader program; Software techniques; Streaming multiprocessors; Scheduling
Optimizing Small-Sample Disk Fault Detection Based on LSTM-GAN Model,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127117823&doi=10.1145%2f3500917&partnerID=40&md5=1e433b0dd37b70e052875a9ae11aa169,"In recent years, researches on disk fault detection based on SMART data combined with different machine learning algorithms have been proven to be effective. However, these methods require a large amount of data. In the early stages of the establishment of a data center or the deployment of new storage devices, the amount of reliability data for disks is relatively limited, and the amount of failed disk data is even less, resulting in the unsatisfactory detection performances of machine learning algorithms.To solve the above problems, we propose a novel small sample disk fault detection (SSDFD)1 optimizing method based on Generative Adversarial Networks (GANs). Combined with the characteristics of hard disk reliability data, the generator of the original GAN is improved based on Long Short-Term Memory (LSTM), making it suitable for the generation of failed disk data. To alleviate the problem of data imbalance and expand the failed disk dataset with reduced amounts of original data, the proposed model is trained through adversarial training, which focuses on the generation of failed disk data. Experimental results on real HDD datasets show that SSDFD can generate enough virtual failed disk data to enable the machine learning algorithm to detect disk faults with increased accuracy under the condition of a few original failed disk data. Furthermore, the model trained with 300 original failed disk data has a significant effect on improving the accuracy of HDD fault detection. The optimal amount of generated virtual data are, 20-30 times that of the original data.  © 2022 Association for Computing Machinery.",Deep learning; Fault detection; Generative adversarial networks; Hard disk drives; Reliability,Generative adversarial networks; Hard disk storage; Learning algorithms; Long short-term memory; Reliability; Virtual storage; Deep learning; Disk data; Faults detection; Hard Disk Drive; Large amounts of data; Machine learning algorithms; Network models; Reliability data; Small samples; SMART datum; Fault detection
SecNVM: An Efficient and Write-Friendly Metadata Crash Consistency Scheme for Secure NVM,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127159595&doi=10.1145%2f3488724&partnerID=40&md5=ed26d4bbc3be7de7c6fbc7cf3d55cc8a,"Data security is an indispensable part of non-volatile memory (NVM) systems. However, implementing data security efficiently on NVM is challenging, since we have to guarantee the consistency of user data and the related security metadata. Existing consistency schemes ignore the recoverability of the SGX style integrity tree (SIT) and the access correlation between metadata blocks, thereby generating unnecessary NVM write traffic. In this article, we propose SecNVM, an efficient and write-friendly metadata crash consistency scheme for secure NVM. SecNVM utilizes the observation that for a lazily updated SIT, the lost tree nodes after a crash can be recovered by the corresponding child nodes in NVM. It reduces the SIT persistency overhead through a restrained write-back metadata cache and exploits the SIT inter-layer dependency for recovery. Next, leveraging the strong access correlation between the counter and DMAC, SecNVM improves the efficiency of security metadata access through a novel collaborative counter-DMAC scheme. In addition, it adopts a lightweight address tracker to reduce the cost of address tracking for fast recovery. Experiments show that compared to the state-of-the-art schemes, SecNVM improves the performance and decreases write traffic a lot, and achieves an acceptable recovery time.  © 2021 Association for Computing Machinery.",Consistency; Metadata; Non-volatile memory; Security,Cost reduction; Forestry; Nonvolatile storage; Recovery; Child node; Consistency; Fast recovery; Integrity tree; Inter-layer dependency; Recoverability; Security; Tree nodes; User data; Write-back; Metadata
TLB-pilot: Mitigating TLB Contention Attack on GPUs with Microarchitecture-Aware Scheduling,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127088454&doi=10.1145%2f3491218&partnerID=40&md5=1da8e08143c0c3067f6447485050e40a,"Co-running GPU kernels on a single GPU can provide high system throughput and improve hardware utilization, but this raises concerns on application security. We reveal that translation lookaside buffer (TLB) attack, one of the common attacks on CPU, can happen on GPU when multiple GPU kernels co-run. We investigate conditions or principles under which a TLB attack can take effect, including the awareness of GPU TLB microarchitecture, being lightweight, and bypassing existing software and hardware mechanisms. This TLB-based attack can be leveraged to conduct Denial-of-Service (or Degradation-of-Service) attacks. Furthermore, we propose a solution to mitigate TLB attacks. In particular, based on the microarchitecture properties of GPU, we introduce a software-based system, TLB-pilot, that binds thread blocks of different kernels to different groups of streaming multiprocessors by considering hardware isolation of last-level TLBs and the application's resource requirement. TLB-pilot employs lightweight online profiling to collect kernel information before kernel launches. By coordinating software-and hardware-based scheduling and employing a kernel splitting scheme to reduce load imbalance, TLB-pilot effectively mitigates TLB attacks. The result shows that when under TLB attack, TLB-pilot mitigates the attack and provides on average 56.2% and 60.6% improvement in average normalized turnaround times and overall system throughput, respectively, compared to the traditional Multi-Process Service based co-running solution. When under TLB attack, TLB-pilot also provides up to 47.3% and 64.3% improvement (41% and 42.9% on average) in average normalized turnaround times and overall system throughput, respectively, compared to a state-of-the-art co-running solution for efficiently scheduling of thread blocks.  © 2021 Association for Computing Machinery.",CUDA; GPU; High performance; TLB contention,Application programs; Computer architecture; Computer hardware; Denial-of-service attack; Graphics processing unit; Program processors; CUDA; Hardware utilization; High performance; Micro architectures; Performance; Software and hardwares; System throughput; Translation lookaside buffer; Translation lookaside buffer contention; Turn-around time; Scheduling
Joint Program and Layout Transformations to Enable Convolutional Operators on Specialized Hardware Based on Constraint Programming,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127089381&doi=10.1145%2f3487922&partnerID=40&md5=73eefdc0d7f016eccc8d3c31647a6341,"The success of Deep Artificial Neural Networks (DNNs) in many domains created a rich body of research concerned with hardware accelerators for compute-intensive DNN operators. However, implementing such operators efficiently with complex hardware intrinsics such as matrix multiply is a task not yet automated gracefully. Solving this task often requires joint program and data layout transformations. First solutions to this problem have been proposed, such as TVM, UNIT, or ISAMIR, which work on a loop-level representation of operators and specify data layout and possible program transformations before the embedding into the operator is performed. This top-down approach creates a tension between exploration range and search space complexity, especially when also exploring data layout transformations such as im2col, channel packing, or padding.In this work, we propose a new approach to this problem. We created a bottom-up method that allows the joint transformation of both computation and data layout based on the found embedding. By formulating the embedding as a constraint satisfaction problem over the scalar dataflow, every possible embedding solution is contained in the search space. Adding additional constraints and optimization targets to the solver generates the subset of preferable solutions.An evaluation using the VTA hardware accelerator with the Baidu DeepBench inference benchmark shows that our approach can automatically generate code competitive to reference implementations. Further, we show that dynamically determining the data layout based on intrinsic and workload is beneficial for hardware utilization and performance. In cases where the reference implementation has low hardware utilization due to its fixed deployment strategy, we achieve a geomean speedup of up to × 2.813, while individual operators can improve as much as × 170.  © 2021 held by the owner/author(s). Publication rights licensed to ACM.",Instruction selection; Intermediate representation; Neural networks; Tensor computations,Carbonation; Complex networks; Constraint satisfaction problems; Constraint theory; Convolution; Embeddings; Metadata; Data layout transformations; Data layouts; Embeddings; Hardware accelerators; Instruction selection; Intermediate representations; Joint programs; Neural-networks; Search spaces; Tensor computation; Neural networks
Marvel: A Data-Centric Approach for Mapping Deep Learning Operators on Spatial Accelerators,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127090371&doi=10.1145%2f3485137&partnerID=40&md5=bf183ccef8ad69d59580b920cdc2c26f,"A spatial accelerator's efficiency depends heavily on both its mapper and cost models to generate optimized mappings for various operators of DNN models. However, existing cost models lack a formal boundary over their input programs (operators) for accurate and tractable cost analysis of the mappings, and this results in adaptability challenges to the cost models for new operators. We consider the recently introduced Maestro Data-Centric (MDC) notation and its analytical cost model to address this challenge because any mapping expressed in the notation is precisely analyzable using the MDC's cost model.In this article, we characterize the set of input operators and their mappings expressed in the MDC notation by introducing a set of conformability rules. The outcome of these rules is that any loop nest that is perfectly nested with affine tensor subscripts and without conditionals is conformable to the MDC notation. A majority of the primitive operators in deep learning are such loop nests. In addition, our rules enable us to automatically translate a mapping expressed in the loop nest form to MDC notation and use the MDC's cost model to guide upstream mappers. Our conformability rules over the input operators result in a structured mapping space of the operators, which enables us to introduce a mapper based on our decoupled off-chip/on-chip approach to accelerate mapping space exploration. Our mapper decomposes the original higher-dimensional mapping space of operators into two lower-dimensional off-chip and on-chip subspaces and then optimizes the off-chip subspace followed by the on-chip subspace. We implemented our overall approach in a tool called Marvel, and a benefit of our approach is that it applies to any operator conformable with the MDC notation. We evaluated Marvel over major DNN operators and compared it with past optimizers.  © 2021 Association for Computing Machinery.",Compilers; Deep learning (spatial) accelerators; Mappers/optimizers,Cost benefit analysis; Deep learning; Space research; Cost models; Data centric; Data-centric approaches; Deep learning (spatial) accelerator; Learning operator; Loop nests; Mapper/optimizer; Off-chip; On chips; Optimizers; Mapping
ReuseTracker: Fast Yet Accurate Multicore Reuse Distance Analyzer,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127188216&doi=10.1145%2f3484199&partnerID=40&md5=01e4ae38ba0cb3e39114e3259e681344,"One widely used metric that measures data locality is reuse distance-the number of unique memory locations that are accessed between two consecutive accesses to a particular memory location. State-of-the-art techniques that measure reuse distance in parallel applications rely on simulators or binary instrumentation tools that incur large performance and memory overheads. Moreover, the existing sampling-based tools are limited to measuring reuse distances of a single thread and discard interactions among threads in multi-threaded programs. In this work, we propose ReuseTracker-a fast and accurate reuse distance analyzer that leverages existing hardware features in commodity CPUs. ReuseTracker is designed for multi-threaded programs and takes cache-coherence effects into account. By utilizing hardware features like performance monitoring units and debug registers, ReuseTracker can accurately profile reuse distance in parallel applications with much lower overheads than existing tools. It introduces only 2.9× runtime and 2.8× memory overheads. Our tool achieves 92% accuracy when verified against a newly developed configurable benchmark that can generate a variety of different reuse distance patterns. We demonstrate the tool's functionality with two use-case scenarios using PARSEC, Rodinia, and Synchrobench benchmark suites where ReuseTracker guides code refactoring in these benchmarks by detecting spatial reuses in shared caches that are also false sharing and successfully predicts whether some benchmarks in these suites can benefit from adjacent cache line prefetch optimization.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Address sampling; Debug registers; Hardware performance counters; Reuse distance,Computer software reusability; Program debugging; Program processors; Address sampling; Debug registers; Hardware features; Hardware performance counters; Memory locations; Memory overheads; Multi-cores; Multi-threaded programs; Parallel application; Reuse distance; Cache memory
Iterative Compilation Optimization Based on Metric Learning and Collaborative Filtering,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127096242&doi=10.1145%2f3480250&partnerID=40&md5=ce1c6cdba75ed905999f79fbd3c3aff5,"Pass selection and phase ordering are two critical compiler auto-tuning problems. Traditional heuristic methods cannot effectively address these NP-hard problems especially given the increasing number of compiler passes and diverse hardware architectures. Recent research efforts have attempted to address these problems through machine learning. However, the large search space of candidate pass sequences, the large numbers of redundant and irrelevant features, and the lack of training program instances make it difficult to learn models well. Several methods have tried to use expert knowledge to simplify the problems, such as using only the compiler passes or subsequences in the standard levels (e.g.,-O1,-O2, and-O3) provided by compiler designers. However, these methods ignore other useful compiler passes that are not contained in the standard levels. Principal component analysis (PCA) and exploratory factor analysis (EFA) have been utilized to reduce the redundancy of feature data. However, these unsupervised methods retain all the information irrelevant to the performance of compilation optimization, which may mislead the subsequent model learning.To solve these problems, we propose a compiler pass selection and phase ordering approach, called Iterative Compilation based on Metric learning and Collaborative filtering (ICMC). First, we propose a data-driven method to construct pass subsequences according to the observed collaborative interactions and dependency among passes on a given program set. Therefore, we can make use of all available compiler passes and prune the search space. Then, a supervised metric learning method is utilized to retain useful feature information for compilation optimization while removing both the irrelevant and the redundant information. Based on the learned similarity metric, a neighborhood-based collaborative filtering method is employed to iteratively recommend a few superior compiler passes for each target program. Last, an iterative data enhancement method is designed to alleviate the problem of lacking training program instances and to enhance the performance of iterative pass recommendations. The experimental results using the LLVM compiler on all 32 cBench programs show the following: (1) ICMC significantly outperforms several state-of-the-art compiler phase ordering methods, (2) it performs the same or better than the standard level-O3 on all the test programs, and (3) it can reach an average performance speedup of 1.20 (up to 1.46) compared with the standard level-O3.  © 2021 Association for Computing Machinery.",Collaborative filtering; Compiler auto-tuning; Iterative compilation; Metric learning,Collaborative filtering; Computational complexity; Curricula; Factor analysis; Heuristic methods; Iterative methods; Learning systems; Optimization; Principal component analysis; Autotuning; Compiler auto-tuning; Iterative compilation; Metric learning; Optimisations; Performance; Phase Ordering; Search spaces; Training program; Tuning problems; Program compilers
SMT-Based Contention-Free Task Mapping and Scheduling on 2D/3D SMART NoC with Mixed Dimension-Order Routing,2022,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127142126&doi=10.1145%2f3487018&partnerID=40&md5=dc09bbc8da74241080a432c2c74326ae,"SMART NoCs achieve ultra-low latency by enabling single-cycle multiple-hop transmission via bypass channels. However, contention along bypass channels can seriously degrade the performance of SMART NoCs by breaking the bypass paths. Therefore, contention-free task mapping and scheduling are essential for optimal system performance. In this article, we propose an SMT (Satisfiability Modulo Theories)-based framework to find optimal contention-free task mappings with minimum application schedule lengths on 2D/3D SMART NoCs with mixed dimension-order routing. On top of SMT's fast reasoning capability for conditional constraints, we develop efficient search-space reduction techniques to achieve practical scalability. Experiments demonstrate that our SMT framework achieves 10× higher scalability than ILP (Integer Linear Programming) with 931.1× (ranges from 2.2× to 1532.1×) and 1237.1× (ranges from 4× to 4373.8×) faster average runtimes for finding optimum solutions on 2D and 3D SMART NoCs and our 2D and 3D extensions of the SMT framework with mixed dimension-order routing also maintain the improved scalability with the extended and diversified routing paths, resulting in reduced application schedule lengths throughout various application benchmarks.  © 2021 Association for Computing Machinery.",Scheduling; SMART NoC; SMT; Task mapping,Benchmarking; Integer programming; Mapping; Scalability; Scheduling; Bypass channel; Contention-free; Dimension orderings; Low latency; Routings; Satisfiability modulo Theories; Schedule length; SMART NoC; Tasks mapping; Tasks scheduling; Network-on-chip
System-level Early-stage Modeling and Evaluation of IVR-Assisted Processor Power Delivery System,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116439519&doi=10.1145%2f3468145&partnerID=40&md5=dcd8a14cdacf9e84b5116a1fb94f0cc9,"Despite being employed in numerous efforts to improve power delivery efficiency, the integrated voltage regulator (IVR) approach has yet to be evaluated rigorously and quantitatively in a full power delivery system (PDS) setting. To fulfill this need, we present a system-level modeling and design space exploration framework called Ivory for IVR-Assisted power delivery systems. Using a novel modeling methodology, it can accurately estimate power delivery efficiency, static performance characteristics, and dynamic transient responses under different load variations and external voltage/frequency scaling conditions. We validate the model over a wide range of IVR topologies with silicon measurement and SPICE simulation. Finally, we present two case studies using architecture-level performance and power simulators. The first case study focuses on optimal PDS design for multi-core systems, which achieves 8.6% power efficiency improvement over conventional off-chip voltage regulator module-(VRM) based PDS. The second case study explores the design tradeoffs for IVR-Assisted PDSs in CPU and GPU systems with fast per-core dynamic voltage and frequency scaling (DVFS). We find 2 μs to be the optimal DVFS timescale, which not only reaps energy benefits (12.5% improvement in CPU and 50.0% improvement in GPU) but also avoids costly IVR overheads. © 2021 ACM.",,Computer aided design; Dynamic frequency scaling; Electric power transmission; Graphics processing unit; Integrated circuit design; SPICE; Topology; Transient analysis; Voltage scaling; Case-studies; Dynamic voltage and frequency scaling; Integrated voltage; Power delivery efficiencies; Power delivery systems; Processor power; Stage models; System levels; Systems-level design; Voltage regulator's; Voltage regulators
Spiking Neural Networks in Spintronic Computational RAM,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116432689&doi=10.1145%2f3475963&partnerID=40&md5=0624db34cfa0fa8aa86bed18baa4fcff,"Spiking Neural Networks (SNNs) represent a biologically inspired computation model capable of emulating neural computation in human brain and brain-like structures. The main promise is very low energy consumption. Classic Von Neumann architecture based SNN accelerators in hardware, however, often fall short of addressing demanding computation and data transfer requirements efficiently at scale. In this article, we propose a promising alternative to overcome scalability limitations, based on a network of in-memory SNN accelerators, which can reduce the energy consumption by up to 150.25= when compared to a representative ASIC solution. The significant reduction in energy comes from two key aspects of the hardware design to minimize data communication overheads: (1) each node represents an in-memory SNN accelerator based on a spintronic Computational RAM array, and (2) a novel, De Bruijn graph based architecture establishes the SNN array connectivity. © 2021 ACM.",computational random access memory; non-volatile memory; Processing in memory; spiking neural networks,Biomimetics; Data transfer; Energy utilization; Graphic methods; Memory architecture; Network architecture; Neural networks; Biologically-inspired; Computation model; Computational random access memory; Computational-RAM; Human brain; Neural computations; Neural-networks; Processing-in-memory; Random access memory; Spiking neural network; Random access storage
All-gather Algorithms Resilient to Imbalanced Process Arrival Patterns,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116423445&doi=10.1145%2f3460122&partnerID=40&md5=3f83fc7d49e5ff2f82177e361fdf6564,"Two novel algorithms for the all-gather operation resilient to imbalanced process arrival patterns (PATs) are presented. The first one, Background Disseminated Ring (BDR), is based on the regular parallel ring algorithm often supplied in MPI implementations and exploits an auxiliary background thread for early data exchange from faster processes to accelerate the performed all-gather operation. The other algorithm, Background Sorted Linear synchronized tree with Broadcast (BSLB), is built upon the already existing PAP-Aware gather algorithm, that is, Background Sorted Linear Synchronized tree (BSLS), followed by a regular broadcast distributing gathered data to all participating processes. The background of the imbalanced PAP subject is described, along with the PAP monitoring and evaluation topics. An experimental evaluation of the algorithms based on a proposed mini-benchmark is presented. The mini-benchmark was performed over 2,000 times in a typical HPC cluster architecture with homogeneous compute nodes. The obtained results are analyzed according to different PATs, data sizes, and process numbers, showing that the proposed optimization works well for various configurations, is scalable, and can significantly reduce the all-gather elapsed times, in our case, up to factor 1.9 or 47% in comparison with the best state-of-The-Art solution. © 2021 Owner/Author.",All-gather; background disseminated ring; background sorted linear synchronized tree with broadcast; MPI; process arrival pattern,Cluster computing; Electronic data interchange; Forestry; Trees (mathematics); All-gather; Arrival patterns; Background disseminated ring; Background sorted linear synchronized tree with broadcast; Fast process; MPI; Novel algorithm; Parallel rings; Process arrival pattern; Ring algorithms; Synchronization
WaFFLe: Gated Cache-Ways with Per-Core Fine-Grained DVFS for Reduced On-Chip Temperature and Leakage Consumption,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116477219&doi=10.1145%2f3471908&partnerID=40&md5=9d811c31fddaf488a417ee727847ee58,"Managing thermal imbalance in contemporary chip multi-processors (CMPs) is crucial in assuring functional correctness of modern mobile as well as server systems. Localized regions with high activity, e.g., register files, ALUs, FPUs, and so on, experience higher temperatures than the average across the chip and are commonly referred to as hotspots. Hotspots affect functional correctness of the underlying circuitry and a noticeable increase in leakage power, which in turn generates heat in a self-reinforced cycle. Techniques that reduce the severity of or completely eliminate hotspots can maintain functional correctness along with improving performance of CMPs. Conventional dynamic thermal management targets the cores to reduce hotspots but often ignores caches, which are known for their high leakage power consumption. This article presents WaFFLe, an approach that targets the leakage power of the last-level cache (LLC) and hotspots occurring at the cores. WaFFLe turns off LLC-ways to reduce leakage power and to generate on-chip thermal buffers. In addition, fine-grained DVFS is applied during long LLC miss induced stalls to reduce core temperature. Our results show that WaFFLe reduces peak and average temperature of a 16-core based homogeneous tiled CMP with up to 8.4 Ö¯ C and 6.2 Ö¯ C, respectively, with an average performance degradation of only 2.5 %. We also show that WaFFLe outperforms a state-of-The-Art cache-based technique and a greedy DVFS policy. © 2021 Owner/Author.",cache coherence; Cache-way shutdown; chip-mutiprocessors; fine grained DVFS; hotspots; leakage power; thermal efficiency,Multiprocessing systems; Thermal management (electronics); Cache Coherence; Cache-way shutdown; Chip-mutiprocessor; Fine grained; Fine grained DVFS; Functional correctness; Hotspots; Leakage power; Multi-processors; Thermal-efficiency; Temperature control
Towards Enhanced System Efficiency while Mitigating Row Hammer,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116442020&doi=10.1145%2f3458749&partnerID=40&md5=be9c3fef9934122850f6d8d4ec3c822f,"In recent years, DRAM-based main memories have become susceptible to the Row Hammer (RH) problem, which causes bits to flip in a row without accessing them directly. Frequent activation of a row, called an aggressor row, causes its adjacent rows' (victim) bits to flip. The state-of-The-Art solution is to refresh the victim rows explicitly to prevent bit flipping. There have been several proposals made to detect RH attacks. These include both probabilistic as well as deterministic counter-based methods. The technique of handling RH attacks, however, remains the same. In this work, we propose an efficient technique for handling the RH problem. We show that the mechanism is agnostic of the detection mechanism. Our RH handling technique omits the necessity of refreshing the victim rows. Instead, we use a small non-volatile Spin-Transfer Torque Magnetic Random Access Memory (STTRAM) that ensures no unnecessary refreshes of the victim rows on the DRAM device and thus allowing more time for normal applications in the same DRAM device. Our model relies on the migration of the aggressor rows. This accounts for removing blocking of the DRAM operations due to the refreshing of victim rows incurred in the previous solution. After extensive evaluation, we found that, compared to the conventional RH mitigation techniques, our model minimizes the blocking time of the memory that is imposed due to explicit refreshing by an average of 80.72% in the worst-case scenario and provides energy savings of about 15.82% on average, across different types of RH-based workloads. A lookup table is necessary to pinpoint the location of a particular row, which, when combined with the STTMRAM, limits the storage overhead to 0.39% of a 2 GB DRAM. Our proposed model prevents repeated refreshing of the same victim rows in different refreshing windows on the DRAM device and leads to an efficient RH handling technique. © 2021 ACM.",DRAM; memory cache; row hammer; STTMRAM,Cache memory; Energy conservation; Hammers; Table lookup; Bit flipping; DRAM devices; Handling technique; Main-memory; Memory cache; Probabilistics; Row hammer; State of the art; STT-MRAM; System efficiency; Dynamic random access storage
Device Hopping: Transparent Mid-Kernel Runtime Switching for Heterogeneous Systems,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116406640&doi=10.1145%2f3471909&partnerID=40&md5=26f98f2a1dc5b4013f8de706f1bde6d9,"Existing OS techniques for homogeneous many-core systems make it simple for single and multithreaded applications to migrate between cores. Heterogeneous systems do not benefit so fully from this flexibility, and applications that cannot migrate in mid-execution may lose potential performance. The situation is particularly challenging when a switch of language runtime would be desirable in conjunction with a migration. We present a case study in making heterogeneous CPU + GPU systems more flexible in this respect. Our technique for fine-grained application migration, allows switches between OpenMP, OpenCL, and CUDA execution, in conjunction with migrations from GPU to CPU, and CPU to GPU. To achieve this, we subdivide iteration spaces into slices, and consider migration on a slice-by-slice basis. We show that slice sizes can be learned offline by machine learning models. To further improve performance, memory transfers are made migration-Aware. The complexity of the migration capability is hidden from programmers behind a high-level programming model. We present a detailed evaluation of our mid-kernel migration mechanism with the First Come, First Served scheduling policy. We compare our technique in a focused evaluation scenario against idealized kernel-by-kernel scheduling, which is typical for current systems, and makes perfect kernel to device scheduling decisions, but cannot migrate kernels mid-execution. Models show that up to 1.33× speedup can be achieved over these systems by adding fine-grained migration. Our experimental results with all nine applicable SHOC and Rodinia benchmarks achieve speedups of up to 1.30× (1.08× on average) over an implementation of a perfect but kernel-migration incapable scheduler when migrated to a faster device. Our mechanism and slice size choices introduce an average slowdown of only 2.44% if kernels never migrate. Lastly, our programming model reduces the code size by at least 88% if compared to manual implementations of migratable kernels. © 2021 ACM.",heterogeneity; high-level parallel programming models; Migration,Application programming interfaces (API); Iterative methods; Parallel programming; Scheduling; Core systems; Fine grained; Heterogeneity; Heterogeneous systems; High-level parallel programming model; Many-core; Migration; Parallel-programming models; Runtimes; Simple++; Graphics processing unit
PICO: A Presburger In-bounds Check Optimization for Compiler-based Memory Safety Instrumentations,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116469739&doi=10.1145%2f3460434&partnerID=40&md5=7f5ee6123e4f681a767848efa2253366,"Memory safety violations such as buffer overflows are a threat to security to this day. A common solution to ensure memory safety for C is code instrumentation. However, this often causes high execution-Time overhead and is therefore rarely used in production. Static analyses can reduce this overhead by proving some memory accesses in bounds at compile time. In practice, however, static analyses may fail to verify in-bounds accesses due to over-Approximation. Therefore, it is important to additionally optimize the checks that reside in the program. In this article, we present PICO, an approach to eliminate and replace in-bounds checks. PICO exactly captures the spatial memory safety of accesses using Presburger formulas to either verify them statically or substitute existing checks with more efficient ones. Thereby, PICO can generate checks of which each covers multiple accesses and place them at infrequently executed locations. We evaluate our LLVM-based PICO prototype with the well-known SoftBound instrumentation on SPEC benchmarks commonly used in related work. PICO reduces the execution-Time overhead introduced by SoftBound by 36% on average (and the code-size overhead by 24%). Our evaluation shows that the impact of substituting checks dominates that of removing provably redundant checks. © 2021 Owner/Author.",C language; LLVM; Optimization; Presburger; spatial memory safety,C (programming language); Codes (symbols); Cost reduction; Bounds check; Buffer overflows; C language; LLVM; Memory safety; Optimisations; Presburger; Safety violations; Spatial memory; Spatial memory safety; Static analysis
Low I/O Intensity-Aware Partial GC Scheduling to Reduce Long-Tail Latency in SSDs,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116443088&doi=10.1145%2f3460433&partnerID=40&md5=3c2679321263497f97aaf9f2b2f11267,"This article proposes a low I/O intensity-Aware scheduling scheme on garbage collection (GC) in SSDs for minimizing the I/O long-Tail latency to ensure I/O responsiveness. The basic idea is to assemble partial GC operations by referring to several determinable factors (e.g., I/O characteristics) and dispatch them to be processed together in idle time slots of I/O processing. To this end, it first makes use of Fourier transform to explore the time slots having relative sparse I/O requests for conducting time-consuming GC operations, as the number of affected I/O requests can be limited. After that, it constructs a mathematical model to further figure out the types and quantities of partial GC operations, which are supposed to be dealt with in the explored idle time slots, by taking the factors of I/O intensity, read/write ratio, and the SSD use state into consideration. Through a series of simulation experiments based on several realistic disk traces, we illustrate that the proposed GC scheduling mechanism can noticeably reduce the long-Tail latency by between 5.5% and 232.3% at the 99.99th percentile, in contrast to state-of-The-Art methods. © 2021 ACM.",garbage collection (GC); I/O characteristics; I/O responsiveness; partial GC operations; SSDs,Scheduling; Garbage collection; I/O characteristic; I/O responsiveness; Idle time; Long tail; Partial garbage collection operation; SSD; Timeslots; Refuse collection
Low-precision Logarithmic Number Systems: Beyond Base-2,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116440152&doi=10.1145%2f3461699&partnerID=40&md5=f4fc5c7e0fadb4da940fb45983692566,"Logarithmic number systems (LNS) are used to represent real numbers in many applications using a constant base raised to a fixed-point exponent making its distribution exponential. This greatly simplifies hardware multiply, divide, and square root. LNS with base-2 is most common, but in this article, we show that for low-precision LNS the choice of base has a significant impact. We make four main contributions. First, LNS is not closed under addition and subtraction, so the result is approximate. We show that choosing a suitable base can manipulate the distribution to reduce the average error. Second, we show that low-precision LNS addition and subtraction can be implemented efficiently in logic rather than commonly used ROM lookup tables, the complexity of which can be reduced by an appropriate choice of base. A similar effect is shown where the result of arithmetic has greater precision than the input. Third, where input data from external sources is not expected to be in LNS, we can reduce the conversion error by selecting a LNS base to match the expected distribution of the input. Thus, there is no one base that gives the global optimum, and base selection is a trade-off between different factors. Fourth, we show that circuits realized in LNS require lower area and power consumption for short word lengths. © 2021 ACM.",digital arithmetic; Logarithmic number system; quantization error,Economic and social effects; Errors; Fixed point arithmetic; Numbering systems; Table lookup; Average errors; Exponentials; External sources; Fixed points; Input datas; Logarithmic number system; Lower precision; Quantization errors; Real number; Square-root; Number theory
Scenario-Aware Program Specialization for Timing Predictability,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116398597&doi=10.1145%2f3473333&partnerID=40&md5=92d1d15f3947a67176cd95211e236b32,"The successful application of static program analysis strongly depends on flow facts of a program such as loop bounds, control-flow constraints, and operating modes. This problem heavily affects the design of real-Time systems, since static program analyses are a prerequisite to determine the timing behavior of a program. For example, this becomes obvious in worst-case execution time (WCET) analysis, which is often infeasible without user-Annotated flow facts. Moreover, many timing simulation approaches use statically derived timings of partial program paths to reduce simulation overhead. Annotating flow facts on binary or source level is either error-prone and tedious, or requires specialized compilers that can transform source-level annotations along with the program during optimization. To overcome these obstacles, so-called scenarios can be used. Scenarios are a design-Time methodology that describe a set of possible system parameters, such as image resolutions, operating modes, or application-dependent flow facts. The information described by a scenario is unknown in general but known and constant for a specific system. In this article,1 we present a methodology for scenario-Aware program specialization to improve timing predictability. Moreover, we provide an implementation of this methodology for embedded software written in C/C++. We show the effectiveness of our approach by evaluating its impact on WCET analysis using almost all of TACLeBench-Achieving an average reduction of WCET of 31%. In addition, we provide a thorough qualitative and evaluation-based comparison to closely related work, as well as two case studies. © 2021 Owner/Author.",analyzability; Predictability; timing analysis; worst-case execution time,Application programs; C++ (programming language); Image resolution; Interactive computer systems; Petroleum reservoir evaluation; Program compilers; Real time systems; Analyzability; On flow; Operating modes; Predictability; Program specialization; Source level; Static program analysis; Timing Analysis; Worst-case execution time; Worst-case execution-time analysis; Timing circuits
SortCache: Intelligent Cache Management for Accelerating Sparse DataWorkloads,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116421699&doi=10.1145%2f3473332&partnerID=40&md5=ca50b206d3e606717397570a71b9ff61,"Sparse data applications have irregular access patterns that stymie modern memory architectures. Although hyper-sparse workloads have received considerable attention in the past, moderately-sparse workloads prevalent in machine learning applications, graph processing and HPC have not. Where the former can bypass the cache hierarchy, the latter fit in the cache. This article makes the observation that intelligent, near-processor cache management can improve bandwidth utilization for data-irregular accesses, thereby accelerating moderately-sparse workloads. We propose SortCache, a processor-centric approach to accelerating sparse workloads by introducing accelerators that leverage the on-chip cache subsystem, with minimal programmer intervention. © 2021 Owner/Author.",graph analytics; intelligent cache; pruned convolutions; Sparse matrix; spgemm,Graph theory; Access patterns; Cache management; Data application; Graph-analytic; Intelligent cache; Machine learning applications; Pruned convolution; Sparse data; Sparse matrices; Spgemm; Memory architecture
Configurable Multi-directional Systolic Array Architecture for Convolutional Neural Networks,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116416777&doi=10.1145%2f3460776&partnerID=40&md5=58b6dc8c7dcdd2b8307986b9aeead7ba,"The systolic array architecture is one of the most popular choices for convolutional neural network hardware accelerators. The biggest advantage of the systolic array architecture is its simple and efficient design principle. Without complicated control and dataflow, hardware accelerators with the systolic array can calculate traditional convolution very efficiently. However, this advantage also brings new challenges to the systolic array. When computing special types of convolution, such as the small-scale convolution or depthwise convolution, the processing element (PE) utilization rate of the array decreases sharply. The main reason is that the simple architecture design limits the flexibility of the systolic array. In this article, we design a configurable multi-directional systolic array (CMSA) to address these issues. First, we added a data path to the systolic array. It allows users to split the systolic array through configuration to speed up the calculation of small-scale convolution. Second, we redesigned the PE unit so that the array has multiple data transmission modes and dataflow strategies. This allows users to switch the dataflow of the PE array to speed up the calculation of depthwise convolution. In addition, unlike other works, we only make a few changes and modifications to the existing systolic array architecture. It avoids additional hardware overheads and can be easily deployed in application scenarios that require small systolic arrays such as mobile terminals. Based on our evaluation, CMSA can increase the PE utilization rate by up to 1.6 times compared to the typical systolic array when running the last layers of ResNet-18. When running depthwise convolution in MobileNet, CMSA can increase the utilization rate by up to 14.8 times. At the same time, CMSA and the traditional systolic arrays are similar in area and energy consumption. © 2021 Owner/Author.",convolutional neural network; hardware accelerator; Systolic array,Convolution; Convolutional neural networks; Energy utilization; Network architecture; Convolutional neural network; Dataflow; Hardware accelerators; Neural network hardware; Processing elements; Simple++; Small scale; Speed up; Systolic array architecture; Utilization rates; Systolic arrays
Domain-Specific Multi-Level IR Rewriting for GPU: The Open Earth Compiler for GPU-Accelerated Climate Simulation,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116466281&doi=10.1145%2f3469030&partnerID=40&md5=9807f62c6ebde9fffb99cc6ea621b534,"Most compilers have a single core intermediate representation (IR) (e.g., LLVM) sometimes complemented with vaguely defined IR-like data structures. This IR is commonly low-level and close to machine instructions. As a result, optimizations relying on domain-specific information are either not possible or require complex analysis to recover the missing information. In contrast, multi-level rewriting instantiates a hierarchy of dialects (IRs), lowers programs level-by-level, and performs code transformations at the most suitable level. We demonstrate the effectiveness of this approach for the weather and climate domain. In particular, we develop a prototype compiler and design stencil-and GPU-specific dialects based on a set of newly introduced design principles. We find that two domain-specific optimizations (500 lines of code) realized on top of LLVM's extensible MLIR compiler infrastructure suffice to outperform state-of-The-Art solutions. In essence, multi-level rewriting promises to herald the age of specialized compilers composed from domain-and target-specific dialects implemented on top of a shared infrastructure. © 2021 Owner/Author.",intermediate representations; stencil computations; Weather and climate,Codes (symbols); Cosine transforms; Earth (planet); Graphics processing unit; Climate simulation; Complex analysis; Domain specific; Domain-specific information; GPU-accelerated; Intermediate representations; Machine instructions; Multilevels; Optimisations; Stencil computations; Program compilers
CIB-HIER: Centralized Input Buffer Design in Hierarchical High-radix Routers,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116444084&doi=10.1145%2f3468062&partnerID=40&md5=9d648c7ea7acbe3936cb9a411abee073,"Hierarchical organization is widely used in high-radix routers to enable efficient scaling to higher switch port count. A general-purpose hierarchical router must be symmetrically designed with the same input buffer depth, resulting in a large amount of unused input buffers due to the different link lengths. Sharing input buffers between different input ports can improve buffer utilization, but the implementation overhead also increases with the number of shared ports. Previous work allowed input buffers to be shared among all router ports, which maximizes the buffer utilization but also introduces higher implementation complexity. Moreover, such design can impair performance when faced with long packets, due to the head-of-line blocking in intermediate buffers. In this work, we explain that sharing unused buffers between a subset of router ports is a more efficient design. Based on this observation, we propose Centralized Input Buffer Design in Hierarchical High-radix Routers (CIB-HIER), a novel centralized input buffer design for hierarchical high-radix routers. CIB-HIER integrates multiple input ports onto a single tile and organizes all unused input buffers in the tile as a centralized input buffer. CIB-HIER only allows the centralized input buffer to be shared between ports on the same tile, without introducing additional intermediate virtual channels or global scheduling circuits. Going beyond the basic design of CIB-HIER, the centralized input buffer can be used to relieve the head-of-line blocking caused by shallow intermediate buffers, by stashing long packets in the centralized input buffer. Experimental results show that CIB-HIER is highly effective and can significantly increase the throughput of high-radix routers. © 2021 ACM.",centralized input buffer; hierarchical organization; High-radix router,Buffer Design; Buffer utilization; Centralised; Centralized input buffer; Head-of-line blocking; Hierarchical organizations; High radix; High-radix router; Input buffers; Input port; Routers
LargeGraph: An Efficient Dependency-Aware GPU-Accelerated Large-Scale Graph Processing,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116421046&doi=10.1145%2f3477603&partnerID=40&md5=7d7ced7fae53fd68f8b0f8813591236d,"Many out-of-GPU-memory systems are recently designed to support iterative processing of large-scale graphs. However, these systems still suffer from long time to converge because of inefficient propagation of active vertices' new states along graph paths. To efficiently support out-of-GPU-memory graph processing, this work designs a system LargeGraph. Different from existing out-of-GPU-memory systems, LargeGraph proposes a dependency-Aware data-driven execution approach, which can significantly accelerate active vertices' state propagations along graph paths with low data access cost and also high parallelism. Specifically, according to the dependencies between the vertices, it only loads and processes the graph data associated with dependency chains originated from active vertices for smaller access cost. Because most active vertices frequently use a small evolving set of paths for their new states' propagation because of power-law property, this small set of paths are dynamically identified and maintained and efficiently handled on the GPU to accelerate most propagations for faster convergence, whereas the remaining graph data are handled over the CPU. For out-of-GPU-memory graph processing, LargeGraph outperforms four cutting-edge systems: Totem (5.19-11.62×), Graphie (3.02-9.41×), Garaph (2.75-8.36×), and Subway (2.45-4.15×). © 2021 Association for Computing Machinery. All rights reserved.",access cost; GPU; Graph processing; out-of-GPU-memory,Crack propagation; Cutting; Graph theory; Access cost; GPU-accelerated; Graph data; Graph path; Graph processing; Iterative processing; Large-scales; Memory systems; Out-of-GPU-memory; Work design; Graphics processing unit
Byte-Select Compression,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116494784&doi=10.1145%2f3462209&partnerID=40&md5=e5e5cb5a0c9235481e0878a6bb526983,"Cache-block compression is a highly effective technique for both reducing accesses to lower levels in the memory hierarchy (cache compression) and minimizing data transfers (link compression). While many effective cache-block compression algorithms have been proposed, the design of these algorithms is largely ad hoc and manual and relies on human recognition of patterns. In this article, we take an entirely different approach. We introduce a class of ""byte-select""compression algorithms, as well as an automated methodology for generating compression algorithms in this class. We argue that, based on upper bounds within the class, the study of this class of byte-select algorithms has potential to yield algorithms with better performance than existing cache-block compression algorithms. The upper bound we establish on the compression ratio is 2X that of any existing algorithm. We then offer a generalized representation of a subset of byte-select compression algorithms and search through the resulting space guided by a set of training data traces. Using this automated process, we find efficient and effective algorithms for various hardware applications. We find that the resulting algorithms exploit novel patterns that can inform future algorithm designs. The generated byte-select algorithms are evaluated against a separate set of traces and evaluations show that Byte-Select has a 23% higher compression ratio on average. While no previous algorithm performs best for all our data sets which include CPU and GPU applications, our generated algorithms do. Using an automated hardware generator for these algorithms, we show that their decompression and compression latency is one and two cycles respectively, much lower than any existing algorithm with a competitive compression ratio. © 2021 ACM.",Byte-select; cache compression; memory compression,Cache memory; Data transfer; Pattern recognition; Byte-select; Cache blocks; Cache compressions; Compression algorithms; Human recognition; Memory compression; Performance; SELECT algorithms; Training data; Upper Bound; Automation
Gem5-X: A Many-core Heterogeneous Simulation Platform for Architectural Exploration and Optimization,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116408901&doi=10.1145%2f3461662&partnerID=40&md5=ef021ce450faef78325790c4b1c8c7cd,"The increasing adoption of smart systems in our daily life has led to the development of new applications with varying performance and energy constraints, and suitable computing architectures need to be developed for these new applications. In this article, we present gem5-X, a system-level simulation framework, based on gem-5, for architectural exploration of heterogeneous many-core systems. To demonstrate the capabilities of gem5-X, real-Time video analytics is used as a case-study. It is composed of two kernels, namely, video encoding and image classification using convolutional neural networks (CNNs). First, we explore through gem5-X the benefits of latest 3D high bandwidth memory (HBM2) in different architectural configurations. Then, using a two-step exploration methodology, we develop a new optimized clustered-heterogeneous architecture with HBM2 in gem5-X for video analytics application. In this proposed clustered-heterogeneous architecture, ARMv8 in-order cluster with in-cache computing engine executes the video encoding kernel, giving 20% performance and 54% energy benefits compared to baseline ARM in-order and Out-of-Order systems, respectively. Furthermore, thanks to gem5-X, we conclude that ARM Out-of-Order clusters with HBM2 are the best choice to run visual recognition using CNNs, as they outperform DDR4-based system by up to 30% both in terms of performance and energy savings. © 2021 ACM.",architectural exploration; cluster; gem5; HBM; heterogeneous architectures; in-cache; Many-core,ARM processors; Cluster computing; Computer architecture; Convolutional neural networks; Network architecture; Signal encoding; Simulation platform; Video signal processing; Architectural exploration; Cluster; Gem5; HBM; Heterogeneous architectures; In-cache; Many-core; New applications; Video analytics; Video encodings; Energy conservation
GraphAttack: Optimizing Data Supply for Graph Applications on In-Order Multicore Architectures,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116436874&doi=10.1145%2f3469846&partnerID=40&md5=60d309903408c39523f27c94e416702e,"Graph structures are a natural representation of important and pervasive data. While graph applications have significant parallelism, their characteristic pointer indirect loads to neighbor data hinder scalability to large datasets on multicore systems. A scalable and efficient system must tolerate latency while leveraging data parallelism across millions of vertices. Modern Out-of-Order (OoO) cores inherently tolerate a fraction of long latencies, but become clogged when running severely memory-bound applications. Combined with large power/area footprints, this limits their parallel scaling potential and, consequently, the gains that existing software frameworks can achieve. Conversely, accelerator and memory hierarchy designs provide performant hardware specializations, but cannot support diverse application demands. To address these shortcomings, we present GraphAttack, a hardware-software data supply approach that accelerates graph applications on in-order multicore architectures. GraphAttack proposes compiler passes to (1) identify idiomatic long-latency loads and (2) slice programs along these loads into data Producer/ Consumer threads to map onto pairs of parallel cores. Each pair shares a communication queue; the Producer asynchronously issues long-latency loads, whose results are buffered in the queue and used by the Consumer. This scheme drastically increases memory-level parallelism (MLP) to mitigate latency bottlenecks. In equal-Area comparisons, GraphAttack outperforms OoO cores, do-All parallelism, prefetching, and prior decoupling approaches, achieving a 2.87× speedup and 8.61× gain in energy efficiency across a range of graph applications. These improvements scale; GraphAttack achieves a 3× speedup over 64 parallel cores. Lastly, it has pragmatic design principles; it enhances in-order architectures that are gaining increasing open-source support. © 2021 ACM.",Graph analytics; hardware-software co-design; parallelism,Application programs; Hardware-software codesign; Large dataset; Memory architecture; Open source software; Program compilers; Software architecture; Graph-analytic; Hardware/software codesign; Large datasets; Long-latency loads; Multi-core systems; Multicore architectures; Natural representation; Out of order; Parallel cores; Parallelism; Energy efficiency
Monolithically Integrating Non-Volatile Main Memory over the Last-Level Cache,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116469411&doi=10.1145%2f3462632&partnerID=40&md5=f6aef4fd799e0c14d67b8dc2f1b56a14,"Many emerging non-volatile memories are compatible with CMOS logic, potentially enabling their integration into a CPU's die. This article investigates such monolithically integrated CPU-main memory chips. We exploit non-volatile memories employing 3D crosspoint subarrays, such as resistive RAM (ReRAM), and integrate them over the CPU's last-level cache (LLC). The regular structure of cache arrays enables co-design of the LLC and ReRAM main memory for area efficiency. We also develop a streamlined LLC/main memory interface that employs a single shared internal interconnect for both the cache and main memory arrays, and uses a unified controller to service both LLC and main memory requests. We apply our monolithic design ideas to a many-core CPU by integrating 3D ReRAM over each core's LLC slice. We find that co-design of the LLC and ReRAM saves 27% of the total LLC-main memory area at the expense of slight increases in delay and energy. The streamlined LLC/main memory interface saves an additional 12% in area. Our simulation results show monolithic integration of CPU and main memory improves performance by 5.3× and 1.7× over HBM2 DRAM for several graph and streaming kernels, respectively. It also reduces the memory system's energy by 6.0× and 1.7×, respectively. Moreover, we show that the area savings of co-design permits the CPU to have 23% more cores and main memory, and that streamlining the LLC/main memory interface incurs a small 4% performance penalty. © 2021 ACM.",Crosspoint architectures; on-die main memory systems; ReRAM,Cache memory; Dynamic random access storage; Integrated circuit design; Nonvolatile storage; RRAM; CMOS logic; Co-designs; Crosspoint architecture; Last-level caches; Main-memory; Memory interface; Memory systems; Monolithically integrated; Non-volatile main memory; On-die main memory system; Memory architecture
SLO-Aware Inference Scheduler for Heterogeneous Processors in Edge Platforms,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116413818&doi=10.1145%2f3460352&partnerID=40&md5=23fb82bf9eaccc528c5bdb97f3340c31,"With the proliferation of applications with machine learning (ML), the importance of edge platforms has been growing to process streaming sensor, data locally without resorting to remote servers. Such edge platforms are commonly equipped with heterogeneous computing processors such as GPU, DSP, and other accelerators, but their computational and energy budget are severely constrained compared to the data center servers. However, as an edge platform must perform the processing of multiple machine learning models concurrently for multimodal sensor data, its scheduling problem poses a new challenge to map heterogeneous machine learning computation to heterogeneous computing processors. Furthermore, processing of each input must provide a certain level of bounded response latency, making the scheduling decision critical for the edge platform. This article proposes a set of new heterogeneity-Aware ML inference scheduling policies for edge platforms. Based on the regularity of computation in common ML tasks, the scheduler uses the pre-profiled behavior of each ML model and routes requests to the most appropriate processors. It also aims to satisfy the service-level objective (SLO) requirement while reducing the energy consumption for each request. For such SLO supports, the challenge of ML computation on GPUs and DSP is its inflexible preemption capability. To avoid the delay caused by a long task, the proposed scheduler decomposes a large ML task to sub-Tasks by its layer in the DNN model. © 2021 ACM.",Edge computing; heterogeneous computing; inference; machine learning; task scheduling,Budget control; Energy utilization; Green computing; Machine learning; Program processors; Scheduling; Computational budget; Computing processors; Edge computing; Heterogeneous computing; Heterogeneous processors; Inference; Machine learning models; Remote servers; Sensors data; Service level objective; Edge computing
Acceleration of Parallel-Blocked QR Decomposition of Tall-and-Skinny Matrices on FPGAs,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108904196&doi=10.1145%2f3447775&partnerID=40&md5=36becc0c65da8d13bdafa58f1c770757,"QR decomposition is one of the most useful factorization kernels in modern numerical linear algebra algorithms. In particular, the decomposition of tall-and-skinny matrices (TSMs) has major applications in areas including scientific computing, machine learning, image processing, wireless networks, and numerical methods. Traditionally, CPUs and GPUs have achieved better throughput on these applications by using large cache hierarchies and compute cores running at a high frequency, leading to high power consumption. With the advent of heterogeneous platforms, however, FPGAs are emerging as a promising viable alternative. In this work, we propose a high-throughput FPGA-based engine that has a very high computational efficiency (ratio of achieved to peak throughput) compared to similar QR solvers running on FPGAs. Although comparable QR solvers achieve an efficiency of 36%, our design exhibits an efficiency of 54%. For TSMs, our experimental results show that our design can outperform highly optimized QR solvers running on CPUs and GPUs. For TSMs with more than 50K rows, our design outperforms the Intel MKL solver running on an Intel quad-core processor by a factor of 1.5×. For TSMs containing 256 columns or less, our design outperforms the NVIDIA CUBLAS solver running on a K40 GPU by a factor of 3.0×. In addition to being fast, our design is energy efficient - competing platforms execute up to 0.6 GFLOPS/Joule, whereas our design executes more than 1.0 GFLOPS/Joule.  © 2021 ACM.",accelerators; FPGA; QR decomposition; reconfigurable computing,Computational efficiency; Energy efficiency; Field programmable gate arrays (FPGA); Image processing; Matrix algebra; Numerical methods; Program processors; Core processors; Energy efficient; Heterogeneous platforms; High frequency HF; High power consumption; High throughput; Numerical Linear Algebra; Q R decomposition; Integrated circuit design
MC-DeF: Creating Customized CGRAs for Dataflow Applications,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108906653&doi=10.1145%2f3447970&partnerID=40&md5=715161cd1e5d8f9453dd2b6c50cce442,"Executing complex scientific applications on Coarse-Grain Reconfigurable Arrays (CGRAs) promises improvements in execution time and/or energy consumption compared to optimized software implementations or even fully customized hardware solutions. Typical CGRA architectures contain of multiple instances of the same compute module that consist of simple and general hardware units such as ALUs, simple processors. However, generality in the cell contents, while convenient for serving a wide variety of applications, penalizes performance and energy efficiency. To that end, a few proposed CGRAs use custom logic tailored to a particular application's specific characteristics in the compute module. This approach, while much more efficient, restricts the versatility of the array. To date, versatility at hardware speeds is only supported with Field programmable gate arrays (FPGAs), that are reconfigurable at a very fine grain. This work proposes MC-DeF, a novel Mixed-CGRA Definition Framework targeting a Mixed-CGRA architecture that leverages the advantages of CGRAs by utilizing a customized cell array, and those of FPGAs by incorporating a separate LUT array used for adaptability. The framework presented aims to develop a complete CGRA architecture. First, a cell structure and functionality definition phase creates highly customized application/domain specific CGRA cells. Then, mapping and routing phases define the CGRA connectivity and cell-LUT array transactions. Finally, an energy and area estimation phase presents the user with area occupancy and energy consumption estimations of the final design. MC-DeF uses novel algorithms and cost functions driven by user defined metrics, threshold values, and area/energy restrictions. The benefits of our framework, besides creating fast and efficient CGRA designs, include design space exploration capabilities offered to the user. The validity of the presented framework is demonstrated by evaluating and creating CGRA designs of nine applications. Additionally, we provide comparisons of MC-DeF with state-of-the-art related works, and show that MC-DeF offers competitive performance (in terms of internal bandwidth and processing throughput) even compared against much larger designs, and requires fewer physical resources to achieve this level of performance. Finally, MC-DeF is able to better utilize the underlying FPGA fabric and achieves the best efficiency (measured in LUT/GOPs).  © 2021 Owner/Author.",CGRA; CGRA framework; FPGA; reconfigurable computing,Application programs; Cells; Computer hardware; Cost functions; Cytology; Energy efficiency; Energy utilization; Integrated circuit design; Reconfigurable hardware; Coarse grain reconfigurable arrays; Competitive performance; Design space exploration; Multiple instances; Physical resources; Scientific applications; Software implementation; User-defined metrics; Field programmable gate arrays (FPGA)
CacheInspector: Reverse Engineering Cache Resources in Public Clouds,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108903940&doi=10.1145%2f3457373&partnerID=40&md5=784d530f834b0692ca62f0009e42c8bf,"Infrastructure-as-a-Service cloud providers sell virtual machines that are only specified in terms of number of CPU cores, amount of memory, and I/O throughput. Performance-critical aspects such as cache sizes and memory latency are missing or reported in ways that make them hard to compare across cloud providers. It is difficult for users to adapt their application's behavior to the available resources. In this work, we aim to increase the visibility that cloud users have into shared resources on public clouds. Specifically, we present CacheInspector, a lightweight runtime that determines the performance and allocated capacity of shared caches on multi-tenant public clouds. We validate CacheInspector's accuracy in a controlled environment, and use it to study the characteristics and variability of cache resources in the cloud, across time, instances, availability regions, and cloud providers. We show that CacheInspector's output allows cloud users to tailor their application's behavior, including their output quality, to avoid suboptimal performance when resources are scarce.  © 2021 ACM.",cache latency; cache throughput; CPU cache; public cloud,Infrastructure as a service (IaaS); Reverse engineering; Cloud providers; Controlled environment; Memory latencies; Multi tenants; Output quality; Public clouds; Shared resources; Sub-optimal performance; Cache memory
Automatic Sublining for Efficient Sparse Memory Accesses,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108911444&doi=10.1145%2f3452141&partnerID=40&md5=075a0455a70575755da68f5a155f4c08,"Sparse memory accesses, which are scattered accesses to single elements of a large data structure, are a challenge for current processor architectures. Their lack of spatial and temporal locality and their irregularity makes caches and traditional stream prefetchers useless. Furthermore, performing standard caching and prefetching on sparse accesses wastes precious memory bandwidth and thrashes caches, deteriorating performance for regular accesses. Bypassing prefetchers and caches for sparse accesses, and fetching only a single element (e.g., 8 B) from main memory (subline access), can solve these issues. Deciding which accesses to handle as sparse accesses and which as regular cached accesses, is a challenging task, with a large potential impact on performance. Not only is performance reduced by treating sparse accesses as regular accesses, not caching accesses that do have locality also negatively impacts performance by significantly increasing their latency and bandwidth consumption. Furthermore, this decision depends on the dynamic environment, such as input set characteristics and system load, making a static decision by the programmer or compiler suboptimal. We propose the Instruction Spatial Locality Estimator (ISLE), a hardware detector that finds instructions that access isolated words in a sea of unused data. These sparse accesses are dynamically converted into uncached subline accesses, while keeping regular accesses cached. ISLE does not require modifying source code or binaries, and adapts automatically to a changing environment (input data, available bandwidth, etc.). We apply ISLE to a graph analytics processor running sparse graph workloads, and show that ISLE outperforms the performance of no subline accesses, manual sublining, and prior work on detecting sparse accesses.  © 2021 ACM.",Graph analytics; sparse computation,Bandwidth; Computer programming; Program compilers; Available bandwidth; Bandwidth consumption; Caching and prefetching; Changing environment; Current processors; Dynamic environments; Memory bandwidths; Spatial and temporal locality; Cache memory
Decreasing the Miss Rate and Eliminating the Performance Penalty of a Data Filter Cache,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108912086&doi=10.1145%2f3449043&partnerID=40&md5=cd32d11bc5393b16833fa652b65a8661,"While data filter caches (DFCs) have been shown to be effective at reducing data access energy, they have not been adopted in processors due to the associated performance penalty caused by high DFC miss rates. In this article, we present a design that both decreases the DFC miss rate and completely eliminates the DFC performance penalty even for a level-one data cache (L1 DC) with a single cycle access time. First, we show that a DFC that lazily fills each word in a DFC line from an L1 DC only when the word is referenced is more energy-efficient than eagerly filling the entire DFC line. For a 512B DFC, we are able to eliminate loads of words into the DFC that are never referenced before being evicted, which occurred for about 75% of the words in 32B lines. Second, we demonstrate that a lazily word filled DFC line can effectively share and pack data words from multiple L1 DC lines to lower the DFC miss rate. For a 512B DFC, we completely avoid accessing the L1 DC for loads about 23% of the time and avoid a fully associative L1 DC access for loads 50% of the time, where the DFC only requires about 2.5% of the size of the L1 DC. Finally, we present a method that completely eliminates the DFC performance penalty by speculatively performing DFC tag checks early and only accessing DFC data when a hit is guaranteed. For a 512B DFC, we improve data access energy usage for the DTLB and L1 DC by 33% with no performance degradation.  © 2021 ACM.",data cache compression; Data filter cache; guaranteeing cache hits; line fills,Energy efficiency; Data access; Data caches; Data filter; Energy efficient; Energy usage; Performance degradation; Performance penalties; Single cycle; Buffer storage
PRISM: Strong Hardware Isolation-based Soft-Error Resilient Multicore Architecture with High Performance and Availability at Low Hardware Overheads,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108910610&doi=10.1145%2f3450523&partnerID=40&md5=c003b85fe3ddbfa860d517a729813a51,"Multicores increasingly deploy safety-critical parallel applications that demand resiliency against soft-errors to satisfy the safety standards. However, protection against these errors is challenging due to complex communication and data access protocols that aggressively share on-chip hardware resources. Research has explored various temporal and spatial redundancy-based resiliency schemes that provide multicores with high soft-error coverage. However, redundant execution incurs performance overheads due to interference effects induced by aggressive resource sharing. Moreover, these schemes require intrusive hardware modifications and fall short in providing efficient system availability guarantees. This article proposes PRISM, a resilient multicore architecture that incorporates strong hardware isolation to form redundant clusters of cores, ensuring a non-interference-based redundant execution environment. A soft error in one cluster does not effect the execution of the other cluster, resulting in high system availability. Implementing strong isolation for shared hardware resources, such as queues, caches, and networks requires logic for partitioning. However, it is less intrusive as complex hardware modifications to protocols, such as hardware cache coherence, are avoided. The PRISM approach is prototyped on a real Tilera Tile-Gx72 processor that enables primitives to implement the proposed cluster-level hardware resource isolation. The evaluation shows performance benefits from avoiding destructive hardware interference effects with redundant execution, while delivering superior system availability.  © 2021 ACM.",hardware interference; Soft-errors; strong isolation,Complex networks; Error correction; Network architecture; Prisms; Radiation hardening; Software architecture; Data Access Protocols; Execution environments; Hardware modifications; Interference effects; Multicore architectures; Parallel application; Performance benefits; Temporal and spatial redundancies; Availability
Understanding Cache Compression,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108906080&doi=10.1145%2f3457207&partnerID=40&md5=cd2d2c91b7cf4a4199af55de8cf6e21d,"Hardware cache compression derives from software-compression research; yet, its implementation is not a straightforward translation, since it must abide by multiple restrictions to comply with area, power, and latency constraints. This study sheds light on the challenges of adopting compression in cache design - from the shrinking of the data until its physical placement. The goal of this article is not to summarize proposals but to put in evidence the solutions they employ to handle those challenges. An in-depth description of the main characteristics of multiple methods is provided, as well as criteria that can be used as a basis for the assessment of such schemes. It is expected that this article will ease the understanding of decisions to be taken for the design of compressed systems and provide directions for future work.  © 2021 ACM.",cache compression; Caches,Software engineering; Cache compressions; Cache design; Latency constraints; Multiple methods; Software compression
Flynn s Reconciliation: Automating the Register Cache Idiom for Cross-accelerator Programming,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108905757&doi=10.1145%2f3458357&partnerID=40&md5=83e412bd056c83200133cde74a446ef9,"A large portion of the recent performance increase in the High Performance Computing (HPC) and Machine Learning (ML) domains is fueled by accelerator cards. Many popular ML frameworks support accelerators by organizing computations as a computational graph over a set of highly optimized, batched general-purpose kernels. While this approach simplifies the kernels' implementation for each individual accelerator, the increasing heterogeneity among accelerator architectures for HPC complicates the creation of portable and extensible libraries of such kernels. Therefore, using a generalization of the CUDA community's warp register cache programming idiom, we propose a new programming idiom (CoRe) and a virtual architecture model (PIRCH), abstracting over SIMD and SIMT paradigms. We define and automate the mapping process from a single source to PIRCH's intermediate representation and develop backends that issue code for three different architectures: Intel AVX512, NVIDIA GPUs, and NEC SX-Aurora. Code generated by our source-to-source compiler for batched kernels, borG, competes favorably with vendor-tuned libraries and is up to 2× faster than hand-tuned kernels across architectures.  © 2021 ACM.",Accelerated computing; batched kernels; cross-architecture compilation; GPGPU; SIMD; SIMT; source-to-source compiler; warp-register cache,Acceleration; Architecture; Binary alloys; Cobalt alloys; Codes (symbols); Libraries; Program processors; Rhenium alloys; Accelerator architectures; Accelerator programming; Computational graph; Extensible library; High performance computing (HPC); Intermediate representations; Recent performance; Virtual architecture; Computer architecture
Early Address Prediction: Efficient Pipeline Prefetch and Reuse,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108912689&doi=10.1145%2f3458883&partnerID=40&md5=c2352a06b38c4fbe28247b78b524a64d,"Achieving low load-to-use latency with low energy and storage overheads is critical for performance. Existing techniques either prefetch into the pipeline (via address prediction and validation) or provide data reuse in the pipeline (via register sharing or L0 caches). These techniques provide a range of tradeoffs between latency, reuse, and overhead. In this work, we present a pipeline prefetching technique that achieves state-of-the-art performance and data reuse without additional data storage, data movement, or validation overheads by adding address tags to the register file. Our addition of register file tags allows us to forward (reuse) load data from the register file with no additional data movement, keep the data alive in the register file beyond the instruction's lifetime to increase temporal reuse, and coalesce prefetch requests to achieve spatial reuse. Further, we show that we can use the existing memory order violation detection hardware to validate prefetches and data forwards without additional overhead. Our design achieves the performance of existing pipeline prefetching while also forwarding 32% of the loads from the register file (compared to 15% in state-of-the-art register sharing), delivering a 16% reduction in L1 dynamic energy (1.6% total processor energy), with an area overhead of less than 0.5%.  © 2021 Owner/Author.",address prediction; energy efficient computing; first level cache; Pipeline prefetching; register sharing,Buffer storage; Additional datum; Memory ordering; Prefetching techniques; Register sharing; State of the art; State-of-the-art performance; Storage overhead; Total processor energy; Pipelines
GraphPEG: Accelerating Graph Processing on GPUs,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108916489&doi=10.1145%2f3450440&partnerID=40&md5=d8cbb6e588645826c616acf81a18660b,"Due to massive thread-level parallelism, GPUs have become an attractive platform for accelerating large-scale data parallel computations, such as graph processing. However, achieving high performance for graph processing with GPUs is non-trivial. Processing graphs on GPUs introduces several problems, such as load imbalance, low utilization of hardware unit, and memory divergence. Although previous work has proposed several software strategies to optimize graph processing on GPUs, there are several issues beyond the capability of software techniques to address. In this article, we present GraphPEG, a graph processing engine for efficient graph processing on GPUs. Inspired by the observation that many graph algorithms have a common pattern on graph traversal, GraphPEG improves the performance of graph processing by coupling automatic edge gathering with fine-grain work distribution. GraphPEG can also adapt to various input graph datasets and simplify the software design of graph processing with hardware-assisted graph traversal. Simulation results show that, in comparison with two representative highly efficient GPU graph processing software framework Gunrock and SEP-Graph, GraphPEG improves graph processing throughput by 2.8× and 2.5× on average, and up to 7.3× and 7.0× for six graph algorithm benchmarks on six graph datasets, with marginal hardware cost.  © 2021 ACM.",GPU; graph processing; hardware acceleration; load balance,Computer hardware; Computer programming; Program processors; Software design; Graph processing; Hardware-assisted; Large scale data; Memory divergences; Processing graphs; Software techniques; Thread level parallelism; Work distribution; Graph algorithms
PAVER: Locality Graph-Based Thread Block Scheduling for GPUs,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108912976&doi=10.1145%2f3451164&partnerID=40&md5=53ec9544dd123ccaf4fc3270d1dd655b,"The massive parallelism present in GPUs comes at the cost of reduced L1 and L2 cache sizes per thread, leading to serious cache contention problems such as thrashing. Hence, the data access locality of an application should be considered during thread scheduling to improve execution time and energy consumption. Recent works have tried to use the locality behavior of regular and structured applications in thread scheduling, but the difficult case of irregular and unstructured parallel applications remains to be explored. We present PAVER, a Priority-Aware Vertex schedulER, which takes a graph-theoretic approach toward thread scheduling. We analyze the cache locality behavior among thread blocks (TBs) through a just-in-time compilation, and represent the problem using a graph representing the TBs and the locality among them. This graph is then partitioned to TB groups that display maximum data sharing, which are then assigned to the same streaming multiprocessor by the locality-aware TB scheduler. Through exhaustive simulation in Fermi, Pascal, and Volta architectures using a number of scheduling techniques, we show that PAVER reduces L2 accesses by 43.3%, 48.5%, and 40.21% and increases the average performance benefit by 29%, 49.1%, and 41.2% for the benchmarks with high inter-TB locality.  © 2021 ACM.",dependency graph; GPGPU; locality; thread block,Benchmarking; Data Sharing; Energy utilization; Graph theory; Graphic methods; Program processors; Exhaustive simulation; Graph theoretic approach; Just-in-time compilation; Massive parallelism; Parallel application; Performance benefits; Scheduling techniques; Streaming multiprocessors; Scheduling
PERI: A Configurable Posit Enabled RISC-V Core,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108910956&doi=10.1145%2f3446210&partnerID=40&md5=0c05173a802f72fa9abe24b7b4d621e8,"Owing to the failure of Dennard's scaling, the past decade has seen a steep growth of prominent new paradigms leveraging opportunities in computer architecture. Two technologies of interest are Posit and RISC-V. Posit was introduced in mid-2017 as a viable alternative to IEEE-754, and RISC-V provides a commercial-grade open source Instruction Set Architecture (ISA). In this article, we bring these two technologies together and propose a Configurable Posit Enabled RISC-V Core called PERI. The article provides insights on how the Single-Precision Floating Point (""F"") extension of RISC-V can be leveraged to support posit arithmetic. We also present the implementation details of a parameterized and feature-complete posit Floating Point Unit (FPU). The configurability and the parameterization features of this unit generate optimal hardware, which caters to the accuracy and energy/area tradeoffs imposed by the applications, a feature not possible with IEEE-754 implementation. The posit FPU has been integrated with the RISC-V compliant SHAKTI C-class core as an execution unit. To further leverage the potential of posit, we enhance our posit FPU to support two different exponent sizes (with posit-size being 32-bits), thereby enabling multiple-precision at runtime. To enable the compilation and execution of C programs on PERI, we have made minimal modifications to the GNU C Compiler (GCC), targeting the ""F""extension of the RISC-V. We compare posit with IEEE-754 in terms of hardware area, application accuracy, and runtime. We also present an alternate methodology of integrating the posit FPU with the RISC-V core as an accelerator using the custom opcode space of RISC-V.  © 2021 ACM.",floating-point; IEEE-754; Posit; processor; RISC-V,Computer hardware; Digital arithmetic; Program compilers; Commercial grade; Configurability; Execution units; Floating point units; Instruction set architecture; Open sources; Parameterized; Single precision; Reduced instruction set computing
KernelFaRer: Replacing Native-Code Idioms with High-Performance Library Calls,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108906305&doi=10.1145%2f3459010&partnerID=40&md5=c71fd20c89db29701c2a115cffa1d916,"Well-crafted libraries deliver much higher performance than code generated by sophisticated application programmers using advanced optimizing compilers. When a code pattern for which a well-tuned library implementation exists is found in the source code of an application, the highest performing solution is to replace the pattern with a call to the library. Idiom-recognition solutions in the past either required pattern matching machinery that was outside of the compilation framework or provided a very brittle solution that would fail even for minor variants in the pattern source code. This article introduces Kernel Find & Replacer (KernelFaRer), an idiom recognizer implemented entirely in the existing LLVM compiler framework. The versatility of KernelFaRer is demonstrated by matching and replacing two linear algebra idioms, general matrix-matrix multiplication (GEMM), and symmetric rank-2k update (SYR2K). Both GEMM and SYR2K are used extensively in scientific computation, and GEMM is also a central building block for deep learning and computer graphics algorithms. The idiom recognition in KernelFaRer is much more robust than alternative solutions, has a much lower compilation overhead, and is fully integrated in the broadly used LLVM compilation tools. KernelFaRer replaces existing GEMM and SYR2K idioms with computations performed by BLAS, Eigen, MKL (Intel's x86), ESSL (IBM's PowerPC), and BLIS (AMD). Gains in performance that reach 2000× over hand-crafted source code compiled at the highest optimization level demonstrate that replacing application code with library call is a performant solution.  © 2021 ACM.",compiler analysis and transformations; GEMM; Idiom recognition; LLVM,Computer graphics; Computer programming languages; Deep learning; Machinery; Pattern matching; Program compilers; Alternative solutions; Application programmers; Graphics algorithms; High-performance libraries; Matrix matrix multiplications; Optimization levels; Optimizing compilers; Scientific computation; Matrix algebra
Fast Key-Value Lookups with Node Tracker,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108914204&doi=10.1145%2f3452099&partnerID=40&md5=51afcccebe7ee3b92ea16a6bb05ae182,"Lookup operations for in-memory databases are heavily memory bound, because they often rely on pointer-chasing linked data structure traversals. They also have many branches that are hard-to-predict due to random key lookups. In this study, we show that although cache misses are the primary bottleneck for these applications, without a method for eliminating the branch mispredictions only a small fraction of the performance benefit is achieved through prefetching alone. We propose the Node Tracker (NT), a novel programmable prefetcher/pre-execution unit that is highly effective in exploiting inter key-lookup parallelism to improve single-thread performance. We extend NT with branch outcome streaming (BOS) to reduce branch mispredictions and show that this achieves an extra 3× speedup. Finally, we evaluate the NT as a pre-execution unit and demonstrate that we can further improve the performance in both single- and multi-threaded execution modes. Our results show that, on average, NT improves single-thread performance by 4.1× when used as a prefetcher; 11.9× as a prefetcher with BOS; 14.9× as a pre-execution unit and 18.8× as a pre-execution unit with BOS. Finally, with 24 cores of the latter version, we achieve a speedup of 203× and 11× over the single-core and 24-core baselines, respectively.  © 2021 ACM.",branch prediction; Hardware and software prefetch; in-memory database applications; pre-execution,Software engineering; Branch mispredictions; Execution units; Lookup operations; Memory bounds; Memory database; Multithreaded; Performance benefits; Single-thread performance
Performance Evaluation of Intel Optane Memory for Managed Workloads,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108912395&doi=10.1145%2f3451342&partnerID=40&md5=266808ffd484160b5722c46d2b67e984,"Intel Optane memory offers non-volatility, byte addressability, and high capacity. It suits managed workloads that prefer large main memory heaps. We investigate Optane as the main memory for managed (Java) workloads, focusing on performance scalability. As the workload (core count) increases, we note Optane's performance relative to DRAM. A few workloads incur a slight slowdown on Optane memory, which helps conserve limited DRAM capacity. Unfortunately, other workloads scale poorly beyond a few core counts. This article investigates scaling bottlenecks for Java workloads on Optane memory, analyzing the application, runtime, and microarchitectural interactions. Poorly scaling workloads allocate objects rapidly and access objects in Optane memory frequently. These characteristics slow down the mutator and substantially slow down garbage collection (GC). At the microarchitecture level, load, store, and instruction miss penalties rise. To regain performance, we partition heaps across DRAM and Optane memory, a hybrid that scales considerably better than Optane alone. We exploit state-of-the-art GC approaches to partition heaps. Unfortunately, existing GC approaches needlessly waste DRAM capacity because they ignore runtime behavior. This article also introduces performance impact-guided memory allocation (PIMA) for hybrid memories. PIMA maximizes Optane utilization, allocating in DRAM only if it improves performance. It estimates the performance impact of allocating heaps in either memory type by sampling. We target PIMA at graph analytics workloads, offering a novel performance estimation method and detailed evaluation. PIMA identifies workload phases that benefit from DRAM with high (94.33%) accuracy, incurring only a 2% sampling overhead. PIMA operates stand-alone or combines with prior approaches to offer new performance versus DRAM capacity trade-offs. This work opens up Optane memory to a real-life role as the main memory for Java workloads.  © 2021 ACM.",analytics; estimation; Intel Optane memory; Java; performance; scalability,Economic and social effects; Experimental mineralogy; Java programming language; Garbage collection; Graph analytics; Micro architectures; Performance estimation; Performance impact; Performance scalability; Runtime behaviors; State of the art; Dynamic random access storage
Cryptographic Software IP Protection without Compromising Performance or Timing Side-channel Leakage,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102980506&doi=10.1145%2f3443707&partnerID=40&md5=72e547d16c00840f3b4a5f7c0f1a4383,"Program obfuscation is a widely used cryptographic software intellectual property (IP) protection technique against reverse engineering attacks in embedded systems. However, very few works have studied the impact of combining various obfuscation techniques on the obscurity (difficulty of reverse engineering) and performance (execution time) of obfuscated programs. In this article, we propose a Genetic Algorithm (GA)-based framework that not only optimizes obscurity and performance of obfuscated cryptographic programs, but it also ensures very low timing side-channel leakage. Our proposed Timing Side Channel Sensitive Program Obfuscation Optimization Framework (TSC-SPOOF) determines the combination of obfuscation transformation functions that produce optimized obfuscated programs with preferred optimization parameters. In particular, TSC-SPOOF employs normalized compression distance (NCD) and channel capacity to measure obscurity and timing side-channel leakage, respectively. We also use RISC-V rocket core running on a Xilinx Zynq FPGA device as part of our framework to obtain realistic results. The experimental results clearly show that our proposed solution leads to cryptographic programs with lower execution time, higher obscurity, and lower timing side-channel leakage than unguided obfuscation. © 2021 ACM.",channel capacity; FPGA; LLVM; NCD; Obfuscation; optimization; timing side-channel,Chromium compounds; Embedded systems; Genetic algorithms; Reverse engineering; Rockets; Timing circuits; Cryptographic software; IP protection; Normalized compression distance; Obfuscation transformations; Optimization framework; Optimization parameter; Program obfuscation; Timing side channels; Side channel attack
Grus: Toward Unified-memory-efficient High-performance Graph Processing on GPU,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102980871&doi=10.1145%2f3444844&partnerID=40&md5=cf4fcc343160545b4d35a6dbf96aa8c3,"Today's GPU graph processing frameworks face scalability and efficiency issues as the graph size exceeds GPU-dedicated memory limit. Although recent GPUs can over-subscribe memory with Unified Memory (UM), they incur significant overhead when handling graph-structured data. In addition, many popular processing frameworks suffer sub-optimal efficiency due to heavy atomic operations when tracking the active vertices. This article presents Grus, a novel system framework that allows GPU graph processing to stay competitive with the ever-growing graph complexity. Grus improves space efficiency through a UM trimming scheme tailored to the data access behaviors of graph workloads. It also uses a lightweight frontier structure to further reduce atomic operations. With easy-to-use interface that abstracts the above details, Grus shows up to 6.4× average speedup over the state-of-the-art in-memory GPU graph processing framework. It allows one to process large graphs of 5.5 billion edges in seconds with a single GPU. © 2021 ACM.",GPU; Graph processing; unified virtual memory,Data handling; Efficiency; Graph structures; Interface states; Memory architecture; Program processors; Atomic operation; Graph complexity; Graph structured data; Memory efficient; Optimal efficiency; Space efficiencies; State of the art; System framework; Graphics processing unit
PETRA: Persistent Transactional Non-blocking Linked Data Structures,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102982238&doi=10.1145%2f3446391&partnerID=40&md5=a36d78cbdf26f1819a38803d6e6b371d,"Emerging byte-addressable Non-Volatile Memories (NVMs) enable persistent memory where process state can be recovered after crashes. To enable applications to rely on persistent data, durable data structures with failure-atomic operations have been proposed. However, they lack the ability to allow users to execute a sequence of operations as transactions. Meanwhile, persistent transactional memory (PTM) has been proposed by adding durability to Software Transactional Memory (STM). However, PTM suffers from high performance overheads and low scalability due to false aborts, logging, and ordering constraints on persistence. In this article, we propose PETRA, a new approach for constructing persistent transactional linked data structures. PETRA natively supports transactions, but unlike PTM, relies on the high-level information from the data structure semantics. This gives PETRA unique advantages in the form of high performance and high scalability. Our experimental results using various benchmarks demonstrate the scalability of PETRA in all workloads and transaction sizes. PETRA outperforms the state-of-the-art PTMs by an order of magnitude in transactions of size greater than one, and demonstrates superior performance in transactions of size one. © 2021 ACM.",concurrency; durability; non-blocking data structure; non-volatile memory; Persistent memory; transactional data structure,Data handling; Data structures; Linked data; Scalability; Semantics; Storage allocation (computer); Atomic operation; High scalabilities; High-level information; Non-volatile memory; Ordering constraints; Persistent memory; Software transactional memory; Transactional memory; Failure (mechanical)
On Predictable Reconfigurable System Design,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102980707&doi=10.1145%2f3436995&partnerID=40&md5=8d7f2077d00697018d26b254d483fc3d,"We propose a design methodology to facilitate rigorous development of complex applications targeting reconfigurable hardware. Our methodology relies on analytical estimation of system performance and area utilisation for a given specific application and a particular system instance consisting of a controlflow machine working in conjunction with one or more reconfigurable dataflow accelerators. The targeted application is carefully analyzed, and the parts identified for hardware acceleration are reimplemented as a set of representative software models. Next, with the results of the application analysis, a suitable system architecture is devised and its performance is evaluated to determine bottlenecks, allowing predictable design. The architecture is iteratively refined, until the final version satisfying the specification requirements in terms of performance and required hardware area is obtained. We validate the presented methodology using a widely accepted convolutional neural network (VGG-16) and an important HPC application (BQCD). In both cases, our methodology relieved and alleviated all system bottlenecks before the hardware implementation was started. As a result the architectures were implemented first time right, achieving state-of-the-art performance within 15% of our modelling estimations. © 2021 ACM.",analytical model; architecture; development methodology; FPGA; performance model; reconfigurable systems,Application programs; Computer hardware; Convolutional neural networks; Iterative methods; Network architecture; Structural design; Analytical estimations; Application analysis; Complex applications; Hardware acceleration; Hardware implementations; Reconfigurable systems; Specification requirement; State-of-the-art performance; Reconfigurable hardware
A Non-Intrusive Tool Chain to Optimize MPSoC End-to-End Systems,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102980548&doi=10.1145%2f3445030&partnerID=40&md5=ea427ce3937ca24a858f3820d9fa5b14,"Multi-core systems are now found in many electronic devices. But does current software design fully leverage their capabilities? The complexity of the hardware and software stacks in these platforms requires software optimization with end-to-end knowledge of the system. To optimize software performance, we must have accurate information about system behavior and time losses. Standard monitoring engines impose tradeoffs on profiling tools, making it impossible to reconcile all the expected requirements: Accurate hardware views, fine-grain measurements, speed, and so on. Subsequently, new approaches have to be examined. In this article, we propose a non-intrusive, accurate tool chain, which can reveal and quantify slowdowns in low-level software mechanisms. Based on emulation, this tool chain extracts behavioral information (time, contention) through hardware side channels, without distorting the software execution flow. This tool consists of two parts. (1) An online acquisition part that dumps hardware platform signals. (2) An offline processing part that consolidates meaningful behavioral information from the dumped data. Using our tool chain, we studied and propose optimizations to MultiProcessor System on Chip (MPSoC) support in the Linux kernel, saving about 60% of the time required for the release phase of the GNU OpenMP synchronization barrier when running on a 64-core MPSoC. © 2021 ACM.",clustered MPSoC; emulation platform; HW/SW optimization; non-intrusive tool chain; SW profiling,Application programming interfaces (API); Firmware; Linux; Multiprocessing systems; Open source software; System-on-chip; Hardware and software; Low level softwares; Multiprocessor system on chips; Non-intrusive tools; Off-line processing; Software optimization; Software performance; Synchronization barriers; Software design
Gretch: A Hardware Prefetcher for Graph Analytics,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102982664&doi=10.1145%2f3439803&partnerID=40&md5=48fba67cbf2b1d5d877dbeb73e1fee0b,"Data-dependent memory accesses (DDAs) pose an important challenge for high-performance graph analytics (GA). This is because such memory accesses do not exhibit enough temporal and spatial locality resulting in low cache performance. Prior efforts that focused on improving the performance of DDAs for GA are not applicable across various GA frameworks. This is because (1) they only focus on one particular graph representation, and (2) they require workload changes to communicate specific information to the hardware for their effective operation. In this work, we propose a hardware-only solution to improving the performance of DDAs for GA across multiple GA frameworks. We present a hardware prefetcher for GA called Gretch, that addresses the above limitations. An important observation we make is that identifying certain DDAs without hardware-software communication is sensitive to the instruction scheduling. A key contribution of this work is a hardware mechanism that activates Gretch to identify DDAs when using either in-order or out-of-order instruction scheduling. Our evaluation shows that Gretch provides an average speedup of 38% over no prefetching, 25% over conventional stride prefetcher, and outperforms prior DDAs prefetchers by 22% with only 1% increase in power consumption when executed on different GA workloads and frameworks. © 2021 ACM.",data-dependent memory accesses; graph analytics; Hardware prefetching,Scheduling; Cache performance; Data dependent; Graph analytics; Graph representation; Hardware mechanism; Instruction scheduling; Specific information; Temporal and spatial; Cache memory
GRAM: A Framework for Dynamically Mixing Precisions in GPU Applications,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102981063&doi=10.1145%2f3441830&partnerID=40&md5=4922b0a8d8b4b1e6fb1ae24be15f7eb3,"This article presents GRAM (GPU-based Runtime Adaption for Mixed-precision) a framework for the effective use of mixed precision arithmetic for CUDA programs. Our method provides a fine-grain tradeoff between output error and performance. It can create many variants that satisfy different accuracy requirements by assigning different groups of threads to different precision levels adaptively at runtime. To widen the range of applications that can benefit from its approximation, GRAM comes with an optional half-precision approximate math library. Using GRAM, we can trade off precision for any performance improvement of up to 540%, depending on the application and accuracy requirement. © 2021 ACM.",approximate computing; GPU; half preicision; mixed precision,Economic and social effects; Fine grains; Gpu-based; Math library; Mixed precision; Output errors; Runtimes; Trade off; Graphics processing unit
A Reusable Characterization of the Memory System Behavior of SPEC2017 and SPEC2006,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102980010&doi=10.1145%2f3446200&partnerID=40&md5=02a5fd8daabcc19fb8e692ac52450c65,"The SPEC CPU Benchmarks are used extensively for evaluating and comparing improvements to computer systems. This ubiquity makes characterization critical for researchers to understand the bottlenecks the benchmarks do and do not expose and where new designs should and should not be expected to show impact. However, in characterization there is a tradeoff between accuracy and reusability: The more precisely we characterize a benchmark's performance on a given system, the less usable it is across different micro-architectures and varying memory configurations. For SPEC, most existing characterizations include system-specific effects (e.g., via performance counters) and/or only look at aggregate behavior (e.g., averages over the full application execution). While such approaches simplify characterization, they make it difficult to separate the applications' intrinsic behavior from the system-specific effects and/or lose the diverse phase-based behaviors. In this work we focus on characterizing the applications' intrinsic memory behaviour by isolating them from micro-architectural configuration specifics. We do this by providing a simplified generic system model that evaluates the applications' memory behavior across multiple cache sizes, with and without prefetching, and over time. The resulting characterization can be reused across a range of systems to understand application behavior and allow us to see how frequently different behaviors occur. We use this approach to compare the SPEC 2006 and 2017 suites, providing insight into their memory system behaviour beyond previous system-specific and/or aggregate results. We demonstrate the ability to use this characterization in different contexts by showing a portion of the SPEC 2017 benchmark suite that could benefit from giga-scale caches, despite aggregate results indicating otherwise. © 2021 ACM.",benchmark characterization; cache sensitivity; memory system characterization; Memory systems; prefetcher sensitivity; workload characterization,Aggregates; Benchmarking; Computer architecture; Computer software reusability; Reusability; Aggregate behavior; Application behaviors; Application execution; Benchmark suites; Intrinsic behavior; Memory configuration; Micro architectures; Performance counters; Cache memory
Efficient Auto-Tuning of Parallel Programs with Interdependent Tuning Parameters via Auto-Tuning Framework (ATF),2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099782495&doi=10.1145%2f3427093&partnerID=40&md5=eaf467ee7ac66b0103dcaab5acaae53e,"Auto-tuning is a popular approach to program optimization: it automatically finds good configurations of a program's so-called tuning parameters whose values are crucial for achieving high performance for a particular parallel architecture and characteristics of input/output data. We present three new contributions of the Auto-Tuning Framework (ATF), which enable a key advantage in general-purpose auto-tuning: efficiently optimizing programs whose tuning parameters have interdependencies among them. We make the following contributions to the three main phases of general-purpose auto-tuning: (1) ATF generates the search space of interdependent tuning parameters with high performance by efficiently exploiting parameter constraints; (2) ATF stores such search spaces efficiently in memory, based on a novel chain-of-trees search space structure; (3) ATF explores these search spaces faster, by employing a multi-dimensional search strategy on its chain-of-trees search space representation. Our experiments demonstrate that, compared to the state-of-the-art, general-purpose auto-tuning frameworks, ATF substantially improves generating, storing, and exploring the search space of interdependent tuning parameters, thereby enabling an efficient overall auto-tuning process for important applications from popular domains, including stencil computations, linear algebra routines, quantum chemistry computations, and data mining algorithms.  © 2021 ACM.",Auto-tuning; interdependent tuning parameters; parallel programs,Forestry; Linear algebra; Parallel architectures; Quantum chemistry; Quantum theory; Data mining algorithm; Input/output datum; Optimizing programs; Parameter constraints; Program optimization; Quantum chemistry computations; Search strategies; Stencil computations; Data mining
On the Anatomy of Predictive Models for Accelerating GPU Convolution Kernels and beyond,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099790155&doi=10.1145%2f3434402&partnerID=40&md5=a4c30176ed8868e02952d8c4ca69e6d8,"Efficient HPC libraries often expose multiple tunable parameters, algorithmic implementations, or a combination of them, to provide optimized routines. The optimal parameters and algorithmic choices may depend on input properties such as the shapes of the matrices involved in the operation. Traditionally, these parameters are manually tuned or set by auto-tuners. In emerging applications such as deep learning, this approach is not effective across the wide range of inputs and architectures used in practice. In this work, we analyze different machine learning techniques and predictive models to accelerate the convolution operator and GEMM. Moreover, we address the problem of dataset generation, and we study the performance, accuracy, and generalization ability of the models. Our insights allow us to improve the performance of computationally expensive deep learning primitives on high-end GPUs as well as low-power embedded GPU architectures on three different libraries. Experimental results show significant improvement in the target applications from 50% up to 300% compared to auto-tuned and high-optimized vendor-based heuristics by using simple decision tree- and MLP-based models.  © 2021 Owner/Author.",GPU computing; neural networks; performance optimization; predictive models; supervised classification; tuning,Convolution; Decision trees; Deep learning; Graphics processing unit; Learning systems; Libraries; Optimization; Program processors; Convolution kernel; Convolution operators; Emerging applications; Generalization ability; Machine learning techniques; Optimal parameter; Target application; Tunable parameter; Predictive analytics
Bayesian Optimization for Efficient Accelerator Synthesis,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099780361&doi=10.1145%2f3427377&partnerID=40&md5=d3c166cbe29239f8121a102aad6edaef,"Accelerator design is expensive due to the effort required to understand an algorithm and optimize the design. Architects have embraced two technologies to reduce costs. High-level synthesis automatically generates hardware from code. Reconfigurable fabrics instantiate accelerators while avoiding fabrication costs for custom circuits. We further reduce design effort with statistical learning. We build an automated framework, called Prospector, that uses Bayesian techniques to optimize synthesis directives, reducing execution latency and resource usage in field-programmable gate arrays. We show in a certain amount of time that designs discovered by Prospector are closer to Pareto-efficient designs compared to prior approaches. Prospector permits new studies for heterogeneous accelerators.  © 2020 ACM.",Bayesian optimization; design space exploration; FPGA; High-level synthesis,Field programmable gate arrays (FPGA); High level synthesis; Pareto principle; Accelerator design; Bayesian optimization; Bayesian techniques; Custom circuits; Fabrication cost; Pareto-efficient; Reconfigurable fabrics; Statistical learning; Logic Synthesis
PolyDL: Polyhedral Optimizations for Creation of High-performance DL Primitives,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099792327&doi=10.1145%2f3433103&partnerID=40&md5=7ec4a272f9329f2c56061c3c4a7b4e18,"Deep Neural Networks (DNNs) have revolutionized many aspects of our lives. The use of DNNs is becoming ubiquitous, including in software for image recognition, speech recognition, speech synthesis, language translation, to name a few. The training of DNN architectures, however, is computationally expensive. Once the model is created, its use in the intended application - the inference task, is computationally heavy too and the inference needs to be fast for real time use. For obtaining high performance today, the code of Deep Learning (DL) primitives optimized for specific architectures by expert programmers exposed via libraries is the norm. However, given the constant emergence of new DNN architectures, creating hand optimized code is expensive, slow and is not scalable. To address this performance-productivity challenge, in this article we present compiler algorithms to automatically generate high-performance implementations of DL primitives that closely match the performance of hand optimized libraries. We develop novel data reuse analysis algorithms using the polyhedral model to derive efficient execution schedules automatically. In addition, because most DL primitives use some variant of matrix multiplication at their core, we develop a flexible framework where it is possible to plug in library implementations of the same in lieu of a subset of the loops. We show that such a hybrid compiler plus a minimal library-use approach results in state-of-the-art performance. We develop compiler algorithms to also perform operator fusions that reduce data movement through the memory hierarchy of the computer system. Using Convolution Neural Network (CNN) models and matrix multiplication operations, we demonstrate that our approach automatically creates high performing DNN building blocks whose performance matches the performance of hand-crafted kernels of Intel's oneDNN library on high end CPUs. At the same time, our techniques take only a fraction of time (1/20 or less) compared to AutoTVM, a deep learning auto-tuner to create optimized implementations.  © 2021 ACM.",data reuse; loop optimization; machine learning; microkernels; Polyhedral compilation,Deep neural networks; Image recognition; Libraries; Matrix algebra; Memory architecture; Network architecture; Neural networks; Program compilers; Speech recognition; Speech synthesis; Convolution neural network; High performance implementations; Language translation; MAtrix multiplication; Matrix multiplication operation; Optimized implementation; Polyhedral optimizations; State-of-the-art performance; Deep learning
Systems-on-Chip with Strong Ordering,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099775548&doi=10.1145%2f3428153&partnerID=40&md5=3194a4c9d5fa9e4b3f764adef65d2a8e,"Sequential consistency (SC) is the most intuitive memory consistency model and the easiest for programmers and hardware designers to reason about. However, the strict memory ordering restrictions imposed by SC make it less attractive from a performance standpoint. Additionally, prior high-performance SC implementations required complex hardware structures to support speculation and recovery. In this article, we introduce the lockstep SC consistency model (LSC), a new memory model based on SC but carefully defined to accommodate the data parallel lockstep execution paradigm of GPUs. We also describe an efficient LSC implementation for an APU system-on-chip (SoC) and show that our implementation performs close to the baseline relaxed model. Evaluation of our implementation shows that the geometric mean performance cost for lockstep SC is just 0.76% for GPU execution and 6.11% for the entire APU SoC compared to a baseline with a weaker memory consistency model. Adoption of LSC in future APU and SoC designs will reduce the burden on programmers trying to write correct parallel programs, while also simplifying the implementation and verification of systems with heterogeneous processing elements and complex memory hierarchies.1  © 2021 ACM.",Consistency model; GPU; lockstep execution,Parallel processing systems; Program processors; Programmable logic controllers; Consistency model; Execution paradigm; Hardware designers; Heterogeneous processing; Memory consistency models; Performance costs; Sequential consistency; System on chips (SoC); System-on-chip
Reliability-aware Garbage Collection for Hybrid HBM-DRAM Memories,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099783964&doi=10.1145%2f3431803&partnerID=40&md5=464de39ef5f9a22b729d8180e04fb658,"Emerging workloads in cloud and data center infrastructures demand high main memory bandwidth and capacity. Unfortunately, DRAM alone is unable to satisfy contemporary main memory demands. High-bandwidth memory (HBM) uses 3D die-stacking to deliver 4-8× higher bandwidth. HBM has two drawbacks: (1) capacity is low, and (2) soft error rate is high. Hybrid memory combines DRAM and HBM to promise low fault rates, high bandwidth, and high capacity. Prior OS approaches manage HBM by mapping pages to HBM versus DRAM based on hotness (access frequency) and risk (susceptibility to soft errors). Unfortunately, these approaches operate at a coarse-grained page granularity, and frequent page migrations hurt performance. This article proposes a new class of reliability-aware garbage collectors for hybrid HBM-DRAM systems that place hot and low-risk objects in HBM and the rest in DRAM. Our analysis of nine real-world Java workloads shows that: (1) newly allocated objects in the nursery are frequently written, making them both hot and low-risk, (2) a small fraction of the mature objects are hot and low-risk, and (3) allocation site is a good predictor for hotness and risk. We propose RiskRelief, a novel reliability-aware garbage collector that uses allocation site prediction to place hot and low-risk objects in HBM. Allocation sites are profiled offline and RiskRelief uses heuristics to classify allocation sites as DRAM and HBM. The proposed heuristics expose Pareto-optimal trade-offs between soft error rate (SER) and execution time. RiskRelief improves SER by 9× compared to an HBM-Only system while at the same time improving performance by 29% compared to a DRAM-Only system. Compared to a state-of-the-art OS approach for reliability-aware data placement, RiskRelief eliminates all page migration overheads, which substantially improves performance while delivering similar SER. Reliability-aware garbage collection opens up a new opportunity to manage emerging HBM-DRAM memories at fine granularity while requiring no extra hardware support and leaving the programming model unchanged.  © 2021 ACM.",garbage collection; high-bandwidth memory; hybrid memories; Soft-error reliability,Bandwidth; Economic and social effects; Error correction; Pareto principle; Radiation hardening; Refuse collection; Reliability; Risk assessment; Access frequency; Fine granularity; Garbage collection; Garbage collectors; Hardware supports; Improving performance; Programming models; State of the art; Dynamic random access storage
A Simple Model for Portable and Fast Prediction of Execution Time and Power Consumption of GPU Kernels,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099788977&doi=10.1145%2f3431731&partnerID=40&md5=e196ec551394e0d7f4a3a51125beec38,"Characterizing compute kernel execution behavior on GPUs for efficient task scheduling is a non-trivial task. We address this with a simple model enabling portable and fast predictions among different GPUs using only hardware-independent features. This model is built based on random forests using 189 individual compute kernels from benchmarks such as Parboil, Rodinia, Polybench-GPU, and SHOC. Evaluation of the model performance using cross-validation yields a median Mean Average Percentage Error (MAPE) of 8.86-52.0% for time and 1.84-2.94% for power prediction across five different GPUs, while latency for a single prediction varies between 15 and 108 ms.  © 2020 ACM.",cross-validation; Execution time prediction; GPGPU; GPU computing; portable performance prediction; power prediction; profiling; random forest,Decision trees; Electric power utilization; Forecasting; Graphics processing unit; Program processors; Range finding; Cross validation; Hardware independent; Model performance; Non-trivial tasks; Percentage error; Power predictions; Simple modeling; Task-scheduling; Computer hardware
Refresh Triggered Computation: Improving the Energy Efficiency of Convolutional Neural Network Accelerators,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099778790&doi=10.1145%2f3417708&partnerID=40&md5=8e382ef74d22d00d828d22060d4c5f79,"To employ a Convolutional Neural Network (CNN) in an energy-constrained embedded system, it is critical for the CNN implementation to be highly energy efficient. Many recent studies propose CNN accelerator architectures with custom computation units that try to improve the energy efficiency and performance of CNNs by minimizing data transfers from DRAM-based main memory. However, in these architectures, DRAM is still responsible for half of the overall energy consumption of the system, on average. A key factor of the high energy consumption of DRAM is the refresh overhead, which is estimated to consume 40% of the total DRAM energy. In this article, we propose a new mechanism, Refresh Triggered Computation (RTC), that exploits the memory access patterns of CNN applications to reduce the number of refresh operations. RTC uses two major techniques to mitigate the refresh overhead. First, Refresh Triggered Transfer (RTT) is based on our new observation that a CNN application accesses a large portion of the DRAM in a predictable and recurring manner. Thus, the read/write accesses of the application inherently refresh the DRAM, and therefore a significant fraction of refresh operations can be skipped. Second, Partial Array Auto-Refresh (PAAR) eliminates the refresh operations to DRAM regions that do not store any data. We propose three RTC designs (min-RTC, mid-RTC, and full-RTC), each of which requires a different level of aggressiveness in terms of customization to the DRAM subsystem. All of our designs have small overhead. Even the most aggressive RTC design (i.e., full-RTC) imposes an area overhead of only 0.18% in a 16 Gb DRAM chip and can have less overhead for denser chips. Our experimental evaluation on six well-known CNNs shows that RTC reduces average DRAM energy consumption by 24.4% and 61.3% for the least aggressive and the most aggressive RTC implementations, respectively. Besides CNNs, we also evaluate our RTC mechanism on three workloads from other domains. We show that RTC saves 31.9% and 16.9% DRAM energy for Face Recognition and Bayesian Confidence Propagation Neural Network (BCPNN), respectively. We believe RTC can be applied to other applications whose memory access patterns remain predictable for a sufficiently long time.  © 2020 Owner/Author.",convolution neural networks; DRAM; DRAM refresh overhead,Backpropagation; Convolution; Data transfer; Dynamic random access storage; Energy efficiency; Energy utilization; Face recognition; Memory architecture; Network architecture; Petroleum reservoir evaluation; Accelerator architectures; Efficiency and performance; Energy efficient; Energy-constrained; Experimental evaluation; High energy consumption; Memory access patterns; New mechanisms; Convolutional neural networks
SPX64: A Scratchpad Memory for General-purpose Microprocessors,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099784949&doi=10.1145%2f3436730&partnerID=40&md5=90f06dd4e7951215f4df31ad7b555836,"General-purpose computing systems employ memory hierarchies to provide the appearance of a single large, fast, coherent memory. In special-purpose CPUs, programmers manually manage distinct, non-coherent scratchpad memories. In this article, we combine these mechanisms by adding a virtually addressed, set-associative scratchpad to a general purpose CPU. Our scratchpad exists alongside a traditional cache and is able to avoid many of the programming challenges associated with traditional scratchpads without sacrificing generality (e.g., virtualization). Furthermore, our design delivers increased security and improves performance, especially for workloads with high locality or that interact with nonvolatile memory.  © 2020 ACM.",cache; persistent memory; Scratchpad memory; security; software managed memory,Program processors; General-purpose computing; General-purpose microprocessors; Memory hierarchy; Non-volatile memory; Scratch pad memory; Scratchpad; Set-associative; Memory architecture
SGXL: Security and Performance for Enclaves Using Large Pages,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099786932&doi=10.1145%2f3433983&partnerID=40&md5=2cafb4832d8cdad391d88529e52caeec,"Intel's SGX architecture offers clients of public cloud computing platforms the ability to create hardware-protected enclaves whose contents are protected from privileged system software. However, SGX relies on system software for enclave memory management. In a sequence of recent papers, researchers have demonstrated that this reliance allows a malicious OS/hypervisor to snoop on the page addresses being accessed from within an enclave via various channels. This page address stream can then be used to infer secrets if the enclave's page access pattern depends upon the secret and this constitutes an important class of side-channels. We propose SGXL, a hardware-software co-designed system that significantly increases the difficulty of any page address-based side-channels through the use of large pages. A large page maps address ranges at a much larger granularity than the default page size (at least 512× larger). SGXL thus significantly lowers resolution of the leaked page address stream and could practically throttle all flavors of page-address based side-channels. We detail the modifications needed to SGX's software stack and the (minor) hardware enhancements required for SGXL to guarantee the use of large pages in the presence of adversarial system software. We empirically show that SGXL could be one of those rare systems that enhances security with the potential of improving performance as well.  © 2020 ACM.",enclaves; Intel SGX; large pages; page-based side-channel attacks,Computer software; Access patterns; Adversarial system; Hardware software co-designed; Improving performance; Memory management; Security and performance; System softwares; Various Channels; Hardware-software codesign
Performance-Energy Trade-off in Modern CMPs,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099783955&doi=10.1145%2f3427092&partnerID=40&md5=a12230727a61e74b16eebb9e027137d6,"Chip multiprocessors (CMPs) are ubiquitous in all computing systems ranging from high-end servers to mobile devices. In these systems, energy consumption is a critical design constraint as it constitutes the most significant operating cost for computing clouds. Analogous to this, longer battery life continues to be an essential user concern in mobile devices. To optimize on power consumption, modern processors are designed with Dynamic Voltage and Frequency Scaling (DVFS) support at the individual core as well as the uncore level. This allows fine-grained control of performance and energy. For an n core processor with m core and uncore frequency choices, the total DVFS configuration space is now m(n+1) (with the uncore accounting for the + 1). In addition to that, in CMPs, the performance-energy trade-off due to core/uncore frequency scaling concerning a single application cannot be determined independently as cores share critical resources like the last level cache (LLC) and the memory. Thus, unlike the uni-processor environment, the energy consumption of an application running on a CMP depends not only on its characteristics but also on those of its co-runners (applications running on other cores). The key objective of our work is to select a suitable core and uncore frequency that minimizes power consumption while limiting application performance degradation within certain pre-defined limits (can be termed as QoS requirements). The key contribution of our work is a learning-based model that is able to capture the interference due to shared cache, bus bandwidth, and memory bandwidth between applications running on multiple cores and predict near-optimal frequencies for core and uncore.  © 2020 ACM.",machine learning; performance-energy trade-off; Resource contention,Bandwidth; Cache memory; Economic and social effects; Electric power utilization; Energy utilization; Green computing; Quality of service; Ubiquitous computing; Voltage scaling; Application performance; Chip multi-processors (CMPs); Configuration space; Critical resources; Dynamic voltage and frequency scaling; Fine-grained control; Lastlevel caches (LLC); Learning Based Models; Dynamic frequency scaling
Efficient Nearest-Neighbor Data Sharing in GPUs,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099788162&doi=10.1145%2f3429981&partnerID=40&md5=0300dd9c87014bf63000e4dcd18cdd6c,"Stencil codes (a.k.a. nearest-neighbor computations) are widely used in image processing, machine learning, and scientific applications. Stencil codes incur nearest-neighbor data exchange because the value of each point in the structured grid is calculated as a function of its value and the values of a subset of its nearest-neighbor points. When running on Graphics Processing Unit (GPUs), stencil codes exhibit a high degree of data sharing between nearest-neighbor threads. Sharing is typically implemented through shared memories, shuffle instructions, and on-chip caches and often incurs performance overheads due to the redundancy in memory accesses. In this article, we propose Neighbor Data (NeDa), a direct nearest-neighbor data sharing mechanism that uses two registers embedded in each streaming processor (SP) that can be accessed by nearest-neighbor SP cores. The registers are compiler-allocated and serve as a data exchange mechanism to eliminate nearest-neighbor shared accesses. NeDa is embedded carefully with local wires between SP cores so as to minimize the impact on density. We place and route NeDa in an open-source GPU and show a small area overhead of 1.3%. The cycle-accurate simulation indicates an average performance improvement of 21.8% and power reduction of up to 18.3% for stencil codes in General-Purpose Graphics Processing Unit (GPGPU) standard benchmark suites. We show that NeDa's performance is within 13.2% of an ideal GPU with no overhead for nearest-neighbor data exchange.  © 2020 ACM.",data re-usage; data sharing; GPUs; Nearest-neighbor computations,Benchmarking; Cache memory; Codes (symbols); Computer graphics; Computer graphics equipment; Electronic data interchange; Graphics processing unit; Image processing; Program processors; Benchmark suites; Cycle-accurate simulation; Exchange mechanism; General purpose graphics processing unit (GPGPU); Nearest neighbors; Power reductions; Scientific applications; Sharing mechanism; Data Sharing
Leveraging Value Equality Prediction for Value Speculation,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099779150&doi=10.1145%2f3436821&partnerID=40&md5=8a5e082bad1897e563eb6d7953eb64eb,"Value Prediction (VP) has recently been gaining interest in the research community, since prior work has established practical solutions for its implementation that provide meaningful performance gains. A constant challenge of contemporary context-based value predictors is to sufficiently capture value redundancy and exploit the predictable execution paths. To do so, modern context-based VP techniques tightly associate recurring values with instructions and contexts by building confidence upon them after a plethora of repetitions. However, when execution monotony exists in the form of intervals, the potential prediction coverage is limited, since prediction confidence is reset at the beginning of each new interval. In this study, we address this challenge by introducing the notion of Equality Prediction (EP), which represents the binary facet of VP. Following a twofold decision scheme (similar to branch prediction), at fetch time, EP makes use of control-flow history to predict equality between the last committed result for this instruction and the result of the currently fetched occurrence. When equality is predicted with high confidence, the last committed value is used. Our simulation results show that this technique obtains the same level of performance as previously proposed state-of-the-art context-based value predictors. However, by virtue of exploiting equality patterns that are not captured by previous VP schemes, our design can improve the speedup of standard VP by 19% on average, when combined with contemporary prediction models.  © 2020 ACM.",equality prediction; general-purpose processor; hardware speculation; Microarchitecture; physical register sharing; value prediction,Forecasting; Branch prediction; Performance Gain; Practical solutions; Prediction confidence; Prediction model; Research communities; Value prediction; Value speculation; Predictive analytics
A Distributed Hardware Monitoring System for Runtime Verification on Multi-Tile MPSoCs,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099791282&doi=10.1145%2f3430699&partnerID=40&md5=e261ae4b75726dc8253f1fda7c76c28a,"Exhaustive verification techniques do not scale with the complexity of today's multi-tile Multi-processor Systems-on-chip (MPSoCs). Hence, runtime verification (RV) has emerged as a complementary method, which verifies the correct behavior of applications executed on the MPSoC during runtime. In this article, we propose a decentralized monitoring architecture for large-scale multi-tile MPSoCs. In order to minimize performance and power overhead for RV, we propose a lightweight and non-intrusive hardware solution. It features a new specialized tracing interconnect that distributes and sorts detected events according to their timestamps. Each tile monitor has a consistent view on a globally sorted trace of events on which the behavior of the target application can be verified using logical and timing requirements. Furthermore, we propose an integer linear programming-based algorithm for the assignment of requirements to monitors to exploit the local resources best. The monitoring architecture is demonstrated for a four-tiled MPSoC with 20 cores implemented on a Virtex-7 field-programmable gate array (FPGA).  © 2020 ACM.",LTL; MPSoCs; networks-on-chip; Runtime verification; tracing,Field programmable gate arrays (FPGA); Integer programming; Multiprocessing systems; Complementary methods; Decentralized monitoring; Distributed hardware; Integer Linear Programming; Monitoring architecture; Multi processor systems; Run-time verification; Verification techniques; System-on-chip
Irregular Register Allocation for Translation of Test-pattern Programs,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099789480&doi=10.1145%2f3427378&partnerID=40&md5=10215dd0cf614bc6fe0e8380a117bebb,"Test-pattern programs are for testing DRAM memory chips. They run on a special embedded system called automated test equipment (ATE). Each ATE manufacturer provides its own programming language, which is mostly low level, thus accessing the registers in the ATE directly. The register structure of each ATE is quite different and highly irregular. Since DRAM chipmakers are often equipped with diverse ATEs from different manufacturers, they employ automatic translation of a program developed for one ATE to a program for different ATEs. This raises an irregular register allocation problem during translation. This article proposes a solution based on partitioned Boolean quadratic programming (PBQP). PBQP has been used for a number of compiler optimizations, including paired register allocation, which our ATE register allocation also requires. Moreover, the interleaved processing in ATE incurs complex register constraints, which we could also formulate elegantly with PBQP. The original PBQP solver is not quite appropriate to use, though, since ATE register allocation does not allow spills, so we devised a more elaborate PBQP solver that trades off the allocation time and allocation search space, to find a solution in a reasonable amount of time. Our experimental results with product-level pattern programs show that the proposed register allocator successfully finds valid solutions in all cases, in the order of tenths of seconds.  © 2020 ACM.",automated test equipment; Irregular register allocation; memory testing; partitioned Boolean quadratic programming; test-pattern programs,Dynamic random access storage; Equipment testing; Manufacture; Quadratic programming; Automated test equipment; Automatic translation; Compiler optimizations; Dram memory; Register allocation; Register constraint; Search spaces; Test Pattern; Program translators
Exploiting Parallelism Opportunities with Deep Learning Frameworks,2021,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099780667&doi=10.1145%2f3431388&partnerID=40&md5=8d98e247db6475950a2c502d1c5a10ad,"State-of-the-art machine learning frameworks support a wide variety of design features to enable a flexible machine learning programming interface and to ease the programmability burden on machine learning developers. Identifying and using a performance-optimal setting in feature-rich frameworks, however, involves a non-trivial amount of performance profiling efforts and often relies on domain-specific knowledge. This article takes a deep dive into analyzing the performance impact of key design features in a machine learning framework and quantifies the role of parallelism. The observations and insights distill into a simple set of guidelines that one can use to achieve much higher training and inference speedup. Across a diverse set of real-world deep learning models, the evaluation results show that the proposed performance tuning guidelines outperform the Intel and TensorFlow recommended settings by 1.30× and 1.38×, respectively.  © 2020 ACM.",Machine learning frameworks; parallel computing; performance analysis,Interface states; Learning systems; Turing machines; Domain-specific knowledge; Evaluation results; Flexible machines; Learning frameworks; Learning models; Performance impact; Performance tuning; State of the art; Deep learning
LLOV: A Fast Static Data-Race Checker for OpenMP Programs,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098873617&doi=10.1145%2f3418597&partnerID=40&md5=a814c0e0aa1b84f5996f481470dd6dde,"In the era of Exascale computing, writing efficient parallel programs is indispensable, and, at the same time, writing sound parallel programs is very difficult. Specifying parallelism with frameworks such as OpenMP is relatively easy, but data races in these programs are an important source of bugs. In this article, we propose LLOV, a fast, lightweight, language agnostic, and static data race checker for OpenMP programs based on the LLVM compiler framework. We compare LLOV with other state-of-the-art data race checkers on a variety of well-established benchmarks. We show that the precision, accuracy, and the F1 score of LLOV is comparable to other checkers while being orders of magnitude faster. To the best of our knowledge, LLOV is the only tool among the state-of-the-art data race checkers that can verify a C/C++ or FORTRAN program to be data race free. © 2020 ACM.",data race detection; OpenMP; polyhedral compilation; program verification; shared memory programming; static analysis,Application programming interfaces (API); Program compilers; Program debugging; Exascale computing; FORTRAN programs; LLVM compilers; OpenMP programs; Orders of magnitude; Parallel program; State of the art; Static datum; C++ (programming language)
IR2Vec: LLVM IR Based Scalable Program Embeddings,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098870321&doi=10.1145%2f3418463&partnerID=40&md5=a137d37f2ec0c45ad5f31e65125d6a10,"We propose IR2VEC, a Concise and Scalable encoding infrastructure to represent programs as a distributed embedding in continuous space. This distributed embedding is obtained by combining representation learning methods with flow information to capture the syntax as well as the semantics of the input programs. As our infrastructure is based on the Intermediate Representation (IR) of the source code, obtained embeddings are both language and machine independent. The entities of the IR are modeled as relationships, and their representations are learned to form a seed embedding vocabulary. Using this infrastructure, we propose two incremental encodings: Symbolic and Flow-Aware. Symbolic encodings are obtained from the seed embedding vocabulary, and Flow-Aware encodings are obtained by augmenting the Symbolic encodings with the flow information. We show the effectiveness of our methodology on two optimization tasks (Heterogeneous device mapping and Thread coarsening). Our way of representing the programs enables us to use non-sequential models resulting in orders of magnitude of faster training time. Both the encodings generated by IR2VEC outperform the existing methods in both the tasks, even while using simple machine learning models. In particular, our results improve or match the state-of-the-art speedup in 11/14 benchmark-suites in the device mapping task across two platforms and 53/68 benchmarks in the thread coarsening task across four different platforms. When compared to the other methods, our embeddings are more scalable, is non-data-hungry, and has better Out-Of-Vocabulary (OOV) characteristics. © 2020 ACM.",compiler optimizations; heterogeneous systems; intermediate representations; LLVM; representation learning,Coarsening; Embeddings; Encoding (symbols); Machinery; Mapping; Ostwald ripening; Semantics; Continuous spaces; Flow informations; Heterogeneous devices; Intermediate representations; Machine learning models; Optimization task; Orders of magnitude; Scalable encoding; Learning systems
NNBench-X: A Benchmarking Methodology for Neural Network Accelerator Designs,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097250170&doi=10.1145%2f3417709&partnerID=40&md5=41e01a213e20c5b3c222715b93892224,"The tremendous impact of deep learning algorithms over a wide range of application domains has encouraged a surge of neural network (NN) accelerator research. Facilitating the NN accelerator design calls for guidance from an evolving benchmark suite that incorporates emerging NN models. Nevertheless, existing NN benchmarks are not suitable for guiding NN accelerator designs. These benchmarks are either selected for general-purpose processors without considering unique characteristics of NN accelerators or lack quantitative analysis to guarantee their completeness during the benchmark construction, update, and customization. In light of the shortcomings of prior benchmarks, we propose a novel benchmarking methodology for NN accelerators with a quantitative analysis of application performance features and a comprehensive awareness of software-hardware co-design. Specifically, we decouple the benchmarking process into three stages: First, we characterize the NN workloads with quantitative metrics and select the representative applications for the benchmark suite to ensure diversity and completeness. Second, we refine the selected applications according to the customized model compression techniques provided by specific software-hardware co-design. Finally, we evaluate a variety of accelerator designs on the generated benchmark suite. To demonstrate the effectiveness of our benchmarking methodology, we conduct a case study of composing an NN benchmark from the TensorFlow Model Zoo and compress these selected models with various model compression techniques. Finally, we evaluate compressed models on various architectures, including GPU, Neurocube, DianNao, and Cambricon-X. © 2020 ACM.",accelerator; benchmark; Neural networks; software-hardware co-designs,Acceleration; Application programs; Deep learning; General purpose computers; Hardware-software codesign; Learning algorithms; Neural networks; Accelerator design; Application performance; Benchmarking methodology; Benchmarking process; General purpose processors; Model compression; Neural network (nn); Quantitative metrics; Benchmarking
OD-SGD: One-Step Delay Stochastic Gradient Descent for Distributed Training,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097252271&doi=10.1145%2f3417607&partnerID=40&md5=4a9ad2e9fe34fcb6e2beae467df99f70,"The training of modern deep learning neural network calls for large amounts of computation, which is often provided by GPUs or other specific accelerators. To scale out to achieve faster training speed, two update algorithms are mainly applied in the distributed training process, i.e., the Synchronous SGD algorithm (SSGD) and Asynchronous SGD algorithm (ASGD). SSGD obtains good convergence point while the training speed is slowed down by the synchronous barrier. ASGD has faster training speed but the convergence point is lower when compared to SSGD. To sufficiently utilize the advantages of SSGD and ASGD, we propose a novel technology named One-step Delay SGD (OD-SGD) to combine their strengths in the training process. Therefore, we can achieve similar convergence point and training speed as SSGD and ASGD separately. To the best of our knowledge, we make the first attempt to combine the features of SSGD and ASGD to improve distributed training performance. Each iteration of OD-SGD contains a global update in the parameter server node and local updates in the worker nodes, the local update is introduced to update and compensate the delayed local weights. We evaluate our proposed algorithm on MNIST, CIFAR-10, and ImageNet datasets. Experimental results show that OD-SGD can obtain similar or even slightly better accuracy than SSGD, while its training speed is much faster, which even exceeds the training speed of ASGD. © 2020 ACM.",compensation; convergence speed; distributed performance; one-step delay; Overlap ratio,Deep learning; Deep neural networks; Gradient methods; Neural networks; Program processors; Stochastic systems; Convergence points; Large amounts; Learning neural networks; One step delay; Stochastic gradient descent; Training process; Training speed; Update algorithms; Speed
SHASTA: Synergic HW-SW Architecture for Spatio-temporal Approximation,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097249138&doi=10.1145%2f3412375&partnerID=40&md5=d2a3f4fd8d276118fa18d8bc25c93bd4,"A key requirement for efficient general purpose approximate computing is an amalgamation of flexible hardware design and intelligent application tuning, which together can leverage the appropriate amount of approximation that the applications engender and reap the best efficiency gains from them. To achieve this, we have identified three important features to build better general-purpose cross-layer approximation systems: Individual per-operation (""spatio-temporally fine-grained"") approximation, hardware-cognizant application tuning for approximation, systemwide approximation-synergy. We build an efficient general purpose approximation system called SHASTA: Synergic HW-SW Architecture for Spatio-Temporal Approximation, to achieve these goals. First, in terms of hardware, SHASTA approximates both compute and memory-SHASTA proposes (a) a form of timing approximation called Slack-control Approximation, which controls the computation timing of each approximation operation and (b) a Dynamic Pre-L1 Load Approximation mechanism to approximate loads prior to cache access. These hardware mechanisms are designed to achieve fine-grained spatio-temporally diverse approximation. Next, SHASTA proposes a Hardware-cognizant Approximation Tuning mechanism to tune an application's approximation to achieve the optimum execution efficiency under the prescribed error tolerance. The tuning mechanism is implemented atop a gradient descent algorithm and, thus, the application's approximation is tuned along the steepest error vs. execution efficiency gradient. Finally, SHASTA is designed with a full-system perspective, which achieves Synergic benefits across its optimizations, building a closer-to-ideal general purpose approximation system. SHASTA is implemented on top of an OOO core and achieves mean speedups/energy savings of 20%-40% over a non-approximate baseline for greater than 90% accuracy-these benefits are substantial for applications executing on a traditional general purpose processing system. SHASTA can be tuned to specific accuracy constraints and execution metrics and is quantitatively shown to achieve 2-15× higher benefits, in terms of performance and energy, compared to prior work. © 2020 ACM.",Approximate computing; approximation tuning; general purpose systems; gradient descent; load approximation; timing approximation,Gradient methods; Memory architecture; Metals; Optimization; Approximation operations; Gradient descent algorithms; Hardware mechanism; Important features; Intelligent applications; Processing systems; Spatio temporal; Tuning mechanism; Approximation algorithms
A RISC-V Simulator and Benchmark Suite for Designing and Evaluating Vector Architectures,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097218957&doi=10.1145%2f3422667&partnerID=40&md5=9c47cb11a388ea558fedfd9f59617997,"Vector architectures lack tools for research. Consider the gem5 simulator, which is possibly the leading platform for computer-system architecture research. Unfortunately, gem5 does not have an available distribution that includes a flexible and customizable vector architecture model. In consequence, researchers have to develop their own simulation platform to test their ideas, which consume much research time. However, once the base simulator platform is developed, another question is the following: Which applications should be tested to perform the experiments? The lack of Vectorized Benchmark Suites is another limitation. To face these problems, this work presents a set of tools for designing and evaluating vector architectures. First, the gem5 simulator was extended to support the execution of RISC-V Vector instructions by adding a parameterizable Vector Architecture model for designers to evaluate different approaches according to the target they pursue. Second, a novel Vectorized Benchmark Suite is presented: A collection composed of seven data-parallel applications from different domains that can be classified according to the modules that are stressed in the vector architecture. Finally, a study of the Vectorized Benchmark Suite executing on the gem5-based Vector Architecture model is highlighted. This suite is the first in its category that covers the different possible usage scenarios that may occur within different vector architecture designs such as embedded systems, mainly focused on short vectors, or High-Performance-Computing (HPC), usually designed for large vectors. © 2020 Owner/Author.",benchmarking; gem5; High-performance computer architecture; vector architectures; vectorization,Benchmarking; Computer architecture; Embedded systems; Simulators; Benchmark suites; Data-parallel applications; Different domains; High performance computing (HPC); Short vectors; System architectures; Usage scenarios; Vector architectures; Vectors
SMAUG: End-to-End Full-Stack Simulation Infrastructure for Deep Learning Workloads,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097241177&doi=10.1145%2f3424669&partnerID=40&md5=112a03597a4e107ba0c6b3176e7d1194,"In recent years, there has been tremendous advances in hardware acceleration of deep neural networks. However, most of the research has focused on optimizing accelerator microarchitecture for higher performance and energy efficiency on a per-layer basis. We find that for overall single-batch inference latency, the accelerator may only make up 25-40%, with the rest spent on data movement and in the deep learning software framework. Thus far, it has been very difficult to study end-to-end DNN performance during early stage design (before RTL is available), because there are no existing DNN frameworks that support end-to-end simulation with easy custom hardware accelerator integration. To address this gap in research infrastructure, we present SMAUG, the first DNN framework that is purpose-built for simulation of end-to-end deep learning applications. SMAUG offers researchers a wide range of capabilities for evaluating DNN workloads, from diverse network topologies to easy accelerator modeling and SoC integration. To demonstrate the power and value of SMAUG, we present case studies that show how we can optimize overall performance and energy efficiency for up to 1.8×-5× speedup over a baseline system, without changing any part of the accelerator microarchitecture, as well as show how SMAUG can tune an SoC for a camera-powered deep learning pipeline. © 2020 Owner/Author.",architectural simulation; Deep neural networks; hardware accelerators,Acceleration; Computer architecture; Computer programming; Deep neural networks; Energy efficiency; Mobile telecommunication systems; Pipeline processing systems; System-on-chip; Custom hardwares; Early stage designs; End-to-end simulation; Hardware acceleration; Learning software; Micro architectures; Network topology; Research infrastructure; Deep learning
Design and Evaluation of an Ultra Low-power Human-quality Speech Recognition System,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097238221&doi=10.1145%2f3425604&partnerID=40&md5=b942be26a54982aae946d728fc2a3b3e,"Automatic Speech Recognition (ASR) has experienced a dramatic evolution since pioneer development of Bell Lab's single-digit recognizer more than 50 years ago. Current ASR systems have taken advantage of the tremendous improvements in AI during the past decade by incorporating Deep Neural Networks into the system and pushing their accuracy to levels comparable to that of humans. This article describes and characterizes a representative ASR system with state-of-the-art accuracy and proposes a hardware platform capable of decoding speech in real-time with a power dissipation close to 1 Watt. The software is based on the so-called hybrid approach with a vocabulary of 200K words and RNN-based language model re-scoring, whereas the hardware consists of a commercially available low-power processor along with two accelerators used for the most compute-intensive tasks. The article shows that high performance can be obtained with very low power, enabling the deployment of these systems in extremely power-constrained environments such as mobile and IoT devices. © 2020 ACM.",Hardware accelerators; low-power hardware,Deep neural networks; Recurrent neural networks; Automatic speech recognition; Compute-intensive tasks; Design and evaluations; Digit recognizers; Hardware platform; Low power processors; Speech recognition systems; State of the art; Speech recognition
Effective Loop Fusion in Polyhedral Compilation Using Fusion Conflict Graphs,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097227295&doi=10.1145%2f3416510&partnerID=40&md5=2b6eb47f835f05313f03a69830bb35a6,"Polyhedral auto-transformation frameworks are known to find efficient loop transformations that maximize locality and parallelism and minimize synchronization. While complex loop transformations are routinely modeled in these frameworks, they tend to rely on ad hoc heuristics for loop fusion. Although there exist multiple loop fusion models with cost functions to maximize locality and parallelism, these models involve separate optimization steps rather than seamlessly integrating with other loop transformations like loop permutation, scaling, and shifting. Incorporating parallelism-preserving loop fusion heuristics into existing affine transformation frameworks like Pluto, LLVM-Polly, PPCG, and PoCC requires solving a large number of Integer Linear Programming formulations, which increase auto-transformation times significantly. In this work, we incorporate polynomial time loop fusion heuristics into the Pluto-lp-dfp framework. We present a data structure called the fusion conflict graph (FCG), which enables us to efficiently model loop fusion in the presence of other affine loop transformations. We propose a clustering heuristic to group the vertices of the FCG, which further enables us to provide three different polynomial time greedy fusion heuristics, namely, maximal fusion, typed fusion, and hybrid fusion, while maintaining the compile time improvements of Pluto-lp-dfp over Pluto. Our experiments reveal that the hybrid fusion model, in conjunction with Pluto's cost function, finds efficient transformations that outperform PoCC and Pluto by mean factors of 1.8× and 1.07×, respectively, with a maximum performance improvement of 14× over PoCC and 2.6× over Pluto. © 2020 ACM.",Affine transformations; fusion conflict graph; pluto algorithm,Cost functions; Integer programming; Mathematical transformations; Polynomial approximation; Ad-hoc heuristics; Affine transformations; Conflict graph; Hybrid fusions; Integer linear programming formulation; Loop transformation; Multiple loops; Polynomial-time; Linear transformations
MemSZ: Squeezing Memory Traffic with Lossy Compression,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097234982&doi=10.1145%2f3424668&partnerID=40&md5=81accf8f9f400eb8813904a4ffa0af9b,"This article describes Memory Squeeze (MemSZ), a new approach for lossy general-purpose memory compression. MemSZ introduces a low latency, parallel design of the Squeeze (SZ) algorithm offering aggressive compression ratios, up to 16:1 in our implementation. Our compressor is placed between the memory controller and the cache hierarchy of a processor to reduce the memory traffic of applications that tolerate approximations in parts of their data. Thereby, the available off-chip bandwidth is utilized more efficiently improving system performance and energy efficiency. Two alternative multi-core variants of the MemSZ system are described. The first variant has a shared last-level cache (LLC) on the processor-die, which is modified to store both compressed and uncompressed data. The second has a 3D-stacked DRAM cache with larger cache lines that match the granularity of the compressed memory blocks and stores only uncompressed data. For applications that tolerate aggressive approximation in large fractions of their data, MemSZ reduces baseline memory traffic by up to 81%, execution time by up to 62%, and energy costs by up to 25% introducing up to 1.8% error to the application output. Compared to the current state-of-the-art lossy memory compression design, MemSZ improves the execution time, energy, and memory traffic by up to 15%, 9%, and 64%, respectively. © 2020 ACM.",Approximate computing; lossy compression; memory compression,Dynamic random access storage; Energy efficiency; Integrated circuit design; Three dimensional integrated circuits; 3d-stacked drams; Cache hierarchies; Improving systems; Lastlevel caches (LLC); Lossy compressions; Memory compression; Memory controller; State of the art; Cache memory
AsynGraph: Maximizing Data Parallelism for Efficient Iterative Graph Processing on GPUs,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097223648&doi=10.1145%2f3416495&partnerID=40&md5=ed1a7f7981458290683861d9b157b14a,"Recently, iterative graph algorithms are proposed to be handled by GPU-accelerated systems. However, in iterative graph processing, the parallelism of GPU is still underutilized by existing GPU-based solutions. In fact, because of the power-law property of the natural graphs, the paths between a small set of important vertices (e.g., high-degree vertices) play a more important role in iterative graph processing's convergence speed. Based on this fact, for faster iterative graph processing on GPUs, this article develops a novel system, called AsynGraph, to maximize its data parallelism. It first proposes an efficient structure-aware asynchronous processing way. It enables the state propagations of most vertices to be effectively conducted on the GPUs in a concurrent way to get a higher GPU utilization ratio through efficiently handling the paths between the important vertices. Specifically, a graph sketch (consisting of the paths between the important vertices) is extracted from the original graph to serve as a fast bridge for most state propagations. Through efficiently processing this sketch more times within each round of graph processing, higher parallelism of GPU can be utilized to accelerate most state propagations. In addition, a forward-backward intra-path processing way is also adopted to asynchronously handle the vertices on each path, aiming to further boost propagations along paths and also ensure smaller data access cost. In comparison with existing GPU-based systems, i.e., Gunrock, Groute, Tigr, and DiGraph, AsynGraph can speed up iterative graph processing by 3.06-11.52, 2.47-5.40, 2.23-9.65, and 1.41-4.05 times, respectively. © 2020 ACM.",convergence speed; data parallelism; GPU; graph processing,Graph algorithms; Graphics processing unit; Iterative methods; Program processors; Asynchronous processing; Convergence speed; Data parallelism; GPU-accelerated; Graph processing; Path processing; Structure-aware; Utilization ratios; Graph theory
DisGCo: A Compiler for Distributed Graph Analytics,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097242643&doi=10.1145%2f3414469&partnerID=40&md5=4cefa24d2d161ee750e607e323a2dd5b,"Graph algorithms are widely used in various applications. Their programmability and performance have garnered a lot of interest among the researchers. Being able to run these graph analytics programs on distributed systems is an important requirement. Green-Marl is a popular Domain Specific Language (DSL) for coding graph algorithms and is known for its simplicity. However, the existing Green-Marl compiler for distributed systems (Green-Marl to Pregel) can only compile limited types of Green-Marl programs (in Pregel canonical form). This severely restricts the types of parallel Green-Marl programs that can be executed on distributed systems. We present DisGCo, the first compiler to translate any general Green-Marl program to equivalent MPI program that can run on distributed systems. Translating Green-Marl programs to MPI (SPMD/MPMD style of computation, distributed memory) presents many other exciting challenges, besides the issues related to differences in syntax, as Green-Marl gives the programmer a unified view of the whole memory and allows the parallel and serial code to be inter-mixed. We first present the set of challenges involved in translating Green-Marl programs to MPI and then present a systematic approach to do the translation. We also present a few optimization techniques to improve the performance of our generated programs. DisGCo is the first graph DSL compiler that can handle all syntactic capabilities of a practical graph DSL like Green-Marl and generate code that can run on distributed systems. Our preliminary evaluation of DisGCo shows that our generated programs are scalable. Further, compared to the state-of-the-art DH-Falcon compiler that translates a subset of Falcon programs to MPI, our generated codes exhibit a geomean speedup of 17.32×. © 2020 ACM.",distributed programming; Graph analytics; GreenMarl,Codes (symbols); Digital subscriber lines; Distributed computer systems; Distributed database systems; Fertilizers; Graph algorithms; Problem oriented languages; Program translators; Syntactics; Canonical form; Distributed Memory; Distributed systems; Domain specific languages; Graph analytics; Optimization techniques; Programmability; State of the art; Program compilers
FastPath_MP: Low Overhead and Energy-efficient FPGA-based Storage Multi-paths,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097234591&doi=10.1145%2f3423134&partnerID=40&md5=7604b358ff47eb0213f6f6a37c5ac35e,"In this article, we present FastPath_MP, a novel low-overhead and energy-efficient storage multi-path architecture that leverages FPGAs to operate transparently to the main processor and improve the performance and energy efficiency of accessing storage devices. We prototyped FastPath_MP on both Arm-FPGA Zynq 7000 SoC and Zynq UltraScale+ MPSoC and evaluated its performance against standard microbenchmarks as well as the real-world in-memory Redis database. Our results show that FastPath_MP achieves up to 82% lower latency, up to 12× higher throughput, and up to 10× more energy efficiency against the baseline storage path of the Linux kernel. © 2020 Owner/Author.",block I/O; FPGA; NVMe; SSDs; storage path; system on a chip,Computer operating systems; Field programmable gate arrays (FPGA); System-on-chip; Virtual storage; Energy efficient; Linux kernel; Low overhead; Micro-benchmarks; Multipaths; Real-world; Energy efficiency
A Black-box Monitoring Approach to Measure Microservices Runtime Performance,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097221697&doi=10.1145%2f3418899&partnerID=40&md5=a35a5b1cfd13bccdec489a30d18ccd45,"Microservices changed cloud computing by moving the applications' complexity from one monolithic executable to thousands of network interactions between small components. Given the increasing deployment sizes, the architectural exploitation challenges, and the impact on data-centers' power consumption, we need to efficiently track this complexity. Within this article, we propose a black-box monitoring approach to track microservices at scale, focusing on architectural metrics, power consumption, application performance, and network performance. The proposed approach is transparent w.r.t. the monitored applications, generates less overhead w.r.t. black-box approaches available in the state-of-the-art, and provides fine-grain accurate metrics. © 2020 ACM.",cloud computing; docker; kubernetes; Microservices; network performance monitoring; performance monitoring; power attribution,Complex networks; Electric power utilization; Application performance; Black box approach; Data centers; Monitoring approach; Network interaction; Run-time performance; Small components; State of the art; Green computing
On Architectural Support for Instruction Set Randomization,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097224936&doi=10.1145%2f3419841&partnerID=40&md5=444644cebed24b8e59c2b962b83aff94,"Instruction Set Randomization (ISR) is able to protect against remote code injection attacks by randomizing the instruction set of each process. Thereby, even if an attacker succeeds to inject code, it will fail to execute on the randomized processor. The majority of existing ISR implementations is based on emulators and binary instrumentation tools that unfortunately: (i) incur significant runtime performance overheads, (ii) limit the ease of deployment, (iii) cannot protect the underlying operating system kernel, and (iv) are vulnerable to evasion attempts that bypass the ISR protection itself. To address these issues, we present the design and implementation of ASIST, an architecture with both hardware and operating system support for ISR. ASIST uses our extended SPARC processor that is mapped onto a FPGA board and runs our modified Linux kernel to support the new features. In particular, before executing a new user-level process, the operating system loads its randomization key into a newly defined register, and the modified processor decodes the process's instructions with this key. Besides that, ASIST uses a separate randomization key for the operating system to protect the base system against attacks that exploit kernel vulnerabilities to run arbitrary code with elevated privileges. Our evaluation shows that ASIST can transparently protect both user-land applications and the operating system kernel from code injection and code reuse attacks, with about 1.5% runtime overhead when using simple encryption schemes, such as XOR and Transposition; more secure ciphers, such as AES, even though they are much more complicated for mapping them to hardware, they are still within acceptable margins,with approximately 10% runtime overhead, when efficiently leveraging the spatial locality of code through modern instruction cache configurations. © 2020 ACM.",Code injection; hardware assisted security; instruction set randomization,Codes (symbols); Linux; Random processes; Architectural support; Binary instrumentations; Design and implementations; Encryption schemes; Instruction-set randomization; Operating system kernel; Operating system support; Run-time performance; Cryptography
EcoTLB: Eventually Consistent TLBs,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097232656&doi=10.1145%2f3409454&partnerID=40&md5=ff623bea3774048abb5299252a828414,"We propose ecoTLB-software-based eventual translation lookaside buffer (TLB) coherence-which eliminates the overhead of the synchronous TLB shootdown mechanism in operating systems that use address space identifiers (ASIDs). With an eventual TLB coherence, ecoTLB improves the performance of free and page swap operations by removing the inter-processor interrupt (IPI) overheads incurred to invalidate TLB entries. We show that the TLB shootdown has implications for page swapping in particular in emerging, disaggregated data centers and demonstrate that ecoTLB can improve both the performance and the specific swapping policy decisions using ecoTLB's asynchronous mechanism. We demonstrate that ecoTLB improves the performance of real-world applications, such as Memcached and Make, that perform page swapping using Infiniswap, a solution for next generation data centers that use disaggregated memory, by up to 17.2%. Moreover, ecoTLB improves the 99th percentile tail latency of Memcached by up to 70.8% due to its asynchronous scheme and improved policy decisions. Furthermore, we show that recent features to improve security in the Linux kernel, like kernel page table isolation (KPTI), can result in significant performance overheads on architectures without support for specific instructions to clear single entries in tagged TLBs, falling back to full TLB flushes. In this scenario, ecoTLB is able to recover the performance lost for supporting KPTI due to its asynchronous shootdown scheme and its support for tagged TLBs. Finally, we demonstrate that ecoTLB improves the performance of free operations by up to 59.1% on a 120-core machine and improves the performance of Apache on a 16-core machine by up to 13.7% compared to baseline Linux, and by up to 48.2% compared to ABIS, a recent state-of-the-art research prototype that reduces the number of IPIs. © 2020 ACM.",asynchrony; TLB; translation coherence,Decision making; Linux; Address space; Dis-aggregated memory; Interprocessors; Next generation data centers; Policy decisions; Specific instruction; Swapping policies; Translation lookaside buffer; Buffer storage
GEVO: GPU Code Optimization Using Evolutionary Computation,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097228723&doi=10.1145%2f3418055&partnerID=40&md5=fca017a5beabb90192d6f08472ff5cf7,"GPUs are a key enabler of the revolution in machine learning and high-performance computing, functioning as de facto co-processors to accelerate large-scale computation. As the programming stack and tool support have matured, GPUs have also become accessible to programmers, who may lack detailed knowledge of the underlying architecture and fail to fully leverage the GPU's computation power. GEVO (Gpu optimization using EVOlutionary computation) is a tool for automatically discovering optimization opportunities and tuning the performance of GPU kernels in the LLVM representation. GEVO uses population-based search to find edits to GPU code compiled to LLVM-IR and improves performance on desired criteria while retaining required functionality. We demonstrate that GEVO improves the execution time of general-purpose GPU programs and machine learning (ML) models on NVIDIA Tesla P100. For the Rodinia benchmarks, GEVO improves GPU kernel runtime performance by an average of 49.48% and by as much as 412% over the fully compiler-optimized baseline. If kernel output accuracy is relaxed to tolerate up to 1% error, GEVO can find kernel variants that outperform the baseline by an average of 51.08%. For the ML workloads, GEVO achieves kernel performance improvement for SVM on the MNIST handwriting recognition (3.24×) and the a9a income prediction (2.93×) datasets with no loss of model accuracy. GEVO achieves 1.79× kernel performance improvement on image classification using ResNet18/CIFAR-10, with less than 1% model accuracy reduction. © 2020 ACM.",approximate computing; Genetic improvement; GPU code optimization; LLVM intermediate representation; multi-objective evolutionary computation,Benchmarking; Character recognition; Codes (symbols); Image enhancement; Knowledge management; Learning systems; Program processors; Support vector machines; Code optimization; Computation power; General purpose gpu; Handwriting recognition; High performance computing; Large scale computation; Required functionalities; Run-time performance; Graphics processing unit
Inter-kernel Reuse-aware Thread Block Scheduling,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090412037&doi=10.1145%2f3406538&partnerID=40&md5=46d328742bce835d45ade31f5acf8723,"As GPUs have become more programmable, their performance and energy benefits have made them increasingly popular. However, while GPU compute units continue to improve in performance, on-chip memories lag behind and data accesses are becoming increasingly expensive in performance and energy. Emerging GPU coherence protocols can mitigate this bottleneck by exploiting data reuse in GPU caches across kernel boundaries. Unfortunately, current GPU thread block schedulers are typically not designed to expose such reuse. This article proposes new hardware thread block schedulers that optimize inter-kernel reuse while using work stealing to preserve load balance. Our schedulers are simple, decentralized, and have extremely low overhead. Compared to a baseline round-robin scheduler, the best performing scheduler reduces average execution time and energy by 19% and 11%, respectively, in regular applications, and 10% and 8%, respectively, in irregular applications. © 2020 ACM.",caches; GPUs; memory systems; scheduling,Computer hardware; Program processors; Average Execution Time; Block scheduling; Coherence protocol; Energy benefits; Hardware threads; Irregular applications; On chip memory; Round robin schedulers; Scheduling
EchoBay,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090419601&doi=10.1145%2f3404993&partnerID=40&md5=9f18dcc0949bf9bf100b2f8c76e445fd,"The increase in computational power of embedded devices and the latency demands of novel applications brought a paradigm shift on how and where the computation is performed. Although AI inference is slowly moving from the cloud to end-devices with limited resources, time-centric recurrent networks like Long-Short Term Memory remain too complex to be transferred on embedded devices without extreme simplifications and limiting the performance of many notable applications. To solve this issue, the Reservoir Computing paradigm proposes sparse, untrained non-linear networks, the Reservoir, that can embed temporal relations without some of the hindrances of Recurrent Neural Networks training, and with a lower memory occupation. Echo State Networks (ESN) and Liquid State Machines are the most notable examples. In this scenario, we propose EchoBay, a comprehensive C++ library for ESN design and training. EchoBay is architecture-agnostic to guarantee maximum performance on different devices (whether embedded or not), and it offers the possibility to optimize and tailor an ESN on a particular case study, reducing at the minimum the effort required on the user side. This can be done thanks to the Bayesian Optimization (BO) process, which efficiently and automatically searches hyper-parameters that maximize a fitness function. Additionally, we designed different optimization techniques that take in consideration resource constraints of the device to minimize memory footprint and inference time. Our results in different scenarios show an average speed-up in training time of 119x compared to Grid and Random search of hyper-parameters, a decrease of 94% of trained models size and 95% in inference time, maintaining comparable performance for the given task. The EchoBay library is Open Source and publicly available at https://github.com/necst/Echobay. © 2020 ACM.",Echo State Networks; EchoBay,C++ (programming language); Optimization; Bayesian optimization; Computational power; Echo state networks; Liquid state machines; Novel applications; Optimization techniques; Reservoir Computing; Resource Constraint; Recurrent neural networks
Schedule Synthesis for Halide Pipelines on GPUs,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090419210&doi=10.1145%2f3406117&partnerID=40&md5=00cffe6789ea3a816a226396718052be,"The Halide DSL and compiler have enabled high-performance code generation for image processing pipelines targeting heterogeneous architectures through the separation of algorithmic description and optimization schedule. However, automatic schedule generation is currently only possible for multi-core CPU architectures. As a result, expert knowledge is still required when optimizing for platforms with GPU capabilities. In this work, we extend the current Halide Autoscheduler with novel optimization passes to efficiently generate schedules for CUDA-based GPU architectures. We evaluate our proposed method across a variety of applications and show that it can achieve performance competitive with that of manually tuned Halide schedules, or in many cases even better performance. Experimental results show that our schedules are on average 10% faster than manual schedules and over 2× faster than previous autoscheduling attempts. © 2020 ACM.",GPU; Halide; image processing; Loop optimizations; scheduling,Graphics processing unit; Image processing; Pipelines; Program processors; Expert knowledge; Heterogeneous architectures; High performance codes; Image processing pipeline; Multi-core cpu architectures; Optimization schedule; Schedule generation; Computer architecture
GPU Fast Convolution via the Overlap-and-Save Method in Shared Memory,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090416104&doi=10.1145%2f3394116&partnerID=40&md5=c9e80ab60b1aa948b935ec51e63c78fa,"We present an implementation of the overlap-and-save method, a method for the convolution of very long signals with short response functions, which is tailored to GPUs. We have implemented several FFT algorithms (using the CUDA programming language), which exploit GPU shared memory, allowing for GPU accelerated convolution. We compare our implementation with an implementation of the overlap-and-save algorithm utilizing the NVIDIA FFT library (cuFFT). We demonstrate that by using a shared-memory-based FFT, we can achieved significant speed-ups for certain problem sizes and lower the memory requirements of the overlap-and-save method on GPUs. © 2020 Owner/Author.",CUDA; Fast convolution; FFT; GPU; overlap-and-save,Convolution; Fast Fourier transforms; Graphics processing unit; Program processors; Fast convolution; FFT algorithm; GPU-accelerated; Memory requirements; Problem size; Response functions; Shared memory; Memory architecture
Zeroploit: Exploiting Zero Valued Operands in Interactive Gaming Applications,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090413040&doi=10.1145%2f3394284&partnerID=40&md5=88a5b059cfd9923c1dcf3fce942a88ea,"In this article, we first characterize register operand value locality in shader programs of modern gaming applications and observe that there is a high likelihood of one of the register operands of several multiply, logical-and, and similar operations being zero, dynamically. We provide intuition, examples, and a quantitative characterization for how zeros originate dynamically in these programs. Next, we show that this dynamic behavior can be gainfully exploited with a profile-guided code optimization called Zeroploit that transforms targeted code regions into a zero-(value-)specialized fast path and a default slow path. The fast path benefits from zero-specialization in two ways, namely: (a) the backward slice of the other operand of a given multiply or logical-and can be skipped dynamically, provided the only use of that other operand is in the given instruction, and (b) the forward slice of instructions originating at the given instruction can be zero-specialized, potentially triggering further backward slice specializations from operations of that forward slice as well. Such specialization helps the fast path avoid redundant dynamic computations as well as memory fetches, while the fast-slow versioning transform helps preserve functional correctness. With an offline value profiler and manually optimized shader programs, we demonstrate that Zeroploit is able to achieve an average speedup of 35.8% for targeted shader programs, amounting to an average frame-rate speedup of 2.8% across a collection of modern gaming applications on an NVIDIA® GeForce RTX™ 2080 GPU. © 2020 ACM.",gaming applications; GPUs; Profile guided optimization; shader programs; value specialization,Software engineering; Code optimization; Dynamic behaviors; Dynamic computations; Functional correctness; Gaming applications; Interactive gaming; Quantitative characterization; Value locality; Application programs
Editorial: A Message from the Editor-in-Chief,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090414656&doi=10.1145%2f3409369&partnerID=40&md5=d064c0362104b36950c12fd4267e3c8f,[No abstract available],,
FPDetect: Efficient Reasoning about Stencil Programs Using Selective Direct Evaluation,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090411704&doi=10.1145%2f3402451&partnerID=40&md5=7278ad4f936c0c6870a87619c9225928,"We present FPDetect, a low-overhead approach for detecting logical errors and soft errors affecting stencil computations without generating false positives. We develop an offline analysis that tightly estimates the number of floating-point bits preserved across stencil applications. This estimate rigorously bounds the values expected in the data space of the computation. Violations of this bound can be attributed with certainty to errors. FPDetect helps synthesize error detectors customized for user-specified levels of accuracy and coverage. FPDetect also enables overhead reduction techniques based on deploying these detectors coarsely in space and time. Experimental evaluations demonstrate the practicality of our approach. © 2020 ACM.",affine analysis; floating point round-off error; interval analysis; silent data corruption; Soft error detection; software bug detection; stencil computations,Digital arithmetic; Radiation hardening; Direct evaluations; Error detectors; Experimental evaluation; Floating points; Off-line analysis; Overhead reductions; Space and time; Stencil computations; Errors
Cooperative Software-hardware Acceleration of K-means on a Tightly Coupled CPU-FPGA System,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090415687&doi=10.1145%2f3406114&partnerID=40&md5=e033517579f3156becffe37eba8c67f5,We consider software-hardware acceleration of K-means clustering on the Intel Xeon+FPGA platform. We design a pipelined accelerator for K-means and combine it with CPU threads to assess performance benefits of (1) acceleration when data are only accessed from system memory and (2) cooperative CPU-FPGA acceleration. Our evaluation shows that the accelerator is up to 12.7×/2.4× faster than a single CPU thread for the assignment/update step of K-means. The cooperative use of threads and FPGA is roughly 1.9× faster than CPU threads alone or the FPGA by itself. Our approach delivers 4×-5× higher throughput compared to existing offload processing approaches. © 2020 ACM.,FPGA-based acceleration; Heterogeneous acceleration; K-Means clustering; Performance evaluation; shared-memory CPU-FPGA systems,Acceleration; K-means clustering; Pipeline processing systems; Fpga platforms; Hardware acceleration; K-means; Performance benefits; Processing approach; Single CPU; System memory; Tightly-coupled; Field programmable gate arrays (FPGA)
Securing Branch Predictors with Two-Level Encryption,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090404609&doi=10.1145%2f3404189&partnerID=40&md5=0343390250ccd5c467a58332bd5bd1b7,"Modern processors rely on various speculative mechanisms to meet performance demand. Branch predictors are one of the most important micro-architecture components to deliver performance. However, they have been under heavy scrutiny because of recent side-channel attacks. Branch predictors are indexed using the PC and recent branch histories. An adversary can manipulate these parameters to access and control the same branch predictor entry that a victim uses. Recent Spectre attacks exploit this to set up speculative-execution-based security attacks. In this article, we aim to mitigate branch predictor side-channels using two-level encryption. At the first level, we randomize the set-index by encrypting the PC using a per-context secret key. At the second level, we encrypt the data in each branch predictor entry. While periodic key changes make the branch predictor more secure, performance degradation can be significant. To alleviate performance degradation, we propose a practical set update mechanism that also considers parallelism in multi-banked branch predictors. We show that our mechanism exhibits only 1.0% and 0.2% performance degradation while changing keys every 10K and 50K cycles, respectively, which is much lower than other state-of-the-art approaches. © 2020 ACM.",Branch predictor; encryption; side-channel,Access control; Computer architecture; Branch predictors; Micro architectures; Modern processors; Performance degradation; Security attacks; Speculative execution; State-of-the-art approach; Update mechanisms; Side channel attack
ArmorAll: Compiler-based Resilience Targeting GPU Applications,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088315304&doi=10.1145%2f3382132&partnerID=40&md5=4447c142ee93085ea9723f519b0ac48c,"The vulnerability of GPUs to soft errors has become a first-class design concern as they are increasingly being used in accuracy-sensitive and safety-critical domains. Existing solutions used to enhance the reliability of GPUs come with significant overhead in terms of area, power, and/or performance. In this article, we propose ArmorAll, a light-weight, adaptive, selective, and portable software solution to protect GPUs against soft errors. ArmorAll consists of a set of purely compiler-based redundancy schemes designed to optimize instruction duplication on GPUs, thereby enabling much more reliable execution. The choice of the scheme determines the subset of instructions that must be duplicated in an application, allowing adaptable fault coverage for different applications. ArmorAll can intelligently select a redundancy scheme that provides the best coverage to an application with an accuracy of 91.7%. The high coverage provided by ArmorAll comes at an average improvement of 64.5% in runtime when using the selected redundancy scheme as compared to the state-of-the-art.  © 2020 ACM.",fault tolerance; GPUs; LLVM; soft errors,Graphics processing unit; Program compilers; Radiation hardening; Fault coverages; Light weight; Portable software; Redundancy scheme; Reliable execution; Safety-critical domain; Soft error; State of the art; Redundancy
A Conflict-free Scheduler for High-performance Graph Processing on Multi-pipeline FPGAs,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088307096&doi=10.1145%2f3390523&partnerID=40&md5=db169a5bc9f281ca813b0e47a7279a1c,"FPGA-based graph processing accelerators are nowadays equipped with multiple pipelines for hardware acceleration of graph computations. However, their multi-pipeline efficiency can suffer greatly from the considerable overheads caused by the read/write conflicts in their on-chip BRAM from different pipelines, leading to significant performance degradation and poor scalability. In this article, we investigate the underlying causes behind such inter-pipeline read/write conflicts by focusing on multi-pipeline FPGAs for accelerating Sparse Matrix Vector Multiplication (SpMV) arising in graph processing. We exploit our key insight that the problem of eliminating inter-pipeline read/write conflicts for SpMV can be formulated as one of solving a row-and column-wise tiling problem for its associated adjacency matrix. However, how to partition a sparse adjacency matrix obtained from any graph with respect to a set of pipelines by both eliminating all the inter-pipeline read/write conflicts and keeping all the pipelines reasonably load-balanced is challenging. We present a conflict-free scheduler, WaveScheduler, that can dispatch different sub-matrix tiles to different pipelines without any read/write conflict. We also introduce two optimizations that are specifically tailored for graph processing, ""degree-aware vertex index renaming""for improving load balancing and ""data re-organization""for enabling sequential off-chip memory access, for all the pipelines. Our evaluation on Xilinx®Alveo™ U250 accelerator card with 16 pipelines shows that WaveScheduler can achieve up to 3.57 GTEPS, running much faster than native scheduling and two state-of-the-art FPGA-based graph accelerators (by 6.48× for ""native,""2.54× for HEGP, and 2.11× for ForeGraph), on average. In particular, these performance gains also scale up significantly as the number of pipelines increases.  © 2020 ACM.",data conflicts; FPGA; Graph processing; multi-pipeline; performance; SpMV,Field programmable gate arrays (FPGA); Graph structures; Graph theory; Matrix algebra; Pipelines; Scheduling; Adjacency matrices; Graph processing; Hardware acceleration; Multiple pipelines; Performance degradation; Performance Gain; Sparse matrix-vector multiplication; Underlying cause; Pipeline processing systems
SIMT-X: Extending Single-Instruction Multi-Threading to Out-of-Order Cores,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088288014&doi=10.1145%2f3392032&partnerID=40&md5=b7755b43fddc2e0986ab94a6bbcc5f62,"This work introduces Single Instruction Multi-Thread Express (SIMT-X), a general-purpose Central Processing Unit (CPU) microarchitecture that enables Graphics Processing Units (GPUs)-style SIMT execution across multiple threads of the same program for high throughput, while retaining the latency benefits of out-of-order execution, and the programming convenience of homogeneous multi-thread processors. SIMT-X leverages the existing Single Instruction Multiple Data (SIMD) back-end to provide CPU/GPU-like processing on a single core with minimal overhead. We demonstrate that although SIMT-X invokes a restricted form of Out-of-Order (OoO), the microarchitecture successfully captures a majority of the benefits of aggressive OoO execution using at most two concurrent register mappings per architectural register, while addressing issues of partial dependencies and supporting a general-purpose Instruction Set Architecture (ISA).  © 2020 Owner/Author.",computer architecture; hardware; microarchitecture; multi-threading; out-of-order; SIMT,Computer graphics; Graphics processing unit; Image coding; Program processors; High throughput; Instruction set architecture; Micro architectures; Multi-threading; Multiple threads; Out of order; Out-of-order execution; Single instruction multiple data; Computer architecture
Runtime Design Space Exploration and Mapping of DCNNs for the Ultra-Low-Power Orlando SoC,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088296169&doi=10.1145%2f3379933&partnerID=40&md5=c256c2729c1695c29271203ecc3e1ef2,"Recent trends in deep convolutional neural networks (DCNNs) impose hardware accelerators as a viable solution for computer vision and speech recognition. The Orlando SoC architecture from STMicroelectronics targets exactly this class of problems by integrating hardware-accelerated convolutional blocks together with DSPs and on-chip memory resources to enable energy-efficient designs of DCNNs. The main advantage of the Orlando platform is to have runtime configurable convolutional accelerators that can adapt to different DCNN workloads. This opens new challenges for mapping the computation to the accelerators and for managing the on-chip resources efficiently. In this work, we propose a runtime design space exploration and mapping methodology for runtime resource management in terms of on-chip memory, convolutional accelerators, and external bandwidth. Experimental results are reported in terms of power/performance scalability, Pareto analysis, mapping adaptivity, and accelerator utilization for the Orlando architecture mapping the VGG-16, Tiny-Yolo(v2), and MobileNet topologies.  © 2020 ACM.",convolutional neural networks; design space exploration; hardware acceleration; Ultra low-power embedded systems,Acceleration; Computer hardware; Convolution; Convolutional neural networks; Deep neural networks; Energy efficiency; Mapping; Memory architecture; Network architecture; Programmable logic controllers; Speech recognition; System-on-chip; Design space exploration; Energy-efficient design; Hardware accelerators; Hardware-accelerated; Mapping methodology; Resource management; STMicroelectronics; Viable solutions; Integrated circuit design
Dynamic Precision Autotuning with TAFFO,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088323404&doi=10.1145%2f3388785&partnerID=40&md5=bfe3654fd3abd7ff6ed279cde9daf2ee,"Many classes of applications, both in the embedded and high performance domains, can trade off the accuracy of the computed results for computation performance. One way to achieve such a trade-off is precision tuning-that is, to modify the data types used for the computation by reducing the bit width, or by changing the representation from floating point to fixed point. We present a methodology for high-accuracy dynamic precision tuning based on the identification of input classes (i.e., classes of input datasets that benefit from similar optimizations). When a new input region is detected, the application kernels are re-compiled on the fly with the appropriate selection of parameters. In this way, we obtain a continuous optimization approach that enables the exploitation of the reduced precision computation while progressively exploring the solution space, thus reducing the time required by compilation overheads. We provide tools to support the automation of the runtime part of the solution, leaving to the user only the task of identifying the input classes. Our approach provides a significant performance boost (up to 320%) on the typical approximate computing benchmarks, without meaningfully affecting the accuracy of the result, since the error remains always below 3%.  © 2020 ACM.",approximate computing; compiler; fixed point; Precision tuning,Digital arithmetic; Economic and social effects; Optimization; Computation performance; Continuous optimization; Dynamic precision; Floating points; High-accuracy; Precision tuning; Reduced precision; Solution space; Benchmarking
Reliability Analysis for Unreliable FSM Computations,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088286496&doi=10.1145%2f3377456&partnerID=40&md5=5f27bc8d78e85b4a6a0c2f0179fab24f,"Finite State Machines (FSMs) are fundamental in both hardware design and software development. However, the reliability of FSM computations remains poorly understood. Existing reliability analyses are mainly designed for generic computations and are unaware of the special error tolerance characteristics in FSM computations. This work introduces RelyFSM-a state-level reliability analysis framework for FSM computations. By modeling the behaviors of unreliable FSM executions and qualitatively reasoning about the transition structures, RelyFSM can precisely capture the inherent error tolerance in FSM computations. Our evaluation with real-world FSM benchmarks confirms both the accuracy and efficiency of RelyFSM.  © 2020 ACM.",error tolerance; Finite state machine; probabilistic model; reliability,Software design; Error tolerance; Hardware design; Inherent errors; Real-world; Transition structures; Reliability analysis
Network Interface Architecture for Remote Indirect Memory Access (RIMA) in Datacenters,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088316924&doi=10.1145%2f3374215&partnerID=40&md5=c0fed74173a63f691cf16da946e67ab1,"Remote Direct Memory Access (RDMA) fabrics such as InfiniBand and Converged Ethernet report latency shorter by a factor of 50 than TCP. As such, RDMA is a potential replacement for TCP in datacenters (DCs) running low-latency applications, such as Web search and memcached. InfiniBand's Shared Receive Queues (SRQs), which use two-sided send/recv verbs (i.e., channel semantics), reduce the amount of pre-allocated, pinned memory (despite optimizations such as InfiniBand's on-demand paging (ODP)) for message buffers. However, SRQs are limited fundamentally to a single message size per queue, which incurs either memory wastage or significant programmer burden for typical DC traffic of an arbitrary number (level of burstiness) of messages of arbitrary size. We propose remote indirect memory access (RIMA), which avoids these pitfalls by providing (1) network interface card (NIC) microarchitecture support for novel queue semantics and (2) a new ""verb""called append. To append a sender's message to a shared queue, the receiver NIC atomically increments the queue's tail pointer by the incoming message's size and places the message in the newly created space. As in traditional RDMA, the NIC is responsible for pointer lookup, address translation, and enforcing virtual memory protections. This indirection of specifying a queue (and not its tail pointer, which remains hidden from senders) handles the typical DC traffic of an arbitrary sender sending an arbitrary number of messages of arbitrary size. Because RIMA's simple hardware adds only 1-2 ns to the multi-\mu s message latency, RIMA achieves the same message latency and throughput as InfiniBand SRQ with unlimited buffering. Running memcached traffic on a 30-node InfiniBand cluster, we show that at similar, low programmer effort, RIMA achieves significantly smaller memory footprint than SRQ. However, while SRQ can be crafted to minimize memory footprint by expending significant programming effort, RIMA provides those benefits with little programmer effort. For memcached traffic, a high-performance key-value cache (FastKV) using RIMA achieves either 3× lower 96 th-percentile latency or significantly better throughput or memory footprint than FastKV using RDMA.  © 2020 ACM.",Indirection; Network Interface; receive queue management; remote append; remote direct memory access,Memory architecture; Negative impedance converters; Queueing theory; Semantics; Transmission control protocol; Virtual addresses; Address translation; Arbitrary number; InfiniBand clusters; Memory footprint; Microarchitecture supports; Network interface architecture; Network interface cards; Remote direct memory access; Cache memory
Dynamic colocation policies with reinforcement learning,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081604195&doi=10.1145%2f3375714&partnerID=40&md5=4cef69064fa5f681ead5a8d9784c739e,"We draw on reinforcement learning frameworks to design and implement an adaptive controller for managing resource contention. During runtime, the controller observes the dynamic system conditions and optimizes control policies that satisfy latency targets yet improve server utilization. We evaluate a physical prototype that guarantees 95th percentile latencies for a search engine and improves server utilization by up to 70%, compared to exclusively reserving servers for interactive services, for varied batch workloads in machine learning. © 2020 Association for Computing Machinery. All rights reserved.",adaptive control; machine learning; Resource contention,Adaptive control systems; Controllers; Learning systems; Search engines; Adaptive Control; Adaptive controllers; Batch workloads; Design and implements; Interactive services; Managing resources; Resource contention; System conditions; Reinforcement learning
A model-based software solution for simultaneous multiple kernels on GPUs,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081572012&doi=10.1145%2f3377138&partnerID=40&md5=eab7fc3f9e1c7fc0871374d29da9b6f5,"As a critical computing resource in multiuser systems such as supercomputers, data centers, and cloud services, a GPU contains multiple compute units (CUs). GPU Multitasking is an intuitive solution to underutilization in GPGPU computing. Recently proposed solutions of multitasking GPUs can be classified into two categories: (1) spatially partitioned sharing (SPS), which coexecutes different kernels on disjointed sets of compute units (CU), and (2) simultaneous multikernel (SMK), which runs multiple kernels simultaneously within a CU. Compared to SPS, SMK can improve resource utilization even further due to the interleaving of instructions from kernels with low dynamic resource contentions. However, it is hard to implement SMK on current GPU architecture, because (1) techniques for applying SMK on top of GPU hardware scheduling policy are scarce and (2) finding an efficient SMK scheme is difficult due to the complex interferences of concurrently executed kernels. In this article, we propose a lightweight and effective performance model to evaluate the complex interferences of SMK. Based on the probability of independent events, our performance model is built from a totally new angle and contains limited parameters. Then, we propose a metric, symbiotic factor, which can evaluate an SMK scheme so that kernels with complementary resource utilization can corun within a CU. Also, we analyze the advantages and disadvantages of kernel slicing and kernel stretching techniques and integrate them to apply SMK on GPUs instead of simulators. We validate our model on 18 benchmarks. Compared to the optimized hardware-based concurrent kernel execution whose kernel launching order brings fast execution time, the results of corunning kernel pairs show 11%, 18%, and 12% speedup on AMD R9 290X, RX 480, and Vega 64, respectively, on average. Compared to the Warped-Slicer, the results show 29%, 18%, and 51% speedup on AMD R9 290X, RX 480, and Vega 64, respectively, on average. © 2020 Association for Computing Machinery. All rights reserved.",concurrent kernel execution; GPGPU,Multitasking; Program processors; Supercomputers; Computing resource; concurrent kernel execution; Effective performance; Fast execution time; GPGPU; Resource utilizations; Scheduling policies; Stretching techniques; Computer hardware
Application-specific arithmetic in high-level synthesis tools,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081564591&doi=10.1145%2f3377403&partnerID=40&md5=af568765d1f50ca7bd7bc43b1496e785,"Thiswork studies hardware-specific optimization opportunities currently unexploited by high-level synthesis compilers. Some of these optimizations are specializations of floating-point operations that respect the usual semantics of the input program without changing the numerical result. Some other optimizations, locally triggered by the programmer thanks to a pragma, assume a different semantics, where floating-point code is interpreted as the specification of computation with real numbers. The compiler is then in charge to ensure an application-level accuracy constraint expressed in the pragma and has the freedom to use non-standard arithmetic hardware when more efficient. These two classes of optimizations are prototyped in the GeCoS source-to-source compiler and evaluated on the Polybench and EEMBC benchmark suites. Latency is reduced by up to 93%, and resource usage is reduced by up to 58%. © 2020 Association for Computing Machinery. All rights reserved.",computer arithmetic; floating point; High-level synthesis; operator specialization,Cobalt compounds; Computer hardware; Digital arithmetic; Germanium compounds; Program compilers; Semantics; Sulfur compounds; Application level; Application specific; Benchmark suites; Computer arithmetic; Floating point operations; Floating points; Numerical results; operator specialization; High level synthesis
Informed prefetching for indirect memory accesses,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081547770&doi=10.1145%2f3374216&partnerID=40&md5=ca297b1ab4eecaa10c99513d8dce6ee0,"Indirect memory accesses have irregular access patterns that limit the performance of conventional software and hardware-based prefetchers. To address this problem, we propose the Array Tracking Prefetcher (ATP), which tracks array-based indirect memory accesses using a novel combination of software and hardware. ATP is first configured by special metadata instructions, which are inserted by programmer or compiler to pass data structure traversal knowledge. It then calculates and issues prefetches based on this information. ATP also employs a novel mechanism for dynamically adjusting prefetching distance to reduce early or late prefetches. ATP yields average speedup of 2.17 as compared to a single-core without prefetching. By contrast, the speedup for conventional software and hardware-based prefetching is 1.84 and 1.32, respectively. For four cores, the average speedup for ATP is 1.85, while the corresponding speedups for software and hardwarebased prefetching are 1.60 and 1.25, respectively. © 2020 Association for Computing Machinery. All rights reserved.",computer architecture; Hardware and software prefetch; indirect memory access,Computer architecture; Memory architecture; Program compilers; Access patterns; Four-core; Memory access; Prefetches; Prefetching; Software and hardwares; Computer hardware
"A novel, highly integrated simulator for parallel and distributed systems",2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081589874&doi=10.1145%2f3378934&partnerID=40&md5=d8b94d1985871c1b3e6af760eca0b10c,"In an era of complex networked parallel heterogeneous systems, simulating independently only parts, components, or attributes of a system-under-design is a cumbersome, inaccurate, and inefficient approach.Moreover, by considering each part of a system in an isolated manner, and due to the numerous and highly complicated interactions between the different components, the system optimization capabilities are severely limited. The presented fully-distributed simulation framework (called as COSSIM) is the first known open-source, highperformance simulator that can handle holistically system-of-systems including processors, peripherals and networks; such an approach is very appealing to both Cyber Physical Systems (CPS) and Highly Parallel Heterogeneous Systems designers and application developers. Our highly integrated approach is further augmented with accurate power estimation and security sub-tools that can tap on all system components and perform security and robustness analysis of the overall system under design-something that was unfeasible up to now. Additionally, a sophisticated Eclipse-based Graphical User Interface (GUI) has been developed to provide easy simulation setup, execution, and visualization of results. COSSIM has been evaluated when executing the widely used Netperf benchmark suite as well as a number of real-world applications. Final results demonstrate that the presented approach has up to 99% accuracy (when compared with the performance of the real system), while the overall simulation time can be accelerated almost linearly with the number of CPUs utilized by the simulator. © 2020 Association for Computing Machinery. All rights reserved.",CPS simulator; Distributed systems simulator; integrated simulator; parallel systems simulator,Benchmarking; Distributed computer systems; Embedded systems; Graphical user interfaces; Network security; Program processors; Simulators; Application developers; Cyber-physical systems (CPS); Distributed simulations; Distributed systems; Graphical user interfaces (GUI); Integrated simulators; Parallel and distributed systems; Parallel system; Open systems
Improving memory efficiency in heterogeneous mpsocs through row-buffer locality-aware forwarding,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081627375&doi=10.1145%2f3377149&partnerID=40&md5=0d639c7ba0d6769693c713d8cd98a92b,"In heterogeneous multicore systems, the memory subsystem plays a critical role, since most core-to-core communications are conducted through the main memory. Memory efficiency has a substantial impact on system performance. Although memory traffic from multimedia cores generally manifests high row-buffer locality, which is beneficial to memory efficiency, the locality is often lost as memory streams are forwarded through networks-on-chip (NoC). Previous studies have discussed the techniques that improve memory visibility to reveal scattered row-buffer hit opportunities to the memory scheduler. However, extending local memory visibility introduces little benefit after the locality has been severely diluted. As the alternative approach, preserving row-buffer locality in the NoC has not been well explored. What is worse, it remains to be studied how to perform network traffic scheduling with the awareness of both memory efficiency and quality-of-service (QoS). In this article, we propose a router design with embedded row-index caches to enable locality-aware packet forwarding. The proposed design requires minor modifications to existing router microarchitecture and can be easily implemented with priority arbiters to integrate QoS support. Extensive evaluations show that the proposed design achieves higher memory efficiency than prior memory-aware routers, in addition to providing QoS support. On basis of extant QoS-aware routers, locality-aware forwarding helps to increase row-buffer hits by 58.32% and reduce memory latency by 14.45% on average. It also introduces a net reduction in DRAM and NoC energy cost by 27.82%. © 2020 Association for Computing Machinery. All rights reserved.",energy efficiency; Memory locality; quality-of-service; row-buffer hit,Dynamic random access storage; Energy efficiency; Integrated circuit design; Network-on-chip; Routers; Scheduling; Visibility; Heterogeneous multi-core systems; Memory efficiency; Memory locality; Memory subsystems; Networks on chips; Packet forwarding; Router microarchitecture; row-buffer hit; Quality of service
Optimizing the SSD burst buffer by traffic detection,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081589216&doi=10.1145%2f3377705&partnerID=40&md5=b903e5870d7a2669e0be43fe3466c62f,"Currently, HPC storage systems still use hard disk drive (HDD) as their dominant storage device. Solid state drive (SSD) is widely deployed as the buffer to HDDs. Burst buffer has also been proposed to manage the SSD buffering of bursty write requests. Although burst buffer can improve I/O performance in many cases, we find that it has some limitations such as requiring large SSD capacity and harmonious overlapping between computation phase and data flushing phase. In this article, we propose a scheme, called SSDUP+.1 SSDUP+ aims to improve the burst buffer by addressing the above limitations. First, to reduce the demand for the SSD capacity, we develop a novel method to detect and quantify the data randomness in the write traffic. Further, an adaptive algorithm is proposed to classify the random writes dynamically. By doing so, much less SSD capacity is required to achieve the similar performance as other burst buffer schemes. Next, to overcome the difficulty of perfectly overlapping the computation phase and the flushing phase, we propose a pipeline mechanism for the SSD buffer, in which data buffering and flushing are performed in pipeline. In addition, to improve the I/O throughput, we adopt a traffic-aware flushing strategy to reduce the I/O interference in HDD. Finally, to further improve the performance of buffering random writes in SSD, SSDUP+ transforms the random writes to sequential writes in SSD by storing the data with a log structure. Further, SSDUP+ uses the AVL tree structure to store the sequence information of the data. We have implemented a prototype of SSDUP+ based on OrangeFS and conducted extensive experiments. The experimental results show that our proposed SSDUP+ can save an average of 50% SSD space while delivering almost the same performance as other common burst buffer schemes. In addition, SSDUP+ can save about 20% SSD space comparedwith the previous version of thiswork, SSDUP,while achieving 20-30% higher I/O throughput than SSDUP. © 2020 Association for Computing Machinery. All rights reserved.",burst buffer; High-performance computing; hybrid storage system; solid state drive,Adaptive algorithms; Buffer storage; Pipelines; Trees (mathematics); Virtual storage; burst buffer; Flushing strategy; High performance computing; Hybrid storage systems; Sequence informations; Solid state drives; Solid state drives (SSD); Traffic detection; Hard disk storage
Enabling highly efficient batched matrix multiplications on SW26010 many-core processor,2020,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081626453&doi=10.1145%2f3378176&partnerID=40&md5=7ba937f4522e63317628387c715bf80f,"We present a systematic methodology for optimizing batched matrix multiplications on SW26010 many-core processor of the Sunway TaihuLight supercomputer. Five surrogate algorithms and amachine learning-based algorithm selector are proposed to fully exploit the computing capability of SW26010 and cope with the sophisticated algorithm characteristics of batched matrix multiplications. Experiment results show that the algorithm selector is able to adaptively choose the appropriate algorithm for various matrix shapes and batch sizes with low overhead and high accuracy. In particular, the optimized batched matrix multiplications can substantially outperform the non-batched version and reach around 84.8% of the performance upper bound. © 2020 Association for Computing Machinery. All rights reserved.",batched GEMM; Batched matrix multiplication; many-core architecture; Sunway TaihuLight; SW26010 processor,Matrix algebra; Supercomputers; batched GEMM; Many-core architecture; MAtrix multiplication; Sunway TaihuLight; SW26010 processor; Computer architecture
Shiftsreduce: Minimizing shifts in racetrack memory 4.0,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077778429&doi=10.1145%2f3372489&partnerID=40&md5=28b61591c0301e6d5fb186b22850262a,"Racetrack memories (RMs) have significantly evolved since their conception in 2008, making them a serious contender in the field of emerging memory technologies. Despite key technological advancements, the access latency and energy consumption of an RM-based system are still highly influenced by the number of shift operations. These operations are required to move bits to the right positions in the racetracks. This article presents data-placement techniques for RMs that maximize the likelihood that consecutive references access nearby memory locations at runtime, thereby minimizing the number of shifts. We present an integer linear programming (ILP) formulation for optimal data placement in RMs, and we revisit existing offset assignment heuristics, originally proposed for random-access memories. We introduce a novel heuristic tailored to a realistic RM and combine it with a genetic search to further improve the solution. We show a reduction in the number of shifts of up to 52.5%, outperforming the state of the art by up to 16.1%. © 2019 Copyright held by the owner/author(s).",Compiler optimization; Data placement; Domain wall memory; Heuristics; Integer linear programming; Racetrack memory; Shifts minimization,Domain walls; Energy utilization; Genetic algorithms; Genetic programming; Integer programming; Program compilers; Compiler optimizations; Data placement; Emerging memory technologies; Heuristics; Integer Linear Programming; Minimizing the number of; Random access memory; Technological advancement; Random access storage
Compiler-support for critical data persistence in NVM,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077774232&doi=10.1145%2f3371236&partnerID=40&md5=531f0e2b3985eeada9be38ddaa57901f,"Non-volatile Main Memories (NVMs) offer a promising way to preserve data persistence and enable computation recovery in case of failure. While the use of NVMs can significantly reduce the overhead of failure recovery, which is the case with High-Performance Computing (HPC) kernels, rewriting existing programs or writing new applications for NVMs is non-trivial. In this article, we present a compiler-support that automatically inserts complex instructions into kernels to achieve NVM data-persistence based on a simple programmer directive. Unlike checkpointing techniques that store the whole system state, our technique only persists user-designated objects as well as some parameters required for safe recovery such as loop induction variables. Also, our technique can reduce the number of data transfer operations, because our compiler coalesces consecutive memory-persisting operations into a single memory transaction per cache line when possible. Our compiler-support is implemented in the LLVM tool-chain and introduces the necessary modifications to loop-intensive computational kernels (e.g., TMM, LU, Gauss, and FFT) to force data persistence. The experiments show that our proposed compiler-support outperforms the most recent checkpointing techniques while its performance overheads are insignificant. © 2019 Copyright held by the owner/author(s).",Compiler-support; Data persistence; NVM; Valid recovery,Application programs; Cache memory; Data transfer; Recovery; Checkpointing techniques; Computational kernels; Data persistence; High performance computing (HPC); Induction variables; Memory transactions; New applications; Non-volatile main memory; Program compilers
Flextended tiles: A flexible extension of overlapped tiles for polyhedral compilation,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077768627&doi=10.1145%2f3369382&partnerID=40&md5=0f8ab9cf6fc927018a2a828444b49444,"Loop tiling to exploit data locality and parallelism plays an essential role in a variety of general-purpose and domain-specific compilers. Affine transformations in polyhedral frameworks implement classical forms of rectangular and parallelogram tiling, but these lead to pipelined start with rather inefficient wavefront parallelism. Multiple extensions to polyhedral compilers evaluated sophisticated shapes such as trapezoid or diamond tiles, enabling concurrent start along the axes of the iteration space; yet these resort to custom schedulers and code generators insufficiently integrated within the general framework. One of these modified shapes referred to as overlapped tiling also lacks a unifying framework to reason about its composition with affine transformations; this prevents its application in general-purpose loop-nest optimizers and the fair comparison with other techniques. We revisit overlapped tiling, recasting it as an affine transformation on schedule trees composable with any affine scheduling algorithm. We demonstrate how to derive tighter tile shapes with less redundant computations. Our method models the traditional “scalene trapezoid” shapes and novel “right-rectangle” variants. It goes beyond the state of the art by avoiding the restriction to a domain-specific language or introducing post-pass rescheduling and custom code generation. We conduct experiments on the PolyMage benchmarks and iterated stencils, validating the effectiveness and applicability of our technique on both general-purpose multicores and GPU accelerators. © 2019 Copyright held by the owner/author(s).",Automatic parallelization; Loop tiling; Polyhedral compilation; Stencil computations,Cache memory; Codes (symbols); Problem oriented languages; Program compilers; Scheduling algorithms; Trees (mathematics); Affine transformations; Automatic Parallelization; Domain specific languages; Loop tiling; Polyhedral compilation; Polyhedral framework; Redundant computation; Stencil computations; Iterative methods
Data-driven mixed precision sparse matrix vector multiplication for GPUs,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077782586&doi=10.1145%2f3371275&partnerID=40&md5=d4aadfb704343ab02dc0861b2e800c68,"We optimize Sparse Matrix Vector multiplication (SpMV) using a mixed precision strategy (MpSpMV) for Nvidia V100 GPUs. The approach has three benefits: (1) It reduces computation time, (2) it reduces the size of the input matrix and therefore reduces data movement, and (3) it provides an opportunity for increased parallelism. MpSpMV’s decision to lower to single precision is data driven, based on individual nonzero values of the sparse matrix. On all real-valued matrices from the Sparse Matrix Collection, we obtain a maximum speedup of 2.61× and average speedup of 1.06× over double precision, while maintaining higher accuracy compared to single precision. © 2019 Copyright held by the owner/author(s).",Correctness; Mixed precision; Sparse matrices; SpMV,Data reduction; Program processors; Computation time; Correctness; Double precision; Mixed precision; Single precision; Sparse matrices; Sparse matrix-vector multiplication; SpMV; Matrix algebra
A metric-guided method for discovering impactful features and architectural insights for skylake-based processors,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077775260&doi=10.1145%2f3369383&partnerID=40&md5=c0c5b1886e1abc6b6d4b1b87f607e7cc,"The slowdown in technology scaling puts architectural features at the forefront of the innovation in modern processors. This article presents a Metric-Guided Method (MGM) that extends Top-Down analysis with carefully selected, dynamically adapted metrics in a structured approach. Using MGM, we conduct two evaluations at the microarchitecture and the Instruction Set Architecture (ISA) levels. Our results show that simple optimizations, such as improved representation of CISC instructions, broadly improve performance, while changes in the Floating-Point execution units had mixed impact. Overall, we report 10 architectural insights—at the microarchitecture, ISA, and compiler fronts—while quantifying their impact on the SPEC CPU benchmarks. © 2019 Copyright held by the owner/author(s).",Benchmarking; Compiler code generation; Instruction set architecture; Microarchitecture; Performance analysis; Performance comparison,Benchmarking; Digital arithmetic; Program compilers; Code Generation; Instruction set architecture; Micro architectures; Performance analysis; Performance comparison; Computer architecture
Declarative loop tactics for domain-specific optimization,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077779980&doi=10.1145%2f3372266&partnerID=40&md5=6a0bdb8cc40d3365e3a8dcc00e3c0ab3,"Increasingly complex hardware makes the design of effective compilers difficult. To reduce this problem, we introduce Declarative Loop Tactics, which is a novel framework of composable program transformations based on an internal tree-like program representation of a polyhedral compiler. The framework is based on a declarative C++ API built around easy-to-program matchers and builders, which provide the foundation to develop loop optimization strategies. Using our matchers and builders, we express computational patterns and core building blocks, such as loop tiling, fusion, and data-layout transformations, and compose them into algorithm-specific optimizations. Declarative Loop Tactics (Loop Tactics for short) can be applied to many domains. For two of them, stencils and linear algebra, we show how developers can express sophisticated domain-specific optimizations as a set of composable transformations or calls to optimized libraries. By allowing developers to add highly customized optimizations for a given computational pattern, we expect our approach to reduce the need for DSLs and to extend the range of optimizations that can be performed by a current general-purpose compiler. © 2019 Copyright held by the owner/author(s).",Declarative loop optimizations; Loop tactics; Polyhedral model,C++ (programming language); Cache memory; Linear algebra; Linear transformations; Mathematical transformations; Complex hardware; Computational patterns; Data layout transformations; Loop optimizations; Loop tactics; Polyhedral modeling; Program representations; Program transformations; Program compilers
Nested MIMD-SIMD parallelization for heterogeneous microprocessors,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077781589&doi=10.1145%2f3368304&partnerID=40&md5=ec4c2dc857762665a7517c0c9abe8fb1,"Heterogeneous microprocessors integrate a CPU and GPU on the same chip, providing fast CPU-GPU communication and enabling cores to compute on data “in place.” This permits exploiting a finer granularity of parallelism on the integrated GPUs, and enables the use of GPUs for accelerating more complex and irregular codes. One challenge, however, is exposing enough parallelism such that both the CPU and GPU are effectively utilized to achieve maximum gain. In this article, we propose exploiting nested parallelism for integrated CPU-GPU chips. We look for loop structures in which one or more regular data parallel loops are nested within a parallel outer loop that can contain irregular code (e.g., with control divergence). By scheduling the outer loop on multiple CPU cores, multiple dynamic instances of the inner regular loop(s) can be scheduled on the GPU cores. This boosts GPU utilization and parallelizes the outer loop. We find that such nested MIMD-SIMD parallelization provides greater levels of parallelism for integrated CPU-GPU chips, and additionally there is ample opportunity to perform such parallelization in OpenMP programs. Our results show nested MIMD-SIMD parallelization provides a 16.1x and 8.67x speedup over sequential execution on a simulator and a physical machine, respectively. Our technique beats CPU-only parallelization by 4.13x and 2.40x, respectively, and GPU-only parallelization by 2.74x and 2.26x, respectively. Compared to the next-best scheme (either CPU- or GPU-only parallelization) per benchmark, our approach provides a 1.46x and 1.23x speedup for the simulator and physical machine, respectively. © 2019 Copyright held by the owner/author(s).",GPU; Heterogeneous microprocessor; MIMD; Nested parallelism; SIMD,Application programming interfaces (API); Codes (symbols); Graphics processing unit; Program processors; Control divergences; Irregular codes; MIMD; Nested Parallelism; OpenMP programs; Parallelizations; Sequential execution; SIMD; Parallel processing systems
DNntune: Automatic benchmarking DNN models for mobile-cloud computing,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077779031&doi=10.1145%2f3368305&partnerID=40&md5=aaa9c3e402a60c84e69c824fb47e89f4,"Deep Neural Networks (DNNs) are now increasingly adopted in a variety of Artificial Intelligence (AI) applications. Meantime, more and more DNNs are moving from cloud to the mobile devices, as emerging AI chips are integrated into mobiles. Therefore, the DNN models can be deployed in the cloud, on the mobile devices, or even mobile-cloud coordinate processing, making it a big challenge to select an optimal deployment strategy under specific objectives. This article proposes a DNN tuning framework, i.e., DNNTune, that can provide layer-wise behavior analysis across a number of platforms. Using DNNTune, this article further selects 13 representative DNN models, including CNN, LSTM, and MLP, and three mobile devices ranging from low-end to high-end, and two AI accelerator chips to characterize the DNN models on these devices to further assist users finding opportunities for mobile-cloud coordinate computing. Our experimental results demonstrate that DNNTune can find a coordinated deployment achieving up to 1.66× speedup and 15% energy saving comparing with mobile-only and cloud-only deployment. © 2019 Copyright held by the owner/author(s).",DNN; Heterogeneous computing; Mobile-cloud computing,Deep neural networks; Energy conservation; Long short-term memory; Behavior analysis; Heterogeneous computing; Layer-wise; Mobile clouds; Optimal deployment; Mobile cloud computing
Building a polyhedral representation from an instrumented execution: Making dynamic analyses of nonaffine programs scalable,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077780818&doi=10.1145%2f3363785&partnerID=40&md5=4127a1ba3f8dd897d2fe9965088a3ce6,"The polyhedral model has been successfully used in production compilers. Nevertheless, only a very restricted class of applications can benefit from it. Recent proposals investigated how runtime information could be used to apply polyhedral optimization on applications that do not statically fit the model. In this work, we go one step further in that direction. We propose the folding-based analysis that, from the output of an instrumented program execution, builds a compact polyhedral representation. It is able to accurately detect affine dependencies, fixed-stride memory accesses, and induction variables in programs. It scales to real-life applications, which often include some nonaffine dependencies and accesses in otherwise affine code. This is enabled by a safe fine-grained polyhedral overapproximation mechanism. We evaluate our analysis on the entire Rodinia benchmark suite, enabling accurate feedback about the potential for complex polyhedral transformations. © 2019 Copyright held by the owner/author(s).",Binary; Compiler optimization; Dynamic dependency graph; Instrumentation; Loop transformations; Performance feedback; Polyhedral model,Software engineering; Binary; Compiler optimizations; Dynamic dependency; Instrumentation; Loop transformation; Performance feedback; Polyhedral modeling; Program compilers
FaiLamP: Relativization transformation for soft error detection in structured address generation,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077782326&doi=10.1145%2f3369381&partnerID=40&md5=9bf355a2765b27c813392c4cb4b69839,"We present FailAmp, a novel LLVM program transformation algorithm that makes programs employing structured index calculations more robust against soft errors. Without FailAmp, an offset error can go undetected; with FailAmp, all subsequent offsets are relativized, building on the faulty one. FailAmp can exploit ISAs such as ARM to further reduce overheads. We verify correctness properties of FailAMP using an SMT solver, and present a thorough evaluation using many high-performance computing benchmarks under a fault injection campaign. FailAmp provides full soft-error detection for address calculation while incurring an average overhead of around 5%. © 2019 Copyright held by the owner/author(s).",Failure amplification; LLVM transformation; Soft error detection; Structured address generation,Benchmarking; Error correction; Radiation hardening; Correctness properties; Fault injection; High performance computing; LLVM transformation; Program transformations; Soft error detection; Structured address; Structured indexes; Error detection
Tiling optimizations for stencil computations using rewrite rules in lift,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077775212&doi=10.1145%2f3368858&partnerID=40&md5=c5d4a17568e1fc4000bb7504e610048a,"Stencil computations are a widely used type of algorithm, found in applications from physical simulations to machine learning. Stencils are embarrassingly parallel, therefore fit on modern hardware such as Graphic Processing Units perfectly. Although stencil computations have been extensively studied, optimizing them for increasingly diverse hardware remains challenging. Domain-specific Languages (DSLs) have raised the programming abstraction and offer good performance; however, this method places the burden on DSL implementers to write almost full-fledged parallelizing compilers and optimizers. Lift has recently emerged as a promising approach to achieve performance portability by using a small set of reusable parallel primitives that DSL or library writers utilize. Lift’s key novelty is in its encoding of optimizations as a system of extensible rewrite rules which are used to explore the optimization space. This article demonstrates how complex multi-dimensional stencil code and optimizations are expressed using compositions of simple 1D Lift primitives and rewrite rules. We introduce two optimizations that provide high performance for stencils in particular: classical overlapped tiling for multi-dimensional stencils and 2.5D tiling specifically for 3D stencils. We provide an in-depth analysis on how the tiling optimizations affects stencils of different shapes and sizes across different applications. Our experimental results show that our approach outperforms existing compiler approaches and hand-tuned codes. © 2019 Copyright held by the owner/author(s).",Code generation; GPU computing; Lift; Performance portability; Stencil,Codes (symbols); Computer software portability; Digital subscriber lines; Graphics processing unit; Lift; Machine learning; Problem oriented languages; Code Generation; Domain specific languages; GPU computing; Graphic processing units; Parallelizing compiler; Performance portability; Programming abstractions; Stencil; Program compilers
Exploring complex brain-simulation workloads on multi-GPU deployments,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077772744&doi=10.1145%2f3371235&partnerID=40&md5=0da531e4d46767d9e371e5756396b250,"In-silico brain simulations are the de-facto tools computational neuroscientists use to understand large-scale and complex brain-function dynamics. Current brain simulators do not scale efficiently enough to large-scale problem sizes (e.g., >100,000 neurons) when simulating biophysically complex neuron models. The goal of this work is to explore the use of true multi-GPU acceleration through NVIDIA’s GPUDirect technology on computationally challenging brain models and to assess their scalability. The brain model used is a state-of-the-art, extended Hodgkin-Huxley, biophysically meaningful, three-compartmental model of the inferior-olivary nucleus. The Hodgkin-Huxley model is the most widely adopted conductance-based neuron representation, and thus the results from simulating this representative workload are relevant for many other brain experiments. Not only the actual network-simulation times but also the network-setup times were taken into account when designing and benchmarking the multi-GPU version, an aspect often ignored in similar previous work. Network sizes varying from 65K to 2M cells, with 10 and 1,000 synapses per neuron were executed on 8, 16, 24, and 32 GPUs. Without loss of generality, simulations were run for 100 ms of biological time. Findings indicate that communication overheads do not dominate overall execution while scaling the network size up is computationally tractable. This scalable design proves that large-network simulations of complex neural models are possible using a multi-GPU design with GPUDirect. © 2019 Copyright held by the owner/author(s).",Multi-GPU; Multi-node; Neural networks,Bioinformatics; Brain models; Graphics processing unit; Neural networks; Neurons; Program processors; Communication overheads; Compartmental model; Hodgkin-Huxley models; Large-scale problem; Multi-gpu; Multi-nodes; Network simulation; Overall execution; Complex networks
BitSAD v2: Compiler optimization and analysis for bitstream computing,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075609787&doi=10.1145%2f3364999&partnerID=40&md5=8062bcc2794158ca9f9500de4ea58d39,"Computer vision and machine learning algorithms operating under a strict power budget require an alternate computing paradigm. While bitstream computing (BC) satisfies these constraints, creating BC systems is difficult. To address the design challenges, we propose compiler extensions to BitSAD, a DSL for BC. Our work enables bit-level software emulation and automated generation of hierarchical hardware, discusses potential optimizations, and proposes compiler phases to implement those optimizations in a hardware-aware manner. Finally, we introduce population coding, a parallelization scheme for stochastic computing that decreases latency without sacrificing accuracy, and provide theoretical and experimental guarantees on its effectiveness. 2019 Copyright held by the owner/author(s). © 2019 Copyright held by the owner/author(s)..",Bitstream computing; Compiler; Pulse density modulation; Stochastic computing,Binary sequences; Budget control; Computer hardware; Learning algorithms; Machine learning; Pulse modulation; Stochastic systems; Automated generation; Bit stream; Compiler; Compiler optimizations; Computing paradigm; Population coding; Pulse density modulation; Stochastic computing; Program compilers
Chunking for dynamic linear pipelines,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075624006&doi=10.1145%2f3363815&partnerID=40&md5=32bdc960edccb38555fd307329c9b337,"Dynamic scheduling and dynamic creation of the pipeline structure are crucial for efficient execution of pipelined programs. Nevertheless, dynamic systems imply higher overhead than static systems. Therefore, chunking is the key to decrease the synchronization and scheduling overhead by grouping activities. We present a chunking algorithm for dynamic systems that handles dynamic linear pipelines, which allow the number and duration of stages to be determined at run-time. The evaluation on 44 cores shows that chunking brings the overhead of dynamic scheduling down to that of a static scheduler, and it enables efficient and scalable execution of fine-grained dynamic linear pipelines. © 2019 Copyright held by the owner/author(s).",Chunking; Dynamic linear pipeline; Loadbalancing; Multi-threading; Parallelization directives; Static and dynamic scheduling,Resource allocation; Scheduling; Chunking; Dynamic linear; Multi-threading; Parallelizations; Static and dynamic scheduling; Pipelines
Exploiting bank conflict-based side-channel timing leakage of GPUs,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075599831&doi=10.1145%2f3361870&partnerID=40&md5=520c12cb5f0a3c3608537de9dd18941d,"To prevent information leakage during program execution, modern software cryptographic implementations target constant-time function, where the number of instructions executed remains the same when program inputs change. However, the underlying microarchitecture behaves differently when processing different data inputs, impacting the execution time of the same instructions. These differences in execution time can covertly leak confidential information through a timing channel. Given the recent reports of covert channels present on commercial microprocessors, a number of microarchitectural features on CPUs have been re-examined from a timing leakage perspective. Unfortunately, a similar microarchitectural evaluation of the potential attack surfaces on GPUs has not been adequately performed. Several prior work has considered a timing channel based on the behavior of a GPU's coalescing unit. In this article, we identify a second finer-grained microarchitectural timing channel, related to the banking structure of the GPU's Shared Memory. By considering the timing channel caused by Shared Memory bank conflicts, we have developed a differential timing attack that can compromise table-based cryptographic algorithms. We implement our timing attack on an Nvidia Kepler K40 GPU and successfully recover the complete 128-bit encryption key of an Advanced Encryption Standard (AES) GPU implementation using 900,000 timing samples. We also evaluate the scalability of our attack method by attacking an implementation of the AES encryption algorithm that fully occupies the compute resources of the GPU. We extend our timing analysis onto other Nvidia architectures: Maxwell, Pascal, Volta, and Turing GPUs. We also discuss countermeasures and experiment with a novel multi-key implementation, evaluating its resistance to our side-channel timing attack and its associated performance overhead. © 2019 Copyright held by the owner/author(s).",GPU security; Microarchitectural attack; Side-channel security,Data privacy; Flocculation; Graphics processing unit; Memory architecture; Program processors; Timing circuits; Advanced Encryption Standard; Confidential information; Cryptographic algorithms; Cryptographic implementation; Micro architectures; Microarchitectural attack; Prevent information leakage; Side-channel; Side channel attack
"The next 700 accelerated layers: From mathematical expressions of network computation graphs to accelerated GPU kernels, automatically",2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073740356&doi=10.1145%2f3355606&partnerID=40&md5=85000074e3a89c2ba74b1ba744524093,"Deep learning frameworks automate the deployment, distribution, synchronization, memory allocation, and hardware acceleration of models represented as graphs of computational operators. These operators wrap high-performance libraries such as cuDNN or NNPACK. When the computation does not match any predefined library call, custom operators must be implemented, often at high engineering cost and performance penalty, limiting the pace of innovation. To address this productivity gap, we propose and evaluate: (1) a domain-specific language with a tensor notation close to the mathematics of deep learning; (2) a Just-In- Time optimizing compiler based on the polyhedral framework; (3) carefully coordinated linear optimization and evolutionary algorithms to synthesize high-performance CUDA kernels; (4) the transparent integration of our flow into PyTorch and Caffe2, providing the fully automatic synthesis of high-performance GPU kernels from simple tensor algebra. The performance is comparable to, and often exceeds the performance of, highly tuned libraries. © 2019 Association for Computing Machinery.",Deep learning layers; GPU acceleration; Polyhedral compilation,Coordination reactions; Cost engineering; Graphics processing unit; Libraries; Linear programming; Problem oriented languages; Tensors; Domain specific languages; GPU accelerations; Hardware acceleration; High-performance libraries; Mathematical expressions; Network computations; Performance penalties; Polyhedral compilation; Deep learning
A neural network prefetcher for arbitrary memory access patterns,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073729435&doi=10.1145%2f3345000&partnerID=40&md5=7151ea7810e1174e6f505e2d086fde6b,"Memory prefetchers are designed to identify and prefetch specific access patterns, including spatiotemporal locality (e.g., strides, streams), recurring patterns (e.g., varying strides, temporal correlation), and specific irregular patterns (e.g., pointer chasing, index dereferencing). However, existing prefetchers can only target premeditated patterns and relations they were designed to handle and are unable to capture access patterns in which they do not specialize. In this article, we propose a context-based neural network (NN) prefetcher that dynamically adapts to arbitrary memory access patterns. Leveraging recent advances in machine learning, the proposed NN prefetcher correlates program and machine contextual information with memory accesses patterns, using online-training to identify and dynamically adapt to unique access patterns exhibited by the code. By targeting semantic locality in this manner, the prefetcher can discern the useful context attributes and learn to predict previously undetected access patterns, even within noisy memory access streams. We further present an architectural implementation of our NN prefetcher, explore its power, energy, and area limitations, and propose several optimizations. We evaluate the neural network prefetcher over SPEC2006, Graph500, and several microbenchmarks and show that the prefetcher can deliver an average speedup of 21.3% for SPEC2006 (up to 2.3×) and up to 4.4× on kernels over a baseline of PC-based stride prefetcher and 30% for SPEC2006 over a baseline with no prefetching. © 2019 Association for Computing Machinery.",Computer architecture; Neural network; Prefetch,Computer architecture; Neural networks; Semantics; Context attributes; Contextual information; Memory access patterns; Micro-benchmarks; Neural network (nn); Online training; Prefetches; Temporal correlations; Memory architecture
PIMBALL: Binary neural networks in spintronic memory,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073722481&doi=10.1145%2f3357250&partnerID=40&md5=184cccc438c953e68f90ea4714375298,"Neural networks span a wide range of applications of industrial and commercial significance. Binary neural networks (BNN) are particularly effective in trading accuracy for performance, energy efficiency, or hardware/ software complexity. Here, we introduce a spintronic, re-configurable in-memory BNN accelerator, PIMBALL: Processing In Memory BNN AcceL(L)erator, which allows for massively parallel and energy efficient computation. PIMBALL is capable of being used as a standard spintronic memory (STT-MRAM) array and a computational substrate simultaneously. We evaluate PIMBALL using multiple image classifiers and a genomics kernel. Our simulation results show that PIMBALL is more energy efficient than alternative CPU-, GPU-, and FPGA-based implementations while delivering higher throughput. © 2019 Association for Computing Machinery.",Binary neural networks; Computational random access memory; Non-volatile memory; Processing in memory,Energy efficiency; Random access storage; Binary neural networks; Energy efficient; FPGA-based implementation; Massively parallels; Non-volatile memory; Processing in memory; Random access memory; Software complexity; MRAM devices
DCMI: A scalable strategy for accelerating iterative stencil loops on FPGAs,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073753727&doi=10.1145%2f3352813&partnerID=40&md5=be2cab61891ce7e2e988026ff44e2377,"Iterative Stencil Loops (ISLs) are the key kernel within a range of compute-intensive applications. To accelerate ISLs with Field Programmable Gate Arrays, it is critical to exploit parallelism (1) among elements within the same iteration and (2) across loop iterations. We propose a novel ISL acceleration scheme called Direct Computation of Multiple Iterations (DCMI) that improves upon prior work by pre-computing the effective stencil coefficients after a number of iterations at design time-resulting in accelerators that use minimal on-chip memory and avoid redundant computation. This enables DCMI to improve throughput by up to 7.7× compared to the state-of-the-art cone-based architecture. © 2019 Association for Computing Machinery.",Accelerator; FPGA; Iterative stencil loops (ISLs),Particle accelerators; Software engineering; Direct computations; Iterative stencil loops; Loop iteration; Multiple iterations; Number of iterations; On chip memory; Redundant computation; State of the art; Field programmable gate arrays (FPGA)
Optimizing remote communication in X10,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073738534&doi=10.1145%2f3345558&partnerID=40&md5=410b931d47b73d24876908b6dbd0e6cb,"X10 is a partitioned global address space programming language that supports the notion of places; a place consists of some data and some lightweight tasks called activities. Each activity runs at a place and may invoke a place-change operation (using the at-construct) to synchronously perform some computation at another place. These place-change operations can be very expensive, as they need to copy all the required data from the current place to the remote place. However, identifying the necessary number of place-change operations and the required data during each place-change operation are non-trivial tasks, especially in the context of irregular applications (like graph applications) that contain complex code with large amounts of cross-referencing objects-not all of those objects may be actually required, at the remote place. In this article, we present AT-Com, a scheme to optimize X10 code with place-change operations. AT-Com consists of two inter-related new optimizations: (i) AT-Opt, which minimizes the amount of data serialized and communicated during place-change operations, and (ii) AT-Pruning, which identifies/elides redundant place-change operations and does parallel execution of place-change operations. AT-Opt uses a novel abstraction, called abstract-place-tree, to capture place-change operations in the program. For each place-change operation, AT-Opt uses a novel inter-procedural analysis to precisely identify the data required at the remote place in terms of the variables in the current scope. AT-Opt then emits the appropriate code to copy the identified data-items to the remote place. AT-Pruning introduces a set of program transformation techniques to emit optimized code such that it avoids the redundant place-change operations. We have implemented AT-Com in the x10v2.6.0 compiler and tested it over the IMSuite benchmark kernels. Compared to the current X10 compiler, the AT-Com optimized code achieved a geometric mean speedup of 18.72× and 17.83× on a four-node (32 cores per node) Intel and two-node (16 cores per node) AMD system, respectively. © 2019 Association for Computing Machinery.",Data serialization; PGAS languages; Program transformation; Remote communication,Abstracting; Codes (symbols); Program compilers; Data serialization; Inter-procedural analysis; Irregular applications; Parallel executions; Partitioned Global Address Space; Program transformation techniques; Program transformations; Remote communication; Metadata
Layup: Layer-adaptive and multi-type intermediate-oriented memory optimization for GPU-based CNNs,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073745036&doi=10.1145%2f3357238&partnerID=40&md5=1d283ac687342d85150c26a3b4ef1a44,"Although GPUs have emerged as the mainstream for the acceleration of convolutional neural network (CNN) training processes, they usually have limited physical memory, meaning that it is hard to train large-scale CNN models. Many methods for memory optimization have been proposed to decrease the memory consumption of CNNs and to mitigate the increasing scale of these networks; however, this optimization comes at the cost of an obvious drop in time performance.We propose a new memory optimization strategy named Layup that realizes both bettermemory efficiency and better time performance. First, a fast layer-type-specific method for memory optimization is presented, based on the new finding that a single memory optimization often shows dramatic differences in time performance for different types of layers. Second, a new memory reuse method is presented in which greater attention is paid to multi-type intermediate data such as convolutional workspaces and cuDNN handle data. Experiments show that Layup can significantly increase the scale of extra-deep network models on a single GPU with lower performance loss. It even can train ResNet with 2,504 layers using 12GB memory, outperforming the state-of-the-art work of SuperNeurons with 1,920 layers (batch size = 16). © 2019 Association for Computing Machinery.",Convolutional neural network; GPU; Memory management,Graphics processing unit; Neural networks; Program processors; Convolutional neural network; Memory consumption; Memory management; Memory optimization; Performance loss; Physical memory; State of the art; Training process; Convolution
MetaStrider: Architectures for scalable memory-centric reduction of sparse data streams,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073710587&doi=10.1145%2f3355396&partnerID=40&md5=68366782d8b8e5ac2b2d51216dea7b74,"Reduction is an operation performed on the values of two or more key-value pairs that share the same key. Reduction of sparse data streams finds application in a wide variety of domains such as data and graph analytics, cybersecurity, machine learning, and HPC applications. However, these applications exhibit low locality of reference, rendering traditional architectures and data representations inefficient. This article presents MetaStrider, a significant algorithmic and architectural enhancement to the state-of-the-art, SuperStrider. Furthermore, these enhancements enable a variety of parallel, memory-centric architectures that we propose, resulting in demonstrated performance that scales near-linearly with available memory-level parallelism. © 2019 Association for Computing Machinery.",DRAM; Memory-centric architectures; Sparse,Dynamic random access storage; Memory architecture; Architectural enhancement; Data representations; Graph analytics; Memory level parallelisms; Scalable memory; Sparse; State of the art; Traditional architecture; Data reduction
Evaluating auto-vectorizing compilers through objective withdrawal of useful information,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073729337&doi=10.1145%2f3356842&partnerID=40&md5=c2bc565a51bbf518079d36aadce4c0f6,"The need for compilers to generate highly vectorized code is at an all-time high with the increasing vectorization capabilities of modern processors. To this end, the information that compilers have at their disposal, either through code analysis or via user annotations, is instrumental for auto-vectorization, and hence for the overall performance. However, the information that is available to compilers at compile time and its accuracy varies greatly, as does the resulting performance of vectorizing compilers. Benchmarks like the Test Suite for Vectorizing Compilers (TSVC) have been developed to evaluate the vectorization capability of such compilers. The overarching approach of TSVC and similar benchmarks is to evaluate the compilers under the best possible scenario (i.e., assuming that compilers have access to all useful contextual information at compile time). Although this idealistic view is useful to observe the capability of compilers for auto-vectorization, it is not a true reflection of the conditions found in real-world applications. In this article, we propose a novel method for evaluating the auto-vectorization capability of compilers. Instead of assuming that compilers have access to a wealth of information at compile time, we formulate a method to objectively supply or withdraw information that would otherwise aid the compiler in the autovectorization process. This method is orthogonal to the approach adopted by TSVC, and as such, it provides the means of assessing the capabilities of modern vectorizing compilers in a more detailed way. Using this new method, we exhaustively evaluated five industry-grade compilers (GNU, Intel, Clang, PGI, and IBM) on four representative vector platforms (AVX-2, AVX-512 (Skylake), AVX-512 (KNL), and AltiVec) using the modified version of TSVC and application-level proxy kernels. The results show the impact that withdrawing information has on the vectorization capabilities of each compiler and also prove the validity of the presented technique. © 2019 Association for Computing Machinery.",Auto-vectorization; Compiler evaluation; Vectorization capability; Vectorization test suite,Software engineering; Application level; Code analysis; Compiler evaluations; Contextual information; Modern processors; User annotations; Vectorization; Wealth of information; Program compilers
A relational theory of locality,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075568211&doi=10.1145%2f3341109&partnerID=40&md5=f2e74258b32d1801aa47780b43e699bf,"In many areas of program and system analysis and optimization, locality is a common concept and has been defined andmeasured in many ways. This article aims to formally establish relations between these previously disparate types of locality. It categorizes locality definitions in three groups and shows whether and how they can be interconverted. For the footprint, a recent metric, it gives a new measurement algorithm that is asymptotically more time/space efficient than previous approaches. Using the conversion relations, the new algorithm derives with the same efficiency different locality metrics developed and used in program analysis, memory management, and cache design. © 2019 Copyright held by the owner/author(s).",Locality; Memory hierarchy; Working set,Software engineering; Cache design; Locality; Measurement algorithms; Memory hierarchy; Memory management; Program analysis; Relational theories; Working set; Cache memory
Simplifying transactional memory support in C++,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075446420&doi=10.1145%2f3328796&partnerID=40&md5=a7b86f59b3121d390c1dabce53573721,"C++ has supported a provisional version of Transactional Memory (TM) since 2015, via a technical specification. However, TM has not seen widespread adoption, and compiler vendors have been slow to implement the technical specification. We conjecture that the proposed TM support is too difficult for programmers to use, too complex for compiler designers to implement and verify, and not industry-proven enough to justify final standardization in its current form. To address these problems, we present a different design for supporting TM in C++. By forbidding explicit self-Abort, and by introducing an executor-based mechanism for running transactions, our approach makes it easier for developers to get code up and running with TM. Our proposal should also be appealing to compiler developers, as it allows a spectrum of levels of support for TM, with varying performance, and varying reliance on hardware TM support in order to provide scalability. While our design does not enable some of the optimizations admitted by the current technical specification, we show that it enables the implementation of robust support for TM in a small, orthogonal compiler extension. Our implementation is able to handle a wide range of transactional programs, delivering low instrumentation overhead and scalability and performance on par with the current state of the art. Based on this experience, we believe our approach to be a viable means of reinvigorating the standardization of TM in C++. © 2019 Copyright held by the owner/author(s).",C++; LLVM; Synchronization; Transactional memory,Cesium; Program compilers; Scalability; Specifications; Standardization; Storage allocation (computer); Synchronization; LLVM; Scalability and performance; State of the art; Technical specifications; Transactional memory; C++ (programming language)
Toward on-chip network security using runtime isolation mapping,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075574472&doi=10.1145%2f3337770&partnerID=40&md5=96c284e947c92f4d6389381651e07f59,"Many-cores execute a large number of diverse applications concurrently. Inter-Application interference can lead to a security threat as timing channel attack in the on-chip network. A non-interference communication in the shared on-chip network is a dominant necessity for secure many-core platforms to leverage the concepts of the cloud and embedded system-on-chip. The current non-interference techniques are limited to static scheduling and need router modification at micro-Architecture level. Mapping of applications can effectively determine the interference among applications in on-chip network. In this work, we explore non-interference approaches through run-Time mapping at software and application level.We map the same group of applications in isolated domain(s) to meet non-interference flows. Through run-Time mapping, we can maximize utilization of the system without leaking information. The proposed run-Time mapping policy requires no router modification in contrast to the best known competing schemes, and the performance degradation is, on average, 16% compared to the state-of-The-Art baselines. © 2019 Copyright held by the owner/author(s).",Non-interference; On-chip interconnect; Runtime mapping; Timing side-channel,Application programs; Computer architecture; Mapping; System-on-chip; Diverse applications; Micro architectures; Non interference; On chip interconnect; Performance degradation; Run-time mapping; Static scheduling; Timing side channels; Network security
Side-channel timing attack of RSA on a GPU,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075547996&doi=10.1145%2f3341729&partnerID=40&md5=bd64ff109168e7be161b8582fb9925a3,"To increase computation throughput, general purpose Graphics Processing Units (GPUs) have been leveraged to accelerate computationally intensive workloads. GPUs have been used as cryptographic engines, improving encryption/decryption throughput and leveraging the GPU's Single Instruction Multiple Thread (SIMT) model. RSA is a widely used public-key cipher and has been ported onto GPUs for signing and decrypting large files. Although performance has been significantly improved, the security of RSA on GPUs is vulnerable to side-channel timing attacks and is an exposure overlooked in previous studies. GPUs tend to be naturally resilient to side-channel attacks, given that they execute a large number of concurrent threads, performing many RSA operations on different data in parallel. Given the degree of parallel execution on a GPU, there will be a significant amount of noise introduced into the timing channel given the thousands of concurrent threads executing concurrently. In this work, we build a timing model to capture the parallel characteristics of an RSA public-key cipher implemented on a GPU. We consider optimizations that include using Montgomery multiplication and slidingwindow exponentiation to implement cryptographic operations. Our timing model considers the challenges of parallel execution, complications that do not occur in single-Threaded computing platforms. Based on our timing model, we launch successful timing attacks on RSA running on a GPU, extracting the private key of RSA. We also present an effective error detection and correction mechanism. Our results demonstrate that GPU acceleration of RSA is vulnerable to side-channel timing attacks. We propose several countermeasures to defend against this class of attacks. © 2019 Copyright held by the owner/author(s).",GPU; RSA; Timing attack,Computer graphics; Graphics processing unit; Program processors; Public key cryptography; Timing circuits; Concurrent threads; Cryptographic engines; Cryptographic operations; Encryption/decryption; Error detection and correction; Montgomery multiplication; Parallel executions; Single Instruction Multiple Threads (SIMT); Side channel attack
Polyhedral compilation for multi-dimensional stream processing,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069441003&doi=10.1145%2f3330999&partnerID=40&md5=36e9bfe466dfd40a5a8b3051ae588abf,"We present a method for compilation of multi-dimensional stream processing programs from affine recurrence equations with unbounded domains into imperative code with statically allocated memory. The method involves a novel polyhedral schedule transformation called periodic tiling. It accommodates existing polyhedral optimizations to improve memory access patterns and expose parallelism. This enables efficient execution of programming languages with unbounded recurrence equations, as well as optimization of existing languages from which this form can be derived. The method is experimentally evaluated on 5 DSP algorithms with large problem sizes. Results show potential for improved throughput compared to hand-optimized C++ (speedups on a 6-core Intel Xeon CPU up to 10× with a geometric mean 3.3×). © 2019 Copyright held by the owner/author(s).",Digital signal processing; Multi-dimensional stream processing; Polyhedral compilation; Recurrence equations,Digital signal processing; Affine recurrence equations; Memory access patterns; Multi dimensional; Polyhedral compilation; Polyhedral optimizations; Recurrence equation; Stream processing; Unbounded domain; C++ (programming language)
A first step toward using quantum computing for low-level WCETs estimations,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069771755&doi=10.1145%2f3335549&partnerID=40&md5=b1bee25f562055e090ae19f0785e3a68,"Low-Level analysis of Worst Case Execution Time (WCET) is an important field for real-time system validation. It stands between computer architecture and mathematics, as it relies strongly on variants of abstract interpretation. One of the features that causes the largest uncertainty regarding WCET evaluation for low-level analysis of sequential execution on a single processor is taking Cache Memory-related Delays (CMRD) and Cache-related Preemption Delays (CRPD) correctly into account. Research work from the 1990s provides a good basic framework for this problem as long as a task runs without preemption. But when preemption of tasks is allowed, although several formalisms exist, their predictive power is lower and the usual approach relies on analyses of NP-hard problems. In this article, we want to show some potential advantages of using a formalism inspired by Quantum Computing (QC) to evaluate CMRDs with preemptions while avoiding the NP-hard problem underneath. The experimental results, with a classic (non-quantum) numerical approach, on a selection of Malardalen benchmark programs display very good accuracy, while the complexity of the evaluation is a low-order polynomial of the number of memory accesses. While it is not yet a fully parallel quantum algorithm, we provide a first roadmap on how to reach such an objective. © 2019 Copyright held by the owner/author(s).",Cache memories; Cache related preemption time; Real-time; Worst-case execution,Abstracting; Buffer storage; Computational complexity; Computer architecture; Interactive computer systems; Quantum computers; Quantum theory; Real time systems; Uncertainty analysis; Abstract interpretations; Cache related preemption time; Low-order polynomials; Numerical approaches; Real time; Sequential execution; Worst-case execution; Worst-case execution time; Cache memory
Memory-access-aware safety and profitability analysis for transformation of accelerator-bound OpenMP loops,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069529838&doi=10.1145%2f3333060&partnerID=40&md5=4049bf358f5da1dbfa265fb1f9ecb62d,"Iteration Point Difference Analysis is a new static analysis framework that can be used to determine the memory coalescing characteristics of parallel loops that target GPU offloading and to ascertain safety and profitability of loop transformations with the goal of improving their memory access characteristics. This analysis can propagate definitions through control flow, works for non-affine expressions, and is capable of analyzing expressions that reference conditionally defined values. This analysis framework enables safe and profitable loop transformations. Experimental results demonstrate potential for dramatic performance improvements. GPU kernel execution time across the Polybench suite is improved by up to 25.5× on an Nvidia P100 with benchmark overall improvement of up to 3.2×. An opportunity detected in a SPEC ACCEL benchmark yields kernel speedup of 86.5× with a benchmark improvement of 3.3×. This work also demonstrates how architecture-aware compilers improve code portability and reduce programmer effort. © 2019 Copyright held by the owner/author(s).",GPUs; Heterogeneous computing; Loop collapsing; Loop interchange; Loop transformations; Memory coalescing; OpenMP; Performance portability,Application programming interfaces (API); Computer software portability; Flocculation; Profitability; Program compilers; Static analysis; GPUs; Heterogeneous computing; Loop collapsing; Loop interchange; Loop transformation; Memory coalescing; OpenMP; Performance portability; Memory architecture
Morphable DRAM cache design for hybrid memory systems,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069508776&doi=10.1145%2f3338505&partnerID=40&md5=02863c644fe63bd82828c5b459659f03,"DRAM caches have emerged as an efficient new layer in the memory hierarchy to address the increasing diversity of memory components. When a small amount of fast memory is combined with slow but large memory, the cache-based organization of the fast memory can provide a SW-transparent solution for the hybrid memory systems. In such DRAM cache designs, their effectiveness is affected by the bandwidth and latency of both fast and slow memory. To quantitatively assess the effect of memory configurations and application patterns on the DRAM cache designs, this article first investigates how three prior approaches perform with six hybrid memory scenarios. From the investigation, we observe no single DRAM cache organization always outperforms the other organizations across the diverse hybrid memory configurations and memory access patterns. Based on this observation, this article proposes a reconfigurable DRAM cache design that can adapt to different hybrid memory combinations and workload patterns. Unlike the fixed tag and data arrays of conventional on-chip SRAM caches, this study advocates to exploit the flexibility of DRAM caches, which can store tags and data to DRAM in any arbitrary way. Using a sample-based mechanism, the proposed DRAM cache controller dynamically finds the best organization from three candidates and applies the best one by reconfiguring the tags and data layout in the DRAM cache. Our evaluation shows that the proposed morphable DRAM cache can outperform the fixed DRAM configurations across six hybrid memory configurations. © 2019 Copyright held by the owner/author(s).",DRAM cache; High bandwidth memory; Hybrid memory systems; Nonvolatile memory; Reconfigurable cache design,Bandwidth; Dynamic random access storage; Integrated circuit design; Memory architecture; Static random access storage; Cache design; Cache organization; High bandwidth; Hybrid memory; Memory access patterns; Memory configuration; Non-volatile memory; Transparent solutions; Cache memory
MH cache: A multi-retention STT-RAM-based low-power last-level cache for mobile hardware rendering systems,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069532626&doi=10.1145%2f3328520&partnerID=40&md5=aa7177e4e2942299c8b217953a8ca2de,"Mobile devices have become the most important devices in our life. However, they are limited in battery capacity. Therefore, low-power computing is crucial for their long lifetime. A spin-transfer torque RAM (STT-RAM) has become emerging memory technology because of its low leakage power consumption. We herein propose MH cache, a multi-retention STT-RAM-based cache management scheme for last-level caches (LLC) to reduce their power consumption for mobile hardware rendering systems. We analyzed the memory access patterns of processes and observed how rendering methods affect process behaviors. We propose a cache management scheme that measures write-intensity of each process dynamically and exploits it to manage a power-efficient multi-retention STT-RAM-based cache. Our proposed scheme uses variable threshold for a process’ write-intensity to determine cache line placement. We explain how to deal with the following issue to implement our proposed scheme. Our experimental results show that our techniques significantly reduce the LLC power consumption by 32% and 32.2% in single- and quad-core systems, respectively, compared to a full STT-RAM LLC. © 2019 Copyright held by the owner/author(s).",Full system experiment; Hardware rendering simulation; Memory access pattern analysis,Cache memory; Electric power utilization; Memory architecture; Cache management schemes; Emerging memory technologies; Hardware renderings; Lastlevel caches (LLC); Low-power computing; Memory access patterns; Spin transfer torque; Variable thresholds; Random access storage
Correct-by-construction parallelization of hard real-time avionics applications on off-the-shelf predictable hardware,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069531892&doi=10.1145%2f3328799&partnerID=40&md5=e43c161194fda164c292738828704b59,"We present the first end-to-end modeling and compilation flow to parallelize hard real-time control applications while fully guaranteeing the respect of real-time requirements on off-the-shelf hardware. It scales to thousands of dataflow nodes and has been validated on two production avionics applications. Unlike classical optimizing compilation, it takes as input non-functional requirements (real time, resource limits). To enforce these requirements, the compiler follows a static resource allocation strategy, from coarse-grain tasks communicating over an interconnection network all the way to individual variables and memory accesses. It controls timing interferences resulting from mapping decisions in a precise, safe, and scalable way. © 2019 Copyright held by the owner/author(s).",Code generation; Parallelization; Static scheduling; Timing analysis,Avionics; Real time control; Code Generation; Correct-by-construction; Non-functional requirements; Off-the-shelf hardwares; Optimizing compilation; Parallelizations; Static scheduling; Timing Analysis; Interconnection networks (circuit switching)
An efficient GPU cache architecture for applications with irregular memory access patterns,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067583311&doi=10.1145%2f3322127&partnerID=40&md5=10f796fce5679a55899bf007c735f49d,"GPUs provide high-bandwidth/low-latency on-chip shared memory and L1 cache to efficiently service a large number of concurrent memory requests. Specifically, concurrent memory requests accessing contiguous memory space are coalesced into warp-wide accesses. To support such large accesses to L1 cache with low latency, the size of L1 cache line is no smaller than that of warp-wide accesses. However, such L1 cache architecture cannot always be efficiently utilized when applications generate many memory requests with irregular access patterns especially due to branch and memory divergences that make requests uncoalesced and small. Furthermore, unlike L1 cache, the shared memory of GPUs is not often used in many applications, which essentially depends on programmers. In this article, we propose Elastic-Cache, which can efficiently support both fine-and coarse-grained L1 cache line management for applications with both regular and irregular memory access patterns to improve the L1 cache efficiency. Specifically, it can store 32-or 64-byte words in non-contiguous memory space to a single 128-byte cache line. Furthermore, it neither requires an extra memory structure nor reduces the capacity of L1 cache for tag storage, since it stores auxiliary tags for fine-grained L1 cache line managements in the shared memory space that is not fully used in many applications. To improve the bandwidth utilization of L1 cache with Elastic-Cache for fine-grained accesses, we further propose Elastic-Plus to issue 32-byte memory requests in parallel, which can reduce the processing latency of memory instructions and improve the throughput of GPUs. Our experiment result shows that Elastic-Cache improves the geometric-mean performance of applications with irregular memory access patterns by 104% without degrading the performance of applications with regular memory access patterns. Elastic-Plus outperforms Elastic-Cache and improves the performance of applications with irregular memory access patterns by 131%. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Cache; GPU; Shared memory; Thread,Bandwidth; Graphics processing unit; Memory architecture; Program processors; Band-width utilization; Cache; Cache architecture; Memory access patterns; Memory divergences; Memory structure; Shared memory; Thread; Cache memory
Caliper: Interference estimator for multi-tenant environments sharing architectural resources,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067561447&doi=10.1145%2f3323090&partnerID=40&md5=7bc55e8d46e2c194bdc60af3a4ed0e15,"We introduce Caliper, a technique for accurately estimating performance interference occurring in shared servers. Caliper overcomes the limitations of prior approaches by leveraging a micro-experiment-based technique. In contrast to state-of-the-art approaches that focus on periodically pausing co-running applications to estimate slowdown, Caliper utilizes a strategic phase-triggered technique to capture interference due to co-location. This enables Caliper to orchestrate an accurate and low-overhead interference estimation technique that can be readily deployed in existing production systems.We evaluate Caliper for a broad spectrum of workload scenarios, demonstrating its ability to seamlessly support up to 16 applications running simultaneously and outperform the state-of-the-art approaches. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Cache contention; Datacenter design; DRAM bandwidth contention; Fairness; Interference; Mutii-core; Performance; System software metrics,Software engineering; Wave interference; Cache contention; Datacenter designs; Fairness; Mutii-core; Performance; System softwares
Coordinated CTA combination and bandwidth partitioning for GPU concurrent kernel execution,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067580249&doi=10.1145%2f3326124&partnerID=40&md5=fcb1bea4851ae7066c7a73a58674d260,"Contemporary GPUs support multiple kernels to run concurrently on the same streaming multiprocessors (SMs). Recent studies have demonstrated that such concurrent kernel execution (CKE) improves both resource utilization and computational throughput. Most of the prior works focus on partitioning the GPU resources at the cooperative thread array (CTA) level or the warp scheduler level to improve CKE. However, significant performance slowdown and unfairness are observed when latency-sensitive kernels co-run with bandwidth-intensive ones. The reason is that bandwidth over-subscription from bandwidth-intensive kernels leads to much aggravated memory access latency, which is highly detrimental to latency-sensitive kernels. Even among bandwidth-intensive kernels, more intensive kernels may unfairly consume much higher bandwidth than less-intensive ones. In this article, we first make a case that such problems cannot be sufficiently solved by managing CTA combinations alone and reveal the fundamental reasons. Then, we propose a coordinated approach for CTA combination and bandwidth partitioning. Our approach dynamically detects co-running kernels as latency sensitive or bandwidth intensive. As both the DRAM bandwidth and L2-to-L1 Network-on-Chip (NoC) bandwidth can be the critical resource, our approach partitions both bandwidth resources coordinately along with selecting proper CTA combinations. The key objective is to allocate more CTA resources for latencysensitive kernels and more NoC/DRAM bandwidth resources to NoC-/DRAM-intensive kernels. We achieve it using a variation of dominant resource fairness (DRF). Compared with two state-of-the-art CKE optimization schemes, SMK [52] and WS [55], our approach improves the average harmonic speedup by 78% and 39%, respectively. Even compared to the best possible CTA combinations, which are obtained from an exhaustive search among all possible CTA combinations, our approach improves the harmonic speedup by up to 51% and 11% on average. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Bandwidth management; Concurrent kernel execution; GPGPU; TLP,Dynamic random access storage; Graphics processing unit; Network-on-chip; Program processors; Weaving; Bandwidth management; Bandwidth partitioning; Concurrent kernel execution; GPGPU; Memory access latency; Network-on-chip(NoC); Resource utilizations; Streaming multiprocessors; Bandwidth
The power-optimised software envelope,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067586528&doi=10.1145%2f3321551&partnerID=40&md5=423a4be43be179c7901b6023f380b2f0,"Advances in processor design have delivered performance improvements for decades. As physical limits are reached, refinements to the same basic technologies are beginning to yield diminishing returns. Unsustainable increases in energy consumption are forcing hardware manufacturers to prioritise energy efficiency in their designs. Research suggests that software modifications may be needed to exploit the resulting improvements in current and future hardware. New tools are required to capitalise on this new class of optimisation. In this article, we present the Power Optimised Software Envelope (POSE) model, which allows developers to assess the potential benefits of power optimisation for their applications. The POSE model is metric agnostic and in this article, we provide derivations using the established Energy-Delay Product metric and the novel Energy-Delay Sum and Energy-Delay Distance metrics that we believe are more appropriate for energy-aware optimisation efforts. We demonstrate POSE on three platforms by studying the optimisation characteristics of applications from the Mantevo benchmark suite. Our results show that the Pathfinder application has very little scope for power optimisation while TeaLeaf has the most, with all other applications in the benchmark suite falling between the two. Finally, we extend our POSE model with a formulation known as System Summary POSE-a meta-heuristic that allows developers to assess the scope a system has for energy-aware software optimisation independent of the code being run. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Energy-aware computing; Energy-efficiency; Power optimisation,Application programs; Benchmarking; Energy utilization; Green computing; Optimization; Power management; Benchmark suites; Distance metrics; Energy delay product; Energy-aware computing; Hardware manufacturers; Potential benefits; Power optimisation; Software modification; Energy efficiency
Efficient checkpointing with recompute scheme for non-volatile main memory,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069190431&doi=10.1145%2f3323091&partnerID=40&md5=e22a0221b56895ac1fe7835ed8470dd8,"Future main memory will likely include Non-Volatile Memory. Non-Volatile Main Memory (NVMM) provides an opportunity to rethink checkpointing strategies for providing failure safety to applications. While there are many checkpointing and logging schemes in the literature, their use must be revisited as they incur high execution time overheads as well as a large number of additional writes to NVMM, which may significantly impact write endurance. In this article, we propose a novel recompute-based failure safety approach and demonstrate its applicability to loop-based code. Rather than keeping a fully consistent logging state, we only log enough state to enable recomputation. Upon a failure, our approach recovers to a consistent state by determining which parts of the computation were not completed and recomputing them. Effectively, our approach removes the need to keep checkpoints or logs, thus reducing execution time overheads and improving NVMM write endurance 8 at the expense of more complex recovery. We compare our new approach against logging and checkpointing on five scientific workloads, including tiled matrix multiplication, on a computer system model that was built on gem5 and supports Intel PMEM instruction extensions. For tiled matrix multiplication, our recompute approach incurs an execution time overhead of only 5%, in contrast to 8% overhead with logging and 207% overhead with checkpointing. Furthermore, recompute only adds 7% additional NVMM writes, compared to 111% with logging and 330% with checkpointing. We also conduct experiments on real hardware, allowing us to run our workloads to completion while varying the number of threads used for computation. These experiments substantiate our simulation-based observations and provide a sensitivity study and performance comparison between the Recompute Scheme and Naive Checkpointing. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Computer architecture; Emerging memory technologies; Memory systems,Computer architecture; Digital storage; Emerging memory technologies; Instruction extensions; MAtrix multiplication; Memory systems; Non-volatile main memory; Non-volatile memory; Performance comparison; Scientific workloads; Matrix algebra
Comprehensive Characterization of an Open Source Document Search Engine,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069430551&doi=10.1145%2f3320346&partnerID=40&md5=c5ee5e149ca30c950481bc191202bc6d,"This work performs a thorough characterization and analysis of the open source Lucene search library. The article describes in detail the architecture, functionality, and micro-Architectural behavior of the search engine, and investigates prominent online document search research issues. In particular, we study how intra-server index partitioning affects the response time and throughput, explore the potential use of low power servers for document search, and examine the sources of performance degradation ands the causes of tail latencies. Some of our main conclusions are the following: (a) intra-server index partitioning can reduce tail latencies but with diminishing benefits as incoming query traffic increases, (b) low power servers given enough partitioning can provide same average and tail response times as conventional high performance servers, (c) index search is a CPU-intensive cache-friendly application, and (d) C-states are the main culprits for performance degradation in document search. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",characterization; Document search; evaluation; experimentation; index partitioning; measurement; parallel index search; parallelism; performance; real hardware,Measurement; Software engineering; Document search; evaluation; experimentation; Index partitioning; parallel index search; parallelism; performance; Search engines
HAWS: Accelerating GPU Wavefront Execution through Selective Out-of-order Execution,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065778192&doi=10.1145%2f3291050&partnerID=40&md5=fdc04067360731eebaa5a826aa288e97,"Graphics Processing Units (GPUs) have become an attractive platform for accelerating challenging applications on a range of platforms, from High Performance Computing (HPC) to full-featured smartphones. They can overcome computational barriers in a wide range of data-parallel kernels. GPUs hide pipeline stalls and memory latency by utilizing efficient thread preemption. But given the demands on the memory hierarchy due to the growth in the number of computing cores on-chip, it has become increasingly difficult to hide all of these stalls. In this article, we propose a novel Hint-Assisted Wavefront Scheduler (HAWS) to bypass long-latency stalls. HAWS starts by enhancing a compiler infrastructure to identify potential opportunities that can bypass memory stalls. HAWS includes a wavefront scheduler that can continue to execute instructions in the shadow of a memory stall, executing instructions speculatively, guided by compiler-generated hints. HAWS increases utilization of GPU resources by aggressively fetching/executing speculatively. Based on our simulation results on the AMD Southern Islands GPU architecture, at an estimated cost of 0.4% total chip area, HAWS can improve application performance by 14.6% on average for memory intensive applications. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Computer architecture; GPU architecture; Simulation; Wavefront scheduler,Computer architecture; Computer graphics; Memory architecture; Program compilers; Scheduling; Wavefronts; Application performance; Computational barriers; Estimated costs; High performance computing (HPC); Memory hierarchy; Memory latencies; Out-of-order execution; Simulation; Graphics processing unit
SAQIP: A scalable architecture for quantum information processors,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065723119&doi=10.1145%2f3311879&partnerID=40&md5=e83fc3c4ab84050dc18a989339f6692c,"Proposing an architecture that efficiently compensates for the inefficiencies of physical hardware with extra resources is one of the key issues in quantum computer design. Although the demonstration of quantum systems has been limited to some dozen qubits, scaling the current small-sized lab quantum systems to large-scale quantum systems that are capable of solving meaningful practical problems can be the main goal of much research. Focusing on this issue, in this article a scalable architecture for quantum information processors, called SAQIP, is proposed. Moreover, a flow is presented to map and schedule a quantum circuit on this architecture. Experimental results show that the proposed architecture and design flow decrease the average latency and the average area of quantum circuits by about 81% and 11%, respectively, for the attempted benchmarks. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Mapping; Quantum architecture; Quantum circuits; Scheduling,Computer architecture; Computer hardware; Integrated circuit design; Mapping; Qubits; Scheduling; Design flows; Practical problems; Proposed architectures; Quantum architecture; Quantum circuit; Quantum Information; Quantum system; Scalable architectures; Quantum optics
Transparent acceleration for heterogeneous platforms with compilation to OpenCL,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065774041&doi=10.1145%2f3319423&partnerID=40&md5=ea9abdb9dcd7888327a8a413c5058c1b,"Multi-accelerator platforms combine CPUs and different accelerator architectures within a single compute node. Such systems are capable of processing parallel workloads very efficiently while being more energy efficient than regular systems consisting of CPUs only. However, the architectures of such systems are diverse, forcing developers to port applications to each accelerator using different programming languages, models, tools, and compilers. Developers not only require domain-specific knowledge but also need to understand the low-level accelerator details, leading to an increase in the design effort and costs. To tackle this challenge, we propose a compilation approach and a practical realization called HTrOP that is completely transparent to the user. HTrOP is able to automatically analyze a sequential CPU application, detect computational hotspots, and generate parallel OpenCL host and kernel code. The potential of HTrOP is demonstrated by offloading hotspots to different OpenCL-enabled resources (currently the CPU, the general-purpose GPU, and the manycore Intel Xeon Phi) for a broad set of benchmark applications. We present an in-depth evaluation of our approach in terms of performance gains and energy savings, taking into account all static and dynamic overheads. We are able to achieve speedups and energy savings of up to two orders of magnitude, if an application has sufficient computational intensity, when compared to a natively compiled application. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Multi-accelerator; OpenCL; Runtime system; Transparent acceleration,Acceleration; Benchmarking; Energy conservation; Energy efficiency; Parallel processing systems; Program compilers; Accelerator architectures; Benchmark applications; Computational intensity; Domain-specific knowledge; General purpose gpu; Heterogeneous platforms; OpenCL; Runtime systems; Computer systems programming
SketchDLC: A sketch on distributed deep learning communication via trace capturing,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065700095&doi=10.1145%2f3312570&partnerID=40&md5=bc2a1900e472982e57d1618ab975bc12,"With the fast development of deep learning (DL), the communication is increasingly a bottleneck for distributed workloads, and a series of optimization works have been done to scale out successfully. Nevertheless, the network behavior has not been investigated much yet. We intend to analyze the network behavior and then carry out some research through network simulation. Under this circumstance, an accurate communication measurement is necessary, as it is an effective way to study the network behavior and the basis for accurate simulation. Therefore, we propose to capture the deep learning communication (DLC) trace to achieve the measurement. To the best of our knowledge, we make the first attempt to capture the communication trace for DL training. In this article, we first provide detailed analyses about the communication mechanism of MXNet, which is a representative framework for distributed DL. Secondly, we define the DLC trace format to describe and record the communication behaviors. Third, we present the implementation of method for trace capturing. Finally, we make some statistics and analyses about the distributed DL training, including communication pattern, overlap ratio between computation and communication, computation overhead, synchronization overhead, update overhead, and so forth. Both the statistics and analyses are based on the trace files captured in a cluster with six machines. On the one hand, our trace files provide a sketch on the DLC, which contributes to understanding the communication details. On the other hand, the captured trace files can be used for figuring out various overheads, as they record the communication behaviors of each node. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Communication trace; Deep learning; Distributed system; Parameter server,Software engineering; Communication behavior; Communication mechanisms; Communication pattern; Communication traces; Computation overheads; Distributed systems; Distributed workloads; Statistics and analysis; Deep learning
Efficient and scalable execution of fine-grained dynamic linear pipelines,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065721965&doi=10.1145%2f3307411&partnerID=40&md5=97b558f9d3c8bda6d1d37f8cff7dd2f9,"We present Pipelite, a dynamic scheduler that exploits the properties of dynamic linear pipelines to achieve high performance for fine-grained workloads. The flexibility of Pipelite allows the stages and their data dependences to be determined at runtime. Pipelite unifies communication, scheduling, and synchronization algorithms with suitable data structures. This unified design introduces the local suspension mechanism and a wait-free enqueue operation, which allow efficient dynamic scheduling. The evaluation on a 44-core machine, using programs from three widely used benchmark suites, shows that Pipelite implies low overhead and significantly outperforms the state of the art in terms of speedup, scalability, and memory usage. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Dynamic communication; Dynamic linear pipeline; Dynamic partitioning; Dynamic synchronization; Multi-threading: dynamic scheduling; Parallelization directives,Scheduling; Dynamic communication; Dynamic linear; Dynamic partitioning; Dynamic scheduling; Parallelizations; Pipelines
Efficient data supply for parallel heterogeneous architectures,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065730130&doi=10.1145%2f3310332&partnerID=40&md5=be81ba0cbbffa5a5178fb3bf0459edbb,"Decoupling techniques have been proposed to reduce the amount of memory latency exposed to high-performance accelerators as they fetch data. Although decoupled access-execute (DAE) and more recent decoupled data supply approaches offer promising single-threaded performance improvements, little work has considered how to extend them into parallel scenarios. This article explores the opportunities and challenges of designing parallel, high-performance, resource-efficient decoupled data supply systems. We propose Mercury, a parallel decoupled data supply system that utilizes thread-level parallelism for high-throughput data supply with good portability attributes. Additionally, we introduce some microarchitectural improvements for data supply units to efficiently handle long-latency indirect loads. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Data access optimization; Decoupled architecture; Heterogeneous architecture,Software engineering; Data access optimizations; Decoupled architecture; Decoupling technique; Heterogeneous architectures; High-throughput data; Memory latencies; Resource-efficient; Thread level parallelism; Parallel architectures
Combining Source-adaptive and Oblivious Routing with Congestion Control in High-performance Interconnects using Hybrid and Direct Topologies,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065797070&doi=10.1145%2f3319805&partnerID=40&md5=c057c00b67de90b6be3d47ff1ec036b2,"Hybrid and direct topologies are cost-efficient and scalable options to interconnect thousands of end nodes in high-performance computing (HPC) systems. They offer a rich path diversity, high bisection bandwidth, and a reduced diameter guaranteeing low latency. In these topologies, efficient deterministic routing algorithms can be used to balance smartly the traffic flows among the available routes. Unfortunately, congestion leads these networks to saturation, where the HoL blocking effect degrades their performance dramatically. Among the proposed solutions to deal with HoL blocking, the routing algorithms selecting alternative routes, such as adaptive and oblivious, can mitigate the congestion effects. Other techniques use queues to separate congested flows from non-congested ones, thus reducing the HoL blocking. In this article, we propose a new approach that reduces HoL blocking in hybrid and direct topologies using source-adaptive and oblivious routing. This approach also guarantees deadlock-freedom as it uses virtual networks to break potential cycles generated by the routing policy in the topology. Specifically, we propose two techniques, called Source-Adaptive Solution for Head-of-Line Blocking Avoidance (SASHA) and Oblivious Solution for Head-of-Line Blocking Avoidance (OSHA). Experiment results, carried out through simulations under different traffic scenarios, show that SASHA and OSHA can significantly reduce the HoL blocking. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Congestion management; HoL blocking; HPC; Hybrid and direct topologies; Interconnection networks; Source-adaptive and oblivious routing,Interconnection networks (circuit switching); Topology; Alternative routes; Bisection bandwidth; Congestion management; Deterministic routing algorithms; Head of line blocking; High performance computing systems; HOL blocking; Oblivious routing; Network routing
Schedule synthesis for halide pipelines through reuse analysis,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065727126&doi=10.1145%2f3310248&partnerID=40&md5=745bcc325944366a9a87bafbbf203812,"Efficient code generation for image processing applications continues to pose a challenge in a domain where high performance is often necessary to meet real-time constraints. The inherently complex structure found in most image-processing pipelines, the plethora of transformations that can be applied to optimize the performance of an implementation, as well as the interaction of these optimizations with locality, redundant computation and parallelism, can be indentified as the key reasons behind this issue. Recent domain-specific languages (DSL) such as the Halide DSL and compiler attempt to encourage high-level design-space exploration to facilitate the optimization process. We propose a novel optimization strategy that aims to maximize producer-consumer locality by exploiting reuse in image-processing pipelines. We implement our analysis as a tool that can be used alongside the Halide DSL to automatically generate schedules for pipelines implemented in Halide and test it on a variety of benchmarks. Experimental results on three different multi-core architectures show an average performance improvement of 40% over the Halide Auto-Scheduler and 75% over a state-of-the art approach that targets the PolyMage DSL. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Compiler optimizations; Halide; Image-processing pipelines; Loop optimizations,Computer architecture; Digital subscriber lines; Pipeline processing systems; Pipelines; Problem oriented languages; Program compilers; Compiler optimizations; Domain specific languages; Halide; Image processing applications; Image processing pipeline; Loop optimizations; Multicore architectures; State-of-the-art approach; Image processing
A self-aware resource management framework for heterogeneous multicore SoCs with diverse QoS targets,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064704445&doi=10.1145%2f3319804&partnerID=40&md5=bb414de437fef3529229e9f3a1c75e8d,"In modern heterogeneous MPSoCs, the management of shared memory resources is crucial in delivering end-to-end QoS. Previous frameworks have either focused on singular QoS targets or the allocation of partitionable resources among CPU applications at relatively slow timescales. However, heterogeneous MPSoCs typically require instant response from the memory system where most resources cannot be partitioned. Moreover, the health of different cores in a heterogeneous MPSoC is often measured by diverse performance objectives. In this work, we propose the Self-Aware Resource Allocation framework for heterogeneous MPSoCs. Priority-based adaptation allows cores to use different target performance and self-monitor their own intrinsic health. In response, the system allocates non-partitionable resources based on priorities. The proposed framework meets a diverse range of QoS demands from heterogeneous cores. Moreover, we present a runtime scheme to configure priority-based adaptation so that distinct sensitivities of heterogeneous QoS targets with respect to memory allocation can be accommodated. In addition, the priority of best-effort cores can also be regulated. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Memory efficiency; Memory system; Quality-of-service; Resource allocation,Memory architecture; Multiprocessing systems; Resource allocation; System-on-chip; Heterogeneous cores; Heterogeneous mpsoc; Heterogeneous multicore; Memory efficiency; Memory systems; Performance objective; Resource management framework; Resources based; Quality of service
Memory-side protection with a capability enforcement co-processor,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065137920&doi=10.1145%2f3302257&partnerID=40&md5=894d2898fca36be8c235cb546dfaa102,"Byte-addressable nonvolatile memory (NVM) blends the concepts of storage and memory and can radically improve data-centric applications, from in-memory databases to graph processing. By enabling large-capacity devices to be shared across multiple computing elements, fabric-attached NVM changes the nature of rack-scale systems and enables short-latency direct memory access while retaining data persistence properties and simplifying the software stack. An adequate protection scheme is paramount when addressing shared and persistent memory, but mechanisms that rely on virtual memory paging suffer from the tension between performance (pushing toward large pages) and protection granularity (pushing toward small pages). To address this tension, capabilities are worth revisiting as a more powerful protection mechanism, but the long time needed to introduce new CPU features hampers the adoption of schemes that rely on instruction-set architecture support. This article proposes the Capability Enforcement Co-Processor (CEP), a programmable memory controller that implements fine-grain protection through the capability model without requiring instruction-set support in the application CPU. CEP decouples capabilities from the application CPU instruction-set architecture, shortens time to adoption, and can rapidly evolve to embrace new persistent memory technologies, from NVDIMMs to native NVM devices, either locally connected or fabric attached in rack-scale configurations. CEP exposes an application interface based on memory handles that get internally converted to extended-pointer capabilities. This article presents a proof of concept implementation of a distributed object store (Redis) with CEP. It also demonstrates a capability-enhanced file system (FUSE) implementation using CEP. Our proof of concept shows that CEP provides fine-grain protection while enabling direct memory access from application clients to the NVM, and that by doing so opens up important performance optimization opportunities (up to 4× reduction in latency in comparison to software-based security enforcement) without compromising security. Finally, we also sketch how a future hybrid model could improve the initial implementation by delegating some CEP functionality to a CHERI-enabled processor. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Byte-addressable persistent memory; Rack scale memory organization; Software capabilities,Application programs; Coprocessor; Law enforcement; Nonvolatile storage; Open access; Application interfaces; Instruction set architecture; Memory organizations; Performance optimizations; Persistent memory; Protection mechanisms; Security enforcement; Software capability; Memory architecture
Accelerating in-memory database selections using latency masking hardware threads,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064609036&doi=10.1145%2f3310229&partnerID=40&md5=b8060b9d0695cd84b595581f9c0391a6,"Inexpensive DRAMs have created new opportunities for in-memory data analytics. However, the major bottleneck in such systems is high memory access latency. Traditionally, this problem is solved with large cache hierarchies that only benefit regular applications. Alternatively, many data-intensive applications exhibit irregular behavior. Hardware multithreading can better cope with high latency seen in such applications. This article implements a multithreaded prototype (MTP) on FPGAs for the relational selection operator that exhibits control flow irregularity. On a standard TPC-H query evaluation, MTP achieves a bandwidth utilization of 83%, while the CPU and the GPU implementations achieve 61% and 64%, respectively. Besides being bandwidth efficient, MTP is also 14.2× and 4.2× more power efficient than CPU and GPU, respectively. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Database; FPGA accelerator; Hardware multithreading; Selection operator,Bandwidth; Data Analytics; Database systems; Field programmable gate arrays (FPGA); Query processing; Band-width utilization; Bandwidth efficient; Data-intensive application; Fpga accelerators; GPU implementation; Hardware multithreading; Memory access latency; Selection operators; Dynamic random access storage
Supporting superpages and lightweight page migration in hybrid memory systems,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064659261&doi=10.1145%2f3310133&partnerID=40&md5=474ee9ae9e988e20d65c8e6666abc37a,"Superpages have long been used to mitigate address translation overhead in large-memory systems. However, superpages often preclude lightweight page migration, which is crucial for performance and energy efficiency in hybrid memory systems composed of DRAM and non-volatile memory (NVM). In this article, we propose a novel memory management mechanism called Rainbow to bridge this fundamental conflict between superpages and lightweight page migration. Rainbow manages NVM at the superpage granularity, and uses DRAM to cache frequently accessed (hot) small pages within each superpage. Correspondingly, Rainbow utilizes split TLBs to support different page sizes. By introducing an efficient hot page identification mechanism and a novel NVM-to-DRAM address remapping mechanism, Rainbow supports lightweight page migration without splintering superpages. Experiment results show that Rainbow can significantly reduce applications' TLB misses by 99.9%, and improve application performance (in terms of IPC) by up to 2.9× (45.3% on average) when compared to a state-of-the-art memory migration policy without a superpage support. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Hybrid main memory; Non-volatile memory; Page migration; Superpage,Buffer storage; Distributed computer systems; Energy efficiency; Nonvolatile storage; Address translation; Application performance; Hybrid main memory; Identification mechanism; Memory management; Non-volatile memory; Page migration; Superpage; Dynamic random access storage
Ducati: High-performance Address Translation by Extending TLB Reach of GPU-accelerated Systems,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065716209&doi=10.1145%2f3309710&partnerID=40&md5=789ab0abf95eb2013f7b90b8d8b18c7c,"Conventional on-chip TLB hierarchies are unable to fully cover the growing application working-set sizes. To make things worse, Last-Level TLB (LLT) misses require multiple accesses to the page table even with the use of page walk caches. Consequently, LLT misses incur long address translation latency and hurt performance. This article proposes two low-overhead hardware mechanisms for reducing the frequency and penalty of on-die LLT misses. The first, Unified CAche and TLB (UCAT), enables the conventional on-die Last-Level Cache to store cache lines and TLB entries in a single unified structure and increases on-die TLB capacity significantly. The second, DRAM-TLB, memoizes virtual to physical address translations in DRAM and reduces LLT miss penalty when UCAT is unable to fully cover total application working-set. DRAM-TLB serves as the next larger level in the TLB hierarchy that significantly increases TLB coverage relative to on-chip TLBs. The combination of these two mechanisms, DUCATI, is an address translation architecture that improves GPU performance by 81% (up to 4.5×) while requiring minimal changes to the existing system design. We show that DUCATI is within 20%, 5%, and 2% the performance of a perfect LLT system when using 4KB, 64KB, and 2MB pages, respectively. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Caches; GPU; High bandwidth memory; TLB; Virtual memory,Distributed computer systems; Dynamic random access storage; Graphics processing unit; Physical addresses; Virtual addresses; Address translation; Caches; Hardware mechanism; High bandwidth; Last-level caches; Unified structure; Virtual memory; Virtual-to-physical address translations; Cache memory
Exploring an alternative cost function for combinatorial register-pressure-aware instruction scheduling,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062523093&doi=10.1145%2f3301489&partnerID=40&md5=25bd9ac2899531d584c2a4e936d78ed9,"Multiple combinatorial algorithms have been proposed for doing pre-allocation instruction scheduling with the objective of minimizing register pressure or balancing register pressure and instruction-level parallelism. The cost function that is minimized in most of these algorithms is the peak register pressure (or the peak excess register pressure). In this work, we explore an alternative register-pressure cost function, which is the Sum of Live Interval Lengths (SLIL). Unlike the peak cost function, which captures register pressure only at the highest pressure point in the schedule, the proposed SLIL cost function captures register pressure at all points in the schedule.Minimizing register pressure at all points is desirable in larger scheduling regions with multiple high-pressure points. This article describes a Branch-and-Bound (B&B) algorithm for minimizing the SLIL cost function. The algorithm is based on two SLIL-specific dynamic lower bounds as well as the history utilization technique proposed in our previous work. The proposed algorithm is implemented into the LLVM Compiler and evaluated experimentally relative to our previously proposed B&B algorithm for minimizing the peak excess register pressure. The experimental results show that the proposed algorithm for minimizing the SLIL cost function produces substantially less spilling than the previous algorithm that minimizes the peak cost function. Execution-time results on various processors show that the proposed B&B algorithm significantly improves the performance of many CPU2006 benchmarks by up to 49% relative to LLVM's default scheduler. The geometric-mean improvement for FP2006 on Intel Core i7 is 4.22%. © 2019 Association for Computing Machinery.",Branch-and-bound enumeration; Compiler optimizations; Instruction-level parallelism (ilp); Np-complete problems; Optimal instruction scheduling; Performance optimization; Register pressure reduction,Benchmarking; Computational complexity; Optimization; Program compilers; Scheduling; Compiler optimizations; Instruction level parallelism; Instruction scheduling; Performance optimizations; Register pressure; Cost functions
ITAP: Idle-time-aware power management for GPU execution units,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062494187&doi=10.1145%2f3291606&partnerID=40&md5=6921caa4d107b85297a76a8236372224,"Graphics Processing Units (GPUS) are widely used as the accelerator of choice for applications with massively data-parallel tasks. However, recent studies show that GPUS suffer heavily from resource underutilization, which, combined with their large static power consumption, imposes a significant power overhead. One of the most power-hungry components of a GPU-the execution units-frequently experience idleness when (1) an underutilized warp is issued to the execution units, leading to partial lane idleness, and (2) there is no active warp to be issued for the execution due to warp stalls (e.g., waiting for memory access and synchronization). Although large in total, the idle time of execution units actually comes from short but frequent stalls, leaving little potential for common power saving techniques, such as power-gating. In this article, we propose ITAP, a novel idle-time-aware power management technique, which aims to effectively reduce the static energy consumption of GPU execution units. By taking advantage of different power management techniques (i.e., power-gating and different levels of voltage scaling), ITAP employs three static power reduction modes with different overheads and capabilities of static power reduction. ITAP estimates the idle period length of execution units using prediction and peek-ahead techniques in a synergistic way and then applies the most appropriate static power reduction mode based on the estimated idle period length. We design ITAP to be power-aggressive or performance-aggressive, not both at the same time. Our experimental results on several workloads show that the power-aggressive design of ITAP outperforms the state-of-the-art solution by an average of 27.6% in terms of static energy savings, with less than 2.1% performance overhead. However, the performance-aggressive design of ITAP improves the static energy savings by an average of 16.9%, while keeping the GPU performance almost unaffected (i.e., up to 0.4% performance overhead) compared to the state-of-the-art static energy savings mechanism. © 2019 Copyright held by the owner/author(s).",Execution units; GPUs; Power-gating; Static power; Voltage-scaling,Computer graphics; Energy conservation; Energy utilization; Industrial management; Program processors; Voltage scaling; Aggressive designs; Execution units; GPUs; Power gatings; Power management techniques; Static power; Static power consumption; Static power reduction; Graphics processing unit
Exploiting SIMD asymmetry in Arm-to-X86 dynamic binary translation,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062236480&doi=10.1145%2f3301488&partnerID=40&md5=b5e2f0b5cd3af268dbb31402421084e2,"Single instruction multiple data (SIMD) has been adopted for decades because of its superior performance and power efficiency. The SIMD capability (i.e., width, number of registers, and advanced instructions) has diverged rapidly on different SIMD instruction-set architectures (ISAs). Therefore, migrating existing applications to another host ISA that has fewer but longer SIMD registers and more advanced instructions raises the issues of asymmetric SIMD capability. To date, this issue has been overlooked and the host SIMD capability is underutilized, resulting in suboptimal performance. In this article, we present a novel binary translation technique called spill-aware superword level parallelism (saSLP), which combines short ARMv8 instructions and registers in the guest binaries to exploit the x86 AVX2 host's parallelism, register capacity, and gather instructions. Our experiment results show that saSLP improves the performance by 1.6× (2.3×) across a number of benchmarks and reduces spilling by 97% (99%) for ARMv8 to x86 AVX2 (AVX-512) translation. Furthermore, with AVX2 (AVX-512) gather instructions, saSLP speeds up several data-irregular applications that cannot be vectorized on ARMv8 NEON by up to 3.9× (4.2×). © 2019 Association for Computing Machinery.",Dynamic binary translation; SIMD; SLP vectorization,ARM processors; Computer architecture; Binary translation; Dynamic binary translation; Irregular applications; SIMD; Single instruction multiple data; Sub-optimal performance; Superword Level Parallelism; Vectorization; Benchmarking
"Accelerating Synchronization Using Moving Compute to Data Model at 1,000-core Multicore Scale",2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062239813&doi=10.1145%2f3300208&partnerID=40&md5=a120b8a2b833b9ed0c0fc01f61263fca,"Thread synchronization using shared memory hardware cache coherence paradigm is prevalent in multicore processors. However, as the number of cores increase on a chip, cache line ping-pong prevents performance scaling for algorithms that deploy fine-grain synchronization. This article proposes an in-hardware moving computation to data model (MC) that pins shared data at dedicated cores. The critical code sections are serialized and executed at these cores in a spatial setting to enable data locality optimizations. In-hardware messages enable non-blocking and blocking communication between cores, without involving the cache coherence protocol. The in-hardware MC model is implemented on Tilera Tile-Gx72 multicore platform to evaluate 8- to 64-core count scale. A simulated RISC-V multicore environment is built to further evaluate the performance scaling advantages of the MC model at 1,024-cores scale. The evaluation using graph and machine-learning benchmarks illustrates that atomic instructions based synchronization scales up to 512 cores, and the MC model at the same core count outperforms by 27% in completion time and 39% in dynamic energy consumption. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Locality; Multicore; Synchronization,Cache memory; Energy utilization; Learning systems; Cache coherence protocols; Data locality optimization; Dynamic energy consumption; Locality; Multi core; Multi-core processor; Multicore environments; Thread synchronization; Synchronization
A system-level simulator for RRAM-based neuromorphic computing chips,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061214122&doi=10.1145%2f3291054&partnerID=40&md5=d4e691aaeb9f98fea0c875a17c55d3a5,"Advances in non-volatile resistive switching random access memory (RRAM) have made it a promising memory technology with potential applications in low-power and embedded in-memory computing devices owing to a number of advantages such as low-energy consumption, low area cost and good scaling. There have been proposals to employ RRAM in architecting chips for neuromorphic computing and artificial neural networks where matrix-vector multiplication can be computed in the analog domain in a single timestep. However, it is challenging to employ RRAMdevices in neuromorphic chips owing to the non-ideal behavior of RRAM. In this article, we propose a cycle-accurate and scalable system-level simulator that can be used to study the effects of using RRAM devices in neuromorphic computing chips. The simulator models a spatial neuromorphic chip architecture containing many neural cores with RRAM crossbars connected via a Network-on-Chip (NoC). We focus on system-level simulation and demonstrate the effectiveness of our simulator in understanding how non-linear RRAM effects such as stuck-at-faults (SAFs), write variability, and random telegraph noise (RTN) can impact an application's behavior. By using our simulator, we show that RTN and write variability can have adverse effects on an application. Nevertheless, we show that these effects can be mitigated through proper design choices and the implementation of a write-verify scheme. © 2019 Copyright held by the owner/author(s).",Neuromorphic computing; RRAM; simulator,Energy utilization; Green computing; Neural networks; RRAM; Simulators; Low energy consumption; Matrix vector multiplication; Network-on-chip(NoC); Neuromorphic computing; Random access memory; Random telegraph noise; System level simulation; System level simulator; Network-on-chip
Blaze-tasks: A framework for computing parallel reductions over tasks,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061212474&doi=10.1145%2f3293448&partnerID=40&md5=e8c06cd45e315dce40034704b2b7b80b,"Compared to threads, tasks are a more fine-grained alternative. The task parallel programming model offers benefits in terms of better performance portability and better load-balancing for problems that exhibit nonuniform workloads. A common scenario of task parallel programming is that a task is recursively decomposed into smaller sub-tasks. Depending on the problem domain, the number of created sub-tasks may be nonuniform, thereby creating potential for significant load imbalances in the system. Dynamic load-balancing mechanisms will distribute the tasks across available threads. The final result of a computation may be modeled as a reduction over the results of all sub-tasks. This article describes a simple, yet effective prototype framework, Blaze-Tasks, for task scheduling and task reductions on shared memory architectures. The framework has been designed with lock-free techniques and generic programming principles in mind. Blaze-Tasks is implemented entirely in C++17 and is thus portable. To load-balance the computation, Blaze-Tasks uses task stealing. To manage contention on a task pool, the number of lock-free attempts to steal a task depends on the distance between thief and pool owner and the estimated number of tasks in a victim's pool. This article evaluates the Blaze framework on Intel and IBM dual-socket systems using nine benchmarks and compares its performance with other task parallel frameworks. While Cilk outperforms Blaze on Intel on most benchmarks, the evaluation shows that Blaze is competitive with OpenMP and other library-based implementations. On IBM, the experiments show that Blaze outperforms other approaches on most benchmarks. © 2019 Copyright held by the owner/author(s).",lock-free algorithms; reductions; Tasks,Application programming interfaces (API); Distributed computer systems; Lakes; Locks (fasteners); Memory architecture; Parallel programming; Reduction; Generic programming; Load imbalance; Lock-free algorithms; Performance portability; Problem domain; Shared memory architecture; Task-scheduling; Tasks; Benchmarking
Improving thread-level parallelism in GPUs through expanding register file to scratchpad memory,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061205614&doi=10.1145%2f3280849&partnerID=40&md5=3324ea04683ec0c060e9b73acab1af6e,"Modern Graphic Processing Units (GPUs) have become pervasive computing devices in datacenters due to their high performance with massive thread level parallelism (TLP). GPUs are equipped with large register files (RF) to support fast context switch between massive threads and scratchpad memory (SPM) to support inter-thread communication within the cooperative thread array (CTA). However, the TLP of GPUs is usually limited by the inefficient resource management of register file and scratchpad memory. This inefficiency also leads to register file and scratchpad memory underutilization. To overcome the above inefficiency,we propose a new resource management approach EXPARS for GPUs. EXPARS provides a larger register file logically by expanding the register file to scratchpad memory. When the available register file becomes limited, our approach leverages the underutilized scratchpad memory to support additional register allocation. Therefore, more CTAs can be dispatched to SMs, which improves the GPU utilization. Our experiments on representative benchmark suites show that the number of CTAs dispatched to each SM increases by 1.28× on average. In addition, our approach improves the GPU resource utilization significantly, with the register file utilization improved by 11.64% and the scratchpad memory utilization improved by 48.20% on average. With better TLP, our approach achieves 20.01% performance improvement on average with negligible energy overhead. © 2018 Copyright is held by the owner/author(s).",GPU; register file; resource utilization; scratchpad memory,Cooperative communication; Graphics processing unit; Natural resources management; Program processors; Resource allocation; Ubiquitous computing; Graphic processing units (GPUs); Pervasive computing devices; Register allocation; Register files; Resource management; Resource utilizations; Scratch pad memory; Thread-level parallelism; Memory architecture
Static prediction of silent stores,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061210099&doi=10.1145%2f3280848&partnerID=40&md5=e6e1856f013eba997645d3ae988660c5,"A store operation is called ""silent"" if it writes in memory a value that is already there. The ability to detect silent stores is important, because they might indicate performance bugs, might enable code optimizations, and might reveal opportunities of automatic parallelization, for instance. Silent stores are traditionally detected via profiling tools. In this article, we depart from this methodology and instead explore the following question: Is it possible to predict silentness by analyzing the syntax of programs? The process of building an answer to this question is interesting in itself, given the stochastic nature of silent stores, which depend on data and coding style. To build such an answer, we have developed a methodology to classify store operations in terms of syntactic features of programs. Based on such features, we develop different kinds of predictors, some of which go much beyond what any trivial approach could achieve. To illustrate how static prediction can be employed in practice, we use it to optimize programs running on nonvolatile memory systems. © 2018 Copyright is held by the owner/author(s).",code optimization; machine learning; nonvolatile memory; Silent stores; static analysis,Codes (symbols); Forecasting; Learning systems; Nonvolatile storage; Stochastic systems; Syntactics; Automatic Parallelization; Code optimization; Non-volatile memory; Performance bugs; Profiling tools; Silent stores; Stochastic nature; Syntactic features; Static analysis
Predicting new workload or CPU performance by analyzing public datasets,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061216461&doi=10.1145%2f3284127&partnerID=40&md5=8804937563ba3aed102fc213426b21f7,"The marketplace for general-purpose microprocessors offers hundreds of functionally similar models, differing by traits like frequency, core count, cache size, memory bandwidth, and power consumption. Their performance depends not only on microarchitecture, but also on the nature of the workloads being executed. Given a set of intended workloads, the consumer needs both performance and price information to make rational buying decisions. Many benchmark suites have been developed tomeasure processor performance, and their results for large collections of CPUs are often publicly available. However, repositories of benchmark results are not always helpful when consumers need performance data for new processors or new workloads. Moreover, the aggregate scores for benchmark suites designed to cover a broad spectrum of workload types can be misleading. To address these problems,we have developed a deep neural network (DNN) model, andwe have used it to learn the relationship between the specifications of Intel CPUs and their performance on the SPEC CPU2006 and Geekbench 3 benchmark suites.We show that we can generate useful predictions for new processors and new workloads.We also cross-predict the two benchmark suites and compare their performance scores. The results quantify the self-similarity of these suites for the first time in the literature. This work should discourage consumers from basing purchasing decisions exclusively on Geekbench 3, and it should encourage academics to evaluate research using more diverse workloads than the SPEC CPU suites alone. © 2019 Copyright held by the owner/author(s).",benchmarking; data mining; performance comparison; Performance prediction,Data mining; Deep neural networks; Forecasting; Program processors; General-purpose microprocessors; Micro architectures; Performance comparison; Performance prediction; Price information; Processor performance; Purchasing decisions; Self-similarities; Benchmarking
Decoupled fused cache: Fusing a decoupled LLC with a DRAM cache,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061202760&doi=10.1145%2f3293447&partnerID=40&md5=608a64e30d5142852a5bde2635deff82,"DRAM caches have shown excellent potential in capturing the spatial and temporal data locality of applications capitalizing on advances of 3D-stacking technology; however, they are still far from their ideal performance. Besides the unavoidable DRAM access to fetch the requested data, tag access is in the critical path, adding significant latency and energy costs. Existing approaches are not able to remove these overheads and in some cases limit DRAM cache design options. For instance, caching DRAM cache tags adds constant latency to every access; accessing the DRAM cache using the TLB calls for OS support and DRAM cachelines as large as a page; reusing the last-level cache (LLC) tags to access the DRAM cache limits LLC performance as it requires indexing the LLC using higher-order address bits. In this article, we introduce Decoupled Fused Cache, a DRAM cache design that alleviates the cost of tag accesses by fusing DRAM cache tags with the tags of the on-chip LLC without affecting LLC performance. In essence, the Decoupled Fused Cache relies in most cases on the LLC tag access to retrieve the required information for accessing the DRAM cache while avoiding additional overheads. Compared to current DRAM cache designs of the same cacheline size, Decoupled Fused Cache improves system performance by 6% on average and by 16% to 18% for large cacheline sizes. Finally, Decoupled Fused Cache reduces DRAM cache traffic by 18% and DRAM cache energy consumption by 7%. © 2018 Copyright is held by the owner/author(s).",3D stacking; Caches; DRAM; memory; processor,Data storage equipment; Dynamic random access storage; Energy utilization; Three dimensional integrated circuits; 3D stacking; 3D stacking technology; Cache energy consumption; Caches; Critical Paths; Ideal performance; Lastlevel caches (LLC); processor; Integrated circuit design
GenMatcher: A generic clustering-based arbitrary matching framework,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061202635&doi=10.1145%2f3281663&partnerID=40&md5=d852fd1a2acefc57f4ac947b0f8aaf28,"Packet classification methods rely upon packet content/header matching against rules. Thus, throughput of matching operations is critical in many networking applications. Further,with the advent of Software Defined Networking (SDN), efficient implementation of software approaches to matching are critical for the overall system performance. This article presents1 GenMatcher, a generic, software-only, arbitrary matching framework for fast, efficient searches. The key idea of our approach is to represent arbitrary rules with efficient prefix-based tries. To support arbitrary wildcards, we rearrange bits within the rules such that wildcards accumulate to one side of the bitstring. Since many non-contiguous wildcards often remain, we use multiple prefix-based tries. The main challenge in this context is to generate efficient trie groupings and expansions to support all arbitrary rules. Finding an optimal mix of grouping and expansion is an NP-complete problem. Our contribution includes a novel, clustering-based grouping algorithm to group rules based upon their bit-level similarities. Our algorithm generates near-optimal trie groupings with low configuration times and provides significantly higher match throughput compared to prior techniques. Experiments with synthetic traffic show that our method can achieve a 58.9X speedup compared to the baseline on a single core processor under a given memory constraint. © 2018 Copyright is held by the owner/author(s).",Arbitrary matching; correlation clustering; performance tradeoff,Computational complexity; Optimization; Arbitrary matching; Correlation clustering; Efficient implementation; Networking applications; Packet classification; Performance trade-off; Single-core processors; Software defined networking (SDN); Clustering algorithms
Energy-efficient runtime management of heterogeneous multicores using online projection,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061207548&doi=10.1145%2f3293446&partnerID=40&md5=b2185d0deb9bf8a56c9c0578be159a41,"Heterogeneous multicores offer flexibility in the form of different core types and Dynamic Voltage and Frequency Scaling (DVFS), defining a vast configuration space. The optimal configuration choice is not always straightforward, even for single applications, and becomes a very difficult problem for dynamically changing scenarios of concurrent applications with unpredictable spawn and termination times and individual performance requirements. This article proposes an integrated approach for runtime decision making for energy efficiency on such systems. The approach consists of a model that predicts performance and power for any possible decision and low-complexity heuristics that use this model to evaluate a subset of possible decisions to choose the best. The model predicts performance by projecting standalone application profiling data to the current status of the system and power by using a set of platform-specific parameters that are determined only once for a given system and are independent of the application mix. Our approach is evaluated with a plethora of dynamic, multi-application scenarios. When considering best effort performance to be adequate, our runtime achieves on average 3% higher energy efficiency compared to the powersave governor and 2× better compared to the other linux governors. Moreover, when also considering individual applications' performance requirements, our runtime is able to satisfy them, giving away 18% of the system's energy efficiency compared to the powersave, which, however, misses the performance targets by 23%; at the same time, our runtime maintains an efficiency advantage of about 55% compared to the other governors, which also satisfy the performance constraints. © 2019 Copyright held by the owner/author(s).",dynamic voltage and frequency scaling; energy efficiency; Heterogeneous multicores; runtime management,Computer operating systems; Decision making; Dynamic frequency scaling; Governors; Voltage scaling; Dynamic voltage and frequency scaling; Heterogeneous Multi-Cores; Individual performance; Integrated approach; Performance constraints; Performance requirements; Runtime management; Standalone applications; Energy efficiency
Dual-page checkpointing: An architectural approach to efficient data persistence for in-memory applications,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061216522&doi=10.1145%2f3291057&partnerID=40&md5=a379da700f3e898c305ae38758bd9308,"Data persistence is necessary for many in-memory applications. However, the disk-based data persistence largely slows down in-memory applications. Emerging non-volatile memory (NVM) offers an opportunity to achieve in-memory data persistence at the DRAM-level performance. Nevertheless, NVM typically requires a software library to operate NVM data, which brings significant overhead. This article demonstrates that a hardware-based high-frequency checkpointing mechanism can be used to achieve efficient in-memory data persistence on NVM. To maintain checkpoint consistency, traditional logging and copy-on-write techniques incur excessive NVMwrites that impair both performance and endurance of NVM; recent work attempts to solve the issue but requires a large amount of metadata in the memory controller. Hence, we design a new dual-page checkpointing system, which achieves low metadata cost and eliminates most excessive NVM writes at the same time. It breaks the traditional trade-off between metadata space cost and extra data writes. Our solution outperforms the state-of-the-art NVM software libraries by 13.6× in throughput, and leads to 34% less NVM wear-out and 1.28× higher throughput than state-of-the-art hardware checkpointing solutions, according to our evaluation with OLTP, graph computing, and machinelearning workloads. © 2019 Copyright held by the owner/author(s).",checkpointing; crash consistency; Data persistence; non-volatile memory (NVM),Economic and social effects; Metadata; Nonvolatile storage; Architectural approach; Check pointing; crash consistency; Data persistence; Emerging non-volatile memory; Memory applications; Non-volatile memory; Software libraries; Dynamic random access storage
Bandwidth and locality aware task-stealing for manycore architectures with bandwidth-asymmetric memory,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061195804&doi=10.1145%2f3291058&partnerID=40&md5=17e7a44689e18134c667911c6d0a6de0,"Parallel computers now start to adopt Bandwidth-Asymmetric Memory architecture that consists of traditional DRAM memory and new High Bandwidth Memory (HBM) for high memory bandwidth. However, existing task schedulers suffer from low bandwidth usage and poor data locality problems in bandwidthasymmetric memory architectures. To solve the two problems, we propose a Bandwidth and Locality Aware Task-stealing (BATS) system, which consists of an HBM-aware data allocator, a bandwidth-aware traffic balancer, and a hierarchical task-stealing scheduler. Leveraging compile-time code transformation and run-time data distribution, the data allocator enables HBM usage automatically without user interference. According to data access hotness, the traffic balancer migrates data to balance memory traffic across memory nodes proportional to their bandwidth. The hierarchical scheduler improves data locality at runtime without a priori program knowledge. Experiments on an Intel Knights Landing server that adopts bandwidth-asymmetric memory show that BATS reduces the execution time of memory-bound programs up to 83.5% compared with traditional task-stealing schedulers. © 2018 Copyright is held by the owner/author(s).",bandwidth; data locality; runtime scheduling; Task-stealing,Balancing; Bandwidth; Cosine transforms; Dynamic random access storage; Metadata; Scheduling; Bandwidth-aware; Data locality; High memory bandwidth; Locality aware; Many-core architecture; Parallel computer; Run-time scheduling; Task-stealing; Memory architecture
SCORE: A novel scheme to efficiently cache overlong ECCs in NAND flash memory,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061187710&doi=10.1145%2f3291052&partnerID=40&md5=c42a2d40072f7c93c3702a5a62a641f8,"Technology scaling and program/erase cycling result in an increasing bit error rate in NAND flash storage. Some solid state drives (SSDs) adopt overlong error correction codes (ECCs), whose redundancy size exceeds the spare area limit of flash pages, to protect user data for improved reliability and lifetime. However, the read performance is significantly degraded, because a logical data page and its ECC redundancy are stored in two flash pages. In this article, we find that caching ECCs has a large potential to reduce flash reads by achieving higher hit rates, compared to caching data. Then, we propose a novel scheme to efficiently cache overlong ECCs, called SCORE, to improve the SSD performance. Exceeding ECC redundancy (called ECC residues) of logically consecutive data pages are grouped into ECC pages. SCORE partitions RAM to cache both data pages and ECC pages in a workload-adaptive manner. Finally, we verify SCORE using extensive trace-driven simulations. The results show that SCORE obtains high ECC hit rates without sacrificing data hit rates, thus improving the read performance by an average of 22% under various workloads, compared to the state-of-the-art schemes. © 2018 Association for Computing Machinery.",cache partitioning; overlong ECC; Solid state drive,Bit error rate; Error correction; Memory architecture; NAND circuits; Random access storage; Redundancy; Cache partitioning; Error correction codes (ECCs); NAND flash memory; overlong ECC; Solid state drives; State-of-the-art scheme; Technology scaling; Trace driven simulation; Flash memory
Automated software protection for the masses against side-channel attacks,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061179260&doi=10.1145%2f3281662&partnerID=40&md5=1f4eae367e5574062cb21fd749806ec2,"We present an approach and a tool to answer the need for effective, generic, and easily applicable protections against side-channel attacks. The protection mechanism is based on code polymorphism, so that the observable behaviour of the protected component is variable and unpredictable to the attacker. Our approach combines lightweight specialized runtime code generation with the optimization capabilities of static compilation. It is extensively configurable. Experimental results show that programs secured by our approach present strong security levels and meet the performance requirements of constrained systems. © 2018 Copyright is held by the owner/author(s).",compilation; hiding; polymorphism; runtime code generation; Side-channel attack,Codes (symbols); Polymorphism; compilation; Constrained systems; hiding; Optimization capabilities; Performance requirements; Protection mechanisms; Run-time code generation; Software protection; Side channel attack
The art of getting deep neural networks in shape,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061178730&doi=10.1145%2f3291053&partnerID=40&md5=ae85003e9cf5acae5323c962b448417d,"Training a deep neural network (DNN) involves selecting a set of hyperparameters that define the network topology and influence the accuracy of the resulting network. Often, the goal is to maximize prediction accuracy on a given dataset. However, non-functional requirements of the trained network - such as inference speed, size, and energy consumption - can be very important as well. In this article, we aim to automate the process of selecting an appropriate DNN topology that fulfills both functional and non-functional requirements of the application. Specifically, we focus on tuning two important hyperparameters, depth and width, which together define the shape of the resulting network and directly affect its accuracy, speed, size, and energy consumption. To reduce the time needed to search the design space, we train a fraction of DNNs and build a model to predict the performances of the remaining ones.We are able to produce tuned ResNets, which are up to 4.22 times faster than original depth-scaled ResNets on a batch of 128 images while matching their accuracy. © 2019 Copyright held by the owner/author(s).",computer vision; Deep neural networks; parallel processing,Computer vision; Energy utilization; Topology; Design spaces; Hyperparameters; Network topology; Non-functional requirements; Parallel processing; Prediction accuracy; Deep neural networks
Speeding up iterative polyhedral schedule optimization with surrogate performance models,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061198394&doi=10.1145%2f3291773&partnerID=40&md5=81d6a0e628f5631a938ebe2cc535dee1,"Iterative program optimization is known to be able to adapt more easily to particular programs and target hardware than model-based approaches. An approach is to generate random program transformations and evaluate their profitability by applying them and benchmarking the transformed program on the target hardware. This procedure's large computational effort impairs its practicality tremendously, though. To address this limitation, we pursue the guidance of a genetic algorithm for program optimization via feedback from surrogate performance models. We train the models on program transformations that were evaluated during previous iterative optimizations. Our representation of programs and program transformations refers to the polyhedron model. The representation is particularly meaningful for an optimization of loop programs that profit a from coarse-grained parallelization for execution on modern multicore-CPUs. Our evaluation reveals that surrogate performance models can be used to speed up the optimization of loop programs.We demonstrate that we can reduce the benchmarking effort required for an iterative optimization and degrade the resulting speedups by an average of 15%. © 2018 Copyright held by the owner/author(s).",Automatic loop optimization; CART; genetic algorithm; OpenMP; parallelization; polyhedron model; random forests; supervised learning; tiling,Application programming interfaces (API); Benchmarking; Decision trees; Genetic algorithms; Geometry; Iterative methods; Profitability; Program processors; Structured programming; Supervised learning; CART; Loop optimizations; OpenMP; Parallelizations; Polyhedron models; Random forests; tiling; Multicore programming
RAGuard: An efficient and user-transparent hardware mechanism against ROP attacks,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061199809&doi=10.1145%2f3280852&partnerID=40&md5=0fa69b377b51eff0586e06f870f4086b,"Control-flow integrity (CFI) is a general method for preventing code-reuse attacks, which utilize benign code sequences to achieve arbitrary code execution. CFI ensures that the execution of a program follows the edges of its predefined static Control-Flow Graph: Any deviation that constitutes a CFI violation terminates the application. Despite decades of research effort, there are still several implementation challenges in efficiently protecting the control flow of function returns (Return-Oriented Programming attacks). The set of valid return addresses of frequently called functions can be large and thus an attacker could bend the backwardedge CFI by modifying an indirect branch target to another within the valid return set. This article proposes RAGuard, an efficient and user-transparent hardware-based approach to prevent Return-Oreiented Programming attacks. RAGuard binds a message authentication code (MAC) to each return address to protect its integrity. To guarantee the security of the MAC and reduce runtime overhead: RAGuard (1) computes theMAC by encrypting the signature of a return address with AES-128, (2) develops a key management module based on a Physical Unclonable Function (PUF) and a True Random Number Generator (TRNG), and (3) uses a dedicated register to reduce MACs' load and store operations of leaf functions.We have evaluated our mechanism based on the open-source LEON3 processor and the results show that RAGuard incurs acceptable performance overhead and occupies reasonable area. © 2018 Association for Computing Machinery.",AES-128; Code-reuse attacks; key management; message authentication code; PUF; return-oriented programming attacks,Application programs; Authentication; Codes (symbols); Data flow analysis; Flow graphs; Number theory; Open source software; Random number generation; AES-128; Code reuse; Key management; Message authentication codes; Return-oriented programming; Cryptography
POWAR: Power-aware routing in HPC networks with on/off links,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061180083&doi=10.1145%2f3293445&partnerID=40&md5=9e9098321a0934eee90f3bab418797e7,"In order to save energy in HPC interconnection networks, one usual proposal is to switch idle links into a low-power mode after a certain time without any transmission, as IEEE Energy Efficient Ethernet standard proposes. Extending the low-power mode mechanism,we propose POWer-Aware Routing (POWAR), a simple power-aware routing and selection function for fat-tree and torus networks. POWAR adapts the amount of network links that can be used, taking into account the network load, and obtaining great energy savings in the network (55%-65%) and the entire system (9%-10%) with negligible performance overhead. © 2019 Copyright held by the owner/author(s).",high-performance computing; interconnection networks; power saving; routing algorithms; Switch architecture,Electric power system interconnection; Energy conservation; Energy efficiency; Interconnection networks (circuit switching); Low power electronics; Network routing; Routing algorithms; Energy-efficient ethernets; High performance computing; Low power modes; Power savings; Power-aware routing; Selection function; Switch architectures; Torus networks; Power management (telecommunication)
An autotuning framework for scalable execution of tiled code via iterative polyhedral compilation,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061198399&doi=10.1145%2f3293449&partnerID=40&md5=347bae2d5239b5c83245a376d405ec8d,"On modern many-core CPUs, performance tuning against complex memory subsystems and scalability for parallelism is mandatory to achieve their potential. In this article, we focus on loop tiling, which plays an important role in performance tuning, and develop a novel framework that analytically models the load balance and empirically autotunes unpredictable cache behaviors through iterative polyhedral compilation using LLVM/Polly. From an evaluation on many-core CPUs, we demonstrate that our autotuner achieves a performance superior to those that use conventional static approaches andwell-known autotuning heuristics. Moreover, our autotuner achieves almost the same performance as a brute-force search-based approach. © 2019 Copyright held by the owner/author(s).",Iterative polyhedral compilation; load balancing; loop tiling; scalability for many-core CPUs,Cache memory; Resource allocation; Scalability; Brute force search; Cache behavior; Iterative polyhedral compilation; Loop tiling; Many core; Memory subsystems; Performance tuning; Static approach; Program processors
AUKE: Automatic kernel code generation for an analogue SIMD focal-plane sensor-processor array,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061198676&doi=10.1145%2f3291055&partnerID=40&md5=e4e968f0358ffbe90ca9ff3a81c2ea1e,"Focal-plane Sensor-Processor Arrays (FPSPs) are new imaging devices with parallel Single Instruction Multiple Data (SIMD) computational capabilities built into every pixel. Compared to traditional imaging devices, FPSPs allow for massive pixel-parallel execution of image processing algorithms. This enables the application of certain algorithms at extreme frame rates (>10,000 frames per second). By performing some earlystage processing in-situ, systems incorporating FPSPs can consume less power compared to conventional approaches using standard digital cameras. In this article, we explore code generation for an FPSP whose 256 × 256 processors operate on analogue signal data, leading to further opportunities for power reduction- and additional code synthesis challenges. While rudimentary image processing algorithms have been demonstrated on FPSPs before, progress with higher-level computer vision algorithms has been sparse due to the unique architecture and limits of the devices. This article presents a code generator for convolution filters for the SCAMP-5 FPSP,with applications in many high-level tasks such as convolutional neural networks, pose estimation, and so on. The SCAMP-5 FPSP has no effective multiply operator. Convolutions have to be implemented through sequences of more primitive operations such as additions, subtractions, and multiplications/divisions by two.We present a code generation algorithm to optimise convolutions by identifying common factors in the different weights and by determining an optimised pattern of pixel-to-pixel data movements to exploit them. We present evaluation in terms of both speed and energy consumption for a suite of well-known convolution filters. Furthermore, an application of the method is shown by the implementation of a Viola-Jones face detection algorithm. © 2019 Copyright held by the owner/author(s).",automatic code generation; Focal-plane sensor-processor arrays; kernel filtering,Codes (symbols); Convolution; Energy utilization; Face recognition; Focusing; In situ processing; Neural networks; Parallel processing systems; Petroleum reservoir evaluation; Pixels; Automatic code generations; Code generation algorithm; Computer vision algorithms; Convolutional neural network; Focal-plane sensor; Image processing algorithm; Kernel filtering; Single instruction multiple data; Automatic programming
Processor-tracing guided region formation in dynamic binary translation,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061176910&doi=10.1145%2f3281664&partnerID=40&md5=c33687fe36fe8e36546470680216fcfa,"Region formation is an important step in dynamic binary translation to select hot code regions for translation and optimization. The quality of the formed regions determines the extent of optimizations and thus determines the final execution performance. Moreover, the overall performance is very sensitive to the formation overhead, because region formation can have a non-trivial cost. For addressing the dual issues of region quality and region formation overhead, this article presents a lightweight region formation method guided by processor tracing, e.g., Intel PT.We leverage the branch history information stored in the processor to reconstruct the program execution profile and effectively form high-quality regions with low cost. Furthermore, we present the designs of lightweight hardware performance monitoring sampling and the branch instruction decode cache to minimize region formation overhead. Using ARM64 to x86-64 translations, the experiment results show that our method achieves a performance speedup of up to 1.53× (1.16× on average) for SPEC CPU2006 benchmarks with reference inputs, compared to the well-known software-based trace formation method, Next Executing Tail (NET). The performance results of x86-64 to ARM64 translations also show a speedup of up to 1.25× over NET for CINT2006 benchmarks with reference inputs. The comparison with a relaxed NETPlus region formation method further demonstrates that our method achieves the best performance and lowest compilation overhead. © 2018 Copyright is held by the owner/author(s).",Dynamic binary translation; hardware performance monitoring; next executing tail; processor tracing; region formation,Program processors; Dynamic binary translation; Hardware performance; next executing tail; processor tracing; region formation; Benchmarking
Metric selection for GPU kernel classification,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061179192&doi=10.1145%2f3295690&partnerID=40&md5=a8f9e8e481f291c9d0b46706298012ce,"Graphics Processing Units (GPUs) are vastly used for running massively parallel programs. GPU kernels exhibit different behavior at runtime and can usually be classified in a simple form as either ""compute-bound"" or ""memory-bound."" Recent GPUs are capable of concurrently running multiple kernels, which raises the question of how to most appropriately schedule kernels to achieve higher performance. In particular, coscheduling of compute-bound and memory-bound kernels seems promising. However, its benefits as well as drawbacks must be determined along with which kernels should be selected for a concurrent execution. Classifying kernels can be performed online by instrumentation based on performance counters. This work conducts a thorough analysis of the metrics collected from various benchmarks fromRodinia and CUDA SDK. The goal is to find the minimum number of effective metrics that enables online classification of kernels with a low overhead. This study employs a wrapper-based feature selection method based on the Fisher feature selection criterion. The results of experiments show that to classify kernels with a high accuracy, only three and five metrics are sufficient on a Kepler and a Pascal GPU, respectively. The proposed method is then utilized for a runtime scheduler. The results show an average speedup of 1.18× and 1.1× compared with a serial and a random scheduler, respectively. © 2019 Copyright held by the owner/author(s).",Classification; concurrency; feature selection; kernel metrics; resource utilization,Classification (of information); Computer graphics; Feature extraction; Program processors; Scheduling; concurrency; Concurrent execution; Kernel classification; kernel metrics; On-line classification; Performance counters; Resource utilizations; Wrapper-based feature selection; Graphics processing unit
Efficient cache performance modeling in GPUs using reuse distance analysis,2019,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057749899&doi=10.1145%2f3291051&partnerID=40&md5=9f1f102919885b704a7ac487a140c51f,"Reuse distance analysis (RDA) is a popular method for calculating locality profiles and modeling cache performance. The present article proposes a framework to apply the RDA algorithm to obtain reuse distance profiles in graphics processing unit (GPU) kernels. To study the implications of hardware-related parameters in RDA, two RDA algorithms were employed, including a high-level cache-independent RDA algorithm, called HLRDA, and a detailed RDA algorithm, called DRDA. DRDA models the effects of reservation fails in cache blocks and miss status holding registers to provide accurate cache-related performance metrics. In this case, the reuse profiles are cache-specific. In a selection of GPU kernels, DRDA obtained the L1 miss-rate breakdowns with an average error of 3.86% and outperformed the state-of-the-art RDA in terms of accuracy. In terms of performance, DRDA is 246,000×slower than the real GPU executions and 11×faster than GPGPUSim. HLRDA ignores the cache-related parameters and its obtained reuse profiles are general, which can be used to calculate miss rates in all cache sizes. Moreover, the average error incurred by HLRDA was 16.9%. © 2018 Association for Computing Machinery.",cache memory; GPU; miss rate; reuse distance analysis,Buffer storage; Cache memory; Computer graphics; Computer graphics equipment; Program processors; Average errors; Cache blocks; Cache performance; Cache size; Miss-rate; Performance metrics; Reuse distance; State of the art; Graphics processing unit
On-GPU thread-data remapping for branch divergence reduction,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061279949&doi=10.1145%2f3242089&partnerID=40&md5=7b174ffdb77aac57963ad31b6e840954,"General Purpose GPU computing (GPGPU) plays an increasingly vital role in high performance computing and other areas like deep learning. However, arising from the SIMD execution model, the branch divergence issue lowers efciency of conditional branching on GPUs, and hinders the development of GPGPU. To achieve runtime on-the-spot branch divergence reduction, we propose the frst on-GPU thread-data remapping scheme. Before kernel launching, our solution inserts codes into GPU kernels immediately before each target branch so as to acquire actual runtime divergence information. GPU software threads can be remapped to datasets multiple times during single kernel execution. We propose two thread-data remapping algorithms that are tailored to the GPU architecture. Effective on two generations of GPUs from both NVIDIA and AMD, our solution achieves speedups up to 2.718 with third-party benchmarks. We also implement three GPGPU frontier benchmarks from areas including computer vision, algorithmic trading and data analytics. They are hindered by more complex divergence coupled with different memory access patterns, and our solution works better than the traditional thread-data remapping scheme in all cases. As a compiler-assisted runtime solution, it can better reduce divergence for divergent applications that gain little acceleration on GPUs for the time being. © 2018 Association for Computing Machinery.",Branch divergence; GPGPU; Parallel computing; SIMD,Data Analytics; Deep learning; Graphics processing unit; Memory architecture; Parallel processing systems; Algorithmic trading; Branch divergence; General-purpose gpu computing; GPGPU; High performance computing; Memory access patterns; SIMD; Thread-data remapping; Program processors
SelSMaP: A selective stride masking prefetching scheme,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055262700&doi=10.1145%2f3274650&partnerID=40&md5=dac88e70cfd7dbe71dbe83310f4037dd,"Data prefetching, which intelligently loads data closer to the processor before demands, is a popular cache performance optimization technique to address the increasing processor-memory performance gap. Although prefetching concepts have been proposed for decades, sophisticated system architecture and emerging applications introduce new challenges. Large instruction windows coupled with out-of-order execution makes the program data access sequence distorted from a cache perspective. Furthermore, big data applications stress memory subsystems heavily with their large working set sizes and complex data access patterns. To address such challenges, this work proposes a high-performance hardware prefetching scheme, SelSMaP. SelSMaP is able to detect both regular and nonuniform stride patterns by taking the minimum observed address offset (called a reference stride) as a heuristic. A stride masking is generated according to the reference stride and is to flter out history accesses whose pattern can be rephrased as uniform stride accesses. Prefetching decision and prefetch degree are determined based on the masking outcome. As SelSMaP prediction logic does not rely on the chronological order of data accesses or program counter information, it is able to unveil the effect of out-of-order execution and compiler optimization. We evaluated SelSMaP with CloudSuite workloads and SPEC CPU2006 benchmarks. SelSMaP achieves an average CloudSuite performance improvement of 30% over nonprefetching systems. With one to two orders of magnitude less storage and much less functional logic, SelSMaP outperforms the highest-performing prefetcher by 8.6% in CloudSuite workloads. © 2018 ACM.",Cache; Memory subsystem; Prefetching,Cache memory; Computer circuits; Memory architecture; Optimization; Program compilers; Cache; Compiler optimizations; High-performance hardware; Large instruction window; Memory subsystems; Performance improvements; Prefetching; Processor-memory performance gap; Big data
Exposing memory access patterns to improve instruction and memory efficiency in GPUs,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056757836&doi=10.1145%2f3280851&partnerID=40&md5=2a7cc4e828f3f1a67f1c172857e656b8,"Modern computing workloads often have high memory intensity, requiring high bandwidth access to memory. The memory request patterns of these workloads vary and include regular strided accesses and indirect (pointer-based) accesses. Such applications require a large number of address generation instructions and a high degree of memory-level parallelism. This article proposes new memory instructions that exploit strided and indirect memory request patterns and improve efficiency in GPU architectures. The new instructions reduce address calculation instructions by offloading addressing to dedicated hardware, and reduce destructive memory request interference by grouping related requests together. Our results show that we can eliminate 33% of dynamic instructions across 16 GPU benchmarks. These improvements result in an overall runtime improvement of 26%, an energy reduction of 18%, and a reduction in energy-delay product of 32%. Copyright © 2018 Association for Computing Machinery. All Rights Reserved.",GPU architecture; Vector instruction sets; Vector memory instructions,Computer hardware; Program processors; Address generation; Computing workloads; Dedicated hardware; Dynamic instructions; Energy delay product; Instruction set; Memory access patterns; Memory efficiency; Memory architecture
Performance tuning and analysis for stencil-based applications on POWER8 processor,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055248142&doi=10.1145%2f3264422&partnerID=40&md5=276131f2c11a3c39fce2228665796cd5,"This article demonstrates an approach for combining general tuning techniques with the POWER8 hardware architecture through optimizing three representative stencil benchmarks. Two typical real-world applications, with kernels similar to those of the winning programs of the Gordon Bell Prize 2016 and 2017, are employed to illustrate algorithm modifcations and a combination of hardware-oriented tuning strategies with the application algorithms. This work flls the gap between hardware capability and software performance of the POWER8 processor, and provides useful guidance for optimizing stencil-based scientifc applications on POWER systems. © 2018 ACM.",,Hardware; Hardware architecture; Performance tuning; Real-world; Software performance; Tuning strategy; Application programs
SCP: Shared cache partitioning for high-performance GEMM,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055273106&doi=10.1145%2f3274654&partnerID=40&md5=9a957d9efdc7882fa99d47639fb2fb22,"GEneral Matrix Multiply (GEMM) is the most fundamental computational kernel routine in the BLAS library. To achieve high performance, in-memory data must be prefetched into fast on-chip caches before they are used. Two techniques, software prefetching and data packing, have been used to effectively exploit the capability of on-chip least recent used (LRU) caches, which are popular in traditional high-performance processors used in high-end servers and supercomputers. However, the market has recently witnessed a new diversity in processor design, resulting in high-performance processors equipped with shared caches with non-LRU replacement policies. This poses a challenge to the development of high-performance GEMM in a multithreaded context. As several threads try to load data into a shared cache simultaneously, interthread cache conflicts will increase signifcantly. We present a Shared Cache Partitioning (SCP) method to eliminate interthread cache conflicts in the GEMM routines, by partitioning a shared cache into physically disjoint sets and assigning different sets to different threads. We have implemented SCP in the OpenBLAS library and evaluated it on Phytium 2000+, a 64-core AArch64 processor with private LRU L1 caches and shared pseudorandom L2 caches (per four-core cluster). Our evaluation shows that SCP has effectively reduced the conflict misses in both L1 and L2 caches in a highly optimized GEMM implementation, resulting in an improvement of its performance by 2.75% to 6.91%. © 2018 ACM.",BLAS; GEMM; High-performance computing; Linear algebra; Optimization,Distributed computer systems; Linear algebra; Optimization; Supercomputers; BLAS; Computational kernels; GEMM; High performance computing; High performance processors; Lru replacements; Shared cache partitioning; Software prefetching; Cache memory
Polyhedral search space exploration in the exastencils code generator,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055289395&doi=10.1145%2f3274653&partnerID=40&md5=fcd0c7c68fe1cf522ba3c6d9e0a01e85,"Performance optimization of stencil codes requires data locality improvements. The polyhedron model for loop transformation is well suited for such optimizations with established techniques, such as the PLuTo algorithm and diamond tiling. However, in the domain of our project ExaStencils, stencil codes, it fails to yield optimal results. As an alternative, we propose a new, optimized, multi-dimensional polyhedral search space exploration and demonstrate its effectiveness: we obtain better results than existing approaches in several cases. We also propose how to specialize the search for the domain of stencil codes, which dramatically reduces the exploration effort without signifcantly impairing performance. © 2018 ACM.",ExaStencils; Polyhedral search space exploration; Polyhedron model; Stencils,Geometry; Space research; ExaStencils; Exploration effort; Loop transformation; Multi dimensional; Performance optimizations; Polyhedron models; Search spaces; Stencils; Codes (symbols)
High-performance generalized tensor operations: A compiler-oriented approach,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053559484&doi=10.1145%2f3235029&partnerID=40&md5=c76093d342fbb9543b352e2fbec1998b,"The efficiency of tensor contraction is of great importance. Compilers cannot optimize it well enough to come close to the performance of expert-tuned implementations. All existing approaches that provide competitive performance require optimized external code. We introduce a compiler optimization that reaches the performance of optimized BLAS libraries without the need for an external implementation or automatic tuning. Our approach provides competitive performance across hardware architectures and can be generalized to deliver the same benefits for algebraic path problems. By making fast linear algebra kernels available to everyone, we expect productivity increases when optimized libraries are not available. © 2018 Association for Computing Machinery.",High-performance computing; Matrix-matrix multiplication; Tensor contractions,Libraries; Program compilers; Tensors; Algebraic path problem; Competitive performance; Compiler optimizations; Hardware architecture; High performance computing; Matrix matrix multiplications; Productivity increase; Tensor contraction; Matrix algebra
Layer-centric memory reuse and data migration for extreme-scale deep learning on many-core architectures,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053886599&doi=10.1145%2f3243904&partnerID=40&md5=c59375ae5fa1de09a9e851b6108abe60,"Due to the popularity of Deep Neural Network (DNN) models, we have witnessed extreme-scale DNN models with the continued increase of the scale in terms of depth and width. However, the extremely high memory requirements for them makeit difficult to run the training processes onsingle many-core architectures such as a Graphic Processing Unit (GPU), which compels researchers to use model parallelism over multiple GPUs to make it work. However, model parallelism always brings very heavy additional overhead. Therefore, running an extreme-scale model in a single GPU is urgently required. There still exist several challenges to reduce the memory footprint for extreme-scale deep learning. To address this tough problem, we first identify the memory usage characteristics for deep and wide convolutional networks, and demonstrate the opportunities for memory reuse at both the intra-layer and inter-layer levels. We then present Layrub, a runtime data placement strategy that orchestrates the execution of the training process. It achieves layer-centric reuse to reduce memory consumption for extreme-scale deep learning that could not previously be run on a single GPU. Experiments show that, compared to the original Caffe, Layrub can cut down the memory usage rate by an average of 58.2% and by up to 98.9%, at the moderate cost of 24.1% higher training execution time on average. Results also show that Layrub outperforms some popular deep learning systems such as GeePS, vDNN, MXNet, and Tensorflow. More importantly, Layrub can tackle extreme-scale deep learning tasks. For example, it makes an extra-deep ResNet with 1,517 layers that can be trained successfully in one GPU with 12GB memory, while other existing deep learning systems cannot. © 2018 Association for Computing Machinery.",Data placement; DNN; GPU; Memory efficiency,Graphics processing unit; Memory architecture; Network architecture; Program processors; Convolutional networks; Data placement; Graphic processing unit(GPU); Many-core architecture; Memory consumption; Memory efficiency; Memory footprint; Memory requirements; Deep neural networks
CODA: Enabling co-location of computation and data for multiple GPU systems,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053516108&doi=10.1145%2f3232521&partnerID=40&md5=28a64a70f21ebf6cc918116206969ac7,"To exploit parallelism and scalability of multiple GPUs in a system, it is critical to place compute and data together. However, two key techniques that have been used to hide memory latency and improve thread-level parallelism (TLP), memory interleaving, and thread block scheduling, in traditional GPU systems are at odds with efficient use of multiple GPUs. Distributing data across multiple GPUs to improve overall memory bandwidth utilization incurs high remote traffic when the data and compute are misaligned. Nondeterministic thread block scheduling to improve compute resource utilization impedes co-placement of compute and data. Our goal in this work is to enable co-placement of compute and data in the presence of fine-grained interleaved memory with a low-cost approach. To this end, we propose a mechanism that identifies exclusively accessed data and place the data along with the thread block that accesses it in the same GPU. The key ideas are (1) the amount of data exclusively used by a thread block can be estimated, and that exclusive data (of any size) can be localized to one GPU with coarse-grained interleaved pages; (2) using the affinity-based thread block scheduling policy, we can co-place compute and data together; and (3) by using dual address mode with lightweight changes to virtual to physical page mappings, we can selectively choose different interleaved memory pages for each data structure. Our evaluations across a wide range of workloads show that the proposed mechanism improves performance by 31% and reduces 38% remote traffic over a baseline system. © 2018 Association for Computing Machinery.",Compiler technique; Compute; Data localization; Hybrid data layout; Multiple GPUs; Profiling,Graphics processing unit; Scheduling; Virtual addresses; Compiler techniques; Compute; Data localization; Hybrid datum; Multiple GPUs; Profiling; Program processors
Global dead-block management for task-parallel programs,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053511642&doi=10.1145%2f3234337&partnerID=40&md5=630698807f7f8ab64739f4aee5ad7ac5,"Task-parallel programs inefficiently utilize the cache hierarchy due to the presence of dead blocks in caches. Dead blocks may occupy cache space in multiple cache levels for a long time without providing any utility until they are finally evicted. Existing dead-block prediction schemes take decisions locally for each cache level and do not efficiently manage the entire cache hierarchy. This article introduces runtime-orchestrated global dead-block management, in which static and dynamic information about tasks available to the runtime system is used to effectively detect and manage dead blocks across the cache hierarchy. In the proposed global management schemes, static information (e.g., when tasks start/finish, and what data regions tasks produce/consume) is combined with dynamic information to detect when/where blocks become dead. When memory regions are deemed dead at some cache level(s), all the associated cache blocks are evicted from the corresponding level(s). We extend the cache controllers at both private and shared cache levels to use the aforementioned information to evict dead blocks. The article does an extensive evaluation of both inclusive and non-inclusive cache hierarchies and shows that the proposed global schemes outperform existing local dead-block management schemes. © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM",Cache hierarchy; Dead blocks; Multi-core architecture; Runtime system; Task parallelism,Computer architecture; Cache hierarchies; Dead blocks; Multicore architectures; Runtime systems; Task parallelism; Information management
BestSF: A sparse meta-format for optimizing SpMV on GPU,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053543948&doi=10.1145%2f3226228&partnerID=40&md5=b0b45a0fe278d45d7fbeb2ca282bb14c,"The Sparse Matrix-Vector Multiplication (SpMV) kernel dominates the computing cost in numerous scientific applications. Many implementations based on different sparse formats were proposed to improve this kernel on the recent GPU architectures. However, it has been widely observed that there is no “best-for-all” sparse format for the SpMV kernel on GPU. Indeed, serious performance degradation of an order of magnitude can be observed without a careful selection of the sparse format to use. To address this problem, we propose in this article BestSF (Best Sparse Format), a new learning-based sparse meta-format that automatically selects the most appropriate sparse format for a given input matrix. To do so, BestSF relies on a cost-sensitive classification system trained using Weighted Support Vector Machines (WSVMs) to predict the best sparse format for each input sparse matrix. Our experimental results on two different NVIDIA GPU architectures using a large number of real-world sparse matrices show that BestSF achieved a noticeable overall performance improvement over using a single sparse format. While BestSF is trained to select the best sparse format in terms of performance (GFLOPS), our further experimental investigations revealed that using BestSF also led, in most of the test cases, to the best energy efficiency (MFLOPS/W). To prove its practical effectiveness, we also evaluate the performance and energy efficiency improvement achieved when using BestSF as a building block in a GPU-based Preconditioned Conjugate Gradient (PCG) iterative solver. © 2018 Association for Computing Machinery.",Energy efficiency; GPU computing; Iterative solvers; Performance modeling; Sparse matrix-vector multiplication (SpMV),Graphics processing unit; Green computing; Matrix algebra; Cost sensitive classifications; Energy efficiency improvements; GPU computing; Iterative solvers; Performance Model; Preconditioned conjugate gradient; Sparse matrix-vector multiplication; Weighted support vector machine; Energy efficiency
Low complexity multiply-accumulate units for convolutional neural networks with weight-sharing,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053517295&doi=10.1145%2f3233300&partnerID=40&md5=a8715688c7bf3358bf3d1560a79f2d5b,"Convolutional neural networks (CNNs) are one of the most successful machine-learning techniques for image, voice, and video processing. CNNs require large amounts of processing capacity and memory bandwidth. Hardware accelerators have been proposed for CNNs that typically contain large numbers of multiply-accumulate (MAC) units, the multipliers of which are large in integrated circuit (IC) gate count and power consumption. “Weight-sharing” accelerators have been proposed where the full range of weight values in a trained CNN are compressed and put into bins, and the bin index is used to access the weight-shared value. We reduce power and area of the CNN by implementing parallel accumulate shared MAC (PASM) in a weight-shared CNN. PASM re-architects the MAC to instead count the frequency of each weight and place it in a bin. The accumulated value is computed in a subsequent multiply phase, significantly reducing gate count and power consumption of the CNN. In this article, we implement PASM in a weight-shared CNN convolution hardware accelerator and analyze its effectiveness. Experiments show that for a clock speed 1GHz implemented on a 45nm ASIC process our approach results in fewer gates, smaller logic, and reduced power with only a slight increase in latency. We also show that the same weight-shared-with-PASM CNN accelerator can be implemented in resource-constrained FPGAs, where the FPGA has limited numbers of digital signal processor (DSP) units to accelerate the MAC operations. © 2018 Association for Computing Machinery.",Arithmetic hardware circuits; ASIC; CNN; FPGA; Multiply accumulate; Power efficiency,Application specific integrated circuits; Convolution; Electric power utilization; Field programmable gate arrays (FPGA); Hardware; Learning systems; Neural networks; Video signal processing; Convolutional neural network; Digital Signal Processor (DSP); Hardware circuits; Machine learning techniques; Multiply accumulate; Multiply-accumulate unit; Power efficiency; Processing capacities; Digital signal processors
Software-directed techniques for improved GPU register file utilization,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054224058&doi=10.1145%2f3243905&partnerID=40&md5=6a36e14b66f9dfe2aacd0b00b951d53a,"Throughput architectures such as GPUs require substantial hardware resources to hold the state of a massive number of simultaneously executing threads. While GPU register files are already enormous, reaching capacities of 256KB per streaming multiprocessor (SM), we find that nearly half of real-world applications we examined are register-bound and would benefit from a larger register file to enable more concurrent threads. This article seeks to increase the thread occupancy and improve performance of these register-bound applications by making more efficient use of the existing register file capacity. Our first technique eagerly deallocates register resources during execution. We show that releasing register resources based on value liveness as proposed in prior states of the art leads to unreliable performance and undue design complexity. To address these deficiencies, our article presents a novel compiler-driven approach that identifies and exploits last use of a register name (instead of the value contained within) to eagerly release register resources. Furthermore, while previous works have leveraged “scalar” and “narrow” operand properties of a program for various optimizations, their impact on thread occupancy has been relatively unexplored. Our article evaluates the effectiveness of these techniques in improving thread occupancy and demonstrates that while any one approach may fail to free very many registers, together they synergistically free enough registers to launch additional parallel work. An in-depth evaluation on a large suite of applications shows that just our early register technique outperforms previous work on dynamic register allocation, and together these approaches, on average, provide 12% performance speedup (23% higher thread occupancy) on register bound applications not already saturating other GPU resources. © 2018 Association for Computing Machinery.",GPU; Register file; Thread occupancy,Graphics processing unit; Program processors; Concurrent threads; Depth evaluations; Hardware resources; Improve performance; Register allocation; Register files; Streaming multiprocessors; Thread occupancy; Computer hardware
Cluster programming using the OpenMP accelerator model,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052703083&doi=10.1145%2f3226112&partnerID=40&md5=6e2b573c3ad71ba5269143391947458d,"Computation offloading is a programming model in which program fragments (e.g., hot loops) are annotated so that their execution is performed in dedicated hardware or accelerator devices. Although offloading has been extensively used to move computation to GPUS, through directive-based annotation standards like OpenMP, offloading computation to very large computer clusters can become a complex and cumbersome task. It typically requires mixing programming models (e.g., OpenMP andMPI) and languages (e.g., C/C++ and Scala), dealing with various access control mechanisms from different cloud providers (e.g., AWS and Azure), and integrating all this into a single application. This article introduces computer cluster nodes as simple OpenMP offloading devices that can be used either from a local computer or from the cluster head-node. It proposes a methodology that transforms OpenMP directives to Spark runtime calls with fully integrated communication management, in a way that a cluster appears to the programmer as yet another accelerator device. Experiments using LLVM 3.8, OpenMP 4.5 on well known cloud infrastructures (Microsoft Azure and Amazon EC2) show the viability of the proposed approach, enable a thorough analysis of its performance, and make a comparison with an MPI implementation. The results show that although data transfers can impose overheads, cloud offloading from a local machine can still achieve promising speedups for larger granularity: up to 115× in 256 cores for the 2MM benchmark using 1GB sparse matrices. In addition, the parallel implementation of a complex and relevant scientific application reveals a 80× speedup on a 320 core machine when executed directly from the headnode of the cluster. © 2018 Association for Computing Machinery.",Apache spark; LLVM; OpenMP,Access control; Application programming interfaces (API); Data transfer; Windows operating system; Access control mechanism; Communication management; Computation offloading; LLVM; Offloading computations; OpenMP; Parallel implementations; Scientific applications; C++ (programming language)
LAPPS: Locality-aware productive prefetching support for PGAS,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052718048&doi=10.1145%2f3233299&partnerID=40&md5=be1928b17b7deb3a514e0f58a2e1d6f2,"Prefetching is a well-known technique to mitigate scalability challenges in the Partitioned Global Address Space (PGAS) model. It has been studied as either an automated compiler optimization or a manual programmer optimization. Using the PGAS locality awareness, we define a hybrid tradeoff. Specifically, we introduce locality-aware productive prefetching support for PGAS. Our novel, user-driven approach strikes a balance between the ease-of-use of compiler-based automated prefetching and the high performance of the laborious manual prefetching. Our prototype implementation in Chapel shows that significant scalability and performance improvements can be achieved with minimal effort in common applications. © 2018 Association for Computing Machinery.",Chapel; PGAS; Prefetching; Runtime system,Program compilers; Scalability; Chapel; Compiler optimizations; Partitioned Global Address Space; PGAS; Prefetching; Prototype implementations; Runtime systems; Scalability and performance; Distributed computer systems
QuMan: Profile-based improvement of cluster utilization,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052712406&doi=10.1145%2f3210560&partnerID=40&md5=a0bad2d4bc46f000da6f5c090e1f5826,"Modern data centers consolidate workloads to increase server utilization and reduce total cost of ownership, and cope with scaling limitations. However, server resource sharing introduces performance interference across applications and, consequently, increases performance volatility, which negatively affects user experience. Thus, a challenging problem is to increase server utilization while maintaining application QoS. In this article, we present QuMan, a server resource manager that uses application isolation and profiling to increase server utilization while controlling degradation of application QoS. Previous solutions, either estimate interference across applications and then restrict colocation to ""compatible"" applications, or assume that application requirements are known. Instead, QuMan estimates the required resources of applications. It uses an isolation mechanism to create properly-sized resource slices for applications, and arbitrarily colocates applications. QuMan's mechanisms can be used with a variety of admission control policies, and we explore the potential of two such policies: (1) A policy that allows users to specify a minimum performance threshold and (2) an automated policy, which operates without user input and is based on a new combined QoS-utilization metric.We implement QuMan on top of Linux servers, and we evaluate its effectiveness using containers and real applications. Our single-node results show that QuMan balances highly effectively the tradeoff between server utilization and application performance, as it achieves 80% server utilization while the performance of each application does not drop below 80% the respective standalone performance. We also deploy QuMan on a cluster of 100 AWS instances that are managed by a modified version of the Sparrow scheduler [37] and, we observe a 48% increase in application performance on a highly utilized cluster, compared to the performance of the same cluster under the same load when it is managed by native Sparrow or Apache Mesos. © 2018 Association for Computing Machinery.",I/O caching; Input/output; Isolation; Resource management; Slices; Solid state disk,Hardware; Software engineering; Input/output; Isolation; Resource management; Slices; Solid state disks; Computer operating systems
Block cooperation: Advancing lifetime of resistive memories by increasing utilization of error correcting codes,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052686606&doi=10.1145%2f3243906&partnerID=40&md5=4a435822ebe9fb2efc4e2db987556860,"Block-level cooperation is an endurance management technique that operates on top of error correction mechanisms to extend memory lifetimes. Once an error recovery scheme fails to recover from faults in a data block, the entire physical page associated with that block is disabled and becomes unavailable to the physical address space. To reduce the page waste caused by early block failures, other blocks can be used to support the failed block, working cooperatively to keep it alive and extend the faulty page's lifetime. We combine the proposed technique with existing error recovery schemes, such as Error Correction Pointers (ECP) and Aegis, to increase memory lifetimes. Block cooperation is realized through metadata sharing in ECP, where one data block shares its unused metadata with another data block. When combined with Aegis, block cooperation is realized through reorganizing data layout, where blocks possessing few faults come to the aid of failed blocks, bringing them back from the dead. Our evaluation using Monte Carlo simulation shows that block cooperation at a single level (or multiple levels) on top of ECP and Aegis, boosts memory lifetimes by 28% (37%) and 8% (14%) on average, respectively. Furthermore, using trace-driven benchmark evaluation shows that lifetime boost can reach to 68% (30%) exploiting metadata sharing (or data layout reorganization). © 2018 Association for Computing Machinery.",Endurance; Error correction; Error recovery; Memory reliability; Phase change memory; Resistive memory; Workload characterization,Computer system recovery; Durability; Error correction; Intelligent systems; Metadata; Monte Carlo methods; Physical addresses; Recovery; Benchmark evaluation; Correction mechanism; Error correcting code; Error recovery schemes; Management techniques; Memory reliability; Resistive memory; Workload characterization; Phase change memory
An alternative TAGE-like conditional branch predictor,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052711055&doi=10.1145%2f3226098&partnerID=40&md5=0ffe2237f7ecc6a66cccfcd33c456325,"TAGE is one of the most accurate conditional branch predictors known today. However, TAGE does not exploit its input information perfectly, as it is possible to obtain significant prediction accuracy improvements by complementing TAGE with a statistical corrector using the same input information. This article proposes an alternative TAGE-like predictor making statistical correction practically superfluous. © 2018 Association for Computing Machinery.",Bayesian confidence estimation; Branch prediction; PPM; TAGE,Pulse position modulation; Software engineering; Branch prediction; Conditional branch; Confidence estimation; Prediction accuracy; TAGE; Hardware
DDRNoC: Dual Data-Rate Network-on-Chip,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057300888&doi=10.1145%2f3200201&partnerID=40&md5=5673433e13af1467a41f81fca09d58a3,"This article introduces DDRNoC, an on-chip interconnection network capable of routing packets at Dual Data Rate. The cycle time of current 2D-mesh Network-on-Chip routers is limited by their control as opposed to the datapath (switch and link traversal), which exhibits significant slack. DDRNoC capitalizes on this observation, allowing two flits per cycle to share the same datapath. Thereby, DDRNoC achieves higher throughput than a Single Data Rate (SDR) network. Alternatively, using lower voltage circuits, the above slack can be exploited to reduce power consumption while matching the SDR network throughput. In addition, DDRNoC exhibits reduced clock distribution power, improving energy efficiency, as it needs a slower clock than a SDR network that routes packets at the same rate. Post place and route results in 28nm technology show that, compared to an iso-voltage (1.1V) SDR network, DDRNoC improves throughput proportionally to the SDR datapath slack. Moreover, a low-voltage (0.95V) DDRNoC implementation converts that slack to power reduction offering the 1.1V SDR throughput at a substantially lower energy cost. © 2018 ACM.",On-chip-interconnect,Clocks; Energy efficiency; Network-on-chip; Routers; Servers; Clock distribution; Dual data rates; Link traversal; Network throughput; On-chip interconnection network; Place and route; Power reductions; Routing packets; Interconnection networks (circuit switching)
Predictable Thread Coarsening,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084185847&doi=10.1145%2f3194242&partnerID=40&md5=9eedcc2ed2df58a2836365107d8ba002,"Thread coarsening on GPUs combines the work of several threads into one. We show how thread coarsening can be implemented as a fully automated compile-time optimisation that estimates the optimal coarsening factor based on a low-cost, approximate static analysis of cache line re-use and an occupancy prediction model. We evaluate two coarsening strategies on three different NVidia GPU architectures. For NVidia reduction kernels we achieve a maximum speedup of 5.08x, and for the Rodinia benchmarks we achieve a mean speedup of 1.30x over 8 of 19 kernels that were determined safe to coarsen. © 2018 ACM.",compiler optimisations; GPU,Cache memory; Ostwald ripening; Program processors; Compile time; Fully automated; Low costs; Nvidia gpu; Occupancy predictions; Optimisations; Rodinia; Coarsening
Extreme-Scale High-Order WENO Simulations of 3-D Detonation Wave with 10 Million Cores,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073942249&doi=10.1145%2f3209208&partnerID=40&md5=8255afb36be9d068e1a460df8e60d4e8,"High-order stencil computations, frequently found in many applications, pose severe challenges to emerging many-core platforms due to the complexities of hardware architectures as well as the sophisticated computing and data movement patterns. In this article, we tackle the challenges of high-order WENO computations in extreme-scale simulations of 3D gaseous waves on Sunway TaihuLight. We design efficient parallelization algorithms and present effective optimization techniques to fully exploit various parallelisms with reduced memory footprints, enhanced data reuse, and balanced computation load. Test results show the optimized code can scale to 9.98 million cores, solving 12.74 trillion unknowns with 23.12 Pflops double-precision performance. © 2018 ACM.",computation reuse; Detonation waves; many-core computing; Sunway TaihuLight; WENO method,Computational efficiency; Computation loads; Data movements; Detonation waves; Double precision; Hardware architecture; Optimization techniques; Parallelization algorithms; Stencil computations; Computer architecture
NUMA-Caffe: NUMA-Aware Deep Learning Neural Networks,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066942115&doi=10.1145%2f3199605&partnerID=40&md5=0582fd29f647885f1ca8642a94b1307c,"Convolution Neural Networks (CNNs), a special subcategory of Deep Learning Neural Networks (DNNs), have become increasingly popular in industry and academia for their powerful capability in pattern classification, image processing, and speech recognition. Recently, they have been widely adopted in High Performance Computing (HPC) environments for solving complex problems related to modeling, runtime prediction, and big data analysis. Current state-of-the-art designs for DNNs on modern multi- and many-core CPU architectures, such as variants of Caffe, have reported promising performance in speedup and scalability, comparable with the GPU implementations. However, modern CPU architectures employ Non-Uniform Memory Access (NUMA) technique to integrate multiple sockets, which incurs unique challenges for designing highly efficient CNN frameworks. Without a careful design, DNN frameworks can easily suffer from long memory latency due to a large number of memory accesses to remote NUMA domains, resulting in poor scalability. To address this challenge, we propose NUMA-aware multi-solver-based CNN design, named NUMA-Caffe, for accelerating deep learning neural networks on multi- and many-core CPU architectures. NUMA-Caffe is independent of DNN topology, does not impact network convergence rates, and provides superior scalability to the existing Caffe variants. Through a thorough empirical study on four contemporary NUMA-based multi- and many-core architectures, our experimental results demonstrate that NUMA-Caffe significantly outperforms the state-of-the-art Caffe designs in terms of both throughput and scalability. © 2018 ACM.",Deep learning; neural network; NUMA; stochastic gradient descent,Deep neural networks; Image processing; Memory architecture; Network architecture; Scalability; Speech recognition; Convolution neural network; Empirical studies; GPU implementation; High performance computing (HPC); Learning neural networks; Many-core architecture; Network convergence; Non uniform memory access; Deep learning
"CACF: A Novel Circuit Architecture Co-optimization Framework for Improving Performance, Reliability and Energy of ReRAM-based Main Memory System",2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069194881&doi=10.1145%2f3195799&partnerID=40&md5=205bb663ee3be0ca1ac7abd78130d96f,"Emerging Resistive Random Access Memory (ReRAM) is a promising candidate as the replacement for DRAM due to its low standby power, high density, high scalability, and nonvolatility. By employing the unique crossbar structure, ReRAM can be constructed with extremely high density. However, the crossbar ReRAM faces some serious challenges in terms of performance, reliability, and energy consumption. First, ReRAM's crossbar structure causes an IR drop problem due to wire resistance and sneak currents, which results in nonuniform access latency in ReRAM banks and reduces its reliability. Second, without access transistors in the crossbar structure, write disturbance results in serious data reliability problem. Third, the access latency, reliability, and energy use of ReRAM arrays are significantly influenced by the data patterns involved in a write operation. To overcome the challenges of the crossbar ReRAM, we propose a novel circuit architecture co-optimization framework for improving the performance, reliability, and energy use of ReRAM-based main memory system, called CACF. The proposed CACF consists of three levels, including the circuit level, circuit architecture level, and architecture level. At the circuit level, to reduce the IR drops along bitlines, we propose a double-sided write driver design by applying write drivers along both sides of bitlines and selectively activating the write drivers. At the circuit architecture level, to address the write disturbance with low overheads, we propose a RESET disturbance detection scheme by adding disturbance reference cells and conditionally performing refresh operations. At the architecture level, a region partition with address remapping method is proposed to leverage the nonuniform access latency in ReRAM banks, and two flip schemes are proposed in different regions to optimize the data patterns involved in a write operation. The experimental results show that CACF improves system performance by 26.1%, decreases memory access latency by 22.4%, shortens running time by 20.1%, and reduces energy consumption by 21.6% on average over an aggressive baseline. Meanwhile, CACF significantly improves the reliability of ReRAM-based memory systems. © 2018 ACM.",crossbar; data patterns; IR drop; nonuniform access latency; ReRAM; write disturbance,Drops; Dynamic random access storage; Electric network analysis; Energy utilization; Reliability; RRAM; Timing circuits; Circuit architectures; Cross-bar structures; Disturbance detection; Improving performance; Low stand-by power; Memory access latency; Resistive Random Access Memory (ReRAM); Write disturbances; Memory architecture
Managing Heterogeneous Datacenters with Tokens,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084173537&doi=10.1145%2f3191821&partnerID=40&md5=670ac4920020c94c9657d32c984d191a,"Ensuring fairness in a system with scarce, preferred resources requires time sharing. We consider a heterogeneous system with a few ""big"" and many ""small"" processors. We allocate heterogeneous processors using a novel token mechanism, which frames the allocation problem as a repeated game. At each round, users request big processors and spend a token if their request is granted. We analyze the game and optimize users' strategies to produce an equilibrium. In equilibrium, allocations balance performance and fairness. Our mechanism outperforms classical, fair mechanisms by 1.7×, on average, in performance gains, and is competitive with a performance maximizing mechanism. © 2018 ACM.",allocation and scheduling; equilibrium; Modeling; optimization; power; resource management,Software engineering; Allocation problems; Balance performance; Data centers; Heterogeneous processors; Heterogeneous systems; Performance Gain; Repeated games; Time-sharing
ReveNAND: A fast-drift-aware resilient 3d NAND flash design,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047115212&doi=10.1145%2f3184744&partnerID=40&md5=5363040337118851523bd95392b897bb,The paradigm shift from planar (two dimensional (2D)) to vertical (three-dimensional (3D)) models has placed theNANDflash technology on the verge of a design evolution that can handle the demands of next-generation storage applications. However it also introduces challenges that may obstruct the realization of such 3D NAND flash. Specifically we observed that the fast threshold drift (fast-drift) in a charge-trap flash-based 3D NAND cell can make it lose a critical fraction of the stored charge relatively soon after programming and generate errors. In this work we first present an elastic read reference (VRef ) scheme (ERR) for reducing such errors in ReveNAND-our fast-drift aware 3D NAND design. To address the inherent limitation of the adaptive VRef we introduce a new intra-block page organization (hitch-hike) that can enable stronger error correction for the error-prone pages. In addition we propose a novel reinforcement-learning-based smart data refill scheme (iRefill) to counter the impact of fast-drift with minimum performance and hardware overhead. Finally we present the first analytic model to characterize fast-drift and evaluate its system-level impact. Our results show that compared to conventional 3D NAND design our ReveNAND can reduce fast-drift errors by 87% on average and can lower the ECC latency and energy overheads by 13× and 10× respectively. © 2018 ACM.,Adaptive read; Fast detrapping; Reinforcement learning; Threshold drift,Digital storage; Error correction; Memory architecture; NAND circuits; Reinforcement learning; Adaptive read; Critical fraction; De-trapping; Hardware overheads; Inherent limitations; Threedimensional (3-d); Threshold drift; Two Dimensional (2 D); Three dimensional computer graphics
Elastic places: An adaptive resource manager for scalable and portable performance,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047121504&doi=10.1145%2f3185458&partnerID=40&md5=e66680c6921b45626e69997fddfa5b06,The diversity and complexity of modern computing platforms makes the development of high-performance software challenging. Designing scalable software requires tuning for a large set of resources including cores (parallelism) memory bandwidths and various levels of private and shared caches as well as developing strategies for optimizing locality. But highly optimized implementations are often inefficient when executed on a different platform. This is the performance portability problem. One approach to scalability and portability is to tune the amount of work per task based on runtime overheads and concurrency. This results in a better balance between parallelism and scheduling overheads but it can neither tune data reuse nor avoid intertask interference. We propose a complementary approach that consists in tuning the amount of resources allocated to tasks and combine it with software-defined task topologies to provide portable locality. These ideas are combined into a low-overhead resource management scheme called Elastic Places. Elastic Places is implemented in the XiTAO software framework but the core ideas are equally applicable to other languages and runtimes. Experimental results on an AMD-based NUMA machine and an Intel Knights Landing system show that elastic places provides both high scalability and performance portability with speed-ups of up to 2.3× on both platforms compared to state-of-the-art runtimes. 2018 Copyright is held by the owner/author(s). © 2018 ACM.,Constructive sharing; Dynamic scheduling; NUMA-aware scheduling; Parallel slackness,Computer programming; Scalability; Constructive sharing; Developing strategy; Dynamic scheduling; Optimized implementation; Parallel slackness; Performance portability; Resource management schemes; Software frameworks; Scheduling
DarkCache: Energy-performance optimization of tiled multi-cores by adaptively power-gating LLC banks,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047160449&doi=10.1145%2f3186895&partnerID=40&md5=84e9550f34036e83400749557f7640f2,The Last Level Cache (LLC) is a key element to improve application performance in multi-cores. To handle the worst case themain design trend employs tiled architectureswith a large LLC organized in banks which goes underutilized in several realistic scenarios. Our proposal named DarkCache aims at properly powering off such unused banks to optimize the Energy-Delay Product (EDP) through an adaptive cache reconfiguration thus aggressively reducing the leakage energy. The implemented solution is general and it can recognize and skip the activation of the DarkCache policy for the few strong memory intensive applications that actually require the use of the entire LLC. The validation has been carried out on 16- and 64-core architectures also accounting for two state-of-the-art methodologies. Compared to the baseline solution DarkCache exhibits a performance overhead within 2% and an average EDP improvement of 32.58% and 36.41% considering 16 and 64 cores respectively. Moreover DarkCache shows an average EDP gain between 16.15% (16 cores) and 21.05% (64 cores) compared to the best state-of-the-art we evaluated and it confirms a good scalability since the gain improves with the size of the architecture. © 2018 ACM.,Energy-performance; Last level cachey; Power gating,Hardware; Software engineering; Application performance; Cache reconfigurations; Energy delay product; Energy performance; Last level cachey; Lastlevel caches (LLC); Power gatings; Realistic scenario; Energy efficiency
Cross-layer memory management to improve DRAM energy efficiency,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047135307&doi=10.1145%2f3196886&partnerID=40&md5=0af28030b6b7eb4173a198b5acb0c726,"Controlling the distribution and usage of memory power is often difficult because these effects typically depend on activity across multiple layers of the vertical execution stack. To address this challenge we construct a novel and collaborative framework that employs object placement cross-layer communication and page-level management to effectively distribute application objects in the DRAM hardware to achieve desired power/performance goals. Thiswork describes the design and implementation of our framework,which is the first to integrate automatic object profiling and analysis at the application layer with fine-grained management of memory hardware resources in the operating system. We demonstrate the utility of this framework by employing it to control memory power consumption more effectively. First we design a custom memoryintensive workload to show the potential of this approach to reduce DRAM energy consumption. Next we develop sampling and profiling-based analyses and modify the code generator in the HotSpot VM to understand object usage patterns and automatically control the placement of hot and cold objects in a partitioned VM heap. This information is communicated to the operating system which uses it to map the logical application pages to the appropriate DRAM modules according to user-defined provisioning goals. The evaluation shows that our Java VM-based framework achieves our goal of significant DRAM energy savings across a variety of workloads without any source code modifications or recompilations. © 2018 ACM.",DRAM; Energy; Memory management; Optimization; Power,Energy conservation; Energy efficiency; Energy utilization; Hardware; Integrated circuit design; Optimization; Application objects; Collaborative framework; Cross-layer communications; Design and implementations; Energy; Memory management; Power; Source code modification; Dynamic random access storage
DiagSim: Systematically diagnosing simulators for healthy simulations,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045200168&doi=10.1145%2f3177959&partnerID=40&md5=8597777a04aecfdc5b844b092ee22912,"Simulators are the most popular and useful tool to study computer architecture and examine new ideas. However, modern simulators have become prohibitively complex (e.g., 200K+ lines of code) to fully understand and utilize. Users therefore end up analyzing and modifying only the modules of interest (e.g., branch predictor, register file) when performing simulations. Unfortunately, hidden details and inter-module interactions of simulators create discrepancies between the expected and actual module behaviors. Consequently, the effect of modifying the target module May be amplified or masked and the users get inaccurate insights from expensive simulations. In this article, we propose DiagSim, an efficient and systematic method to diagnose simulators. It ensures the target modules behave as expected to perform simulation in a healthy (i.e., accurate and correct) way. DiagSim is efficient in that it quickly pinpoints the modules showing discrepancies and guides the users to inspect the behavior without investigating the whole simulator. DiagSim is systematic in that it hierarchically tests the modules to guarantee the integrity of individual diagnosis and always provide reliable results. We construct DiagSim based on generic category-based diagnosis ideas to encourage easy expansion of the diagnosis. We diagnose three popular open source simulators and discover hidden details including implicitly reserved resources, un-documented latency factors, and hard-coded module parameter values. We observe that these factors have large performance impacts (up to 156%) and illustrate that our diagnosis can correctly detect and eliminate them. © 2018 ACM.",Microbenchmarks; Simulator diagnosis; Timing simulator verification,Computer architecture; Branch predictors; Lines of code; Micro-benchmarks; Performance impact; Register files; Reliable results; Systematic method; Timing simulators; Simulators
Efficient and scalable graph parallel processing with symbolic execution,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045218258&doi=10.1145%2f3170434&partnerID=40&md5=498f97290f38aeda760fd139bd62faae,"Existing graph processing essentially relies on the underlying iterative execution with synchronous (Sync) and/or asynchronous (Async) engine. Nevertheless, they both suffer from a wide class of inherent serialization arising from data interdependencies within a graph. In this article, we present SymGraph, a judicious graph engine with symbolic iteration that enables the parallelism of dependent computation on vertices. SymGraph allows using abstract symbolic value (instead of the concrete value) for the computation if the desired data is unavailable. To maximize the potential of symbolic iteration, we propose a chain of tailored sophisticated techniques, enabling SymGraph to scale out with a new milestone of efficiency for large-scale graph processing. We evaluate SymGraph in comparison to Sync, Async, and a hybrid of Sync and Async engines. Our results on 12 nodes show that SymGraph outperforms all three graph engines by 1.93x (vs. Sync), 1.98x (vs. Async), and 1.57x (vs. Hybrid) on average. In particular, the performance for PageRank on 32 nodes can be dramatically improved by 16.5x (vs. Sync), 23.3x (vs. Async), and 12.1x (vs. Hybrid), respectively. The efficiency of SymGraph is also validated with at least one order of magnitude improvement in contrast to three specialized graph systems (Naiad, GraphX, and PGX.D). © 2018 ACM.",Efficiency; Graph processing; Scalability; Symbolic execution,Abstracting; Efficiency; Engines; Iterative methods; Model checking; Scalability; Desired datum; Graph processing; Iterative executions; PageRank; Parallel processing; Scale outs; Symbolic execution; Symbolic value; Graph theory
SynchroTrace: Synchronization-Aware architecture-Agnostic traces for lightweight multicore simulation of CMP and HPC workloads,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045203838&doi=10.1145%2f3158642&partnerID=40&md5=34bcccd54278150c6dd4188ad55f9d87,"Trace-driven simulation of chip multiprocessor (CMP) systems offers many advantages over execution-driven simulation, such as reducing simulation time and complexity, allowing portability, and scalability. However, trace-based simulation approaches have difficulty capturing and accurately replaying multithreaded traces due to the inherent nondeterminism in the execution of multithreaded programs. In this work, we present SynchroTrace, a scalable, flexible, and accurate trace-based multithreaded simulation methodology. By recording synchronization events relevant to modern threading libraries (e.g., Pthreads and OpenMP) and dependencies in the traces, independent of the host architecture, the methodology is able to accurately model the nondeterminism of multithreaded programs for different hardware platforms and threading paradigms. Through capturing high-level instruction categories, the SynchroTrace average CPI trace Replay timing model offers fast and accurate simulation of many-core in-order CMPs. We perform two case studies to validate the SynchroTrace simulation flow against the gem5 full-system simulator: (1) a constraint-based design space exploration with traditional CMP benchmarks and (2) a thread-scalability study with HPC-representative applications. The results from these case studies show that (1) our trace-based approach with trace filtering has a peak speedup of up to 18.7× over simulation in gem5 full-system with an average of 9.6× speedup, (2) SynchroTrace maintains the thread-scaling accuracy of gem5 and can efficiently scale up to 64 threads, and (3) SynchroTrace can trace in one platform and model any platform in early stages of design. © 2018 ACM.",Chip multiprocessor; Computational modeling; Design space exploration; Multi-threading; Trace-driven simulation,Application programming interfaces (API); Benchmarking; Multiprocessing systems; Multitasking; Scalability; Systems analysis; Chip Multiprocessor; Computational model; Design space exploration; Multi-threading; Trace driven simulation; Computer aided software engineering
Extending moore’s law via computationally error-Tolerant computing,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045203783&doi=10.1145%2f3177837&partnerID=40&md5=17af51afbbf5cca24b6670b1782f0647,"Dennard scaling has ended. Lowering the voltage supply (Vdd) to sub-volt levels causes intermittent losses in signal integrity, rendering further scaling (down) no longer acceptable as a means to lower the power required by a processor core. However, it is possible to correct the occasional errors caused due to lower Vddin an efficient manner and effectively lower power. By deploying the right amount and kind of redundancy, we can strike a balance between overhead incurred in achieving reliability and energy savings realized by permitting lower Vdd. One promising approach is the Redundant Residue Number System (RRNS) representation. Unlike other error correcting codes, RRNS has the important property of being closed under addition, subtraction and multiplication, thus enabling computational error correction at a fraction of an overhead compared to conventional approaches. We use the RRNS scheme to design a Computationally-Redundant, Energy-Efficient core, including the microarchitecture, Instruction Set Architecture (ISA) and RRNS centered algorithms. From the simulation results, this RRNS system can reduce the energy-delay-product by about 3× for multiplication intensive workloads and by about 2× in general, when compared to a non-error-correcting binary core. © 2018 ACM.",Error-tolerant computing; Low energy; Reliability,Energy conservation; Energy efficiency; Error correction; Numbering systems; Redundancy; Reliability; Computational error; Conventional approach; Energy delay product; Error correcting code; Error tolerant; Instruction set architecture; Low-energy; Redundant residue number systems; Computer architecture
Energy-Performance considerations for data offloading to FPGA-Based accelerators over PCIe,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045183736&doi=10.1145%2f3180263&partnerID=40&md5=411b38a729d66345879506fec7714e9f,"Modern data centers increasingly employ FPGA-based heterogeneous acceleration platforms as a result of their great potential for continued performance and energy efficiency. Today, FPGAs provide more hardware parallelism than is possible with GPUs or CPUs, whereas C-like programming environments facilitate shorter development time, even close to software cycles. In this work, we address limitations and overheads in access and transfer of data to accelerators over common CPU-accelerator interconnects such as PCIe. We present three different FPGA accelerator dispatching methods for streaming applications (e.g., multimedia, vision computing). The first uses zero-copy data transfers and on-chip scratchpad memory (SPM) for energy efficiency, and the second uses also zero-copy but shared copy engines among different accelerator instances and local external memory. The third uses the processor’s memory management unit to acquire the physical address of user pages and uses scatter-gather data transfers with SPM. Even though all techniques exhibit advantages in terms of scalability and relieve the processor from control overheads through using integrated schedulers, the first method presents the best energy-efficient acceleration in streaming applications. © 2018 ACM.",Adaptive dispatching; PCIe-based FPGA accelerators; Scatter-gather DMA; Zero-copy streaming accelerators,Acceleration; C (programming language); Data transfer; Field programmable gate arrays (FPGA); Green computing; Memory management units; Physical addresses; Program processors; Adaptive dispatching; Dispatching methods; Energy performance; Fpga accelerators; Hardware parallelisms; Programming environment; Streaming applications; Zero copy; Energy efficiency
Visual program manipulation in the polyhedral model,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045208325&doi=10.1145%2f3177961&partnerID=40&md5=23f8abf0814c219410d18aee0f1b3969,"Parallelism is one of the key performance sources in modern computer systems. When heuristics-based automatic parallelization fails to improve performance, a cumbersome and error-prone manual transformation is often required. As a solution, we propose an interactive visual approach building on the polyhedral model that visualizes exact dependencies and parallelism; decomposes and replays a complex automatically computed transformation step by step; and allows for directly manipulating the visual representation as a means of transforming the program with immediate feedback. User studies suggest that our visualization is understood by experts and nonexperts alike, and that it May favor an exploratory approach. © 2018 ACM.",Direct manipulation; Polyhedral model,Software engineering; Automatic Parallelization; Direct manipulation; Error prones; Immediate feedbacks; Improve performance; Modern computer systems; Polyhedral modeling; Visual representations; Hardware
Improving energy efficiency of coarse-Grain reconfigurable arrays through modulo schedule compression/Decompression,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045187329&doi=10.1145%2f3162018&partnerID=40&md5=92c6f05a502029a4172c2c24f45e6c98,"Modulo-scheduled course-grain reconfigurable array (CGRA) processors excel at exploiting loop-level parallelism at a high performance per watt ratio. The frequent reconfiguration of the array, however, causes between 25% and 45% of the consumed chip energy to be spent on the instruction memory and fetches therefrom. This article presents a hardware/software codesign methodology for such architectures that is able to reduce both the size required to store the modulo-scheduled loops and the energy consumed by the instruction decode logic. The hardware modifications improve the spatial organization of a CGRA’s execution plan by reorganizing the configuration memory into separate partitions based on a statistical analysis of code. A compiler technique optimizes the generated code in the temporal dimension by minimizing the number of signal changes. The optimizations achieve, on average, a reduction in code size of more than 63% and in energy consumed by the instruction decode logic by 70% for a wide variety of application domains. Decompression of the compressed loops can be performed in hardware with no additional latency, rendering the presented method ideal for low-power CGRAs running at high frequencies. The presented technique is orthogonal to dictionary-based compression schemes and can be combined to achieve a further reduction in code size. © 2018 ACM.",Coarse-grain reconfigurable array; Code compression; Energy reduction,Decoding; Energy efficiency; Hardware; Hardware-software codesign; Coarse grain reconfigurable arrays; Code compression; Dictionary-based compressions; Energy reduction; Hardware modifications; Hardware/software codesign methodologies; Loop-level parallelism; Minimizing the number of; Computer circuits
Performance optimization of the HPCG benchmark on the sunway TaihuLight supercomputer,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045202434&doi=10.1145%2f3182177&partnerID=40&md5=07afc4dd5613a3c8dc5fa5d217bbc151,"In this article, we present some key techniques for optimizing HPCG on Sunway TaihuLight and demonstrate how to achieve high performance in memory-bound applications by exploiting specific characteristics of the hardware architecture. In particular, we utilize a block multicoloring approach for parallelization and propose methods such as requirement-based data mapping and customized gather collective to enhance the effective memory bandwidth. Experiments indicate that the optimized HPCG code can sustain 77% of the theoretical memory bandwidth and scale to the full system of more than 10 million cores, with an aggregated performance of 480.8 Tflop/s and a weak scaling efficiency of 87.3%. © 2018 ACM.",Heterogeneous many-core processor; HPCG; Performance optimization; Sunway taihulight,Bandwidth; Supercomputers; Hardware architecture; Heterogeneous many cores; HPCG; Memory bandwidths; Memory-bound applications; Parallelizations; Performance optimizations; Sunway taihulight; Benchmarking
Optimizing convolutional neural networks on the sunway TaihuLight supercomputer,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045202025&doi=10.1145%2f3177885&partnerID=40&md5=54d75bf290495fe03e58b1e539c70cfb,"The Sunway TaihuLight supercomputer is powered by SW26010, a new 260-core processor designed with on-chip fusion of heterogeneous cores. In this article, we present our work on optimizing the training process of convolutional neural networks (CNNs) on the Sunway TaihuLight supercomputer. Specifically, a highly efficient library (swDNN) and a customized Caffe framework (swCaffe) are proposed. Architecture-oriented optimization methods targeting the many-core architecture of SW26010 are introduced and are able to achieve 48× speedup for the convolution routine in swDNN and 4× speedup for the complete training process of the VGG-16 network using swCaffe, compared to the unoptimized algorithm and framework. Compared to the cuDNN library and the Caffe framework based on the NVIDIA K40m GPU, the proposed swDNN library and swCaffe framework on SW26010 have nearly half the performance of K40m in single -precision and have 3.6× and 1.8× speedup over K40m in double precision, respectively. © 2018 ACM.",Convolutional neural network; Deep learning; Heterogeneous many-core architecture; Sunway taihulight supercomputer,Convolution; Deep learning; Network architecture; Neural networks; Supercomputers; Convolutional neural network; Double precision; Heterogeneous cores; Heterogeneous many-core architectures; Many-core architecture; Optimization method; Single precision; Training process; Computer architecture
SIMPO: A scalable In-Memory persistent object framework using NVRAM for reliable big data computing,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045218316&doi=10.1145%2f3167972&partnerID=40&md5=a6b557ccd699541bd301edea4c9a7e7d,"While CPU architectures are incorporating many more cores to meet ever-bigger workloads, advance in fault-tolerance support is indispensable for sustaining system performance under reliability constraints. Emerging non-volatile memory technologies are yielding fast, dense, and energy-efficient NVRAM that can dethrone SSD drives for persisting data. Research on using NVRAM to enable fast in-memory data persistence is ongoing. In this work, we design and implement a persistent object framework, dubbed scalable in-memory persistent object (SIMPO), which exploits NVRAM, alongside DRAM, to support efficient object persistence in highly threaded big data applications. Based on operation logging, we propose a new programming model that classifies functions into instant and deferrable groups. SIMPO features a streamlined execution model, which allows lazy evaluation of deferrable functions and is well suited to big data computing workloads that would see improved data locality and concurrency. Our log recording and checkpointing scheme is effectively optimized towards NVRAM, mitigating its long write latency through write-combining and consolidated flushing techniques. Efficient persistent object management with features including safe references and memory leak prevention is also implemented and tailored to NVRAM. We evaluate a wide range of SIMPO-enabled applications with machine learning, high-performance computing, and database workloads on an emulated hybrid memory architecture and a real hybrid memory machine with NVDIMM. Compared with native applications without persistence, experimental results show that SIMPO incurs less than 5% runtime overhead on both platforms and even gains up to 2.5× speedup and 84% increase in throughput in highly threaded situations on the two platforms, respectively, thanks to the streamlined execution model. © 2018 ACM.",Fault tolerance; Non-volatile memory; Persistent objects,Dynamic random access storage; Energy efficiency; Fault tolerance; Function evaluation; Learning systems; Memory architecture; Nonvolatile storage; Big data applications; Emerging Non-volatile memory technology; High performance computing; Hybrid memory architectures; Memory leak preventions; Non-volatile memory; Persistent objects; Reliability constraints; Big data
Improving parallelism in hardware transactional memory,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045200068&doi=10.1145%2f3177962&partnerID=40&md5=02fae23c3ae8f71ded3a6d65fccd3f0e,"Today’s hardware transactional memory (HTM) systems rely on existing coherence protocols, which implement a requester-wins strategy. This, in turn, leads to poor performance when transactions frequently conflict, causing them to resort to a non-speculative fallback path. Often, such a path severely limits parallelism. In this article, we propose very simple architectural changes to the existing requester-wins HTM implementations that enhance conflict resolution between hardware transactions and thus improve their parallelism. Our idea is compatible with existing HTM systems, requires no changes to target applications that employ traditional lock synchronization, and is shown to provide robust performance benefits. © 2018 ACM.",Cache coherence protocols; Hardware transactional memory; Lock elision; Requester-wins,Hardware; Locks (fasteners); Multiprocessing systems; Storage allocation (computer); Architectural changes; Cache coherence protocols; Coherence protocol; Conflict Resolution; Hardware transactional memory; Lock elisions; Requester-wins; Target application; Cache memory
Improving MLC PCM performance through relaxed write and read for intermediate resistance levels,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045182473&doi=10.1145%2f3177965&partnerID=40&md5=f0d947c5d5bfea17197bb729c2adf5a6,"Phase Change Memory (PCM) is one of the most promising candidates to be used at the main memory level of the memory hierarchy due to poor scalability, considerable leakage power, and high cost/bit of DRAM. PCM is a new resistive memory that is capable of storing data based on resistance values. The wide resistance range of PCM allows for storing multiple bits per cell (MLC) rather than a single bit per cell (SLC). Unfortunately, higher density of MLC PCM comes at the expense of longer read/write latency, higher soft error rate, higher energy consumption, and earlier wearout compared to the SLC PCM. Some studies suggest removing the most error-prone level to mitigate soft error and write latency of MLC PCM, hence introducing a less dense memory called Tri-Level memory. Another scheme, called M-Metric, proposes a new read metric to address the soft error problem in MLC PCM. In order to deal with the limited lifetime of PCM, some extra storage per memory line is required to correct permanent hard errors (stuck-at faults). Since the extra storage is used only when permanent faults occur, it has a low utilization for a long time before hard errors start to occur. In this article, we utilize the extra storage to improve the read/write latency in a 2-bit MLC PCM using a relaxation scheme for reading and writing the cells for intermediate resistance levels. More specifically, we combine the most time-consuming levels (intermediate resistance levels) to reduce the number of resistance levels (making a Tri-Level PCM) and therefore improve write latency. We then store some error correction metadata in the extra storage section to successfully retrieve the exact data values in the read operation. We also modify the Tri-Level PCM cell to increase its read latency when the M-Metric scheme is used. Evaluation results show that the proposed scheme improves read latency by 57.2%, write latency by 56.1%, and overall system performance (IPC) by 26.9% over the baseline. It is noteworthy that combining the proposed scheme and FPC compression method improves read latency by 75.2%, write latency by 67%, and overall system performance (IPC) by 37.4%. © 2018 ACM.",Energy consumption; M-metric; Read latency; Tri-level PCM; Write speed,Cells; Cytology; Energy utilization; Error correction; Phase change memory; Radiation hardening; Semiconductor junctions; Compression methods; Evaluation results; Intermediate resistance; M-metric; Phase change memory (pcm); Read latencies; Relaxation schemes; Write speed; Dynamic random access storage
Enabling SIMT execution model on homogeneous multi-Core system,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045186557&doi=10.1145%2f3177960&partnerID=40&md5=4f496e2bc9bc6791513e61df4b6251d1,"Single-instruction multiple-thread (SIMT) machine emerges as a primary computing device in high-performance computing, since the SIMT execution paradigm can exploit data-level parallelism effectively. This article explores the SIMT execution potential on homogeneous multi-core processors, which generally run in multiple-instruction multiple-data (MIMD) mode when utilizing the multi-core resources. We address three architecture issues in enabling SIMT execution model on multi-core processor, including multithreading execution model, kernel thread context placement, and thread divergence. For the SIMT execution model, we propose a fine-grained multithreading mechanism on an ARM-based multi-core system. Each of the processor cores stores the kernel thread contexts in its L1 data cache for per-cycle thread-switching requirement. For divergence-intensive kernels, an Inner Conditional Statement First (ICS-First) mechanism helps early re-convergence to occur and significantly improves the performance. The experiment results show that effectiveness in data-parallel processing reduces on average 36% dynamic instructions, and boosts the SIMT executions to achieve on average 1.52× and up to 5× speedups over the MIMD counterpart for OpenCL benchmarks for single issue in-order processor cores. By using the explicit vectorization optimization on the kernels, the SIMT model gains further benefits from the SIMD extension and achieves 1.71× speedup over the MIMD approach. The SIMT model using in-order superscalar processor cores outperforms the MIMD model that uses superscalar out-of-order processor cores by 40%. The results show that, to exploit data-level parallelism, enabling the SIMT model on homogeneous multi-core processors is important. © 2018 ACM.",Control divergence; Data-level parallelism; OpenCL; SIMD processors; Spatiotemporal SIMT,Data handling; Intelligent control; Multitasking; Parallel processing systems; Program processors; Control divergences; Data-level parallelism; OpenCL; Simd processors; Spatiotemporal SIMT; Computer architecture
GPU performance vs. Thread-Level parallelism: Scalability analysis and a novel way to improve TLP,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045222909&doi=10.1145%2f3177964&partnerID=40&md5=a4733dee02afe08de15bc0176445e019,"Graphics Processing Units (GPUs) leverage massive thread-level parallelism (TLP) to achieve high computation throughput and hide long memory latency. However, recent studies have shown that the GPU performance does not scale with the GPU occupancy or the degrees of TLP that a GPU supports, especially for memory-intensive workloads. The current understanding points to L1 D-cache contention or off-chip memory bandwidth. In this article, we perform a novel scalability analysis from the perspective of throughput utilization of various GPU components, including off-chip DRAM, multiple levels of caches, and the interconnect between L1 D-caches and L2 partitions. We show that the interconnect bandwidth is a critical bound for GPU performance scalability. For the applications that do not have saturated throughput utilization on a particular resource, their performance scales well with increased TLP. To improve TLP for such applications efficiently, we propose a fast context switching approach. When a warp/thread block (TB) is stalled by a long latency operation, the context of the warp/TB is spilled to spare on-chip resource so that a new warp/TB can be launched. The switched-out warp/TB is switched back when another warp/TB is completed or switched out. With this fine-grain fast context switching, higher TLP can be supported without increasing the sizes of critical resources like the register file. Our experiment shows that the performance can be improved by up to 47% and a geometric mean of 22% for a set of applications with unsaturated throughput utilization. Compared to the state-of-the-art TLP improvement scheme, our proposed scheme achieves 12% higher performance on average and 16% for unsaturated benchmarks. © 2018 ACM.",Context switching; GPGPU; Latency hiding; TLP,Bandwidth; Benchmarking; Cache memory; Computer graphics; Dynamic random access storage; Image coding; Integrated circuit interconnects; Program processors; Scalability; Throughput; Weaving; Context switching; GPGPU; Interconnect bandwidth; Latency hiding; Performance scalability; Saturated throughput; Scalability analysis; Thread-level parallelism; Graphics processing unit
Benzene: An energy-Efficient distributed hybrid cache architecture for manycore systems,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045200060&doi=10.1145%2f3177963&partnerID=40&md5=d50df56d02c3fb3d18348759e05a5b61,"This article proposes Benzene, an energy-efficient distributed SRAM/STT-RAM hybrid cache for manycore systems running multiple applications. It is based on the observation that a naïve application of hybrid cache techniques to distributed caches in a manycore architecture suffers from limited energy reduction due to uneven utilization of scarce SRAM. We propose two-level optimization techniques: intra-bank and interbank. Intra-bank optimization leverages highly associative cache design, achieving more uniform distribution of writes within a bank. Inter-bank optimization evenly balances the amount of write-intensive data across the banks. Our evaluation results show that Benzene significantly reduces energy consumption of distributed hybrid caches. © 2018 ACM.",Distributed; Energy-efficient; Hybrid cache; Manycore systems; STT-RAM,Benzene; Energy utilization; Distributed; Energy efficient; Hybrid caches; Manycore systems; Stt rams; Energy efficiency
"A case for a more effective, power-Efficient turbo boosting",2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045208460&doi=10.1145%2f3170433&partnerID=40&md5=9cbf14f72b2ddb1daf656552d0a7c68b,"Single-thread performance and throughput often pose different design constraints and require compromises. Mainstream CPUs today incorporate a non-trivial number of cores, even for mobile devices. For power and thermal considerations, by default, a single core does not operate at the maximum performance level. When operating conditions allow, however, commercial products often rely on turbo boosting, which temporarily increases the clock frequency to increase single-thread performance. However, increasing clock speed May result in a poor performance return for invested energy. In this article, we make a case for a more effective boosting strategy, which invests energy in activities with the best estimated return. In addition to running faster clocks, we can also use a look-ahead thread to overlap the penalties of cache misses and branch mispredicts. Overall, for similar power consumptions, the proposed adaptive turbo boosting strategy can achieve about twice the performance benefits while halving the energy overhead. © 2018 ACM.",Helper threads; Implicit parallelism; Turbo boosting,Program processors; Commercial products; Design constraints; Helper thread; Implicit parallelisms; Operating condition; Performance benefits; Single-thread performance; Turbo boosting; Clocks
Reusing the optimized code for javascript ahead-of-time compilation,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061190469&doi=10.1145%2f3291056&partnerID=40&md5=101ab7729ffba6435c59be3a71002bbb,"As web pages and web apps increasingly include heavy JavaScript code, JavaScript performance has been a critical issue. Modern JavaScript engines achieve a remarkable performance by employing tiered-execution architecture based on interpreter, baseline just-in-time compiler (JITC), and optimizing JITC. Unfortunately, they suffer from a substantial compilation overhead, which can take more than 50% of the whole running time. A simple idea to reduce the compilation overhead is ahead-of-time compilation (AOTC), which reuses the code generated in the previous run. In fact, existing studies that reuse the bytecode generated by the interpreter or the machine code generated by the baseline JITC have shown tangible performance benefits [12, 31, 41]. However, there has been no study to reuse the machine code generated by the optimizing JITC, which heavily uses profile-based optimizations, thus not easily reusable. We propose a novel AOTC that can reuse the optimized machine code for high-performance JavaScript engines. Unlike previous AOTCs, we need to resolve a few challenging issues related to reusing profile-based optimized code and relocating dynamic addresses. Our AOTC improves the performance of a commercial JavaScript engine by 6.36 times (max) and 1.99 times (average) for Octane benchmarks, by reducing the compilation overhead and by running the optimized code from the first invocation of functions. It also improves the loading time of six web apps by 1.28 times, on average. © 2018 Copyright held by the owner/author(s).",ahead-of-time compilation; JavaScript; JavaScriptCore engine; optimizing just-in-time compilation; web app,Benchmarking; Cost reduction; Engines; High level languages; Just in time production; Websites; ahead-of-time compilation; Architecture-based; Critical issues; Javascript; Just in time compilers; Just-in-time compilation; Performance benefits; Web App; Codes (symbols)
AVPP: Address-first value-next predictor with value prefetching for improving the efficiency of load value prediction,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061175817&doi=10.1145%2f3239567&partnerID=40&md5=1939ff19822b60cdb28f56b067758d75,"Value prediction improves instruction level parallelism in superscalar processors by breaking true data dependencies. Although this technique can significantly improve overall performance, most of the state-of-the-art value prediction approaches require high hardware cost, which is the main obstacle for its wide adoption in current processors. To tackle this issue, we revisit load value prediction as an efficient alternative to the classical approaches that predict all instructions. By speculating only on loads, the pressure over shared resources (e.g., the Physical Register File) and the predictor size can be substantially reduced (e.g., more than 90% reduction compared to recent works).We observe that existing value predictors cannot achieve very high performance when speculating only on load instructions. To solve this problem, we propose a new, accurate and low-cost mechanism for predicting the values of load instructions: The Address-first Value-next Predictor with Value Prefetching (AVPP). The key idea of our predictor is to predict the load address first (which, we find, is much more predictable than the value) and to use a small non-speculative Value Table (VT)-indexed by the predicted address-to predict the value next. To increase the coverage of AVPP, we aim to increase the hit rate of the VT by predicting also the load address of a future instance of the same load instruction and prefetching its value in the VT. We show that AVPP is relatively easy to implement, requiring only 2.5% of the area of a 32KB L1 data cache. We compare our mechanism with five state-of-the-art value prediction techniques, evaluated within the context of load value prediction, in a relatively narrow out-of-order processor. On average, our AVPP predictor achieves 11.2% speedup and 3.7% of energy savings over the baseline processor, outperforming all the state-of-the-art predictors in 16 of the 23 benchmarks we evaluate.We evaluate AVPP implemented together with different prefetching techniques, showing additive performance gains (20% average speedup). In addition, we propose a new taxonomy to classify different value predictor policies regarding predictor update, predictor availability, and in-flight pending updates. We evaluate these policies in detail. © 2018 Copyright is held by the owner/author(s).",address prediction; instruction level parallelism; load value prediction; prefetching; speculative execution; Value prediction,Benchmarking; Energy conservation; Parallel processing systems; Value engineering; Instruction level parallelism; Load-value prediction; Prefetching; Speculative execution; Value prediction; Forecasting
Poker: Permutation-based SIMD execution of intensive tree search by path encoding,2018,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061175177&doi=10.1145%2f3280850&partnerID=40&md5=5250a7939805693a116c4b2fc92af212,"We introduce Poker, a permutation-based approach for vectorizing multiple queries over B+-trees. Our key insight is to combine vector loads and path-encoding-based permutations to alleviate memory latency while keeping the number of key comparisons needed for a query to a minimum. Implemented as a C++ template library, Poker represents a general-purpose solution for vectorizing the queries over indexing trees on multicore processors equipped with SIMD units. For a set of five representative benchmarks evaluated with 24 configurations each, Poker outperforms the state of the art by 2.11x with one single thread and 2.28x with eight threads on an Intel Broadwell processor that supports 256-bit AVX2, on average. In addition, stripmining queries will further improve Poker's performance by 1.21x (with one single thread) and 1.31x (with eight threads), on average. © 2018 Copyright is held by the owner/author(s).",intensive tree search; permute; SIMD,Encoding (symbols); C++ template library; Memory latencies; Multi-core processor; Multiple queries; permute; SIMD; State of the art; Tree search; Signal encoding
SCALO: Scalability-aware parallelism orchestration for multi-threaded workloads,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041451680&doi=10.1145%2f3158643&partnerID=40&md5=c241a3d64cc0dab7f2fb85ba8dd81d7a,"Shared memory machines continue to increase in scale by adding more parallelism through additional cores and complex memory hierarchies. Often, executing multiple applications concurrently, dividing among them hardware threads, provides greater efficiency rather than executing a single application with large thread counts. However, contention for shared resources can limit the improvement of concurrent application execution: orchestrating the number of threads used by each application and is essential. In this article, we contribute SCALO, a solution to orchestrate concurrent application execution to increase throughput. SCALO monitors co-executing applications at runtime to evaluate their scalability. Its optimizing thread allocator analyzes these scalability estimates to adapt the parallelism of each program. Unlike previous approaches, SCALO differs by including dynamic contention effects on scalability and by controlling the parallelism during the execution of parallel regions. Thus, it improves throughput when other state-of-theart approaches fail and outperforms them by up to 40% when they succeed. © 2017 ACM.",Dynamic concurrency throttling; Multi-program co-scheduling; Speedup modeling,Multiprocessing systems; Application execution; Co-scheduling; Hardware threads; Memory hierarchy; Multiple applications; Number of threads; Shared memory machines; Shared resources; Scalability
Cairo: A compiler-assisted technique for enabling instruction-level offloading of processing-in-memory,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041449001&doi=10.1145%2f3155287&partnerID=40&md5=6a37da7b54903f2027c3bd0a0bcc199e,"Three-dimensional (3D)-stacking technology and the memory-wall problem have popularized processingin- memory (PIM) concepts again, which offers the benefits of bandwidth and energy savings by offloading computations to functional units inside the memory. Several memory vendors have also started to integrate computation logics into the memory, such as Hybrid Memory Cube (HMC), the latest version of which supports up to 18 in-memory atomic instructions. Although industry prototypes have motivated studies for investigating efficient methods and architectures for PIM, researchers have not proposed a systematic way for identifying the benefits of instruction-level PIM offloading. As a result, compiler support for recognizing offloading candidates and utilizing instruction-level PIM offloading is unavailable. In this article, we analyze the advantages of instruction-level PIM offloading in the context of HMC-atomic instructions for graphcomputing applications and propose CAIRO, a compiler-assisted technique and decision model for enabling instruction-level offloading of PIM without any burden on programmers. To develop CAIRO, we analyzed how instruction offloading enables performance gain in both CPU and GPU workloads. Our studies show that performance gain from bandwidth savings, the ratio of number of cache misses to total cache accesses, and the overhead of host atomic instructions are the key factors in selecting an offloading candidate. Based on our analytical models, we characterize the properties of beneficial and nonbeneficial candidates for offloading. We evaluate CAIRO with 27 multithreaded CPU and 36 GPU benchmarks. In our evaluation, CAIRO not only doubles the speedup for a set of PIM-beneficial workloads by exploiting HMC-atomic instructions but also prevents slowdown caused by incorrect offloading decisions for other workloads. © 2017 ACM.",3D stacking; Compiler technique; Emerging technologies; Hybrid Memory Cube (HMC); Instruction-level offloading; Processing-in-memory; Profiling,Atoms; Bandwidth; Computation theory; Embedded systems; Energy conservation; Graphics processing unit; Program compilers; 3D stacking; Compiler techniques; Emerging technologies; Hybrid memory; Instruction-level; Processing in memory; Profiling; Three dimensional integrated circuits
Optimization of triangular and banded matrix operations using 2d-packed layouts,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041449489&doi=10.1145%2f3162016&partnerID=40&md5=85652f1d7bfaef042ea966512055a898,"Over the past few years, multicore systems have become increasingly powerful and thereby very useful in high-performance computing. However, many applications, such as some linear algebra algorithms, still cannot take full advantage of these systems. This is mainly due to the shortage of optimization techniques dealing with irregular control structures. In particular, the well-known polyhedral model fails to optimize loop nests whose bounds and/or array references are not affine functions. This is more likely to occur when handling sparse matrices in their packed formats. In this article, we propose using 2d-packed layouts and simple affine transformations to enable optimization of triangular and banded matrix operations. The benefit of our proposal is shown through an experimental study over a set of linear algebra benchmarks. © 2017 ACM.",2d-packed layouts; Code optimization and parallelization; Polyhedral model; Sparse matrices,Algebra; Linear algebra; Linear transformations; 2d-packed layouts; Affine transformations; High performance computing; Linear algebra algorithms; Optimization techniques; Parallelizations; Polyhedral modeling; Sparse matrices; Matrix algebra
Data-driven concurrency for high performance computing,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041436453&doi=10.1145%2f3162014&partnerID=40&md5=e380bffcff1e74d87f6b967816544f9a,"In this work, we utilize dynamic dataflow/data-driven techniques to improve the performance of high performance computing (HPC) systems. The proposed techniques are implemented and evaluated through an efficient, portable, and robust programming framework that enables data-driven concurrency on HPC systems. The proposed framework is based on data-driven multithreading (DDM), a hybrid control-flow/dataflow model that schedules threads based on data availability on sequential processors. The proposed framework was evaluated using several benchmarks, with different characteristics, on two different systems: a 4-node AMD system with a total of 128 cores and a 64-node Intel HPC system with a total of 768 cores. The performance evaluation shows that the proposed framework scales well and tolerates scheduling overheads and memory latencies effectively. We also compare our framework to MPI, DDM-VM, and OmpSs@Cluster. The comparison results show that the proposed framework obtains comparable or better performance. © 2017 ACM.",Data-driven multithreading; Distributed execution; High performance computing; Runtime system,Multitasking; Comparison result; Data-driven multithreading; Distributed execution; High performance computing; High performance computing systems; Robust programming; Runtime systems; Sequential processors; Distributed computer systems
CG-OoO: Energy-efficient coarse-grain out-of-order execution near in-order energy with near out-of-order performance,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041442684&doi=10.1145%2f3151034&partnerID=40&md5=7afbbd2ff6b65b8a16e43c537f804929,"We introduce the Coarse-Grain Out-of-Order (CG-OoO) general-purpose processor designed to achieve close to In-Order (InO) processor energy while maintaining Out-of-Order (OoO) performance. CG-OoO is an energy-performance-proportional architecture. Block-level code processing is at the heart of this architecture; CG-OoO speculates, fetches, schedules, and commits code at block-level granularity. It eliminates unnecessary accesses to energy-consuming tables and turns large tables into smaller, distributed tables that are cheaper to access. CG-OoO leverages compiler-level code optimizations to deliver efficient static code and exploits dynamic block-level and instruction-level parallelism. CG-OoO introduces Skipahead, a complexity effective, limited out-of-order instruction scheduling model. Through the energy efficiency techniques applied to the compiler and processor pipeline stages, CG-OoO closes 62% of the average energy gap between the InO and OoO baseline processors at the same area and nearly the same performance as the OoO. This makes CG-OoO 1.8? more efficient than the OoO on the energy-delay product inverse metric. CG-OoO meets the OoO nominal performance while trading off the peak scheduling performance for superior energy efficiency. © 2017 ACM.",Block-level execution; CPU architecture; Energy efficiency,Architecture; Codes (symbols); Embedded systems; General purpose computers; Indium compounds; Pipeline processing systems; Program compilers; Scheduling; Block-level execution; CPU architecture; Energy delay product; General purpose processors; Instruction level parallelism; Instruction scheduling; Out-of-order execution; Scheduling performance; Energy efficiency
A framework for automated and controlled floating-point accuracy reduction in graphics applications on GPUs,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041436601&doi=10.1145%2f3151032&partnerID=40&md5=9e0cfdc81a4d57bd28a88feeca0efd08,"Reducing the precision of floating-point values can improve performance and/or reduce energy expenditure in computer graphics, among other, applications. However, reducing the precision level of floating-point values in a controlled fashion needs support both at the compiler and at the microarchitecture level. At the compiler level, a method is needed to automate the reduction of precision of each floating-point value. At the microarchitecture level, a lower precision of each floating-point register can allow more floating-point values to be packed into a register file. This, however, calls for new register file organizations. This article proposes an automated precision-selection method and a novel GPU register file organization that can store floating-point register values at arbitrary precisions densely. The automated precision-selection method uses a data-driven approach for setting the precision level of floating-point values, given a quality threshold and a representative set of input data. By allowing a small, but acceptable, degradation in output quality, our method can remove a significant amount of the bits needed to represent floating-point values in the investigated kernels (between 28% and 60%). Our proposed register file organization exploits these lowerprecision floating-point values by packing several of them into the same physical register. This reduces the register pressure per thread by up to 48%, and by 27% on average, for a negligible output-quality degradation. This can enable GPUs to keep up to twice as many threads in flight simultaneously.",Approximate computing; Floating-point precision; GPU; LLVM; Register file,Automation; Computer architecture; Computer graphics; Computer graphics equipment; File organization; Graphics processing unit; Program compilers; Program processors; Approximate computing; Data-driven approach; Floating points; Floating-point accuracies; Graphics applications; LLVM; Register file organizations; Register files; Digital arithmetic
HAShCache: Heterogeneity-aware shared dramcache for integrated heterogeneous systems,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041437367&doi=10.1145%2f3158641&partnerID=40&md5=6f66353080e02164657261007a8de511,"Integrated Heterogeneous System (IHS) processors pack throughput-oriented General-Purpose Graphics Pprocessing Units (GPGPUs) alongside latency-oriented Central Processing Units (CPUs) on the same die sharing certain resources, e.g., shared last-level cache, Network-on-Chip (NoC), and the main memory. The demands for memory accesses and other shared resources from GPU cores can exceed that of CPU cores by two to three orders of magnitude. This disparity poses significant problems in exploiting the full potential of these architectures. In this article, we propose adding a large-capacity stacked DRAM, used as a shared last-level cache, for the IHS processors. However, adding the DRAMCache naively, leaves significant performance on the table due to the disparate demands from CPU and GPU cores for DRAMCache and memory accesses. In particular, the imbalance can significantly reduce the performance benefits that the CPU cores would have otherwise enjoyed with the introduction of the DRAMCache, necessitating a heterogeneity-aware management of this shared resource for improved performance. In this article, we propose three simple techniques to enhance the performance of CPU application while ensuring very little to no performance impact to the GPU. Specifically, we propose (i) PrIS, a prioritization scheme for scheduling CPU requests at the DRAMCache controller; (ii) ByE, a selective and temporal bypassing scheme for CPU requests at the DRAMCache; and (iii) Chaining, an occupancy controlling mechanism for GPU lines in the DRAMCache through pseudo-associativity. The resulting cache, Heterogeneity-Aware Shared DRAMCache (HAShCache), is heterogeneity-aware and can adapt dynamically to address the inherent disparity of demands in an IHS architecture. Experimental evaluation of the proposed HAShCache results in an average system performance improvement of 41% over a naive DRAMCache and over 200% improvement over a baseline system with no stacked DRAMCache. © 2017 ACM.",3D-stacked memory; Cache sharing; DRAM cache; Integrated CPU-GPU processors,Cache memory; Dynamic random access storage; Memory architecture; Network architecture; Network-on-chip; Praseodymium compounds; Program processors; Three dimensional integrated circuits; 3D-stacked memory; Cache sharing; Controlling mechanism; Experimental evaluation; Heterogeneous systems; Performance benefits; Prioritization schemes; Three orders of magnitude; Graphics processing unit
Optimizing affine controlwith semantic factorizations,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041432974&doi=10.1145%2f3162017&partnerID=40&md5=cf8b7d6d46befd312ab86c78e53bdeb1,"Hardware accelerators generated by polyhedral synthesis techniques make extensive use of affine expressions (affine functions and convex polyhedra) in control and steering logic. Since the control is pipelined, these affine objects must be evaluated at the same time for different values, which forbids aggressive reuse of operators. In this article, we propose a method to factorize a collection of affine expressions without preventing pipelining. Our key contributions are (i) to use semantic factorizations exploiting arithmetic properties of addition and multiplication and (ii) to rely on a cost function whose minimization ensures correct usage of FPGA resources. Our algorithm is totally parameterized by the cost function, which can be customized to fit a target FPGA. Experimental results on a large pool of linear algebra kernels show a significant improvement compared to traditional low-level RTL optimizations. In particular, we show how our method reduces resource consumption by revealing hidden strength reductions. © 2017 ACM.",FPGA; High-level synthesis; Polyhedral synthesis,Computational complexity; Factorization; Field programmable gate arrays (FPGA); High level synthesis; Linear algebra; Semantics; Affine function; Arithmetic property; Convex polyhedrons; Hardware accelerators; Resource consumption; Rtl optimizations; Strength reduction; Synthesis techniques; Cost functions
ReDirect: Reconfigurable directories for multicore architectures,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041440088&doi=10.1145%2f3162015&partnerID=40&md5=3b157a8965452515dad683ebd083c0b5,"As we enter the dark silicon era, architects should not envision designs in which every transistor remains turned on permanently but rather ones in which portions of the chip are judiciously turned on/off depending on the characteristics of a workload. At the same time, due to the increasing cost per transistor, architects should also consider new ways to re-purpose transistors to increase their architectural value. In this work, we consider the design of directory-based cache coherence in light of the dark silicon era and the need to re-purpose transistors. We point out that directories are not needed all of the time, and we argue that directories (and coherence) should be off unless it is actually needed for correctness. In our design, directories will be disabled and powered off for workloads with no sharing. Then only when parallel workloads need cache coherence will directories be enabled in proportion to the sharing that is present. At the same time, we exploit the structural similarities of directories and cache. If a directory is idle, then we reconfigure it to be used as extra capacity in the last-level cache. Since our novel approach can keep most directories off, we are free to select sparse overprovisioned directory designs that are reconfigurable to large amounts of cache that can significantly boost performance when the directory is idle. We call these combined features Reconfigured Dark Directories, since directories are usually dark (off) and can be reconfigured. Our results for Reconfigurable Dark Directories running SPEC 2006 applications show a performance benefit, on average, of 17% for an 8 × overprovisioned fully mapped directory on a 64-tile system under low system concurrency (10% under heavy concurrency), or a 29% average speedup for a 2 × overprovisioned directory on 256-tile system (10% under heavy concurrency) to systems with a conventional sparse directory design using the same overprovisioning factor. © 2017 ACM.",Multicore; Reconfigurable,Cache memory; Software architecture; Transistors; Combined features; Last-level caches; Multi core; Multicore architectures; Parallel workloads; Performance benefits; Reconfigurable; Structural similarity; Reconfigurable architectures
Compiler-assisted loop hardening against fault attacks,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041436009&doi=10.1145%2f3141234&partnerID=40&md5=e634cf4703ca0368ad23c66b9f477b69,"Secure elements widely used in smartphones, digital consumer electronics, and payment systems are subject to fault attacks. To thwart such attacks, software protections are manually inserted requiring experts and time. The explosion of the Internet of Things (IoT) in home, business, and public spaces motivates the hardening of a wider class of applications and the need to offer security solutions to non-experts. This article addresses the automated protection of loops at compilation time, covering the widest range of control- and data-flow patterns, in both shape and complexity. The security property we consider is that a sensitive loop must always perform the expected number of iterations; otherwise, an attack must be reported. We propose a generic compile-time loop hardening scheme based on the duplication of termination conditions and of the computations involved in the evaluation of such conditions. We also investigate how to preserve the security property along the compilation flow while enabling aggressive optimizations. We implemented this algorithm in LLVM 4.0 at the Intermediate Representation (IR) level in the backend. On average, the compiler automatically hardens 95% of the sensitive loops of typical security benchmarks, and 98% of these loops are shown to be robust to simulated faults. Performance and code size overhead remain quite affordable, at 12.5% and 14%, respectively. © 2017 ACM.",Compiler; Physical attacks; Software protection,Data flow analysis; Hardening; Internet of things; Compiler; Digital consumer electronics; Intermediate representations; Internet of thing (IOT); Number of iterations; Physical attacks; Software protection; Termination condition; Program compilers
MBZip: Multiblock data compression,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041442887&doi=10.1145%2f3151033&partnerID=40&md5=a6b8eedc7347cb295affc779e4c7b134,"Compression techniques at the last-level cache and the DRAM play an important role in improving system performance by increasing their effective capacities. A compressed block in DRAM also reduces the transfer time over the memory bus to the caches, reducing the latency of a LLC cache miss. Usually, compression is achieved by exploiting data patterns present within a block. But applications can exhibit data locality that spread across multiple consecutive data blocks.We observe that there is significant opportunity available for compressing multiple consecutive data blocks into one single block, both at the LLC and DRAM. Our studies using 21 SPEC CPU applications show that, at the LLC, around 25% (on average) of the cache blocks can be compressed into one single cache block when grouped together in groups of 2 to 8 blocks. In DRAM, more than 30% of the columns residing in a single DRAM page can be compressed into one DRAM column, when grouped together in groups of 2 to 6. Motivated by these observations, we propose a mechanism, namely, MBZip, that compresses multiple data blocks into one single block (called a zipped block), both at the LLC and DRAM. At the cache, MBZip includes a simple tag structure to index into these zipped cache blocks and the indexing does not incur any redirectional delay. At the DRAM, MBZip does not need any changes to the address computation logic and works seamlessly with the conventional/existing logic. MBZip is a synergistic mechanism that coordinates these zipped blocks at the LLC and DRAM. Further, we also explore silent writes at the DRAM and show that certain writes need not access the memory when blocks are zipped. MBZip improves the system performance by 21.9%, with a maximum of 90.3% on a 4-core system. © 2017 ACM.",CPU cache; Memory; Performance,Cache memory; Computation theory; Data storage equipment; Dynamic random access storage; Address computation; Compression techniques; Data locality; Effective capacity; Improving systems; Last-level caches; Performance; Synergistic mechanism; Data compression
Could compression be of general use? Evaluating memory compression across domains,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041433421&doi=10.1145%2f3138805&partnerID=40&md5=f0f627132625a238cfd00211d1c7b0d3,"Recent proposals present compression as a cost-effective technique to increase cache and memory capacity and bandwidth. While these proposals show potentials of compression, there are several open questions to adopt these proposals in real systems including the following: (1) Do these techniques work for real-world workloads running for long time? (2) Which application domains would potentially benefit the most from compression? (3) At which level of memory hierarchy should we apply compression: caches, main memory, or both? In this article, our goal is to shed light on some main questions on applicability of compression.We evaluate compression in the memory hierarchy for selected examples from different application classes. We analyze real applications with real data and complete runs of several benchmarks. While simulators provide a pretty accurate framework to study potential performance/energy impacts of ideas, they mostly limit us to a small range of workloads with short runtimes. To enable studying real workloads, we introduce a fast and simple methodology to get samples of memory and cache contents of a real machine (a desktop or a server). Compared to a cycle-accurate simulator, our methodology allows us to study realworkloads aswell as benchmarks. Our toolset is not a replacement for simulators but mostly complements them. While we can use a simulator to measure performance/energy impact of a particular compression proposal, here with our methodology we can study the potentials with long running workloads in early stages of the design. Using our toolset, we evaluate a collection of workloads from different domains, such as a web server of CS department of UW - Madison for 24h, Google Chrome (watching a 1h-long movie on YouTube), and Linux games (playing for about an hour).We also use several benchmarks from different domains, including SPEC, mobile, and big data. We run these benchmarks to completion. Using these workloads and our toolset, we analyze different compression properties for both real applications and benchmarks. We focus on eight main hypotheses on compression, derived from previous work on compression. These properties (Table 2) act as foundation of several proposals on compression, so performance of those proposals depends very much on these basic properties. Overall, our results suggest that compression could be of general use both in main memory and caches. On average, the compression ratio is ≥2 for 64% and 54% of workloads, respectively, for memory and cache data. Our evaluation indicates significant potential for both cache and memory compression, with higher compressibility in memory due to abundance of zero blocks. Among application domains we studied, servers show on average the highest compressibility, while our mobile benchmarks show the lowest compressibility. For comparing benchmarks with real workloads, we show that (1) it is critical to run benchmarks to completion or considerably long runtimes to avoid biased conclusions, and (2) SPEC benchmarks are good representative of real Desktop applications in terms of compressibility of their datasets. However, this does not hold for all compression properties. For example, SPEC benchmarks have much better compression locality (i.e., neighboring blocks have similar compressibility) than real workloads. Thus, it is critical for designers to consider wider range of workloads, including real applications, to evaluate their compression techniques. © 2017 ACM.",Cache and memory design; Compression; Energy efficiency; Multi-core systems; Performance,Bandwidth compression; Benchmarking; Big data; Compaction; Compressibility; Computer architecture; Computer operating systems; Cost effectiveness; Data compression ratio; Energy efficiency; Memory architecture; Real time systems; Simulators; Cache and memory compression; Compression properties; Compression techniques; Cycle-accurate simulators; Desktop applications; Memory design; Multi-core systems; Performance; Cache memory
Fuse: Accurate multiplexing of hardware performance counters across executions,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041443850&doi=10.1145%2f3148054&partnerID=40&md5=8b0d8b5379526e2571e8f239a831c57b,"Collecting hardware event counts is essential to understanding program execution behavior. Contemporary systems offer few Performance Monitoring Counters (PMCs), thus only a small fraction of hardware events can be monitored simultaneously. We present new techniques to acquire counts for all available hardware events with high accuracy by multiplexing PMCs across multiple executions of the same program, then carefully reconciling and merging the multiple profiles into a single, coherent profile. We present a new metric for assessing the similarity of statistical distributions of event counts and show that our execution profiling approach performs significantly better than Hardware Event Multiplexing.",Hardware event monitoring; Hardware event multiplexing; Performance monitoring counters; Task-parallel performance analysis,Software engineering; Event monitoring; Hardware performance counters; High-accuracy; Performance monitoring; Program execution; Statistical distribution; Task parallel; Hardware
ECS: Error-correcting strings for lifetime improvements in nonvolatile memories,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041442877&doi=10.1145%2f3151083&partnerID=40&md5=99b848a1c9cbb72e8d97364d91c70cdc,"Emerging nonvolatile memories (NVMs) suffer from low write endurance, resulting in early cell failures (hard errors), which reduce memory lifetime. It was recognized early on that conventional error-correcting codes (ECCs), which are designed for soft errors, are a poor choice for addressing hard errors in NVMs. This led to the evolution of hard error correction schemes like dynamically replicated memory (DRM), errorcorrecting pointers (ECPs), SAFER, FREE-p, PAYG, and Zombie memory to improve NVM lifetime. Whereas these approaches made significant inroads in addressing hard errors and low memory lifetime in NVMs, overcoming the challenges of underutilization of error-correcting resources and/or implementation overhead (e.g., codec latency, hardware support) remain areas of active research and development. This article proposes error-correcting strings (ECSs) as a high-utilization, low-latency solution for hard error correction in single-/multi-/triple-level cell (SLC/MLC/TLC) NVMs. At its core, ECS adopts a baseoffset approach to store pointers to the failed memory cells; in this work, base is the address of the first failed cell in a memory block and offsets are the distances between successive failed cells in that memory block. Unlike ECP, which uses fixed-length pointers, ECS uses variable-length offsets to point to the failed cells, thereby realizing more pointers to tolerate more hard errors per memory block. Further, this article proposes eXtended-ECS (XECS), a page-level error correction architecture, which employs dynamic on-demand ECS allocation and opportunistic pattern-based data compression to improve NVM lifetime by 2x over ECP-6 for comparable overhead and negligible impact to system performance. Finally, this article demonstrates that ECS is a drop-in replacement for ECP to extend the lifetime of state-of-the-art ECP-based techniques like PAYG and Zombie memory; ECS is also compatible with MLC/TLC NVMs, where it complements drift-induced soft error reduction techniques like ECC and incomplete data mapping to simultaneously extend NVM lifetime. © 2017 ACM.",Main memory; Nonvolatile memories; Reliability,Cells; Cytology; Error correction; Radiation hardening; Reliability; Emerging non-volatile memory; Error correcting code; Hardware supports; High utilizations; Lifetime improvement; Main memory; Non-volatile memory; Research and development; Errors
Improving the efficiency of GPGPU work-queue through data awareness,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041447001&doi=10.1145%2f3151035&partnerID=40&md5=53e1d666caabeeb8115c9b6fd2453c04,"The architecture and programming model of current GPGPUs are best suited for applications that are dominated by structured control and data flows across large regular datasets. Parallel workloads with irregular control and data structures cannot easily harness the processing power of the GPGPU. One approach for mapping these irregular-parallel workloads to GPGPUs is using work-queues. The work-queue approach improves the utilization of SIMD units by only processing useful works that are dynamically generated during execution.As current GPGPUs lack necessary supports forwork-queues, a software-basedwork-queue implementation often suffers from memory contention and load balancing issues. In this article, we present a novel hardware work-queue design named DaQueue, which incorporates three data-aware features to improve the efficiency of work-queues on GPGPUs. We evaluate our proposal on the irregular-parallel workloads and carry out a case study on a path tracing pipeline with a cycle-level simulator. Experimental results show that for the tested workloads, DaQueue improves performance by 1.53× on average and up to 1.91×. Compared to a hardware worklist approach that is the state-of-the-art prior work, DaQueue can achieve an average of 33.92% extra speedup with less hardware area cost. © 2017 ACM.",Data awareness; GPGPU; Work-queue,Data flow analysis; Efficiency; Hardware; Program processors; Data awareness; GPGPU; Memory contentions; Parallel workloads; Processing power; Programming models; State of the art; Work-queue; Queueing theory
SLOOP: QoS-Supervised loop execution to reduce energy on heterogeneous architectures,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041446952&doi=10.1145%2f3148053&partnerID=40&md5=b7af926590c166a1c1955496cce405f0,"Most systems allocate computational resources to each executing task without any actual knowledge of the application's Quality-of-Service (QoS) requirements. Such best-effort policies lead to overprovisioning of the resources and increase energy loss. This work assumes applications with soft QoS requirements and exploits the inherent timing slack to minimize the allocated computational resources to reduce energy consumption. We propose a lightweight progress-tracking methodology based on the outer loops of application kernels. It builds on online history and uses it to estimate the total execution time. The prediction of the execution time and the QoS requirements are then used to schedule the application on a heterogeneous architecture with big out-of-order cores and small (LITTLE) in-order cores and select the minimum operating frequency, using DVFS, that meets the deadline. Our scheme is effective in exploiting the timing slack of each application. We show that it can reduce the energy consumption by more than 20% without missing any computational deadlines.",Big-LITTLE; Energy; Heterogeneous architecture; Quality of service; Soft real-time; Streaming applications,Energy dissipation; Energy utilization; Ships; Big-LITTLE; Energy; Heterogeneous architectures; Soft real time; Streaming applications; Quality of service
Generating fine-grain multithreaded applications using a multigrain approach,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040367479&doi=10.1145%2f3155288&partnerID=40&md5=af3e8216a0bd72edb45b3f3c4fd3f1a3,"The recent evolution in hardware landscape, aimed at producing high-performance computing systems capable of reaching extreme-scale performance, has reignited the interest in fine-grain multithreading, particularly at the intranode level. Indeed, popular parallel programming environments, such as OpenMP, which features a simple interface for the parallelization of programs, are now incorporating fine-grain constructs. However, since coarse-grain directives are still heavily used, the OpenMP runtime is forced to support both coarse- and fine-grain models of execution, potentially reducing the advantages obtained when executing an application in a fully fine-grain environment. To evaluate the type of applications that benefit from executing in a unified fine-grain program execution model, this article presents a multigrain parallel programming environment for the generation of fine-grain multithreaded applications from programs featuring OpenMP's API, allowing OpenMP programs to be run on top of a fine-grain event-driven program execution model. Experimental results with five scientific benchmarks show that fine-grain applications, generated by and run on our environment with two runtimes implementing a fine-grain event-driven program execution model, are competitive and can outperform their OpenMP counterparts, especially for data-intensive workloads with irregular and dynamic parallelism, reaching speedups as high as 2.6× for Graph500 and 51× for NAS Data Cube. © 2017 ACM.",Codelets; DARTS; Extreme-scale computing; Multigrain parallel compiler; Runtime; SWARM,Application programming interfaces (API); Benchmarking; Computer architecture; Multitasking; Parallel programming; Codelets; DARTS; Extreme scale; Parallel compilers; Runtimes; SWARM; Application programs
Triple engine processor (TEP): A heterogeneous near-memory processor for diverse kernel operations,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040310808&doi=10.1145%2f3155920&partnerID=40&md5=bd688dfc97510b9139d541f7ed5ffa2b,"The advent of 3D memory stacking technology, which integrates a logic layer and stacked memories, is expected to be one of the most promising memory technologies to mitigate the memory wall problem by leveraging the concept of near-memory processing (NMP). With the ability to process data locally within the logic layer of stackedmemory, a variety of emerging big data applications can achieve significant performance and energy-efficiency benefits. Various approaches to the NMP logic layer architecture have been studied to utilize the advantage of stacked memory. While significant acceleration of specific kernel operations has been derived from previous NMP studies, an NMP-based system using an NMP logic architecture capable of handling some specific kernel operations can suffer from performance and energy efficiency degradation caused by a significant communication overhead between the host processor and NMP stack. In this article, we first analyze the kernel operations that can greatly improve the performance of NMPbased systems in diverse emerging applications, and thenwe analyze the architecture to efficiently process the extracted kernel operations. This analysis confirms that three categories of processing engines for NMP logic are required for efficient processing of a variety of emerging applications, and thuswe propose a Triple Engine Processor (TEP), a heterogeneous near-memory processor with three types of computing engines. These three types of engines are an in-order core, a coerce-grain reconfigurable processor (CGRA), and dedicated hardware. The proposed TEP provides about 3.4 times higher performance and 33% greater energy savings than the baseline 3D memory system. © 2017 ACM.",3D stacked memory; Big data application; Heterogeneous architecture; Near-memory processing (NMP),Big data; Computation theory; Computer circuits; Energy conservation; Energy efficiency; Engines; Memory architecture; Three dimensional integrated circuits; 3D-stacked memory; Big data applications; Communication overheads; Efficiency degradation; Emerging applications; Heterogeneous architectures; Near-memory processing (NMP); Reconfigurable processors; Computer architecture
Power consumption models for multi-tenant server infrastructures,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041434824&doi=10.1145%2f3148965&partnerID=40&md5=57a8ce5347aee08162ddc3d59197c3bd,"Multi-tenant virtualized infrastructures allow cloud providers to minimize costs through workload consolidation. One of the largest costs is power consumption, which is challenging to understand in heterogeneous environments. We propose a power modeling methodology that tackles this complexity using a divide-andconquer approach. Our results outperform previous research work, achieving a relative error of 2% on average and under 4% in almost all cases. Models are portable across similar architectures, enabling predictions of power consumption before migrating a tenant to a different hardware platform. Moreover, we show the models allow us to evaluate colocations of tenants to reduce overall consumption. © 2017 ACM.",Multi-tenant cloud infrastructures; Power consumption models; Virtualization,Electric power utilization; Virtualization; Cloud infrastructures; Cloud providers; Hardware platform; Heterogeneous environments; Power consumption model; Relative errors; Server infrastructure; Workload consolidation; Green computing
Cache exclusivity and sharing: Theory and optimization,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041438727&doi=10.1145%2f3134437&partnerID=40&md5=9e5de895701e046b615f86ac11667dec,"A problem on multicore systems is cache sharing, where the cache occupancy of a program depends on the cache usage of peer programs. Exclusive cache hierarchy as used on AMD processors is an effective solution to allow processor cores to have a large private cache while still benefitting from shared cache. The shared cache stores the ""victims"" (i.e., data evicted from private caches). The performance depends on how victims of co-run programs interact in shared cache. This article presents a new metric called the victim footprint (VFP). It is measured once per program in its solo execution and can then be combined to compute the performance of any exclusive cache hierarchy, replacing parallel testing with theoretical analysis. The work evaluates the VFP by using it to analyze cache sharing by parallel mixes of sequential programs, comparing the accuracy of the theory to hardware counter results, and measuring the benefit of exclusivity-aware analysis and optimization. © 2017 ACM.",Cache analysis; Cache sharing; Exclusive cache,Parallel processing systems; Software testing; Cache analysis; Cache hierarchies; Cache sharing; Effective solution; Exclusive cache; Hardware counters; Multi-core systems; Sequential programs; Multicore programming
"Cooperative multi-agent reinforcement learning-based co-optimization of cores, caches, and on-chip network",2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041434246&doi=10.1145%2f3132170&partnerID=40&md5=1f1b1104eda83ee83af674a4564863ad,"Modern multi-core systems provide huge computational capabilities, which can be used to run multiple processes concurrently. To achieve the best possible performance within limited power budgets, the various system resources need to be allocated effectively. Any mismatch between runtime resource requirement and allocation leads to a sub-optimal energy-delay product (EDP). Different optimization techniques exist for addressing the problem of mismatch between the dynamic requirement and runtime allocation of the system resources. Choosing between multiple optimizations at runtime is complex due to the non-additive effects, making the scenario suitable for the application of machine learning techniques. We present a novel method,Machine LearnedMachines (MLM), by using online reinforcement learning (RL) to perform dynamic partitioning of the last level cache (LLC), along with dynamic voltage and frequency scaling (DVFS) of the core and uncore (interconnection network and LLC). We have proposed and evaluated three different MLM co-optimization techniques based on independent and cooperative multi-agent learners. We show that the co-optimization results in a much lower system EDP than any of the techniques applied individually. We explore various RL models targeted toward optimization of different system metrics and study their effects on a system EDP, system throughput (STP), and Fairness. The various proposed techniques have been extensively evaluated with a mix of 20 workloads on a 4-core system using Spec2006 benchmarks.We have further evaluated our cooperative MLM techniques on a 16-core system. The results show an average of 20.5% and 19.1% system EDP improvement on a 4-core and 16-core system, respectively, with limited degradation of STP and Fairness. © 2017 ACM.",And frequency scaling; Cache management; Computer architectures; Dynamic voltage; Energy efficient computation; Low power architectures,Budget control; Dynamic frequency scaling; Energy efficiency; Interconnection networks (circuit switching); Learning systems; Memory architecture; Multi agent systems; Network architecture; Outsourcing; Reinforcement learning; Voltage scaling; Cache management; Dynamic voltage; Energy efficient; Frequency-scaling; Low power architecture; Computer architecture
Energy-efficient compilation of irregular task-parallel loops,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040307279&doi=10.1145%2f3136063&partnerID=40&md5=7ffc51061fc60d3d034dff5ec37c55ea,"Energy-efficient compilation is an important problem for multi-core systems. In this context, irregular programs with task-parallel loops present interesting challenges: The threads with lesser work-loads (noncritical- threads) wait at the join-points for the thread with maximum work-load (critical-thread); this leads to significant energy wastage. This problem becomes more interesting in the context of multi-socket-multi-core (MSMC) systems, where different sockets may run at different frequencies, but all the cores connected to a socket run at a single frequency. In such a configuration, even though the load-imbalance among the cores may be significant, an MSMC-oblivious technique may miss the opportunities to reduce energy consumption, if the load-imbalance across the sockets is minimal. This problem becomes further challenging in the presence of mutual-exclusion, where scaling the frequencies of a socket executing the non-critical-threads can impact the execution time of the critical-threads. In this article, we propose a scheme (X10Ergy) to obtain energy gains with minimal impact on the execution time, for task-parallel languages, such as X10, HJ, and so on. X10Ergy takes as input a loop-chunked program (parallel-loop iterations divided into chunks and each chunk is executed by a unique thread). X10Ergy follows a mixed compile-time + runtime approach that (i) uses static analysis to efficiently compute the work-load of each chunk at runtime, (ii) computes the ""remaining"" work-load of the chunks running on the cores of each socket at regular intervals and tunes the frequency of the sockets accordingly, (iii) groups the threads into different sockets (based on the remaining work-load of their respective chunks), and (iv) in the presence of atomic-blocks, models the effect of frequency-scaling on the critical-thread.We implemented X10Ergy for X10 and have obtained encouraging results for the IMSuite kernels. © 2017 ACM.",DVFS; Irregular task parallel programs; Multi-socket-multi-core systems,Embedded systems; Energy utilization; Multicore programming; Parallel programming; Static analysis; Different frequency; DVFS; Frequency-scaling; Irregular programs; Multi-core systems; Mutual exclusions; Reduce energy consumption; Task parallel; Energy efficiency
A transactional correctness tool for abstract data types,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041442761&doi=10.1145%2f3148964&partnerID=40&md5=9773fc5bbb7a286a6a2980a601bb00c7,"Transactional memory simplifies multiprocessor programming by providing the guarantee that a sequential block of code in the form of a transaction will exhibit atomicity and isolation. Transactional data structures offer the same guarantee to concurrent data structures by enabling the atomic execution of a composition of operations. The concurrency control of transactional memory systems preserves atomicity and isolation by detecting read/write conflicts among multiple concurrent transactions. State-of-the-art transactional data structures improve on this concurrency control protocol by providing explicit transaction-level synchronization for only non-commutative operations. Since read/write conflicts are handled by thread-level concurrency control, the correctness of transactional data structures cannot be evaluated according to the read/write histories. This presents a challenge for existing correctness verification techniques for transactional memory, because correctness is determined according to the transitions taken by the transactions in the presence of read/write conflicts. In this article, we present Transactional Correctness tool for Abstract Data Types (TxC-ADT), the first tool that can check the correctness of transactional data structures. TxC-ADT elevates the standard definitions of transactional correctness to be in terms of an abstract data type, an essential aspect for checking correctness of transactions that synchronize only for high-level semantic conflicts. To accommodate a diverse assortment of transactional correctness conditions, we present a technique for defining correctness as a happens-before relation. Defining a correctness condition in this manner enables an automated approach in which correctness is evaluated by generating and analyzing a transactional happens-before graph during model checking. A transactional happens-before graph is maintained on a per-thread basis, making our approach applicable to transactional correctness conditions that do not enforce a total order on a transactional execution. We demonstrate the practical applications of TxC-ADT by checking Lock Free Transactional Transformation and Transactional Data Structure Libraries for serializability, strict serializability, opacity, and causal consistency © 2017 ACM.",Concurrency; Correctness verification; Transactional data structure,Abstract data types; Abstracting; Data structures; Metadata; Model checking; Semantics; Storage allocation (computer); Concurrency; Concurrency control protocols; Concurrent data structures; Concurrent transactions; Correctness conditions; Correctness verifications; Transactional data; Transactional memory; Concurrency control
Bringing parallel patterns out of the corner: The P3ARSEC benchmark suite,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032636022&doi=10.1145%2f3132710&partnerID=40&md5=aa002a60907d9460ad8dc3e15b557989,"High-level parallel programming is an active research topic aimed at promoting parallel programming methodologies that provide the programmer with high-level abstractions to develop complex parallel software with reduced time to solution. Pattern-based parallel programming is based on a set of composable and customizable parallel patterns used as basic building blocks in parallel applications. In recent years, a considerable effort has been made in empowering this programming model with features able to overcome shortcomings of early approaches concerning flexibility and performance. In this article, we demonstrate that the approach is flexible and efficient enough by applying it on 12 out of 13 PARSEC applications. Our analysis, conducted on three different multicore architectures, demonstrates that pattern-based parallel programming has reached a good level of maturity, providing comparable results in terms of performance with respect to both other parallel programming methodologies based on pragma-based annotations (i.e., OpenMP and OmpSs) and native implementations (i.e., Pthreads). Regarding the programming effort, we also demonstrate a considerable reduction in lines of code and code churn compared to Pthreads and comparable results with respect to other existing implementations. © 2017 ACM.",Algorithmic skeletons; Benchmarking; Multicore programming; Parallel patterns; Parsec,Application programming interfaces (API); Benchmarking; Computer programming; Multiprocessing systems; Parallel programming; Software architecture; Algorithmic skeleton; Basic building block; High-level abstraction; Multicore architectures; Parallel application; Parallel patterns; Parsec; Programming methodology; Multicore programming
MiCOMP: Mitigating the compiler phase-ordering problem using optimization sub-sequences and machine learning,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029531423&doi=10.1145%2f3124452&partnerID=40&md5=7e875964fb481e1813384b826aa9f3a2,"Recent compilers offer a vast number of multilayered optimizations targeting different code segments of an application. Choosing among these optimizations can significantly impact the performance of the code being optimized. The selection of the right set of compiler optimizations for a particular code segment is a very hard problem, but finding the best ordering of these optimizations adds further complexity. Finding the best ordering represents a long standing problem in compilation research, named the phase-ordering problem. The traditional approach of constructing compiler heuristics to solve this problem simply cannot cope with the enormous complexity of choosing the right ordering of optimizations for every code segment in an application. This article proposes an automatic optimization frameworkwe call MiCOMP,which Mitigates the Compiler Phase-ordering problem. We perform phase ordering of the optimizations in LLVM's highest optimization level using optimization sub-sequences and machine learning. The idea is to cluster the optimization passes of LLVM's O3 setting into different clusters to predict the speedup of a complete sequence of all the optimization clusters instead of having to deal with the ordering of more than 60 different individual optimizations. The predictive model uses (1) dynamic features, (2) an encoded version of the compiler sequence, and (3) an exploration heuristic to tackle the problem. Experimental results using the LLVM compiler framework and the Cbench suite show the effectiveness of the proposed clustering and encoding techniques to application-based reordering of passes, while using a number of predictive models. We perform statistical analysis on the results and compare against (1) random iterative compilation, (2) standard optimization levels, and (3) two recent prediction approaches. We show that MiCOMP's iterative compilation using its sub-sequences can reach an average performance speedup of 1.31 (up to 1.51). Additionally, we demonstrate that MiCOMP's prediction model outperforms the -O1, -O2, and -O3 optimization levels within using just a few predictions and reduces the prediction error rate down to only 5%. Overall, it achieves 90% of the available speedup by exploring less than 0.001% of the optimization space. © 2017 ACM.",Autotuning; Optimizations; Phase-ordering; Supervised-learning,Artificial intelligence; Codes (symbols); Forecasting; Iterative methods; Learning systems; Problem solving; Program compilers; Supervised learning; Automatic optimization; Autotuning; Compiler optimizations; Iterative compilation; Phase Ordering; Prediction error rates; Standard optimization; Traditional approaches; Optimization
HAP: Hybrid-memory-aware partition in shared last-level cache,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029504527&doi=10.1145%2f3106340&partnerID=40&md5=a4735ba88f87fe38d46fa57085a0e533,"Data-center servers benefit from large-capacity memory systems to run multiple processes simultaneously. Hybrid DRAM-NVM memory is attractive for increasing memory capacity by exploiting the scalability of Non-Volatile Memory (NVM). However, current LLC policies are unaware of hybrid memory. Cache misses to NVMintroduce high cost due to long NVMlatency.Moreover, evicting dirty NVM data suffer fromlong write latency. We propose hybrid memory aware cache partitioning to dynamically adjust cache spaces and give NVM dirty data more chances to reside in LLC. Experimental results show Hybrid-memory-Aware Partition (HAP) improves performance by 46.7% and reduces energy consumption by 21.9% on average against LRU management. Moreover, HAP averagely improves performance by 9.3% and reduces energy consumption by 6.4% against a state-of-The-Art cache mechanism. © 2017 ACM.",CPU cache; Hybrid memory,
Programming heterogeneous systems from an image processing DSL,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028621454&doi=10.1145%2f3107953&partnerID=40&md5=401139e0b51efddb7da3d8a2b5bebc00,"Specialized image processing accelerators are necessary to deliver the performance and energy efficiency required by important applications in computer vision, computational photography, and augmented reality. But creating, ""programming,"" and integrating this hardware into a hardware/software system is difficult.We address this problem by extending the image processing language Halide so users can specify which portions of their applications should become hardware accelerators, and then we provide a compiler that uses this code to automatically create the accelerator along with the ""glue"" code needed for the user's application to access this hardware. Starting with Halide not only provides a very high-level functional description of the hardware but also allows our compiler to generate a complete software application, which accesses the hardware for acceleration when appropriate. Our system also provides high-level semantics to explore different mappings of applications to a heterogeneous system, including the flexibility of being able to change the throughput rate of the generated hardware. We demonstrate our approach by mapping applications to a commercial Xilinx Zynq system. Using its FPGA with two low-power ARM cores, our design achieves up to 6× higher performance and 38× lower energy compared to the quad-core ARM CPU on an NVIDIA Tegra K1, and 3.5× higher performance with 12× lower energy compared to the K1's 192-core GPU. © 2017 ACM.",domain specific languages; FPGAs; high-level synthesis; Image processing,Application programs; Augmented reality; Color photography; Computer hardware; Computer programming languages; Computer systems programming; Digital subscriber lines; Energy efficiency; Field programmable gate arrays (FPGA); Green computing; Hardware; High level languages; High level synthesis; Mapping; Problem oriented languages; Program compilers; Semantics; Computational photography; Domain specific languages; Hardware accelerators; Hardware/software systems; Heterogeneous systems; High level semantics; Mapping applications; Software applications; Image processing
An architecture for integrated near-data processors,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029544669&doi=10.1145%2f3127069&partnerID=40&md5=6bed2fc2c80dbef6c92959596430b675,"To increase the performance of data-intensive applications, we present an extension to a CPU architecture that enables arbitrary near-data processing capabilities close to the main memory. This is realized by introducing a component attached to the CPU system-bus and a component at the memory side. Together they support hardware-managed coherence and virtual memory support to integrate the near-data processors in a shared-memory environment. We present an implementation of the components, as well as a systemsimulator, providing detailed performance estimations. With a variety of syntheticworkloadswe demonstrate the performance of the memory accesses, the mixed fine-And coarse-grained coherence mechanisms, and the near-data processor communication mechanism. Furthermore, we quantify the inevitable start-up penalty regarding coherence and data writeback, and argue that near-data processingworkloads should access data several times to offset this penalty. A case study based on the Graph500 benchmark confirms the small overhead for the proposed coherence mechanisms and shows the ability to outperform a real CPU by a factor of two. © 2017 ACM.",Coherence; Computer architecture; Data locality; Graph500; Near-data processing; Virtual memory,
Extending halide to improve software development for imaging DSPs,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029543730&doi=10.1145%2f3106343&partnerID=40&md5=6b4ef70a4f5d75ac45ccc24ecae7e7ed,"Specialized Digital Signal Processors (DSPs), which can be found in a wide range of modern devices, play an important role in power-efficient, high-performance image processing. Applications including camera sensor post-processing and computer vision benefit from being (partially) mapped onto such DSPs. However, due to their specialized instruction sets and dependence on low-level code optimization, developing applications for DSPs is more time-consuming and error-prone than for general-purpose processors. Halide is a domain-specific language (DSL) that enables low-effort development of portable, high-performance imaging pipelines-A combination of qualities that is hard, if not impossible, to find among DSP programming models. We propose a set of extensions and modifications to Halide to generate code for DSP C compilers, focusing specifically on diverse SIMD target instruction sets and heterogeneous scratchpad memory hierarchies. We implement said techniques for a commercial DSP found in an Intel Image Processing Unit (IPU), demonstrating that this solution can be used to achieve performance within 20% of highly tuned, manually written C code, while leading to a reduction in code complexity. By comparing performance of Halide algorithms using our solution to results on CPU and GPU targets, we confirm the value of using DSP targets with Halide. © 2017 ACM.",Accelerator; Asip; DSP; Embedded processor; Halide; Image processing unit; IPU; Optimization; Parallel computing; Performance optimization; Scratchpad memory; SIMD; VLIW,C (programming language); Codes (symbols); Computer programming languages; Digital devices; Digital signal processing; Digital signal processors; General purpose computers; Memory architecture; Modems; Multiprocessing systems; Optimization; Parallel processing systems; Particle accelerators; Problem oriented languages; Signal processing; Software design; Very long instruction word architecture; Asip; Embedded processors; Halide; Performance optimizations; Scratch pad memory; SIMD; VLIW; Image processing
Efficient generation of compact execution traces for multicore architectural simulations,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029508488&doi=10.1145%2f3106342&partnerID=40&md5=120e210deb386f729aa7a751df4c78ca,"Requiring no functional simulation, trace-driven simulation has the potential of achieving faster simulation speeds than execution-driven simulation of multicore architectures. An efficient, on-The-fly, high-fidelity trace generation method for multithreaded applications is reported. The generated trace is encoded in an instruction-like binary format that can be directly ""interpreted"" by a timing simulator to simulate a general load/store or x8-like architecture. A complete tool suite that has been developed and used for evaluation of the proposed method showed that it produces smaller traces over existing trace compression methods while retaining good fidelity including all threading-And synchronization-related events. © 2017 ACM.",Execution traces; Multicore simulations; Trace compression; Trace-based simulations,Software architecture; Architectural simulation; Execution trace; Execution-driven simulation; Multi-threaded application; Multicore architectures; Multicore simulations; Trace compression; Trace-based simulation; Architecture
Providing predictable performance via a slowdown estimation model,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028595930&doi=10.1145%2f3124451&partnerID=40&md5=6a2d82ffff96f97e5a177491e73160ac,"Interapplication interference at shared main memory slows down different applications differently. A few slowdown estimation models have been proposed to provide predictable performance by quantifying memory interference, but they have relatively low accuracy. Thus, we propose a more accurate slowdown estimation model called SEM at main memory. First, SEM unifies the slowdown estimation model by measuring IPC directly. Second, SEM uses the per-bank structure to monitor memory interference and improves estimation accuracy by considering write interference, row-buffer interference, and data bus interference. The evaluation results show that SEM has significantly lower slowdown estimation error (4.06%) compared to STFM (30.15%) and MISE (10.1%). © 2017 ACM.",bandwidth allocation; Memory access scheduling; predictable performance; slowdown estimation,Frequency allocation; Software engineering; Bank structures; Estimation errors; Estimation models; Evaluation results; Main memory; Memory access scheduling; Memory interferences; predictable performance; Hardware
SWITCHES: A lightweight runtime for dataflow execution of tasks on many-cores,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029490087&doi=10.1145%2f3127068&partnerID=40&md5=69d15f398df4db8a4ce9229df9563534,"SWITCHES is a task-based dataflow runtime that implements a lightweight distributed triggering system for runtime dependence resolution and uses static scheduling and compile-Time assignment policies to reduce runtime overheads. Unlike other systems, the granularity of loop-Tasks can be increased to favor data-locality, even when having dependences across different loops. SWITCHES introduces explicit task resource allocation mechanisms for efficient allocation of resources and adopts the latest OpenMP Application Programming Interface (API), as to maintain high levels of programming productivity. It provides a source-To-source tool that automatically produces thread-based code. Performance on an Intel Xeon-Phi shows good scalability and surpasses OpenMP by an average of 32%. © 2017 ACM.",Dataflow; Many-core; Parallel programming; Runtime system; SWITCHES; Tasks,Parallel programming; Resource allocation; Switches; Time switches; Allocation mechanism; Dataflow; Dataflow executions; Efficient allocations; Many core; OpenMP applications; Runtime systems; Tasks; Application programming interfaces (API)
Iterative schedule optimization for parallelization in the polyhedron model,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028606154&doi=10.1145%2f3109482&partnerID=40&md5=d8cafcfdc75bb47997b79078d9060540,"The polyhedron model is a powerful model to identify and apply systematically loop transformations that improve data locality (e.g., via tiling) and enable parallelization. In the polyhedron model, a loop transformation is, essentially, represented as an affine function. Well-established algorithms for the discovery of promising transformations are based on performance models. These algorithms have the drawback of not being easily adaptable to the characteristics of a specific program or target hardware. An iterative search for promising loop transformations is more easily adaptable and can help to learn better models. We present an iterative optimization method in the polyhedron model that targets tiling and parallelization. The method enables either a sampling of the search space of legal loop transformations at random or a more directed search via a genetic algorithm. For the latter, we propose a set of novel, tailored reproduction operators.We evaluate our approach against existing iterative and model-driven optimization strategies. We compare the convergence rate of our genetic algorithm to that of random exploration. Our approach of iterative optimization outperforms existing optimization techniques in that it finds loop transformations that yield significantly higher performance. If well configured, then random exploration turns out to be very effective and reduces the need for a genetic algorithm. © 2017 ACM.",Automatic loop optimization; genetic algorithm; OpenMP; parallelization; polyhedron model; tiling,Application programming interfaces (API); Discovery wells; Genetic algorithms; Geometry; Optimization; Loop optimizations; OpenMP; Parallelizations; Polyhedron models; tiling; Iterative methods
MATOG: Array layout auto-Tuning for CUDA,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029503558&doi=10.1145%2f3106341&partnerID=40&md5=8c5eacd051d04b9bf77690e573169e69,"Optimal code performance is (besides correctness and accuracy) the most important objective in compute intensive applications. In many of these applications, Graphic Processing Units (GPUs) are used because of their high amount of compute power. However, caused by their massively parallel architecture, the code has to be specifically adjusted to the underlying hardware to achieve optimal performance and therefore has to be reoptimized for each new generation. In reality, this is usually not the case as productive code is normally at least several years old and nobody has the time to continuously adjust existing code to new hardware. In recent years more and more approaches have emerged that automatically tune the performance of applications toward the underlying hardware. In this article, we present the MATOG auto-Tuner and its concepts. It abstracts the array memory access in CUDA applications and automatically optimizes the code according to the used GPUs. MATOG only requires few profiling runs to analyze even complex applications, while achieving significant speedups over non-optimized code, independent of the used GPU generation and without the need to manually tune the code. © 2017 ACM.",Array layouts; Auto-Tuning; CUDA; GPGPU; MATOG,
Improving loop dependence analysis,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028616346&doi=10.1145%2f3095754&partnerID=40&md5=cb1f6e65230cbda21dd5af334e3876bf,"Programmers can no longer depend on new processors to have significantly improved single-thread performance. Instead, gains have to come from other sources such as the compiler and its optimization passes. Advanced passes make use of information on the dependencies related to loops. We improve the quality of that information by reusing the information given by the programmer for parallelization. We have implemented a prototype based on GCC into which we also add a new optimization pass. Our approach improves the amount of correctly classified dependencies resulting in 46% average improvement in single-thread performance for kernel benchmarks compared to GCC 6.1. © 2017 ACM.",automatic vectorization; OpenMP; SIMD,Application programming interfaces (API); Program compilers; Automatic vectorization; Dependence analysis; OpenMP; Parallelizations; SIMD; Single-thread performance; Benchmarking
Symmetry in software synthesis,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026624459&doi=10.1145%2f3095747&partnerID=40&md5=06672b8b13e0b82c8679a81995fc0f55,"With the surge of multi- and many-core systems, much research has focused on algorithms for mapping and scheduling on these complex platforms. Large classes of these algorithms face scalability problems. This is why diverse methods are commonly used for reducing the search space. While most such approaches leverage the inherent symmetry of architectures and applications, they do it in a problem-specific and intuitive way. However, intuitive approaches become impractical with growing hardware complexity, like Network-on-Chip interconnect or heterogeneous cores. In this article, we present a formal framework that can determine the inherent local and global symmetry of architectures and applications algorithmically and leverage these for problems in software synthesis. Our approach is based on the mathematical theory of groups and a generalization called inverse semigroups. We evaluate our approach in two state-of-the-art mapping frameworks. Even for the platforms with a handful of cores of today and moderate-sized benchmarks, our approach consistently yields reductions of the overall execution time of algorithms. We obtain a speedup of more than 10× for one use-case and saved 10% of time in another. Copyright 2017  is held by the owner/author(s). Publication rights licensed to ACM.",Automation; Clusters; Design-space exploration; Group theory; Heterogeneous; Inverse-semigroups; Mapping; Metaheuristics; Network-on-chip; Scalability; Symmetry,Automation; Complex networks; Crystal symmetry; Group theory; Mapping; Network architecture; Network-on-chip; Scalability; Servers; Clusters; Design space exploration; Heterogeneous; Meta heuristics; Semigroups; Application programs
On the interactions between value prediction and compiler optimizations in the context of EOLE,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026506440&doi=10.1145%2f3090634&partnerID=40&md5=892b5869c6d0377b4aed15a26556e8ce,"Increasing instruction-level parallelism is regaining attractiveness within the microprocessor industry. The {Early | Out-of-order | Late} Execution (EOLE) microarchitecture and Differential Value TAgged GEometric (D-VTAGE) value predictor were recently introduced to solve practical issues of Value Prediction (VP). In particular, they remove the most significant difficulties that forbade an effective VP hardware. In this study, we present a detailed evaluation of the potential of VP in the context of EOLE/D-VTAGE and different compiler options. Our study shows that if no single general rule always applies—more optimization might sometimes lead to more performance—unoptimized codes often get a large benefit from the prediction of redundant loads. © 2017 ACM",General-purpose processor; High performance; Single-thread performance; {Early | Out-of-order | Late} Execution microarchitecture,Computer architecture; Embedded systems; Forecasting; General purpose computers; Program processors; Value engineering; Compiler optimizations; General purpose processors; High performance; Instruction level parallelism; Micro architectures; Microprocessor industry; Practical issues; Single-thread performance; Program compilers
CACTI 7: New tools for interconnect exploration in innovative off-chip memories,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027026353&doi=10.1145%2f3085572&partnerID=40&md5=427ad4033a9056e40786ab8045cddfec,"Historically, server designers have opted for simple memory systems by picking one of a few commoditized DDR memory products. We are already witnessing a major upheaval in the off-chip memory hierarchy, with the introduction of many new memory products-buffer-on-board, LRDIMM, HMC, HBM, and NVMs, to name a few. Given the plethora of choices, it is expected that different vendors will adopt different strategies for their high-capacity memory systems, often deviating from DDR standards and/or integrating new functionality within memory systems. These strategies will likely differ in their choice of interconnect and topology, with a significant fraction of memory energy being dissipated in I/O and data movement. To make the case for memory interconnect specialization, this paper makes three contributions. First, we design a tool that carefully models I/O power in the memory system, explores the design space, and gives the user the ability to define new types of memory interconnects/topologies. The tool is validated against SPICE models, and is integrated into version 7 of the popular CACTI package. Our analysis with the tool shows that several design parameters have a significant impact on I/O power. We then use the tool to help craft novel specialized memory system channels. We introduce a new relay-onboard chip that partitions a DDR channel into multiple cascaded channels. We show that this simple change to the channel topology can improve performance by 22% for DDR DRAM and lower cost by up to 65% for DDR DRAM. This new architecture does not require any changes to DIMMs, and it efficiently supports hybrid DRAM/NVM systems. Finally, as an example of a more disruptive architecture, we design a custom DIMM and parallel bus that moves away from the DDR3/DDR4 standards. To reduce energy and improve performance, the baseline data channel is split into three narrow parallel channels and the on-DIMM interconnects are operated at a lower frequency. In addition, this allows us to design a two-tier error protection strategy that reduces data transfers on the interconnect. This architecture yields a performance improvement of 18% and a memory power reduction of 23%. The cascaded channel and narrow channel architectures serve as case studies for the new tool and show the potential for benefit from re-organizing basic memory interconnects. © 2017 ACM.",DRAM; Interconnects; Memory; NVM; Tools,Data reduction; Data storage equipment; Data transfer; Integrated circuit design; Integrated circuit interconnects; Memory architecture; Optical interconnects; System buses; Tools; Topology; Channel topology; Design parameters; Disruptive architecture; Error protection; High-capacity memory; Improve performance; Lower frequencies; Power reductions; Dynamic random access storage
Band-pass prefetching: An effective prefetch management mechanism using prefetch-fraction metric in multi-core systems,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027013736&doi=10.1145%2f3090635&partnerID=40&md5=ed53ebf14e4f0d7ccd994bfde9513475,"In multi-core systems, an application's prefetcher can interfere with the memory requests of other applications using the shared resources, such as last level cache and memory bandwidth. In order to minimize prefetcher-caused interference, prior mechanisms have been proposed to dynamically control prefetcher aggressiveness at runtime. These mechanisms use several parameters to capture prefetch usefulness as well as prefetcher-caused interference, performing aggressive control decisions. However, these mechanisms do not capture the actual interference at the shared resources and most often lead to incorrect aggressiveness control decisions. Therefore, prior works leave scope for performance improvement. Toward this end, we propose a solution to manage prefetching in multicore systems. In particular, we make two fundamental observations: First, a positive correlation exists between the accuracy of a prefetcher and the amount of prefetch requests it generates relative to an application's total (demand and prefetch) requests. Second, a strong positive correlation exists between the ratio of total prefetch to demand requests and the ratio of average last level cache miss service times of demand to prefetch requests. In this article, we propose Band-pass prefetching that builds on those two observations, a simple and low-overhead mechanism to effectively manage prefetchers in multicore systems. Our solution consists of local and global prefetcher aggressiveness control components, which altogether, control the flow of prefetch requests between a range of prefetch to demand requests ratios. From our experiments on 16-core multi-programmed workloads, on systems using stream prefetching, we observe that Band-pass prefetching achieves 12.4% (geometric-mean) improvement on harmonic speedup over the baseline that implements no prefetching, while aggressive prefetching without prefetcher aggressiveness control and state-of-the-art HPAC, P-FST, and CAFFEINE achieve 8.2%, 8.4%, 1.4%, and 9.7%, respectively. Further evaluation of the proposed Band-pass prefetching mechanism on systems using AMPM prefetcher shows similar performance trends. For a 16-core system, Band-pass prefetching requires only a modest hardware cost of 239 bytes. © 2017 ACM.",Intercore interference; Memory systems; Multicore; Prefetching,Bandpass filters; Control components; Management mechanisms; Memory systems; Multi core; Multi-core systems; Positive correlations; Prefetching; Stream prefetching; Cache memory
Decoupling data supply from computation for latency-tolerant communication in heterogeneous architectures,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027015996&doi=10.1145%2f3075620&partnerID=40&md5=72831c6013d21b1f8f62031dd682783a,"In today's computers, heterogeneous processing is used to meet performance targets at manageable power. In adopting increased compute specialization, however, the relative amount of time spent on communication increases. System and software optimizations for communication often come at the costs of increased complexity and reduced portability. The Decoupled Supply-Compute (DeSC) approach offers a way to attack communication latency bottlenecks automatically, while maintaining good portability and low complexity. Our work expands prior Decoupled Access Execute techniques with hardware/software specialization. For a range of workloads, DeSC offers roughly 2× speedup, and additional specialized compression optimizations reduce traffic between decoupled units by 40%. © 2017 ACM.",Accelerators; Communication management; Decoupled architecture,Hardware; Particle accelerators; Software engineering; Communication latency; Communication management; Decoupled architecture; Hardware/software; Heterogeneous architectures; Heterogeneous processing; Performance targets; Software optimization; Computer software portability
Scratchpad sharing in GPUs,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027057064&doi=10.1145%2f3075619&partnerID=40&md5=baba745b7dd8ff30cb09d12a9efa5f53,"General-Purpose Graphics Processing Unit (GPGPU) applications exploit on-chip scratchpad memory available in the Graphics Processing Units (GPUs) to improve performance. The amount of thread level parallelism (TLP) present in the GPU is limited by the number of resident threads, which in turn depends on the availability of scratchpad memory in its streaming multiprocessor (SM). Since the scratchpad memory is allocated at thread block granularity, part of the memory may remain unutilized. In this article, we propose architectural and compiler optimizations to improve the scratchpad memory utilization. Our approach, called Scratchpad Sharing, addresses scratchpad under-utilization by launching additional thread blocks in each SM. These thread blocks use unutilized scratchpad memory and also share scratchpad memory with other resident blocks. To improve the performance of scratchpad sharing, we propose Owner Warp First (OWF) scheduling that schedules warps from the additional thread blocks effectively. The performance of this approach, however, is limited by the availability of the part of scratchpad memory that is shared among thread blocks. We propose compiler optimizations to improve the availability of shared scratchpad memory. We describe an allocation scheme that helps in allocating scratchpad variables such that shared scratchpad is accessed for short duration. We introduce a new hardware instruction, relssp, that when executed releases the shared scratchpad memory. Finally, we describe an analysis for optimal placement of relssp instructions, such that shared scratchpad memory is released as early as possible, but only after its last use, along every execution path. We implemented the hardware changes required for scratchpad sharing and the relssp instruction using the GPGPU-Sim simulator and implemented the compiler optimizations in Ocelot framework. We evaluated the effectiveness of our approach on 19 kernels from 3 benchmarks suites: CUDA-SDK, GPGPUSim, and Rodinia. The kernels that under-utilize scratchpad memory show an average improvement of 19% and maximum improvement of 92.17% in terms of the number of instruction executed per cycle when compared to the baseline approach, without affecting the performance of the kernels that are not limited by scratchpad memory. © 2017 ACM.",Code motion; Control flow graph; Scratchpad sharing; Thread level parallelism,Computer graphics; Computer graphics equipment; Computer hardware; Data flow analysis; Flow graphs; Graphics processing unit; Hardware; Image coding; Program compilers; Program processors; Weaving; Code motion; Compiler optimizations; Control flow graphs; General purpose graphics processing unit (GPGPU); Improve performance; Scratchpad; Streaming multiprocessors; Thread level parallelism; Memory architecture
An integrated vector-scalar design on an in-order ARM core,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027059763&doi=10.1145%2f3075618&partnerID=40&md5=a04d55c76ecc452808ebd6bc954930c7,"In the low-end mobile processor market, power, energy, and area budgets are significantly lower than in the server/desktop/laptop/high-end mobile markets. It has been shown that vector processors are a highly energyefficient way to increase performance; however, adding support for them incurs area and power overheads that would not be acceptable for low-end mobile processors. In this work, we propose an integrated vectorscalar design for the ARM architecture that mostly reuses scalar hardware to support the execution of vector instructions. The key element of the design is our proposed block-based model of execution that groups vector computational instructions together to execute them in a coordinated manner. We implemented a classic vector unit and compare its results against our integrated design. Our integrated design improves the performance (more than 6×) and energy consumption (up to 5×) of a scalar in-order core with negligible area overhead (only 4.7% when using a vector register with 32 elements). In contrast, the area overhead of the classic vector unit can be significant (around 44%) if a dedicated vector floating-point unit is incorporated. Our block-based vector execution outperforms the classic vector unit for all kernels with floating-point data and also consumes less energy. We also complement the integrated design with three energy/performanceefficient techniques that further reduce power and increase performance. The first proposal covers the design and implementation of chaining logic that is optimized to work with the cache hierarchy through vector memory instructions, the second proposal reduces the number of reads/writes from/to the vector register file, and the third idea optimizes complex memory access patterns with the memory shape instruction and unified indexed vector load. © 2017 ACM.",Energy efficiency; Low-power; Mobile processors; Vector processors,ARM processors; Array processing; Budget control; Cache memory; Commerce; Computation theory; Digital arithmetic; Energy efficiency; Energy utilization; Integrated circuit design; Memory architecture; Design and implementations; Floating point units; Floating-point data; Low Power; Memory access patterns; Mobile processors; Model of executions; Vector processors; Vectors
DawnCC: Automatic annotation for data parallelism and offloading,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027025554&doi=10.1145%2f3084540&partnerID=40&md5=ac95500ebf0d779651c281d3f6c0a820,"Directive-based programming models, such as OpenACC and OpenMP, allow developers to convert a sequential program into a parallel one with minimum human intervention. However, inserting pragmas into production code is a difficult and error-prone task, often requiring familiarity with the target program. This difficulty restricts the ability of developers to annotate code that they have not written themselves. This article provides a suite of compiler-related methods to mitigate this problem. Such techniques rely on symbolic range analysis, a well-known static technique, to achieve two purposes: populate source code with data transfer primitives and to disambiguate pointers that could hinder automatic parallelization due to aliasing. We have materialized our ideas into a tool, DawnCC, which can be used stand-alone or through an online interface. To demonstrate its effectiveness, we show how DawnCC can annotate the programs available in PolyBench without any intervention from users. Such annotations lead to speedups of over 100× in an Nvidia architecture and over 50× in an ARM architecture. © 2017 ACM.",Automatic parallelization; Static analysis,Application programming interfaces (API); Codes (symbols); Data transfer; Interfaces (materials); Automatic annotation; Automatic Parallelization; Error prone tasks; Human intervention; Programming models; Sequential programs; Static techniques; Symbolic range analysis; Static analysis
Dirty-block tracking in a direct-mapped DRAM cache with self-balancing dispatch,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019877979&doi=10.1145%2f3068460&partnerID=40&md5=037ba5d92767dc752da8aee35e8b6d61,"Recently, processors have begun integrating 3D stacked DRAMs with the cores on the same package, and there have been several approaches to effectively utilizing the on-package DRAMs as caches. This article presents an approach that combines the previous approaches in a synergistic way by devising a module called the dirty-block tracker to maintain the dirtiness of each block in a dirty region. The approach avoids unnecessary tag checking for a write operation if the corresponding block in the cache is not dirty. Our simulation results show that the proposed technique achieves a 10.3% performance improvement on average over the state-of-the-art DRAM cache technique. © 2017 ACM.",3D-stacked memory; DRAM Cache; memory bandwidth,Dynamic random access storage; Three dimensional integrated circuits; 3d-stacked drams; 3D-stacked memory; Memory bandwidths; Self-balancing; State of the art; Write operations; Cache memory
"CompEx++: Compression-expansion coding for energy, latency, and lifetime improvements in MLC/TLC NVMs",2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018478204&doi=10.1145%2f3050440&partnerID=40&md5=956b7a20ce883189ea88bcf41ec9dbf4,"Multilevel/triple-level cell nonvolatile memories (MLC/TLC NVMs) such as phase-change memory (PCM) and resistive RAM (RRAM) are the subject of active research and development as replacement candidates for DRAM, which is limited by its high refresh power and poor scaling potential. In addition to the benefits of nonvolatility (low refresh power) and improved scalability, MLC/TLC NVMs offer high data density and memory capacity over DRAM. However, the viability of MLC/TLC NVMs is limited primarily due to the high programming energy and latency as well as the low endurance of NVM cells; these are primarily attributed to the iterative program-and-verify procedure necessary for programming the NVM cells. This article proposes compression-expansion (CompEx) coding, a low overhead scheme that synergistically integrates pattern-based compression with expansion coding to realize simultaneous energy, latency, and lifetime improvements in MLC/TLC NVMs. CompEx coding is agnostic to the choice of compression technique; in this work, we evaluate CompEx coding using both frequent pattern compression (FPC) and base-delta-immediate (BΔI) compression. CompEx coding integrates FPC/BΔI with (k,m)q ""expansion"" coding; expansion codes are a class of q-ary linear block codes that encode data using only the low energy states of a q-ary NVM cell. CompEx coding simultaneously reduces energy and latency and improves lifetime for negligible-to-no memory overhead and negligible logic overhead (≈10k gates, which is <0.1% per NVM module). Furthermore, we also propose CompEx++ coding, which extends CompEx coding by leveraging the variable compressibility of pattern-based compression techniques. CompEx++ coding integrates custom expansion codes to each of the compression patterns to exploit maximum energy/latency benefits of CompEx coding. Our full-system simulations using TLC RRAM show that CompEx/CompEx++ coding reduces total memory energy by 57%/61% and write latency by 23.5%/26%; these improvements translate to a 5.7%/10.6% improvement in IPC, a 11.8%/19.9% improvement in main memory bandwidth, and 1.8x improvement in lifetime over classical binary coding using data-comparison write. CompEx/CompEx++ coding thus addresses the programming energy/latency and lifetime challenges of MLC/TLC NVMs that pose a serious technological roadblock to their adoption in high-performance computing systems. © 2017 ACM.",Compression; Energy; Latency; Main memory; Nonvolatile memories,Cells; Compaction; Computation theory; Cytology; Dynamic random access storage; Expansion; Iterative methods; Phase change memory; Random access storage; RRAM; Energy; Full-system simulation; High performance computing systems; Latency; Main memory; Non-volatile memory; Phase change memory (pcm); Research and development; Codes (symbols)
Exceptionization: A Java VM optimization for non-Java languages,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018465990&doi=10.1145%2f3046681&partnerID=40&md5=a23f49c12177085772550426ecd4f8cc,"Java virtual machine (JVM) has recently evolved into a general-purpose language runtime environment to execute popular programming languages such as JavaScript, Ruby, Python, and Scala. These languages have complex non-Java features, including dynamic typing and first-class function, so additional language runtimes (engines) are provided on top of the JVM to support them with bytecode extensions. Although there are high-performance JVMs with powerful just-in-time (JIT) compilers, running these languages efficiently on the JVM is still a challenge. This article introduces a simple and novel technique for the JVM JIT compiler called exceptionization to improve the performance of JVM-based language runtimes. We observed that the JVM executing some non-Java languages encounters at least 2 times more branch bytecodes than Java, most of which are highly biased to take only one target. Exceptionization treats such a highly biased branch as some implicit exception-throwing instruction. This allows the JVM JIT compiler to prune the infrequent target of the branch from the frequent control flow, thus compiling the frequent control flow more aggressively with better optimization. If a pruned path were taken, then it would run like a Java exception handler, that is, a catch block. We also devised de-exceptionization, a mechanism to cope with the case when a pruned path is executed more often than expected. Since exceptionization is a generic JVM optimization, independent of any specific language runtime, it would be generally applicable to other language runtimes on the JVM. Our experimental result shows that exceptionization accelerates the performance of several non-Java languages. For example, JavaScript-on-JVM runs faster by as much as 60% and by 6% on average, when experimented with the Octane benchmark suite on Oracle's latest Nashorn JavaScript engine and HotSpot 1.9 JVM. Furthermore, the performance of Ruby-on-JVM shows an improvement by as much as 60% and by 6% on average, while Python-on-JVM improves by as much as 6% and by 2% on average. We found that exceptionization is more effective to apply to the branch bytecode of the language runtime itself than the bytecode corresponding to the application code or the bytecode of the Java class libraries. This implies that the performance benefit of exceptionization comes from better JIT compilation of the language runtime of non-Java languages. © 2017 ACM.",Java virtual machine; Javascript; Just-in-time compiler; Non-java languages,Engines; High level languages; Just in time production; Network security; Ruby; Virtual machine; General purpose languages; Java language; Java virtual machines; Javascript; Just in time compilers; Just-in-time compiler; Performance benefits; Runtime environments; Program compilers
Significance-aware program execution on unreliable hardware,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018972216&doi=10.1145%2f3058980&partnerID=40&md5=6657b1ee3e3e97f9ace40c0f16d586a2,"This article introduces a significance-centric programming model and runtime support that sets the supply voltage in a multicore CPU to sub-nominal values to reduce the energy footprint and provide mechanisms to control output quality. The developers specify the significance of application tasks respecting their contribution to the output quality and provide check and repair functions for handling faults. On a multicore system, we evaluate five benchmarks using an energy model that quantifies the energy reduction. When executing the least-significant tasks unreliably, our approach leads to 20% CPU energy reduction with respect to a reliable execution and has minimal quality degradation. © 2017 ACM.",Energy efficiency; Quality of output; Significance aware computing; Unreliable hardware,Energy efficiency; Green computing; Hardware; Multicore programming; Application tasks; Multi-core systems; Program execution; Programming models; Quality degradation; Reliable execution; Significance aware computing; Unreliable hardwares; Quality control
WCET-aware dynamic I-cache locking for a single task,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017179446&doi=10.1145%2f3046683&partnerID=40&md5=bd83f23b5d99915d8e22d175a6b33132,"Caches are widely used in embedded systems to bridge the increasing speed gap between processors and off-chip memory. However, caches make it significantly harder to compute the worst-case execution time (WCET) of a task. To alleviate this problem, cache locking has been proposed. We investigate the WCET-aware I-cache locking problem and propose a novel dynamic I-cache locking heuristic approach for reducing the WCET of a task. For a nonnested loop, our approach aims at selecting a minimum set of memory blocks of the loop as locked cache contents by using the min-cut algorithm. For a loop nest, our approach not only aims at selecting a minimum set of memory blocks of the loop nest as locked cache contents but also finds a good loading point for each selected memory block. We propose two algorithms for finding a good loading point for each selected memory block, a polynomial-time heuristic algorithm and an integer linear programming (ILP)-based algorithm, further reducing the WCET of each loop nest. We have implemented our approach and compared it to two state-of-the-art I-cache locking approaches by using a set of benchmarks from the MRTC benchmark suite. The experimental results show that the polynomial-time heuristic algorithm for finding a good loading point for each selected memory block performs almost equally as well as the ILP-based algorithm. Compared to the partial locking approach proposed in Ding et al. [2012], our approach using the heuristic algorithm achieves the average improvements of 33%, 15%, 9%, 3%, 8%, and 11% for the 256B, 512B, 1KB, 4KB, 8KB, and 16KB caches, respectively. Compared to the dynamic locking approach proposed in Puaut [2006], it achieves the average improvements of 9%, 19%, 18%, 5%, 11%, and 16% for the 256B, 512B, 1KB, 4KB, 8KB, and 16KB caches, respectively. © 2017 ACM 1544-3566/2017/03-ART4 $15.00.",Dynamic I-cache locking; Minimum cut; Worst-case execution time,Cache memory; Embedded systems; Gears; Heuristic algorithms; Heuristic methods; Heuristic programming; Integer programming; Locks (fasteners); Polynomial approximation; Stress analysis; Benchmark suites; Cache locking; Heuristic approach; Integer Linear Programming; Min-cut algorithm; Minimum cut; Off-chip memory; Worst-case execution time; Loading
Ld: Low-overhead GPU race detection without access monitoring,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017136819&doi=10.1145%2f3046678&partnerID=40&md5=3b383252b7d8514339fe3043c61c74fd,"Data race detection has become an important problem in GPU programming. Previous designs of CPU racechecking tools are mainly task parallel and incur high overhead on GPUs due to access instrumentation, especially when monitoring many thousands of threads routinely used by GPU programs. This article presents a novel data-parallel solution designed and optimized for the GPU architecture. It includes compiler support and a set of runtime techniques. It uses value-based checking, which detects the races reported in previous work, finds new races, and supports race-free deterministic GPU execution. More important, race checking is massively data parallel and does not introduce divergent branching or atomic synchronization. Its slowdown is less than 5× for over half of the tests and 10 × on average, which is orders of magnitude more efficient than the cuda-memcheck tool by Nvidia and the methods that use fine-grained access instrumentation. © 2017 ACM 1544-3566/2017/03-ART9 $15.00.",GPU race detection; Instrumentation-free; Low overhead; Value-based checking,Program processors; Access monitoring; Data race detection; GPU programming; Instrumentation-free; Low overhead; Orders of magnitude; Race detection; Value-based; Graphics processing unit
Energy transparency for deeply embedded programs,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017125630&doi=10.1145%2f3046679&partnerID=40&md5=0062cabab36f399734f1a94e3068dafc,"Energy transparency is a concept that makes a program's energy consumption visible, from hardware up to software, through the different system layers. Such transparency can enable energy optimizations at each layer and between layers, as well as help both programmers and operating systems make energy-Aware decisions. In this article, we focus on deeply embedded devices, typically used for Internet of Things (IoT) applications, and demonstrate how to enable energy transparency through existing static resource analysis (SRA) techniques and a new target-Agnostic profiling technique, without hardware energy measurements. Our novel mapping technique enables software energy consumption estimations at a higher level than the Instruction Set Architecture (ISA), namely the LLVM intermediate representation (IR) level, and therefore introduces energy transparency directly to the LLVM optimizer. We apply our energy estimation techniques to a comprehensive set of benchmarks, including single-and multithreaded embedded programs from two commonly used concurrency patterns: Task farms and pipelines. Using SRA, our LLVM IR results demonstrate a high accuracy with a deviation in the range of 1%from the ISA SRA. Our profiling technique captures the actual energy consumption at the LLVM IR level with an average error of 3%. © 2017 ACM 1544-3566/2017/03-ART8 15.00.",Deeply embedded systems; IoT; LLVM; Profiling; Static analysis; WCET,Computer architecture; Energy utilization; Hardware; Internet of things; Power management; Static analysis; Transparency; Energy optimization; Instruction set architecture; Intermediate representations; Internet of Things (IOT); LLVM; Profiling; Software energy consumption; WCET; Embedded systems
Defragmentation of tasks in many-core architecture,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017113766&doi=10.1145%2f3050437&partnerID=40&md5=7f37113fe89e7308f8e4ffe1def94537,"Many-cores can execute multiple multithreaded tasks in parallel. A task performs most efficiently when it is executed over a spatially connected and compact subset of cores so that performance loss due to communication overhead imposed by the task's threads spread across the allocated cores is minimal. Over a span of time, unallocated cores can get scattered all over the many-core, creating fragments in the task mapping. These fragments can prevent efficient contiguous mapping of incoming new tasks leading to loss of performance. This problem can be alleviated by using a task defragmenter, which consolidates smaller fragments into larger fragments wherein the incoming tasks can be efficiently executed. Optimal defragmentation of a many-core is an NP-hard problem in the general case. Therefore, we simplify the original problem to a problem that can be solved optimally in polynomial time. In this work, we introduce a concept of exponentially separable mapping (ESM), which defines a set of task mapping constraints on a many-core. We prove that an ESM enforcing many-core can be defragmented optimally in polynomial time. © 2017 ACM.",Many-core; Multiagent systems; Task defragmentation,Computational complexity; Mapping; Multi agent systems; Optimization; Polynomial approximation; Communication overheads; Compact subsets; De-fragmentation; Loss of performance; Many core; Many-core architecture; Performance loss; Polynomial-time; Computer architecture
Micro-sector cache: Improving space utilization in sectored DRAM caches,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017168817&doi=10.1145%2f3046680&partnerID=40&md5=fa4bcc8a937270b18226c208473aad1c,"Recent research proposals on DRAM caches with conventional allocation units (64 or 128 bytes) as well as large allocation units (512 bytes to 4KB) have explored ways to minimize the space/latency impact of the tag store and maximize the effective utilization of the bandwidth. In this article, we study sectored DRAM caches that exercise large allocation units called sectors, invest reasonably small storage to maintain tag/state, enable space- and bandwidth-efficient tag/state caching due to low tag working set size and large data coverage per tag element, and minimize main memory bandwidth wastage by fetching only the useful portions of an allocated sector. However, the sectored caches suffer from poor space utilization, since a large sector is always allocated even if the sector utilization is low. The recently proposed Unison cache addresses only a special case of this problem by not allocating the sectors that have only one active block. We propose Micro-sector cache, a locality-aware sectored DRAM cache architecture that features a flexible mechanism to allocate cache blocks within a sector and a locality-aware sector replacement algorithm. Simulation studies on a set of 30 16-way multi-programmed workloads show that our proposal, when incorporated in an optimized Unison cache baseline, improves performance (weighted speedup) by 8%, 14%, and 16% on average, respectively, for 1KB, 2KB, and 4KB sectors at 128MB capacity. These performance improvements result from significantly better cache space utilization, leading to 18%, 21%, and 22% average reduction in DRAM cache read misses, respectively, for 1KB, 2KB, and 4KB sectors at 128MB capacity. We evaluate our proposal for DRAM cache capacities ranging from 128MB to 1GB. © 2017 ACM 1544-3566/2017/03-ART7 $15.00.",DRAM cache; Sectored cache; Space utilization,Bandwidth; Digital storage; Bandwidth efficient; Cache architecture; Flexible mechanisms; Recent researches; Replacement algorithm; Sectored cache; Simulation studies; Space utilization; Dynamic random access storage
Main memory in HPC: Do we need more or could we live with less?,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014964284&doi=10.1145%2f3023362&partnerID=40&md5=8e507d8d96531f5ccfd307446439c32c,"An important aspect of High-Performance Computing (HPC) system design is the choice of main memory capacity. This choice becomes increasingly important now that 3D-stacked memories are entering the market. Compared with conventional Dual In-line Memory Modules (DIMMs), 3D memory chiplets provide better performance and energy efficiency but lower memory capacities. Therefore, the adoption of 3D-stacked memories in the HPC domain depends on whether we can find use cases that require much less memory than is available now. This study analyzes the memory capacity requirements of important HPC benchmarks and applications. We find that the High-Performance Conjugate Gradients (HPCG) benchmark could be an important success story for 3D-stacked memories in HPC, but High-Performance Linpack (HPL) is likely to be constrained by 3D memory capacity. The study also emphasizes that the analysis of memory footprints of production HPC applications is complex and that it requires an understanding of application scalability and target category, i.e., whether the users target capability or capacity computing. The results show that most of the HPC applications under study have per-core memory footprints in the range of hundreds of megabytes, but we also detect applications and use cases that require gigabytes per core. Overall, the study identifies the HPC applications and use cases with memory footprints that could be provided by 3D-stacked memory chiplets, making a first step toward adoption of this novel technology in the HPC domain. © 2017 ACM.",high-performance computing; HPCG; HPL; Memory capacity requirements; production HPC applications,Benchmarking; Energy efficiency; 3D-stacked memory; Application scalability; Dual-in line memory modules; High performance computing; High performance computing systems; High performance linpack; HPCG; Memory capacity; Three dimensional integrated circuits
ALEA: A fine-grained energy profiling tool,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017109508&doi=10.1145%2f3050436&partnerID=40&md5=202d9308d71c9c219e4a5595d8a3171f,"Energy efficiency is becoming increasingly important, yet few developers understand how source code changes affect the energy and power consumption of their programs. To enable them to achieve energy savings, we must associate energy consumption with software structures, especially at the fine-grained level of functions and loops. Most research in the field relies on direct power/energy measurements taken from on-board sensors or performance counters. However, this coarse granularity does not directly provide the needed fine-grained measurements. This article presents ALEA, a novel fine-grained energy profiling tool based on probabilistic analysis for fine-grained energy accounting. ALEA overcomes the limitations of coarse-grained power-sensing instruments to associate energy information effectively with source code at a fine-grained level. We demonstrate and validate that ALEA can perform accurate energy profiling at various granularity levels on two different architectures: Intel Sandy Bridge and ARM big.LITTLE. ALEA achieves a worst-case error of only 2% for coarse-grained code structures and 6% for fine-grained ones, with less than 1% runtime overhead. Our use cases demonstrate that ALEA supports energy optimizations, with energy savings of up to 2.87 times for a latency-critical option pricing workload under a given power budget. © 2017 ACM.",ALEA; Energy efficiency; Energy profiling; Power measurement; Sampling,Budget control; Codes (symbols); Economics; Electric power measurement; Energy conservation; Energy utilization; Sampling; ALEA; Energy optimization; Energy profiling; Granularity levels; Performance counters; Probabilistic analysis; Software structures; Source code changes; Energy efficiency
Pareto governors for energy-optimal computing,2017,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017162737&doi=10.1145%2f3046682&partnerID=40&md5=6259de1ab8a6d96a4991a27d842a8508,"The original definition of energy-proportional computing does not characterize the energy efficiency of recent reconfigurable computers, resulting in nonintuitive ""super-proportional"" behavior. This article introduces a new definition of ideal energy-proportional computing, new metrics to quantify computational energy waste, and new SLA-aware OS governors that seek Pareto optimality to achieve power-efficient performance. © 2017 ACM.",DVFS; Performance-per-watt; Prefetching; RAPL; SLA,Governors; Green computing; Pareto principle; DVFS; Optimal computing; Pareto-optimality; Performance-per-watt; Power efficient; Prefetching; RAPL; Reconfigurable computer; Energy efficiency
Selecting heterogeneous cores for diversity,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008226665&doi=10.1145%2f3014165&partnerID=40&md5=ca4a47f04ed9be65c6aa06f596a4d899,"Mobile devices with heterogeneous processors are becomingmainstream.With a heterogeneous processor, the runtime scheduler can pick the best CPU core for a given task based on program characteristics, performance requirements, and power limitations. For a heterogeneous processor to be effective, it must contain a diverse set of cores to match a range of runtime requirements and program behaviors. Selecting a diverse set of cores is, however, a non-trivial problem. Power and performance are dependent on both program features and the microarchitectural features of cores, and a selection of cores must satisfy the competing demands of different types of programs. We present a method of core selection that chooses cores at a range of power-performance points. Our algorithm is based on the observation that it is not necessary for a core to consistently have high performance or low power; one type of core can fulfill different roles for different types of programs. Given a power budget, cores selected with our method provide an average speedup of 6% on EEMBC mobile benchmarks and a 24% speedup on SPEC 2006 integer benchmarks over the state-of-the-art core selection method. © 2016 Copyright is held by the owner/author(s).",Core selection; Design space exploration; Diversity; Flexibility; Heterogeneous; Power-aware; Single-ISA,Budget control; Power management; Core selection; Design space exploration; Diversity; Flexibility; Heterogeneous; Power-aware; Single-ISA; Program processors
Fine-grain power breakdown of modern out-of-order cores and its implications on skylake-based systems,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008181935&doi=10.1145%2f3018112&partnerID=40&md5=f0b24fb38b5845cc9cde77567e1d13ae,"A detailed analysis of power consumption at low system levels becomes important as a means for reducing the overall power consumption of a system and its thermal hot spots. This work presents a new power estimation method that allows understanding the power breakdown of an application when running on modern processor architecture such as the newly released Intel Skylake processor. This work also provides a detailed power and performance characterization report for the SPEC CPU2006 benchmarks, analysis of the data using side-by-side power and performance breakdowns, as well as few interesting case studies. © 2016 ACM.",Energy; Performance-counters; Power,Electric power utilization; Energy; Modern processors; Performance characterization; Performance counters; Power; Power breakdowns; Power estimation method; Thermal hot spots; Benchmarking
Some mathematical facts about optimal cache replacement,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008227539&doi=10.1145%2f3017992&partnerID=40&md5=3a19be9f05e50d81af0d71d14757a722,"This article exposes and proves some mathematical facts about optimal cache replacement that were previously unknown or not proved rigorously. An explicit formula is obtained, giving OPT hits and misses as a function of past references. Several mathematical facts are derived from this formula, including a proof that OPT miss curves are always convex, and a new algorithm called OPT tokens, for reasoning about optimal replacement. © 2016 ACM.",Cache; Optimal replacement,Hardware; Software engineering; Cache; Cache replacement; Explicit formula; Mathematical facts; nocv1; Optimal replacement; Cache memory
Pot: Deterministic transactional execution,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008157509&doi=10.1145%2f3017993&partnerID=40&md5=3e1199d9b67ee4b8e32311e18ab79b73,"This article presents Pot, a system that leverages the concept of preordered transactions to achieve deterministic multithreaded execution of programs that use Transactional Memory. Preordered transactions eliminate the root cause of nondeterminism in transactional execution: they provide the illusion of executing in a deterministic serial order, unlike traditional transactions that appear to execute in a nondeterministic order that can change from execution to execution. Pot uses a new concurrency control protocol that exploits the serialization order to distinguish between fast and speculative transaction execution modes in order to mitigate the overhead of imposing a deterministic order. We build two Pot prototypes: one using STM and another using off-the-shelf HTM. To the best of our knowledge, Pot enables deterministic execution of programs using off-the-shelf HTM for the first time. An experimental evaluation shows that Pot achieves deterministic execution of TM programs with low overhead, sometimes even outperforming nondeterministic executions, and clearly outperforming the state of the art. © 2016 ACM.",Concurrency control; Determinism; Deterministic multithreading; Hardware transactional memory; Software transactional memory,Java programming language; Storage allocation (computer); Concurrency control protocols; Determinism; Deterministic execution; Experimental evaluation; Hardware transactional memory; Multi-threading; Multithreaded executions; Software transactional memory; Concurrency control
Designing a tunable nested data-parallel programming system,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008925673&doi=10.1145%2f3012011&partnerID=40&md5=a62862837f26bfd2d4e12dab29f7e0ad,"This article describes Surge, a nested data-parallel programming system designed to simplify the porting and tuning of parallel applications to multiple target architectures. Surge decouples high-level specification of computations, expressed using a C++ programming interface, from low-level implementation details using two first-class constructs: schedules and policies. Schedules describe the valid ways in which data-parallel operators may be implemented, while policies encapsulate a set of parameters that govern platform-specific code generation. These two mechanisms are used to implement a code generation system that analyzes computations and automatically generates a search space of valid platform-specific implementations. An input and architecture-adaptive autotuning system then explores this search space to find optimized implementations. We express in Surge five real-world benchmarks from domains such as machine learning and sparse linear algebra and from the high-level specifications, Surge automatically generates CPU and GPU implementations that perform on par with or better than manually optimized versions. © 2016 ACM.",Autotuning; Nested data parallelism; Performance portability,C++ (programming language); Computer architecture; Learning systems; Linear algebra; Parallel programming; Specifications; Architecture-adaptive; Autotuning; GPU implementation; High level specification; Nested data parallelisms; Optimized implementation; Parallel application; Performance portability; Computer systems programming
Energy-proportional photonic interconnects,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008627189&doi=10.1145%2f3018110&partnerID=40&md5=ada7360e7740f4e19696ed390d459a85,"Photonic interconnects have emerged as the prime candidate technology for efficient networks on chip at future process nodes. However, the high optical loss of many nanophotonic components coupled with the low efficiency of current laser sources results in exceedingly high total power requirements for the laser. As optical interconnects stay on even during periods of system inactivity, most of this power is wasted, which has prompted research on laser gating. Unfortunately, prior work has been complicated by the long laser turn-on delays and has failed to deliver the full savings. In this article, we propose ProLaser, a laser control mechanism that monitors the requests sent on the interconnect, the cache, and the coherence directory to detect highly correlated events and turn on proactively the lasers of a photonic interconnect.While ProLaser requires fast lasers with a turn-on delay of a few nanoseconds, a technology that is still experimental, several types of such lasers that are suitable for power gating have already been manufactured over the last decade. Overall, ProLaser saves 42% to 85% of the laser power, outperforms the current state of the art by 2× on average, and closely tracks (within 2%-6%) a perfect prediction scheme with full knowledge of future interconnect requests. Moreover, the power savings of ProLaser allow the cores to exploit a higher-power budget and run faster, achieving speedups of 1.5 to 1.7× (1.6× on average). © 2016 ACM 1544-3566/2016/12-ART54 $15.00.",Energy efficiency; Energy proportionality; Laser control; Nanophotonic interconnects; Power efficiency,Budget control; Energy efficiency; Nanophotonics; Network-on-chip; Energy proportionalities; Highly-correlated; Laser control; Networks on chips; Photonic interconnects; Power efficiency; Prediction schemes; State of the art; Integrated circuit interconnects
Static and dynamic frequency scaling on multicore CPUs,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008932279&doi=10.1145%2f3011017&partnerID=40&md5=7586ba39ee2098b89fd09c8c0d6ef4a9,"Dynamic Voltage and Frequency Scaling (DVFS) typically adapts CPU power consumption by modifying a processor's operating frequency (and the associated voltage). Typical DVFS approaches include using default strategies such as running at the lowest or the highest frequency or reacting to the CPU's runtime load to reduce or increase frequency based on the CPU usage. In this article, we argue that a compile-time approach to CPU frequency selection is achievable for affine program regions and can significantly outperform runtime-based approaches. We first propose a lightweight runtime approach that can exploit the properties of the power profile specific to a processor, outperforming classical Linux governors such as powersave or on-demand for computational kernels. We then demonstrate that, for affine kernels in the application, a purely compile-time approach to CPU frequency and core count selection is achievable, providing significant additional benefits over the runtime approach. Our framework relies on a one-time profiling of the target CPU, along with a compile-time categorization of loop-based code segments in the application. These are combined to determine at compile-time the frequency and the number of cores to use to execute each affine region to optimize energy or energy-delay product. Extensive evaluation on 60 benchmarks and 5 multi-core CPUs show that our approach systematically outperforms the powersave Linux governor while also improving overall performance. © 2016 ACM.",Affine programs; CPU energy; Static analysis; Voltage and frequency scaling,Benchmarking; Computer operating systems; Governors; Linux; Program processors; Static analysis; Voltage scaling; Affine program; Computational kernels; Dynamic voltage and frequency scaling; Energy delay product; Frequency selection; Frequency-scaling; Operating frequency; Runtime approach; Dynamic frequency scaling
Aggregate flow-based performance fairness in CMPs,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008867805&doi=10.1145%2f3014429&partnerID=40&md5=d3cc77733259e422905e48f01e2844ad,"In CMPs, multiple co-executing applications create mutual interference when sharing the underlying network-on-chip architecture. Such interference causes different performance slowdowns to different applications. To mitigate the unfairness problem, we treat traffic initiated from the same thread as an aggregate flow such that causal request/reply packet sequences can be allocated to resources consistently and fairly according to online profiled traffic injection rates. Our solution comprises three coherent mechanisms from rate profiling, rate inheritance, and rate-proportional channel scheduling to facilitate and realize unbiased workload-adaptive resource allocation. Full-system evaluations in GEM5 demonstrate that, compared to classic packet-centric and latest application-prioritization approaches, our approach significantly improves weighted speed-up for all multi-application mixtures and achieves nearly ideal performance fairness. © 2016 ACM.",Computer architecture; Performance fairness; Quality of service,Aggregates; Network architecture; Network-on-chip; Quality of service; Adaptive resource allocations; Channel scheduling; Ideal performance; Mutual interference; Performance fairness; System evaluation; Underlying networks; Unfairness problem; Computer architecture
User-assisted store recycling for dynamic task graph schedulers,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008953525&doi=10.1145%2f3018111&partnerID=40&md5=736df18bc3bfe8f85cb029bb7c7cf892,"The emergence of the multi-core era has led to increased interest in designing effective yet practical parallel programming models. Models based on task graphs that operate on single-assignment data are attractive in several ways. Notably, they can support dynamic applications and precisely represent the available concurrency. However, for efficient execution, they also require nuanced algorithms for scheduling and memory management. In this article, we consider memory-efficient dynamic scheduling of task graphs. Specifically, we present a novel approach for dynamically recycling the memory locations assigned to data items as they are produced by tasks. We develop algorithms to identify memory-efficient store recycling functions by systematically evaluating the validity of a set of user-provided or automatically generated alternatives. Because recycling functions can be input data-dependent, we have also developed support for continued correct execution of a task graph in the presence of a potentially incorrect store recycling function. Experimental evaluation demonstrates that this approach to automatic store recycling incurs little to no overheads, achieves memory usage comparable to the best manually derived solutions, often produces recycling functions valid across problem sizes and input parameters, and efficiently recovers from an incorrect choice of store recycling functions. © 2016 ACM.",Dynamic scheduling; Memory management; Task parallelism,Function evaluation; Multicore programming; Parallel programming; Recycling; Scheduling algorithms; Automatically generated; Dynamic applications; Dynamic scheduling; Experimental evaluation; Memory locations; Memory management; Parallel programming model; Task parallelism; Scheduling
A software cache partitioning system for hash-based caches,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008146706&doi=10.1145%2f3018113&partnerID=40&md5=cf97df8380fdea50594bb68ef7969100,"Contention on the shared Last-Level Cache (LLC) can have a fundamental negative impact on the performance of applications executed on modern multicores. An interesting software approach to address LLC contention issues is based on page coloring, which is a software technique that attempts to achieve performance isolation by partitioning a shared cache through careful memory management. The key assumption of traditional page coloring is that the cache is physically addressed. However, recent multicore architectures (e.g., Intel Sandy Bridge and later) switched from a physical addressing scheme to a more complex scheme that involves a hash function. Traditional page coloring is ineffective on these recent architectures. In this article, we extend page coloring to work on these recent architectures by proposing a mechanism able to handle their hash-based LLC addressing scheme. Just as for traditional page coloring, the goal of this new mechanism is to deliver performance isolation by avoiding contention on the LLC, thus enabling predictable performance. We implement this mechanism in the Linux kernel, and evaluate it using several benchmarks from the SPEC CPU2006 and PARSEC 3.0 suites. Our results show that our solution is able to deliver performance isolation to concurrently running applications by enforcing partitioning of a Sandy Bridge LLC, which traditional page coloring techniques are not able to handle. © 2016 ACM.",Hash-based cache; Last-level cache; Linux; Operating system; Page coloring,Coloring; Computer architecture; Computer operating systems; Hash functions; Linux; Software architecture; Hash-based cache; Last-level caches; Lastlevel caches (LLC); Multicore architectures; Operating system; Page colorings; Running applications; Software techniques; Physical addresses
MaxPB: Accelerating PCM write by maximizing the power budget utilization,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006988524&doi=10.1145%2f3012007&partnerID=40&md5=dfbbe1c9d2ec4061b0d6b11568cf3ed7,"Phase Change Memory (PCM) is one of the promising memory technologies but suffers from some critical problems such as poor write performance and high write energy consumption. Due to the high write energy consumption and limited power supply, the size of concurrent bit-write is restricted inside one PCM chip. Typically, the size of concurrent bit-write is much less than the cache line size and it is normal that many serially executed write units are consumed to write down the data block to PCM when using it as the main memory. Existing state-of-the-art PCM write schemes, such as FNW (Flip-N-Write) and two-stage-write, address the problem of poor performance by improving the write parallelism under the power constraints. The parallelism is obtained via reducing the data amount and leveraging power as well as time asymmetries, respectively. However, due to the extremely pessimistic assumptions of current utilization (FNW) and optimistic assumptions of asymmetries (two-stage-write), these schemes fail to maximize the power supply utilization and hence improve the write parallelism. In this article, we propose a novel PCM write scheme, called MaxPB (Maximize the Power Budget utilization) to maximize the power budget utilization with minimum changes about the circuits design. MaxPB is a ""think before acting"" method. The main idea of MaxPB is to monitor the actual power needs of all data units first and then effectively package them into the least number of write units under the power constraints. Experimental results show the efficiency and performance improvements on MaxPB. For example, fourcore PARSEC and SPEC experimental results show that MaxPB gets 32.0% and 20.3% more read latency reduction, 26.5% and 16.1% more write latency reduction, 24.3% and 15.6% more running time decrease, 1.32× and 0.92× more speedup, as well as 30.6% and 18.4% more energy consumption reduction on average compared with the state-of-the-art FNW and two-stage-write write schemes, respectively. © 2016 ACM.",,Budget control; Cache memory; Energy utilization; Phase change memory; Critical problems; Efficiency and performance; Latency reduction; Limited power supply; Memory technology; Phase change memory (pcm); Power constraints; State of the art; Data reduction
Impact of intrinsic profiling limitations on effectiveness of adaptive optimizations,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007009554&doi=10.1145%2f3008661&partnerID=40&md5=df71d038e4ffae825bc12cde0039c55f,"Many performance optimizations rely on or are enhanced by runtime profile information. However, both offline and online profiling techniques suffer from intrinsic and practical limitations that affect the quality of delivered profile data. The quality of profile data is its ability to accurately predict (relevant aspects of) future program behavior. While these limitations are known, their impact on the effectiveness of profile-guided optimizations, compared to the ideal performance, is not as well understood. We define ideal performance for adaptive optimizations as that achieved with a precise profile of future program behavior. In this work, we study and quantify the performance impact of fundamental profiling limitations by comparing the effectiveness of typical adaptive optimizations when using the best profiles generated by offline and online schemes against a baseline where the adaptive optimization is given access to profile information about the future execution of the program. We model and compare the behavior of three adaptive JVM optimizations - heap memory management using object usage profiles, code cache management using method usage profiles, and selective just-in-time compilation using method hotness profiles - for the Java DaCapo benchmarks. Our results provide insight into the advantages and drawbacks of current profiling strategies and shed light on directions for future profiling research. © 2016 ACM.",,Software engineering; Adaptive optimization; Cache management; Current profiling; Ideal performance; Just-in-time compilation; Memory management; Performance impact; Performance optimizations; Hardware
A reconfiguration algorithm for power-aware parallel applications,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007013341&doi=10.1145%2f3004054&partnerID=40&md5=a00d811c638ac9bd9d439f90822ac2fa,"In current computing systems, many applications require guarantees on their maximum power consumption to not exceed the available power budget. On the other hand, for some applications, it could be possible to decrease their performance, yet maintain an acceptable level, in order to reduce their power consumption. To provide such guarantees, a possible solution consists in changing the number of cores assigned to the application, their clock frequency, and the placement of application threads over the cores. However, power consumption and performance have different trends depending on the application considered and on its input. Finding a configuration of resources satisfying user requirements is, in the general case, a challenging task. In this article, we propose NORNIR, an algorithm to automatically derive, without relying on historical data about previous executions, performance and power consumption models of an application in different configurations. By using these models, we are able to select a close-to-optimal configuration for the given user requirement, either performance or power consumption. The configuration of the application will be changed on-the-fly throughout the execution to adapt to workload fluctuations, external interferences, and/or application's phase changes. We validate the algorithm by simulating it over the applications of the PARSEC benchmark suit. Then, we implement our algorithm and we analyse its accuracy and overhead over some of these applications on a real execution environment. Eventually, we compare the quality of our proposal with that of the optimal algorithm and of some state-of-the-art solutions. © 2016 ACM.",,Budget control; Electric power utilization; Green computing; Power management; Application threads; Execution environments; External interference; Optimal algorithm; Parallel application; Power consumption model; Reconfiguration algorithm; User requirements; Benchmarking
Cooperative caching for GPUs,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006966404&doi=10.1145%2f3001589&partnerID=40&md5=c726079e89b47d86efa282838da6dc99,"The rise of general-purpose computing on GPUs has influenced architectural innovation on them. The introduction of an on-chip cache hierarchy is one such innovation. High L1 miss rates on GPUs, however, indicate inefficient cache usage due to myriad factors, such as cache thrashing and extensive multithreading. Such high L1 miss rates in turn place high demands on the shared L2 bandwidth. Extensive congestion in the L2 access path therefore results in high memory access latencies. In memory-intensive applications, these latencies get exposed due to a lack of active compute threads to mask such high latencies. In this article, we aim to reduce the pressure on the shared L2 bandwidth, thereby reducing the memory access latencies that lie in the critical path. We identify significant replication of data among private L1 caches, presenting an opportunity to reuse data among L1s. We further show how this reuse can be exploited via an L1 Cooperative Caching Network (CCN), thereby reducing the bandwidth demand on L2. In the proposed architecture, we connect the L1 caches with a lightweight ring network to facilitate intercore communication of shared data. We show that this technique reduces traffic to the L2 cache by an average of 29%, freeing up the bandwidth for other accesses. We also show that the CCN reduces the average memory latency by 24%, thereby reducing core stall cycles by 26% on average. This translates into an overall performance improvement of 14.7% on average (and up to 49%) for applications that exhibit reuse across L1 caches. In doing so, the CCN incurs a nominal area and energy overhead of 1.3% and 2.5%, respectively. Notably, the performance improvement with our proposed CCN compares favorably to the performance improvement achieved by simply doubling the number of L2 banks by up to 34%. © 2016 ACM.",,Bandwidth; Cache memory; Multitasking; Program processors; Architectural innovation; Cooperative caching; Energy overheads; General-purpose computing; Inter-core communications; Memory access latency; Performance improvements; Proposed architectures; Memory architecture
Accuracy bugs: A new class of concurrency bugs to exploit algorithmic noise tolerance,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007015375&doi=10.1145%2f3017991&partnerID=40&md5=0579b837ff401c2d091330df3b1463a6,"Parallel programming introduces notoriously difficult bugs, usually referred to as concurrency bugs. This article investigates the potential for deviating from the conventional wisdom of writing concurrency bug-free, parallel programs. It explores the benefit of accepting buggy but approximately correct parallel programs by leveraging the inherent tolerance of emerging parallel applications to inaccuracy in computations. Under algorithmic noise tolerance, a new class of concurrency bugs, accuracy bugs, degrade the accuracy of computation (often at acceptable levels) rather than causing catastrophic termination. This study demonstrates how embracing accuracy bugs affects the application output quality and performance and analyzes the impact on execution semantics. © 2016 ACM.",,Application programs; Parallel programming; Semantics; Algorithmic noise tolerances; Concurrency bugs; Execution semantics; Output quality; Parallel application; Parallel program; Program debugging
UMH: A hardware-based unified memory hierarchy for systems with multiple discrete GPUs,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007002393&doi=10.1145%2f2996190&partnerID=40&md5=f8d2881ef7939386c863e924f56f755d,"In this article, we describe how to ease memory management between a Central Processing Unit (CPU) and one or multiple discrete Graphic Processing Units (GPUs) by architecting a novel hardware-based Unified Memory Hierarchy (UMH). Adopting UMH, a GPU accesses the CPU memory only if it does not find its required data in the directories associated with its high-bandwidth memory, or the NMOESI coherency protocol limits the access to that data. Using UMH with NMOESI improves performance of a CPU-multiGPU system by at least 1.92× in comparison to alternative software-based approaches. It also allows the CPU to access GPUs modified data by at least 13× faster. © 2016 ACM.",,Graphics processing unit; Hardware; Image coding; Memory architecture; Program processors; Storage allocation (computer); CPU memory; Graphic processing units (GPUs); High bandwidth; Memory hierarchy; Memory management; Multi-GPU Systems; Novel hardware; Memory management units
Extending the WCET problem to optimize for runtime-reconfigurable processors,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006968694&doi=10.1145%2f3014059&partnerID=40&md5=86379839e0779ec26bc511202a87b8e6,"The correctness of a real-time system does not depend on the correctness of its calculations alone but also on the non-functional requirement of adhering to deadlines. Guaranteeing these deadlines by static timing analysis, however, is practically infeasible for current microarchitectures with out-of-order scheduling pipelines, several hardware threads, and multiple (shared) cache layers. Novel timing-analyzable features are required to sustain the strongly increasing demand for processing power in real-time systems. Recent advances in timing analysis have shown that runtime-reconfigurable instruction set processors are one way to escape the scarcity of analyzable processing power while preserving the flexibility of the system. When moving calculations from software to hardware by means of reconfigurable custom instructions (CIs) - additional to a considerable speedup - the overestimation of a task's worst-case execution time (WCET) can be reduced. CIs typically implement functionality that corresponds to several hundred instructions on the central processing unit (CPU) pipeline. While analyzing instructions for worst-case latency may introduce pessimism, the latency of CIs - executed on the reconfigurable fabric - is precisely known. In this work, we introduce the problem of selecting reconfigurable CIs to optimize the WCET of an application. We model this problem as an extension to state-of-the-art integer linear programming (ILP)-based program path analysis. This way, we enable optimization based on accurate WCET estimates with integration of information about global program flow, for example, infeasible paths. We present an optimal solution with effective techniques to prune the search space and a greedy heuristic that performs a maximum number of steps linear in the number of partitions of reconfigurable area available. Finally, we show the effectiveness of optimizing the WCET on a reconfigurable processor by evaluating a complex multimedia application with multiple reconfigurable CIs for several hardware parameters. © 2016 ACM.",,Computer hardware; Hardware; Integer programming; Interactive computer systems; Pipelines; Program processors; Real time systems; Reconfigurable hardware; Regression analysis; Integer Linear Programming; Multimedia applications; Non-functional requirements; Out-of-order scheduling; Reconfigurable processors; Run-time reconfigurable; Static timing analysis; Worst-case execution time; Pipeline processing systems
Memory access scheduling based on dynamic multilevel priority in shared DRAM systems,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006998659&doi=10.1145%2f3007647&partnerID=40&md5=b0283439c2101662f5a9ed1f24461eaf,"Interapplication interference at shared main memory severely degrades performance and increasing DRAM frequency calls for simple memory schedulers. Previous memory schedulers employaper-application ranking scheme for high system performance or a per-group ranking scheme for low hardware cost, but few provide a balance. We propose DMPS, a memory scheduler based on dynamic multilevel priority. First, DMPS uses ""memory occupancy"" to measure interference quantitatively. Second, DMPS groups applications, favors latency-sensitive groups, and dynamically prioritizes applications by employing a per-level ranking scheme. The simulation results show that DMPS has 7.2% better system performance and 22% better fairness over FRFCFS at low hardware complexity and cost. © 2016 ACM.",,Hardware; Scheduling; Hardware complexity; Low hardware costs; Main memory; Memory access scheduling; ON dynamics; Dynamic random access storage
Accelerating intercommunication in highly parallel systems,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007017630&doi=10.1145%2f3005717&partnerID=40&md5=796d4fe1c0c764cd49d3b11ada736de9,"Every HPC system consists of numerous processing nodes interconnect using a number of different interprocess communication protocols such as Messaging Passing Interface (MPI) and Global Arrays (GA). Traditionally, research has focused on optimizing these protocols and identifying the most suitable ones for each system and/or application. Recently, there has been a proposal to unify the primitive operations of the different inter-processor communication protocols through the Portals library. Portals offer a set of low-level communication routines which can be composed in order to implement the functionality of different intercommunication protocols. However, Portals modularity comes at a performance cost, since it adds one more layer in the actual protocol implementation. This work aims at closing the performance gap between a generic and reusable intercommunication layer, such as Portals, and the several monolithic and highly optimized intercommunication protocols. This is achieved through the development of a novel hardware offload engine efficiently implementing the basic Portals' modules. Our innovative system is up to two2 orders of magnitude faster than the conventional software implementation of Portals' while the speedup achieved over the conventional monolithic software implementations of MPI and GAs is more than an order of magnitude. The power consumption of our hardware system is less than 1/100th of what a low-power CPU consumes when executing the Portal's software while its silicon cost is less than 1/10th of that of a very simple RISC CPU. Moreover, our design process is also innovative since we have first modeled the hardware within an untimed virtual prototype which allowed for rapid design space exploration; then we applied a novel methodology to transform the untimed description into an efficient timed hardware description, which was then transformed into a hardware netlist through a High-Level Synthesis (HLS) tool. © 2016 ACM.",,Computer software reusability; Distributed computer systems; High level synthesis; Hardware descriptions; Innovative systems; Inter processor communication; Interprocess communication; Orders of magnitude; Primitive operations; Protocol implementation; Software implementation; Hardware
Evaluation of histogram of oriented gradients soft errors criticality for automotive applications,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997522618&doi=10.1145%2f2998573&partnerID=40&md5=bc6a057b6a9edbaf9786b559786b11c5,"Pedestrian detection reliability is a key problem for autonomous or aided driving, and methods that use Histogram of Oriented Gradients (HOG) are very popular. Embedded Graphics Processing Units (GPUs) are exploited to run HOG in a very efficient manner. Unfortunately, GPUs architecture has been shown to be particularly vulnerable to radiation-induced failures. This article presents an experimental evaluation and analytical study of HOG reliability. We aim at quantifying and qualifying the radiation-induced errors on pedestrian detection applications executed in embedded GPUs. We analyze experimental results obtained executing HOG on embedded GPUs from two different vendors, exposed for about 100 hours to a controlled neutron beam at Los Alamos National Laboratory. We consider the number and position of detected objects as well as precision and recall to discriminate critical erroneous computations. The reported analysis shows that, while being intrinsically resilient (65% to 85% of output errors only slightly impact detection), HOG experienced some particularly critical errors that could result in undetected pedestrians or unnecessary vehicle stops. Additionally, we perform a fault-injection campaign to identify HOG critical procedures. We observe that Resize and Normalize are the most sensitive and critical phases, as about 20% of injections generate an output error that significantly impacts HOG detection. With our insights, we are able to find those limited portions of HOG that, if hardened, are more likely to increase reliability without introducing unnecessary overhead. © 2016 ACM.",HOG; Pedestrian detection,Computer graphics; Crashworthiness; Graphic methods; Object detection; Program processors; Radiation hardening; Reliability; Automotive applications; Experimental evaluation; Graphics processing units; Histogram of oriented gradients; Histogram of oriented gradients (HOG); Los Alamos National Laboratory; Pedestrian detection; Precision and recall; Errors
LDAC: Locality-aware data access control for large-scale multicore cache hierarchies,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997207540&doi=10.1145%2f2983632&partnerID=40&md5=6fbb56034a8a9a31b4700244f3b24a8c,"The trend of increasing the number of cores to achieve higher performance has challenged efficient management of on-chip data. Moreover, many emerging applications process massive amounts of data with varying degrees of locality. Therefore, exploiting locality to improve on-chip traffic and resource utilization is of fundamental importance. Conventional multicore cache management schemes either manage the private cache (L1) or the Last-Level Cache (LLC), while ignoring the other. We propose a holistic locality-aware cache hierarchy management protocol for large-scale multicores. The proposed scheme improves on-chip data access latency and energy consumption by intelligently bypassing cache line replication in the L1 caches, and/or intelligently replicating cache lines in the LLC. The approach relies on low overhead yet highly accurate in-hardware runtime classification of data locality at both L1 cache and the LLC. The decision to bypass L1 and/or replicate in LLC is then based on the measured reuse at the fine granularity of cache lines. The locality tracking mechanism is decoupled from the sharer tracking structures that cause scalability concerns in traditional cache coherence protocols. Moreover, the complexity of the protocol is low since no additional coherence states are created. However, the proposed classifier incurs a 5.6KB per-core storage overhead. On a set of parallel benchmarks, the locality-aware protocol reduces average energy consumption by 26% and completion time by 16%, when compared to the state-of-the-art Reactive-NUCA multicore cache management scheme. © 2016 ACM.",Cache; Locality; Multicore,Access control; Cache memory; Energy utilization; Multiprocessing systems; Cache; Cache coherence protocols; Cache management schemes; Classification of data; Emerging applications; Lastlevel caches (LLC); Locality; Multi core; Digital storage
Concurrent JavaScript parsing for faster loading of web apps,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997199405&doi=10.1145%2f3004281&partnerID=40&md5=81ca7de417a4180eed09134658ce3691,"JavaScript is a dynamic language mainly used as a client-side web script. Nowadays, web is evolving into an application platform with its web apps, and JavaScript increasingly undertakes complex computations and interactive user interfaces, requiring a high-performance JavaScript engine. There have been many optimizations for efficient JavaScript engines, but one component that has not been optimized much is JavaScript parsing. A JavaScript function needs to be parsed before being executed, and the parsing overhead takes a substantial portion of JavaScript execution time for web apps, especially during app loading. This article proposes concurrent parsing of JavaScript, which performs the parsing of JavaScript functions in advance on different threads, while the main thread is executing the parsed JavaScript functions. This can hide the parsing overhead from the main execution thread, reducing the JavaScript execution time, thus reducing the overall app loading time. More specifically, we separated JavaScript parsing and made it run on different threads without violating the execution semantics of JavaScript. We also designed an efficient multi-threaded parsing architecture, which reduces the synchronization overhead and schedules the parsing requests appropriately. Finally, we explored two methods of choosing the target functions for concurrent parsing: one based on profiled information and the other based on speculative heuristics. We performed experiments on the WebKit browser with the JSC engine for real web apps. The result shows that the proposed concurrent parsing can improve the JavaScript performance during app loading by as much as 64% and by 39.7% on average. This improves the whole app loading performance tangibly, by as much as 32.7% and by 18.2%, on average. © 2016 ACM.",Concurrent parsing; JavaScript; Javascript engine; Parser; Web app; Web browser,Cost reduction; Engines; Heuristic methods; High level languages; Optimization; Semantics; User interfaces; Web browsers; Application platforms; Concurrent parsing; Execution semantics; Interactive user interfaces; Javascript; Loading performance; Parser; Web App; Loading
AIM: Energy-efficient aggregation inside the memory hierarchy,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994803187&doi=10.1145%2f2994149&partnerID=40&md5=ed386b263f3be6a996a2c8cf4c18a1db,"In this article, we propose Aggregation-in-Memory (AIM), a new processing-in-memory system designed for energy efficiency and near-Term adoption. In order to efficiently perform aggregation, we implement simple aggregation operations in main memory and develop a locality-Adaptive host architecture for inmemory aggregation, called cache-conscious aggregation. Through this, AIM executes aggregation at the most energy-efficient location among all levels of the memory hierarchy. Moreover, AIM minimally changes existing sequential programming models and provides fully automated compiler toolchain, thereby allowing unmodified legacy software to use AIM. Evaluations show that AIM greatly improves the energy efficiency of main memory and the system performance. © 2016 ACM.",Aggregation; Localityadaptive execution; Near-data processing; Processing-in-memory,Agglomeration; Cache memory; Computer architecture; Data handling; Memory architecture; Program compilers; Aggregation operation; Cache-conscious; Energy efficient; Localityadaptive execution; Memory hierarchy; Processing in memory; Processing-In-Memory systems; Sequential programming; Energy efficiency
Hardware-Accelerated cross-Architecture full-system virtualization,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994904832&doi=10.1145%2f2996798&partnerID=40&md5=326bdd679adb64d20bb6b9bdb61c82a0,"Hardware virtualization solutions provide users with benefits ranging from application isolation through server consolidation to improved disaster recovery and faster server provisioning. While hardware assistance for virtualization is supported by all major processor architectures, including Intel, ARM, PowerPC, and MIPS, these extensions are targeted at virtualization of the same architecture, for example, an x86 guest on an x86 host system. Existing techniques for cross-Architecture virtualization, for example, an ARM guest on an x86 host, still incur a substantial overhead for CPU, memory, and I/O virtualization due to the necessity for software emulation of these mismatched system components. In this article, we present a new hardwareaccelerated hypervisor called CAPTIVE, employing a range of novel techniques that exploit existing hardware virtualization extensions for improving the performance of full-system cross-platform virtualization. We illustrate how (1) guest memory management unit (MMU) events and operations can be mapped onto host memory virtualization extensions, eliminating the need for costly softwareMMUemulation, (2) a block-based dynamic binary translation engine inside the virtual machine can improve CPU virtualization performance, (3) memory-mapped guest I/O can be efficiently translated to fast I/O specific calls to emulated devices, and (4) the cost for asynchronous guest interrupts can be reduced. For an ARM-based Linux guest system running on an x86 host with Intel VT support, we demonstrate application performance levels, based on SPEC CPU2006 benchmarks, of up to 5.88× over state-of-The-Art QEMU and 2.5× on average, achieving a guest dynamic instruction throughput of up to 1280 MIPS (million instructions per second) and 915.52 MIPS, on average. © 2016 ACM.",Virtualization,ARM processors; Benchmarking; Computer operating systems; Hardware; Memory management units; Physical addresses; Virtual reality; Application performance; Dynamic binary translation; Hardware virtualization; Memory virtualization; Million instructions per seconds; Processor architectures; Server consolidation; System virtualization; Virtualization
Synergistic analysis of evolving graphs,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994807432&doi=10.1145%2f2992784&partnerID=40&md5=1ea1502e2848fde33c0c0969b419290e,"Evolving graph processing involves repeating analyses, which are often iterative, over multiple snapshots of the graph corresponding to different points in time. Since the snapshots of an evolving graph share a great number of vertices and edges, traditional approaches that process these snapshots one at a time without exploiting this overlap contain much wasted effort on both data loading and computation, making them extremely inefficient. In this article, we identify major sources of inefficiencies and present two optimization techniques to address them. First, we propose a technique for amortizing the fetch cost by merging fetching of values for different snapshots of the same vertex. Second, we propose a technique for amortizing the processing cost by feeding values computed by earlier snapshots into later snapshots. We have implemented these optimizations in two distributed graph processing systems, namely, GraphLab and ASPIRE. Our experiments with multiple real evolving graphs and algorithms show that, on average fetch amortization speeds up execution of GraphLab and ASPIRE by 5.2× and 4.1×, respectively. Amortizing the processing cost yields additional average speedups of 2× and 7.9×, respectively. © 2016 ACM.",Graph processing; Message aggregation; Temporal graphs,Costs; Depreciation; Evolving graphs; Graph processing; Message aggregation; Optimization techniques; Processing costs; Temporal graphs; Traditional approaches; Iterative methods
A cross-platform SpMV framework on many-core architectures,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997096244&doi=10.1145%2f2994148&partnerID=40&md5=0954683cafc35a13ca1c5a6de87860d1,"Sparse Matrix-Vector multiplication (SpMV) is a key operation in engineering and scientific computing. Although the previous work has shown impressive progress in optimizing SpMV on many-core architectures, load imbalance and high memory bandwidth remain the critical performance bottlenecks. We present our novel solutions to these problems, for both GPUs and Intel MIC many-core architectures. First, we devise a new SpMV format, called Blocked Compressed Common Coordinate (BCCOO). BCCOO extends the blocked Common Coordinate (COO) by using bit flags to store the row indices to alleviate the bandwidth problem. We further improve this format by partitioning the matrix into vertical slices for better data locality. Then, to address the load imbalance problem, we propose a highly efficient matrix-based segmented sum/scan algorithm for SpMV, which eliminates global synchronization. At last, we introduce an autotuning framework to choose optimization parameters. Experimental results show that our proposed framework has a significant advantage over the existing SpMV libraries. In single precision, our proposed scheme outperforms clSpMV COCKTAIL format by 255% on average on AMD FirePro W8000, and outperforms CUSPARSE V7.0 by 73.7% on average and outperforms CSR5 by 53.6% on average on GeForce Titan X; in double precision, our proposed scheme outperforms CUSPARSE V7.0 by 34.0% on average and outperforms CSR5 by 16.2% on average on Tesla K20, and has equivalent performance compared with CSR5 on Intel MIC. ©2016 ACM.",BCCOO; CUDA; GPU; Intel MIC; OpenCL; Parallel algorithms; Segmented scan; SpMV,Bandwidth; Matrix algebra; Microwave integrated circuits; Optimization; Parallel algorithms; Program processors; BCCOO; CUDA; Intel MIC; OpenCL; Segmented scan; SpMV; Computer architecture
RATT-ECC: Rate adaptive two-tiered error correction codes for reliable 3D die-stacked memory,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989318882&doi=10.1145%2f2957758&partnerID=40&md5=dfbc39c7226d395d542a1d89a933a9df,"This article proposes a rate-adaptive, two-tiered error-correction scheme (RATT-ECC) that provides strong reliability (1010x reduction in raw FIT rate) for an HBM-like 3D DRAM system. The tier-1 code is a strong symbol-based code that can correct errors due to small granularity faults and detect errors caused by large granularity faults; the tier-2 code is an XOR-based code that corrects errors detected by the tier-1 code. The rate-adaptive feature of RATT-ECC enables permanent bank failures to be handled through sparing. It can also be used to significantly reduce the refresh power consumption without decreasing reliability and timing performance. © 2016 ACM.",3D memory; Error control coding; Performance; Reliability,Dynamic random access storage; Error correction; Error detection; Errors; Reliability; 3D memory; Adaptive features; Error control coding; Error correction codes; Error-correction schemes; Performance; Stacked memory; Timing performance; Codes (symbols)
An accurate cross-layer approach for online architectural vulnerability estimation,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989332848&doi=10.1145%2f2975588&partnerID=40&md5=9124b050c2404ddc23bd5151952b473f,"Processor soft-error rates are projected to increase as feature sizes scale down, necessitating the adoption of reliability-enhancing techniques, but power and performance overhead remain a concern of such techniques. Dynamic cross-layer techniques are a promising way to improve the cost-effectiveness of resilient systems. As a foundation for making such a system, we propose a cross-layer approach for estimating the architectural vulnerability of a processor core online that works by combining information from software, compiler, and microarchitectural layers at runtime. The hardware layer combines themetadata from software and compiler layers with microarchitectural measurements to estimate architectural vulnerability online. We describe our design and evaluate it in detail on a set of SPEC CPU 2006 applications. We find that our online AVF estimate is highly accurate with respect to a postmortem AVF analysis, with only 0.46% average absolute error. Also, our design incurs negligible performance impact for SPEC2006 applications and about 1.2% for a Monte Carlo application, requires approximately 1.4% area overhead, and costs about 3.3% more power on average. We compare our technique against two prior online AVF estimation techniques, one using a linear regression to estimate AVF and another based on PVF-HVF; our evaluation finds that our approach, on average, is more accurate. Our case study of a Monte Carlo simulation shows that our AVF estimate can adapt to the inherent resiliency of the algorithm. Finally, we demonstrate the effectiveness of our approach using a dynamic protection scheme that limits vulnerability to soft errors while reducing the energy consumption by an average of 4.8%, and with a target normalized SER of 10%, compared to enabling a simple parity+ECC protection at all times. © 2016 ACM.",Architectural vulnerability factor; Cross-layer reliability,Cost effectiveness; Energy utilization; Error correction; Integrated circuit design; Intelligent systems; Program compilers; Radiation hardening; Architectural vulnerability factor; Average absolute error; Cross layer; Cross-layer approach; Cross-layer techniques; Dynamic protections; Estimation techniques; Performance impact; Monte Carlo methods
Hardware-assisted thread and data mapping in hierarchical multicore architectures,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989347258&doi=10.1145%2f2975587&partnerID=40&md5=77e1f172e2f78b72c370d22745433e78,"The performance and energy efficiency of modern architectures depend on memory locality, which can be improved by thread and data mappings considering the memory access behavior of parallel applications. In this article, we propose intense pages mapping, a mechanism that analyzes the memory access behavior using information about the time the entry of each page resides in the translation lookaside buffer. It provides accurate information with a very low overhead. We present experimental results with simulation and real machines, with average performance improvements of 13.7% and energy savings of 4.4%, which come from reductions in cache misses and interconnection traffic. © 2016 ACM.",Cache memory; Communication; Data mapping; Data sharing; NUMA; Thread mapping,Buffer storage; Communication; Energy conservation; Energy efficiency; Mapping; Memory architecture; Software architecture; Data mappings; Data Sharing; Modern architectures; Multicore architectures; NUMA; Parallel application; Performance improvements; Translation lookaside buffer; Cache memory
Optimization models for three on-chip network problems,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989337985&doi=10.1145%2f2943781&partnerID=40&md5=02bd87a01fa331cc7957f8b78d1d13c4,"We model three on-chip network design problems-memory controller placement, resource allocation in heterogeneous on-chip networks, and their combination-as mathematical optimization problems. We model the first two problems as mixed integer linear programs. We model the third problem as a mixed integer nonlinear program, which we then linearize exactly. Sophisticated optimization algorithms enable solutions to be obtained much more efficiently. Detailed simulations using synthetic traffic and benchmark applications validate that our designs provide better performance than solutions proposed previously. Our work provides further evidence toward suitability of optimization models in searching/pruning architectural design space. © 2016 ACM.",On-chip network; Optimization models,Benchmarking; Embedded systems; Nonlinear programming; Benchmark applications; Mathematical optimization problems; Memory controller; Mixed integer linear program; Mixed integer nonlinear program; On-chip networks; Optimization algorithms; Optimization models; Integer programming
Yet another compressed cache: A low-cost yet effective compressed cache,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989311540&doi=10.1145%2f2976740&partnerID=40&md5=e244db0567d3cc935a43b0de843e8b07,"Cache memories play a critical role in bridging the latency, bandwidth, and energy gaps between cores and off-chip memory. However, caches frequently consume a significant fraction of a multicore chip's area and thus account for a significant fraction of its cost. Compression has the potential to improve the effective capacity of a cache, providing the performance and energy benefits of a larger cache while using less area. The design of a compressed cache must address two important issues: (i) a low-latency, low-overhead compression algorithm that can represent a fixed-size cache block using fewer bits and (ii) a cache organization that can efficiently store the resulting variable-size compressed blocks. This article focuses on the latter issue. Here, we propose Yet Another Compressed Cache (YACC), a new compressed cache design that targets improving effective cache capacity with a simple design. YACC uses super-blocks to reduce tag overheads while packing variable-size compressed blocks to reduce internal fragmentation. YACC achieves the benefits of two state-of-the art compressed caches-Decoupled Compressed Cache (DCC) [Sardashti and Wood 2013a, 2013b] and Skewed Compressed Cache (SCC) [Sardashti et al. 2014]-with a more practical and simpler design. YACC's cache layout is similar to conventional caches, with a largely unmodified tag array and unmodified data array. Compared to DCC and SCC, YACC requires neither the significant extra metadata (i.e., back pointers) needed by DCC to track blocks nor the complexity and overhead of skewed associativity (i.e., indexing ways differently) needed by SCC. An additional advantage over previous work is that YACC enables modern replacement mechanisms, such as RRIP. For our benchmark set, compared to a conventional uncompressed 8MB LLC, YACC improves performance by 8% on average and up to 26%, and reduces total energy by 6% on average and up to 20%. An 8MB YACC achieves approximately the same performance and energy improvements as a 16MB conventional cache at a much smaller silicon footprint, with only 1.6% greater area than an 8MB conventional cache. YACC performs comparably to DCC and SCC but is much simpler to implement. © 2016 ACM.",Cache design; Compression; Energy efficiency; Multicore systems; Performance,Benchmarking; Compaction; Energy efficiency; Cache design; Cache organization; Compression algorithms; Effective capacity; Energy benefits; Multi-core systems; Off-chip memory; Performance; Cache memory
Maximizing heterogeneous processor performance under power constraints,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989331450&doi=10.1145%2f2976739&partnerID=40&md5=a56900f0589addd0ade905695ac5ef85,"Heterogeneous processors (e.g., ARM's big.LITTLE) improve performance in power-constrained environments by executing applications on the 'little' low-power core and move them to the 'big' high-performance core when there is available power budget. The total time spent on the big core depends on the rate at which the application dissipates the available power budget. When applications with different big-core power consumption characteristics concurrently execute on a heterogeneous processor, it is best to give a larger share of the power budget to applications that can run longer on the big core, and a smaller share to applications that run for a very short duration on the big core. This article investigates mechanisms to manage the available power budget on power-constrained heterogeneous processors. We show that existing proposals that schedule applications onto a big core based on various performance metrics are not high performing, as these strategies do not optimize over an entire power period and are unaware of the applications' power/performance characteristics. We use linear programming to design the DPDP power management technique, which guarantees optimal performance on heterogeneous processors. We mathematically derive a metric (Delta Performance by Delta Power) that takes into account the power/performance characteristics of each running application and allows our power-management technique to decide how best to distribute the available power budget among the co-running applications at minimal overhead. Our evaluations with a 4-core heterogeneous processor consisting of big.LITTLE pairs show that DPDP improves performance by 16% on average and up to 40% compared to a strategy that globally and greedily optimizes the power budget. We also show that DPDP outperforms existing heterogeneous scheduling policies that use performance metrics to decide how best to schedule applications on the big core. © 2016 ACM.",DPDP; Heterogeneous chip multiprocessors; Power management; Scheduling,Budget control; Energy management; Industrial management; Linear programming; Power management; DPDP; Heterogeneous chip multiprocessor; Heterogeneous processors; Heterogeneous Scheduling; Improve performance; Performance metrics; Power management techniques; Running applications; Scheduling
Implementing dense optical flow computation on a heterogeneous FPGA SoC in C,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983627545&doi=10.1145%2f2948976&partnerID=40&md5=fb9ff136c465752dbaac6fecfc60f96c,"High-quality optical flow computation algorithms are computationally intensive. The low computational speed of such algorithms causes difficulties for real-world applications. In this article, we propose an optimized implementation of the classical Combine-Brightness-Gradient (CBG) model on the Xilinx ZYNQ FPGA-SoC, by taking advantage of the inherent algorithmic parallelism and ZYNQ architecture. The execution time decreases to 0.82 second with a lower power consumption (1.881W). It is better than software implementation on PC (Intel i7-3520M, 2.9GHz), which costs 2.635 seconds and 35W. We use C rather than HDLs to describe the algorithm for rapid prototyping. © 2016 ACM.",FPGA-accelerated heterogeneous SoC; High-level synthesis (HLS); Variational optical flow computation; ZYNQ,Algorithms; Field programmable gate arrays (FPGA); High level synthesis; Optical flows; Reconfigurable hardware; Computational speed; Dense optical flow; High quality; Lower-power consumption; Optical flow computation; Optimized implementation; Software implementation; ZYNQ; System-on-chip
Variable liberalization,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983591491&doi=10.1145%2f2963101&partnerID=40&md5=8438d5a36482ccfc4aa81f897186c17c,"In the wake of the current trend of increasing the number of cores on a chip, compiler optimizations for improving the memory performance have assumed increased importance. Loop fusion is one such key optimization that can alleviate memory and bandwidth wall and thus improve parallel performance. However, we find that loop fusion in interesting memory-intensive applications is prevented by the existence of dependences between temporary variables that appear in different loop nests. Furthermore, known techniques of allowing useful transformations in the presence of temporary variables, such as privatization and expansion, prove insufficient in such cases. In this work, we introduce variable liberalization, a technique that selectively removes dependences on temporary variables in different loop nests to achieve loop fusion while preserving the semantical correctness of the optimized program. This removal of extra-stringent dependences effectively amounts to variable expansion, thus achieving the benefit of an increased degree of freedom for program transformation but without an actual expansion. Hence, there is no corresponding increase in the memory footprint incurred. We implement liberalization in the Pluto polyhedral compiler and evaluate its performance on nine hot regions in five real applications. Results demonstrate parallel performance improvement of 1.92× over the Intel compiler, averaged over the nine hot regions, and an overall improvement of as much as 2.17× for an entire application, on an eight-core Intel Xeon processor. © 2016 ACM.",Dependence refinement; Liberalization; Loop fusion; Parallelization; Polyhedral compiler; Scheduling,Degrees of freedom (mechanics); Expansion; Privatization; Scheduling; Dependence refinement; Liberalization; Loop fusion; Parallelizations; Polyhedral compiler; Program compilers
FinPar: A parallel financial benchmark,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979072329&doi=10.1145%2f2898354&partnerID=40&md5=0bc4c895d15a71bac76cf71941d9e5cb,"Commodity many-core hardware is now mainstream, but parallel programming models are still lagging behind in efficiently utilizing the application parallelism. There are (at least) two principal reasons for this. First, real-world programs often take the form of a deeply nested composition of parallel operators, but mapping the available parallelism to the hardware requires a set of transformations that are tedious to do by hand and beyond the capability of the common user. Second, the best optimization strategy, such as what to parallelize and what to efficiently sequentialize, is often sensitive to the input dataset and therefore requires multiple code versions that are optimized differently, which also raises maintainability problems. This article presents three array-based applications from the financial domain that are suitable for GPGPU execution. Common benchmark-design practice has been to provide the same code for the sequential and parallel versions that are optimized for only one class of datasets. In comparison, we document (1) all available parallelism via nested map-reduce functional combinators, in a simple Haskell implementation that closely resembles the original code structure, (2) the invariants and code transformations that govern the main trade-offs of a data-sensitive optimization space, and (3) report target CPU and multiversion GPGPU code together with an evaluation that demonstrates optimization trade-offs and other difficulties. We believe that this work provides useful insight into the language constructs and compiler infrastructure capable of expressing and optimizing such applications, and we report in-progress work in this direction. © 2016 ACM.",Data-parallel functional language; Fission; Fusion; Strength reduction,Benchmarking; Commerce; Computational linguistics; Cosine transforms; Economic and social effects; Fusion reactions; Hardware; Parallel programming; Program compilers; Program processors; Reconfigurable hardware; Code transformation; Data parallel; Fission; Haskell implementation; Language constructs; Optimization strategy; Parallel programming model; Strength reduction; Codes (symbols)
MInGLE: An efficient framework for domain acceleration using low-power specialized functional units,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975517921&doi=10.1145%2f2898356&partnerID=40&md5=613458500256d746c2d94e521faaa2a8,"The end of Dennard scaling leads to new research directions that try to cope with the utilization wall in modern chips, such as the design of specialized architectures. Processor customization utilizes transistors more efficiently, optimizing not only for performance but also for power. However, hardware specialization for each application is costly and impractical due to time-to-market constraints. Domain-specific specialization is an alternative that can increase hardware reutilization across applications that share similar computations. This article explores the specialization of low-power processors with custom instructions (CIs) that run on a specialized functional unit. We are the first, to our knowledge, to design CIs for an application domain and across basic blocks, selecting CIs that maximize both performance and energy efficiency improvements. We present the Merged Instructions Generator for Large Efficiency (MInGLE), an automated framework that identifies and selects CIs. Our framework analyzes large sequences of code (across basic blocks) to maximize acceleration potential while also performing partial matching across applications to optimize for reuse of the specialized hardware. To do this, we convert the code into a new canonical representation, the Merging Diagram, which represents the code's functionality instead of its structure. This is key to being able to find similarities across such large code sequences from different applications with different coding styles. Groups of potential CIs are clustered depending on their similarity score to effectively reduce the search space. Additionally, we create new CIs that cover not only whole-body loops but also fragments of the code to optimize hardware reutilization further. For a set of 11 applications from the media domain, our framework generates CIs that significantly improve the energy-delay product (EDP) and performance speedup. CIs with the highest utilization opportunities achieve an average EDP improvement of 3.8× compared to a baseline processor modeled after an Intel Atom. We demonstrate that we can efficiently accelerate a domain with partially matched CIs, and that their design time, from identification to selection, stays within tractable bounds. © 2016 ACM.",Acceleration; Canonical representation; Clustering; Customization; Domain specific,Acceleration; Energy efficiency; Hardware; Integrated circuit design; Reconfigurable hardware; Canonical representations; Clustering; Customization; Domain specific; Energy delay product; Energy efficiency improvements; Low power processors; Specialized hardware; Codes (symbols)
Exploiting hierarchical locality in deep parallel architectures,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975253375&doi=10.1145%2f2897783&partnerID=40&md5=5ee66304d973fe6f6bb0b857851dbc68,"Parallel computers are becoming deeply hierarchical. Locality-aware programming models allow programmers to control locality at one level through establishing affinity between data and executing activities. This, however, does not enable locality exploitation at other levels. Therefore, we must conceive an efficient abstraction of hierarchical locality and develop techniques to exploit it. Techniques applied directly by programmers, beyond the first level, burden the programmer and hinder productivity. In this article, we propose the Parallel Hierarchical Locality Abstraction Model for Execution (PHLAME). PHLAME is an execution model to abstract and exploit machine hierarchical properties through locality-aware programming and a runtime that takes into account machine characteristics, as well as a data sharing and communication profile of the underlying application. This article presents and experiments with concepts and techniques that can drive such runtime system in support of PHLAME. Our experiments show that our techniques scale up and achieve performance gains of up to 88%. ©2016 ACM.",Hierarchical locality exploitation; PGAS; PHAST; PHLAME; Productivity,Abstracting; Digital storage; Productivity; Abstraction model; Hierarchical locality exploitation; Parallel computer; Performance Gain; PGAS; PHAST; PHLAME; Programming models; Parallel architectures
An online and real-time fault detection and localization mechanism for network-on-chip architectures,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975217471&doi=10.1145%2f2930670&partnerID=40&md5=68b3180b157871e359340d5b162f1e88,"Networks-on-Chip (NoC) are becoming increasingly susceptible to emerging reliability threats. The need to detect and localize the occurrence of faults at runtime is steadily becoming imperative. In this work, we propose NoCAlert, a comprehensive online and real-time fault detection and localization mechanism that demonstrates 0% false negatives within the interconnect for the fault models and stimulus set used in this study. Based on the concept of invariance checking, NoCAlert employs a group of lightweight microchecker modules that collectively implement real-time hardware assertions. The checkers operate concurrently with normal NoC operation, thus eliminating the need for periodic, or triggered-based, self-testing. Based on the pattern/signature of asserted checkers, NoCAlert can pinpoint the location of the fault at various granularity levels. Most important, 97% of the transient and 90% of the permanent faults are detected instantaneously, within a single clock cycle upon fault manifestation. The fault localization accuracy ranges from 90% to 100%, depending on the desired localization granularity. Extensive cycle-accurate simulations in a 64-node CMP and analysis at the RTL netlist-level demonstrate the efficacy of the proposed technique. © 2016 ACM.",Fault detection/diagnosis; Fault localization; Networks-on-chip; NoC,Computer architecture; Distributed computer systems; Embedded systems; Fault detection; Cycle-accurate simulation; Fault detection/diagnosis; Fault localization; Granularity levels; Network-on-chip architectures; Networks on chips; Real time fault detection; Single-clock-cycle; Network-on-chip
Dynamic process migration based on block access patterns occurring in storage servers,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975275153&doi=10.1145%2f2899002&partnerID=40&md5=607ba13f34ee62f16449136af8cc69b6,"An emerging trend in developing large and complex applications on today's high-performance computers is to couple independent components into a comprehensive application. The components may employ the global file system to exchange their data when executing the application. In order to reduce the time required for input/output (I/O) data exchange and data transfer in the coupled systems or other applications, this article proposes a dynamic process migration mechanism on the basis of block access pattern similarity for utilizing the local file cache to exchange the data.We first introduce the scheme of the block access counting diagram to profile the process access pattern during a time period on the storage server. Next, we propose an algorithm that compares the access patterns of processes running on different computing nodes. Last, processes are migrated in order to group processes with similar access patterns. Consequently, the processes on the computing node can exchange their data by accessing the local file cache, instead of the global file system. The experimental results show that the proposed process migration mechanism can reduce the execution time required by the application because of the shorter I/O time, as well as yield attractive I/O throughput. In summary, this dynamic process migration technique can work fairly well for distributed applications whose data dependency rely on distributed file systems. © 2016 ACM 1544-3566/2016/06-ART20 $15.00.",Access counting diagram; Block access events; Distributed file system; I/O performance; Pattern similarity; Process migration,Data transfer; Dynamics; Electronic data interchange; File organization; Access counting diagram; Block access events; Distributed file systems; Pattern similarity; Process migration; Digital storage
A bimodal scheduler for coarse-grained reconfigurable arrays,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974621050&doi=10.1145%2f2893475&partnerID=40&md5=5f3f1b40991c38dc86603a9b377e27e7,"Compilers for Course-Grained Reconfigurable Array (CGRA) architectures suffer from long compilation times and code quality levels far below the theoretical upper bounds. This article presents a new scheduler, called the Bimodal Modulo Scheduler (BMS), to map inner loops onto (heterogeneous) CGRAs of the Architecture for Dynamically Reconfigurable Embedded Systems (ADRES) family. BMS significantly outperforms existing schedulers for similar architectures in terms of generated code quality and compilation time. This is achieved by combining new schemes for backtracking with extended and adapted forms of priority functions and cost functions, as described in the article. BMS is evaluated by mapping multimedia and software-defined radio benchmarks onto tuned ADRES instances. © 2016 ACM.",Cost functions; Modulo scheduling; Placement and routing,Embedded systems; Reconfigurable architectures; Reconfigurable hardware; Scheduling; Software radio; Coarse-grained reconfigurable arrays; Code quality; Modulo scheduling; Placement and routing; Priority functions; Reconfigurable array; Reconfigurable embedded systems; Software-defined radios; Cost functions
COBAYN: Compiler autotuning framework using Bayesian networks,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975506485&doi=10.1145%2f2928270&partnerID=40&md5=c5553fe697b3926f2f787b7a84d455b5,"The variety of today's architectures forces programmers to spend a great deal of time porting and tuning application codes across different platforms. Compilers themselves need additional tuning, which has considerable complexity as the standard optimization levels, usually designed for the average case and the specific target architecture, often fail to bring the best results. This article proposes COBAYN: Compiler autotuning framework using Bayesian Networks, an approach for a compiler autotuning methodology using machine learning to speed up application performance and to reduce the cost of the compiler optimization phases. The proposed framework is based on the application characterization done dynamically by using independent microarchitecture features and Bayesian networks. The article also presents an evaluation based on using static analysis and hybrid feature collection approaches. In addition, the article compares Bayesian networks with respect to several state-of-the-art machine-learning models. Experiments were carried out on an ARM embedded platform and GCC compiler by considering two benchmark suites with 39 applications. The set of compiler configurations, selected by the model (less than 7% of the search space), demonstrated an application performance speedup of up to 4.6× on Polybench (1.85× on average) and 3.1× on cBench (1.54× on average) with respect to standard optimization levels. Moreover, the comparison of the proposed technique with (i) random iterative compilation, (ii) machine learning-based iterative compilation, and (iii) noniterative predictive modeling techniques shows, on average, 1.2×, 1.37×, and 1.48×speedup, respectively. Finally, the proposed method demonstrates 4×and 3×speedup, respectively, on cBench and Polybench in terms of exploration efficiency given the same quality of the solutions generated by the random iterative compilation model. © 2016 ACM.",Bayesian networks; Design space exploration; Statistical inference,Artificial intelligence; Bayesian networks; Benchmarking; Complex networks; Embedded systems; Iterative methods; Learning systems; Network architecture; Static analysis; Application performance; Compiler optimizations; Design space exploration; Iterative compilation; Machine learning models; Standard optimization; Statistical inference; Target architectures; Program compilers
A new compilation flow for software-defined radio applications on heterogeneous MPSoCs,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974626000&doi=10.1145%2f2910583&partnerID=40&md5=b871ae7045e4d741c12c035a9e801d8b,"The advent of portable software-defined radio (SDR) technology is tightly linked to the resolution of a difficult problem: efficient compilation of signal processing applications on embedded computing devices. Modern wireless communication protocols use packet processing rather than infinite stream processing and also introduce dependencies between data value and computation behavior leading to dynamic dataflow behavior. Recently, parametric dataflow has been proposed to support dynamicity while maintaining the high level of analyzability needed for efficient real-life implementations of signal processing computations. This article presents a new compilation flow that is able to compile parametric dataflow graphs. Built on the LLVM compiler infrastructure, the compiler offers an actor-based C++ programming model to describe parametric graphs, a compilation front end for graph analysis, and a back end that currently matches the Magali platform: a prototype heterogeneous MPSoC dedicated to LTE-Advanced. We also introduce an innovative scheduling technique, called microscheduling, allowing one to adapt the mapping of parametric dataflow programs to the specificities of the different possibleMPSoCs targeted. A specific focus on FIFO sizing on the target architecture is presented. The experimental results show compilation of 3GPP LTE-Advanced demodulation on Magali with tight memory size constraints. The compiled programs achieve performance similar to handwritten code. © 2016 ACM.",Compiler; Dataflow; HeterogeneousMPSoC; Programming model; Scheduling; Software-defined radio,4G mobile communication systems; Analog circuits; Application programs; C++ (programming language); Computer programming; Data flow analysis; Embedded systems; Flow graphs; Mobile telecommunication systems; Multiprocessing systems; Packet networks; Program compilers; Radio; Radio receivers; Scheduling; Signal processing; Standards; System-on-chip; Wireless telecommunication systems; Compiler; Dataflow; HeterogeneousMPSoC; Programming models; Software-defined radios; Software radio
Optimizing indirect branches in dynamic binary translators,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964662668&doi=10.1145%2f2866573&partnerID=40&md5=1f6462eb66581bf79110f139f88f4b9d,"Dynamic binary translation is a technology for transparently translating and modifying a program at the machine code level as it is running. A significant factor in the performance of a dynamic binary translator is its handling of indirect branches. Unlike direct branches, which have a known target at translation time, an indirect branch requires translating a source program counter address to a translated program counter address every time the branch is executed. This translation can impose a serious runtime penalty if it is not handled efficiently. MAMBO-X64, a dynamic binary translator that translates 32-bit ARM (AArch32) code to 64-bit ARM (AArch64) code, uses three novel techniques to improve the performance of indirect branch translation. Together, these techniques allow MAMBO-X64 to achieve a very low performance overhead of only 10% on average compared to native execution of 32-bit programs. Hardware-assisted function returns use a software return address stack to predict the targets of function returns, making use of several novel optimizations while also exploiting hardware return address prediction. This technique has a significant impact on most benchmarks, reducing binary translation overhead compared to native execution by 40% on average and by 90% on some benchmarks. Branch table inference, an algorithm for detecting and translating branch tables, can reduce the overhead of translated code by up to 40% on some SPEC CPU2006 benchmarks. The remaining indirect branches are handled using a fast atomic hash table, which is optimized to work with multiple threads. This last technique translates indirect branches using a single shared hash table while avoiding expensive synchronization in performance-critical lookup code. This allows the performance to be on par with thread-private hash tables while having superior memory scalability.",Code cache; Dynamic binary translation; Indirect branch,Bins; Codes (symbols); Cost reduction; Data structures; Hardware; Inference engines; Binary translation; Code cache; Dynamic binary translation; Hardware-assisted; Indirect branch; Multiple threads; Program counters; Return address stacks; Program translators
MAMBO: A Low-Overhead Dynamic Binary Modification Tool for ARM,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968724715&doi=10.1145%2f2896451&partnerID=40&md5=f053ad5498f76319f07686b4918df9b9,"As the ARM architecture expands beyond its traditional embedded domain, there is a growing interest in dynamic binary modification (DBM) tools for general-purpose multicore processors that are part of the ARM family. Existing DBM tools for ARM suffer from introducing large overheads in the execution of applications. The specific questions that this article addresses are (i) how to develop such DBM tools for the ARM architecture and (ii) whether new optimisations are plausible and needed. We describe the general design of MAMBO, a new DBM tool for ARM, which we release together with this publication, and introduce novel optimisations to handle indirect branches. In addition, we explore scenarios in which it may be possible to relax the transparency offered by DBM tools to allow extra optimisations to be applied. These scenarios arise from analysing the most typical usages: for example, application binaries without handcrafted assembly. The performance evaluation shows that MAMBO introduces small overheads for SPEC CPU2006 and PARSEC 3.0 when comparing with the execution times of the unmodified programs: a geometric mean overhead of 28% on a Cortex-A9 and of 34% on a Cortex-A15 for CPU2006, and between 27% and 32%, depending on the number of threads, for PARSEC on a Cortex-A15. © 2016 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",Dynamic binary modification; indirect branch; software code cache,Bins; Memory architecture; Multicore programming; Application binaries; ARM architecture; Binary modification; Embedded domains; Indirect branch; Multi-core processor; Number of threads; Software code caches; ARM processors
Autotuning runtime specialization for sparse matrix-vector multiplication,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971629591&doi=10.1145%2f2851500&partnerID=40&md5=1222408989419cfe49e951d7a88347d2,"Runtime specialization is used for optimizing programs based on partial information available only at runtime. In this paper we apply autotuning on runtime specialization of Sparse Matrix-VectorMultiplication to predict a best specialization method among several. In 91% to 96% of the predictions, either the best or the second-best method is chosen. Predictions achieve average speedups that are very close to the speedups achievable when only the best methods are used. By using an efficient code generator and a carefully designed set of matrix features, we show the runtime costs can be amortized to bring performance benefits for many real-world cases. © 2016 ACM 1544-3566/2016/03-ART515.00.",Autotuning; Runtime code generation; Sparse matrix-vector multiplication,Forecasting; Network components; Program compilers; Autotuning; Optimizing programs; Partial information; Performance benefits; Run-time code generation; Runtime specialization; Sparse matrices; Sparse matrix-vector multiplication; Automatic programming
Examining and reducing the influence of sampling errors on feedback-driven optimizations,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964720977&doi=10.1145%2f2851502&partnerID=40&md5=6589f5578911ab94e52c5e9713590546,"Feedback-driven optimization (FDO) is an important component in mainstream compilers. By allowing the compiler to reoptimize the program based on some profiles of the program's dynamic behaviors, it often enhances the quality of the generated code substantially. A barrier for using FDO is that it often requires many training runs to collect enough profiles to amortize the sensitivity of program optimizations to program input changes. Various sampling techniques have been explored to alleviate this time-consuming process. However, the lowered profile accuracy caused by sampling often hurts the benefits of FDO. This article gives the first systematic study in how sampling rates affect the accuracy of collected profiles and how the accuracy correlates with the usefulness of the profile for modern FDO. Studying basic block and edge profiles for FDO in two mature compilers reveals several counterintuitive observations, one of which is that profiling accuracy does not strongly correlate with the benefits of the FDO. A detailed analysis identifies three types of sampling-caused errors that critically impair the quality of the profiles for FDO. It then introduces a simple way to rectify profiles based on the findings. Experiments demonstrate that the simple rectification fixes most of those critical errors in sampled profiles and significantly enhances the effectiveness of FDO. © 2016 ACM.",Feedback-driven optimization; Influence of sampling errors; Performance,Errors; Dynamic behaviors; Performance; Profile accuracies; Program optimization; Sampling errors; Sampling rates; Sampling technique; Systematic study; Program compilers
Understanding and mitigating covert channels through branch predictors,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961240302&doi=10.1145%2f2870636&partnerID=40&md5=c645dd2ec7191f6c8b284e07a13feba8,"Covert channels through shared processor resources provide secret communication between two malicious processes: the trojan and the spy. In this article, we classify, analyze, and compare covert channels through dynamic branch prediction units in modern processors. Through experiments on a real hardware platform, we compare contention-based channel and the channel that is based on exploiting the branch predictor's residual state. We analyze these channels in SMT and single-threaded environments under both clean and noisy conditions. Our results show that the residual state-based channel provides a cleaner signal and is effective even in noisy execution environments with another application sharing the same physical core with the trojan and the spy. We also estimate the capacity of the branch predictor covert channels and describe a software-only mitigation technique that is based on randomizing the state of the predictor tables on context switches. We show that this protection eliminates all covert channels through the branch prediction unit with minimal impact on performance. © 2016 ACM.",Branch predictor; Covert channel; Security,Hardware; Software engineering; Application sharing; Branch predictors; Covert channels; Execution environments; Mitigation techniques; Processor resources; Secret communications; Security; Malware
A compiler approach for exploiting partial SIMD parallelism,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964596026&doi=10.1145%2f2886101&partnerID=40&md5=8f56bfabd1f6d8606ea896fc1de43281,"Existing vectorization techniques are ineffective for loops that exhibit little loop-level parallelism but some limited superword-level parallelism (SLP). We show that effectively vectorizing such loops requires partial vector operations to be executed correctly and efficiently, where the degree of partial SIMD parallelism is smaller than the SIMD datapath width. We present a simple yet effective SLP compiler technique called PAVER (PArtial VEctorizeR), formulated and implemented in LLVM as a generalization of the traditional SLP algorithm, to optimize such partially vectorizable loops. The key idea is to maximize SIMD utilization by widening vector instructions used while minimizing the overheads caused by memory access, packing/ unpacking, and/or masking operations, without introducing new memory errors or new numeric exceptions. For a set of 9 C/C++/Fortran applications with partial SIMD parallelism, PAVER achieves significantly better kernel and whole-program speedups than LLVM on both Intel's AVX and ARM's NEON.",Basic block vectorization; Loop vectorization; Partial SIMD parallelism; Partial vectorization; SLP vectorization,Application programs; C (programming language); Compiler techniques; Loop vectorization; Loop-level parallelism; Partial SIMD parallelism; Superword Level Parallelism; Vector operations; Vectorization; Vectorization techniques; Program compilers
Power efficient hardware transactional memory: Dynamic issue of transactions,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964619834&doi=10.1145%2f2875425&partnerID=40&md5=0fe905eea3b2cc65469a9f3336789855,"Transactional Memory (TM) is no longer just an academic interest as industry has started to adopt the idea in its commercial products. In this paper, we propose Dynamic Transaction Issue (DTI), a new scheme that can be easily implemented on top of existing Hardware TM (HTM) systems, provided additional messages. Instead of wasting power and energy in transaction aborts, Dynamic Transaction Issue puts a processor core into a low-power state when there is a reasonable suspicion that the current transaction running on it will be aborted soon in the future. We have implemented Dynamic Transaction Issue on a cycle-accurate simulator of a multicore processor system with out-of-order superscalar cores, augmented with a power package and a TM package which add accurate dynamic power estimates and a TM framework to the simulator. Our simulation results show that Dynamic Transaction Issue can achieve energy savings up to 37% from the energy consumption of a base machine with no mechanism to suppress useless aborts. We also compare Dynamic Transaction Issue with various alternative hardware TM mechanisms.",Dynamic issue; Energy consumption; Hardware transactional memory; Power consumption; Transactional memory,Electric power utilization; Energy conservation; Energy utilization; Hardware; Reconfigurable hardware; Storage allocation (computer); Commercial products; Cycle-accurate simulators; Hardware transactional memory; Low power state; Multi-core processor; Power efficient; Processor cores; Transactional memory; Dynamics
Hardware performance counter-based malware identification and detection with adaptive compressive sensing,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964562348&doi=10.1145%2f2857055&partnerID=40&md5=b86c584a12292058433e6b4289900d78,"Hardware Performance Counter-based (HPC) runtime checking is an effective way to identify malicious behaviors of malware and detect malicious modifications to a legitimate program's control flow. To reduce the overhead in the monitored system which has limited storage and computing resources, we present a ""sample-locally-analyze-remotely"" technique. The sampled HPC data are sent to a remote server for further analysis. To minimize the I/O bandwidth required for transmission, the fine-grained HPC profiles are compressed into much smaller vectors with Compressive Sensing. The experimental results demonstrate an 80% I/O bandwidth reduction after applying Compressive Sensing, without compromising the detection and identification capabilities. © 2016 ACM.",Compressive sensing; Hardware performance counters; Malware identification and detection,Bandwidth; Compressed sensing; Computer crime; Computer hardware; Digital storage; Hardware; Reconfigurable hardware; Signal reconstruction; Bandwidth reductions; Compressive sensing; Computing resource; Detection and identifications; Hardware performance counters; Malicious behavior; Monitored systems; Run-time checking; Malware
Building heterogeneous unified virtual memories (UVMs) without the Overhead,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964556504&doi=10.1145%2f2889488&partnerID=40&md5=b326a29f43cda46c3c46fa9640e62bfa,"This work proposes a novel scheme to facilitate heterogeneous systems with unified virtual memory. Research proposals implement coherence protocols for sequential consistency (SC) between central processing unit (CPU) cores and between devices. Such mechanisms introduce severe bottlenecks in the system; therefore, we adopt the heterogeneous-race-free (HRF)memorymodel. The use ofHRF simplifies the coherency protocol and the graphics processing unit (GPU) memory management unit (MMU). Our protocol optimizes CPU and GPU demands separately, with the GPU part being simpler while the CPU is more elaborate and latency aware. We achieve an average 45% speedup and 45% electronic data processing reduction (20% energy) over the corresponding SC implementation. ©2016 ACM.",Directory-less protocol; GPU MMU design; Heterogeneous coherence; Multicore; Virtual coherence protocol,Computer graphics; Computer graphics equipment; Data handling; Physical addresses; Program processors; Coherence protocol; Directory-less protocols; Electronic data processing; Graphics Processing Unit; Heterogeneous systems; Multi core; Research proposals; Sequential consistency; Memory management units
R-GPU: A reconfigurable GPU architecture,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961245117&doi=10.1145%2f2890506&partnerID=40&md5=dc076d7d85d4529f6886b0d521a23678,"Over the last decade, Graphics Processing Unit (GPU) architectures have evolved from a fixed-function graphics pipeline to a programmable, energy-efficient compute accelerator for massively parallel applications. The compute power arises from the GPU's Single Instruction/Multiple Threads architecture: concurrently running many threads and executing them as Single Instruction/Multiple Data-style vectors. However, compute power is still lost due to cycles spent on data movement and control instructions instead of data computations. Even more cycles are lost on pipeline stalls resulting from long latency (memory) operations. To improve not only performance but also energy efficiency, we introduce R-GPU: a reconfigurable GPU architecture with communicating cores. R-GPU is an addition to a GPU, which can still be used as such, but also has the ability to reorganize the cores of a GPU in a reconfigurable network. In R-GPU data movement and control is implicit in the configuration of the network. Each core executes a fixed instruction, reducing instruction decode count and increasing energy efficiency. On a number of benchmarks we show an average performance improvement of 2.1× over the same GPU without modifications.We further make a conservative power estimation of R-GPU which shows that power consumption can be reduced by 6%, leading to an energy consumption reduction of 55%, while area only increases by a mere 4%. © 2016 ACM.",Design; GPGPU; Performance; Reconfigurable architecture,Benchmarking; Computer graphics; Design; Energy efficiency; Energy utilization; Network architecture; Pipeline processing systems; Pipelines; Program processors; Reconfigurable architectures; Reconfigurable hardware; Control instruction; GPGPU; Graphics pipeline; Graphics Processing Unit; Massively parallels; Performance; Reconfigurable network; Single instruction/multiple datum; Computer graphics equipment
Clustering-based selection for the exploration of compiler optimization sequences,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964689475&doi=10.1145%2f2883614&partnerID=40&md5=e7810cadb162b151cfd0562036dca81d,"A large number of compiler optimizations are nowadays available to users. These optimizations interact with each other and with the input code in several and complex ways. The sequence of application of optimization passes can have a significant impact on the performance achieved. The effect of the optimizations is both platform and application dependent. The exhaustive exploration of all viable sequences of compiler optimizations for a given code fragment is not feasible. As this exploration is a complex and time-consuming task, several researchers have focused on Design Space Exploration (DSE) strategies both to select optimization sequences to improve the performance of each function of the application and to reduce the exploration time. In this article, we present a DSE scheme based on a clustering approach for grouping functions with similarities and exploration of a reduced search space resulting from the combination of optimizations previously suggested for the functions in each group. The identification of similarities between functions uses a data mining method that is applied to a symbolic code representation. The data mining process combines three algorithms to generate clusters: The Normalized Compression Distance, the Neighbor Joining, and a new ambiguity-based clustering algorithm. Our experiments for evaluating the effectiveness of the proposed approach address the exploration of optimization sequences in the context of the ReflectC compiler, considering 49 compilation passes while targeting a Xilinx MicroBlaze processor, and aiming at performance improvements for 51 functions and four applications. Experimental results reveal that the use of our clustering-based DSE approach achieves a significant reduction in the total exploration time of the search space (20× over a Genetic Algorithm approach) at the same time that considerable performance speedups (41% over the baseline) were obtained using the optimized codes. Additional experiments were performed considering the LLVM compiler, considering 124 compilation passes, and targeting a LEON3 processor. The results show that our approach achieved geometricmean speedups of 1.49×, 1.32×, and 1.24× for the best 10, 20, and 30 functions, respectively, and a global improvement of 7% over the performance obtained when compiling with -O2.",Clustering; Design space exploration; Phase-ordering problem,Codes (symbols); Data mining; Genetic algorithms; Optimization; Program compilers; Space time codes; Systems analysis; Additional experiments; Clustering; Combination of optimizations; Compiler optimizations; Design space exploration; Genetic algorithm approach; Normalized compression distance; Phase Ordering; Clustering algorithms
Boosting the priority of garbage: Scheduling collection on heterogeneous multicore processors,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961246728&doi=10.1145%2f2875424&partnerID=40&md5=381b1fbf638c7b91b5b9dbd252905e14,"While hardware is evolving toward heterogeneous multicore architectures, modern software applications are increasingly written in managed languages. Heterogeneity was born of a need to improve energy efficiency; however, we want the performance of our applications not to suffer from limited resources. How best to schedule managed language applications on a mix of big, out-of-order cores and small, in-order cores is an open question, complicated by the host of service threads that perform key tasks such as memory management. These service threads compete with the application for core and memory resources, and garbage collection (GC) must sometimes suspend the application if there is not enough memory available for allocation. In this article, we explore concurrent garbage collection's behavior, particularly when it becomes critical, and how to schedule it on a heterogeneous system to optimize application performance. While some applications see no difference in performance when GC threads are run on big versus small cores, others-those with GC criticality-see up to an 18% performance improvement. We develop a new, adaptive scheduling algorithm that responds to GC criticality signals from the managed runtime, giving more big-core cycles to the concurrent collector when it is under pressure and in danger of suspending the application. Our experimental results show that our GC-criticality-aware scheduler is robust across a range of heterogeneous architectures with different core counts and frequency scaling and across heap sizes. Our algorithm is performance and energy neutral for GC-uncritical Java applications and significantly speeds up GC-critical applications by 16%, on average, while being 20% more energy efficient for a heterogeneous multicore with three big cores and one small core. © 2016 ACM.",Algorithms; Automatic memory management; Concurrent collection; Design; Energy; Energy-efficiency; GC criticality; Heterogeneous multicore architectures; Managed runtimes; Performance,Algorithms; Application programs; Computational linguistics; Computer architecture; Criticality (nuclear fission); Design; Dynamic frequency scaling; Energy efficiency; Java programming language; Memory architecture; Refuse collection; Scheduling; Scheduling algorithms; Software architecture; Automatic memory management; Energy; Heterogeneous multicore architectures; Performance; Runtimes; Multitasking
Dynamic memory balancing for virtualization,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964660322&doi=10.1145%2f2851501&partnerID=40&md5=c419e336541e6b8606e231abd6002740,"Allocating memory dynamically for virtual machines (VMs) according to their demands provides significant benefits as well as great challenges. Efficient memory resource management requires knowledge of the memory demands of applications or systems at runtime. A widely proposed approach is to construct a miss ratio curve (MRC) for a VM, which not only summarizes the current working set size (WSS) of the VM but also models the relationship between its performance and the target memory allocation size. Unfortunately, the cost of monitoring and maintaining the MRC structures is nontrivial. This article first introduces a lowcost WSS tracking system with effective optimizations on data structures, as well as an efficient mechanism to decrease the frequency of monitoring. We also propose a Memory Balancer (MEB), which dynamically reallocates guest memory based on the predicted WSS. Our experimental results show that our prediction schemes yield a high accuracy of 95.2% and low overhead of 2%. Furthermore, the overall system throughput can be significantly improved with MEB, which brings a speedup up to 7.4 for two to four VMs and 4.54 for an overcommitted system with 16 VMs.",Data center; Memory; Performance; Virtualization; Working set size,Data storage equipment; Storage allocation (computer); Virtual reality; Data centers; Memory resources; Miss ratio curves; Performance; Prediction schemes; System throughput; Virtualizations; Working set; Structural optimization
Thread-aware adaptive prefetcher on multicore systems: Improving the performance for multithreaded workloads,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964562250&doi=10.1145%2f2890505&partnerID=40&md5=a6012afc834b367a3b31f858ad2d1984,"Most processors employ hardware data prefetching techniques to hide memory access latencies. However, the prefetching requests from different threads on a multicore processor can cause severe interference with prefetching and/or demand requests of others. The data prefetching can lead to significant performance degradation due to shared resource contention on shared memory multicore systems. This article proposes a thread-aware data prefetching mechanism based on low-overhead runtime information to tune prefetching modes and aggressiveness, mitigating the resource contention in the memory system. Our solution has three new components: (1) a self-tuning prefetcher that uses runtime feedback to dynamically adjust data prefetching modes and arguments of each thread, (2) a filtering mechanism that informs the hardware about which prefetching request can cause shared data invalidation and should be discarded, and (3) a limiter thread acceleration mechanism to estimate and accelerate the critical thread which has the longest completion time in the parallel region of execution. On a set of multithreaded parallel benchmarks, our thread-aware data prefetchingmechanism improves the overall performance of 64-core system by 13% over a multimode prefetch baseline system with two-level cache organization and conventional modified, exclusive, shared, and invalid-based directory coherence protocol.We compare our approach with the feedback directed prefetching technique and find that it provides 9% performance improvement on multicore systems, while saving the memory bandwidth consumption.",Data prefetcher; Multicore; Multithreaded; Self-tuning; Thread-aware,Benchmarking; Distributed computer systems; Hardware; Memory architecture; Parallel processing systems; Program processors; Reconfigurable hardware; Data prefetcher; Multi core; Multithreaded; Selftuning; Thread-aware; Cache memory
Resistive GP-SIMD processing-in-memory,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954489585&doi=10.1145%2f2845084&partnerID=40&md5=d1170d90c2b2de0a16c622a3023bffd8,"GP-SIMD, a novel hybrid general-purpose SIMD architecture, addresses the challenge of data synchronization by in-memory computing, through combining data storage and massive parallel processing. In this article, we explore a resistive implementation of the GP-SIMD architecture. In resistive GP-SIMD, a novel resistive row and column addressable 4F2 crossbar is utilized, replacing the modified CMOS 190F2 SRAM storage previously proposed for GP-SIMD architecture. The use of the resistive crossbar allows scaling the GP-SIMD from few millions to few hundred millions of processing units on a single silicon die. The performance, power consumption and power efficiency of a resistive GP-SIMD are compared with the CMOS version.We find that PiM architectures and, specifically, GP-SIMD benefit more than other many-core architectures from using resistive memory. A framework for in-place arithmetic operation on a single multivalued resistive cell is explored, demonstrating a potential to become a building block for next-generation PiM architectures. © 2016 ACM 1544-3566/2016/01-ART57 15.00.",In-memory computing; Memristor; Phrases: GP-SIMD; PIM; Processing in memory; Resistive RAM; SIMD,Digital storage; Energy efficiency; Memory architecture; Random access storage; Static random access storage; Memristor; Phrases: GP-SIMD; PIM; Processing in memory; Resistive rams; SIMD; Computer architecture
Simultaneous multi-layer access: Improving 3d-stacked memory bandwidth at low cost,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954546484&doi=10.1145%2f2832911&partnerID=40&md5=24c1f041fa1082c07caafe6158c26286,"3D-stacked DRAM alleviates the limited memory bandwidth bottleneck that exists in modern systems by leveraging through silicon vias (TSVs) to deliver higher external memory channel bandwidth. Today's systems, however, cannot fully utilize the higher bandwidth offered by TSVs, due to the limited internal bandwidth within each layer of the 3D-stacked DRAM. We identify that the bottleneck to enabling higher bandwidth in 3D-stacked DRAM is now the global bitline interface, the connection between the DRAM row buffer and the peripheral IO circuits. The global bitline interface consists of a limited and expensive set of wires and structures, called global bitlines and global sense amplifiers, whose high cost makes it difficult to simply scale up the bandwidth of the interface within a single DRAM layer in the 3D stack. We alleviate this bandwidth bottleneck by exploiting the observation that several global bitline interfaces already exist across the multiple DRAM layers in current 3D-stacked designs, but only a fraction of them are enabled at the same time. We propose a new 3D-stacked DRAM architecture, called Simultaneous Multi-Layer Access (SMLA), which increases the internal DRAM bandwidth by accessing multiple DRAM layers concurrently, thus making much greater use of the bandwidth that the TSVs offer. To avoid channel contention, the DRAM layers must coordinate with each other when simultaneously transferring data. We propose two approaches to coordination, both of which deliver four times the bandwidth for a four-layer DRAM, over a baseline that accesses only one layer at a time.Our first approach,Dedicated-IO, statically partitions the TSVs by assigning each layer to a dedicated set of TSVs that operate at a higher frequency. Unfortunately, Dedicated-IO requires a nonuniform design for each layer (increasing manufacturing costs), and its DRAM energy consumption scales linearly with the number of layers. Our second approach, Cascaded-IO, solves both issues by instead time multiplexing all of the TSVs across layers. Cascaded-IO reduces DRAM energy consumption by lowering the operating frequency of higher layers. Our evaluations show that SMLA provides significant performance improvement and energy reduction across a variety of workloads (55%/18% on average for multiprogrammed workloads, respectively) over a baseline 3D-stacked DRAM, with low overhead. © 2015 ACM 1544-3566/2015/12-ART63 15.00.",,Amplifiers (electronic); Bandwidth; Costs; Design for manufacturability; Dynamic random access storage; Electronics packaging; Energy utilization; Integrated circuit interconnects; Multiprogramming; 3D-stacked memory; Bandwidth bottlenecks; Channel contention; Higher frequencies; Manufacturing cost; Operating frequency; Through silicon vias; Time multiplexing; Three dimensional integrated circuits
Citadel: Efficiently protecting stacked memory from tsv and large granularity failures,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954550384&doi=10.1145%2f2840807&partnerID=40&md5=88262ec3c338d5e8fad5faa1d8c2b320,"Stacked memory modules are likely to be tightly integrated with the processor. It is vital that these memory modules operate reliably, as memory failure can require the replacement of the entire socket. To make matters worse, stacked memory designs are susceptible to newer failure modes (e.g., due to faulty throughsilicon vias, or TSVs) that can cause large portions of memory, such as a bank, to become faulty. To avoid data loss from large-granularity failures, the memory system may use symbol-based codes that stripe the data for a cache line across several banks (or channels). Unfortunately, such data-striping reduces memory-level parallelism, causing significant slowdown and higher power consumption. This article proposes Citadel, a robust memory architecture that allows the memory system to retain each cache line within one bank. By retaining cache lines within banks, Citadel enables a high-performance and low-power memory system and also efficiently protects the stacked memory system from large-granularity failures. Citadel consists of three components; TSV-Swap, which can tolerate both faulty data-TSVs and faulty address-TSVs; Tri-Dimensional Parity (3DP), which can tolerate column failures, row failures, and bank failures; and Dynamic Dual-Granularity Sparing (DDS), which can mitigate permanent faults by dynamically sparing faulty memory regions either at a row granularity or at a bank granularity. Our evaluations with real-world data for DRAM failures show that Citadel provides performance and power similar to maintaining the entire cache line in the same bank, and yet provides 700? higher reliability than ChipKill-like ECC codes. ©2016 ACM 1544-3566/2016/01-ART49 15.00.",,Dynamic random access storage; Memory architecture; Outages; Three dimensional integrated circuits; Column failure; Low-power memory; Memory failures; Memory level parallelisms; Memory systems; Permanent faults; Three component; Through silicon vias; Cache memory
RFVP: Rollback-free value prediction with safe-To-Approximate loads,2016,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954502471&doi=10.1145%2f2836168&partnerID=40&md5=0ada19b22c05d254b91e08e7dd4fd3dc,"This article aims to tackle two fundamental memory bottlenecks: limited off-chip bandwidth (bandwidth wall) and long access latency (memory wall). To achieve this goal, our approach exploits the inherent error resilience of a wide range of applications. We introduce an approximation technique, called Rollback-Free Value Prediction (RFVP). When certain safe-To-Approximate load operationsmiss in the cache, RFVP predicts the requested values. However, RFVP does not check for or recover from load-value mispredictions, hence, avoiding the high cost of pipeline flushes and re-executions. RFVP mitigates the memory wall by enabling the execution to continue without stalling for long-latency memory accesses. To mitigate the bandwidth wall, RFVP drops a fraction of load requests that miss in the cache after predicting their values. Dropping requests reduces memory bandwidth contention by removing them from the system. The drop rate is a knob to control the trade-off between performance/energy efficiency and output quality. Our extensive evaluations show that RFVP, when used in GPUs, yields significant performance improvement and energy reduction for a wide range of quality-loss levels. We also evaluate RFVP's latency benefits for a single core CPU. The results show performance improvement and energy reduction for a wide variety of applications with less than 1% loss in quality.",GPUs; Load value approximation; memory bandwidth; memory latency; value prediction,Bandwidth; Drops; Economic and social effects; Forecasting; Program processors; Value engineering; GPUs; Load values; Memory bandwidths; Memory latencies; Value prediction; Quality control
The Droplet Search Algorithm for Kernel Scheduling,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194380699&doi=10.1145%2f3650109&partnerID=40&md5=f4ae53f6157199857c83107d21ed2cd0,"Kernel scheduling is the problem of finding the most efficient implementation for a computational kernel. Identifying this implementation involves experimenting with the parameters of compiler optimizations, such as the size of tiling windows and unrolling factors. This article shows that it is possible to organize these parameters as points in a coordinate space. The function that maps these points to the running time of kernels, in general, will not determine a convex surface. However, this article provides empirical evidence that the origin of this surface (an unoptimized kernel) and its global optimum (the fastest kernel) reside on a convex region. We call this hypothesis the ""droplet expectation.""Consequently, a search method based on the Coordinate Descent algorithm tends to find the optimal kernel configuration quickly if the hypothesis holds. This approach-called Droplet Search-has been available in Apache TVM since April of 2023. Experimental results with six large deep learning models on various computing devices (ARM, Intel, AMD, and NVIDIA) indicate that Droplet Search is not only as effective as other AutoTVM search techniques but also 2 to 10 times faster. Moreover, models generated by Droplet Search are competitive with those produced by TVM's AutoScheduler (Ansor), despite the latter using 4 to 5 times more code transformations than AutoTVM. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",kernel scheduling; optimization; search; Tensor compiler,Cosine transforms; Deep learning; Program compilers; Scheduling algorithms; Compiler optimizations; Computational kernels; Coordinate space; Efficient implementation; Kernel scheduling; Optimisations; Running time; Search; Search Algorithms; Tensor compiler; Drops
Camouflage: Utility-Aware Obfuscation for Accurate Simulation of Sensitive Program Traces,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194370260&doi=10.1145%2f3650110&partnerID=40&md5=3ae5a8501840316d8fbfb219dc3c1f79,"Trace-based simulation is a widely used methodology for system design exploration. It relies on realistic traces that represent a range of behaviors necessary to be evaluated, containing a lot of information about the application, its inputs and the underlying system on which it was generated. Consequently, generating traces from real-world executions risks leakage of sensitive information. To prevent this, traces can be obfuscated before release. However, this can undermine their ideal utility, i.e., how realistically a program behavior was captured. To address this, we propose Camouflage, a novel obfuscation framework, designed with awareness of the necessary architectural properties required to preserve trace utility, while ensuring secrecy of the inputs used to generate the trace. Focusing on memory access traces, our extensive evaluation on various benchmarks shows that camouflaged traces preserve the performance measurements of the original execution, with an average τ correlation of 0.66. We model input secrecy as an input indistinguishability problem and show that the average security loss is 7.8%, which is better than traces generated from the state-of-The-Art. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Performance characterization; Privacy of traces; Synthetic trace generation,Design Exploration; Performance characterization; Privacy of trace; Program behavior; Real-world; Sensitive informations; Synthetic trace generation; Trace generation; Trace-based simulation; Underlying systems; Benchmarking
"NEM-GNN-DAC/ADC-less, Scalable, Reconfigurable, Graph and Sparsity-Aware Near-Memory Accelerator for Graph Neural Networks",2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194382242&doi=10.1145%2f3652607&partnerID=40&md5=731ff503fa39339e1b90702ecdb339cd,"Graph neural networks (GNNs) are of great interest in real-life applications such as citation networks and drug discovery owing to GNN's ability to apply machine learning techniques on graphs. GNNs utilize a two-step approach to classify the nodes in a graph into pre-defined categories. The first step uses a combination kernel to perform data-intensive convolution operations with regular memory access patterns. The second step uses an aggregation kernel that operates on sparse data having irregular access patterns. These mixed data patterns render CPU/GPU-based compute energy-inefficient. Von Neumann based accelerators like AWB-GCN [7] suffer from increased data movement, as the data-intensive combination requires large data movement to/from memory to perform computations. ReFLIP [8] performs resistive random access memory based in-memory (PIM) compute to overcome data movement costs. However, ReFLIP suffers from increased area requirement due to dedicated accelerator arrangement, and reduced performance due to limited parallelism and energy due to fundamental issues in ReRAM-based compute. This article presents a scalable (non-exponential storage requirement), DAC/ADC-less PIM-based combination, with (i) early compute termination and (ii) pre-compute by reconfiguring SOC components. Graph and sparsity-Aware near-memory aggregation using the proposed compute-As-soon-As-ready (CAR) broadcast approach improves performance and energy further. NEM-GNN achieves g1/480-230x, g1/480-300x, g1/4850-1,134x, and g1/47-8x improvement over ReFLIP, in terms of performance, throughput, energy efficiency, and compute density. © 2024 Copyright held by the owner/author(s).",broadcast; compute-As-soon-As-ready; early compute termination; Graph neural networks; graph-Aware; L1 cache; pre-compute; processing in memory; sparsity-Aware,Cache memory; Energy efficiency; Learning systems; System-on-chip; Broadcast; Compute-as-soon-as-ready; Data movements; Early compute termination; Graph neural networks; Graph-aware; L1 caches; Pre-compute; Processing-in-memory; Sparsity-aware; Graph neural networks
"Architectural Support for Sharing, Isolating and Virtualizing FPGA Resources",2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194382524&doi=10.1145%2f3648475&partnerID=40&md5=0d89e4a37efdd5342f44cfd1fee8384c,"FPGAs are increasingly popular in cloud environments for their ability to offer on-demand acceleration and improved compute efficiency. Providers would like to increase utilization, by multiplexing customers on a single device, similar to how processing cores and memory are shared. Nonetheless, multi-Tenancy still faces major architectural limitations including: (a) inefficient sharing of memory interfaces across hardware tasks (HT) exacerbated by technological limitations and peculiarities, (b) insufficient solutions for performance and data isolation and high quality of service, and (c) absent or simplistic allocation strategies to effectively distribute external FPGA memory across HT. This article presents a full-stack solution for enabling multi-Tenancy on FPGAs. Specifically, our work proposes an intra-fpga virtualization layer to share FPGA interfaces and its resources across tenants. To achieve efficient inter-connectivity between virtual FPGAs (vFGPAs) and external interfaces, we employ a compact network-on-chip architecture to optimize resource utilization. Dedicated memory management units implement the concept of virtual memory in FPGAs, providing mechanisms to isolate the address space and enable memory protection. We also introduce a memory segmentation scheme to effectively allocate FPGA address space and enhance isolation through hardware-software support, while preserving the efficacy of memory transactions. We assess our solution on an Alveo U250 Data Center FPGA Card, employing 10 real-world benchmarks from the Rodinia and Rosetta suites. Our framework preserves the performance of HT from a non-virtualized environment, while enhancing the device aggregate throughput through resource sharing; up to 3.96x in isolated and up to 2.31x in highly congested settings, where an external interface is shared across four vFPGAs. Finally, our work ensures high-quality of service, with HT achieving up to 0.95x of their native performance, even when resource sharing introduces interference from other accelerators. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",FPGA; heterogeneous computing; multi-Tenancy; virtualization,Distributed computer systems; Memory architecture; Network-on-chip; Quality of service; Virtual addresses; Virtual reality; Virtualization; Address space; Architectural support; Hardware tasks; Heterogeneous computing; High quality; Multi tenancies; Performance; Quality-of-service; Resources sharing; Virtualizations; Field programmable gate arrays (FPGA)
TEA+: A Novel Temporal Graph Random Walk Engine with Hybrid Storage Architecture,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194396505&doi=10.1145%2f3652604&partnerID=40&md5=725334c5cded73dad750b1e46b358b38,"Many real-world networks are characterized by being temporal and dynamic, wherein the temporal information signifies the changes in connections, such as the addition or removal of links between nodes. Employing random walks on these temporal networks is a crucial technique for understanding the structural evolution of such graphs over time. However, existing state-of-The-Art sampling methods are designed for traditional static graphs, and as such, they struggle to efficiently handle the dynamic aspects of temporal networks. This deficiency can be attributed to several challenges, including increased sampling complexity, extensive index space, limited programmability, and a lack of scalability.In this article, we introduce TEA+, a robust, fast, and scalable engine for conducting random walks on temporal graphs. Central to TEA+ is an innovative hybrid sampling method that amalgamates two Monte Carlo sampling techniques. This fusion significantly diminishes space complexity while maintaining a fast sampling speed. Additionally, TEA+ integrates a range of optimizations that significantly enhance sampling efficiency. This is further supported by an effective graph updating strategy, skilled in managing dynamic graph modifications and adeptly handling the insertion and deletion of both edges and vertices. For ease of implementation, we propose a temporal-centric programming model, designed to simplify the development of various random walk algorithms on temporal graphs. To ensure optimal performance across storage constraints, TEA+ features a degree-Aware hybrid storage architecture, capable of adeptly scaling in different memory environments. Experimental results showcase the prowess of TEA+, as it attains up to three orders of magnitude speedups compared to current random walk engines on extensive temporal graphs. © 2024 Copyright held by the owner/author(s).",External Storage; Graph algorithm; Random walk; Temporal graph,Complex networks; Graphic methods; Memory architecture; Monte Carlo methods; Network architecture; Random processes; External storage; Graph algorithms; Hybrid storages; Random Walk; Real-world networks; Sampling method; Storage architectures; Temporal graphs; Temporal information; Temporal networks; Engines
XMeta: SSD-HDD-Hybrid Optimization for Metadata Maintenance of Cloud-scale Object Storage,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194413915&doi=10.1145%2f3652606&partnerID=40&md5=81b2c1540d64491f58e1f65e499d966a,"Object storage has been widely used in the cloud. Traditionally, the size of object metadata is much smaller than that of object data, and thus existing object storage systems (such as Ceph and Oasis) can place object data and metadata, respectively, on hard disk drives (HDDs) and solid-state drives (SSDs) to achieve high I/O performance at a low monetary cost. Currently, however, a wide range of cloud applications organize their data as large numbers of small objects of which the data size is close to (or even smaller than) the metadata size, thus greatly increasing the cost if placing all metadata on expensive SSDs.This article presents xMeta, an SSD-HDD-hybrid optimization for metadata maintenance of cloud-scale object storage. We observed that a substantial portion of the metadata of small objects is rarely accessed and thus can be stored on HDDs with little performance penalty. Therefore, xMeta first classifies the hot and cold metadata based on the frequency of metadata accesses of upper-layer applications and then adaptively stores the hot metadata on SSDs and the cold metadata on HDDs. We also propose a merging mechanism for hot metadata to further improve the efficiency of SSD storage and optimize range key query and insertion for hot metadata by designing composite keys. We have integrated the xMeta metadata service with Ceph to realize a high-performance, low-cost object store (called xCeph). The extensive evaluation shows that xCeph outperforms the original Ceph by an order of magnitude in the space requirement of SSD storage, while improving the throughput by up to 2.7×. © 2024 Copyright held by the owner/author(s).",cloud-scale object storage; Optimization for metadata maintenance,Cloud computing; Costs; Hard disk storage; Metadata; Cloud-scale object storage; Hard Disk Drive; Hybrid optimization; Object data; Object storage systems; Object storages; Optimisations; Optimization for metadatum maintenance; Performance; Small objects; Maintenance
Orchard: Heterogeneous Parallelism and Fine-grained Fusion for Complex Tree Traversals,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194418773&doi=10.1145%2f3652605&partnerID=40&md5=800603402075ce4f50e351bb60e11ac6,"Many applications are designed to perform traversals on tree-like data structures. Fusing and parallelizing these traversals enhance the performance of applications. Fusing multiple traversals improves the locality of the application. The runtime of an application can be significantly reduced by extracting parallelism and utilizing multi-Threading. Prior frameworks have tried to fuse and parallelize tree traversals using coarse-grained approaches, leading to missed fine-grained opportunities for improving performance. Other frameworks have successfully supported fine-grained fusion on heterogeneous tree types but fall short regarding parallelization. We introduce a new framework Orchard built on top of Grafter. Orchard's novelty lies in allowing the programmer to transform tree traversal applications by automatically applying fine-grained fusion and extracting heterogeneous parallelism. Orchard allows the programmer to write general tree traversal applications in a simple and elegant embedded Domain-Specific Language (eDSL). We show that the combination of fine-grained fusion and heterogeneous parallelism performs better than each alone when the conditions are met. © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesAutomatic parallelization; compilers; fusion; optimization,Problem oriented languages; Trees (mathematics); Additional key word and phrasesautomatic parallelization; Compiler; Fine grained; Key words; Optimisations; Parallelizations; Parallelizing; Performance; Runtimes; Tree traversal; Orchards
Cerberus: Triple Mode Acceleration of Sparse Matrix and Vector Multiplication,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194362729&doi=10.1145%2f3653020&partnerID=40&md5=201457a4b20bb18fc579a97dd3673f1e,"The multiplication of sparse matrix and vector (SpMV) is one of the most widely used kernels in high-performance computing as well as machine learning acceleration for sparse neural networks. The design space of SpMV accelerators has two axes: Algorithm and matrix representation. There have been two widely used algorithms and data representations. Two algorithms, scalar multiplication and dot product, can be combined with two sparse data representations, compressed sparse and bitmap formats for the matrix and vector. Although the prior accelerators adopted one of the possible designs, it is yet to be investigated which design is the best one across different hardware resources and workload characteristics. This paper first investigates the impact of design choices with respect to the algorithm and data representation. Our evaluation shows that no single design always outperforms the others across different workloads, but the two best designs (i.e., compressed sparse format and bitmap format with dot product) have complementary performance with trade-offs incurred by the matrix characteristics. Based on the analysis, this study proposes Cerberus, a triple-mode accelerator supporting two sparse operation modes in addition to the base dense mode. To allow such multi-mode operation, it proposes a prediction model based on matrix characteristics under a given hardware configuration, which statically selects the best mode for a given sparse matrix with its dimension and density information. Our experimental results show that Cerberus provides 12.1× performance improvements from a dense-only accelerator, and 1.5× improvements from a fixed best SpMV design. © 2024 Copyright held by the owner/author(s).",accelerator; Sparse Matrix-Vector Multiplication (SpMV),Acceleration; Computer hardware; Economic and social effects; Product design; Vectors; Bitmap formats; Data representations; matrix; Performance; Sparse matrices; Sparse matrix-vector multiplication; Sparse vectors; Sparse-Matrix Vector multiplications; Triple modes; Matrix algebra
FASA-DRAM: Reducing DRAM Latency with Destructive Activation and Delayed Restoration,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187444195&doi=10.1145%2f3649455&partnerID=40&md5=3744efd05263a9220a67c9dfe9dd5b15,"DRAM memory is a performance bottleneck for many applications, due to its high access latency. Previous work has mainly focused on data locality, introducing small but fast regions to cache frequently accessed data, thereby reducing the average latency. However, these locality-based designs have three challenges in modern multi-core systems: (1) inter-Application interference leads to random memory access traffic, (2) fairness issues prevent the memory controller from over-prioritizing data locality, and (3) write-intensive applications have much lower locality and evict substantial dirty entries. With frequent data movement between the fast in-DRAM cache and slow regular arrays, the overhead induced by moving data may even offset the performance and energy benefits of in-DRAM caching.In this article, we decouple the data movement process into two distinct phases. The first phase is Load-Reduced Destructive Activation (LRDA), which destructively promotes data into the in-DRAM cache. The second phase is Delayed Cycle-Stealing Restoration (DCSR), which restores the original data when the DRAM bank is idle. LRDA decouples the most time-consuming restoration phase from activation, and DCSR hides the restoration latency through prevalent bank-level parallelism. We propose FASA-DRAM, incorporating destructive activation and delayed restoration techniques to enable both in-DRAM caching and proactive latency-hiding mechanisms. Our evaluation shows that FASA-DRAM improves the average performance by 19.9% and reduces average DRAM energy consumption by 18.1% over DDR4 DRAM for four-core workloads, with less than 3.4% extra area overhead. Furthermore, FASA-DRAM outperforms state-of-The-Art designs in both performance and energy efficiency. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",DRAM energy consumption; DRAM latency; In-DRAM cache,Benchmarking; Cache memory; Chemical activation; Data reduction; Dynamic random access storage; Energy efficiency; Restoration; Access latency; Cycle-stealing; Data locality; Data movements; DRAM energy consumption; DRAM latency; Energy-consumption; In-DRAM cache; Performance; Performance bottlenecks; Energy utilization
Tyche: An Efficient and General Prefetcher for Indirect Memory Accesses,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195036929&doi=10.1145%2f3641853&partnerID=40&md5=201780c5c1abe39827be11dc3b9ca7ca,"Indirect memory accesses (IMAs, i.e., A[f(B[i])]) are typical memory access patterns in applications such as graph analysis, machine learning, and database. IMAs are composed of producer-consumer pairs, where the consumers' memory addresses are derived from the producers' memory data. Due to the built-in value-dependent feature, IMAs exhibit poor locality, making prefetching ineffective. Hindered by the challenges of recording the potentially complex graphs of instruction dependencies among IMA producers and consumers, current state-of-The-Art hardware prefetchers either (a) exhibit inadequate IMA identification abilities or (b) rely on the run-Ahead mechanism to prefetch IMAs intermittently and insufficiently.To solve this problem, we propose Tyche,1 an efficient and general hardware prefetcher to enhance IMA performance. Tyche adopts a bilateral propagation mechanism to precisely excavate the instruction dependencies in simple chains with moderate length (rather than complex graphs). Based on the exact instruction dependencies, Tyche can accurately identify various IMA patterns, including nonlinear ones, and generate accurate prefetching requests continuously. Evaluated on broad benchmarks, Tyche achieves an average performance speedup of 16.2% over the state-of-The-Art spatial prefetcher Berti. More importantly, Tyche outperforms the state-of-The-Art IMA prefetchers IMP, Gretch, and Vector Runahead, by 15.9%, 12.8%, and 10.7%, respectively, with a lower storage overhead of only 0.57 KB. © 2024 Copyright held by the owner/author(s).",Data prefetching; hardware prefetching; indirect memory accesses; microarchitecture,Digital storage; Data pre-fetching; Data-prefetching; Hardware prefetching; Indirect memory access; Memory access; Memory access patterns; Micro architectures; Performance; Prefetching; State of the art; Benchmarking
SLAP: Segmented Reuse-Time-Label Based Admission Policy for Content Delivery Network Caching,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194420545&doi=10.1145%2f3646550&partnerID=40&md5=b12aa3abf672631eabafe8f24b1f7328,"""Learned""admission policies have shown promise in improving Content Delivery Network (CDN) cache performance and lowering operational costs. Unfortunately, existing learned policies are optimized with a few fixed cache sizes while in reality, cache sizes often vary over time in an unpredictable manner. As a result, existing solutions cannot provide consistent benefits in production settings.We present SLAP, a learned CDN cache admission approach based on segmented object reuse time prediction. SLAP predicts an object's reuse time range using the Long-Short-Term-Memory model and admits objects that will be reused (before eviction) given the current cache size. SLAP decouples model training from cache size, allowing it to adapt to arbitrary sizes. The key to our solution is a novel segmented labeling scheme that makes SLAP without requiring precise prediction on object reuse time. To further make SLAP a practical and efficient solution, we propose aggressive reusing of computation and training on sampled traces to optimize model training, and a specialized predictor architecture that overlaps prediction computation with miss object fetching to optimize model inference. Our experiments using production CDN traces show that SLAP achieves significantly lower write traffic (38%-59%), longer SSDs lifetime (104%-178%), a consistently higher hit rate (3.2%-11.7%), and requires no effort to adapt to changing cache sizes, outperforming existing policies. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",admission policy; Content delivery network; segmented,Admission policies; Cache performance; Cache size; Content delivery network; Model training; Network caching; Object reuse; Reuse; Segmented; Time label; Forecasting
An Instruction Inflation Analyzing Framework for Dynamic Binary Translators,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194391315&doi=10.1145%2f3640813&partnerID=40&md5=8eb07ae4bb4baa63daf6019de9e58e5a,"Dynamic binary translators (DBTs) are widely used to migrate applications between different instruction set architectures (ISAs). Despite extensive research to improve DBT performance, noticeable overhead remains, preventing near-native performance, especially when translating from complex instruction set computer (CISC) to reduced instruction set computer (RISC). For computational workloads, the main overhead stems from translated code quality. Experimental data show that state-of-the-art DBT products have dynamic code inflation of at least 1.46. This indicates that on average, more than 1.46 host instructions are needed to emulate one guest instruction. Worse, inflation closely correlates with translated code quality. However, the detailed sources of instruction inflation remain unclear.To understand the sources of inflation, we present Deflater, an instruction inflation analysis framework comprising a mathematical model, a collection of black-box unit tests called BenchMIAOes, and a trace-based simulator called InflatSim. The mathematical model calculates overall inflation based on the inflation of individual instructions and translation block optimizations. BenchMIAOes extract model parameters from DBTs without accessing DBT source code. InflatSim implements the model and uses the extracted parameters from BenchMIAOes to simulate a given DBT's behavior. Deflater is a valuable tool to guide DBT analysis and improvement. Using Deflater, we simulated inflation for three state-of-the-art CISC-to-RISC DBTs: ExaGear, Rosetta2, and LATX, with inflation errors of 5.63%, 5.15%, and 3.44%, respectively for SPEC CPU 2017, gaining insights into these commercial DBTs. Deflater also efficiently models inflation for the open source DBT QEMU and suggests optimizations that can substantially reduce inflation. Implementing the suggested optimizations confirms Deflater's effective guidance, with 4.65% inflation error, and gains 5.47x performance improvement. © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesDynamic binary translation; overhead analysis; translation inflation,Computer architecture; Additional key word and phrasesdynamic binary translation; Binary translation; Code quality; Instruction set; Key words; Optimisations; Overhead analyse; Performance; Reduced instruction set computers; Translation inflation; Surveying
Cost-Aware Service Placement and Scheduling in the Edge-Cloud Continuum,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193585724&doi=10.1145%2f3640823&partnerID=40&md5=f542ce7636e7abb6db64217741418837,"The edge to data center computing continuum is the aggregation of computing resources located anywhere between the network edge (e.g., close to 5G antennas), and servers in traditional data centers. Kubernetes is the de facto standard for the orchestration of services in data center environments, where it is very efficient. It, however, fails to give the same performance when including edge resources. At the edge, resources are more limited, and networking conditions are changing over time.In this article, we present a methodology that lowers the costs of running applications in the edge-To-cloud computing continuum. This methodology can adapt to changing environments, e.g., moving end-users. We are also monitoring some Key Performance Indicators of the applications to ensure that cost optimizations do not negatively impact their Quality of Service. In addition, to ensure that performances are optimal even when users are moving, we introduce a background process that periodically checks if a better location is available for the service and, if so, moves the service. To demonstrate the performance of our scheduling approach, we evaluate it using a vehicle cooperative perception use case, a representative 5G application. With this use case, we can demonstrate that our scheduling approach can robustly lower the cost in different scenarios, while other approaches that are already available fail in either being adaptive to changing environments or will have poor cost-effectiveness in some scenarios.  © 2024 Copyright held by the owner/author(s).",5G; Cloud computing; container orchestration; edge computing; Kubernetes; resource allocation; scheduling,5G mobile communication systems; Benchmarking; Cost effectiveness; Quality of service; 5g; Changing environment; Cloud-computing; Container orchestration; Datacenter; Edge computing; Edge resources; Kubernetes; Performance; Resources allocation; Edge computing
Winols: A Large-Tiling Sparse Winograd CNN Accelerator on FPGAs,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194378174&doi=10.1145%2f3643682&partnerID=40&md5=97e95f677eccb9717c732917330dd13f,"Convolutional Neural Networks (CNNs) can benefit from the computational reductions provided by the Winograd minimal filtering algorithm and weight pruning. However, harnessing the potential of both methods simultaneously introduces complexity in designing pruning algorithms and accelerators. Prior studies aimed to establish regular sparsity patterns in the Winograd domain, but they were primarily suited for small tiles, with domain transformation dictating the sparsity ratio. The irregularities in data access and domain transformation pose challenges in accelerator design, especially for larger Winograd tiles. This paper introduces ""Winols,""an innovative algorithm-hardware co-design strategy that emphasizes the strengths of the large-Tiling Winograd algorithm. Through a spatial-To-Winograd relevance degree evaluation, we extensively explore domain transformation and propose a cross-domain pruning technique that retains sparsity across both spatial and Winograd domains. To compress pruned weight matrices, we invent a relative column encoding scheme. We further design an FPGA-based accelerator for CNN models with large Winograd tiles and sparse matrix-vector operations. Evaluations indicate our pruning method achieves up to 80% weight tile sparsity in the Winograd domain without compromising accuracy. Our Winols accelerator outperforms dense accelerator by a factor of 31.7× in inference latency. When compared with prevailing sparse Winograd accelerators, Winols reduces latency by an average of 10.9×, and improves DSP and energy efficiencies by over 5.6× and 5.7×, respectively. When compared with the CPU and GPU platform, Winols accelerator with tile size 8× 8 achieves 24.6× and 2.84× energy efficiency improvements, respectively. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cross-domain pruning; Large-Tiling; sparse CNNs; Winograd accelerator,Convolutional neural networks; Energy efficiency; Integrated circuit design; Metadata; Computational reduction; Convolutional neural network; Cross-domain; Cross-domain pruning; Domain transformation; Filtering algorithm; Large-tiling; Sparse convolutional neural network; Winograd; Winograd accelerator; Field programmable gate arrays (FPGA)
An Efficient Hybrid Deep Learning Accelerator for Compact and Heterogeneous CNNs,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194321471&doi=10.1145%2f3639823&partnerID=40&md5=67d964379590bb4095aa9231e20a6e39,"Resource-efficient Convolutional Neural Networks (CNNs) are gaining more attention. These CNNs have relatively low computational and memory requirements. A common denominator among such CNNs is having more heterogeneity than traditional CNNs. This heterogeneity is present at two levels: intra-layer type and inter-layer type. Generic accelerators do not capture these levels of heterogeneity, which harms their efficiency. Consequently, researchers have proposed model-specific accelerators with dedicated engines. When designing an accelerator with dedicated engines, one option is to dedicate one engine per CNN layer. We refer to accelerators designed with this approach as single-engine single-layer (SESL). This approach enables optimizing each engine for its specific layer. However, such accelerators are resource-demanding and unscalable. Another option is to design a minimal number of dedicated engines such that each engine handles all layers of one type. We refer to these accelerators as single-engine multiple-layer (SEML). SEML accelerators capture the inter-layer-Type but not the intra-layer-Type heterogeneity.We propose the Fixed Budget Hybrid CNN Accelerator (FiBHA), a hybrid accelerator composed of an SESL part and an SEML part, each processing a subset of CNN layers. FiBHA captures more heterogeneity than SEML while being more resource-Aware and scalable than SESL. Moreover, we propose a novel module, Fused Inverted Residual Bottleneck (FIRB), a fine-grained and memory-light SESL architecture building block. The proposed architecture is implemented and evaluated using high-level synthesis (HLS) on different Field Programmable Gate Arrays representing various resource budgets. Our evaluation shows that FiBHA improves the throughput by up to 4x and 2.5x compared to state-of-The-Art SESL and SEML accelerators, respectively. Moreover, FiBHA reduces memory and energy consumption compared to an SEML accelerator. The evaluation also shows that FIRB reduces the required memory by up to 54%, and energy requirements by up to 35% compared to traditional pipelining. © 2024 Copyright held by the owner/author(s).",Convolutional neural networks (CNNs); deep learning; FPGA; hardware software co-design; hybrid accelerator; pipelined accelerator,Budget control; Convolution; Convolutional neural networks; Deep neural networks; Energy utilization; Engines; Hardware-software codesign; High level synthesis; Integrated circuit design; Memory architecture; Network architecture; Pipeline processing systems; Pipelines; Convolutional neural network; Deep learning; Fixed budget; Hardware/software codesign; Hybrid accelerator; Multiple layers; Pipelined accelerator; Single engines; Single layer; Field programmable gate arrays (FPGA)
Highly Efficient Self-checking Matrix Multiplication on Tiled AMX Accelerators,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194364597&doi=10.1145%2f3633332&partnerID=40&md5=0784fe2cb86ac591fcb9811c76da0dfe,"General Matrix Multiplication (GEMM) is a computationally expensive operation that is used in many applications such as machine learning. Hardware accelerators are increasingly popular for speeding up GEMM computation, with Tiled Matrix Multiplication (TMUL) in recent Intel processors being an example. Unfortunately, the TMUL hardware is susceptible to errors, necessitating online error detection. The Algorithm-based Error Detection (ABED) technique is a powerful technique to detect errors in matrix multiplications. In this article, we consider implementation of an ABED technique that integrates seamlessly with the TMUL hardware to minimize performance overhead. Unfortunately, rounding errors introduced by floating-point operations do not allow a straightforward implementation of ABED in TMUL. Previously an error bound was considered for addressing rounding errors in ABED. If the error detection threshold is set too low, it will a trigger false alarm, while a loose bound will allow errors to escape detection. In this article, we propose an adaptive error threshold that takes into account the TMUL input values to address the problem of false triggers and error escapes and provide a taxonomy of various error classes. This threshold is obtained from theoretical error analysis but is not easy to implement in hardware. Consequently, we relax the threshold such that it can be easily computed in hardware. While ABED ensures error-free computation, it does not guarantee full coverage of all hardware faults. To address this problem, we propose an algorithmic pattern generation technique to ensure full coverage for all hardware faults. To evaluate the benefits of our proposed solution, we conducted fault injection experiments and show that our approach does not produce any false alarms or detection escapes for observable errors. We conducted additional fault injection experiments on a Deep Neural Network (DNN) model and find that if a fault is not detected, it does not cause any misclassification. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",ABED; GEMM computation; hardware acceleration,Deep neural networks; Digital arithmetic; Matrix algebra; Software testing; Algorithm-based error detection; Error-detection techniques; Falsealarms; Fault injection; General matrix multiplication computation; Hardware acceleration; Hardware faults; MAtrix multiplication; Rounding errors; Self checking; Error detection
Coherence Attacks and Countermeasures in Interposer-based Chiplet Systems,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194401768&doi=10.1145%2f3633461&partnerID=40&md5=4a3a7e10ded72c11ed5a0951e5b18210,"Industry is moving towards large-scale hardware systems that bundle processor cores, memories, accelerators, and so on. via 2.5D integration. These components are fabricated separately as chiplets and then integrated using an interposer as an interconnect carrier. This new design style is beneficial in terms of yield and economies of scale, as chiplets may come from various vendors and are relatively easy to integrate into one larger sophisticated system. However, the benefits of this approach come at the cost of new security challenges, especially when integrating chiplets that come from untrusted or not fully trusted, third-party vendors.In this work, we explore these challenges for modern interposer-based systems of cache-coherent, multi-core chiplets. First, we present basic coherence-oriented hardware Trojan attacks that pose a significant threat to chiplet-based designs and demonstrate how these basic attacks can be orchestrated to pose a significant threat to interposer-based systems. Second, we propose a novel scheme using an active interposer as a generic, secure-by-construction platform that forms a physical root of trust for modern 2.5D systems. The implementation of our scheme is confined to the interposer, resulting in little cost and leaving the chiplets and coherence system untouched. We show that our scheme prevents a range of coherence attacks with low overheads on system performance, g1/44%. Further, we demonstrate that our scheme scales efficiently as system size and memory capacities increase, resulting in reduced performance overheads. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Coherence attacks; Coherence systems; countermeasures; interposer Technology,Computer hardware; Economics; Hardware security; 2.5-D integration; Coherence attack; Coherence system; Countermeasure; Design styles; Economy of scale; Hardware system; Interposer technology; Large-scales; Processor cores; Malware
Extension VM: Interleaved Data Layout in Vector Memory,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186115918&doi=10.1145%2f3631528&partnerID=40&md5=ed2be75eeacef8e78594d500650c41c2,"While vector architecture is widely employed in processors for neural networks, signal processing, and high-performance computing; however, its performance is limited by inefficient column-major memory access. The column-major access limitation originates from the unsuitable mapping of multidimensional data structures to two-dimensional vector memory spaces. In addition, the traditional data layout mapping method creates an irreconcilable conflict between row- and column-major accesses. Ideally, both row- and column-major accesses can take advantage of the bank parallelism of vector memory. To this end, we propose the Interleaved Data Layout (IDL) method in vector memory, which can distribute vector elements into different banks regardless of whether they are in the row- or column-major category, so that any vector memory access can benefit from bank parallelism. Additionally, we propose an Extension Vector Memory (EVM) architecture to achieve IDL in vector memory. EVM can support two data layout methods and vector memory access modes simultaneously. The key idea is to continuously distribute the data that needs to be accessed from the main memory to different banks during the loading period. Thus, EVM can provide a larger spatial locality level through careful programming and the extension ISA support. The experimental results showed a 1.43-fold improvement of state-of-the-art vector processors by the proposed architecture, with an area cost of only 1.73%. Furthermore, the energy consumption was reduced by 50.1%. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",data layout; processing-in-memory; Vector architecture; vector memory,Data handling; Energy utilization; Mapping; Memory architecture; Network architecture; Signal processing; Vector spaces; Data layouts; Layout methods; Memory access; Neural-networks; Performance; Performance computing; Processing-in-memory; Signal-processing; Vector architectures; Vector memory; Vectors
WIPE: A Write-Optimized Learned Index for Persistent Memory,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189939023&doi=10.1145%2f3634915&partnerID=40&md5=bea90cd5783375dd6df3f0045234be12,"Learned Index, which utilizes effective machine learning models to accelerate locating sorted data positions, has gained increasing attention in many big data scenarios. Using efficient learned models, the learned indexes build large nodes and flat structures, thereby greatly improving the performance. However, most of the state-of-the-art learned indexes are designed for DRAM, and there is hence an urgent need to enable high-performance learned indexes for emerging Non-Volatile Memory (NVM). In this article, we first evaluate and analyze the performance of the existing learned indexes on NVM. We discover that these learned indexes encounter severe write amplification and write performance degradation due to the requirements of maintaining large sorted/semi-sorted data nodes. To tackle the problems, we propose a novel three-tiered architecture of write-optimized persistent learned index, which is named WIPE, by adopting unsorted fine-granularity data nodes to achieve high write performance on NVM. Thereinto, we devise a new root node construction algorithm to accelerate searching numerous small data nodes. The algorithm ensures stable flat structure and high read performance in large-size datasets by introducing an intermediate layer (i.e., index nodes) and achieving accurate prediction of index node positions from the root node. Our extensive experiments on Intel DCPMM show that WIPE can improve write throughput and read throughput by up to 3.9× and 7×, respectively, compared to the state-of-the-art learned indexes. Also, WIPE can recover from a system crash in ∼18 ms. WIPE is free as an open-source software package. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",key-value store; learned index; NVM-based learned index; persistent index,Large datasets; Open source software; Open systems; Flat structures; Index nodes; Key-value stores; Learned index; Non-volatile memory; Non-volatile memory-based learned index; Performance; Persistent index; Root nodes; State of the art; Dynamic random access storage
Dedicated Hardware Accelerators for Processing of Sparse Matrices and Vectors: A Survey,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194371726&doi=10.1145%2f3640542&partnerID=40&md5=c43bd14b7ee94c45b4add7b2557f9888,"Performance in scientific and engineering applications such as computational physics, algebraic graph problems or Convolutional Neural Networks (CNN), is dominated by the manipulation of large sparse matrices-matrices with a large number of zero elements. Specialized software using data formats for sparse matrices has been optimized for the main kernels of interest: SpMV and SpMSpM matrix multiplications, but due to the indirect memory accesses, the performance is still limited by the memory hierarchy of conventional computers. Recent work shows that specific hardware accelerators can reduce memory traffic and improve the execution time of sparse matrix multiplication, compared to the best software implementations. The performance of these sparse hardware accelerators depends on the choice of the sparse format, COO, CSR, etc, the algorithm, inner-product, outer-product, Gustavson, and many hardware design choices. In this article, we propose a systematic survey which identifies the design choices of state-of-The-Art accelerators for sparse matrix multiplication kernels. We introduce the necessary concepts and then present, compare, and classify the main sparse accelerators in the literature, using consistent notations. Finally, we propose a taxonomy for these accelerators to help future designers make the best choices depending on their objectives. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",accelerators; High performance computing (HPC); memory hierarchy; sparse matrices,Computer hardware; Memory architecture; Product design; Dedicated hardware; Hardware accelerators; High performance computing; MAtrix multiplication; Memory hierarchy; Performance; Performance computing; Scientific and engineering applications; Sparse matrices; Sparse vectors; Matrix algebra
ApHMM: Accelerating Profile Hidden Markov Models for Fast and Energy-efficient Genome Analysis,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186120812&doi=10.1145%2f3632950&partnerID=40&md5=3fd9ef31fc65aac92bc8dd34a5b7e550,"Profile hidden Markov models (pHMMs) are widely employed in various bioinformatics applications to identify similarities between biological sequences, such as DNA or protein sequences. In pHMMs, sequences are represented as graph structures, where states and edges capture modifications (i.e., insertions, deletions, and substitutions) by assigning probabilities to them. These probabilities are subsequently used to compute the similarity score between a sequence and a pHMM graph. The Baum-Welch algorithm, a prevalent and highly accurate method, utilizes these probabilities to optimize and compute similarity scores. Accurate computation of these probabilities is essential for the correct identification of sequence similarities. However, the Baum-Welch algorithm is computationally intensive, and existing solutions offer either software-only or hardware-only approaches with fixed pHMM designs. When we analyze state-of-the-art works, we identify an urgent need for a flexible, high-performance, and energy-efficient hardware-software co-design to address the major inefficiencies in the Baum-Welch algorithm for pHMMs. We introduce ApHMM, the first flexible acceleration framework designed to significantly reduce both computational and energy overheads associated with the Baum-Welch algorithm for pHMMs. ApHMM employs hardware-software co-design to tackle the major inefficiencies in the Baum-Welch algorithm by (1) designing flexible hardware to accommodate various pHMM designs, (2) exploiting predictable data dependency patterns through on-chip memory with memoization techniques, (3) rapidly filtering out unnecessary computations using a hardware-based filter, and (4) minimizing redundant computations. ApHMM achieves substantial speedups of 15.55×–260.03×, 1.83×–5.34×, and 27.97× when compared to CPU, GPU, and FPGA implementations of the Baum-Welch algorithm, respectively. ApHMM outperforms state-of-the-art CPU implementations in three key bioinformatics applications: (1) error correction, (2) protein family search, and (3) multiple sequence alignment, by 1.29×–59.94×, 1.03×–1.75×, and 1.03×–1.95×, respectively, while improving their energy efficiency by 64.24×–115.46×, 1.75×, and 1.96×. © 2024 Copyright held by the owner/author(s).",Bioinformatics; genomics; profile hidden markov models; the Baum-Welch Algorithm,Bioinformatics; Computer hardware; DNA sequences; Energy efficiency; Error correction; Genes; Hardware-software codesign; Hidden Markov models; Integrated circuit design; Molecular biology; Proteins; Software design; Trellis codes; Baum-Welch algorithms; Bioinformatics applications; Energy efficient; Genomics; Hardware/software codesign; Modeling designs; Profile hidden Markov model; Similarity scores; State of the art; The baum-welch algorithm; Genome
Improving Utilization of Dataflow Unit for Multi-Batch Processing,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186121068&doi=10.1145%2f3637906&partnerID=40&md5=580304ce232df5f7dead914c946f31f8,"Dataflow architectures can achieve much better performance and higher efficiency than general-purpose core, approaching the performance of a specialized design while retaining programmability. However, advanced application scenarios place higher demands on the hardware in terms of cross-domain and multi-batch processing. In this article, we propose a unified scale-vector architecture that can work in multiple modes and adapt to diverse algorithms and requirements efficiently. First, a novel reconfigurable interconnection structure is proposed, which can organize execution units into different cluster typologies as a way to accommodate different data-level parallelism. Second, we decouple threads within each DFG node into consecutive pipeline stages and provide architectural support. By time-multiplexing during these stages, dataflow hardware can achieve much higher utilization and performance. In addition, the task-based program model can also exploit multi-level parallelism and deploy applications efficiently. Evaluated in a wide range of benchmarks, including digital signal processing algorithms, CNNs, and scientific computing algorithms, our design attains up to 11.95× energy efficiency (performance-per-watt) improvement over GPU (V100), and 2.01× energy efficiency improvement over state-of-the-art dataflow architectures. © 2024 Copyright held by the owner/author(s).",batch processing; decoupled architecture; network-on-chip; Utilization,Application programs; Batch data processing; Benchmarking; Computer architecture; Computer hardware; Digital signal processing; Energy efficiency; Integrated circuit design; Network architecture; Batch processing; Data-flow architectures; Dataflow; Decoupled architecture; Higher efficiency; Networks on chips; Performance; Performance efficiency; Programmability; Utilization; Network-on-chip
Exploring Data Layout for Sparse Tensor Times Dense Matrix on GPUs,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186112219&doi=10.1145%2f3633462&partnerID=40&md5=2deed596425bd8ad5c0f5263057e6519,"An important sparse tensor computation is sparse-tensor-dense-matrix multiplication (SpTM), which is used in tensor decomposition and applications. SpTM is a multi-dimensional analog to sparse-matrix-dense-matrix multiplication (SpMM). In this article, we employ a hierarchical tensor data layout that can unfold a multidimensional tensor to derive a 2D matrix, making it possible to compute SpTM using SpMM kernel implementations for GPUs. We compare two SpMM implementations to the state-of-the-art PASTA sparse tensor contraction implementation using: (1) SpMM with hierarchical tensor data layout; and, (2) unfolding followed by an invocation of cuSPARSE’s SpMM. Results show that SpMM can outperform PASTA 70.9% of the time, but none of the three approaches is best overall. Therefore, we use a decision tree classifier to identify the best performing sparse tensor contraction kernel based on precomputed properties of the sparse tensor. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",data layout; Sparse tensors; SpMM,Decision trees; Matrix algebra; Program processors; Data layouts; Dense matrix; Hierarchical tensors; MAtrix multiplication; Multi dimensional; Sparse matrices; Sparse tensors; Sparse-matrix-dense-matrix multiplication; Tensor contraction; Tensor decomposition; Tensors
Assessing the Impact of Compiler Optimizations on GPUs Reliability,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192626836&doi=10.1145%2f3638249&partnerID=40&md5=455abcd767c7aa2b3f10d4f9f37b9291,"Graphics Processing Units (GPUs) compilers have evolved in order to support general-purpose programming languages for multiple architectures. NVIDIA CUDA Compiler (NVCC) has many compilation levels before generating the machine code and applies complex optimizations to improve performance. These optimizations modify how the software is mapped in the underlying hardware; thus, as we show in this article, they can also affect GPU reliability. We evaluate the effects on the GPU error rate of the optimization flags applied at the NVCC Parallel Thread Execution (PTX) compiling phase by analyzing two NVIDIA GPU architectures (Kepler and Volta) and two compiler versions (NVCC 10.2 and 11.3). We compare and combine fault propagation analysis based on software fault injection, hardware utilization distribution obtained with application-level profiling, and machine instructions radiation-induced error rate measured with beam experiments. We consider eight different workloads and 144 combinations of compilation flags, and we show that optimizations can impact the GPUs' error rate of up to an order of magnitude. Additionally, through accelerated neutron beam experiments on a NVIDIA Kepler GPU, we show that the error rate of the unoptimized GEMM (-O0 flag) is lower than the optimized GEMM's (-O3 flag) error rate. When the performance is evaluated together with the error rate, we show that the most optimized versions (-O1 and-O3) always produce a higher amount of correct data than the unoptimized code (-O0). © 2024 Copyright held by the owner/author(s).",error rate; Graphics processing units; neutron-induced errors; reliability; reliability,Application programs; Codes (symbols); Computer graphics; Computer hardware; Errors; Program compilers; Reliability analysis; Software reliability; Compiler optimizations; Complex optimization; Error rate; Fault propagation; General-purpose programming language; Improve performance; Machine codes; Neutron-induced error; Optimisations; Propagation analysis; Graphics processing unit
A Concise Concurrent B+-Tree for Persistent Memory,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194388807&doi=10.1145%2f3638717&partnerID=40&md5=edecd438917f9771aeb1436d45012712,"Persistent memory (PM) presents a unique opportunity for designing data management systems that offer improved performance, scalability, and instant restart capability. As a widely used data structure for managing data in such systems, B+-Tree must address the challenges presented by PM in both data consistency and device performance. However, existing studies suffer from significant performance degradation when maintaining data consistency on PM. To settle this problem, we propose a new concurrent B+-Tree, CC-Tree, optimized for PM. CC-Tree ensures data consistency while providing high concurrent performance, thanks to several technologies, including partitioned metadata, log-free split, and lock-free read. We conducted experiments using state-of-The-Art indices, and the results demonstrate significant performance improvements, including approximately 1.2-1.6x search, 1.5-1.7x insertion, 1.5-2.8x update, 1.9-4x deletion, 0.9-10x range scan, and up to 1.55-1.82x in hybrid workloads. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",B<sup>+</sup>-Tree; Persistent memory,Concurrency control; Database systems; Information management; B trees; Data consistency; Data management system; Device performance; Lock-free; Performance; Performance degradation; Performance scalability; Persistent memory; State of the art; Trees (mathematics)
Fast Convolution Meets Low Precision: Exploring Efficient Quantized Winograd Convolution on Modern CPUs,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186111726&doi=10.1145%2f3632956&partnerID=40&md5=305f91d0fc415edeb243fa740a3989b1,"Low-precision computation has emerged as one of the most effective techniques for accelerating convolutional neural networks and has garnered widespread support on modern hardware. Despite its effectiveness in accelerating convolutional neural networks, low-precision computation has not been commonly applied to fast convolutions, such as the Winograd algorithm, due to numerical issues. In this article, we propose an effective quantized Winograd convolution, named LoWino, which employs an in-side quantization method in the Winograd domain to reduce the precision loss caused by transformations. Meanwhile, we present an efficient implementation that integrates well-designed optimization techniques, allowing us to fully exploit the capabilities of low-precision computation on modern CPUs. We evaluate LoWino on two Intel Xeon Scalable Processor platforms with representative convolutional layers and neural network models. The experimental results demonstrate that our approach can achieve an average of 1.84× and 1.91× operator speedups over state-of-the-art implementations in the vendor library while preserving accuracy loss at a reasonable level. © 2024 Copyright held by the owner/author(s).",Deep learning; low-precision computation; winograd convolution,Deep learning; Multilayer neural networks; Network layers; Neural network models; Program processors; Convolutional neural network; Deep learning; Efficient implementation; Fast convolution; Low-precision computation; Lower precision; Quantisation; Winograd; Winograd convolution; Winograd's algorithms; Convolution
COWS for High Performance: Cost Aware Work Stealing for Irregular Parallel Loop,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186085519&doi=10.1145%2f3633331&partnerID=40&md5=16fa5189269c878dfc33852d272a7a39,"Parallel libraries such as OpenMP distribute the iterations of parallel-for-loops among the threads, using a programmer-specified scheduling policy. While the existing scheduling policies perform reasonably well in the context of balanced workloads, in computations that involve highly imbalanced workloads it is extremely non-trivial to obtain an efficient distribution of work (even using non-static scheduling methods like dynamic and guided). In this paper, we present a scheme called COst aware Work Stealing (COWS) to efficiently extend the idea of work-stealing to OpenMP. In contrast to the traditional work-stealing schedulers, COWS takes into consideration that (i) not all iter ations of a parallel-for-loops may take the same amount of time. (ii) identifying a suitable victim for stealing is important for load-balancing, and (iii) queues lead to significant overheads in traditional work-stealing and should be avoided. We present two variations of COWS: WSRI (a naive work-stealing scheme based on the number of remaining iterations) and WSRW (work-stealing scheme based on the amount of remaining workload). Since in irregular loops like those found in graph analytics it is not possible to statically compute the cost of the iterations of the parallel-for-loops, we use a combined compile-time + runtime approach, where the remaining workload of a loop is computed efficiently at runtime by utilizing the code generated by our compile-time component. We have performed an evaluation over seven different benchmark programs, using five different input datasets, on two different hardware across a varying number of threads; leading to a total number of 275 configurations. We show that in 225 out of 275 configurations, compared to the best OpenMP scheduling scheme for that configuration, our approach achieves clear performance gains. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Cost aware work stealing; High performance,Balanced work-load; Compile time; Cost aware work stealing; Cost-aware; High performance; Parallel library; Parallel loops; Performance; Performance costs; Scheduling policies; Application programming interfaces (API)
ISP Agent: A Generalized In-storage-processing Workload Offloading Framework by Providing Multiple Optimization Opportunities,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186098445&doi=10.1145%2f3632951&partnerID=40&md5=9d7744fb2f23198e0bdf3e39cc1e44a4,"As solid-state drives (SSDs) with sufficient computing power have recently become the dominant devices in modern computer systems, in-storage processing (ISP), which processes data within the storage without transferring it to the host memory, is being utilized in various emerging applications. The main challenge of ISP is to deliver storage data to the offloaded workload. This is difficult because of the information gap between the host and storage, the data consistency problem between the host and offloaded workloads, and SSD-specific hardware limitations. Moreover, because the offloaded workloads use internal SSD resources, host I/O performance might be degraded due to resource conflicts. Although several ISP frameworks have been proposed, existing ISP approaches that do not deeply consider the internal SSD behavior are often insufficient to support efficient ISP workload offloading with high programmability. In this article, we propose an ISP agent, a lightweight ISP workload offloading framework for SSD devices. The ISP agent provides I/O and memory interfaces that allow users to run existing function codes on SSDs without major code modifications, and separates the resources for the offloaded workloads from the existing SSD firmware to minimize interference with host I/O processing. The ISP agent also provides further optimization opportunities for the offloaded workload by considering SSD architectures. We have implemented the ISP agent on the OpenSSD Cosmos+ board and evaluated its performance using synthetic benchmarks and a real-world ISP-assisted database checkpointing application. The experimental results demonstrate that the ISP agent enhances host application performance while increasing ISP programmability, and that the optimization opportunities provided by the ISP agent can significantly improve ISP-side performance without compromising host I/O processing. © 2024 Association for Computing Machinery. All rights reserved.",firmware; In-storage processing; programming model; solid state drive,Benchmarking; Codes (symbols); Computing power; Computing power; In-storage processing; Modern computer systems; Multiple optimizations; Optimisations; Performance; Process data; Programmability; Programming models; Solid state drive; Firmware
Hardware-hardened Sandbox Enclaves for Trusted Serverless Computing,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186114145&doi=10.1145%2f3632954&partnerID=40&md5=72db11f97cceb33722788f5a8de739ac,"In cloud-based serverless computing, an application consists of multiple functions provided by mutually distrusting parties. For secure serverless computing, the hardware-based trusted execution environment (TEE) can provide strong isolation among functions. However, not only protecting each function from the host OS and other functions, but also protecting the host system from the functions, is critical for the security of the cloud servers. Such an emerging trusted serverless computing poses new challenges: Each TEE must be isolated from the host system bi-directionally, and the system calls from it must be validated. In addition, the resource utilization of each TEE must be accountable in a mutually trusted way. However, the current TEE model cannot efficiently represent such trusted serverless applications. To overcome the lack of such hardware support, this article proposes an extended TEE model called Cloister, designed for trusted serverless computing. Cloister proposes four new key techniques. First, it extends the hardware-based memory isolation in SGX to confine a deployed function only within its TEE (enclave). Second, it proposes a trusted monitor enclave that filters and validates system calls from enclaves. Third, it provides a trusted resource accounting mechanism for enclaves that is agreeable to both service developers and cloud providers. Finally, Cloister accelerates enclave loading by redesigning its memory verification for fast function deployment. Using an emulated Intel SGX platform with the proposed extensions, this article shows that trusted serverless applications can be effectively supported with small changes in the SGX hardware. © 2024 Copyright held by the owner/author(s).",Hardware; Security; Serverless computing; Trusted Execution Environment,Hardware security; Trusted computing; Cloud servers; Cloud-based; Environment models; Hardware; Multiple function; Resources utilizations; Security; Serverless computing; System calls; Trusted execution environments; Computer hardware
QoS-pro: A QoS-enhanced Transaction Processing Framework for Shared SSDs,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185902006&doi=10.1145%2f3632955&partnerID=40&md5=dd4c05c8b990b06224c26ac01a83f024,"Solid State Drives (SSDs) are widely used in data-intensive scenarios due to their high performance and decreasing cost. However, in shared environments, concurrent workloads can interfere with each other, leading to a violation of Quality of Service (QoS). While QoS mechanisms like fairness guarantees and latency constraints have been integrated into SSDs, existing transaction processing frameworks offer limited QoS guarantees and can significantly degrade overall performance in a shared environment. The reason is that the internal components of an SSD, originally designed to exploit parallelism, struggle to coordinate effectively when QoS mechanisms are applied to them. This article proposes a novel QoS-enhanced transaction processing framework, called QoS-pro, which enhances QoS guarantees for concurrent workloads while maintaining high parallelism for SSDs. QoS-pro achieves this by redesigning transaction processing procedures to fully exploit the parallelism of shared SSDs and enhancing QoS-oriented transaction translation and scheduling with parallelism features in mind. In terms of fairness guarantees, QoS-pro outperforms state-of-the-art methods by achieving 96% fairness improvement and 64% maximum latency reduction. QoS-pro also shows almost no loss in throughput when compared with parallelism-oriented methods. Additionally, QoS-pro triggers the fewest Garbage Collection (GC) operations and minimally affects concurrently running workloads during GC operations. © 2024 Association for Computing Machinery. All rights reserved.",address mapping; parallelism; QoS; SSD; transaction scheduling,Digital storage; Address mappings; Fairness guarantee; Parallelism; Performance; Quality of service guarantees; Quality-of-service; Service mechanism; Solid state drive; Transaction processing; Transaction scheduling; Quality of service
SAC: An Ultra-Efficient Spin-based Architecture for Compressed DNNs,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186102680&doi=10.1145%2f3632957&partnerID=40&md5=2e71258913ba0749cb93410305fa78fb,"Deep Neural Networks (DNNs) have achieved great progress in academia and industry. But they have become computational and memory intensive with the increase of network depth. Previous designs seek breakthroughs in software and hardware levels to mitigate these challenges. At the software level, neural network compression techniques have effectively reduced network scale and energy consumption. However, the conventional compression algorithm is complex and energy intensive. At the hardware level, the improvements in the semiconductor process have effectively reduced power and energy consumption. However, it is difficult for the traditional Von-Neumann architecture to further reduce the power consumption, due to the memory wall and the end of Moore’s law. To overcome these challenges, the spintronic device based DNN machines have emerged for their non-volatility, ultra low power, and high energy efficiency. However, there is no spin-based design that has achieved innovation at both the software and hardware level. Specifically, there is no systematic study of spin-based DNN architecture to deploy compressed networks. In our study, we present an ultra-efficient Spin-based Architecture for Compressed DNNs (SAC), to substantially reduce power consumption and energy consumption. Specifically, we propose a One-Step Compression algorithm (OSC) to reduce the computational complexity with minimum accuracy loss. We also propose a spin-based architecture to realize better performance for the compressed network. Furthermore, we introduce a novel computation flow that enables the reuse of activations and weights. Experimental results show that our study can reduce the computational complexity of compression algorithm from O(Tk3) to O(k2 logk), and achieve 14× ∼ 40× compression ratio. Furthermore, our design can attain a 2× enhancement in power efficiency and a 5× improvement in computational efficiency compared to the Eyeriss. Our models are available at an anonymous link https://bit.ly/39cdtTa. © 2024 Copyright held by the owner/author(s).",Artificial intelligence chips; Neural network hardware; Neuromorphic computing,Complex networks; Computational complexity; Computational efficiency; Computing power; Electric power utilization; Energy efficiency; Memory architecture; Network architecture; Artificial intelligence chip; Compression algorithms; Energy-consumption; Network compression; Network depths; Neural network hardware; Neural-networks; Neuromorphic computing; Software and hardwares; Ultra-efficient; Deep neural networks
Fine-grain Quantitative Analysis of Demand Paging in Unified Virtual Memory,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186127546&doi=10.1145%2f3632953&partnerID=40&md5=72496dc6c336f00d21eed951fcaaf891,"The abstraction of a shared memory space over separate CPU and GPU memory domains has eased the burden of portability for many HPC codebases. However, users pay for ease of use provided by system-managed memory with a moderate-to-high performance overhead. NVIDIA Unified Virtual Memory (UVM) is currently the primary real-world implementation of such abstraction and offers a functionally equivalent testbed for in-depth performance study for both UVM and future Linux Heterogeneous Memory Management (HMM) compatible systems. The continued advocacy for UVM and HMM motivates improvement of the underlying system. We focus on UVM-based systems and investigate the root causes of UVM overhead, a non-trivial task due to complex interactions of multiple hardware and software constituents and the desired cost granularity. In our prior work, we delved deeply into UVM system architecture and showed internal behaviors of page fault servicing in batches. We provided quantitative evaluation of batch handling for various applications under different scenarios, including prefetching and oversubscription. We revealed that the driver workload depends on the interactions among application access patterns, GPU hardware constraints, and host OS components. Host OS components have significant overhead present across implementations, warranting close attention. This extension furthers our prior study in three aspects: fine-grain cost analysis and breakdown, extension to multiple GPUs, and investigation of platforms with different GPU-GPU interconnects. We take a top-down approach to quantitative batch analysis and uncover how constituent component costs accumulate and overlap, governed by synchronous and asynchronous operations. Our multi-GPU analysis shows reduced cost of GPU-GPU batch workloads compared to CPU-GPU workloads. We further demonstrate that while specialized interconnects, NVLink, can improve batch cost, their benefits are limited by host OS software overhead and GPU oversubscription. This study serves as a proxy for future shared memory systems, such as those that interface with HMM, and the development of interconnects. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Accelerated Computing; GPGPU; Heterogeneous Memory Management; Unified Virtual Memory; Virtual Memory,Abstracting; Computer operating systems; Graphics processing unit; Memory architecture; Program processors; Accelerated computing; Demand paging; Finer grains; GPGPU; Heterogeneous memory; Heterogeneous memory management; Memory-management; Unified virtual memory; Virtual memory; Cost benefit analysis
Abakus: Accelerating k-mer Counting with Storage Technology,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186120752&doi=10.1145%2f3632952&partnerID=40&md5=807dca32d4c88bff653de6e4c5e209d1,"This work seeks to leverage Processing-with-storage-technology (PWST) to accelerate a key bioinformatics kernel called k-mer counting, which involves processing large files of sequence data on the disk to build a histogram of fixed-size genome sequence substrings and thereby entails prohibitively high I/O overhead. In particular, this work proposes a set of accelerator designs called Abakus that offer varying degrees of tradeoffs in terms of performance, efficiency, and hardware implementation complexity. The key to these designs is a set of domain-specific hardware extensions to accelerate the key operations for k-mer counting at various levels of the SSD hierarchy, with the goal of enhancing the limited computing capabilities of conventional SSDs, while exploiting the parallelism of the multi-channel, multi-way SSDs. Our evaluation suggests that Abakus can achieve 8.42×, 6.91×, and 2.32× speedup over the CPU-, GPU-, and near-data processing solutions. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",application-specific acceleration; bioinformatics; Computer architecture; storage device,Bioinformatics; Data handling; Virtual storage; Accelerator design; Application specific; Application-specific acceleration; Fixed size; Genome sequences; Large files; Sequence data; Storage device; Storage technology; Sub-strings; Computer architecture
QuCloud+: A Holistic Qubit Mapping Scheme for Single/Multi-programming on 2D/3D NISQ Quantum Computers,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186111632&doi=10.1145%2f3631525&partnerID=40&md5=97180bd865eea82136723e1f6e94d35d,"Qubit mapping for NISQ superconducting quantum computers is essential to fidelity and resource utilization. The existing qubit mapping schemes meet challenges, e.g., crosstalk, SWAP overheads, diverse device topologies, etc., leading to qubit resource underutilization and low fidelity in computing results. This article introduces QuCloud+, a new qubit mapping scheme that tackles these challenges. QuCloud+ has several new designs. (1) QuCloud+ supports single/multi-programming quantum computing on quantum chips with 2D/3D topology. (2) QuCloud+ partitions physical qubits for concurrent quantum programs with the crosstalk-aware community detection technique and further allocates qubits according to qubit degree, improving fidelity, and resource utilization. (3) QuCloud+ includes an X-SWAP mechanism that avoids SWAPs with high crosstalk errors and enables inter-program SWAPs to reduce the SWAP overheads. (4) QuCloud+ schedules concurrent quantum programs to be mapped and executed based on estimated fidelity for the best practice. Experimental results show that, compared with the existing typical multi-programming study [12], QuCloud+ achieves up to 9.03% higher fidelity and saves on the required SWAPs during mapping, reducing the number of CNOT gates inserted by 40.92%. Compared with a recent study [30] that enables post-mapping gate optimizations to further reduce gates, QuCloud+ reduces the post-mapping circuit depth by 21.91% while using a similar number of gates. © 2024 Copyright held by the owner/author(s).",fidelity; multi-programming; Quantum computing; Quantum OS; qubit mapping,Crosstalk; Qubits; Topology; Diverse devices; Fidelity; Low fidelities; Mapping scheme; Multi-programming; Quanta computers; Quantum Computing; Quantum OS; Qubit mapping; Resources utilizations; Mapping
Rcmp: Reconstructing RDMA-Based Memory Disaggregation via CXL,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186118091&doi=10.1145%2f3634916&partnerID=40&md5=a3ec3d77ab31540ccf41231b2283a2a5,"Memory disaggregation is a promising architecture for modern datacenters that separates compute and memory resources into independent pools connected by ultra-fast networks, which can improve memory utilization, reduce cost, and enable elastic scaling of compute and memory resources. However, existing memory disaggregation solutions based on remote direct memory access (RDMA) suffer from high latency and additional overheads including page faults and code refactoring. Emerging cache-coherent interconnects such as CXL offer opportunities to reconstruct high-performance memory disaggregation. However, existing CXL-based approaches have physical distance limitation and cannot be deployed across racks. In this article, we propose Rcmp, a novel low-latency and highly scalable memory disaggregation system based on RDMA and CXL. The significant feature is that Rcmp improves the performance of RDMA-based systems via CXL, and leverages RDMA to overcome CXL’s distance limitation. To address the challenges of the mismatch between RDMA and CXL in terms of granularity, communication, and performance, Rcmp (1) provides a global page-based memory space management and enables fine-grained data access, (2) designs an efficient communication mechanism to avoid communication blocking issues, (3) proposes a hot-page identification and swapping strategy to reduce RDMA communications, and (4) designs an RDMA-optimized RPC framework to accelerate RDMA transfers. We implement a prototype of Rcmp and evaluate its performance by using micro-benchmarks and running a key-value store with YCSB benchmarks. The results show that Rcmp can achieve 5.2× lower latency and 3.8× higher throughput than RDMA-based systems. We also demonstrate that Rcmp can scale well with the increasing number of nodes without compromising performance. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",CXL; Memory disaggregation; RDMA,Cache memory; Information management; Memory architecture; Compute resources; CXL; Datacenter; Disaggregation; Low latency; Memory disaggregation; Memory resources; Performance; Remote direct memory access; Ultra-fast; Benchmarking
WA-Zone: Wear-Aware Zone Management Optimization for LSM-Tree on ZNS SSDs,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186390359&doi=10.1145%2f3637488&partnerID=40&md5=228c737a695d7558890608f82bf6d677,"ZNS SSDs divide the storage space into sequential-write zones, reducing costs of DRAM utilization, garbage collection, and over-provisioning. The sequential-write feature of zones is well-suited for LSM-based databases, where random writes are organized into sequential writes to improve performance. However, the current compaction mechanism of LSM-tree results in widely varying access frequencies (i.e., hotness) of data and thus incurs an extreme imbalance in the distribution of erasure counts across zones. The imbalance significantly limits the lifetime of SSDs. Moreover, the current zone-reset method involves a large number of unnecessary erase operations on unused blocks, further shortening the SSD lifetime. Considering the access pattern of LSM-tree, this article proposes a wear-aware zone-management technique, termed WA-Zone, to effectively balance inter- and intra-zone wear in ZNS SSDs. In WA-Zone, a wear-aware zone allocator is first proposed to dynamically allocate data with different hotness to zones with corresponding lifetimes, enabling an even distribution of the erasure counts across zones. Then, a partial-erase-based zone-reset method is presented to avoid unnecessary erase operations. Furthermore, because the novel zone-reset method might lead to an unbalanced distribution of erasure counts across blocks in a zone, a wear-aware block allocator is proposed. Experimental results based on the FEMU emulator demonstrate the proposed WA-Zone enhances the ZNS-SSD lifetime by 5.23×, compared with the baseline scheme. © 2024 Association for Computing Machinery. All rights reserved.",data allocation; LSM-tree; wear-leveling; ZNS SSDs,Dynamic random access storage; II-VI semiconductors; Trees (mathematics); Zinc sulfide; 'current; Data allocation; Erase operation; Garbage collection; LSM-tree; Optimisations; Reducing costs; Storage spaces; Wear-Leveling; ZNS SSD; Wear of materials
Efficient Cross-platform Multiplexing of Hardware Performance Counters via Adaptive Grouping,2024,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186116989&doi=10.1145%2f3629525&partnerID=40&md5=dd8debff786e9baa74b2f485617dad2c,"Collecting sufficient microarchitecture performance data is essential for performance evaluation and workload characterization. There are many events to be monitored in a modern processor while only a few hardware performance monitoring counters (PMCs) can be used, so multiplexing is commonly adopted. However, inefficiency commonly exists in state-of-the-art profiling tools when grouping events for multiplexing PMCs. It has the risk of inaccurate measurement and misleading analysis. Commercial tools can leverage PMCs, but they are closed source and only support their specified platforms. To this end, we propose an approach for efficient cross-platform microarchitecture performance measurement via adaptive grouping, aiming to improve the metrics’ sampling ratios. The approach generates event groups based on the number of available PMCs detected on arbitrary machines while avoiding the scheduling pitfall of Linux perf_event subsystem. We evaluate our approach with SPEC CPU 2017 on four mainstream x86-64 and AArch64 processors and conduct comparative analyses of efficiency with two other state-of-the-art tools, LIKWID and ARM Top-down Tool. The experimental results indicate that our approach gains around 50% improvement in the average sampling ratio of metrics without compromising the correctness and reliability. © 2024 Copyright held by the owner/author(s).",hardware performance counter; linux event scheduling; multiplexing; performance measurement; Performance monitoring unit,Computer architecture; Risk assessment; Adaptive grouping; Cross-platform; Event scheduling; Hardware performance counters; Linux event scheduling; Micro architectures; Performance measurements; Performance monitoring unit; Performance-monitoring; State of the art; Linux
Fastensor: Optimise the Tensor I/O Path from SSD to GPU for Deep Learning Training,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181462477&doi=10.1145%2f3630108&partnerID=40&md5=984cf4cc3528c24f417e622b124e387a,"In recent years, benefiting from the increase in model size and complexity, deep learning has achieved tremendous success in computer vision (CV) and (NLP). Training deep learning models using accelerators such as GPUs often requires much iterative data to be transferred from NVMe SSD to GPU memory. Much recent work has focused on data transfer during the pre-processing phase and has introduced techniques such as multiprocessing and GPU Direct Storage (GDS) to accelerate it. However, tensor data during training (such as Checkpoints, logs, and intermediate feature maps), which is also time-consuming, is often transferred using traditional serial, long-I/O-path transfer methods.In this article, based on GDS technology, we built Fastensor, an efficient tool for tensor data transfer between the NVMe SSDs and GPUs. To achieve higher tensor data I/O throughput, we optimized the traditional data I/O process. We also proposed a data and runtime context-aware tensor I/O algorithm. Fastensor can select the most suitable data transfer tool for the current tensor from a candidate set of tools during model training. The optimal tool is derived from a dictionary generated by our adaptive exploration algorithm in the first few training iterations. We used Fastensor's unified interface to test the read/write bandwidth and energy consumption of different transfer tools for different sizes of tensor blocks. We found that the execution efficiency of different tensor transfer tools is related to both the tensor block size and the runtime context.We then deployed Fastensor in the widely applicable Pytorch deep learning framework. We showed that Fastensor could perform superior in typical scenarios of model parameter saving and intermediate feature map transfer with the same hardware configuration. Fastensor achieves a 5.37x read performance improvement compared to torch.save() when used for model parameter saving. When used for intermediate feature map transfer, Fastensor can increase the supported training batch size by 20x, while the total read and write speed is increased by 2.96x compared to the torch I/O API.  © 2023 Copyright held by the owner/author(s).",Deep learning; GPU direct storage; neural networks; tensor I/O,Data transfer; Deep learning; Digital storage; Energy utilization; Iterative methods; Learning systems; Program processors; Tensors; Deep learning; Feature map; GPU direct storage; Learning models; Model size; Modeling complexity; Modeling parameters; Neural-networks; Runtime context; Tensor I/O; Graphics processing unit
gPPM: A Generalized Matrix Operation and Parallel Algorithm to Accelerate the Encoding/Decoding Process of Erasure Codes,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181438132&doi=10.1145%2f3625005&partnerID=40&md5=cbc85658e45752c058bd756744d17b48,"Erasure codes are widely deployed in modern storage systems, leading to frequent usage of their encoding/decoding operations. The encoding/decoding process for erasure codes is generally carried out using the parity-check matrix approach. However, this approach is serial and computationally expensive, mainly due to dealing with matrix operations, which results in low encoding/decoding performance. These drawbacks are particularly evident for newer erasure codes, including SD and LRC codes. To address these limitations, this article introduces the Partitioned and Parallel Matrix (PPM) algorithm. This algorithm partitions the parity-check matrix, parallelizes encoding/decoding operations, and optimizes calculation sequence to facilitate fast encoding/decoding of these codes. Furthermore, we present a generalized PPM (gPPM) algorithm that surpasses PPM in performance by employing fine-grained dynamic matrix calculation sequence selection. Unlike PPM, gPPM is also applicable to erasure codes such as RS code. Experimental results demonstrate that PPM improves the encoding/decoding speed of SD and LRC codes by up to 210.81%. Besides, gPPM achieves up to 102.41% improvement over PPM and 32.25% improvement over RS regarding encoding/decoding speed.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",computational cost; Erasure codes; fault tolerance; parallelism; performance optimization; storage system,Encoding (symbols); Forward error correction; Matrix algebra; Signal encoding; Computational costs; Decoding process; Encoding/decoding; Erasure codes; matrix; Matrix operations; Parallelism; Parity check matrices; Performance optimizations; Storage systems; Fault tolerance
Autovesk: Automatic Vectorized Code Generation from Unstructured Static Kernels Using Graph Transformations,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186116391&doi=10.1145%2f3631709&partnerID=40&md5=999bb623336daf584ae044eac2f2da92,"Leveraging the SIMD capability of modern CPU architectures is mandatory to take full advantage of their increased performance. To exploit this capability, binary executables must be vectorized, either manually by developers or automatically by a tool. For this reason, the compilation research community has developed several strategies for transforming scalar code into a vectorized implementation. However, most existing automatic vectorization techniques in modern compilers are designed for regular codes, leaving irregular applications with non-contiguous data access patterns at a disadvantage. In this article, we present a new tool, Autovesk, that automatically generates vectorized code from scalar code, specifically targeting irregular data access patterns. We describe how our method transforms a graph of scalar instructions into a vectorized one, using different heuristics to reduce the number or cost of instructions. Finally, we demonstrate the effectiveness of our approach on various computational kernels using Intel AVX-512 and ARM SVE. We compare the speedups of Autovesk vectorized code over GCC, Clang LLVM, and Intel automatic vectorization optimizations. We achieve competitive results on linear kernels and up to 11× speedups on irregular kernels. © 2023 Copyright held by the owner/author(s).",Automatic vectorization; code generation; graph transformations,Automatic programming; Codes (symbols); Graph theory; Automatic vectorization; Binary executables; Codegeneration; CPU architecture; Data access patterns; Graph Transformation; Performance; Research communities; Scalar code; Vectorization techniques; Heuristic methods
Advancing Direct Convolution Using Convolution Slicing Optimization and ISA Extensions,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181728877&doi=10.1145%2f3625004&partnerID=40&md5=42e23908ab86f68f978bc0c069b4cf7e,"Convolution is one of the most computationally intensive operations that must be performed for machine learning model inference. A traditional approach to computing convolutions is known as the Im2Col + BLAS method. This article proposes SConv: a direct-convolution algorithm based on an MLIR/LLVM code-generation toolchain that can be integrated into machine-learning compilers. This algorithm introduces: (a) Convolution Slicing Analysis (CSA) - a convolution-specific 3D cache-blocking analysis pass that focuses on tile reuse over the cache hierarchy; (b) Convolution Slicing Optimization - a code-generation pass that uses CSA to generate a tiled direct-convolution macro-kernel; and (c) Vector-based Packing - an architecture-specific optimized input-tensor packing solution based on vector-register shift instructions for convolutions with unitary stride. Experiments conducted on 393 convolutions from full ONNX-MLIR machine learning models indicate that the elimination of the Im2Col transformation and the use of fast packing routines result in a total packing time reduction, on full model inference, of 2.3×-4.0× on Intel x86 and 3.3×-5.9× on IBM POWER10. The speed-up over an Im2Col + BLAS method based on current BLAS implementations for end-to-end machine-learning model inference is in the range of 11%-27% for Intel x86 and 11%-34% for IBM POWER10 architectures. The total convolution speedup for model inference is 13%-28% on Intel x86 and 23%-39% on IBM POWER10. SConv also outperforms BLAS GEMM, when computing pointwise convolutions in more than 82% of the 219 tested instances.  © 2023 Copyright held by the owner/author(s).",cache blocking; compilers; Convolution; packing,C (programming language); Codes (symbols); Learning algorithms; Machine learning; Program compilers; Blocking analysis; Cache blocking; Codegeneration; Compiler; Convolution algorithm; Machine learning compilers; Machine learning models; Model inference; Optimisations; Traditional approaches; Convolution
Multiply-and-Fire: An Event-Driven Sparse Neural Network Accelerator,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181503477&doi=10.1145%2f3630255&partnerID=40&md5=4ee7d793d73396eb41ceb0b7ea2e3624,"Deep neural network inference has become a vital workload for many systems from edge-based computing to data centers. To reduce the performance and power requirements for deep neural networks (DNNs) running on these systems, pruning is commonly used as a way to maintain most of the accuracy of the system while significantly reducing the workload requirements. Unfortunately, accelerators designed for unstructured pruning typically employ expensive methods to either determine non-zero activation-weight pairings or reorder computation. These methods require additional storage and memory accesses compared to the more regular data access patterns seen in structurally pruned models. However, even existing works that focus on the more regular access patterns seen in structured pruning continue to suffer from inefficient designs, which either ignore or expensively handle activation sparsity leading to low performance.To address these inefficiencies, we leverage structured pruning and propose the multiply-and-fire (MnF) technique, which aims to solve these problems in three ways: (a) the use of a novel event-driven dataflow that naturally exploits activation sparsity without complex, high-overhead logic; (b) an optimized dataflow takes an activation-centric approach, which aims to maximize the reuse of activation data in computation and ensures the data are only fetched once from off-chip global and on-chip local memory; and (c) based on the proposed event-driven dataflow, we develop an energy-efficient, high-performance sparsity-aware DNN accelerator. Our results show that our MnF accelerator achieves a significant improvement across a number of modern benchmarks and presents a new direction to enable highly efficient AI inference for both CNN and MLP workloads. Overall, this work achieves a geometric mean of 11.2× higher energy efficiency and 1.41× speedup compared to a state-of-the-art sparsity-aware accelerator.  © 2023 Copyright held by the owner/author(s).",architecture; dataflow; Deep neural networks; energy-efficient accelerator; event-driven; hardware accelerator; sparse processing,Chemical activation; Computation theory; Computing power; Data flow analysis; Digital storage; Energy efficiency; Memory architecture; Dataflow; Edge-based; Energy efficient; Energy-efficient accelerator; Event-driven; Hardware accelerators; Network inference; Performance; Sparse neural networks; Sparse processing; Deep neural networks
At the Locus of Performance: Quantifying the Effects of Copious 3D-Stacked Cache on HPC Workloads,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181487217&doi=10.1145%2f3629520&partnerID=40&md5=b48a0cdc524479cecae33c433c290697,"Over the last three decades, innovations in the memory subsystem were primarily targeted at overcoming the data movement bottleneck. In this paper, we focus on a specific market trend in memory technology: 3D-stacked memory and caches. We investigate the impact of extending the on-chip memory capabilities in future HPC-focused processors, particularly by 3D-stacked SRAM. First, we propose a method oblivious to the memory subsystem to gauge the upper-bound in performance improvements when data movement costs are eliminated. Then, using the gem5 simulator, we model two variants of a hypothetical LARge Cache processor (LARC), fabricated in 1.5 nm and enriched with high-capacity 3D-stacked cache. With a volume of experiments involving a broad set of proxy-applications and benchmarks, we aim to reveal how HPC CPU performance will evolve, and conclude an average boost of 9.56× for cache-sensitive HPC applications, on a per-chip basis. Additionally, we exhaustively document our methodological exploration to motivate HPC centers to drive their own technological agenda through enhanced co-design.  © 2023 Copyright held by the owner/author(s).",3D-stacked memory; Emerging architecture study; gem5 simulation; proxy-applications,Benchmarking; Cache memory; Memory architecture; Three dimensional integrated circuits; 3D-stacked memory; Data movements; Emerging architecture study; Emerging architectures; Gem5 simulation; Market trends; Memory subsystems; Memory technology; Performance; Proxy-application; Static random access storage
Critical Data Backup with Hybrid Flash-Based Consumer Devices,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186114636&doi=10.1145%2f3631529&partnerID=40&md5=b69e5d5256eff1fa05b5e9d11119d450,"Hybrid flash-based storage constructed with high-density and low-cost flash memory has become increasingly popular in consumer devices in the last decade due to its low cost. However, its poor reliability is one of the major concerns. To protect critical data for guaranteeing user experience, some methods are proposed to improve the reliability of consumer devices with non-hybrid flash storage. However, with the widespread use of hybrid storage, these methods will result in severe problems, including significant performance and endurance degradation. This is caused by the fact that the different characteristics of flash memory in hybrid storage are not considered, e.g., performance, endurance, and access granularity. To address these problems, a critical data backup (CDB) design is proposed to ensure critical data reliability at a low cost. The basic idea is to accumulate two copies of critical data in the fast memory first to make full use of its performance and endurance. Then, one copy will be migrated to the slow memory in the stripe to avoid the write amplification caused by different access granularity between them. By respecting the different characteristics of flash memory in hybrid storage, CDB can achieve encouraging performance and endurance improvement compared with the state-of-the-art. Furthermore, to avoid performance and lifetime degradation caused by the backup data occupying too much space of fast memory, CDB Pro is designed. Two advanced schemes are integrated. One is making use of the pseudo-single-level-cell (pSLC) technique to make a part of slow memory become high-performance. By supplying some high-performance space, data will be fully updated before being evicted to slow memory. More invalid data are generated which reduces eviction costs. Another is to categorize data into three types according to their different life cycles. By putting the same type of data in a block, the eviction efficiency is improved. Therefore, both can improve device performance and lifetime based on CDB. Experiments are conducted to prove the efficiency of CDB and CDB Pro. Experimental results show that compared with the state-of-the-arts, CDB can ensure critical data reliability with lower device performance and lifetime loss whereas CDB Pro can diminish the loss further. © 2023 Copyright held by the owner/author(s).",critical data backup; high-density NAND flash memory; hybrid SSDs; RAID-1,Costs; Efficiency; Flash-based SSDs; Life cycle; Memory architecture; NAND circuits; Consumer devices; Critical data; Critical data backup; Data backups; High-density NAND flash memory; Hybrid SSD; Low-costs; NAND flash memory; Performance; RAID-1; Reliability
ULEEN: A Novel Architecture for Ultra-low-energy Edge Neural Networks,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181461501&doi=10.1145%2f3629522&partnerID=40&md5=a5e51a137d1c2fd56532b1a88b78b719,"""Extreme edge""1 devices, such as smart sensors, are a uniquely challenging environment for the deployment of machine learning. The tiny energy budgets of these devices lie beyond what is feasible for conventional deep neural networks, particularly in high-throughput scenarios, requiring us to rethink how we approach edge inference. In this work, we propose ULEEN, a model and FPGA-based accelerator architecture based on weightless neural networks (WNNs). WNNs eliminate energy-intensive arithmetic operations, instead using table lookups to perform computation, which makes them theoretically well-suited for edge inference. However, WNNs have historically suffered from poor accuracy and excessive memory usage. ULEEN incorporates algorithmic improvements and a novel training strategy inspired by binary neural networks (BNNs) to make significant strides in addressing these issues. We compare ULEEN against BNNs in software and hardware using the four MLPerf Tiny datasets and MNIST. Our FPGA implementations of ULEEN accomplish classification at 4.0-14.3 million inferences per second, improving area-normalized throughput by an average of 3.6× and steady-state energy efficiency by an average of 7.1× compared to the FPGA-based Xilinx FINN BNN inference platform. While ULEEN is not a universally applicable machine learning model, we demonstrate that it can be an excellent choice for certain applications in energy- and latency-critical edge environments.  © 2023 Copyright held by the owner/author(s).",edge computing; high throughput computing; inference; MLPerf tiny; neural networks; Weightless neural networks; WiSARD,Budget control; Deep neural networks; Edge computing; Energy efficiency; Learning systems; Low power electronics; Network architecture; Table lookup; Binary neural networks; Edge computing; Energy; High-throughput computing; Inference; Mlperf tiny; Neural-networks; Novel architecture; Weightless neural networks; WiSARD; Field programmable gate arrays (FPGA)
PARALiA: A Performance Aware Runtime for Auto-tuning Linear Algebra on Heterogeneous Systems,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181679222&doi=10.1145%2f3624569&partnerID=40&md5=ae5a91fe5571fcabdb490b57d1315911,"Dense linear algebra operations appear very frequently in high-performance computing (HPC) applications, rendering their performance crucial to achieve optimal scalability. As many modern HPC clusters contain multi-GPU nodes, BLAS operations are frequently offloaded on GPUs, necessitating the use of optimized libraries to ensure good performance. Unfortunately, multi-GPU systems are accompanied by two significant optimization challenges: data transfer bottlenecks as well as problem splitting and scheduling in multiple workers (GPUs) with distinct memories. We demonstrate that the current multi-GPU BLAS methods for tackling these challenges target very specific problem and data characteristics, resulting in serious performance degradation for any slightly deviating workload. Additionally, an even more critical decision is omitted because it cannot be addressed using current scheduler-based approaches: the determination of which devices should be used for a certain routine invocation. To address these issues we propose a model-based approach: using performance estimation to provide problem-specific autotuning during runtime. We integrate this autotuning into an end-to-end BLAS framework named PARALiA. This framework couples autotuning with an optimized task scheduler, leading to near-optimal data distribution and performance-aware resource utilization. We evaluate PARALiA in an HPC testbed with 8 NVIDIA-V100 GPUs, improving the average performance of GEMM by 1.7× and energy efficiency by 2.5× over the state-of-the-art in a large and diverse dataset and demonstrating the adaptability of our performance-aware approach to future heterogeneous systems.  © 2023 Copyright held by the owner/author(s).",BLAS optimization; Graphics processing units; performance prediction,Computer graphics; Data transfer; Energy efficiency; Large dataset; Linear algebra; Program processors; 'current; Autotuning; BLAS optimization; Dense linear algebra; Heterogeneous systems; Linear algebra operations; Optimisations; Performance; Performance prediction; Runtimes; Graphics processing unit
MicroProf: Code-level Attribution of Unnecessary Data Transfer in Microservice Applications,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181717889&doi=10.1145%2f3622787&partnerID=40&md5=4871ccdeebea7b4d2c0f56cbb9bd75ea,"The microservice architecture style has gained popularity due to its ability to fault isolation, ease of scaling applications, and developer's agility. However, writing applications in the microservice design style has its challenges. Due to the loosely coupled nature, services communicate with others through standard communication APIs. This incurs significant overhead in the application due to communication protocol and data transformation. An inefficient service communication at the microservice application logic can further overwhelm the application. We perform a grey literature review showing that unnecessary data transfer is a real challenge in the industry. To the best of our knowledge, no effective tool is currently available to accurately identify the origins of unnecessary microservice communications that lead to significant performance overhead and provide guidance for optimization.To bridge the knowledge gap, we propose MicroProf, a dynamic program analysis tool to detect unnecessary data transfer in Java-based microservice applications. At the implementation level, MicroProf proposes novel techniques such as remote object sampling and hardware debug registers to monitor remote object usage. MicroProf reports the unnecessary data transfer at the application source code level. Furthermore, MicroProf pinpoints the opportunities for communication API optimization. MicroProf is evaluated on four well-known applications involving two real-world applications and two benchmarks, identifying five inefficient remote invocations. Guided by MicroProf, API optimization achieves an 87.5% reduction in the number of fields within REST API responses. The empirical evaluation further reveals that the optimized services experience a speedup of up to 4.59×.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",dynamic program analysis; Microservice; unnecessary communication,Application programming interfaces (API); Application programs; Benchmarking; Metadata; Program debugging; Architecture styles; Design styles; Dynamic program analysis; Fault isolation; Loosely coupled; Microservice; Optimisations; Remote object; Scalings; Unnecessary communication; Data transfer
JiuJITsu: Removing Gadgets with Safe Register Allocation for JIT Code Generation,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184121935&doi=10.1145%2f3631526&partnerID=40&md5=0b3e16365155a40dda7c3d6dc0e63f84,"Code-reuse attacks have the capability to craft malicious instructions from small code fragments, commonly referred to as “gadgets.” These gadgets are generated by JIT (Just-In-Time) engines as integral components of native instructions, with the flexibility to be embedded in various fields, including Displacement. In this article, we introduce a novel approach for potential gadget insertion, achieved through the manipulation of ModR/M and SIB bytes via JavaScript code. This manipulation influences a JIT engine’s register allocation and code generation algorithms. These newly generated gadgets do not rely on constants and thus evade existing constant blinding schemes. Furthermore, they can be combined with 1-byte constants, a combination that proves to be challenging to defend against using conventional constant blinding techniques. To showcase the feasibility of our approach, we provide proof-of-concept (POC) code for three distinct types of gadgets. Our research underscores the potential for attackers to exploit ModR/M and SIB bytes within JIT-generated native instructions. In response, we propose a practical defense mechanism to mitigate such attacks. We introduce JiuJITsu, a security-enhanced register allocation scheme designed to prevent harmful register assignments during the JIT code generation phase, thereby thwarting the generation of these malicious gadgets. We conduct a comprehensive analysis of JiuJITsu’s effectiveness in defending against code-reuse attacks. Our findings demonstrate that it incurs a runtime overhead of under 1% when evaluated using JetStream2 benchmarks and real-world websites. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",code generation; gadgets; instruction encodings; JavaScript; JIT engines; register allocation,High level languages; Just in time production; Signal encoding; Code fragments; Code reuse; Codegeneration; Gadget; Instruction encoding; Javascript; Just-in-time; Just-in-time engine; Register allocation; Time code; Engines
FlowPix: Accelerating Image Processing Pipelines on an FPGA Overlay using a Domain Specific Compiler,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181756446&doi=10.1145%2f3629523&partnerID=40&md5=b8a1cc40fe013800f17a9f73c84778d5,"The exponential performance growth guaranteed by Moore's law has started to taper in recent years. At the same time, emerging applications like image processing demand heavy computational performance. These factors inevitably lead to the emergence of domain-specific accelerators (DSA) to fill the performance void left by conventional architectures. FPGAs are rapidly evolving towards becoming an alternative to custom ASICs for designing DSAs because of their low power consumption and a higher degree of parallelism. DSA design on FPGAs requires careful calibration of the FPGA compute and memory resources towards achieving optimal throughput.Hardware Descriptive Languages (HDL) like Verilog have been traditionally used to design FPGA hardware. HDLs are not geared towards any domain, and the user has to put in much effort to describe the hardware at the register transfer level. Domain Specific Languages (DSLs) and compilers have been recently used to weave together handwritten HDLs templates targeting a specific domain. Recent efforts have designed DSAs with image-processing DSLs targeting FPGAs. Image computations in the DSL are lowered to pre-existing templates or lower-level languages like HLS-C. This approach requires expensive FPGA re-flashing for every new workload. In contrast to this fixed-function hardware approach, overlays are gaining traction. Overlays are DSAs resembling a processor, which is synthesized and flashed on the FPGA once but is flexible enough to process a broad class of computations through soft reconfiguration. Less work has been reported in the context of image processing overlays. Image processing algorithms vary in size and shape, ranging from simple blurring operations to complex pyramid systems. The primary challenge in designing an image-processing overlay is maintaining flexibility in mapping different algorithms.This paper proposes a DSL-based overlay accelerator called FlowPix for image processing applications. The DSL programs are expressed as pipelines, with each stage representing a computational step in the overall algorithm. We implement 15 image-processing benchmarks using FlowPix on a Virtex-7-690t FPGA. The benchmarks range from simple blur operations to complex pipelines like Lucas-Kande optical flow. We compare FlowPix against existing DSL-to-FPGA frameworks like Hetero-Halide and Vitis Vision library that generate fixed-function hardware. On most benchmarks, we see up to 25% degradation in latency with approximately a 1.7x to 2x increase in the FPGA LUT consumption. Our ability to execute any benchmark without incurring the high costs of hardware synthesis, place-and-route, and FPGA re-flashing justifies the slight performance loss and increased resource consumption that we experience. FlowPix achieves an average frame rate of 170 FPS on HD frames of 1920 × 1080 pixels in the implemented benchmarks.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Domain specific languages; FPGAs; overlay accelerators,Benchmarking; Computer hardware description languages; Digital subscriber lines; Image processing; Integrated circuit design; Pipeline processing systems; Pipelines; Problem oriented languages; Program compilers; Domain specific; Domains specific languages; Emerging applications; Exponentials; Image processing pipeline; Images processing; Moore Law; Overlay accelerator; Performance; Simple++; Field programmable gate arrays (FPGA)
Mapi-Pro: An Energy Efficient Memory Mapping Technique for Intermittent Computing,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181469060&doi=10.1145%2f3629524&partnerID=40&md5=63a1eb5d98a4b814e09ea8339cb7a48d,"Battery-less technology evolved to replace battery usage in space, deep mines, and other environments to reduce cost and pollution. Non-volatile memory (NVM) based processors were explored for saving the system state during a power failure. Such devices have a small SRAM and large non-volatile memory. To make the system energy efficient, we need to use SRAM efficiently. So we must select some portions of the application and map them to either SRAM or FRAM. This paper proposes an ILP-based memory mapping technique for intermittently powered IoT devices. Our proposed technique gives an optimal mapping choice that reduces the system's Energy-Delay Product (EDP). We validated our system using TI-based MSP430FR6989 and MSP430F5529 development boards. Our proposed memory configuration consumes 38.10% less EDP than the baseline configuration and 9.30% less EDP than the existing work under stable power. Our proposed configuration achieves 20.15% less EDP than the baseline configuration and 26.87% less EDP than the existing work under unstable power. This work supports intermittent computing and works efficiently during frequent power failures.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",ILP; Intermittent power; Memory-Mapping; MSP430FR6989; NVM,Electric batteries; Energy efficiency; Ferroelectric RAM; Mapping; Energy delay product; Energy efficient; ILP; Intermittent power; Mapping techniques; Memory mapping; Msp430fr6989; Non-volatile memory; Power; Power failure; Static random access storage
RACE: An Efficient Redundancy-aware Accelerator for Dynamic Graph Neural Network,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181461456&doi=10.1145%2f3617685&partnerID=40&md5=bf68145c8c7dd46d74d7ec004db86f9e,"Dynamic Graph Neural Network (DGNN) has recently attracted a significant amount of research attention from various domains, because most real-world graphs are inherently dynamic. Despite many research efforts, for DGNN, existing hardware/software solutions still suffer significantly from redundant computation and memory access overhead, because they need to irregularly access and recompute all graph data of each graph snapshot. To address these issues, we propose an efficient redundancy-aware accelerator, RACE, which enables energy-efficient execution of DGNN models. Specifically, we propose a redundancy-aware incremental execution approach into the accelerator design for DGNN to instantly achieve the output features of the latest graph snapshot by correctly and incrementally refining the output features of the previous graph snapshot and also enable regular accesses of vertices' input features. Through traversing the graph on the fly, RACE identifies the vertices that are not affected by graph updates between successive snapshots to reuse these vertices' states (i.e., their output features) of the previous snapshot for the processing of the latest snapshot. The vertices affected by graph updates are also tracked to incrementally recompute their new states using their neighbors' input features of the latest snapshot for correctness. In this way, the processing and accessing of many graph data that are not affected by graph updates can be correctly eliminated, enabling smaller redundant computation and memory access overhead. Besides, the input features, which are accessed more frequently, are dynamically identified according to graph topology and are preferentially resident in the on-chip memory for less off-chip communications. Experimental results show that RACE achieves on average 1139× and 84.7× speedups for DGNN inference, with average 2242× and 234.2× energy savings, in comparison with the state-of-the-art software DGNN running on Intel Xeon CPU and NVIDIA A100 GPU, respectively. Moreover, for DGNN inference, RACE obtains on average 13.1×, 11.7×, 10.4×, and 7.9× speedup and 14.8×, 12.9×, 11.5×, and 8.9× energy savings over the state-of-the-art Graph Neural Network accelerators, i.e., AWB-GCN, GCNAX, ReGNN, and I-GCN, respectively.  © 2023 Copyright held by the owner/author(s).",dynamic graph neural network; efficiency; hardware accelerator; Redundancy-aware,Energy efficiency; Graph neural networks; Graph theory; Memory architecture; Dynamic graph; Dynamic graph neural network; Graph data; Graph neural networks; Hardware accelerators; Input features; Memory access; Network inference; Redundancy-aware; Redundant computation; Redundancy
DxPU: Large-scale Disaggregated GPU Pools in the Datacenter,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181439279&doi=10.1145%2f3617995&partnerID=40&md5=5e6a50c7a8d10fc221823e88e4c3ee59,"The rapid adoption of AI and convenience offered by cloud services have resulted in the growing demands for GPUs in the cloud. Generally, GPUs are physically attached to host servers as PCIe devices. However, the fixed assembly combination of host servers and GPUs is extremely inefficient in resource utilization, upgrade, and maintenance. Due to these issues, the GPU disaggregation technique has been proposed to decouple GPUs from host servers. It aggregates GPUs into a pool and allocates GPU node(s) according to user demands. However, existing GPU disaggregation systems have flaws in software-hardware compatibility, disaggregation scope, and capacity.In this article, we present a new implementation of datacenter-scale GPU disaggregation, named DxPU. DxPU efficiently solves the above problems and can flexibly allocate as many GPU node(s) as users demand. To understand the performance overhead incurred by DxPU, we build up a performance model for AI specific workloads. With the guidance of modeling results, we develop a prototype system, which has been deployed into the datacenter of a leading cloud provider for a test run. We also conduct detailed experiments to evaluate the performance overhead caused by our system. The results show that the overhead of DxPU is less than 10%, compared with native GPU servers, in most of user scenarios.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Clouds; clusters; data centers,Computer hardware; Program processors; Cloud services; Cluster; Datacenter; Disaggregation; Growing demand; Host servers; Large-scales; Performance; Resources utilizations; User demands; Graphics processing unit
DAG-Order: An Order-Based Dynamic DAG Scheduling for Real-Time Networks-on-Chip,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186123672&doi=10.1145%2f3631527&partnerID=40&md5=42bef29d7f09d099a3e352f41baf4fb5,"With the high-performance requirement of safety-critical real-time tasks, the platforms of many-core processors with high parallelism are widely utilized, where network-on-chip (NoC) is generally employed for inter-core communication due to its scalability and high efficiency. Unfortunately, large uncertainties are suffered on NoCs from both the overly parallel architecture and the distributed scheduling strategy (e.g., wormhole flow control), which complicates the response time upper bounds estimation (i.e., either unsafe or pessimistic). For DAG-based real-time parallel tasks, to solve this problem, we propose DAG-Order, an order-based dynamic DAG scheduling approach, which strictly guarantees NoC real-time services. First, rather than build the new analysis to fit the widely used best-effort wormhole NoC, DAG-Order is built upon a kind of advanced low-latency NoC with SLT (Single-cycle Long-range Traversal) to avoid the unpredictable parallel transmission on the shared source-destination link of wormhole NoCs. Second, DAG-Order is a non-preemptive dynamic scheduling strategy, which jointly considers communication as well as computation workloads, and fits SLT NoC. With such an order-based dynamic scheduling strategy, the provably bound safety is ensured by enforcing certain order constraints among DAG edges/vertices that eliminate the execution-timing anomaly at runtime. Third, the order constraints are further relaxed for higher average-case runtime performance without compromising bound safety. Finally, an effective heuristic algorithm seeking a proper schedule order is developed to tighten the bounds. Experiments on synthetic and realistic benchmarks demonstrate that DAG-Order performs better than the state-of-the-art related scheduling methods. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,Heuristic algorithms; Information services; Network architecture; Parallel architectures; Real time systems; Safety engineering; DAG scheduling; Dynamic scheduling; Many-core processors; Networks on chips; Ordering constraints; Performance requirements; Real time network; Real-time tasks; Scheduling strategies; Single cycle; Network-on-chip
Characterizing Multi-Chip GPU Data Sharing,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181513434&doi=10.1145%2f3629521&partnerID=40&md5=7a5556d1468b66d34b3de54bb810cc87,"Multi-chip Graphics Processing Unit (GPU) systems are critical to scale performance beyond a single GPU chip for a wide variety of important emerging applications. A key challenge for multi-chip GPUs, though, is how to overcome the bandwidth gap between inter-chip and intra-chip communication. Accesses to shared data, i.e., data accessed by multiple chips, pose a major performance challenge as they incur remote memory accesses possibly congesting the inter-chip links and degrading overall system performance. This article characterizes the shared dataset in multi-chip GPUs in terms of (1) truly versus falsely shared data, (2) how the shared dataset scales with input size, (3) along which dimensions the shared dataset scales, and (4) how sensitive the shared dataset is with respect to the input's characteristics, i.e., node degree and connectivity in graph workloads. We observe significant variety in scaling behavior across workloads: some workloads feature a shared dataset that scales linearly with input size, whereas others feature sublinear scaling (following a or relationship). We further demonstrate how the shared dataset affects the optimum last-level cache organization (memory-side versus SM-side) in multi-chip GPUs, as well as optimum memory page allocation and thread scheduling policy. Sensitivity analyses demonstrate the insights across the broad design space.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",data sharing; Graphics processing unit (GPU); multi-GPU systems,Cache memory; Computer graphics; Computer graphics equipment; Data handling; Program processors; Sensitivity analysis; Data Sharing; Emerging applications; Graphic processing unit; Input size; Inter-chip; Multi-graphic processing unit system; Multichips; Performance; Shared data; Unit systems; Graphics processing unit
Improving Computation and Memory Efficiency for Real-world Transformer Inference on GPUs,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180398856&doi=10.1145%2f3617689&partnerID=40&md5=1a6dfd4e36facb5ff8bba4f90fafb406,"Transformer models have emerged as a leading approach in the field of natural language processing (NLP) and are increasingly being deployed in production environments. Graphic processing units (GPUs) have become a popular choice for the transformer deployment and often rely on the batch processing technique to ensure high hardware performance. Nonetheless, the current practice for transformer inference encounters computational and memory redundancy due to the heavy-tailed distribution of sequence lengths in NLP scenarios, resulting in low practical performance. In this article, we propose a unified solution for improving both computation and memory efficiency of the real-world transformer inference on GPUs. The solution eliminates the redundant computation and memory footprint across a transformer model. At first, a GPU-oriented computation approach is proposed to process the self-attention module in a fine-grained manner, eliminating its redundant computation. Next, the multi-layer perceptron module continues to use the word-accumulation approach to eliminate its redundant computation. Then, to better unify the fine-grained approach and the word-accumulation approach, it organizes the data layout of the self-attention module in block granularity. Since aforementioned approaches make the required memory size largely reduce and constantly fluctuate, we propose the chunk-based approach to enable a better balance between memory footprint and allocation/free efficiency. Our experimental results show that our unified solution achieves a decrease of average latency by 28% on the entire transformer model, 63.8% on the self-attention module, and reduces memory footprint of intermediate results by 7.8×, compared with prevailing frameworks.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",graphics processing unit; natural language processing; Parallel computing; transformer inference,Batch data processing; Computational efficiency; Computer graphics; Computer graphics equipment; Natural language processing systems; Program processors; Computation efficiency; Language processing; Memory efficiency; Memory footprint; Natural language processing; Natural languages; Parallel com- puting; Redundant computation; Transformer inference; Transformer modeling; Graphics processing unit
High-performance Deterministic Concurrency Using Lingua Franca,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180183903&doi=10.1145%2f3617687&partnerID=40&md5=2ca84903f24f10d4ba9f777352d2d22f,"Actor frameworks and similar reactive programming techniques are widely used for building concurrent systems. They promise to be efficient and scale well to a large number of cores or nodes in a distributed system. However, they also expose programmers to nondeterminism, which often makes implementations hard to understand, debug, and test. The recently proposed reactor model is a promising alternative that enables deterministic concurrency. In this article, we present an efficient, parallel implementation of reactors and demonstrate that the determinacy of reactors does not imply a loss in performance. To show this, we evaluate Lingua Franca (LF), a reactor-oriented coordination language. LF equips mainstream programming languages with a deterministic concurrency model that automatically takes advantage of opportunities to exploit parallelism. Our implementation of the Savina benchmark suite demonstrates that, in terms of execution time, the runtime performance of LF programs even exceeds popular and highly optimized actor frameworks. We compare against Akka and CAF, which LF outperforms by 1.86× and 1.42×, respectively.  © 2023 Copyright held by the owner/author(s).",concurrency; Coordination; determinism; performance,Concurrency; Concurrent systems; Coordination; Determinism; Deterministics; Distributed systems; Non Determinism; Performance; Programming technique; Reactive programming; Benchmarking
A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181724266&doi=10.1145%2f3617686&partnerID=40&md5=ec2035cc4db2543208dba9348d4a7a62,"Computing-in-Memory (CIM) architectures using Non-volatile Memories (NVMs) have emerged as a promising way to address the ""memory wall""problem in traditional Von Neumann architectures. CIM accelerators can perform arithmetic or Boolean logic operations in NVMs by fully exploiting their high parallelism for bit-wise operations. These accelerators are often used in cooperation with general-purpose processors to speed up a wide variety of artificial neural network applications. In such a heterogeneous computing architecture, the legacy software should be redesigned and re-engineered to utilize new CIM accelerators. In this article, we propose a compilation tool to automatically migrate legacy programs to such heterogeneous architectures based on the low-level virtual machine (LLVM) compiler infrastructure. To accelerate some computations such as vector-matrix multiplication in CIM accelerators, we identify several typical computing patterns from LLVM intermediate representations, which are oblivious to high-level programming paradigms. Our compilation tool can modify accelerable LLVM IRs to offload them to CIM accelerators automatically, without re-engineering legacy software. Experimental results show that our compilation tool can translate many legacy programs to CIM-supported binary executables effectively, and improve application performance and energy efficiency by up to 51× and 309×, respectively, compared with general-purpose x86 processors.  © 2023 Copyright held by the owner/author(s).",accelerator; compilation; LLVM-IR; ReRAM,Application programs; Computation theory; General purpose computers; Memory architecture; Network architecture; Neural networks; Program compilers; RRAM; Arithmetic logic; Boolean logic operations; Compilation; Computation offloading; Legacy software; Low level virtual machines; Low-level virtual machine-IR; Memory wall; Neumann architecture; Non-volatile memory; Energy efficiency
Smart-DNN+: A Memory-efficient Neural Networks Compression Framework for the Model Inference,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181458961&doi=10.1145%2f3617688&partnerID=40&md5=ff0155a1b61660e5f367076bd07cd7b9,"Deep Neural Networks (DNNs) have achieved remarkable success in various real-world applications. However, running a Deep Neural Network (DNN) typically requires hundreds of megabytes of memory footprints, making it challenging to deploy on resource-constrained platforms such as mobile devices and IoT. Although mainstream DNNs compression techniques such as pruning, distillation, and quantization can reduce the memory overhead of model parameters during DNN inference, they suffer from three limitations: (i) low model compression ratio for the lightweight DNN structures with little redundancy, (ii) potential degradation in model inference accuracy, and (iii) inadequate memory compression ratio is attributable to ignoring the layering property of DNN inference. To address these issues, we propose a lightweight memory-efficient DNN inference framework called Smart-DNN+, which significantly reduces the memory costs of DNN inference without degrading the model quality. Specifically, 1 Smart-DNN+ applies a layerwise binary-quantizer with a remapping mechanism to greatly reduce the model size by quantizing the typical floating-point DNN weights of 32-bit to the 1-bit signs layer by layer. To maintain model quality, 2 Smart-DNN+ employs a bucket-encoder to keep the compressed quantization error by encoding the multiple similar floating-point residuals into the same integer bucket IDs. When running the compressed DNN in the user's device, 3 Smart-DNN+ utilizes a partially decompressing strategy to greatly reduce the required memory overhead by first loading the compressed DNNs in memory and then dynamically decompressing the required materials for model inference layer by layer.Experimental results on popular DNNs and datasets demonstrate that Smart-DNN+ achieves lower 0.17%-0.92% memory costs at lower runtime overheads compared with the states of the art without degrading the inference accuracy. Moreover, Smart-DNN+ potentially reduces the inference runtime up to 2.04× that of conventional DNN inference workflow.  © 2023 Copyright held by the owner/author(s).",memory overhead; Model compression; neural network; quantization,Cost reduction; Digital arithmetic; Distillation; Signal encoding; Memory cost; Memory efficient; Memory overheads; Model compression; Model inference; Modeling quality; Network compression; Network inference; Neural-networks; Quantisation; Deep neural networks
SplitZNS: Towards an Efficient LSM-Tree on Zoned Namespace SSDs,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168762646&doi=10.1145%2f3608476&partnerID=40&md5=cff428c554ac8b314e3084546a4aaa7c,"The Zoned Namespace (ZNS) Solid State Drive (SSD) is a nascent form of storage device that offers novel prospects for the Log Structured Merge Tree (LSM-tree). ZNS exposes erase blocks in SSD as append-only zones, enabling the LSM-tree to gain awareness of the physical layout of data. Nevertheless, LSM-tree on ZNS SSDs necessitates Garbage Collection (GC) owing to the mismatch between the gigantic zones and relatively small Sorted String Tables (SSTables). Through extensive experiments, we observe that a smaller zone size can reduce data migration in GC at the cost of a significant performance decline owing to inadequate parallelism exploitation. In this article, we present SplitZNS, which introduces small zones by tweaking the zone-to-chip mapping to maximize GC efficiency for LSM-tree on ZNS SSDs. Following the multi-level peculiarity of LSM-tree and the inherent parallel architecture of ZNS SSDs, we propose a number of techniques to leverage and accelerate small zones to alleviate the performance impact due to underutilized parallelism. (1) First, we use small zones selectively to prevent exacerbating write slowdowns and stalls due to their suboptimal performance. (2) Second, to enhance parallelism utilization, we propose SubZone Ring, which employs a per-chip FIFO buffer to imitate a large zone writing style; (3) Read Prefetcher, which prefetches data concurrently through multiple chips during compactions; (4) and Read Scheduler, which assigns query requests the highest priority. We build a prototype integrated with SplitZNS to validate its efficiency and efficacy. Experimental results demonstrate that SplitZNS achieves up to 2.77× performance and reduces data migration considerably compared to the lifetime-based data placement.1  © 2023 Copyright held by the owner/author(s).",garbage collection; LSM-tree; Zoned Namespace,Efficiency; Refuse collection; Trees (mathematics); Virtual storage; Zinc sulfide; Collection efficiency; Data-migration; Garbage collection; Log structured merge trees; Multilevels; Namespaces; Performance; Performance impact; Physical layout; Zoned namespace; Parallel architectures
MFFT: A GPU Accelerated Highly Efficient Mixed-Precision Large-Scale FFT Framework,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168758763&doi=10.1145%2f3605148&partnerID=40&md5=aa2b08b1af82bfc503a65a70889b380d,"Fast Fourier transform (FFT) is widely used in computing applications in large-scale parallel programs, and data communication is the main performance bottleneck of FFT and seriously affects its parallel efficiency. To tackle this problem, we propose a new large-scale FFT framework, MFFT, which optimizes parallel FFT with a new mixed-precision optimization technique, adopting the ""high precision computation, low precision communication""strategy. To enable ""low precision communication"", we propose a shared-exponent floating-point number compression technique, which reduces the volume of data communication, while maintaining higher accuracy. In addition, we apply a two-phase normalization technique to further reduce the round-off error. Based on the mixed-precision MFFT framework, we apply several optimization techniques to improve the performance, such as streaming of GPU kernels, MPI message combination, kernel optimization, and memory optimization. We evaluate MFFT on a system with 4,096 GPUs. The results show that shared-exponent MFFT is 1.23 × faster than that of double-precision MFFT on average, and double-precision MFFT achieves performance 3.53× and 9.48× on average higher than open source library 2Decomp & FFT (CPU-based version) and heFFTe (AMD GPU-based version), respectively. The parallel efficiency of double-precision MFFT increased from 53.2% to 78.1% compared with 2Decomp & FFT, and shared-exponent MFFT further increases the parallel efficiency to 83.8%.  © 2023 Copyright held by the owner/author(s).",GPUs; Large-scale FFT; mixed-precision; MPI,Application programs; Convolutional codes; Digital arithmetic; Efficiency; Graphics processing unit; Program processors; Data-communication; Double precision; Large-scale fast fourier transform; Large-scales; Lower precision; Mixed precision; MPI; Optimization techniques; Parallel efficiency; Performance; Fast Fourier transforms
Approx-RM: Reducing Energy on Heterogeneous Multicore Processors under Accuracy and Timing Constraints,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168762653&doi=10.1145%2f3605214&partnerID=40&md5=f0dd1e09d4e2271532ac4e974d986fda,"Reducing energy consumption while providing performance and quality guarantees is crucial for computing systems ranging from battery-powered embedded systems to data centers. This article considers approximate iterative applications executing on heterogeneous multi-core platforms under user-specified performance and quality targets. We note that allowing a slight yet bounded relaxation in solution quality can considerably reduce the required iteration count and thereby can save significant amounts of energy. To this end, this article proposes Approx-RM, a resource management scheme that reduces energy expenditure while guaranteeing a specified performance as well as accuracy target. Approx-RM predicts the number of iterations required to meet the relaxed accuracy target at runtime. The time saved generates execution-time slack, which allows Approx-RM to allocate fewer resources on a heterogeneous multi-core platform in terms of DVFS, core type, and core count to save energy while meeting the performance target. Approx-RM contributes with lightweight methods for predicting the iteration count needed to meet the accuracy target and the resources needed to meet the performance target. Approx-RM uses the aforementioned predictions to allocate just enough resources to comply with quality of service constraints to save energy. Our evaluation shows energy savings of 31.6%, on average, compared to Race-to-idle when the accuracy is only relaxed by 1%. Approx-RM incurs timing and energy overheads of less than 0.1%.  © 2023 Copyright held by the owner/author(s).",approximate iterative applications; DVFS; Energy efficiency; heterogeneous multicore processors; quality of service; resource management,Embedded systems; Energy utilization; Green computing; Iterative methods; Natural resources management; Quality of service; Resource allocation; Timing circuits; Approximate iterative application; DVFS; Energy; Heterogeneous multicore; Heterogeneous multicore processors; Multi-core platforms; Performance; Performance targets; Quality-of-service; Resource management; Energy efficiency
The Impact of Page Size and Microarchitecture on Instruction Address Translation Overhead,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168774728&doi=10.1145%2f3600089&partnerID=40&md5=a6c92de9c2792b3e7ddfa610f9f7aacf,"As the volume of data processed by applications has increased, considerable attention has been paid to data address translation overheads, leading to the widespread use of larger page sizes (""superpages"") and multi-level translation lookaside buffers (TLBs). However, far less attention has been paid to instruction address translation and its relation to TLB and pipeline structure. In prior work, we quantified the impact of using code superpages on a variety of widely used applications, ranging from compilers to web user-interface frameworks, and the impact of sharing page table pages for executables and shared libraries. Within this article, we augment those results by first uncovering the effects that microarchitectural differences between Intel Skylake and AMD Zen+, particularly their different TLB organizations, have on instruction address translation overhead. This analysis provides some key insights into the microarchitectural design decisions that impact the cost of instruction address translation. First, a lower-level (level 2) TLB that has both instruction and data mappings competing for space within the same structure allows better overall performance and utilization when using code superpages. Code superpages not only reduce instruction address translation overhead but also indirectly reduce data address translation overhead. In fact, for a few applications, the use of just a few code superpages has a larger impact on overall performance than the use of a much larger number of data superpages. Second, a level 1 (L1) TLB with separate structures for different page sizes may require careful tuning of the superpage promotion policy for code, and a correspondingly suboptimal utilization of the level 2 TLB. In particular, increasing the number of superpages when the size of the L1 superpage structure is small may result in more L1 TLB misses for some applications. Moreover, on some microarchitectures, the cost of these misses can be highly variable, because replacement is delayed until all of the in-flight instructions mapped by the victim entry are retired. Hence, more superpage promotions can result in a performance regression. Finally, our findings also make a case for first-class OS support for superpages on ordinary files containing executables and shared libraries, as well as a more aggressive superpage policy for code.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",address translation; instruction access; memory hierarchy; microarchitecture; page table; Superpage; Translation Lookaside Buffer,Codes (symbols); Computer architecture; Cost benefit analysis; Cost reduction; Libraries; User interfaces; Address translation; Instruction access; Level-1; Memory hierarchy; Micro architectures; Page sizes; Page table; Performance; Superpage; Translation lookaside buffer; Websites
MPU: Memory-centric SIMT Processor via In-DRAM Near-bank Computing,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168763372&doi=10.1145%2f3603113&partnerID=40&md5=458284a94daeb345aee5534274a7779e,"With the growing number of data-intensive workloads, GPU, which is the state-of-the-art single-instruction-multiple-thread (SIMT) processor, is hindered by the memory bandwidth wall. To alleviate this bottleneck, previously proposed 3D-stacking near-bank computing accelerators benefit from abundant bank-internal bandwidth by bringing computations closer to the DRAM banks. However, these accelerators are specialized for certain application domains with simple architecture data paths and customized software mapping schemes. For general-purpose scenarios, lightweight hardware designs for diverse data paths, architectural supports for the SIMT programming model, and end-to-end software optimizations remain challenging.To address these issues, we propose Memory-centric Processing Unit (MPU), the first SIMT processor based on 3D-stacking near-bank computing architecture. First, to realize diverse data paths with small overheads, MPU adopts a hybrid pipeline with the capability of offloading instructions to near-bank compute-logic. Second, we explore two architectural supports for the SIMT programming model, including a near-bank shared memory design and a multiple activated row-buffers enhancement. Third, we present an end-to-end compilation flow for MPU to support CUDA programs. To fully utilize MPU's hybrid pipeline, we develop a backend optimization for the instruction offloading decision. The evaluation results of MPU demonstrate 3.46× speedup and 2.57× energy reduction compared with an NVIDIA Tesla V100 GPU on a set of representative data-intensive workloads.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",3D memory system; parallel computing; Processing-in-memory,Application programs; Bandwidth; Computation theory; Dynamic random access storage; Graphics processing unit; Pipelines; Three dimensional integrated circuits; 3D memory; 3d memory system; 3D stacking; Data paths; Data-intensive workloads; Memory systems; Multiple threads; Parallel com- puting; Processing units; Processing-in-memory; Memory architecture
Accelerating Convolutional Neural Network by Exploiting Sparsity on GPUs,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168768902&doi=10.1145%2f3600092&partnerID=40&md5=df9bfdb21be2a666519307430154e1af,"The convolutional neural network (CNN) is an important deep learning method, which is widely used in many fields. However, it is very time consuming to implement the CNN where convolution usually takes most of the time. There are many zero values in feature maps and filters, which leads to redundant calculations and memory accesses if dense methods are used to compute convolution. Many works recently have made use of sparsity to skip the calculations for zero values to reduce the inference time of the CNN. On the graphics processing unit platform, current works cannot fully exploit the sparsity of the feature map and achieve satisfactory performance. Therefore, we design a new parallel strategy to transform the feature map into a new storage format to avoid the redundant computation of zero values on graphics processing units. Also considering the sparsity in the feature map, we propose a fused storage format to combine the convolution operation with the following pooling operation, to further improve the performance. We carry out experiments with mainstream CNN models and achieve better performance compared with cuDNN and cuSPARSE. For VGG-19, ResNet-50, DenseNet-121, and RegNetX-16GF, 1.97×, 2.23×, 2.74×, and 1.58× speedups respectively are obtained over cuDNN. The speedups over cuSPARSE respectively are 2.10×, 1.83×, 2.35×, and 1.35× when only using the first method.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Convolutional neural network; GPU; performance tuning; shared memory; SpMV,Computer graphics; Computer graphics equipment; Convolution; Convolutional neural networks; Deep learning; Learning systems; Program processors; And filters; Convolutional neural network; Feature map; Learning methods; Performance; Performance tuning; Shared memory; SpMV; Storage formats; Zero values; Graphics processing unit
ASM: An Adaptive Secure Multicore for Co-located Mutually Distrusting Processes,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168766103&doi=10.1145%2f3587480&partnerID=40&md5=38cbf8e3b3298b08d88b2d83bf401d7c,"With the ever-increasing virtualization of software and hardware, the privacy of user-sensitive data is a fundamental concern in computation outsourcing. Secure processors enable a trusted execution environment to guarantee security properties based on the principles of isolation, sealing, and integrity. However, the shared hardware resources within the microarchitecture are increasingly being used by co-located adversarial software to create timing-based side-channel attacks. State-of-the-art secure processors implement the strong isolation primitive to enable non-interference for shared hardware but suffer from frequent state purging and resource utilization overheads, leading to degraded performance. This article proposes ASM, an adaptive secure multicore architecture that enables a reconfigurable, yet strongly isolated execution environment. For outsourced security-critical processes, the proposed security kernel and hardware extensions allow either a given process to execute using all available cores or co-execute multiple processes on strongly isolated clusters of cores. This spatio-temporal execution environment is configured based on resource demands of processes, such that the secure processor mitigates state purging overheads and maximizes hardware resource utilization.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",hardware security; Secure architecture; strong isolation,Computer hardware; Hardware security; Reconfigurable architectures; Sensitive data; Side channel attack; Software architecture; Co-located; Execution environments; Multi-cores; Secure architectures; Secure processors; Sensitive datas; Software and hardwares; Strong isolation; Trusted execution environments; Virtualizations; Purging
Cache Programming for Scientific Loops Using Leases,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168770168&doi=10.1145%2f3600090&partnerID=40&md5=c9d7c6dd6b5c6fa80271041699703768,"Cache management is important in exploiting locality and reducing data movement. This article studies a new type of programmable cache called the lease cache. By assigning leases, software exerts the primary control on when and how long data stays in the cache. Previous work has shown an optimal solution for an ideal lease cache.This article develops and evaluates a set of practical solutions for a physical lease cache emulated in FPGA with the full suite of PolyBench benchmarks. Compared to automatic caching, lease programming can further reduce data movement by 10% to over 60% when the data size is 16 times to 3,000 times the cache size, and the techniques in this article realize over 80% of this potential. Moreover, lease programming can reduce data movement by another 0.8% to 20% after polyhedral locality optimization.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Cache management; cache replacement policy; compiler transformations; lease cache; phase marking; reuse interval distribution,Cache memory; Program compilers; Cache management; Cache replacement policy; Compiler transformations; Data movements; Lease cache; Management IS; Phase marking; Primary control; Reuse; Reuse interval distribution; Data reduction
TNT: A Modular Approach to Traversing Physically Heterogeneous NOCs at Bare-wire Latency,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168769473&doi=10.1145%2f3597611&partnerID=40&md5=8d2d7d8ccff755d867386ba3774d4231,"The ideal latency for on-chip network traversal would be the delay incurred from wire traversal alone. Unfortunately, in a realistic modular network, the latency for a packet to traverse the network is significantly higher than this wire delay. The main limiter to achieving lower latency is the modular quantization of network traversal into hops. Beyond this, the physical heterogeneity in real-world systems further complicate the ability to reach ideal wire-only delay.In this work, we propose TNT or Transparent Network Traversal. TNT targets ideal network latency by attempting source to destination network traversal as a single multi-cycle long-hop', bypassing the quantization effects of intermediate routers via transparent data/information flow. TNT is built in a modular tile-scalable manner via a novel control path performing neighbor-to-neighbor interactions but enabling end-to-end transparent flit traversal. Further, TNT's fine grained on-the-fly delay tracking allows it to cope with physical NOC heterogeneity across the chip.Analysis on Ligra graph workloads shows that TNT can reduce NOC latency by as much as 43% compared to the state of the art and allows efficiency gains up to 38%. Further, it can achieve more than 3x the benefits of the best/closest alternative research proposal, SMART [43].  © 2023 Copyright held by the owner/author(s).",clock cycle slack; interconnection networks; microarchitecture; On-chip networks,Integrated circuit interconnects; Interconnection networks (circuit switching); Network-on-chip; Routers; Bare wires; Clock cycle slack; Clock cycles; Interconnection-Network; Micro architectures; Modular approach; Modular network; Modulars; On-chip networks; Wire delays; Wire
Jointly Optimizing Job Assignment and Resource Partitioning for Improving System Throughput in Cloud Datacenters,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168765229&doi=10.1145%2f3593055&partnerID=40&md5=66440d5d6855fc13e67fe28c64be7fe3,"Colocating multiple jobs on the same server has been widely applied for improving resource utilization in cloud datacenters. However, the colocated jobs would contend for the shared resources, which could lead to significant performance degradation. An efficient approach for eliminating performance interference is to partition the shared resources among the colocated jobs. However, this makes the resource management in datacenters very challenging. In this paper, we propose JointOPT, the first resource management framework that optimizes job assignment and resource partitioning jointly for improving the throughput of cloud datacenters. JointOPT uses a local search based algorithm to find the near optimal job assignment configuration, and uses a deep reinforcement learning (DRL) based approach to dynamically partition the shared resources among the colocated jobs. In order to reduce the interaction overhead with real systems, it leverages deep learning to estimate job performance without running them on real servers. We conduct extensive experiments to evaluate JointOPT and the results show that JointOPT significantly outperforms the state-of-the-art baselines, with an advantage from 13.3% to 47.7%.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep learning; deep reinforcement learning; job assignment; performance interference; resource partitioning; transfer learning; Workload consolidation,Deep learning; Natural resources management; Optimization; Resource allocation; Datacenter; Deep learning; Deep reinforcement learning; Job assignments; Performance; Performance interference; Reinforcement learnings; Resource partitioning; Transfer learning; Workload consolidation; Reinforcement learning
Hierarchical Model Parallelism for Optimizing Inference on Many-core Processor via Decoupled 3D-CNN Structure,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168762074&doi=10.1145%2f3605149&partnerID=40&md5=0bd4bfcf826d0a94e3b56473c9873fed,"The tremendous success of convolutional neural network (CNN) has made it ubiquitous in many fields of human endeavor. Many applications such as biomedical analysis and scientific data analysis involve analyzing volumetric data. This spawns huge demand for 3D-CNN. Although accelerators such as GPU may provide higher throughput on deep learning applications, they may not be available in all scenarios. CPU, especially many-core CPU with non-uniform memory access (NUMA) architecture, remains an attractive choice for deep learning inference in many scenarios.In this article, we propose a distributed inference solution for 3D-CNN that targets on the emerging ARM many-core CPU platform. A hierarchical partition approach is claimed to accelerate 3D-CNN inference by exploiting characteristics of memory and cache on ARM many-core CPU. Based on the hierarchical model partition approach, other optimization techniques such as NUMA-aware thread scheduling and optimization of 3D-img2row convolution are designed to exploit the potential of ARM many-core CPU for 3D-CNN. We evaluate our proposed inference solution with several classic 3D-CNNs: C3D, 3D-resnet34, 3D-resnet50, 3D-vgg11, and P3D. Our experimental results show that our solution can boost the performance of the 3D-CNN inference, and achieve much better scalability, with a negligible fluctuation in accuracy. When employing our 3D-CNN inference solution on ACL libraries, it can outperform naive ACL implementations by 11× to 50× on ARM many-core processor. When employing our 3D-CNN inference solution on NCNN libraries, it can outperform the naive NCNN implementations by 5.2× to 14.2× on ARM many-core processor.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",3D-CNN; ARM; manycore CPUs; model parallelism; NUMA,Cache memory; Convolution; Hierarchical systems; Libraries; Memory architecture; Program processors; Volumetric analysis; 3d-convolutional neural network; ARM; Convolutional neural network; Hierarchical model; Many-core; Many-core processors; Manycore CPU; Model parallelism; Network inference; Nonuniform memory access; Deep learning
Turn-based Spatiotemporal Coherence for GPUs,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168757555&doi=10.1145%2f3593054&partnerID=40&md5=0233b4090d6ff2fecc00946826f37974,This article introduces turn-based spatiotemporal coherence. Spatiotemporal coherence is a novel coherence implementation that assigns write permission to epochs (or turns) as opposed to a processor core. This paradigm shift in the assignment of write permissions satisfies all conditions of a coherence protocol with virtually no coherence overhead. We discuss the implementation of this coherence mechanism on a baseline GPU. The evaluation shows that spatiotemporal coherence achieves a speedup of 7.13% for workloads with read data reuse across kernels compared to the baseline software-managed GPU coherence implementation while also providing write atomicity and avoiding the need for software inserted acquire-release operations.1  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.,Cache coherency; GPU,Computer software reusability; Program processors; Cache coherency; Coherence protocol; Condition; Data reuse; Paradigm shifts; Processor cores; Spatiotemporal coherence; Graphics processing unit
GraphTune: An Efficient Dependency-Aware Substrate to Alleviate Irregularity in Concurrent Graph Processing,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168764525&doi=10.1145%2f3600091&partnerID=40&md5=27cca6648bc9954ec703f711fb629112,"With the increasing need for graph analysis, massive Concurrent iterative Graph Processing (CGP) jobs are usually performed on the common large-scale real-world graph. Although several solutions have been proposed, these CGP jobs are not coordinated with the consideration of the inherent dependencies in graph data driven by graph topology. As a result, they suffer from redundant and fragmented accesses of the same underlying graph dispersed over distributed platform, because the same graph is typically irregularly traversed by these jobs along different paths at the same time.In this work, we develop GraphTune, which can be integrated into existing distributed graph processing systems, such as D-Galois, Gemini, PowerGraph, and Chaos, to efficiently perform CGP jobs and enhance system throughput. The key component of GraphTune is a dependency-aware synchronous execution engine in conjunction with several optimization strategies based on the constructed cross-iteration dependency graph of chunks. Specifically, GraphTune transparently regularizes the processing behavior of the CGP jobs in a novel synchronous way and assigns the chunks of graph data to be handled by them based on the topological order of the dependency graph so as to maximize the performance. In this way, it can transform the irregular accesses of the chunks into more regular ones so that as many CGP jobs as possible can fully share the data accesses to the common graph. Meanwhile, it also efficiently synchronizes the communications launched by different CGP jobs based on the dependency graph to minimize the communication cost. We integrate it into four cutting-edge distributed graph processing systems and a popular out-of-core graph processing system to demonstrate the efficiency of GraphTune. Experimental results show that GraphTune improves the throughput of CGP jobs by 3.1 ∼ 6.2, 3.8 ∼ 8.5, 3.5 ∼ 10.8, 4.3 ∼ 12.4, and 3.8 ∼ 6.9 times over D-Galois, Gemini, PowerGraph, Chaos, and GraphChi, respectively.  © 2023 Copyright held by the owner/author(s).",,Cutting; Data driven; Dependency graphs; Graph analysis; Graph data; Graph processing; Graph topology; Large-scales; Processing jobs; Processing systems; Real-world graphs; Topology
rNdN: Fast Query Compilation for NVIDIA GPUs,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168761894&doi=10.1145%2f3603503&partnerID=40&md5=aa106098312a9442c3da794e68b7d65b,"GPU database systems are an effective solution to query optimization, particularly with compilation and data caching. They fall short, however, in end-to-end workloads, as existing compiler toolchains are too expensive for use with short-running queries. In this work, we define and evaluate a runtime-suitable query compilation pipeline for NVIDIA GPUs that extracts high performance with only minimal optimization. In particular, our balanced approach successfully trades minor slowdowns in execution for major speedups in compilation, even as data sizes increase. We demonstrate performance benefits compared to both CPU and GPU database systems using interpreters and compilers, extending query compilation for GPUs beyond cached use cases.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",assemblers; compilers; database queries; GPUs; instruction scheduling; JIT; register allocation; SQL,Program compilers; Query languages; Search engines; Assembler; Compiler; Database queries; Effective solution; Fast query; Instruction scheduling; JIT; Query compilation; Register allocation; SQL; Query processing
Fast One-Sided RDMA-Based State Machine Replication for Disaggregated Memory,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163167156&doi=10.1145%2f3587096&partnerID=40&md5=cfdc29964fcf052cf2ea81c5ba3eedde,"Disaggregated memory architecture has risen in popularity for large datacenters with the advantage of improved resource utilization, failure isolation, and elasticity. Replicated state machines (RSMs) have been extensively used for reliability and consistency. In traditional RSM protocols, each replica stores replicated data and has the computing power to participate in some part of the protocols. However, traditional RSM protocols fail to work in the disaggregated memory architecture due to asymmetric resources on CPU nodes and memory nodes. This article proposes ECHO, a fast one-sided RDMA-based RSM protocol with lightweight log replication and remote applying, efficient linearizability guarantee, and fast coordinator failure recovery. ECHO enables all operations in the protocol to be efficiently executed using only one-sided RDMA, without the participation of any computing resource in the memory pool. To provide lightweight log replication and remote applying, ECHO couples the replicated log and the state machine to avoid dual-copy and performs remote applying by updating pointers. To enable efficient remote log state management, ECHO leverages a hitchhiked log state updating scheme to eliminate extra network round trips. To provide efficient linearizability guarantee, ECHO performs immediate remote applying after log replication and leverages the local locks at the coordinator to ensure linear consistency. Moreover, ECHO adopts a commit-Aware log cache to make data visible immediately after being committed. To achieve fast failure recovery, ECHO leverages a commit point identification scheme to reduce the overhead of log consistency recovery. Experimental results demonstrate that ECHO outperforms the state-of-The-Art RSM protocol (namely Sift) in multiple scenarios. For example, ECHO achieves 27%-52% higher throughput on typical write-intensive workloads. Moreover, ECHO reduces the consistency recovery time by three orders of magnitude for coordinator failure. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesReplicated state machine; consensus; memory disaggregation; RDMA,Computer system recovery; Computing power; Network architecture; Recovery; Additional key word and phrasesreplicated state machine; Consensus; Dis-aggregated memory; Disaggregation; Key words; Linearizability; Memory disaggregation; RDMA; State machine replication; State-machine; Memory architecture
Multi-objective Hardware-Aware Neural Architecture Search with Pareto Rank-preserving Surrogate Models,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163196502&doi=10.1145%2f3579853&partnerID=40&md5=e25f411ee3c423dd470e04835d5bb576,"Deep learning (DL) models such as convolutional neural networks (ConvNets) are being deployed to solve various computer vision and natural language processing tasks at the edge. It is a challenge to find the right DL architecture that simultaneously meets the accuracy, power, and performance budgets of such resource-constrained devices. Hardware-Aware Neural Architecture Search (HW-NAS) has recently gained steam by automating the design of efficient DL models for a variety of target hardware platforms. However, such algorithms require excessive computational resources. Thousands of GPU days are required to evaluate and explore an architecture search space such as FBNet [45]. State-of-The-Art approaches propose using surrogate models to predict architecture accuracy and hardware performance to speed up HW-NAS. Existing approaches use independent surrogate models to estimate each objective, resulting in non-optimal Pareto fronts. In this article, HW-PR-NAS,1 a novel Pareto rank-preserving surrogate model for edge computing platforms, is presented. Our model integrates a new loss function that ranks the architectures according to their Pareto rank, regardless of the actual values of the various objectives. We employ a simple yet effective surrogate model architecture that can be generalized to any standard DL model. We then present an optimized evolutionary algorithm that uses and validates our surrogate model. Our approach has been evaluated on seven edge hardware platforms from various classes, including ASIC, FPGA, GPU, and multi-core CPU. The evaluation results show that HW-PR-NAS achieves up to 2.5× speedup compared to state-of-The-Art methods while achieving 98% near the actual Pareto front.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesDeep neural networks; design space exploration; execution time; hardware-Aware neural architecture; model accuracy; power consumption; surrogate models,Budget control; Computer hardware; Computing power; Convolutional neural networks; Deep learning; Graphics processing unit; Natural language processing systems; Network architecture; Additional key word and phrasesdeep neural network; Design space exploration; Execution time; Hardware-aware neural architecture; Key words; Learning models; Modeling accuracy; Neural architectures; Neural-networks; Surrogate modeling; Electric power utilization
Source Matching and Rewriting for MLIR Using String-Based Automata,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162053315&doi=10.1145%2f3571283&partnerID=40&md5=679afb7136084a5f4f786bb030aea1ea,"A typical compiler flow relies on a uni-directional sequence of translation/optimization steps that lower the program abstract representation, making it hard to preserve higher-level program information across each transformation step. On the other hand, modern ISA extensions and hardware accelerators can benefit from the compiler's ability to detect and raise program idioms to acceleration instructions or optimized library calls. Although recent works based on Multi-Level IR (MLIR) have been proposed for code raising, they rely on specialized languages, compiler recompilation, or in-depth dialect knowledge. This article presents Source Matching and Rewriting (SMR), a user-oriented source-code-based approach for MLIR idiom matching and rewriting that does not require a compiler expert's intervention. SMR uses a two-phase automaton-based DAG-matching algorithm inspired by early work on tree-pattern matching. First, the idiom Control-Dependency Graph (CDG) is matched against the program's CDG to rule out code fragments that do not have a control-flow structure similar to the desired idiom. Second, candidate code fragments from the previous phase have their Data-Dependency Graphs (DDGs) constructed and matched against the idiom DDG. Experimental results show that SMR can effectively match idioms from Fortran (FIR) and C (CIL) programs while raising them as BLAS calls to improve performance. Additional experiments also show performance improvements when using SMR to enable code replacement in areas like approximate computing and hardware acceleration.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesIdiom recognition; automata; hardware accelerators; MLIR; rewriting,Abstracting; Automata theory; C (programming language); Codes (symbols); Pattern matching; Program compilers; Program translators; Trees (mathematics); Additional key word and phrasesidiom recognition; Automaton; Code fragments; Dependency graphs; Hardware accelerators; Key words; Matchings; Multi-level IR; Multilevels; Rewriting; Flow graphs
Scale-out Systolic Arrays,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159271801&doi=10.1145%2f3572917&partnerID=40&md5=8e54097be426f700c4068fe9480356e8,"Multi-pod systolic arrays are emerging as the architecture of choice in DNN inference accelerators. Despite their potential, designing multi-pod systolic arrays to maximize effective throughput/Watt-i.e., throughput/Watt adjusted when accounting for array utilization-poses a unique set of challenges. In this work, we study three key pillars in multi-pod systolic array designs, namely array granularity, interconnect, and tiling. We identify optimal array granularity across workloads and show that state-of-The-Art commercial accelerators use suboptimal array sizes for single-Tenancy workloads. We, then evaluate the bandwidth/latency trade-offs in interconnects and show that Butterfly networks offer a scalable topology for accelerators with a large number of pods. Finally, we introduce a novel data tiling scheme with custom partition size to maximize utilization in optimally sized pods. We propose Scale-out Systolic Arrays, a multi-pod inference accelerator for both single-and multi-Tenancy based on these three pillars. We show that SOSA exhibits scaling of up to 600 TeraOps/s in effective throughput for state-of-The-Art DNN inference workloads, and outperforms state-of-The-Art multi-pod accelerators by a factor of 1.5 ×.1  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesDNN accelerators; scale-out architecture,Economic and social effects; Network architecture; Additional key word and phrasesdnn accelerator; Array design; Array sizes; Butterfly networks; Effective throughput; Key words; Optimal arrays; Scale-out architectures; State of the art; Trade off; Systolic arrays
Autotuning Convolutions Is Easier Than You Think,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162075232&doi=10.1145%2f3570641&partnerID=40&md5=cc7fe7d8d914e042f1ee95c28cff5472,"A wide range of scientific and machine learning applications depend on highly optimized implementations of tensor computations. Exploiting the full capacity of a given processor architecture remains a challenging task, due to the complexity of the microarchitectural features that come into play when seeking near-peak performance. Among the state-of-The-Art techniques for loop transformations for performance optimization, AutoScheduler [Zheng et al. 2020a] tends to outperform other systems. It often yields higher performance as compared to vendor libraries, but takes a large number of runs to converge, while also involving a complex training environment. In this article, we define a structured configuration space that enables much faster convergence to high-performance code versions, using only random sampling of candidates. We focus on two-dimensional convolutions on CPUs. Compared to state-of-The-Art libraries, our structured search space enables higher performance for typical tensor shapes encountered in convolution stages in deep learning pipelines. Compared to auto-Tuning code generators like AutoScheduler, it prunes the search space while increasing the density of efficient implementations. We analyze the impact on convergence speed and performance distribution, on two Intel x86 processors and one ARM AArch64 processor. We match or outperform the performance of the state-of-The-Art oneDNN library and TVM's AutoScheduler, while reducing the autotuning effort by at least an order of magnitude.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesCode generation; convolution; microkernel; optimisation space,Codes (symbols); Deep learning; Libraries; Optimization; Program processors; Tensors; Additional key word and phrasescode generation; Autotuning; Key words; Machine learning applications; Micro kernel; Optimisations; Optimization space; Performance; Search spaces; State of the art; Convolution
An Optimized Framework for Matrix Factorization on the New Sunway Many-core Platform,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162164801&doi=10.1145%2f3571856&partnerID=40&md5=92a594ea1c751c80c997cab56288426b,"Matrix factorization functions are used in many areas and often play an important role in the overall performance of the applications. In the LAPACK library, matrix factorization functions are implemented with blocked factorization algorithm, shifting most of the workload to the high-performance Level-3 BLAS functions. But the non-blocked part, the panel factorization, becomes the performance bottleneck, especially for small-and medium-size matrices that are the common cases in many real applications. On the new Sunway many-core platform, the performance bottleneck of panel factorization can be alleviated by keeping the panel in the LDM for the panel factorization. Therefore, we propose a new framework for implementing matrix factorization functions on the new Sunway many-core platform, facilitating the in-LDM panel factorization. The framework provides a template class with wrapper functions, which integrates inter-CPE communication for the Level-1 and Level-2 BLAS functions with flexible interfaces and can accommodate different partitioning schemes. With the framework, writing panel factorization code with data residing in the LDM space can be done with much higher productivity. We implemented three functions (dgetrf, dgeqrf, and dpotrf) based on the framework and compared our work with a CPE_BLAS version, which uses the original LAPACK implementation linked with optimized BLAS library that runs on the CPE mesh. Using the most favorable partitioning, the panel factorization part achieves speedup of up to 26.3, 19.1, and 18.2 for the three matrix factorization functions. For the whole function, our implementation is based on a carefully tuned recursion framework, and we added specific optimization to some subroutines used in the factorization functions. Overall, we obtained average speedup of 9.76 on dgetrf, 10.12 on dgeqrf, and 4.16 on dpotrf, compared to the CPE_BLAS version. Based on the current template class, our work can be extended to support more categories of linear algebra functions.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesMatrix factorization; LAPACK; manycore processors; optimization; parallelization; SW26010Pro,Matrix algebra; Space platforms; Additional key word and phrasesmatrix factorization; Key words; LAPACK; Many-core; Many-core processors; Matrix factorizations; Optimisations; Parallelizations; Performance bottlenecks; Sw26010pro; Factorization
ACTION: Adaptive Cache Block Migration in Distributed Cache Architectures,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162034149&doi=10.1145%2f3572911&partnerID=40&md5=a2a30bda5ebf0717d461167652c30900,"Chip multiprocessors (CMP) with more cores have more traffic to the last-level cache (LLC). Without a corresponding increase in LLC bandwidth, such traffic cannot be sustained, resulting in performance degradation. Previous research focused on data placement techniques to improve access latency in Non-Uniform Cache Architectures (NUCA). Placing data closer to the referring core reduces traffic in cache interconnect. However, earlier data placement work did not account for the frequency with which specific memory references are accessed. The difficulty of tracking access frequency for all memory references is one of the main reasons why it was not considered in NUCA data placement. In this research, we present a hardware-Assisted solution called ACTION (Adaptive Cache Block Migration) to track the access frequency of individual memory references and prioritize placement of frequently referred data closer to the affine core. ACTION mechanism implements cache block migration when there is a detectable change in access frequencies due to a shift in the program phase. ACTION counts access references in the LLC stream using a simple and approximate method and uses a straightforward placement and migration solution to keep the hardware overhead low. We evaluate ACTION on a 4-core CMP with a 5x5 mesh LLC network implementing a partitioned D-NUCA against workloads exhibiting distinct asymmetry in cache block access frequency. Our simulation results indicate that ACTION can improve CMP performance by up to 7.5% over state-of-The-Art (SOTA) D-NUCA solutions.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesCache; Counting Bloom Filter; frequent pages; Non-uniform cache access (NUCA),Cache memory; Memory architecture; Access frequency; Additional key word and phrasescache; Cache blocks; Chip Multiprocessor; Counting bloom filters; Frequent page; Key words; Last-level caches; Non-uniform cache access; Network architecture
User-driven Online Kernel Fusion for SYCL,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162165192&doi=10.1145%2f3571284&partnerID=40&md5=819a02d55ac0cd48df7a21304df16be7,"Heterogeneous programming models are becoming increasingly popular to support the ever-evolving hardware architectures, especially for new and emerging specialized accelerators optimizing specific tasks. While such programs provide performance portability of the existing applications across various heterogeneous architectures to some extent, short-running device kernels can affect an application performance due to overheads of data transfer, synchronization, and kernel launch. While in applications with one or two short-running kernels the overhead can be negligible, it can be noticeable when these short-running kernels dominate the overall number of kernels in an application, as it is the case in graph-based neural network models, where there are several small memory-bound nodes alongside few large compute-bound nodes. To reduce the overhead, combining several kernels into a single, more optimized kernel is an active area of research. However, this task can be time-consuming and error-prone given the huge set of potential combinations. This can push programmers to seek a tradeoff between (a) task-specific kernels with low overhead but hard to maintain and (b) smaller modular kernels with higher overhead but easier to maintain. While there are DSL-based approaches, such as those provided for machine learning frameworks, which offer the possibility of such a fusion, they are limited to a particular domain and exploit specific knowledge of that domain and, as a consequence, are hard to port elsewhere. This study explores the feasibility of a user-driven kernel fusion through an extension to the SYCL API to address the automation of kernel fusion. The proposed solution requires programmers to define the subgraph regions that are potentially suitable for fusion without any modification to the kernel code or the function signature. We evaluate the performance benefit of our approach on common neural networks and study the performance improvement in detail.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesNeural networks; just-in-Time compilers; runtime environments,Application programs; Graphic methods; Network architecture; Neural network models; Additional key word and phrasesneural network; Hardware architecture; Heterogeneous programming; Just-in-time compiler; Kernel fusion; Key words; Programming models; Runtime environments; Specific tasks; User driven; Data transfer
Vitruvius+: An Area-Efficient RISC-V Decoupled Vector Coprocessor for High Performance Computing Applications,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160312502&doi=10.1145%2f3575861&partnerID=40&md5=1608131c0b244dae5ec364356310b1bf,"The maturity level of RISC-V and the availability of domain-specific instruction set extensions, like vector processing, make RISC-V a good candidate for supporting the integration of specialized hardware in processor cores for the High Performance Computing (HPC) application domain. In this article,1 we present Vitruvius+, the vector processing acceleration engine that represents the core of vector instruction execution in the HPC challenge that comes within the EuroHPC initiative. It implements the RISC-V vector extension (RVV) 0.7.1 and can be easily connected to a scalar core using the Open Vector Interface standard. Vitruvius+ natively supports long vectors: 256 double precision floating-point elements in a single vector register. It is composed of a set of identical vector pipelines (lanes), each containing a slice of the Vector Register File and functional units (one integer, one floating point). The vector instruction execution scheme is hybrid in-order/out-of-order and is supported by register renaming and arithmetic/memory instruction decoupling. On a stand-Alone synthesis, Vitruvius+ reaches a maximum frequency of 1.4 GHz in typical conditions (TT/0.80V/25°C) using GlobalFoundries 22FDX FD-SOI. The silicon implementation has a total area of 1.3 mm2 and maximum estimated power of g1/4920 mW for one instance of Vitruvius+ equipped with eight vector lanes.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesRISC-V; HPC; SIMD; vector accelerator,Acceleration; Coprocessor; Digital arithmetic; Pipeline processing systems; Reduced instruction set computing; Additional key word and phrasesrisc-V; Area-Efficient; Co-processors; High performance computing; High-performance computing applications; Key words; Performance computing; SIMD; Vector accelerators; Vector processing; Vectors
Unified Buffer: Compiling Image Processing and Machine Learning Applications to Push-Memory Accelerators,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162179069&doi=10.1145%2f3572908&partnerID=40&md5=440307c66c53b38e5fb4dcbafe388be2,"Image processing and machine learning applications benefit tremendously from hardware acceleration. Existing compilers target either FPGAs, which sacrifice power and performance for programmability, or ASICs, which become obsolete as applications change. Programmable domain-specific accelerators, such as coarse-grained reconfigurable arrays (CGRAs), have emerged as a promising middle-ground, but they have traditionally been difficult compiler targets since they use a different memory abstraction. In contrast to CPUs and GPUs, the memory hierarchies of domain-specific accelerators use push memories: memories that send input data streams to computation kernels or to higher or lower levels in the memory hierarchy and store the resulting output data streams. To address the compilation challenge caused by push memories, we propose that the representation of these memories in the compiler be altered to directly represent them by combining storage with address generation and control logic in a single structure-a unified buffer. The unified buffer abstraction enables the compiler to separate generic push memory optimizations from the mapping to specific memory implementations in the backend. This separation allows our compiler to map high-level Halide applications to different CGRA memory designs, including some with a ready-valid interface. The separation also opens the opportunity for optimizing push memory elements on reconfigurable arrays. Our optimized memory implementation, the Physical Unified Buffer, uses a wide-fetch, single-port SRAM macro with built-in address generation logic to implement a buffer with two read and two write ports. It is 18% smaller and consumes 31% less energy than a physical buffer implementation using a dual-port memory that only supports two ports. Finally, our system evaluation shows that enabling a compiler to support CGRAs leads to performance and energy benefits. Over a wide range of image processing and machine learning applications, our CGRA achieves 4.7× better runtime and 3.5× better energy-efficiency compared to an FPGA.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesHardware accelerators; machine learning; memory abstraction; polyhedral analysis,Abstracting; Computation theory; Computer circuits; Computer hardware description languages; Energy efficiency; Field programmable gate arrays (FPGA); Image processing; Memory architecture; Program compilers; Static random access storage; Additional key word and phraseshardware accelerator; Coarse-grained reconfigurable arrays; Domain specific; Images processing; Key words; Machine learning applications; Machine-learning; Memory abstraction; Memory hierarchy; Polyhedral analysis; Machine learning
FlexPointer: Fast Address Translation Based on Range TLB and Tagged Pointers,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161979589&doi=10.1145%2f3579854&partnerID=40&md5=88871bb88348522d01a63415cc20844b,"Page-based virtual memory relies on TLBs to accelerate the address translation. Nowadays, the gap between application workloads and the capacity of TLB continues to grow, bringing many costly TLB misses and making the TLB a performance bottleneck. Previous studies seek to narrow the gap by exploiting the contiguity of physical pages. One promising solution is to group pages that are both virtually and physically contiguous into a memory range. Recording range translations can greatly increase the TLB reach, but ranges are also hard to index because they have arbitrary bounds. The processor has to compare against all the boundaries to determine which range an address falls in, which restricts the usage of memory ranges. In this article, we propose a tagged-pointer-based scheme, FlexPointer, to solve the range indexing problem. The core insight of FlexPointer is that large memory objects are rare, so we can create memory ranges based on such objects and assign each of them a unique ID. With the range ID integrated into pointers, we can index the range TLB with IDs and greatly simplify its structure. Moreover, because the ID is stored in the unused bits of a pointer and is not manipulated by the address generation, we can shift the range lookup to an earlier stage, working in parallel with the address generation. According to our trace-based simulation results, FlexPointer can reduce nearly all the L1 TLB misses, and page walks for a variety of memory-intensive workloads. Compared with a 4K-page baseline system, FlexPointer shows a 14% performance improvement on average and up to 2.8x speedup in the best case. For other workloads, FlexPointer shows no performance degradation.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesTagged pointer; address translation; TLB reach,Additional key word and phrasestagged pointer; Address generation; Address translation; Key words; Lookups; Performance bottlenecks; Range-based; TLB reach; Trace-based simulation; Virtual memory; Virtual addresses
"HyGain: High-performance, Energy-efficient Hybrid Gain Cell-based Cache Hierarchy",2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162089656&doi=10.1145%2f3572839&partnerID=40&md5=8bebc79590216df6149c15ea3dc5743d,"In this article, we propose a ""full-stack""solution to designing high-Apacity and low-latency on-chip cache hierarchies by starting at the circuit level of the hardware design stack. We propose a novel half VDD precharge 2T Gain Cell (GC) design for the cache hierarchy. The GC has several desirable characteristics, including ∼50% higher storage density and ∼50% lower dynamic energy as compared to the traditional 6T SRAM, even after accounting for peripheral circuit overheads. We also demonstrate data retention time of 350 us (∼17.5× of eDRAM) at 28 nm technology with VDD = 0.9V and temperature = 27°C that, combined with optimizations like staggered refresh, makes it an ideal candidate to architect all levels of on-chip caches. We show that compared to 6T SRAM, for a given area budget, GC-based caches, on average, provide 30% and 36% increase in IPC for single-and multi-programmed workloads, respectively, on contemporary workloads, including SPEC CPU 2017. We also observe dynamic energy savings of 42% and 34% for single-and multi-programmed workloads, respectively. Finally, in a quest to utilize the best of all worlds, we combine GC with STT-RAM to create hybrid hierarchies. We show that a hybrid hierarchy with GC caches at L1 and L2 and an LLC split between GC and STT-RAM is able to provide a 46% benefit in energy-delay product (EDP) as compared to an all-SRAM design, and 13% as compared to an all-GC cache hierarchy, averaged across multi-programmed workloads.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesCache memory; emerging memories; Gain Cell,Budget control; Cache memory; Integrated circuit design; Static random access storage; 6T-SRAM; 6T-SRAMs; Additional key word and phrasescache memory; Cache hierarchies; Cell-based; Dynamic energy; Emerging memory; Gain cell; Key words; On-chip cache; Energy efficiency
A Fast and Flexible FPGA-based Accelerator for Natural Language Processing Neural Networks,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148935129&doi=10.1145%2f3564606&partnerID=40&md5=beb1e95869be703d809cf0b99cc07472,"Deep neural networks (DNNs) have become key solutions in the natural language processing (NLP) domain. However, the existing accelerators customized for their narrow target models cannot support diverse NLP models. Therefore, naively running complex NLP models on the existing accelerators often leads to very marginal performance improvements. For these reasons, architects are now in dire need of a new accelerator that can run various NLP models while taking its full performance potential. In this article, we propose FlexRun, an FPGA-based modular accelerator to efficiently support diverse and complex NLP models. First, we identify key components commonly used by NLP models and implement them on top of a current state-of-The-Art FPGA-based accelerator. Next, FlexRun conducts an in-depth design space exploration to find the best accelerator architecture for a target NLP model. Last, FlexRun automatically reconfigures the accelerator based on the exploration results. Our FlexRun design outperforms the current state-of-The-Art FPGA-based accelerator by 1.21×-2.73× and 1.15×-1.50× for BERT and GPT2, respectively. Compared to Nvidia's V100 GPU, FlexRun achieves 2.69× higher performance on average for various BERT and GPT2 models. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesNeural networks; FPGA; modular architecture; Natural langauge processing,Complex networks; Deep neural networks; Integrated circuit design; Natural language processing systems; Network architecture; 'current; Additional key word and phrasesneural network; Key words; Language processing; Modular architectures; Natural langauge processing; Natural languages; Performance; Processing model; State of the art; Field programmable gate arrays (FPGA)
Quantifying Resource Contention of Co-located Workloads with the System-level Entropy,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149032207&doi=10.1145%2f3563696&partnerID=40&md5=f6f2e52676e110907840fa8a495dd681,"The workload co-location, such as deploying offline analysis workloads with online service workloads on the same node, has become common for modern data centers. Workload co-location deployment improves data center resource utilization significantly. Still, it also introduces resource contention, resulting in the online service's quality of service fluctuation, which we call performance interference. As the online service is a tail-latency-sensitive workload, the tail-latency metric can reflect the performance interference degree of the co-location workloads at the application level. However, to guide system design and evaluation, quantitatively evaluating the resource contention of the co-located workloads at the system level is also essential. This article proposes a novel metric called System-Level Entropy (SLE). As a system-level metric, SLE can measure quantitatively resource contention of the co-location systems and perform the apples-To-Apples comparison between systems. The experimental results show that SLE can accurately reflect the performance interference of workloads and then evaluate the system resource contention. We also demonstrate two case studies of the SLE. We quantify the affinity of different co-location combinations, including three online services and five offline workloads. Furthermore, we evaluate the effects of state-of-The-Art isolated mechanisms (the container and the CPU-Affinity binding) with these co-location combinations. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesCo-location; entropy; resource contention,Fruits; Location; Quality of service; Additional key word and phrasesco-location; Co-located; Colocations; Datacenter; Key words; Off-line analysis; On-line service; Performance; Resource contention; System levels; Entropy
SpecTerminator: Blocking Speculative Side Channels Based on Instruction Classes on RISC-V,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149036540&doi=10.1145%2f3566053&partnerID=40&md5=6c62a8c94c9f43edd2dbebc3d20e0a8d,"In modern processors, speculative execution has significantly improved the performance of processors, but it has also introduced speculative execution vulnerabilities. Recent defenses are based on the delayed execution to block various speculative side channels, but we show that several of the current state-of-The-Art defenses fail to block some of the available speculative side channels, and the current most secure defense introduces a performance overhead of up to 24.5%.We propose SpecTerminator, the first defense framework based on instruction classes that can comprehensively and precisely block all existing speculative side channels. In SpecTerminator, a novel speculative side channel classification scheme based on the features of secret transmission is proposed, and the sensitive instructions in the speculative window are classified and identified using optimized hardware taint tracking and instruction masking techniques to accurately determine the scope of leakage. Then, according to the execution characteristics of these instructions, dedicated delayed execution strategies, such as TLB request ignoring, selective issue, and extended delay-on-miss, are designed for each type of sensitive instruction to precisely control that these instructions are delayed only in pipeline stages that are at risk of leakage. In contrast to previous defenses based on the Gem5 simulator, we have innovatively implemented defenses against Spectre attacks based on the open-source instruction set RISC-V on an FPGA-Accelerated simulation platform that is more similar to real hardware. To evaluate the security of SpecTerminator, we have replicated various existing x86-based Spectre variants on RISC-V. On SPEC 2006, SpecTerminator defends against Spectre attacks based on memory hierarchy side channels with a performance overhead of 2.6% and against all existing Spectre attacks with a performance overhead of 6.0%. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSpectre; RISC-V; side channel; taint tracking,Hardware security; Network security; Open systems; Side channel attack; 'current; Additional key word and phrasesspecter; Blockings; Key words; Modern processors; Performance; RISC-V; Side-channel; Speculative execution; Taint tracking; Simulation platform
YaConv: Convolution with Low Cache Footprint,2023,ACM Transactions on Architecture and Code Optimization,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149051640&doi=10.1145%2f3570305&partnerID=40&md5=c1697e71cafc7d9af444d6a1552fc924,"This article introduces YaConv, a new algorithm to compute convolution using GEMM microkernels from a Basic Linear Algebra Subprograms library that is efficient for multiple CPU architectures. Previous approaches either create a copy of each image element for each filter element or reload these elements into cache for each GEMM call, leading to redundant instances of the image elements in cache. Instead, YaConv loads each image element once into the cache and maximizes the reuse of these elements. The output image is computed by scattering results of the GEMM microkernel calls to the correct locations in the output image. The main advantage of this new algorithm - which leads to better performance in comparison to the existing im2col approach on several architectures - is a more efficient use of the memory hierarchy. The experimental evaluation on convolutional layers from PyTorch, along with a parameterized study, indicates an average 24% speedup over im2col convolution. Increased performance comes as a result of 3× reduction in L3 cache accesses and 2× fewer branch instructions. © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesGEMM; cache performance; convolution,Linear algebra; Memory architecture; Additional key word and phrasesgemm; Basic linear algebra subprograms; Cache performance; CPU architecture; Filter elements; Image elements; Key words; Micro kernel; Performance; Reuse; Convolution
