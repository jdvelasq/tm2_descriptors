Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
Trade: Precise dynamic race detection for scalable transactional memory systems,2015,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056075249&doi=10.1145%2f2786021&partnerID=40&md5=a3ad88e24e396a7825e0388c97fface0,"As other multithreaded programs, transactional memory (TM) programs are prone to race conditions. Previous work focuses on extending existing definitions of data race for lock-based applications to TM applications, which requires all transactions to be totally ordered ""as if"" serialized by a global lock. This approach poses implementation constraints on the STM that severely limits TM applications' performance. This article shows that forcing total ordering among all running transactions, while sufficient, is not necessary. We introduce an alternative data race definition, relaxed transactional data race, that requires ordering of only conflicting transactions. The advantages of our relaxed definition are twofold: First, unlike the previous definition, this definition can be applied to a wide range of TMs, including those that do not enforce transaction total ordering. Second, within a single execution, it exposes a higher number of data races, which considerably reduces debugging time. Based on this definition, we propose a novel and precise race detection tool for C/C++ TM applications (TRADE), which detects data races by tracking happens-before edges among conflicting transactions. Our experiments reveal that TRADE precisely detects data races for STAMP applications running on modern STMs with overhead comparable to state-of-the-art race detectors for lock-based applications. Our experiments also show that in a single run, TRADE identifies several races not discovered by 10 separate runs of a race detection tool based on the previous data race definition. © 2015 ACM.",Correctness; Debugging; Race detection; Transactional memory,Commerce; Computer debugging; Concurrency control; Inspection equipment; Locks (fasteners); Multitasking; Storage allocation (computer); Correctness; Debugging-time; Multi-threaded programs; Number of datum; Race detection; State of the art; Transactional data; Transactional memory; C++ (programming language)
Remote memory access programming in MPI-3,2015,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054207154&doi=10.1145%2f2780584&partnerID=40&md5=3508c762346f3f20a033db46abf7c13e,"The Message Passing Interface (MPI) 3.0 standard, introduced in September 2012, includes a significant update to the one-sided communication interface, also known as remote memory access (RMA). In particular, the interface has been extended to better support popular one-sided and global-address-space parallel programming models to provide better access to hardware performance features and enable new data-access modes. We present the new RMA interface and specify formal axiomatic models for data consistency and access semantics. Such models can help users reason about details of the semantics that are hard to extract from the English prose in the standard. It also fosters the development of tools and compilers, enabling them to automatically analyze, optimize, and debug RMA programs. © 2015 ACM.",MPI; One-sided communication; RMA,Interfaces (computer); Memory architecture; Parallel programming; Program compilers; Program debugging; Semantics; Axiomatic models; Data consistency; Global address spaces; Hardware performance; Message passing interface; One sided communication; Parallel programming model; Remote memory access; Message passing
Parallel scheduling of task trees with limited memory,2015,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976894941&doi=10.1145%2f2779052&partnerID=40&md5=df4af2c05349d43cf26f3992e1322ede,"This article investigates the execution of tree-shaped task graphs using multiple processors. Each edge of such a tree represents some large data. A task can only be executed if all input and output data fit into memory, and a data can only be removed from memory after the completion of the task that uses it as an input data. Such trees arise in the multifrontal method of sparse matrix factorization. The peak memory needed for the processing of the entire tree depends on the execution order of the tasks. With one processor, the objective of the tree traversal is to minimize the required memory. This problem was well studied, and optimal polynomial algorithms were proposed. Here, we extend the problem by considering multiple processors, which is of obvious interest in the application area of matrix factorization.With multiple processors comes the additional objective to minimize the time needed to traverse the tree-that is, to minimize the makespan. Not surprisingly, this problem proves to be much harder than the sequential one. We study the computational complexity of this problem and provide inapproximability results even for unit weight trees. We design a series of practical heuristics achieving different trade-offs between the minimization of peak memory usage and makespan. Some of these heuristics are able to process a tree while keeping the memory usage under a given memory limit. The different heuristics are evaluated in an extensive experimental evaluation using realistic trees. © 2015 ACM.",Approximation algorithms; Memory usage; Multicriteria optimization; Pebble game; Scheduling; Task graphs,Approximation algorithms; Economic and social effects; Factorization; Forestry; Matrix algebra; Multiobjective optimization; Scheduling; Experimental evaluation; Memory usage; Multicriteria optimization; Multifrontal methods; Pebble game; Polynomial algorithm; Sparse matrix factorization; Task graph; Trees (mathematics)
Time-warp: Efficient abort reduction in transactional memory,2015,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978770997&doi=10.1145%2f2775435&partnerID=40&md5=650b615cef0f01794117fc45d835478b,"The multicore revolution that took place one decade ago has turned parallel programming into a major concern for the mainstream software development industry. In this context, Transactional Memory (TM) has emerged as a simpler and attractive alternative to that of lock-based synchronization, whose complexity and error-proneness are widely recognized. The notion of permissiveness in TM translates to only aborting a transaction when it cannot be accepted in any history that guarantees a target correctness criterion. This theoretically powerful property is often neglected by state-of-the-art TMs because it imposes considerable algorithmic costs. Instead, these TMs opt to maximize their implementation's efficiency by aborting transactions under overly conservative conditions. As a result, they risk rejecting a significant number of safe executions. In this article, we seek to identify a sweet spot between permissiveness and efficiency by introducing the Time-Warp Multiversion (TWM) algorithm. TWM is based on the key idea of allowing an update transaction that has performed stale reads (i.e.,missed the writes of concurrently committed transactions) to be serialized by ""committing it in the past,"" which we call a time-warp commit. At its core, TWM uses a novel, lightweight validation mechanism with little computational overhead. TWMalso guarantees that read-only transactions can never be aborted. Further, TWMguarantees Virtual World Consistency, a safety property that is deemed as particularly relevant in the context of TM. We demonstrate the practicality of this approach through an extensive experimental study: we compare TWM with five other TMs, representative of typical alternative design choices, and on a wide variety of benchmarks. This study shows an average performance improvement across all considered workloads and TMs of 65% in high concurrency scenarios, with gains extending up to 9x with the most favorable benchmarks. These results are a consequence of TWM's ability to achieve drastic reduction of aborts in scenarios of nonminimal contention, while introducing little overhead (approximately 10%) in worst-case, synthetically designed scenarios (i.e., no contention or contention patterns that cannot be optimized using TWM). © 2015 ACM.",Multiversion concurrency control; Permissiveness; Software transactional memory; Spurious abort,Benchmarking; Efficiency; Multicore programming; Parallel programming; Software design; Storage allocation (computer); Virtual reality; Conservative conditions; Lock-based synchronization; Multiversion concurrency control; Performance improvements; Permissiveness; Software development industries; Software transactional memory; Spurious abort; Concurrency control
Supporting time-based QoS requirements in software transactional memory,2015,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027467641&doi=10.1145%2f2779621&partnerID=40&md5=9d01c99bc0f3462f80ed798d9a15a6cd,"Software transactional memory (STM) is an optimistic concurrency control mechanism that simplifies parallel programming. However, there has been little interest in its applicability to reactive applications in which there is a required response time for certain operations. We propose supporting such applications by allowing programmers to associate time with atomic blocks in the form of deadlines and quality-of-service (QoS) requirements. Based on statistics of past executions, we adjust the execution mode of transactions by decreasing the level of optimism as the deadline approaches. In the presence of concurrent deadlines, we propose different conflict resolution policies. Execution mode switching mechanisms allow the meeting of multiple deadlines in a consistent manner, with potential QoS degradations being split fairly among several threads as contention increases, and avoiding starvation. Our implementation consists of extensions to an STM runtime that allow gathering statistics and switching execution modes. We also propose novel contention managers adapted to transactional workloads subject to deadlines. The experimental evaluation shows that our approaches significantly improve the likelihood of a transaction meeting its deadline and QoS requirement, even in cases where progress is hampered by conflicts and other concurrent transactions with deadlines. © 2015 ACM.",Contention management; Fairness; Quality of service; Scheduling; Transactional memory,Concurrency control; Parallel programming; Scheduling; Storage allocation (computer); Concurrent transactions; Contention managements; Experimental evaluation; Fairness; Optimistic concurrency controls; Qualityof-service requirement (QoS); Software transactional memory; Transactional memory; Quality of service
Linear and competitive strategies for continuous robot formation problems,2015,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036588200&doi=10.1145%2f2742341&partnerID=40&md5=b113f90c8b82c79f96ed0a732c5abd4a,"We study a scenario in which n mobile robots with a limited viewing range are distributed in the Euclidean plane and have to solve a formation problem. The formation problems we consider are the GATHERING problem and the CHAIN-FORMATION problem. In the GATHERING problem, the robots have to gather in one (not predefined) point, while in the CHAIN-FORMATION problem they have to form a connected communication chain of minimal length between two stationary base stations. Each robot may base its decisions where to move only on the current relative positions of neighboring robots (that are within its viewing range); that is, besides having a limited viewing range, the robots are oblivious (they do not use information from the past), have none or only very limited identities, and they do not have a common sense of direction. Variants of these problems (especially for the GATHERING problem) have been studied extensively in different discrete time models. In contrast, our work focuses on a continuous time model; that is, the robots continuously sense the positions of other robots within their viewing range and continuously adapt their speed and direction according to some simple, local rules. Hereby, we assume that the robots have a maximum movement speed of one. We show that this idealized idea of continuous sensing allows us to solve the mentioned formation problems in linear time O(n) (which, given the maximum speed of one, immediately yields a maximum traveled distance of O(n)). Note that in the more classical discrete time models, the best known strategies need at least O(n2) or even O(n2 log n) timesteps to solve these problems. For the GATHERING problem, our analysis solves a problem left open by Gordon et al. [2004], where the authors could prove that gathering in a continuous model is possible in finite time, but were not able to give runtime bounds. Apart from these linear bounds, we also provide runtime bounds for both formation problems that relate the runtime of our strategies to the runtime of an optimal, global algorithm. Specifically, we show that our strategy for the GATHERING problem is log OPT-competitive and the strategy for the CHAIN-FORMATION problem is log n-competitive. Here, by c-competitive, we mean that our (local) strategy is asymptotically by at most a factor of c slower than an optimal, global strategy. © 2015 ACM.",Distributed algorithms; Local algorithms; Mobile robots; Robot gathering,Chains; Continuous time systems; Geometry; Mobile robots; Parallel algorithms; Communication chains; Competitive strategy; Continuous modeling; Continuous sensing; Continuous time modeling; Discrete-time model; Local algorithm; Relative positions; Problem solving
Hedonic clustering games,2015,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987948542&doi=10.1145%2f2742345&partnerID=40&md5=9988324a33cd2dc63714ba83bb33de34,"Clustering, the partitioning of objects with respect to a similarity measure, has been extensively studied as a global optimization problem. We investigate clustering from a game-theoretic approach, and consider the class of hedonic clustering games. Here, a self-organized clustering is obtained via decisions made by independent players, corresponding to the elements clustered. Being a hedonic setting, the utility of each player is determined by the identity of the othermembers of her cluster. This class of games seems to be quite robust, as it fits with rather different, yet commonly used, clustering criteria. Specifically, we investigate hedonic clustering games in two different models: fixed clustering, which subdivides into k-median and k-center, and correlation clustering. We provide a thorough analysis of these games, characterizing Nash equilibria, and proving upper and lower bounds on the price of anarchy and price of stability. For fixed clustering we focus on the existence of a Nash equilibrium, as it is a rather nontrivial issue in this setting. We study it both for general metrics and special cases, such as line and tree metrics. In the correlation clustering model, we study both minimization and maximization variants, and provide almost tight bounds on both the price of anarchy and price of stability. © 2015 ACM.",Clustering games; Hedonic games; Price of anarchy; Price of stability,Global optimization; Clustering games; Correlation clustering; Global optimization problems; Hedonic games; Price of anarchy; Price of Stability; Self-organized clustering; Upper and lower bounds; Game theory
Noise-tolerant explicit stencil computations for nonuniform process execution rates,2015,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040823625&doi=10.1145%2f2742351&partnerID=40&md5=bbaf46a13c7c717bc8f7dc0bf1560d0c,"Next-generation HPC computing platforms are likely to be characterized by significant, unpredictable nonuniformities in execution time among compute nodes and cores. The resulting load imbalances from this nonuniformity are expected to arise from a variety of sources - manufacturing discrepancies, dynamic power management, runtime component failure, OS jitter, software-mediated resiliency, and TLB/- cache performance variations, for example. It is well understood that existing algorithms with frequent points of bulk synchronization will perform relatively poorly in the presence of these sources of process nonuniformity. Thus, recasting classic bulk synchronous algorithms into more asynchronous, coarse-grained parallelism is a critical area of research for next-generation computing. We propose a class of parallel algorithms for explicit stencil computations that can tolerate these nonuniformities by decoupling per process communication and computation in order for each process to progress asynchronously while maintaining solution correctness. These algorithms are benchmarked with a 1D domain decomposed (""slabbed"") implementation of the 2D heat equation as amodel problem, and are tested in the presence of simulated nonuniform process execution rates. The resulting performance is compared to a classic bulk synchronous implementation of the model problem. Results show that the runtime of this article's algorithm on a machine with simulated process nonuniformities is 5-99% slower than the runtime of its classic counterpart on a machine free of nonuniformities. However, when both algorithms are run on a machine with comparable synthetic process nonuniformities, this article's algorithm is 1-37 times faster than its classic counterpart. © 2015 ACM.",Barrier relaxation; BSP; Bulk synchronous parallelism; Coarse-grained parallelism; Dynamic barrier relaxation; Dynamic ghost zone; Dynamic ghost-zone optimization; Dynamic stencil; Heat equation; PDE; Relaxed BSP; Stencil,Heat transfer; Barrier relaxation; Bulk synchronous parallelism; Coarse-grained; Heat equation; Relaxed BSP; Stencil; Partial differential equations
Near-optimal scheduling mechanisms for deadline-sensitive jobs in large computing clusters,2015,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962320282&doi=10.1145%2f2742343&partnerID=40&md5=4eb9211f4082024aac97391aee8a78bf,"We consider a market-based resource allocation model for batch jobs in cloud computing clusters. In our model, we incorporate the importance of the due date of a job rather than the number of servers allocated to it at any given time. Each batch job is characterized by the work volume of total computing units (e.g., CPU hours) along with a bound on maximum degree of parallelism. Users specify, along with these job characteristics, their desired due date and a value for finishing the job by its deadline. Given this specification, the primary goal is to determine the scheduling of cloud computing instances under capacity constraints in order to maximize the social welfare (i.e., sum of values gained by allocated users). Our main result is a new (C/C-k.s/s-1)-approximation algorithm for this objective, where C denotes cloud capacity, k is the maximal bound on parallelized execution (in practical settings, k < C) and s is the slackness on the job completion time, that is, the minimal ratio between a specified deadline and the earliest finish time of a job. Our algorithm is based on utilizing dual fitting arguments over a strengthened linear program to the problem. Based on the new approximation algorithm, we construct truthful allocation and pricing mechanisms, in which reporting the true value and other properties of the job (deadline, work volume, and the parallelism bound) is a dominant strategy for all users. To that end, we extend known results for single-value settings to provide a general framework for transforming allocation algorithms into truthful mechanisms in domains of single-value and multi-properties. We then show that the basic mechanism can be extended under proper Bayesian assumptions to the objective of maximizing revenues, which is important for public clouds. We empirically evaluate the benefits of our approach through simulations on data-center job traces, and show that the revenues obtained under our mechanism are comparable with an ideal fixed-price mechanism, which sets an on-demand price using oracle knowledge of users' valuations. Finally, we discuss how our model can be extended to accommodate uncertainties in job work volumes, which is a practical challenge in cloud settings. © 2015 ACM.",Cloud computing; Economic models; Resource allocation; Scheduling algorithms; Truthful mechanisms,Approximation algorithms; Cloud computing; Economics; Linear programming; Resource allocation; Scheduling; Allocation algorithm; Capacity constraints; Dominant strategy; Economic models; Job characteristics; Market-based resource allocation; Pricing mechanism; Truthful mechanisms; Scheduling algorithms
Guest editor introduction,2015,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056359532&doi=10.1145%2f2716306&partnerID=40&md5=d42d13400531afda242b13179030a369,[No abstract available],,
The shape of the search tree for the maximum clique problem and the implications for parallel branch and bound,2015,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944592758&doi=10.1145%2f2742359&partnerID=40&md5=25d13836ae22fe2497b089d30a065b83,"Finding a maximum clique in a given graph is one of the fundamental NP-hard problems. We compare two multicore thread-parallel adaptations of a state-of-the-art branch-and-bound algorithm for the maximum clique problem and provide a novel explanation as to why they are successful. We show that load balance is sometimes a problem but that the interaction of parallel search order and the most likely location of solutions within the search space is often the dominating consideration. We use this explanation to propose a new low-overhead, scalable work-splitting mechanism. Our approach uses explicit early diversity to avoid strong commitment to the weakest heuristic advice and late resplitting for balance. More generally, we argue that, for branch-and-bound, parallel algorithm design should not be performed independently of the underlying sequential algorithm. © 2015 ACM.",Heuristics; Irregular parallel problems; Maximum clique problem; Multicore algorithms; Parallel branch and bound,Computational complexity; Heuristics; Irregular parallel problems; Maximum clique problems; Multi-core algorithm; Parallel branch and bounds; Branch and bound method
Simple virtual channel allocation for high-throughput and high-frequency on-chip routers,2015,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028771363&doi=10.1145%2f2742349&partnerID=40&md5=d7e184d82d6fa0294e83f289805d3a08,"Packet-switched network-on-chip (NoC) has provided a scalable solution to the communications for tiledmulticore processors. However, the virtual channel (VC) buffers in the NoC consume significant dynamic and leakage power. To improve the energy efficiency of the router design, it is advantageous to use small buffer sizes while still maintaining throughput of the network. This article proposes two new virtual channel allocation (VA) mechanisms, termed fixed VC assignment with dynamic VC allocation (FVADA) and adjustable VC assignment with dynamic VC allocation (AVADA). VCs are designated to output ports and allocated to packets according to such assignment. This can help to reduce the head-of-line blocking. Such VC-output port assignment can also be adjusted dynamically to accommodate traffic changes. Simulation results show that both mechanisms can improve network throughput by 41% on average. Real traffic evaluation shows a network latency reduction of up to 66%. In addition, AVADA can outperform the baseline in throughput with only half of the buffer size. Finally, we are able to achieve comparable or better throughput than a previous dynamic VC allocator while reducing its critical path delay by 57%. Hence, the proposed VA mechanisms are suitable for low-power, high-throughput, and high-frequency NoC designs. © 2015 ACM.",Chip multiprocessor; Dynamic allocation; Network-on-chip; Thousand-core; Virtual channel; Virtual channel allocation,Dynamics; Energy efficiency; Packet switching; Routers; Servers; Throughput; Chip Multiprocessor; Critical path delays; Dynamic allocations; Head of line blocking; Network latencies; Network throughput; Thousand-core; Virtual channels; Network-on-chip
Runtime resource allocation for software pipelines,2015,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045964218&doi=10.1145%2f2742347&partnerID=40&md5=e224a88b61c7eb6de79166e75f677361,"Efficiently allocating the computational resources of many-core systems is one of the most prominent challenges, especially when resource requirements may vary unpredictably at runtime. This is even more challenging when facing unreliable cores - a scenario that becomes common as the number of cores increases and integration sizes shrink. To address this challenge, this article presents an optimal method for the allocation of the resources to software-pipelined applications. Here we show how runtime observations of the resource requirements of tasks can be used to adapt resource allocations. Furthermore, we show how the optimum can be traded for a high degree of scalability by clustering applications in a distributed, hierarchical manner. To diminish the negative effects of unreliable cores, this article shows how self-organization can effectively restore the integrity of such a hierarchy when it is corrupted by a failing core. Experiments on Intel's 48-core Single-Chip Cloud Computer and in a many-core simulator show that a significant improvement in system throughput can be achieved over the current state of the art. © 2015 ACM.",Distributed systems; Many-core systems; Resource allocation; Runtime system management; Software pipelines; Task mapping,Application programs; Pipelines; Resource allocation; Distributed systems; Many core; Runtime systems; Software pipeline; Task mapping; Distributed computer systems
Profitable scheduling on multiple speed-scalable processors,2015,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056430930&doi=10.1145%2f2809872&partnerID=40&md5=062b11461b3e09568ac218b3bb1df99b,"We present a new online algorithm for profit-oriented scheduling on multiple speed-scalable processors and provide a tight analysis of the algorithm's competitiveness. Our results generalize and improve upon work by Chan et al. [2010], which considers a single speed-scalable processor. Using significantly different techniques, we can not only extend their model to multiprocessors but also prove an enhanced and tight competitive ratio for our algorithm. In our scheduling problem, jobs arrive over time and are preemptable. They have different workloads, values, and deadlines. The scheduler may decide not to finish a job but instead to suffer a loss equaling the job's value. However, to process a job's workload until its deadline the scheduler must invest a certain amount of energy. The cost of a schedule is the sum of lost values and invested energy. In order to finish a job, the scheduler has to determine which processors to use and set their speeds accordingly. A processor's energy consumption is power Pα (s) integrated over time, where Pα (s) = sα is the power consumption when running at speed s. Since we consider the online variant of the problem, the scheduler has no knowledge about future jobs. This problem was introduced by Chan et al. [2010] for the case of a single processor. They presented an online algorithm that is αα + 2eα-competitive. We provide an online algorithm for the case of multiple processors with an improved competitive ratio of αα. © 2015 ACM 2329-4949/2015/09-ART19 $15.00",Convex programming; Energy; Online algorithms; Primal-dual; Scheduling,Convex optimization; Energy utilization; Profitability; Speed; Competitive ratio; Energy; Multiple processors; On-line algorithms; Primal-dual; Scalable processors; Scheduling problem; Single processors; Scheduling
Sybilcast: Broadcast on the open airwaves,2015,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054527488&doi=10.1145%2f2809810&partnerID=40&md5=b1cb3ca1cd580362414328cc62f7a0d7,"Consider a scenario where many wireless users are attempting to download data from a single base station. While most of the users are honest, some users may be malicious and attempt to obtain more than their fair share of the bandwidth. One possible strategy for attacking the system is to simulate multiple fake identities, each of which is given its own equal share of the bandwidth. Such an attack is often referred to as a sybil attack. To counter such behavior, we propose SybilCast, a protocol for multichannel wireless networks that limits the number of fake identities and, in doing so, ensures that each honest user gets at least a constant fraction of his or her fair share of the bandwidth. As a result, each honest user can complete his or her data download in asymptotically optimal time. A key aspect of this protocol is balancing the rate at which new identities are admitted and the maximum number of fake identities that can coexist while keeping the overhead low. Besides sybil attacks, our protocol can also tolerate spoofing and jamming. © 2015 ACM 2329-4949/2015/09-ART16 $15.00",Fairness; Sybil attack; Wireless networks,Bandwidth; Wireless networks; Asymptotically optimal; Fair share; Fairness; Multi-channel wireless networks; Protocol cans; Sybil attack; Wireless users; Computer crime
Introduction to the special issue on PPoPP'12,2015,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056348590&doi=10.1145%2f2716343&partnerID=40&md5=663057fd4c820a32fdf470373cc82cd1,[No abstract available],,
Work-efficient matrix inversion in polylogarithmic time,2015,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041486563&doi=10.1145%2f2809812&partnerID=40&md5=74eb8dd46bdc00c17309b36512a8a266,"We present an algorithm for inversion of symmetric positive definite matrices that combines the practical requirement of an optimal number of arithmetic operations and the theoretical goal of a polylogarithmic critical path length. The algorithm reduces inversion to matrix multiplication. It uses Strassen's recursion scheme, but on the critical path it breaks the recursion early, switching to an asymptotically inefficient yet fast use of Newton's method. We also show that the algorithm is numerically stable. Overall, we get a candidate for a massively parallel algorithm that scales to exascale systems even on relatively small inputs. © 2015 ACM 2329-4949/2015/09-ART15 $15.00",Linear algebra; Matrix inversion; Newton approximation; Numerics; Parallel algorithms; Polylogarithmic time; Strassen's inversion algorithm,Approximation algorithms; Linear algebra; Newton-Raphson method; Parallel algorithms; Inversion algorithm; Matrix inversions; Newton approximations; Numerics; Polylogarithmic time; Matrix algebra
SciPAL: Expression templates and composition closure objects for high performance computational physics with CUDA and openMP,2015,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994100565&doi=10.1145%2f2686886&partnerID=40&md5=fb4e550795b0596e8de3c94772a82607,"We present SciPAL (scientific parallel algorithms library), a C++-based, hardware-independent open-source library. Its core is a domain-specific embedded language for numerical linear algebra. The main fields of application are finite element simulations, coherent optics and the solution of inverse problems. Using Sci- PAL, algorithms can be stated in a mathematically intuitive way in terms of matrix and vector operations. Existing algorithms can easily be adapted to GPU-based computing by proper template specialization. Our library is compatible with the finite element library deal.II and provides a port of deal.II's most frequently used linear algebra classes to CUDA (NVidia's extension of the programming languages C and C++ for programming their GPUs). SciPAL's operator-based API for BLAS operations particularly aims at simplifying the usage of NVidia's CUBLAS. For non-BLAS array arithmetic SciPAL's expression templates are able to generate CUDA kernels at compile time.We demonstrate the benefits of SciPAL using the iterative principal component analysis as example which is the core algorithm for the spike-sorting problem in neuroscience. © 2015 ACM.",Abstract parallel programming; BLAS; C++ library; Domain-specific languages; Expression templates; numerical linear algebra; Operator overloading,Application programming interfaces (API); Computer systems programming; Inverse problems; Iterative methods; Linear algebra; Object oriented programming; Open source software; Open systems; Parallel programming; Principal component analysis; Problem oriented languages; Program processors; BLAS; C++ libraries; Domain specific languages; Expression templates; Numerical Linear Algebra; Operator overloading; C++ (programming language)
Introduction to the special issue on SPAA 2013,2015,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056422810&doi=10.1145%2f2809923&partnerID=40&md5=915039769ba274d66198591699f9154c,[No abstract available],,
Coalescing-branching random walks on graphs,2015,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979735480&doi=10.1145%2f2817830&partnerID=40&md5=74f590cc4100b18b2ef7bdaa0a57d4cb,"We study a distributed randomized information propagation mechanism in networks we call the coalescing-branching random walk (cobra walk, for short). A cobra walk is a generalization of the well-studied “standard” random walk, and is useful in modeling and understanding the Susceptible-Infected- Susceptible (SIS)-type of epidemic processes in networks. It can also be helpful in performing light-weight information dissemination in resource-constrained networks. A cobra walk is parameterized by a branching factor k. The process starts from an arbitrary vertex, which is labeled active for step 1. In each step of a cobra walk, each active vertex chooses k random neighbors to become active for the next step (“branching”). A vertex is active for step t + 1 only if it is chosen by an active vertex in step t (“coalescing”). This results in a stochastic process in the underlying network with properties that are quite different from both the standard random walk (which is equivalent to the cobra walk with branching factor 1) as well as other gossip-based rumor spreading mechanisms. We focus on the cover time of the cobra walk, which is the number of steps for the walk to reach all the vertices, and derive almost-tight bounds for various graph classes. We show an O(log2 n) high probability bound for the cover time of cobra walks on expanders, if either the expansion factor or the branching factor is sufficiently large; we also obtain an O(log n) high probability bound for the partial cover time, which is the number of steps needed for the walk to reach at least a constant fraction of the vertices. We also show that the cover time of the cobra walk is, with high probability, O(nlog n) on any n-vertex tree for k ≥ 2, Õ(n1/d) on a d-dimensional grid for k ≥ 2, and O(log n) on the complete graph. © 2015 ACM.",Cover time; Epidemic processes; Information spreading,Flocculation; Information dissemination; Random processes; Stochastic systems; Trees (mathematics); Branching random walks; Cover time; D-dimensional grids; Epidemic process; Information propagation; Information spreading; Resource-constrained network; Susceptible-infected-susceptible; Graph theory
"Algorithm-based fault tolerance for dense matrix factorizations, multiple failures and accuracy",2015,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016271812&doi=10.1145%2f2686892&partnerID=40&md5=804ce9ba0a9628138261243bd811a767,"Densematrix factorizations, such as LU, Cholesky and QR, are widely used for scientific applications that require solving systems of linear equations, eigenvalues and linear least squares problems. Such computations are normally carried out on supercomputers, whose ever-growing scale induces a fast decline of the Mean Time To Failure (MTTF). This article proposes a new hybrid approach, based on Algorithm-Based Fault Tolerance (ABFT), to help matrix factorizations algorithms survive fail-stop failures. We consider extreme conditions, such as the absence of any reliable node and the possibility of losing both data and checksum from a single failure. We will present a generic solution for protecting the right factor, where the updates are applied, of all above mentioned factorizations. For the left factor, where the panel has been applied, we propose a scalable checkpointing algorithm. This algorithm features high degree of checkpointing parallelism and cooperatively utilizes the checksum storage leftover from the right factor protection. The fault-tolerant algorithms derived from this hybrid solution is applicable to a wide range of dense matrix factorizations, with minormodifications. Theoretical analysis shows that the fault tolerance overhead decreases inversely to the scaling in the number of computing units and the problem size. Experimental results of LU and QR factorization on the Kraken (Cray XT5) supercomputer validate the theoretical evaluation and confirm negligible overhead, with- and without-errors. Applicability to tolerate multiple failures and accuracy after multiple recovery is also considered. © 2015 ACM.",ABFT; Fault-tolerance; High performance computing; Linear algebra,Digital storage; Eigenvalues and eigenfunctions; Fault tolerance; Linear algebra; Matrix algebra; Supercomputers; ABFT; Algorithm based fault tolerance; Fault tolerant algorithms; High performance computing; Linear least squares problems; Scalable Checkpointing; Scientific applications; Systems of linear equations; Factorization
Power management of extreme-scale networks with on/off links in runtime systems,2015,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992728987&doi=10.1145%2f2687001&partnerID=40&md5=659fee31bd21c030c729f6bd3cd3ddf9,"Networks are among major power consumers in large-scale parallel systems. During execution of common parallel applications, a sizeable fraction of the links in the high-radix interconnects are either never used or are underutilized. We propose a runtime system based adaptive approach to turn off unused links, which has various advantages over the previously proposed hardware and compiler based approaches. We discuss why the runtime system is the best system component to accomplish this task, and test the effectiveness of our approach using real applications (including NAMD, MILC), and application benchmarks (including NAS Parallel Benchmarks, Stencil). These codes are simulated on representative topologies such as 6-D Torus and multilevel directly connected network (similar to IBM PERCS in Power 775 and Dragonfly in Cray Aries). For common applications with near-neighbor communication pattern, our approach can save up to 20% of total machine's power and energy, without any performance penalty. © 2015 ACM.",Algorithms; Design; Measurement; Performance,Algorithms; Design; Measurement; Power management; Topology; Communication pattern; Connected networks; Large-scale parallel systems; NAS parallel benchmarks; Parallel application; Performance; Performance penalties; Real applications; Benchmarking
Avoiding communication in successive band reduction,2015,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027863495&doi=10.1145%2f2686877&partnerID=40&md5=20d28feffe93a2f2e6e515131c8fb18b,"The running time of an algorithm depends on both arithmetic and communication (i.e., data movement) costs, and the relative costs of communication are growing over time. In this work, we present sequential and distributed-memory parallel algorithms for tridiagonalizing full symmetric and symmetric band matrices that asymptotically reduce communication compared to previous approaches. The tridiagonalization of a symmetric band matrix is a key kernel in solving the symmetric eigenvalue problem for both full and band matrices. In order to preserve structure, tridiagonalization routines use annihilate-and-chase procedures that previously have suffered from poor data locality and high parallel latency cost. We improve both by reorganizing the computation and obtain asymptotic improvements. We also propose new algorithms for reducing a full symmetric matrix to band form in a communication-efficient manner. In this article, we consider the cases of computing eigenvalues only and of computing eigenvalues and all eigenvectors. © 2015 ACM.",Band reduction; Communication avoiding algorithms; Symmetric eigenvalue problem,Matrix algebra; Band reduction; Communication avoiding algorithms; Data movements; Distributed Memory; Relative costs; Symmetric eigenvalue problems; Symmetric matrices; Tridiagonalization; Eigenvalues and eigenfunctions
Collective algorithms for multiported torus networks,2015,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013212617&doi=10.1145%2f2686882&partnerID=40&md5=0fd426997ca1b1c5f682dd5216fcb686,"Modern supercomputers with torus networks allow each node to simultaneously pass messages on all of its links. However, most collective algorithms are designed to only use one link at a time. In this work, we present novel multiported algorithms for the scatter, gather, all-gather, and reduce-scatter operations. Our algorithms can be combined to create multiported reduce, all-reduce, and broadcast algorithms. Several of these algorithms involve a new technique where we relax the MPI message-ordering constraints to achieve high performance and restore the correct ordering using an additional stage of redundant communication. According to our models, on an n-dimensional torus, our algorithms should allow for nearly a 2n-fold improvement in communication performance compared to known, single-ported torus algorithms. In practice, we have achieved nearly 6x better performance on a 32k-node 3-dimensional torus. © 2015 ACM.",Collective algorithms; Message-passing,Supercomputers; 3-dimensional; All-reduce; Broadcast algorithm; Communication performance; Message ordering; Torus networks; Message passing
IRIS: A robust information system against insider DoS attacks,2015,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048096670&doi=10.1145%2f2809806&partnerID=40&md5=ae622f9b4ce892d4f6cef81dd5292c2b,"In this work, we present the first scalable distributed information system, that is, a system with low storage overhead, that is provably robust against denial-of-service (DoS) attacks by a current insider. We allow a current insider to have complete knowledge about the information system and to have the power to block any ε-fraction of its servers by a DoS attack, where ε can be chosen up to a constant. The task of the system is to serve any collection of lookup requests with at most one per nonblocked server in an efficient way despite this attack. Previously, scalable solutions were only known for DoS attacks of past insiders, where a past insider only has complete knowledge about some past time point t0 of the information system. Scheideler et al. [Awerbuch and Scheideler 2007; Baumgart et al. 2009] showed that in this case, it is possible to design an information system so that any information that was inserted or last updated after t0 is safe against a DoS attack. But their constructions would not work at all for a current insider. The key idea behind our IRIS system is to make extensive use of coding. More precisely, we present two alternative distributed coding strategies with an at most logarithmic storage overhead that can handle up to a constant fraction of blocked servers. © 2015 ACM 2329-4949/2015/10-ART18 $15.00",Denial-of-service attacks; DHT; Distributed systems,Distributed computer systems; Information systems; Information use; Network security; Denial of Service; Distributed coding; Distributed information systems; Distributed systems; Low-storage; Scalable solution; Storage overhead; Time points; Denial-of-service attack
On-the-fly pipeline parallelism,2015,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014502837&doi=10.1145%2f2809808&partnerID=40&md5=d980a46130fb31c3ccd6c54b73252da1,"Pipeline parallelism organizes a parallel program as a linear sequence of stages. Each stage processes elements of a data stream, passing each processed data element to the next stage, and then taking on a new element before the subsequent stages have necessarily completed their processing. Pipeline parallelism is used especially in streaming applications that perform video, audio, and digital signal processing. Three out of 13 benchmarks in PARSEC, a popular software benchmark suite designed for shared-memory multiprocessors, can be expressed as pipeline parallelism. Whereas most concurrency platforms that support pipeline parallelism use a “construct-and-run” approach, this article investigates “on-the-fly” pipeline parallelism, where the structure of the pipeline emerges as the program executes rather than being specified a priori. On-the-fly pipeline parallelism allows the number of stages to vary from iteration to iteration and dependencies to be data dependent. We propose simple linguistics for specifying on-the-fly pipeline parallelism and describe a provably efficient scheduling algorithm, the PIPER algorithm, which integrates pipeline parallelism into a work-stealing scheduler, allowing pipeline and fork-join parallelism to be arbitrarily nested. The PIPER algorithm automatically throttles the parallelism, precluding “runaway” pipelines. Given a pipeline computation with T1 work and T∞ span (critical-path length), PIPER executes the computation on P processors in TP ≤ T1/P+ O(T∞ +lg P) expected time. PIPER also limits stack space, ensuring that it does not grow unboundedly with running time. We have incorporated on-the-fly pipeline parallelism into a Cilk-based work-stealing runtime system. Our prototype Cilk-P implementation exploits optimizations such as “lazy enabling” and “dependency folding.” We have ported the three PARSEC benchmarks that exhibit pipeline parallelism to run on Cilk-P. One of these, x264, cannot readily be executed by systems that support only construct-and-run pipeline parallelism. Benchmark results indicate that Cilk-P has low serial overhead and good scalability. On x264, for example, Cilk-P exhibits a speedup of 13.87 over its respective serial counterpart when running on 16 processors. © 2015 ACM 2329-4949/2015/09-ART17 $15.00",Cilk; Multicore; Multithreading; On-the-fly pipelining; Parallel programming; Pipeline parallelism; Scheduling; Work stealing,Digital signal processing; Integrated circuit testing; Iterative methods; Multicore programming; Multitasking; Parallel processing systems; Parallel programming; Pipelines; Scheduling; Scheduling algorithms; Cilk; Multi core; Multi-threading; On the flies; Pipeline parallelisms; Work-stealing; Pipeline processing systems
High-performance and scalable GPU graph traversal,2015,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013666792&doi=10.1145%2f2717511&partnerID=40&md5=25e0149b8634405043ece8caec5be50b,"Breadth-First Search (BFS) is a core primitive for graph traversal and a basis for many higher-level graph analysis algorithms. It is also representative of a class of parallel computations whose memory accesses and work distribution are both irregular and data dependent. Recent work has demonstrated the plausibility of GPU sparse graph traversal, but has tended to focus on asymptotically inefficient algorithms that perform poorly on graphs with nontrivial diameter. We present a BFS parallelization focused on fine-grained task management constructed from efficient prefix sum computations that achieves an asymptotically optimal O(|V| + |E|) gd work complexity. Our implementation delivers excellent performance on diverse graphs, achieving traversal rates in excess of 3.3 billion and 8.3 billion traversed edges per second using single- and quad-GPU configurations, respectively. This level of performance is several times faster than state-of-the-art implementations on both CPU and GPU platforms. © 2015 ACM.",Breadth-first search; GPU; Graph algorithms; Graph traversal; Parallel algorithms; Prefix sum; Sparse graphs,Graphics processing unit; Parallel algorithms; Breadth-first search; Graph algorithms; Graph traversals; Prefix sum; Sparse graphs; Graph theory
Fast greedy algorithms in MapReduce and streaming,2015,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978499270&doi=10.1145%2f2809814&partnerID=40&md5=5124a9e909283c1f1490e2916ee74bfa,"Greedy algorithms are practitioners' best friends-they are intuitive, are simple to implement, and often lead to very good solutions. However, implementing greedy algorithms in a distributed setting is challenging since the greedy choice is inherently sequential, and it is not clear how to take advantage of the extra processing power. Our main result is a powerful sampling technique that aids in parallelization of sequential algorithms. Armed with this primitive, we then adapt a broad class of greedy algorithms to the MapReduce paradigm; this class includes maximum cover and submodular maximization subject to p-system constraint problems. Our method yields efficient algorithms that run in a logarithmic number of rounds while obtaining solutions that are arbitrarily close to those produced by the standard sequential greedy algorithm. We begin with algorithms for modular maximization subject to a matroid constraint and then extend this approach to obtain approximation algorithms for submodular maximization subject to knapsack or p-system constraints. © 2015 ACM 2329-4949/2015/09-ART14 $15.00",Algorithm analysis; Approximation algorithms; Distributed computing; Greedy algorithms; MapReduce; Submodular function,Distributed computer systems; Algorithm analysis; Greedy algorithms; Map-reduce; Parallelizations; Processing power; Sampling technique; Sequential algorithm; Submodular functions; Approximation algorithms
Lock cohorting: A general technique for designing NUMA locks,2015,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954143807&doi=10.1145%2f2686884&partnerID=40&md5=189c28ad9376c85db2484d40ea5e5f1c,"Multicore machines are quickly shifting to NUMA and CC-NUMA architectures, making scalable NUMAaware locking algorithms, ones that take into account the machine's nonuniform memory and caching hierarchy, ever more important. This article presents lock cohorting, a general new technique for designing NUMA-aware locks that is as simple as it is powerful. Lock cohorting allows one to transform any spin-lock algorithm, with minimal nonintrusive changes, into a scalable NUMA-aware spin-lock. Our new cohorting technique allows us to easily create NUMA-aware versions of the TATAS-Backoff, CLH, MCS, and ticket locks, to name a few. Moreover, it allows us to derive a CLH-based cohort abortable lock, the first NUMA-aware queue lock to support abortability. We empirically compared the performance of cohort locks with prior NUMA-aware and classic NUMAoblivious locks on a synthetic micro-benchmark, a real world key-value store application memcached, as well as the libc memory allocator. Our results demonstrate that cohort locks perform as well or better than known locks when the load is low and significantly out-perform them as the load increases. © 2015 ACM.",Concurrency; Hierarchical locks; Locks; Multicore; Mutex; Mutual exclusion; NUMA; Spin locks,Benchmarking; Design; Memory architecture; Concurrency; Hierarchical locks; Multi core; Mutex; Mutual exclusions; NUMA; Spin lock; Locks (fasteners)
Automatic parallelization of a class of irregular loops for distributed memory systems,2014,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056347192&doi=10.1145%2f2660251&partnerID=40&md5=dbd99783396eb428df67ad32c6f2bea2,"Many scientific applications spend significant time within loops that are parallel, except for dependences from associative reduction operations. However these loops often contain data-dependent control-flow and array-access patterns. Traditional optimizations that rely on purely static analysis fail to generate parallel code in such cases. This article proposes an approach for automatic parallelization for distributed memory environments, using both static and runtime analysis. We formalize the computations that are targeted by this approach and develop algorithms to detect such computations. We also describe algorithms to generate a parallel inspector that performs a runtime analysis of control-flow and array-access patterns, and a parallel executor to take advantage of this information. The effectiveness of the approach is demonstrated on several benchmarks that were automatically transformed using a prototype compiler. For these, the inspector overheads and performance of the executor code were measured. The benefit on real-world applications was also demonstrated through similar manual transformations of an atmospheric modeling software. © 2014 ACM.",Distributed-memory systems; Inspector-executor; Irregular applications; Parallelization,Memory architecture; Static analysis; Automatic Parallelization; Distributed Memory; Distributed memory systems; Inspector-executor; Irregular applications; Parallelizations; Reduction operation; Scientific applications; Application programs
A simple parallel cartesian tree algorithm and its application to parallel suffix tree construction,2014,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032167254&doi=10.1145%2f2661653&partnerID=40&md5=f53f1c8d8057d6c5f995c8c158a5643f,"We present a simple linear work and space, and polylogarithmic time parallel algorithm for generating multiway Cartesian trees. We show that bottom-up traversals of the multiway Cartesian tree on the interleaved suffix array and longest common prefix array of a string can be used to answer certain string queries. By adding downward pointers in the tree (e.g. using a hash table), we can also generate suffix trees from suffix arrays on arbitrary alphabets in the same bounds. In conjunction with parallel suffix array algorithms, such as the skew algorithm, this gives a rather simple linear work parallel, O(n∈ ) time (0 < ∈ < 1), algorithm for generating suffix trees over an integer alphabet σ {1, . . . , n}, where n is the length of the input string. It also gives a linear work parallel algorithm requiring O(log2 n) time with high probability for constantsized alphabets. More generally, given a sorted sequence of strings and the longest common prefix lengths between adjacent elements, the algorithm will generate a patricia tree (compacted trie) over the strings. Of independent interest, we describe a work-efficient parallel algorithm for solving the all nearest smaller values problem using Cartesian trees, which is much simpler than the work-efficient parallel algorithm described in previous work. We also present experimental results comparing the performance of the algorithm to existing sequential implementations and a second parallel algorithm that we implement. We present comparisons for the Cartesian tree algorithm on its own and for constructing a suffix tree. The results show that on a variety of strings our algorithm is competitive with the sequential version on a single processor and achieves good speedup on multiple processors. We present experiments for three applications that require only the Cartesian tree, and also for searching using the suffix tree. © 2014 ACM.",Cartesian trees; Suffix trees,Forestry; Indexing (of information); Parallel algorithms; Parallel processing systems; Query processing; Algorithm for solving; Cartesian Trees; Longest common prefixes; Multiple processors; Polylogarithmic time; Sequential implementation; Single processors; Suffix-trees; Trees (mathematics)
Architecture and performance of the hardware accelerators in IBM's PowerEN processor,2014,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034047128&doi=10.1145%2f2588888&partnerID=40&md5=89a3a2bc43cbe071700315526645ddad,"Computation at the edge of a datacenter has unique characteristics. It deals with streaming data from multiple sources, going to multiple destinations, often requiring repeated application of one or more of several standard algorithmic kernels. These kernels, related to encryption, compression, XML Parsing and regular expression searching on the data, demand a high data processing rate and power efficiency. This suggests the use of hardware acceleration for key functions. However, robust general purpose processing support is necessary to orchestrate the flow of data between accelerators, as well as perform tasks that are not suited to acceleration. Further, these accelerators must be tightly integrated with the general purpose computation in order to keep invocation overhead and latency low. The accelerators must be easy for software to use, and the system must be flexible enough to support evolving networking standards. In this article, we describe and evaluate the architecture of IBM's PowerEN processor, with a focus on PowerEN's architectural enhancements and its on-chip hardware accelerators. PowerEN unites the throughput of application-specific accelerators with the programmability of general purpose cores on a single coherent memory architecture. Hardware acceleration improves throughput by orders of magnitude in some cases compared to equivalent computation on the general purpose cores. By offloading work to the accelerators, general purpose cores are freed to simultaneously work on computation less suited to acceleration. © 2014 ACM.",Accelerator; Architecture; Compression; Crypto; Pattern matching; Performance; PowerEN; XML,Architecture; Compaction; Cryptography; Data handling; Hardware; Memory architecture; Particle accelerators; Pattern matching; XML; Architectural enhancement; Architecture and performance; Crypto; General-purpose computations; Hardware acceleration; Hardware accelerators; Performance; Poweren; Acceleration
Adaptive prefetching on power7: Improving performance and power consumption,2014,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944678772&doi=10.1145%2f2588789&partnerID=40&md5=f9e3b1046da8c3a899875e6bb1184617,"Hardware data prefetch engines are integral parts of many general purpose server-class microprocessors in the field today. Some prefetch engines allow users to change some of their parameters. But, the prefetcher is usually enabled in a default configuration during system bring-up, and dynamic reconfiguration of the prefetch engine is not an autonomic feature of current machines. Conceptually, however, it is easy to infer that commonly used prefetch algorithms""when applied in a fixed mode""will not help performance in many cases. In fact, they may actually degrade performance due to useless bus bandwidth consumption and cache pollution, which in turn, will also waste power. We present an adaptive prefetch scheme that dynamically modifies the prefetch settings in order to adapt to workloads' requirements. We use a commercial processor, namely the IBM POWER7 as a vehicle for our study. First we characterize""in terms of performance and power consumption""the prefetcher in that processor using microbenchmarks and SPEC CPU2006.We then present our adaptive prefetch mechanism showing performance improvements with respect to the default prefetch setting up to 2.7X and 1.3X for single-threaded and multiprogrammed workloads, respectively. Adaptive prefetching is also able to reduce power consumption in some cases. Finally, we also evaluate our mechanism with SPECjbb2005, improving both performance and power consumption. © 2014 ACM.",Adaptive system; Performance; Power consumption; Prefetching,Adaptive systems; Commercial vehicles; Dynamic models; Engines; Multiprogramming; Adaptive prefetching; Default configurations; Dynamic re-configuration; Improving performance; Performance; Performance improvements; Prefetch algorithms; Prefetching; Electric power utilization
Introduction to the special section on top papers from PACT-21,2014,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056335055&doi=10.1145%2f2609798&partnerID=40&md5=2d7d1a0d953f10238d22aa344d55069b,[No abstract available],,
A methodology for automatic generation of executable communication specifications from parallel MPI applications,2014,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020700651&doi=10.1145%2f2660249&partnerID=40&md5=504fc5637ea4edafb7e22411edb52626,"Portable parallel benchmarks are widely used for performance evaluation of HPC systems. However, because these are manually produced, they generally represent a greatly simplified view of application behavior, missing the subtle but important-to-performance nuances that may exist in a complete application. This work contributes novel methods to automatically generate highly portable and customizable communication benchmarks from HPC applications.We utilize ScalaTrace, a lossless yet scalable parallel-application tracing framework to collect selected aspects of the run-time behavior of HPC applications, including communication operations and computation time, while abstracting away the details of the computation proper. We subsequently generate benchmarks with nearly identical run-time behavior to the original applications. Results demonstrate that the generated benchmarks are in fact able to preserve the run-time behavior (including both the communication pattern and the execution time) of the original applications. Such automated benchmark generation is without precedent and particularly valuable for proprietary, export-controlled, or classified application codes. © 2014 ACM.",Languages; Measurement; Performance,Computer networks; Measurement; Query languages; Software engineering; Application behaviors; Automatic Generation; Communication operation; Communication pattern; Parallel application; Parallel benchmarks; Performance; Performance evaluations; Benchmarking
Enhancing performance optimization of multicore/multichip nodes with data structure metrics,2014,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930684927&doi=10.1145%2f2588788&partnerID=40&md5=d314cea61e7498c56be08d9b590fe829,"Program performance optimization is usually based solely on measurements of execution behavior of code segments using hardware performance counters. However, memory access patterns are critical performance limiting factors for today's multicore chips where performance is highly memory bound. Therefore diagnoses and selection of optimizations based only on measurements of the execution behavior of code segments are incomplete because they do not incorporate knowledge of memory access patterns and behaviors. This article presents a low-overhead tool (MACPO) that captures memory traces and computes metrics for the memory access behavior of source-level (C, C++, Fortran) data structures. MACPO explicitly targets the measurement and metrics important to performance optimization for multicore chips. The article also presents a complete process for integrating measurement and analyses of code execution with measurements and analyses of memory access patterns and behaviors for performance optimization, specifically targeting multicore chips and multichip nodes of clusters. MACPO uses more realistic cache models for computation of latency metrics than those used by previous tools. Evaluation of the effectiveness of adding memory access behavior characteristics of data structures to performance optimization was done on subsets of the ASCI, NAS and Rodinia parallel benchmarks and two versions of one application program from a domain not represented in these benchmarks. Adding characteristics of the behavior of data structures enabled easier diagnoses of bottlenecks and more accurate selection of appropriate optimizations than with only code centric behavior measurements. The performance gains ranged from a few percent to 38 percent. © 2014 ACM.",Data structures; Memory; Optimization; Performance,Application programs; Benchmarking; C++ (programming language); Codes (symbols); Data storage equipment; Data structures; Memory architecture; Optimization; Shape optimization; Behavior characteristic; Behavior measurements; Hardware performance counters; Integrating measurement; Memory access patterns; Performance; Performance limiting factor; Performance optimizations; Structural optimization
ACM transactions on parallel computing: An introduction,2014,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056337662&doi=10.1145%2f2661651&partnerID=40&md5=c2d4e047b0a145acf4472a28b9d3217d,[No abstract available],,
Optimal Algorithms for Right-sizing Data Centers,2022,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146415296&doi=10.1145%2f3565513&partnerID=40&md5=2d3351c8b5a430586f987a30ac063982,"Electricity cost is a dominant and rapidly growing expense in data centers. Unfortunately, much of the consumed energy is wasted, because servers are idle for extended periods of time. We study a capacity management problem that dynamically right-sizes a data center, matching the number of active servers with the varying demand for computing capacity. We resort to a data-center optimization problem introduced by Lin, Wierman, Andrew, and Thereska [25, 27] that, over a time horizon, minimizes a combined objective function consisting of operating cost, modeled by a sequence of convex functions, and server switching cost. All prior work addresses a continuous setting in which the number of active servers, at any time, may take a fractional value. In this article, we investigate for the first time the discrete data-center optimization problem where the number of active servers, at any time, must be integer valued. Thereby, we seek truly feasible solutions. First, we show that the offline problem can be solved in polynomial time. Our algorithm relies on a new, yet intuitive graph theoretic model of the optimization problem and performs binary search in a layered graph. Second, we study the online problem and extend the algorithm Lazy Capacity Provisioning (LCP) by Lin et al. [25, 27] to the discrete setting. We prove that LCP is 3-competitive. Moreover, we show that no deterministic online algorithm can achieve a competitive ratio smaller than 3. Hence, while LCP does not attain an optimal competitiveness in the continuous setting, it does so in the discrete problem examined here. We prove that the lower bound of 3 also holds in a problem variant with more restricted operating cost functions, introduced by Lin et al. [25]. In addition, we develop a randomized online algorithm that is 2-competitive against an oblivious adversary. It is based on the algorithm of Bansal et al. [7] (a deterministic, 2-competitive algorithm for the continuous setting) and uses randomized rounding to obtain an integral solution. Moreover, we prove that 2 is a lower bound for the competitive ratio of randomized online algorithms, so our algorithm is optimal. We prove that the lower bound still holds for the more restricted model. Finally, we address the continuous setting and give a lower bound of 2 on the best competitiveness of online algorithms. This matches an upper bound by Bansal et al. [7]. A lower bound of 2 was also shown by Antoniadis and Schewior [4]. We develop an independent proof that extends to the scenario with more restricted operating cost. © 2022 Association for Computing Machinery.",discrete setting; Homogeneous servers; lower bounds; online algorithm; polynomial-time offline algorithm,Competition; Cost functions; Graph theory; Integer programming; Polynomial approximation; Competitive ratio; Datacenter; Discrete settings; Homogeneous server; Low bound; Off-line algorithm; On-line algorithms; Optimization problems; Polynomial-time; Polynomial-time offline algorithm; Operating costs
A Family of Relaxed Concurrent Queues for Low-Latency Operations and Item Transfers,2022,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146427332&doi=10.1145%2f3565514&partnerID=40&md5=fd3671dc5bda2fc8ba21f971ab3629e3,"The producer-consumer communication over shared memory is a critical function of current scalable systems. Queues that provide low latency and high throughput on highly utilized systems can improve the overall performance perceived by the end users. In order to address this demand, we set as priority to achieve both high operation performance and item transfer speed. The Relaxed Concurrent Queues (RCQs) are a family of queues that we have designed and implemented for that purpose. Our key idea is a relaxed ordering model that splits the enqueue and dequeue operations into a stage of sequential assignment to a queue slot and a stage of concurrent execution across the slots. At each slot, we apply no order restrictions among the operations of the same type. We define several variants of the RCQ algorithms with respect to offered concurrency, required hardware instructions, supported operations, occupied memory space, and precondition handling. For specific RCQ algorithms, we provide pseudo-code definitions and reason about their correctness and progress properties. Additionally, we theoretically estimate and experimentally validate the worst-case distance between an RCQ algorithm and a strict first-in-first-out (FIFO) queue. We developed prototype implementations of the RCQ algorithms and experimentally compare them with several representative strict FIFO and relaxed data structures over a range of workload and system settings. The RCQS algorithm is a provably linearizable lock-free member of the RCQ family. We experimentally show that RCQS achieves factors to orders of magnitude advantage over the state-of-the-art strict or relaxed queue algorithms across several latency and throughput statistics of the queue operations and item transfers. © 2022 Association for Computing Machinery.",array-based queues; blocking; data structures; linearizable; lock-free; Producer-consumer; relaxed ordering,Concurrency control; Locks (fasteners); Queueing theory; Array-based queue; Blockings; Critical functions; Linearizable; Lock-free; Low latency; Producer-consumer; Queue algorithms; Relaxed ordering; Shared memory; Data structures
Checkpointing Workflows à la Young/Daly Is Not Good Enough,2022,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146430346&doi=10.1145%2f3548607&partnerID=40&md5=25ec68d84940c11be22b9f54b0f8684d,"This article revisits checkpointing strategies when workflows composed of multiple tasks execute on a parallel platform. The objective is to minimize the expectation of the total execution time. For a single task, the Young/Daly formula provides the optimal checkpointing period. However, when many tasks execute simultaneously, the risk that one of them is severely delayed increases with the number of tasks. To mitigate this risk, a possibility is to checkpoint each task more often than with the Young/Daly strategy. But is it worth slowing each task down with extra checkpoints? Does the extra checkpointing make a difference globally? This article answers these questions. On the theoretical side, we prove several negative results for keeping the Young/Daly period when many tasks execute concurrently, and we design novel checkpointing strategies that guarantee an efficient execution with high probability. On the practical side, we report comprehensive experiments that demonstrate the need to go beyond the Young/Daly period and to checkpoint more often for a wide range of application/platform settings. © 2022 Association for Computing Machinery.",Checkpoint; concurrent tasks; workflow; Young/Daly formula,Application platforms; Check pointing; Checkpoint; Concurrent tasks; High probability; Multiple tasks; Optimal checkpointing; Parallel platforms; Work-flows; Young/daly formula
Simple Concurrent Connected Components Algorithms,2022,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139250656&doi=10.1145%2f3543546&partnerID=40&md5=3888feed53ad17df0a5418419589a12c,"We study a class of simple algorithms for concurrently computing the connected components of an n-vertex, m-edge graph. Our algorithms are easy to implement in either the COMBINING CRCW PRAM or the MPC computing model. For two related algorithms in this class, we obtain O(lg n) step and O(m lg n) work bounds.1 For two others, we obtain O(lg2 n) step and O(m lg2 n) work bounds, which are tight for one of them. All our algorithms are simpler than related algorithms in the literature. We also point out some gaps and errors in the analysis of previous algorithms. Our results show that even a basic problem like connected components still has secrets to reveal. © 2022 Association for Computing Machinery.",Connected components; PRAM algorithms; simplicity,Computing model; Connected component; Connected component algorithm; PRAM algorithms; Related algorithms; SIMPLE algorithm; Simple++; SIMPLER algorithms; Simplicity
Design and Implementation of a Coarse-grained Dynamically Reconfigurable Multimedia Accelerator,2022,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141095038&doi=10.1145%2f3543544&partnerID=40&md5=1b568771942f952c53174f65b05e5b40,"This article proposes and implements a Coarse-grained dynamically Reconfigurable Architecture, named Reconfigurable Multimedia Accelerator (REMAC). REMAC architecture is driven by the pipelined multi-instruction-multi-data execution model for exploiting multi-level parallelism of the computation-intensive loops in multimedia applications. The novel architecture of REMAC's reconfigurable processing unit (RPU) allows multiple iterations of a kernel loop can execute concurrently in the pipelining fashion by the temporal overlapping of the configuration fetch, execution, and store processes as much as possible. To address the huge bandwidth required by parallel processing units, REMAC architecture is proposed to efficiently exploit the abundant data locality in the kernel loops to decrease data access bandwidth while increase the efficiency of pipelined execution. In addition, a novel architecture of dedicated hierarchy data memory system is proposed to increase data reuse between iterations and make data always available for parallel operation of RPU. The proposed architecture was modeled at RTL using VHDL language. Several benchmark applications were mapped onto REMAC to validate the high-flexibility and high-performance of the architecture and prove that it is appropriate for a wide set of multimedia applications. The experimental results show that REMAC's performance is better than Xilinx Virtex-II, ADRES, REMUS-II, and TI C64+ DSP. © 2022 Association for Computing Machinery.",Coarse-grained dynamically Reconfigurable Architecture; pipelined multi-instruction-multi-data; Reconfigurable computing; reconfigurable multi-issue processing unit; Reconfigurable Multimedia Accelerator,Benchmarking; Computer hardware description languages; Memory architecture; Parallel architectures; Pipeline processing systems; Pipelines; Reconfigurable architectures; Coarse-grained; Coarse-grained dynamically reconfigurable architecture; Dynamically reconfigurable architecture; Multi-datum; Multi-issue; Multimedia accelerator; Pipelined multi-instruction-multi-data; Processing units; Reconfigurable; Reconfigurable computing; Reconfigurable multi-issue processing unit; Reconfigurable multimedium accelerator; Reconfigurable- computing; Bandwidth
Multi-Interval DomLock: Toward Improving Concurrency in Hierarchies,2022,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141079075&doi=10.1145%2f3543543&partnerID=40&md5=73d308d18bfb04d15e9723569831bdf8,"Locking has been a predominant technique depended upon for achieving thread synchronization and ensuring correctness in multi-threaded applications. It has been established that the concurrent applications working with hierarchical data witness significant benefits due to multi-granularity locking (MGL) techniques compared to either fine-or coarse-grained locking. The de facto MGL technique used in hierarchical databases is intention locks, which uses a traversal-based protocol for hierarchical locking. A recent MGL implementation, dominator-based locking (DomLock), exploits interval numbering to balance the locking cost and concurrency and outperforms intention locks for non-tree-structured hierarchies. We observe, however, that depending upon the hierarchy structure and the interval numbering, DomLock pessimistically declares subhierarchies to be locked when in reality they are not. This increases the waiting time of locks and, in turn, reduces concurrency. To address this issue, we present Multi-Interval DomLock (MID), a new technique to improve the degree of concurrency of interval-based hierarchical locking. By adding additional intervals for each node, MID helps in reducing the unnecessary lock rejections due to false-positive lock status of sub-hierarchies. Unleashing the hidden opportunities to exploit more concurrency allows the parallel threads to finish their operations quickly, leading to notable performance improvement. We also show that with sufficient number of intervals, MID can avoid all the lock rejections due to false-positive lock status of nodes. MID is general and can be applied to any arbitrary hierarchy of trees, Directed Acyclic Graphs (DAGs), and cycles. It also works with dynamic hierarchies wherein the hierarchical structure undergoes updates. We illustrate the effectiveness of MID using STMBench7 and, with extensive experimental evaluation, show that it leads to significant throughput improvement (up to 141%, average 106%) over DomLock. © 2022 Association for Computing Machinery.",concurrency; false subsumptions; Hierarchical data structures; locking; STMBench7; synchronization,Concurrency control; Database systems; Directed graphs; Forestry; Trees (mathematics); Concurrency; False positive; False subsumption; Hierarchical data structure; Hierarchical locking; Interval number; Locking; Locking technique; Multi-granularity; Stmbench7; Locks (fasteners)
ISpan: Parallel Identification of Strongly Connected Components with Spanning Trees,2022,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141074549&doi=10.1145%2f3543542&partnerID=40&md5=7f80a926b98ea465e9add0a2790a5fae,"Detecting strongly connected components (SCCs) in a directed graph is crucial for understanding the structure of graphs. Most real-world graphs have one large SCC that contains the majority of the vertices as well as many small SCCs whose sizes are reversely proportional to the frequency of their occurrences. For both types of SCCs, current approaches that rely on depth first search (DFS) or breadth first search (BFS) face the challenges of both strict synchronization requirements and high computation cost. In this article, we advocate a new paradigm of identifying SCCs with simple spanning trees since SCC detection requires only the knowledge of connectivity among the vertices. We have developed a prototype called iSpan, which consists of parallel, relaxed synchronization construction of spanning trees for detecting large and small SCCs combined with fast trims for small SCCs. We further scale iSpan to the distributed memory system by applying different distribution strategies to the data and task parallel jobs. Not limited, we also extend iSpan to the GPU architecture. The evaluations show that iSpan is able to significantly outperform current state-of-the-art DFS-and BFS-based methods by an average 18× and 4×, respectively. © 2022 Association for Computing Machinery.",GPU; graph; parallel computation; spanning tree; Strongly connected component,Directed graphs; Memory architecture; Trees (mathematics); 'current; Breadth-first-search; Computation costs; Depth first; Graph; Parallel Computation; Real-world graphs; Spanning tree; Strongly connected component; Structure of graph; Graphics processing unit
Improving the Speed and Quality of Parallel Graph Coloring,2022,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141091709&doi=10.1145%2f3543545&partnerID=40&md5=2009ac807df05fa8b44865d5c9288217,"Graph coloring assigns a color to each vertex of a graph such that no two adjacent vertices get the same color. It is a key building block in many applications. In practice, solutions that require fewer distinct colors and that can be computed faster are typically preferred. Various coloring heuristics exist that provide different quality versus speed tradeoffs. The highest-quality heuristics tend to be slow. To improve performance, several parallel implementations have been proposed. This paper describes two improvements of the widely used LDF heuristic. First, we present a ""shortcutting""approach to increase the parallelism by non-speculatively breaking data dependencies. Second, we present ""color reduction""techniques to boost the solution of LDF. On 18 graphs from various domains, the shortcutting approach yields 2.5 times more parallelism in the mean, and the color-reduction techniques improve the result quality by up to 20%. Our deterministic CUDA implementation running on a Titan V is 2.9 times faster in the mean and uses as few or fewer colors as the best GPU codes from the literature. © 2022 Association for Computing Machinery.",color reduction; GPU computing; Graph coloring; parallelism; shortcutting,Graph theory; Graphics processing unit; Adjacent vertices; Building blockes; Colour reductions; GPU computing; Graph colorings; High quality; Improve performance; Parallelism; Reduction techniques; Shortcutting; Color
FgSpMSpV: A Fine-grained Parallel SpMSpV Framework on HPC Platforms,2022,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139178412&doi=10.1145%2f3512770&partnerID=40&md5=fd8cccdc3fae87d8c0370fb94188c634,"Sparse matrix-sparse vector (SpMSpV) multiplication is one of the fundamental and important operations in many high-performance scientific and engineering applications. The inherent irregularity and poor data locality lead to two main challenges to scaling SpMSpV over high-performance computing (HPC) systems: (i) a large amount of redundant data limits the utilization of bandwidth and parallel resources; (ii) the irregular access pattern limits the exploitation of computing resources. This paper proposes a fine-grained parallel SpMSpV (fgSpMSpV) framework on Sunway TaihuLight supercomputer to alleviate the challenges for large-scale real-world applications. First, fgSpMSpV adopts an MPIOpenMP parallelization model to exploit the multi-stage and hybrid parallelism of heterogeneous HPC architectures and accelerate both pre-/post-processing and main SpMSpV computation. Second, fgSpMSpV utilizes an adaptive parallel execution to reduce the pre-processing, adapt to the parallelism and memory hierarchy of the Sunway system, while still tame redundant and random memory accesses in SpMSpV, including a set of techniques like the fine-grained partitioner, re-collection method, and Compressed Sparse Column Vector (CSCV) matrix format. Third, fgSpMSpV uses several optimization techniques to further utilize the computing resources. fgSpMSpV on the Sunway TaihuLight gains a noticeable performance improvement from the key optimization techniques with various sparsity of the input. Additionally, fgSpMSpV is implemented on an NVIDIA Tesal P100 GPU and applied to the breath-first-search (BFS) application. fgSpMSpV on a P100 GPU obtains the speedup of up to over the state-of-the-art SpMSpV algorithms, and the BFS application using fgSpMSpV achieves the speedup of up to over the state-of-the-arts. © 2022 Association for Computing Machinery.",Heterogeneous; HPC; manycore; optimization; parallelism; SpMSpV,Graphics processing unit; Matrix algebra; Memory architecture; Fine grained; Heterogeneous; High-performance computing; Many-core; Optimisations; Parallelism; Performance computing; Sparse matrices; Sparse matrix-sparse vector; Sparse vectors; Supercomputers
Joinable Parallel Balanced Binary Trees,2022,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136470358&doi=10.1145%2f3512769&partnerID=40&md5=945b1a7f84306b2e11bb088cd73acc09,"In this article, we show how a single function, join, can be used to implement parallel balanced binary search trees (BSTs) simply and efficiently. Based on join, our approach applies to multiple balanced tree data structures, and a variety of functions for ordered sets and maps. We describe our technique as an algorithmic framework called join-based algorithms. We show that the join function fully captures what is needed for rebalancing trees for a variety of tree algorithms, as long as the balancing scheme satisfies certain properties, which we refer to as joinable trees. We discuss four balancing schemes that are joinable: AVL trees, red-black trees, weight-balanced trees, and treaps. We present a variety of tree algorithms that apply to joinable trees, including insert, delete, union, intersection, difference, split, range, filter, and so on, most of them also parallel. These algorithms are generic across balancing schemes. Many algorithms are optimal in the comparison model, and we provide a general proof to show the efficiency in work for joinable trees. The algorithms are highly parallel, all with polylogarithmic span (parallel dependence). Specifically, the set-set operations union, intersection, and difference have work O(m log(n/m + 1)) and polylogarithmic span for input set sizes n and m ≤ n. We implemented and tested our algorithms on the four balancing schemes. In general, all four schemes have quite similar performance, but the weight-balanced tree slightly outperforms the others. They have the same speedup characteristics, getting around 73× speedup on 72 cores (144 hyperthreads). Experimental results also show that our implementation outperforms existing parallel implementations, and our sequential version achieves close or much better performance than the sequential merging algorithm in C++ Standard Template Library (STL) on various input sizes. Copyright © 2022 held by the owner/author(s).",Balanced binary trees; parallel; searching; union,Multitasking; Trees (mathematics); Balanced binary searches; Balanced binary tree; Balanced trees; Binary search trees; Parallel; Performance; Polylogarithmic; Searching; Tree algorithms; Union; Binary trees
Fast Concurrent Data Sketches,2022,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139221719&doi=10.1145%2f3512758&partnerID=40&md5=5734b94e64fd0710311fd9c7eeb3a5c1,"Data sketches are approximate succinct summaries of long data streams. They are widely used for processing massive amounts of data and answering statistical queries about it. Existing libraries producing sketches are very fast, but do not allow parallelism for creating sketches using multiple threads or querying them while they are being built. We present a generic approach to parallelising data sketches efficiently and allowing them to be queried in real time, while bounding the error that such parallelism introduces. Utilising relaxed semantics and the notion of strong linearisability, we prove our algorithm's correctness and analyse the error it induces in some specific sketches. Our implementation achieves high scalability while keeping the error small. We have contributed one of our concurrent sketches to the open-source data sketches library. © 2022 Association for Computing Machinery.",analysis of distributed algorithms; Concurrency; design; persistence; synchronization,Semantics; Algorithm correctness; Analyse of distributed algorithm; Concurrency; Data stream; Generic approach; Multiple threads; Persistence; Real- time; Relaxed semantics; Statistical queries; Errors
BQ: A Lock-Free Queue with Batching,2022,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127512656&doi=10.1145%2f3512757&partnerID=40&md5=ad10cf45da534c1379937f7f4197d9fb,"Concurrent data structures provide fundamental building blocks for concurrent programming. Standard concurrent data structures may be extended by allowing a sequence of operations to be submitted as a batch for later execution. A sequence of such operations can then be executed more efficiently than the standard execution of one operation at a time. In this article, we develop a novel algorithmic extension to the prevalent FIFO queue data structure that exploits such batching scenarios. An implementation in C++ on a multicore demonstrates significant performance improvement of more than an order of magnitude (depending on the batch lengths and the number of threads) compared to previous queue implementations. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Concurrent algorithms; concurrent data structures; FIFO queue; linearizability; lock-freedom,C++ (programming language); Concurrency control; Data structures; Locks (fasteners); Algorithmics; Concurrent algorithms; Concurrent data structures; Concurrent programming; FIFO queue; Fundamental building blocks; Linearizability; Lock freedoms; Lock-free; Sequences of operations; Queueing theory
Metrics and Design of an Instruction Roofline Model for AMD GPUs,2022,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127526466&doi=10.1145%2f3505285&partnerID=40&md5=1241c4629ad6d2c8a6fd8c36dbc43246,"Due to the recent announcement of the Frontier supercomputer, many scientific application developers are working to make their applications compatible with AMD (CPU-GPU) architectures, which means moving away from the traditional CPU and NVIDIA-GPU systems. Due to the current limitations of profiling tools for AMD GPUs, this shift leaves a void in how to measure application performance on AMD GPUs. In this article, we design an instruction roofline model for AMD GPUs using AMD's ROCProfiler and a benchmarking tool, BabelStream (the HIP implementation), as a way to measure an application's performance in instructions and memory transactions on new AMD hardware. Specifically, we create instruction roofline models for a case study scientific application, PIConGPU, an open source particle-in-cell simulations application used for plasma and laser-plasma physics on the NVIDIA V100, AMD Radeon Instinct MI60, and AMD Instinct MI100 GPUs. When looking at the performance of multiple kernels of interest in PIConGPU we find that although the AMD MI100 GPU achieves a similar, or better, execution time compared to the NVIDIA V100 GPU, profiling tool differences make comparing performance of these two architectures hard. When looking at execution time, GIPS, and instruction intensity, the AMD MI60 achieves the worst performance out of the three GPUs used in this work. © 2022 Association for Computing Machinery.",AMD GPU; instruction roofline model; performance modeling; ROCProfiler; Roofline model,Benchmarking; Computer hardware; Laser produced plasmas; Plasma interactions; Plasma simulation; Program processors; Supercomputers; AMD GPU; Application performance; Instruction roofline model; Performance Modeling; Profiling tools; Rocprofiler; Roofline models; Scientific applications; Graphics processing unit
Bandwidth-Optimal Random Shuffling for GPUs,2022,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127529600&doi=10.1145%2f3505287&partnerID=40&md5=ed7bf681414a110dca6b53680a98e158,"Linear-time algorithms that are traditionally used to shuffle data on CPUs, such as the method of Fisher-Yates, are not well suited to implementation on GPUs due to inherent sequential dependencies, and existing parallel shuffling algorithms are unsuitable for GPU architectures because they incur a large number of read/write operations to high latency global memory. To address this, we provide a method of generating pseudo-random permutations in parallel by fusing suitable pseudo-random bijective functions with stream compaction operations. Our algorithm, termed ""bijective shuffle""trades increased per-thread arithmetic operations for reduced global memory transactions. It is work-efficient, deterministic, and only requires a single global memory read and write per shuffle input, thus maximising use of global memory bandwidth. To empirically demonstrate the correctness of the algorithm, we develop a statistical test for the quality of pseudo-random permutations based on kernel space embeddings. Experimental results show that the bijective shuffle algorithm outperforms competing algorithms on GPUs, showing improvements of between one and two orders of magnitude and approaching peak device bandwidth. © 2022 Association for Computing Machinery.",GPU; Shuffling,Bandwidth; Clustering algorithms; Program processors; Arithmetic operations; Bijective functions; Compaction operations; Linear-time algorithms; Pseudo-random; Pseudorandom permutation; Read/write operations; Sequential dependencies; Shuffling; Shuffling algorithm; Graphics processing unit
High-performance 3D Unstructured Mesh Deformation Using Rank Structured Matrix Computations,2022,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127511345&doi=10.1145%2f3512756&partnerID=40&md5=f84189a86a42cdf8f9eee236292424e4,"The Radial Basis Function (RBF) technique is an interpolation method that produces high-quality unstructured adaptive meshes. However, the RBF-based boundary problem necessitates solving a large dense linear system with cubic arithmetic complexity that is computationally expensive and prohibitive in terms of memory footprint. In this article, we accelerate the computations of 3D unstructured mesh deformation based on RBF interpolations by exploiting the rank structured property of the matrix operator. The main idea consists in approximating the matrix off-diagonal tiles up to an application-dependent accuracy threshold. We highlight the robustness of our multiscale solver by assessing its numerical accuracy using realistic 3D geometries. In particular, we model the 3D mesh deformation on a population of the novel coronaviruses. We report and compare performance results on various parallel systems against existing state-of-the-art matrix solvers. © 2022 Association for Computing Machinery.",3D mesh deformation; COVID-19; high-performance computing; hydrodynamics; low-rank matrix approximation; radial basis functions,3D modeling; Deformation; Functions; Heat conduction; Interpolation; Linear systems; Matrix algebra; Mesh generation; 3d mesh deformations; COVID-19; High-performance computing; Low-rank matrix approximations; Matrix computation; Mesh deformation; Performance; Performance computing; Rank structured matrix; Unstructured meshes; Radial basis function networks
Engineering In-place (Shared-memory) Sorting Algorithms,2022,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127513913&doi=10.1145%2f3505286&partnerID=40&md5=341caf002c644d618bba9525220aa0c3,"We present new sequential and parallel sorting algorithms that now represent the fastest known techniques for a wide range of input sizes, input distributions, data types, and machines. Somewhat surprisingly, part of the speed advantage is due to the additional feature of the algorithms to work in-place, i.e., they do not need a significant amount of space beyond the input array. Previously, the in-place feature often implied performance penalties. Our main algorithmic contribution is a blockwise approach to in-place data distribution that is provably cache-efficient. We also parallelize this approach taking dynamic load balancing and memory locality into account.Our new comparison-based algorithm In-place Parallel Super Scalar Samplesort (IPS4o), combines this technique with branchless decision trees. By taking cases with many equal elements into account and by adapting the distribution degree dynamically, we obtain a highly robust algorithm that outperforms the best previous in-place parallel comparison-based sorting algorithms by almost a factor of three. That algorithm also outperforms the best comparison-based competitors regardless of whether we consider in-place or not in-place, parallel or sequential settings.Another surprising result is that IPS4o even outperforms the best (in-place or not in-place) integer sorting algorithms in a wide range of situations. In many of the remaining cases (often involving near-uniform input distributions, small keys, or a sequential setting), our new In-place Parallel Super Scalar Radix Sort (IPS2Ra) turns out to be the best algorithm.Claims to have the-in some sense-""best""sorting algorithm can be found in many papers which cannot all be true. Therefore, we base our conclusions on an extensive experimental study involving a large part of the cross product of 21 state-of-the-art sorting codes, 6 data types, 10 input distributions, 4 machines, 4 memory allocation strategies, and input sizes varying over 7 orders of magnitude. This confirms the claims made about the robust performance of our algorithms while revealing major performance problems in many competitors outside the concrete set of measurements reported in the associated publications. This is particularly true for integer sorting algorithms giving one reason to prefer comparison-based algorithms for robust general-purpose sorting. © 2022 Association for Computing Machinery.",branch prediction; In-place algorithm,Balancing; Cache memory; Decision trees; Sorting; Algorithmics; Branch prediction; Datatypes; In-place algorithms; Input distributions; Input size; Parallel sorting algorithms; Performance penalties; Shared memory; Sorting algorithm; Memory architecture
Deterministic Constant-Amortized-RMR Abortable Mutex for CC and DSM,2021,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121237300&doi=10.1145%2f3490559&partnerID=40&md5=7ec6b0f0de98611dd0eb83d282b98a5e,"The abortable mutual exclusion problem, proposed by Scott and Scherer in response to the needs in real-time systems and databases, is a variant of mutual exclusion that allows processes to abort from their attempt to acquire the lock. Worst-case constant remote memory reference algorithms for mutual exclusion using hardware instructions such as Fetch&Add or Fetch&Store have long existed for both cache coherent (CC) and distributed shared memory multiprocessors, but no such algorithms are known for abortable mutual exclusion. Even relaxing the worst-case requirement to amortized, algorithms are only known for the CC model.In this article, we improve this state of the art by designing a deterministic algorithm that uses Fetch&Store to achieve amortized O(1) remote memory reference in both the CC and distributed shared memory models. Our algorithm supports Fast Abort (a process aborts within six steps of receiving the abort signal) and has the following additional desirable properties: it supports an arbitrary number of processes of arbitrary names, requires only O(1) space per process, and satisfies a novel fairness condition that we call Airline FCFS. Our algorithm is short with fewer than a dozen lines of code.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",abortable; AFCFS; amortized; constant RMR; Fast Abort; mutex; Mutual exclusion,Cache memory; Memory architecture; Real time systems; Abortable; AFCFS; Amortized; Constant RMR; Deterministics; Exclusion problem; Fast abort; Mutex; Mutual exclusions; Remote memory references; Interactive computer systems
Randomized Local Network Computing: Derandomization beyond Locally Checkable Labelings,2021,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121234656&doi=10.1145%2f3470640&partnerID=40&md5=70426213492085da646cc812f5b3fe25,"We carry on investigating the line of research questioning the power of randomization for the design of distributed algorithms. In their seminal paper, Naor and Stockmeyer [STOC 1993] established that, in the context of network computing in which all nodes execute the same algorithm in parallel, any construction task that can be solved locally by a randomized Monte-Carlo algorithm can also be solved locally by a deterministic algorithm. This result, however, holds only for distributed tasks such that the correctness of their solutions can be locally checked by a deterministic algorithm. In this article, we extend the result of Naor and Stockmeyer to a wider class of tasks. Specifically, we prove that the same derandomization result holds for every task such that the correctness of their solutions can be locally checked using a 2-sided error randomized Monte-Carlo algorithm.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",derandomization; Distributed algorithms; locality,Distributed computer systems; Parallel algorithms; Derandomization; Deterministic algorithms; Distributed tasks; Local networks; Locality; Locally checkable labeling; Monte carlo algorithms; Network computing; Power; Randomisation; Monte Carlo methods
Adaptive Erasure Coded Fault Tolerant Linear System Solver,2021,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121217004&doi=10.1145%2f3490557&partnerID=40&md5=57a3a365ff188e687df35ffe2e83ed54,"As parallel and distributed systems scale, fault tolerance is an increasingly important problem - particularly on systems with limited I/O capacity and bandwidth. Erasure coded computations address this problem by augmenting a given problem instance with redundant data and then solving the augmented problem in a fault oblivious manner in a faulty parallel environment. In the event of faults, a computationally inexpensive procedure is used to compute the true solution from a potentially fault-prone solution. These techniques are significantly more efficient than conventional solutions to the fault tolerance problem.In this article, we show how we can minimize, to optimality, the overhead associated with our problem augmentation techniques for linear system solvers. Specifically, we present a technique that adaptively augments the problem only when faults are detected. At any point in execution, we only solve a system whose size is identical to the original input system. This has several advantages in terms of maintaining the size and conditioning of the system, as well as in only adding the minimal amount of computation needed to tolerate observed faults. We present, in detail, the augmentation process, the parallel formulation, and evaluation of performance of our technique. Specifically, we show that the proposed adaptive fault tolerance mechanism has minimal overhead in terms of FLOP counts with respect to the original solver executing in a non-faulty environment, has good convergence properties, and yields excellent parallel performance. We also demonstrate that our approach significantly outperforms an optimized application-level checkpointing scheme that only checkpoints needed data structures.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adaptive fault tolerance; Fault tolerance; linear solver,Linear systems; Adaptive fault tolerances; Fault-prone; Fault-tolerant; Linear solver; Linear system solver; Optimality; Parallel and distributed systems; Parallel environment; Problem instances; Redundant data; Fault tolerance
A High-throughput Parallel Viterbi Algorithm via Bitslicing,2021,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121236250&doi=10.1145%2f3470642&partnerID=40&md5=ae914a3c3bc6459a25c7b168c81c6d97,"In this work, we present a novel bitsliced high-performance Viterbi algorithm suitable for high-throughput and data-intensive communication. A new column-major data representation scheme coupled with the bitsliced architecture is employed in our proposed Viterbi decoder that enables the maximum utilization of the parallel processing units in modern parallel accelerators. With the help of the proposed alteration of the data scheme, instead of the conventional bit-by-bit operations, 32-bit chunks of data are processed by each processing unit. This means that a single bitsliced parallel Viterbi decoder is capable of decoding 32 different chunks of data simultaneously. Here, the Viterbi's Add-Compare-Select procedure is implemented with our proposed bitslicing technique, where it is shown that the bitsliced operations for the Viterbi internal functionalities are efficient in terms of their performance and complexity. We have achieved this level of high parallelism while keeping an acceptable bit error rate performance for our proposed methodology. Our suggested hard and soft-decision Viterbi decoder implementations on GPU platforms outperform the fastest previously proposed works by and , achieving 21.41 and 8.24 Gbps on Tesla V100, respectively.  © 2021 Association for Computing Machinery.",bitslicing; convolutional codes; CUDA; HPC; viterbi algorithm,Bit error rate; Convolutional codes; Decoding; Graphics processing unit; Bitslicing; CUDA; Data intensive; Data representations; High-throughput; HPC; Performance; Processing units; Viterbi; Viterbi decoder; Viterbi algorithm
Pointer-Based Divergence Analysis for OpenCL 2.0 Programs,2021,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121244461&doi=10.1145%2f3470644&partnerID=40&md5=fe82808eee2d600ad80c22a6507bc16c,"A modern GPU is designed with many large thread groups to achieve a high throughput and performance. Within these groups, the threads are grouped into fixed-size SIMD batches in which the same instruction is applied to vectors of data in a lockstep. This GPU architecture is suitable for applications with a high degree of data parallelism, but its performance degrades seriously when divergence occurs. Many optimizations for divergence have been proposed, and they vary with the divergence information about variables and branches. A previous analysis scheme viewed pointers and return values from functions as divergence directly, and only focused on OpenCL 1.x. In this article, we present a novel scheme that reports the divergence information for pointer-intensive OpenCL programs. The approach is based on extended static single assignment (SSA) and adds some special functions and annotations from memory SSA and gated SSA. The proposed scheme first constructs extended SSA, which is then used to build a divergence relation graph that includes all of the possible points-to relationships of the pointers and initialized divergence states. The divergence state of the pointers can be determined by propagating the divergence state of the divergence relation graph. The scheme is further extended for interprocedural cases by considering function-related statements. The proposed scheme was implemented in an LLVM compiler and can be applied to OpenCL programs. We analyzed 10 programs with 24 kernels, with a total analyzed program size of 1,306 instructions in an LLVM intermediate representation, with 885 variables, 108 branches, and 313 pointer-related statements. The total number of divergent pointers detected was 146 for the proposed scheme, 200 for the scheme in which the pointer was always divergent, and 155 for the current LLVM default scheme; the total numbers of divergent variables detected were 458, 519, and 482, respectively, with 31, 34, and 32 divergent branches. These experimental results indicate that the proposed scheme is more precise than both a scheme in which a pointer is always divergent and the current LLVM default scheme.  © 2021 Association for Computing Machinery.",Compiler; divergence analysis; graphics processing units; pointer analysis,Computer graphics; Program compilers; 'current; Data parallelism; Divergence analyse; Divergents; Fixed size; High-throughput; Performance; Pointer analysis; Static single assignments; Thread groups; Graphics processing unit
Online Non-preemptive Scheduling on Unrelated Machines with Rejections,2021,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113863417&doi=10.1145%2f3460880&partnerID=40&md5=8fc5094d42c88307624f78bcee961841,"When a computer system schedules jobs there is typically a significant cost associated with preempting a job during execution. This cost can be incurred from the expensive task of saving the memory's state or from loading data into and out of memory. Thus, it is desirable to schedule jobs non-preemptively to avoid the costs of preemption. There is a need for non-preemptive system schedulers for desktops, servers, and data centers. Despite this need, there is a gap between theory and practice. Indeed, few non-preemptive online schedulers are known to have strong theoretical guarantees. This gap is likely due to strong lower bounds on any online algorithm for popular objectives. Indeed, typical worst-case analysis approaches, and even resource-augmented approaches such as speed augmentation, result in all algorithms having poor performance guarantees. This article considers online non-preemptive scheduling problems in the worst-case rejection model where the algorithm is allowed to reject a small fraction of jobs. By rejecting only a few jobs, this article shows that the strong lower bounds can be circumvented. This approach can be used to discover algorithmic scheduling policies with desirable worst-case guarantees. Specifically, the article presents algorithms for the following three objectives: minimizing the total flow-time, minimizing the total weighted flow-time plus energy where energy is a convex function, and minimizing the total energy under the deadline constraints. The algorithms for the first two problems have a small constant competitive ratio while rejecting only a constant fraction of jobs. For the last problem, we present a constant competitive ratio without rejection. Beyond specific results, the article asserts that alternative models beyond speed augmentation should be explored to aid in the discovery of good schedulers in the face of the requirement of being online and non-preemptive. © 2021 Owner/Author.",energy; non-preemptive scheduling; Online algorithms; primal-dual; rejections,Functions; Deadline constraint; On-line algorithms; On-line schedulers; Scheduling policies; Theoretical guarantees; Theory and practice; Unrelated machines; Worst-case analysis; Scheduling
Efficient Parallel 3D Computation of the Compressible Euler Equations with an Invariant-domain Preserving Second-order Finite-element Scheme,2021,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108359896&doi=10.1145%2f3470637&partnerID=40&md5=3142481a6199be908e01e8d50fda3786,"We discuss the efficient implementation of a high-performance second-order collocation-type finite-element scheme for solving the compressible Euler equations of gas dynamics on unstructured meshes. The solver is based on the convex-limiting technique introduced by Guermond et al. (SIAM J. Sci. Comput. 40, A3211-A3239, 2018). As such, it is invariant-domain preserving; i.e., the solver maintains important physical invariants and is guaranteed to be stable without the use of ad hoc tuning parameters. This stability comes at the expense of a significantly more involved algorithmic structure that renders conventional high-performance discretizations challenging. We develop an algorithmic design that allows SIMD vectorization of the compute kernel, identify the main ingredients for a good node-level performance, and report excellent weak and strong scaling of a hybrid thread/MPI parallelization.  © 2021 Association for Computing Machinery.",Compressible euler; conservation law; convex limiting; finite element method; heterogeneous architecture; hybrid parallelization; invariant-domain preserving; SIMD,Euler equations; Gas dynamics; Parallel architectures; Compressible euler; Compressible euler equations; Conservation law; Convex limiting; Heterogeneous architectures; Hybrid parallelization; Invariant-domain preserving; Performance; Second orders; SIMD; Finite element method
External-memory Dictionaries in the Affine and PDAM Models,2021,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115645832&doi=10.1145%2f3470635&partnerID=40&md5=e3a07577fce775efc9ac57afa009a334,"Storage devices have complex performance profiles, including costs to initiate IOs (e.g., seek times in hard drives), parallelism and bank conflicts (in SSDs), costs to transfer data, and firmware-internal operations.The Disk-access Machine (DAM) model simplifies reality by assuming that storage devices transfer data in blocks of size B and that all transfers have unit cost. Despite its simplifications, the DAM model is reasonably accurate. In fact, if B is set to the half-bandwidth point, where the latency and bandwidth of the hardware are equal, then the DAM approximates the IO cost on any hardware to within a factor of 2.Furthermore, the DAM model explains the popularity of B-trees in the 1970s and the current popularity of BI -trees and log-structured merge trees. But it fails to explain why some B-trees use small nodes, whereas all BI -trees use large nodes. In a DAM, all IOs, and hence all nodes, are the same size.In this article, we show that the affine and PDAM models, which are small refinements of the DAM model, yield a surprisingly large improvement in predictability without sacrificing ease of use. We present benchmarks on a large collection of storage devices showing that the affine and PDAM models give good approximations of the performance characteristics of hard drives and SSDs, respectively.We show that the affine model explains node-size choices in B-trees and BI -trees. Furthermore, the models predict that B-trees are highly sensitive to variations in the node size, whereas BI -trees are much less sensitive. These predictions are born out empirically.Finally, we show that in both the affine and PDAM models, it pays to organize data structures to exploit varying IO size. In the affine model, BI -trees can be optimized so that all operations are simultaneously optimal, even up to lower-order terms. In the PDAM model, BI -trees (or B-trees) can be organized so that both sequential and concurrent workloads are handled efficiently.We conclude that the DAM model is useful as a first cut when designing or analyzing an algorithm or data structure but the affine and PDAM models enable the algorithm designer to optimize parameter choices and fill in design details.  © 2021 Association for Computing Machinery.",External memory; performance models; write optimization,Bandwidth; Benchmarking; Dams; Firmware; Hard disk storage; Trees (mathematics); Virtual storage; Affine model; B trees; External memory; Hard drives; Machine modelling; Optimisations; Performance Modeling; Performance profile; Seek time; Write optimization; Data structures
Parallel Minimum Cuts in Near-linear Work and Low Depth,2021,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113878852&doi=10.1145%2f3460890&partnerID=40&md5=0c157f7eb371c4666dcb9aaee1bbe7dd,"We present the first near-linear work and poly-logarithmic depth algorithm for computing a minimum cut in an undirected graph. Previous parallel algorithms with poly-logarithmic depth required at least quadratic work in the number of vertices. In a graph with n vertices and m edges, our randomized algorithm computes the minimum cut with high probability in O(m log4 n) work and O(log3 n) depth. This result is obtained by parallelizing a data structure that aggregates weights along paths in a tree, in addition exploiting the connection between minimum cuts and approximate maximum packings of spanning trees. In addition, our algorithm improves upon bounds on the number of cache misses incurred to compute a minimum cut.  © 2021 ACM.",cache-oblivious algorithms; graph algorithms; Minimum cut; minimum path data structure; parallel algorithms,Graph algorithms; Graph structures; High probability; Linear works; Maximum packing; Minimum cut; Parallelizing; Randomized Algorithms; Spanning tree; Undirected graph; Trees (mathematics)
Massively Parallel Computation via Remote Memory Access,2021,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115662652&doi=10.1145%2f3470631&partnerID=40&md5=b97850b8a327ea15b1eecdbe7233cbe7,"We introduce the Adaptive Massively Parallel Computation (AMPC) model, which is an extension of the Massively Parallel Computation (MPC) model. At a high level, the AMPC model strengthens the MPC model by storing all messages sent within a round in a distributed data store. In the following round, all machines are provided with random read access to the data store, subject to the same constraints on the total amount of communication as in the MPC model. Our model is inspired by the previous empirical studies of distributed graph algorithms [8, 30] using MapReduce and a distributed hash table service [17].This extension allows us to give new graph algorithms with much lower round complexities compared to the best-known solutions in the MPC model. In particular, in the AMPC model we show how to solve maximal independent set in O(1) rounds and connectivity/minimum spanning tree in O(log logm/n n rounds both using O(nδ) space per machine for constant δ< 1. In the same memory regime for MPC, the best-known algorithms for these problems require poly log n rounds. Our results imply that the 2-CYCLE conjecture, which is widely believed to hold in the MPC model, does not hold in the AMPC model.  © 2021 Association for Computing Machinery.",Datasets; gaze detection; neural networks; text tagging,Dataset; Distributed data stores; Gaze detection; Massively parallels; Neural-networks; Parallel Computation; Parallel computation model; Remote memory access; Text tagging; MapReduce
Study of Fine-grained Nested Parallelism in CDCL SAT Solvers,2021,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115618924&doi=10.1145%2f3470639&partnerID=40&md5=595bc94cc90da05221c5ae920c83160e,"Boolean satisfiability (SAT) is an important performance-hungry problem with applications in many problem domains. However, most work on parallelizing SAT solvers has focused on coarse-grained, mostly embarrassing, parallelism. Here, we study fine-grained parallelism that can speed up existing sequential SAT solvers, which all happen to be of the so-called Conflict-Directed Clause Learning variety. We show the potential for speedups of up to 382× across a variety of problem instances. We hope that these results will stimulate future research, particularly with respect to a computer architecture open problem we present.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Boolean satisfiability (SAT) solver; nested parallelism; parallel algorithms,Decision theory; Model checking; Boolean satisfiability; Boolean satisfiability  solv; Coarse-grained; Fine grained; Fine-grained parallelism; Nested Parallelism; Parallelizing; Performance; Problem domain; Satisfiability solvers; Computer architecture
Introduction to the Special Issue for SPAA 2019,2021,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115604223&doi=10.1145%2f3477610&partnerID=40&md5=3e2822d3dc3b372e9b2023d797fb92ac,[No abstract available],,
Introduction to the Special Issue for SPAA 2018-Part 2,2021,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113908220&doi=10.1145%2f3460890&partnerID=40&md5=c124b1fe8e5d9ab0e6098df6660b01f4,[No abstract available],,
Constant-Length Labeling Schemes for Deterministic Radio Broadcast,2021,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115639544&doi=10.1145%2f3470633&partnerID=40&md5=4c95991d12bacf1fbf7154b9bc3cbdc0,"Broadcast is one of the fundamental network communication primitives. One node of a network, called the source, has a message that has to be learned by all other nodes. We consider broadcast in radio networks, modeled as simple undirected connected graphs with a distinguished source. Nodes communicate in synchronous rounds. In each round, a node can either transmit a message to all its neighbours, or stay silent and listen. At the receiving end, a node v hears a message from a neighbour w in a given round if v listens in this round and if w is its only neighbour that transmits in this round. If more than one neighbour of a node v transmits in a given round, we say that a collision occurs at v. We do not assume collision detection: in case of a collision, node v does not hear anything (except the background noise that it also hears when no neighbour transmits).We are interested in the feasibility of deterministic broadcast in radio networks. If nodes of the network do not have any labels, deterministic broadcast is impossible even in the four-cycle. On the other hand, if all nodes have distinct labels, then broadcast can be carried out, e.g., in a round-robin fashion, and hence O(log n)-bit labels are sufficient for this task in n-node networks. In fact, O(log "")-bit labels, where Δis the maximum degree, are enough to broadcast successfully. Hence, it is natural to ask if very short labels are sufficient for broadcast. Our main result is a positive answer to this question. We show that every radio network can be labeled using 2 bits in such a way that broadcast can be accomplished by some universal deterministic algorithm that does not know the network topology nor any bound on its size. Moreover, at the expense of an extra bit in the labels, we can get the following additional strong property of our algorithm: there exists a common round in which all nodes know that broadcast has been completed. Finally, we show that 3-bit labels are also sufficient to solve both versions of broadcast in the case where it is not known a priori which node is the source.  © 2021 Association for Computing Machinery.",Broadcast; feasibility; labelling scheme; radio network,Radio broadcasting; Broadcast; Collision detection; Communication primitives; Deterministics; Feasibility; Labeling scheme; Network communications; Radio networks; Simple++; Undirected connected graphs; Radio
Hashgraph scalable hash tables using a sparse graph data structure,2021,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113898515&doi=10.1145%2f3460872&partnerID=40&md5=2d5a815b16b9fea86ba46a1c83b3fba9,"In this article, we introduce HashGraph, a new scalable approach for building hash tables that uses concepts taken from sparse graph representations - hence, the name HashGraph. HashGraph introduces a new way to deal with hash-collisions that does not use ""open-addressing""or ""separate-chaining,""yet it has the benefits of both these approaches. HashGraph currently works for static inputs. Recent progress with dynamic graph data structures suggests that HashGraph might be extendable to dynamic inputs as well. We show that HashGraph can deal with a large number of hash values per entry without loss of performance. Last, we show a new querying algorithm for value lookups. We experimentally compare HashGraph to several state-of-the-art implementations and find that it outperforms them on average 2× when the inputs are unique and by as much as 40× when the input contains duplicates. The implementation of HashGraph in this article is for NVIDIA GPUs. HashGraph can build a hash table at a rate of 2.5 billion keys per second on a NVIDIA GV100 GPU and can query at nearly the same rate.  © 2021 ACM.",graph data structure; Hash table; sparse data structures,Graph theory; Program processors; Dynamic graph; Hash collisions; Loss of performance; Open addressing; Recent progress; Scalable approach; Sparse graphs; State of the art; Graph structures
Lock-free Contention Adapting Search Trees,2021,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113866118&doi=10.1145%2f3460874&partnerID=40&md5=ce32b8bc327b3d7d060eea8c787e59a8,"Concurrent key-value stores with range query support are crucial for the scalability and performance of many applications. Existing lock-free data structures of this kind use a fixed synchronization granularity. Using a fixed synchronization granularity in a concurrent key-value store with range query support is problematic as the best performing synchronization granularity depends on a number of factors that are difficult to predict, such as the level of contention and the number of items that are accessed by range queries. We present the first linearizable lock-free key-value store with range query support that dynamically adapts its synchronization granularity. This data structure is called the lock-free contention adapting search tree (LFCA tree). An LFCA tree automatically performs local adaptations of its synchronization granularity based on heuristics that take contention and the performance of range queries into account. We show that the operations of LFCA trees are linearizable, that the lookup operation is wait-free, and that the remaining operations (insert, remove and range query) are lock-free. Our experimental evaluation shows that LFCA trees achieve more than twice the throughput of related lock-free data structures in many scenarios. Furthermore, LFCA trees are able to perform substantially better than data structures with a fixed synchronization granularity over a wide range of scenarios due to their ability to adapt to the scenario at hand. © 2021 ACM.",adaptivity; Concurrent data structure; linearizability; lock-freedom; range query; wait-freedom,Data structures; Forestry; Locks (fasteners); Synchronization; Trees (mathematics); Experimental evaluation; Key-value stores; Local adaptation; Lock-free data structures; Lookup operations; Number of factors; Scalability and performance; Search trees; Keys (for locks)
On the distributed complexity of large-scale graph computations,2021,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113500275&doi=10.1145%2f3460900&partnerID=40&md5=29aa4436053f58445088ea841cef9da3,"Motivated by the increasing need to understand the distributed algorithmic foundations of large-scale graph computations, we study some fundamental graph problems in a message-passing model for distributed computing where k ≥ 2 machines jointly perform computations on graphs with n nodes (typically, n ≫ k). The input graph is assumed to be initially randomly partitioned among the k machines, a common implementation in many real-world systems. Communication is point-to-point, and the goal is to minimize the number of communication rounds of the computation. Our main contribution is the General Lower Bound Theorem, a theorem that can be used to show non-trivial lower bounds on the round complexity of distributed large-scale data computations. This result is established via an information-theoretic approach that relates the round complexity to the minimal amount of information required by machines to solve the problem. Our approach is generic, and this theorem can be used in a “cookbook” fashion to show distributed lower bounds for several problems, including non-graph problems. We present two applications by showing (almost) tight lower bounds on the round complexity of two fundamental graph problems, namely, PageRank computation and triangle enumeration. These applications show that our approach can yield lower bounds for problems where the application of communication complexity techniques seems not obvious or gives weak bounds, including and especially under a stochastic partition of the input. We then present distributed algorithms for PageRank and triangle enumeration with a round complexity that (almost) matches the respective lower bounds; these algorithms exhibit a round complexity that scales superlinearly in k, improving significantly over previous results [Klauck et al., SODA 2015]. Specifically, we show the following results: • PageRank: We show a lower bound of Ω (n/k2 ) rounds and present a distributed algorithm that computes an approximation of the PageRank of all the nodes of a graph in Õ (n/k2 ) rounds. • Triangle enumeration: We show that there exist graphs with m edges where any distributed algorithm requires Ω (m/k5/3 ) rounds. This result also implies the first non-trivial lower bound of Ω (n1/3 ) rounds for the congested clique model, which is tight up to logarithmic factors. We then present a distributed algorithm that enumerates all the triangles of a graph in Õ (m/k5/3 + n/k4/3 ) rounds. © 2021 Association for Computing Machinery.",Distributed graph algorithms; Lower bounds; PageRank; Triangle enumeration,Approximation algorithms; Graph structures; Graph theory; Information theory; Message passing; Stochastic systems; Algorithmic foundations; Amount of information; Communication complexity; Communication rounds; Information-theoretic approach; Lower bound theorem; Message passing models; PageRank computations; Graph algorithms
Introduction to the Special Issue for SPAA 2018,2021,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105155570&doi=10.1145%2f3456774&partnerID=40&md5=66ccc779441b37e3f1cde9e315fba2f9,[No abstract available],,
Theoretically Efficient Parallel Graph Algorithms Can Be Fast and Scalable,2021,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104839757&doi=10.1145%2f3434393&partnerID=40&md5=ad384ec910c602af78ec97e2de00fa3b,"There has been significant recent interest in parallel graph processing due to the need to quickly analyze the large graphs available today. Many graph codes have been designed for distributed memory or external memory. However, today even the largest publicly-available real-world graph (the Hyperlink Web graph with over 3.5 billion vertices and 128 billion edges) can fit in the memory of a single commodity multicore server. Nevertheless, most experimental work in the literature report results on much smaller graphs, and the ones for the Hyperlink graph use distributed or external memory. Therefore, it is natural to ask whether we can efficiently solve a broad class of graph problems on this graph in memory. This paper shows that theoretically-efficient parallel graph algorithms can scale to the largest publicly-available graphs using a single machine with a terabyte of RAM, processing them in minutes. We give implementations of theoretically-efficient parallel algorithms for 20 important graph problems. We also present the interfaces, optimizations, and graph processing techniques that we used in our implementations, which were crucial in enabling us to process these large graphs quickly. We show that the running times of our implementations outperform existing state-of-the-art implementations on the largest real-world graphs. For many of the problems that we consider, this is the first time they have been solved on graphs at this scale. We have made the implementations developed in this work publicly-available as the Graph Based Benchmark Suite (GBBS).  © 2021 ACM.",Parallel graph algorithms; parallel graph processing,Graphic methods; Hypertext systems; Random access storage; Benchmark suites; Distributed Memory; Graph processing; Multi-core servers; Parallel graph algorithms; Real-world graphs; Single- machines; State of the art; Graph algorithms
The Price of Bounded Preemption,2021,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104822644&doi=10.1145%2f3434377&partnerID=40&md5=ae94c117d9a76d7ada348ac07197b2ab,"In this article we provide a tight bound for the price of preemption for scheduling jobs on a single machine (or multiple machines). The input consists of a set of jobs to be scheduled and of an integer parameter k ≥ 1. Each job has a release time, deadline, length (also called processing time), and value associated with it. The goal is to feasibly schedule a subset of the jobs so that their total value is maximal; while preemption of a job is permitted, a job may be preempted no more than k times. The price of preemption is the worst possible (i.e., largest) ratio of the optimal non-bounded-preemptive scheduling to the optimal k-bounded-preemptive scheduling. Our results show that allowing at most k preemptions suffices to guarantee a Θ(min {logk+1 n, logk+1P}) fraction of the total value achieved when the number of preemptions is unrestricted (where n is the number of the jobs and P the ratio of the maximal length to the minimal length), giving us an upper bound for the price; a specific scenario serves to prove the tightness of this bound. We further show that when no preemptions are permitted at all (i.e., k=0), the price is Θ (min {n, log P}). As part of the proof, we introduce the notion of the Bounded-Degree Ancestor-Free Sub-Forest (BAS). We investigate the problem of computing the maximal-value BAS of a given forest and give a tight bound for the loss factor, which is Θ(logk+1 n) as well, where n is the size of the original forest and k is the bound on the degree of the sub-forest.  © 2021 ACM.",bounded preemptions; bounded-degree sub-forest; multiple machines; Scheduling jobs,Scheduling; Integer parameters; Maximal values; Minimal lengths; Multiple machine; Pre-emptive scheduling; Processing time; Scheduling jobs; Single- machines; Forestry
Mitigating Inter-Job Interference via Process-Level Quality-of-Service,2021,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104874456&doi=10.1145%2f3434397&partnerID=40&md5=b12421cf197e339a8c0cd9c1488762c7,"Jobs on most high-performance computing (HPC) systems share the network with other concurrently executing jobs. Network sharing leads to contention that can severely degrade performance. This article investigates the use of Quality of Service (QoS) mechanisms to reduce the negative impacts of network contention. QoS allows users to manage resource sharing between network flows and to provide bandwidth guarantees to specific flows. Our results show that careful use of QoS reduces the impact of network contention for specific jobs, resulting in up to a 40% performance improvement. In some cases, it completely eliminates the impact of contention. It achieves these improvements with limited negative impact to other jobs; any job that experiences performance loss typically degrades less than 5%, and often much less. Our approach can help ensure that HPC machines maintain high levels of throughput as per-node compute power continues to increase faster than network bandwidth.  © 2021 ACM.",High-performance computing; network contention; quality of service,Bandwidth; Bandwidth guarantee; High performance computing systems; Network bandwidth; Network contention; Network sharing; Performance loss; Process levels; Resource sharing; Quality of service
Dynamic Representations of Sparse Distributed Networks,2021,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104834413&doi=10.1145%2f3434395&partnerID=40&md5=2f8f5101e491387e3be2b7dcec6b6909,"In 1999, Brodal and Fagerberg (BF) gave an algorithm for maintaining a low outdegree orientation of a dynamic uniformly sparse graph. Specifically, for a dynamic graph on n-vertices, with arboricity bounded by I' at all times, the BF algorithm supports edge updates in O(log n) amortized update time, while keeping the maximum outdegree in the graph bounded by O(I'). Such an orientation provides a basic data structure for uniformly sparse graphs, which found applications to several dynamic graph algorithms, including adjacency queries and labeling schemes, maximal and approximate matching, approximate vertex cover, forest decomposition, and distance oracles. A significant weakness of the BF algorithm is the possible temporary blowup of the maximum outdegree, following edge insertions. Although BF eventually reduces all outdegrees to O(I'), some vertices may reach an outdegree of ω(n) during the process, and hence local memory usage at the vertices, which is an important quality measure in distributed systems, cannot be bounded. We show how to modify the BF algorithm to guarantee that the outdegrees of all vertices are bounded by O(I') at all times, without hurting any of its other properties and present an efficient distributed implementation of the modified algorithm. This provides the first representation of distributed networks in which the local memory usage at all vertices is bounded by the arboricity (which is essentially the average degree of the densest subgraph) rather than the maximum degree. For settings where there is no strict limitation on the local memory, one may take the temporary outdegree blowup to the extreme and allow a permanent outdegree blowup. This allows us to address the second significant weakness of the BF algorithm - its inherently global nature: An insertion of an edge (u,v) may trigger changes in the orientations of edges that are arbitrarily far away from u and v. Such a non-local scheme may be prohibitively expensive in various practical applications. We suggest an alternative local scheme, which does not guarantee any outdegree bound on the vertices, yet is just as efficient as the BF scheme for some of the aforementioned applications. For example, we obtain a local dynamic algorithm for maintaining a maximal matching with sub-logarithmic update time in uniformly sparse networks, providing an exponential improvement over the state of the art in this context. We also present a distributed implementation of this scheme and some of its applications.  © 2021 ACM.",arboricity; distributed network; dynamic graph algorithm; edge orientation; local algorithm,Distributed database systems; Graph algorithms; Graph structures; Approximate matching; Distributed implementation; Distributed networks; Distributed systems; Dynamic graph algorithms; Dynamic representation; Maximal matchings; Modified algorithms; Graph theory
Scalable Pattern Matching in Metadata Graphs via Constraint Checking,2021,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104825583&doi=10.1145%2f3434391&partnerID=40&md5=150b0a70216b0d08a6280a1c30477565,"Pattern matching is a fundamental tool for answering complex graph queries. Unfortunately, existing solutions have limited capabilities: They do not scale to process large graphs and/or support only a restricted set of search templates or usage scenarios. Moreover, the algorithms at the core of the existing techniques are not suitable for today's graph processing infrastructures relying on horizontal scalability and shared-nothing clusters, as most of these algorithms are inherently sequential and difficult to parallelize. We present an algorithmic pipeline that bases pattern matching on constraint checking. The key intuition is that each vertex and edge participating in a match has to meet a set of constraints implicitly specified by the search template. These constraints can be verified independently and typically are less expensive to compute than searching the full template. The pipeline we propose generates these constraints and iterates over them to eliminate all the vertices and edges that do not participate in any match, thus reducing the background graph to a subgraph that is the union of all template matches - the complete set of all vertices and edges that participate in at least one match. Additional analysis can be performed on this annotated, reduced graph, such as full match enumeration, match counting, or computing vertex/edge centrality. Furthermore, a vertex-centric formulation for constraint checking algorithms exists, and this makes it possible to harness existing high-performance, vertex-centric graph processing frameworks. This technique (i) enables highly scalable pattern matching in metadata (labeled) graphs; (ii) supports arbitrary patterns with 100% precision; (iii) enables tradeoffs between precision and time-to-solution, while always selects all vertices and edges that participate in matches, thus offering 100% recall; and (iv) supports a set of popular data analytics scenarios. We implement our approach on top of HavoqGT, an open-source asynchronous graph processing framework, and demonstrate its advantages through strong and weak scaling experiments on massive scale real-world (up to 257 billion edges) and synthetic (up to 4.4 trillion edges) labeled graphs, respectively, and at scales (1,024 nodes / 36,864 cores), orders of magnitude larger than used in the past for similar problems. This article serves two purposes: First, it synthesises the knowledge accumulated during a long-term project [Reza et al. 2017, 2018; Tripoul et al. 2018]. Second, it presents new system features, usage scenarios, optimizations, and comparisons with related work that strengthen the confidence that pattern matching based on iterative pruning via constraint checking is an effective and scalable approach in practice. The new contributions include the following: (i) We demonstrate the ability of the constraint checking approach to efficiently support two additional search scenarios that often emerge in practice, interactive incremental search and exploratory search. (ii) We empirically compare our solution with two additional state-of-the-art systems, Arabsque [Teixeira et al. 2015] and TriAD [Gurajada et al. 2014]. (iii) We show the ability of our solution to accommodate a more diverse range of datasets with varying properties, e.g., scale, skewness, label distribution, and match frequency. (iv) We introduce or extend a number of system features (e.g., work aggregation, load balancing, and the ability to cap the generated traffic) and design optimizations and demonstrate their advantages with respect to improving performance and scalability. (v) We present bottleneck analysis and insights into artifacts that influence performance. (vi) We present a theoretical complexity argument that motivates the performance gains we observe.  © 2021 ACM.",distributed computing; graph processing; Pattern matching; subgraph isomorphism,Clustering algorithms; Data Analytics; Graph algorithms; Graphic methods; Iterative methods; Metadata; Pipelines; Scalability; Template matching; Bottleneck analysis; Design optimization; Improving performance; Label distribution; Orders of magnitude; Scaling experiments; State-of-the-art system; Theoretical complexity; Graph theory
"Introduction to the TOPC special issue on innovations in systems for irregular applications, part 2",2020,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097252690&doi=10.1145%2f3419771&partnerID=40&md5=16eed10ab984032d4fc153e4c99cf990,[No abstract available],,
A modern fortran interface in OpenSHMEM need for interoperability with parallel fortran using coarrays,2020,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097219482&doi=10.1145%2f3418084&partnerID=40&md5=410469b976e001d2cb75184fca776fa6,"Languages and libraries based on Partitioned Global Address Space (PGAS) programming models are convenient for exploiting scalable parallelism on large applications across different domains with irregular memory access patterns. OpenSHMEM is a PGAS-based library interface specification. As a result of using legacy non-standard Fortran features, support for Fortran language bindings is deprecated in OpenSHMEM specification version 1.4. In this work, we propose a new OpenSHMEM interface using the Fortran-C interoperability bind(C) feature introduced in Fortran 2003 language standard. This new interface is implemented over the existing C-language bindings in the OpenSHMEM specification. Through this work, we intend to showcase the expressiveness of the new proposed interface, along with its productivity and performance benefits that can be extracted in applications with irregular memory access patterns.  © 2020 ACM.",CAF; Coarray Fortran; Fortran; MPI; OpenSHMEM; parallel Fortran using Coarrays; PGAS; RMA libraries,Interoperability; Memory architecture; Specifications; Different domains; Interface specification; Language bindings; Language standards; Memory access patterns; Partitioned Global Address Space; Performance benefits; Programming models; FORTRAN (programming language)
A high accuracy preserving parallel algorithm for compact schemes for DNS,2020,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097219361&doi=10.1145%2f3418073&partnerID=40&md5=448c49bd3e6e2ae7b6efb832a2082230,"A new accuracy-preserving parallel algorithm employing compact schemes is presented for direct numerical simulation of the Navier-Stokes equations. Here the connotation of accuracy preservation is having the same level of accuracy obtained by the proposed parallel compact scheme, as the sequential code with the same compact scheme. Additional loss of accuracy in parallel compact schemes arises due to necessary boundary closures at sub-domain boundaries. An attempt to circumvent this has been done in the past by the use of Schwarz domain decomposition and compact filters in ""A new compact scheme for parallel computing using domain decomposition,""J. Comput. Phys. 220, 2 (2007), 654 - 677, where a large number of overlap points was necessary to reduce error. A parallel compact scheme with staggered grids has been used to report direct numerical simulation of transition and turbulence by the Schwarz domain decomposition method. In the present research, we propose a new parallel algorithm with two benefits. First, the number of overlap points is reduced to a single common boundary point between any two neighboring sub-domains, thereby saving the number of points used, with resultant speed-up. Second, with a proper design, errors arising due to sub-domain boundary closure schemes are reduced to a user designed error tolerance, bringing the new parallel scheme on par with sequential computing. Error reduction is achieved by using global spectral analysis, introduced in ""Analysis of central and upwind compact schemes,""J. Comput. Phys. 192, 2, (2003) 677 - 694, which analyzes any discrete computing method in the full domain integrally. The design of the parallel compact scheme is explained, followed by a demonstration of the accuracy of the method by solving benchmark flows: (1) periodic two-dimensional Taylor-Green vortex problem; (2) flow inside two-dimensional square lid-driven cavity (LDC) at high Reynolds number; and (3) flow inside a non-periodic three-dimensional cubic LDC with the staggered grid arrangement.  © 2020 ACM.",boundary closure schemes; compact schemes; DNS; lid-driven cavity; Parallel computing; Taylor-Green vortex,Direct numerical simulation; Domain decomposition methods; Errors; Numerical methods; Numerical models; Parallel algorithms; Reynolds number; Spectrum analysis; Computing methods; Error tolerance; High Reynolds number; Lid-driven cavities; Loss of accuracy; Sequential computing; Taylor-Green vortex; Upwind compact scheme; Navier Stokes equations
Toward a microarchitecture for efficient execution of irregular applications,2020,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097223886&doi=10.1145%2f3418082&partnerID=40&md5=319221d02de9980b7a5051576ec06ba5,"Given the increasing importance of efficient data-intensive computing, we find that modern processor designs are not well suited to the irregular memory access patterns often found in these algorithms. Applications and algorithms that do not exhibit spatial and temporal memory request locality induce high latency and low memory bandwidth due to the high cache miss rate. In response to the performance penalties inherently present in applications with irregular memory accesses, we introduce a GoblinCore-64 (GC64) architecture and a unique memory hierarchy that are explicitly designed to exploit memory performance from irregular memory access patterns. GC64 provides a pressure-driven hardware-managed concurrency control to minimize pipeline stalls and lower the latency of context switches. A novel memory coalescing model is also introduced to enhance the performance of memory systems via request aggregations. We have evaluated the performance benefits of our approach using a series of 24 benchmarks and the results show nearly 50% memory request reductions and a performance acceleration of up to 14.6×.  © 2020 ACM.",context switching; Data-intensive computing; irregular algorithms; thread concurrency,Benchmarking; Concurrency control; Flocculation; Memory architecture; Data-intensive computing; Irregular applications; Memory access patterns; Micro architectures; Performance acceleration; Performance benefits; Performance penalties; Request aggregations; Cache memory
Optimizing the linear fascicle evaluation algorithm for multi-core and many-core systems,2020,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097228564&doi=10.1145%2f3418075&partnerID=40&md5=b56244fefd50cf071ceec9000ef7b5b9,"Sparse matrix-vector multiplication (SpMV) operations are commonly used in various scientific and engineering applications. The performance of the SpMV operation often depends on exploiting regularity patterns in the matrix. Various representations and optimization techniques have been proposed to minimize the memory bandwidth bottleneck arising from the irregular memory access pattern involved. Among recent representation techniques, tensor decomposition is a popular one used for very large but sparse matrices. Post sparse-tensor decomposition, the new representation involves indirect accesses, making it challenging to optimize for multi-cores and even more demanding for the massively parallel architectures, such as on GPUs. Computational neuroscience algorithms often involve sparse datasets while still performing long-running computations on them. The Linear Fascicle Evaluation (LiFE) application is a popular neuroscience algorithm used for pruning brain connectivity graphs. The datasets employed herein involve the Sparse Tucker Decomposition (STD) - a widely used tensor decomposition method. Using this decomposition leads to multiple indirect array references, making it very difficult to optimize on both multi-core and many-core systems. Recent implementations of the LiFE algorithm show that its SpMV operations are the key bottleneck for performance and scaling. In this work, we first propose target-independent optimizations to optimize the SpMV operations of LiFE decomposed using the STD technique, followed by target-dependent optimizations for CPU and GPU systems. The target-independent techniques include: (1) standard compiler optimizations to prevent unnecessary and redundant computations, (2) data restructuring techniques to minimize the effects of indirect array accesses, and (3) methods to partition computations among threads to obtain coarse-grained parallelism with low synchronization overhead. Then, we present the target-dependent optimizations for CPUs such as: (1) efficient synchronization-free thread mapping and (2) utilizing BLAS calls to exploit hardware-specific speed. Following that, we present various GPU-specific optimizations to optimally map threads at the granularity of warps, thread blocks, and grid. Furthermore, to automate the CPU-based optimizations developed for this algorithm, we also extend the PolyMage domain-specific language, embedded in Python. Our highly optimized and parallelized CPU implementation obtains a speedup of 6.3× over the naive parallel CPU implementation running on 16-core Intel Xeon Silver (Skylake-based) system. In addition to that, our optimized GPU implementation achieves a speedup of 5.2× over a reference-optimized GPU code version on NVIDIA's GeForce RTX 2080 Ti GPU and a speedup of 9.7× over our highly optimized and parallelized CPU implementation.  © 2020 ACM.",connectome; GPU; indirect array accesses; LiFE algorithm; multi-core; sparse tucker decomposition; SpMV; tensor decomposition; tractography,Computational neuroscience; Computer hardware; Graph algorithms; Memory architecture; Neurology; Parallel architectures; Problem oriented languages; Program processors; Tensors; Weaving; Compiler optimizations; Domain specific languages; Memory access patterns; Multi core and many cores; Optimization techniques; Representation techniques; Scientific and engineering applications; Sparse matrix-vector multiplication; Matrix algebra
Automated bug detection for high-level synthesis of multi-threaded irregular applications,2020,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097233649&doi=10.1145%2f3418086&partnerID=40&md5=61dcfa549246def73539c95e6e9c8193,"Field Programmable Gate Arrays (FPGAs) are becoming an appealing technology in datacenters and High Performance Computing. High-Level Synthesis (HLS) of multi-threaded parallel programs is increasingly used to extract parallelism. Despite great leaps forward in HLS and related debugging methodologies, there is a lack of contributions in automated bug identification for HLS of multi-threaded programs. This work defines a methodology to automatically detect and isolate bugs in parallel circuits generated with HLS. The technique relies on hardware/software Discrepancy Analysis and exploits a pattern-matching algorithm based on Finite State Automata to compare multiple hardware and software threads. Overhead, advantages, and limitations are evaluated on designs generated with an open-source HLS compiler supporting OpenMP.  © 2020 ACM.",Debugging; FPGA; HLS; irregular; multi-threading,Application programming interfaces (API); Field programmable gate arrays (FPGA); Open source software; Pattern matching; Program debugging; Hardware and software; Hardware/software; High performance computing; Irregular applications; Multi-threaded programs; Parallel circuits; Parallel program; Pattern matching algorithms; High level synthesis
Programming strategies for irregular algorithms on the emu chick,2020,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093590486&doi=10.1145%2f3418077&partnerID=40&md5=e6b9ebfe81448a13f8446d9e47d25684,"The Emu Chick prototype implements migratory memory-side processing in a novel hardware system. Rather than transferring large amounts of data across the system interconnect, the Emu Chick moves lightweight thread contexts to near-memory cores before the beginning of each remote memory read. Previous work has characterized the performance of the Chick prototype in terms of memory bandwidth and programming differences from more typical, non-migratory platforms, but there has not yet been an analysis of algorithms on this system. This work evaluates irregular algorithms that could benefit from the lightweight, memory-side processing of the Chick and demonstrates techniques and optimization strategies for achieving performance in sparse matrix-vector multiply operation (SpMV), breadth-first search (BFS), and graph alignment across up to eight distributed nodes encompassing 64 nodelets in the Chick system. We also define and justify relative metrics to compare prototype FPGA-based hardware with established ASIC architectures. The Chick currently supports up to 68x scaling for graph alignment, 80 MTEPS for BFS on balanced graphs, and 50% of measured STREAM bandwidth for SpMV.  © 2020 ACM.",EMU architecture,Alignment; Bandwidth; Analysis of algorithms; ASIC architecture; Breadth-first search; Distributed nodes; Large amounts of data; Memory bandwidths; Optimization strategy; Sparse matrix-vector multiply; Graph algorithms
A Recursive Algebraic Coloring Technique for Hardware-efficient Symmetric Sparse Matrix-vector Multiplication,2020,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092774566&doi=10.1145%2f3399732&partnerID=40&md5=177dcd4a49ccdae4781cae1ec5b359a7,"The symmetric sparse matrix-vector multiplication (SymmSpMV) is an important building block for many numerical linear algebra kernel operations or graph traversal applications. Parallelizing SymmSpMV on today's multicore platforms with up to 100 cores is difficult due to the need to manage conflicting updates on the result vector. Coloring approaches can be used to solve this problem without data duplication, but existing coloring algorithms do not take load balancing and deep memory hierarchies into account, hampering scalability and full-chip performance. In this work, we propose the recursive algebraic coloring engine (RACE), a novel coloring algorithm and open-source library implementation that eliminates the shortcomings of previous coloring methods in terms of hardware efficiency and parallelization overhead. We describe the level construction, distance-k coloring, and load balancing steps in RACE, use it to parallelize SymmSpMV, and compare its performance on 31 sparse matrices with other state-of-the-art coloring techniques and Intel MKL on two modern multicore processors. RACE outperforms all other approaches substantially. By means of a parameterized roofline model, we analyze the SymmSpMV performance in detail and discuss outliers. While we focus on SymmSpMV in this article, our algorithm and software are applicable to any sparse matrix operation with data dependencies that can be resolved by distance-k coloring.  © 2020 Owner/Author.",graph algorithms; graph coloring; memory hierarchies; scheduling; Sparse matrix; sparse symmetric matrix-vector multiplication,Balancing; Coloring; Open source software; Open systems; Coloring algorithms; Data dependencies; Hardware efficiency; Multi-core platforms; Multi-core processor; Numerical Linear Algebra; Open-source libraries; Sparse matrix-vector multiplication; Matrix algebra
Efficient Abortable-locking Protocol for Multi-level NUMA Systems,2020,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092792052&doi=10.1145%2f3399728&partnerID=40&md5=808e0aa2b7e778f3379c76224724b091,"The popularity of Non-Uniform Memory Access (NUMA) architectures has led to numerous locality-preserving hierarchical lock designs, such as HCLH, HMCS, and cohort locks. Locality-preserving locks trade fairness for higher throughput. Hence, some instances of acquisitions can incur long latencies, which can be intolerable for certain applications. Few locks allow a waiting thread to abort on a timeout. State-of-the-art abortable locks are not fully locality aware, introduce high overheads, and are unsuitable for frequent aborts. Enhancing locality-aware locks with lightweight timeout capability is critical for their adoption. In this article, we describe the design and implementation of the HMCS-T lock, a Hierarchical MCS (HMCS) lock variant that admits timeout. HMCS-T maintains the locality benefits of HMCS while ensuring aborts are lightweight. HMCS-T offers the progress guarantee missing in most abortable queuing locks. The resulting locking algorithm is complex and stateful. Proving the correctness of a complex, stateful synchronization algorithm is challenging. We prove the correctness of HMCS-T by first decomposing the problem via a novel technique that uses non-deterministic finite acceptors (NFAs). The decomposed problems become small enough to be mechanically model checked without a state-space explosion. Then, we generalize the correctness proof of any arbitrary lock configuration using a construction argument.  © 2020 ACM.",formal verification; HMCS lock; HMCS-T lock; MCS lock; model checking; NUMA systems; scalability; Spin locks; synchronization,Memory architecture; Correctness proofs; Decomposed problem; Design and implementations; Hierarchical locks; Locality-preserving; Non uniform memory access; State-space explosion; Synchronization algorithm; Locks (fasteners)
Groute: Asynchronous multi-GPU programming model with applications to large-scale graph processing,2020,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092780355&doi=10.1145%2f3399730&partnerID=40&md5=b8ec41b25199c750b739847ee32b8435,"Nodes with multiple GPUs are becoming the platform of choice for high-performance computing. However, most applications are written using bulk-synchronous programming models, which may not be optimal for irregular algorithms that benefit from low-latency, asynchronous communication. This article proposes constructs for asynchronous multi-GPU programming and describes their implementation in a thin runtime environment called Groute. Groute also implements common collective operations and distributed work-lists, enabling the development of irregular applications without substantial programming effort. We demonstrate that this approach achieves state-of-the-art performance and exhibits strong scaling for a suite of irregular applications on eight-GPU and heterogeneous systems, yielding over 7× speedup for some algorithms.  © 2020 ACM.",asynchronous programming; irregular algorithms; Multi-GPU,Program processors; Asynchronous communication; Collective operations; Heterogeneous systems; High performance computing; Irregular applications; Runtime environments; State-of-the-art performance; Synchronous programming; Graphics processing unit
Combining SIMD and Many/Multi-core Parallelism for Finite-state Machines with Enumerative Speculation,2020,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092781314&doi=10.1145%2f3399714&partnerID=40&md5=1ffd103e024f1bc649967ea7724d6449,"Finite-state Machine (FSM) is the key kernel behind many popular applications, including regular expression matching, text tokenization, and Huffman decoding. Parallelizing FSMs is extremely difficult because of the strong dependencies and unpredictable memory accesses. Previous efforts have largely focused on multi-core parallelization and used different approaches, including speculative and enumerative execution, both of which have been effective but also have limitations. With increasing width and improving flexibility in SIMD instruction sets, this article focuses on combining SIMD and many/multi-core parallelism for FSMs. We have developed a novel strategy, called enumerative speculation. Instead of speculating on a single state as in speculative execution or enumerating all possible states as in enumerative execution, our strategy speculates transitions from several possible states, reducing the prediction overheads of speculation approach and the large amount of redundant work in the enumerative approach. A simple lookback approach produces a set of guessed states to achieve high speculation success rates in our enumerative speculation. In addition, to enable continued scalability of enumerative speculation with a large number of threads, we have developed a parallel merge method. We evaluate our method with four popular FSM applications: Huffman decoding, regular expression matching, HTML tokenization, and Div7. We obtain up to 2.5× speedup using SIMD on 1 core and up to 95× combining SIMD with 60 cores of an Intel Xeon Phi. On a single core, we outperform the best single-state speculative execution version by an average of 1.6×, and in combining SIMD and many-core parallelism, outperform enumerative execution by an average of 2×. Finally, when evaluate on a GPU, we show that our parallel merge implementations are 2.02 - 6.74× more efficient than corresponding sequential merge implementations and achieve better scalability on an Nvidia V100 GPU.  © 2020 ACM.",break dependence; Finite-state machine; SIMD,Decoding; Mergers and acquisitions; Pattern matching; Scalability; Huffman Decoding; Memory access; Novel strategies; Number of threads; Parallelizations; Regular-expression matching; SIMD instructions; Speculative execution; Finite automata
Introduction to the Special Issue on PPoPP 2017 (Part 2),2020,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092797571&doi=10.1145%2f3407185&partnerID=40&md5=e0acc7e26c42c6e6f09ce9ae369ea005,[No abstract available],,
KiWi: A key-value map for scalable real-time analytics,2020,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092762773&doi=10.1145%2f3399718&partnerID=40&md5=8870526cdf12d18303dd9f751a397bcd,"We present KiWi, the first atomic KV-map to efficiently support simultaneous large scans and real-time access. The key to achieving this is treating scans as first class citizens and organizing the data structure around them. KiWi provides wait-free scans, whereas its put operations are lightweight and lock-free. It optimizes memory management jointly with data structure access. We implement KiWi and compare it to state-of-the-art solutions. Compared to other KV-maps providing atomic scans, KiWi performs either long scans or concurrent puts an order of magnitude faster. Its scans are twice as fast as non-atomic ones implemented via iterators in the Java skiplist.  © 2020 ACM.",Concurrent data structures; key-value maps,Atoms; Data structures; Fruits; Information management; Iterators; Key values; Lock-free; Memory management; Real-time access; Real-time analytics; State of the art; Wait free; Birds
Algorithms and Data Structures for Matrix-Free Finite Element Operators with MPI-Parallel Sparse Multi-Vectors,2020,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089606898&doi=10.1145%2f3399736&partnerID=40&md5=3d978968397975481ee49b50a1030058,"Traditional solution approaches for problems in quantum mechanics scale as O(M3), where M is the number of electrons. Various methods have been proposed to address this issue and obtain a linear scaling O(M). One promising formulation is the direct minimization of energy. Such methods take advantage of physical localization of the solution, allowing users to seek it in terms of non-orthogonal orbitals with local support. This work proposes a numerically efficient implementation of sparse parallel vectors within the open-source finite element library deal.II. The main algorithmic ingredient is the matrix-free evaluation of the Hamiltonian operator by cell-wise quadrature. Based on an a-priori chosen support for each vector, we develop algorithms and data structures to perform (i) matrix-free sparse matrix multivector products (SpMM), (ii) the projection of an operator onto a sparse sub-space (inner products), and (iii) post-multiplication of a sparse multivector with a square matrix. The node-level performance is analyzed using a roofline model. Our matrix-free implementation of finite element operators with sparse multivectors achieves a performance of 157 GFlop/s on an Intel Cascade Lake processor with 20 cores. Strong and weak scaling results are reported for a representative benchmark problem using quadratic and quartic finite element bases.  © 2020 ACM.",density functional theory; Finite element method; matrix-free method,Data structures; Finite element method; Mathematical operators; Quantum theory; Vector spaces; Algorithms and data structures; Bench-mark problems; Direct minimization; Efficient implementation; Hamiltonian operators; Matrix-free implementation; Number of electrons; Solution approach; Matrix algebra
CoREC: Scalable and Resilient In-memory Data Staging for In-situWorkflows,2020,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085989667&doi=10.1145%2f3391448&partnerID=40&md5=b61411649dc0be254cc094e47ded1e0d,"The dramatic increase in the scale of current and planned high-end HPC systems is leading new challenges, such as the growing costs of data movement and IO, and the reduced mean time between failures (MTBF) of system components. In-situ workflows, i.e., executing the entire application workflows on the HPC system, have emerged as an attractive approach to address data-related challenges by moving computations closer to the data, and staging-based frameworks have been effectively used to support in-situ workflows at scale. However, the resilience of these staging-based solutions has not been addressed, and they remain susceptible to expensive data failures. Furthermore, naive use of data resilience techniques such as n-way replication and erasure codes can impact latency and/or result in significant storage overheads. In this article, we present CoREC, a scalable and resilient in-memory data staging runtime for large-scale in-situ workflows. CoREC uses a novel hybrid approach that combines dynamic replication with erasure coding based on data access patterns. It also leverages multiple levels of replications and erasure coding to support diverse data resiliency requirements. Furthermore, the article presents optimizations for load balancing and conflict-avoiding encoding, and a low overhead, lazy data recovery scheme. We have implemented the CoREC runtime and have deployed with the DataSpaces staging service on leadership class computing machines and present an experimental evaluation in the article. The experiments demonstrate that CoREC can tolerate in-memory data failures while maintaining low latency and sustaining high overall storage efficiency at large scales. © 2020 ACM.",Data resilience; data staging; erasure codes; in-situ workflows; replication,Balancing; Data handling; Computing machines; Data access patterns; Dynamic replication; Experimental evaluation; Mean time between failures; Storage efficiency; Storage overhead; System components; Digital storage
An Enhanced DynamicWeighted Incremental Technique for QoS Support in NoC,2020,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085979558&doi=10.1145%2f3391442&partnerID=40&md5=4f8748b0ef5c1abc963bae982b9b0283,"Providing Quality-of-Service (QoS) in many-core network-on-chip (NoC) platforms is critical due to the high level of resource sharing in such systems. This article presents a hard-built Equality-of-Service (EoS) and Differential-Service (DS) as subsets of QoS in NoC using weighted round-robin arbitration policy. In the proposed technique, packets can be injected with variable initial weights. An enhanced dynamic weight incremental technique is proposed that automatically increases the weights according to the contention degree that packets face along their paths. The proposed technique provides EoS for all packets that are injected with equal initial weights. Furthermore, the router's input port bandwidth is shared among the passing flows according to the portion of packets' initial weights in the presence of contention (saturation). This provides DS in the network. The area and timing overhead of the proposed technique is insignificant (<%3). However, it achieves a remarkable improvement in the performance against baseline routers. The simulation using synthetic traffic patterns shows up to 67% average EoS improvement in both network throughput and worst-case delay (WCD). Two case studies are conducted for observing the DS. The first case study shows how a hotspot traffic bandwidth can be shared differently among other nodes according to the flows' initial weights. The second case study explores the DS in an experiment running traffic patterns generated from several real applications task graphs. This case study shows when all packets of a specific application are injected with larger initial weights than the others, its WCD can be reduced up to 50% compared to the time when all applications' packets are injected with equal initial weights. © 2020 ACM.",equality/quality-of-service; Network-on-chip; weighted round-robin arbiter,Bandwidth; Quality of service; Routers; Differential services; Hot-spot traffic; Incremental techniques; Network throughput; Real applications; Resource sharing; Traffic pattern; Weighted round robins; Network-on-chip
Generating Massive Scale-free Networks: Novel Parallel Algorithms using the Preferential Attachment Model,2020,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086007342&doi=10.1145%2f3391446&partnerID=40&md5=165a79d360fb567505796ebbef0d8026,"Recently, there has been substantial interest in the study of various random networks as mathematical models of complex systems. As real-life complex systems grow larger, the ability to generate progressively large random networks becomes all the more important. This motivates the need for efficient parallel algorithms for generating such networks. Naïve parallelization of sequential algorithms for generating random networks is inefficient due to inherent dependencies among the edges and the possibility of creating duplicate (parallel) edges. In this article, we present message passing interface-based distributed memory parallel algorithms for generating random scale-free networks using the preferential-attachment model. Our algorithms are experimentally verified to scale very well to a large number of processing elements (PEs), providing near-linear speedups. The algorithms have been exercised with regard to scale and speed to generate scale-free networks with one trillion edges in 6 minutes using 1,000 PEs. © 2020 ACM.",distributed algorithms; Network science; preferential attachment; random networks,Large scale systems; Message passing; Parallel algorithms; Distributed Memory; Linear speed-up; Message passing interface; Parallelizations; Preferential attachment model; Processing elements; Random network; Sequential algorithm; Complex networks
FEAST: A Lightweight Lock-free Concurrent Binary Search Tree,2020,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086007211&doi=10.1145%2f3391438&partnerID=40&md5=e4679f26a747aa53412aa46ebb422d3c,"We present a lock-free algorithm for concurrent manipulation of a binary search tree (BST) in an asynchronous shared memory system that supports search, insert, and delete operations. In addition to read and write instructions, our algorithm uses (single-word) compare-and-swap (CAS) and bit-test-and-set (BTS) read-modify-write (RMW) instructions, both of which are commonly supported by many modern processors including Intel 64 and AMD64. In contrast to most of the existing concurrent algorithms for a binary search tree, our algorithm is edge-based rather than node-based. When compared to other concurrent algorithms for a binary search tree, modify (insert and delete) operations in our algorithm (a) work on a smaller section of the tree, (b) execute fewer RMW instructions, or (c) use fewer dynamically allocated objects. In our experiments, our lock-free algorithm significantly outperformed all other algorithms for a concurrent binary search tree especially when the contention was high. We also describe modifications to our basic lock-free algorithm so that the amortized complexity of any operation in the modified algorithm can be bounded by the sum of the tree height and the point contention to within a constant factor while preserving the other desirable features of our algorithm. © 2020 ACM.",binary search tree; concurrent data structure; lock-free algorithm; Multicore system,Binary trees; C (programming language); Forestry; Locks (fasteners); Asynchronous shared memory systems; Binary search trees; Compare and swaps; Concurrent algorithms; Desirable features; Lock-free algorithms; Modern processors; Modified algorithms; Trees (mathematics)
A Time-space Efficient Algorithm for Parallel k-way In-place Merging based on Sequence Partitioning and Perfect Shuffle,2020,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086001194&doi=10.1145%2f3391443&partnerID=40&md5=f1bd105892b2eab87d23ed1ea6e90e9a,"The huge data volumes, big data, and the emergence of new parallel architectures lead to revisiting classic computer science topics. The motivation of the proposed work for revisiting the parallel k-way in-place merging is primarily related to the unsuitability of the current state-of-the-art parallel algorithms for multicore CPUs with shared memory. These architectures can be profitably employed to solve this problem in parallel. Recently, Intel introduced the parallel Standard Template Library (STL) implementation for multicore CPUs, but it has no in-place merge function with the in-place property. We propose Partition-Shuffle-merge (PS-merge) to address this problem. PS-merge includes combining sequence partitioning with the in-place perfect shuffle effect to address the k-way merge task. At first, each sequence is divided into t equal-sized partitions or ranges. Thus, each partition is spread over at most k sequences. Then, perfect shuffle is utilized as a replacement for the classic block rearrangement. Finally, range subpartitions are merged using a sequential in-place merging algorithm. To evaluate the proposed algorithm, as PS-merge produces the standard merging format, we compare this algorithm against the state-of-the-art methods, bitonic merge, a parallel binary merge tree, and lazy-merge. PS-merge shows a significant improvement in overall execution time. © 2020 ACM.",In-place merging; k-way merging; Lazy-merge; parallel algorithm; PS-merge,Binary trees; Memory architecture; Parallel architectures; Program processors; Trees (mathematics); Multi-core cpus; Overall execution; Perfect shuffle; Place merging; Shared memory; Standard template library; State of the art; State-of-the-art methods; Merging
GPOP: A Scalable Cache-and Memory-efficient Framework for Graph Processing over Parts,2020,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083285364&doi=10.1145%2f3380942&partnerID=40&md5=b62ac925bbbdfa1821e55f2f1ddd63ee,"The past decade has seen the development of many shared-memory graph processing frameworks intended to reduce the effort of developing high-performance parallel applications. However, many of these frameworks, based on Vertex-centric or Edge-centric paradigms suffer from several issues, such as poor cache utilization, irregular memory accesses, heavy use of synchronization primitives, or theoretical inefficiency, that deteriorate over-all performance and scalability. Recently, we proposed a cache and memory-efficient partition-centric paradigm for computing PageRank [26]. In this article, we generalize this approach to develop a novel Graph Processing Over Parts (GPOP) framework that is cache efficient, scalable, and work efficient. GPOP induces locality in memory accesses by increasing granularity of execution to vertex subsets called ""parts,"" thereby dramatically improving the cache performance of a variety of graph algorithms. It achieves high scalability by enabling completely lock and atomic free computation. GPOP's built-in analytical performance model enables it to use a hybrid of source and part-centric communication modes in a way that ensures work efficiency each iteration, while simultaneously boosting high bandwidth sequential memory accesses. Finally, the GPOP framework is designed with programmability in mind. It completely abstracts away underlying parallelism and programming model details from the user and provides an easy to program set of APIs with the ability to selectively continue the active vertex set across iterations. Such functionality is useful for many graph algorithms but not intrinsically supported by the current frameworks. We extensively evaluate the performance of GPOP for a variety of graph algorithms, using several large datasets. We observe that GPOP incurs up to 9×, 6.8×, and 5.5× less L2 cache misses compared to Ligra, GraphMat, and Galois, respectively. In terms of execution time, GPOP is up to 19×, 9.3×, and 3.6× faster than Ligra, GraphMat, and Galois, respectively. © 2020 ACM.",data analytics; Graph algorithms; graph frameworks; graph traversal; memory access; programmability,Application programming interfaces (API); Graph algorithms; Iterative methods; Large dataset; Scalability; Analytical performance model; Cache utilization; Communication mode; High scalabilities; Parallel application; Performance and scalabilities; Programming models; Synchronization primitive; Cache memory
Accelerating Scientific Computing in the Post-Moore's Era,2020,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083233363&doi=10.1145%2f3380940&partnerID=40&md5=011522b8366e57772caa1514e7c6c16e,"Novel uses of graphical processing units for accelerated computation revolutionized the field of high-performance scientific computing by providing specialized workflows tailored to algorithmic requirements. As the era of Moore's law draws to a close, many new non-von Neumann processors are emerging as potential computational accelerators, including those based on the principles of neuromorphic computing, tensor algebra, and quantum information. While development of these new processors is continuing to mature, the potential impact on accelerated computing is anticipated to be profound. We discuss how different processing models can advance computing in key scientific paradigms: machine learning and constraint satisfaction. Significantly, each of these new processor types utilizes a fundamentally different model of computation, and this raises questions about how to best use such processors in the design and implementation of applications. While many processors are being developed with a specific domain target, the ubiquity of spin-glass models and neural networks provides an avenue for multi-functional applications. This also hints at the infrastructure needed to integrate next-generation processing units into future high-performance computing systems. © 2020 ACM.",constraint satisfaction problems; Graph algorithms; machine learning; neuromorphic computing; optical Ising machines; quantum computing,Quantum optics; Spin glass; Constraint Satisfaction; Design and implementations; Graphical processing unit (GPUs); High performance computing systems; High performance scientific computing; Model of computation; Neuromorphic computing; Von Neumann processor; Graphics processing unit
"Introduction to the TOPC Special Issue on Innovations in Systems for Irregular Applications, Part 1",2020,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083266896&doi=10.1145%2f3383318&partnerID=40&md5=696bd323b75acf7c261bfaf89e13e794,[No abstract available],,
Load-balancing Sparse Matrix Vector Product Kernels on GPUs,2020,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083216197&doi=10.1145%2f3380930&partnerID=40&md5=5643dac45863551266c417d391abfaca,"Efficient processing of Irregular Matrices on Single Instruction, Multiple Data (SIMD)-type architectures is a persistent challenge. Resolving it requires innovations in the development of data formats, computational techniques, and implementations that strike a balance between thread divergence, which is inherent for Irregular Matrices, and padding, which alleviates the performance-detrimental thread divergence but introduces artificial overheads. To this end, in this article, we address the challenge of designing high performance sparse matrix-vector product (SpMV) kernels designed for Nvidia Graphics Processing Units (GPUs). We present a compressed sparse row (CSR) format suitable for unbalanced matrices. We also provide a load-balancing kernel for the coordinate (COO) matrix format and extend it to a hybrid algorithm that stores part of the matrix in SIMD-friendly Ellpack format (ELL) format. The ratio between the ELL-and the COO-part is determined using a theoretical analysis of the nonzeros-per-row distribution. For the over 2,800 test matrices available in the Suite Sparse matrix collection, we compare the performance against SpMV kernels provided by NVIDIA's cuSPARSE library and a heavily-tuned sliced ELL (SELL-P) kernel that prevents unnecessary padding by considering the irregular matrices as a combination of matrix blocks stored in ELL format. © 2020 ACM.",GPUs; irregular matrices; Sparse Matrix Vector Product (SpMV),"Computer graphics; Graphics processing unit; Product design; Program processors; Compressed sparse row; Computational technique; Hybrid algorithms; Nvidia graphics; Single instruction , multiple datum; Sparse matrices; Sparse matrix vector products; Thread divergences; Matrix algebra"
Software Prefetching for Unstructured Mesh Applications,2020,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083249344&doi=10.1145%2f3380932&partnerID=40&md5=973a0a77d956631d5164eba6408926cc,"This article demonstrates the utility and implementation of software prefetching in an unstructured finite volume computational fluid dynamics code of representative size and complexity to an industrial application and across a number of modern processors. We present the benefits of auto-tuning for finding the optimal prefetch distance values across different computational kernels and architectures and demonstrate the importance of choosing the right prefetch destination across the available cache levels for best performance. We discuss the impact of the data layout on the number of prefetch instructions required in kernels with indirect addressing patterns and show how to best implement them in an existing large-scale computational fluid dynamics application. Through this, we show significant full application speed-ups on a range of processors and realistic test cases in both single core/tile and full socket configurations, such as 1.14× on the Intel Xeon Sandy Bridge, 1.09× on the Intel Xeon Broadwell, 1.29× on the Intel Xeon Skylake, 1.99× on the in-order Intel Xeon Phi Knights Corner coprocessor, and 1.51× on the out-of-order Intel Xeon Phi Knights Landing many-core processor. © 2020 Owner/Author.",auto-tuning; irregular memory access; performance optimisation; Software prefetching; unstructured mesh,Computational fluid dynamics; Two phase flow; Computational Fluid Dynamics codes; Computational kernels; Distance values; Indirect addressing; Many-core processors; Modern processors; Software prefetching; Unstructured mesh applications; Application programs
Shared-memory Parallel Maximal Clique Enumeration from Static and Dynamic Graphs,2020,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083175679&doi=10.1145%2f3380936&partnerID=40&md5=9b07b4e0b9a634ffa50b5e22440d369d,"Maximal Clique Enumeration (MCE) is a fundamental graph mining problem and is useful as a primitive in identifying dense structures in a graph. Due to the high computational cost of MCE, parallel methods are imperative for dealing with large graphs. We present shared-memory parallel algorithms for MCE, with the following properties: (1) the parallel algorithms are provably work-efficient relative to a state-of-the-art sequential algorithm, (2) the algorithms have a provably small parallel depth, showing they can scale to a large number of processors, and (3) our implementations on a multicore machine show good speedup and scaling behavior with increasing number of cores and are substantially faster than prior shared-memory parallel algorithms for MCE; for instance, on certain input graphs, while prior works either ran out of memory or did not complete in five hours, our implementation finished within a minute using 32 cores. We also present work-efficient parallel algorithms for maintaining the set of all maximal cliques in a dynamic graph that is changing through the addition of edges. © 2020 ACM.",batch parallel algorithm; dynamic graph; maximal clique enumeration; Parallel algorithm; scalable graph processing; work depth analysis,Graph structures; Memory architecture; Parallel algorithms; All maximal cliques; Computational costs; Dense structures; Maximal clique enumerations; Multi-core machines; Sequential algorithm; Shared-memory parallels; State of the art; Graph algorithms
Acceleration of PageRank with Customized Precision Based on Mantissa Segmentation,2020,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083201742&doi=10.1145%2f3380934&partnerID=40&md5=4903036fc2f3407a1f0e7489c3f20d1b,"We describe the application of a communication-reduction technique for the PageRank algorithm that dynamically adapts the precision of the data access to the numerical requirements of the algorithm as the iteration converges. Our variable-precision strategy, using a customized precision format based on mantissa segmentation (CPMS), abandons the IEEE 754 single-and double-precision number representation formats employed in the standard implementation of PageRank, and instead handles the data in memory using a customized floating-point format. The customized format enables fast data access in different accuracy, prevents overflow/underflow by preserving the IEEE 754 double-precision exponent, and efficiently avoids data duplication, since all bits of the original IEEE 754 double-precision mantissa are preserved in memory, but re-organized for efficient reduced precision access. With this approach, the truncated values (omitting significand bits), as well as the original IEEE double-precision values, can be retrieved without duplicating the data in different formats. Our numerical experiments on an NVIDIA V100 GPU (Volta architecture) and a server equipped with two Intel Xeon Platinum 8168 CPUs (48 cores in total) expose that, compared with a standard IEEE double-precision implementation, the CPMS-based PageRank completes about 10% faster if high-accuracy output is needed, and about 30% faster if reduced output accuracy is acceptable. © 2020 ACM.",adaptive-precision; GPUs; high-performance; large-scale irregular graphs; multi-core processors; PageRank,Digital arithmetic; IEEE Standards; Program processors; Communication reduction; Iteration converges; Number representation; Numerical experiments; PageRank algorithm; Precision formats; Reduced precision; Variable precision; Iterative methods
ROC: A Reconfigurable Optical Computer for Simulating Physical Processes,2020,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083158453&doi=10.1145%2f3380944&partnerID=40&md5=01696000bc00b595dea1b1a45e773d01,"Due to the end of Moore's law and Dennard scaling, we are entering a new era of processors. Computing systems are increasingly facing power and performance challenges due to both device-and circuit-related challenges with resistive and capacitive charging. Non-von Neumann architectures are needed to support future computations through innovative post-Moore's law architectures. To enable these emerging architectures with high-performance and at ultra-low power, both parallel computation and inter-node communication on-the-chip can be supported using photons. To this end, we introduce ROC, a reconfigurable optical computer that can solve partial differential equations (PDEs). PDE solvers form the basis for many traditional simulation problems in science and engineering that are currently performed on supercomputers. Instead of solving problems iteratively, the proposed engine uses a resistive mesh architecture to solve a PDE in a single iteration (one-shot). Instead of using actual electrical circuits, the physical underlying hardware emulates such structures using a silicon-photonics mesh that splits light into separate pathways, allowing it to add or subtract optical power analogous to programmable resistors. The time to obtain the PDE solution then only depends on the time-of-flight of a photon through the programmed mesh, which can be on the order of 10's of picoseconds given the millimeter-compact integrated photonic circuit. Numerically validated experimental results show that, over multiple configurations, ROC can achieve several orders of magnitude improvement over state-of-the-art GPUs when speed, power, and size are taken into account. Further, it comes within approximately 90% precision of current numerical solvers. As such, ROC can be a viable reconfigurable, approximate computer with the potential for more precise results when replacing silicon-photonics building blocks with nanoscale photonic lumped-elements. © 2020 ACM.",analog computing; approximate computing; nanophotonic computing; optical computing; PDE solver accelerator; post-Moore's law processors,Computer graphics; Mesh generation; Optical data processing; Photonic devices; Photons; Program processors; Supercomputers; Emerging architectures; Integrated photonic circuit; Inter-node communication; Multiple configurations; Partial Differential Equations (PDEs); Performance challenges; Programmable resistors; Science and engineering; Silicon photonics
Processor-oblivious record and replay,2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077790827&doi=10.1145%2f3365659&partnerID=40&md5=e7c6636efcd9cfbabaf1ed8983a1dd00,"Record-and-replay systems are useful tools for debugging non-deterministic parallel programs by first recording an execution and then replaying that execution to produce the same access pattern. Existing record-and-replay systems generally target thread-based execution models, and record the behaviors and interleavings of individual threads. Dynamic multithreaded languages and libraries, such as the Cilk family, OpenMP, TBB, and the like, do not have a notion of threads. Instead, these languages provide a processor-oblivious model of programming, where programs expose task parallelism using high-level constructs such as spawn/sync without regard to the number of threads/cores available to run the program. Thread-based record-and-replay would violate the processor-oblivious nature of these programs, as they incorporate the number of threads into the recorded information, constraining the replayed execution to the same number of threads. In this article, we present a processor-oblivious record-and-replay scheme for dynamic multithreaded languages where record and replay can use different number of processors and both are scheduled using work stealing. We provide theoretical guarantees for our record and replay scheme—namely that record is optimal for programs with one lock and replay is near-optimal for all cases. In addition, we implemented this scheme in the Cilk Plus runtime system and our evaluation indicates that processor-obliviousness does not cause substantial overheads. © 2019 Association for Computing Machinery.",Deterministic replay; Dynamic program analysis; Reproducible debugging; Work stealing,Application programming interfaces (API); High level languages; Parallel processing systems; Program processors; Deterministic replay; Dynamic program analysis; Number of threads; Parallel program; Record-and-replay; Task parallelism; Theoretical guarantees; Work stealing; Program debugging
Extracting SIMD parallelism from recursive task-parallel programs,2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077791841&doi=10.1145%2f3365663&partnerID=40&md5=4ed5e950b8c62c7045360a47bfc11fbe,"The pursuit of computational efficiency has led to the proliferation of throughput-oriented hardware, from GPUs to increasingly wide vector units on commodity processors and accelerators. This hardware is designed to execute data-parallel computations in a vectorized manner efficiently. However, many algorithms are more naturally expressed as divide-and-conquer, recursive, task-parallel computations. In the absence of data parallelism, it seems that such algorithms are not well suited to throughput-oriented architectures. This article presents a set of novel code transformations that expose the data parallelism latent in recursive, task-parallel programs. These transformations facilitate straightforward vectorization of task-parallel programs on commodity hardware. We also present scheduling policies that maintain high utilization of vector resources while limiting space usage. Across several task-parallel benchmarks, we demonstrate both efficient vector resource utilization and substantial speedup on chips using Intel’s SSE4.2 vector units, as well as accelerators using Intel’s AVX512 units. We then show through rigorous sampling that, in practice, our vectorization techniques are effective for a much larger class of programs. © 2019 Copyright held by the owner/author(s).",Recursive programs; Task parallelism; Vectorization,Computational efficiency; Cosine transforms; Program processors; Vectors; Code transformation; Commodity processors; Recursive programs; Resource utilizations; Scheduling policies; Task parallelism; Vectorization; Vectorization techniques; Vector spaces
Introduction to the special issue on PPOPP 2017 (Part 1),2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078319931&doi=10.1145%2f3373151&partnerID=40&md5=6c491fdf6732f370a73c74a00898460d,[No abstract available],,
TaPIR: Embedding recursive fork-join parallelism into LLVM’s intermediate representation,2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077788723&doi=10.1145%2f3365655&partnerID=40&md5=fec53728b78b671c188d644188dc45bb,"Tapir (pronounced TAY-per) is a compiler intermediate representation (IR) that embeds recursive fork-join parallelism, as supported by task-parallel programming platforms such as Cilk and OpenMP, into a mainstream compiler’s IR. Mainstream compilers typically treat parallel linguistic constructs as syntactic sugar for function calls into a parallel runtime. These calls prevent the compiler from performing optimizations on and across parallel control constructs. Remedying this situation has generally been thought to require an extensive reworking of compiler analyses and code transformations to handle parallel semantics. Tapir leverages the “serial-projection property,” which is commonly satisfied by task-parallel programs, to handle the semantics of these programs without an extensive rework of the compiler. For recursive fork-join programs that satisfy the serial-projection property, Tapir enables effective compiler optimization of parallel programs with only minor changes to existing compiler analyses and code transformations. Tapir uses the serial-projection property to order logically parallel fine-grained tasks in the program’s control-flow graph. This ordered representation of parallel tasks allows the compiler to optimize parallel codes effectively with only minor modifications. For example, to implement Tapir/LLVM, a prototype of Tapir in the LLVM compiler, we added or modified less than 3,000 lines of LLVM’s half-million-line core middle-end functionality. These changes sufficed to enable LLVM’s existing compiler optimizations for serial code—including loop-invariant-code motion, common-subexpression elimination, and tail-recursion elimination—to work with parallel control constructs such as parallel loops and Cilk’s cilk_spawn keyword. Tapir also supports parallel optimizations, such as loop scheduling, which restructure the parallel control flow of the program. By making use of existing LLVM optimizations and new parallel optimizations, Tapir/LLVM can optimize recursive fork-join programs more effectively than traditional compilation methods. On a suite of 35 Cilk application benchmarks, Tapir/LLVM produces more efficient executables for 30 benchmarks, with faster 18-core running times for 26 of them, compared to a nearly identical compiler that compiles parallel linguistic constructs the traditional way. © 2019 Association for Computing Machinery.",,Application programming interfaces (API); Benchmarking; Cosine transforms; Data flow analysis; Flow graphs; Parallel flow; Parallel programming; Semantics; Code transformation; Common subexpression elimination; Compilation methods; Compiler optimizations; Intermediate representations; Loop-invariant code motion; Parallel optimization; Projection property; Program compilers
Pagoda: A GPU runtime system for narrow tasks,2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075633440&doi=10.1145%2f3365657&partnerID=40&md5=4de9612493b822f81fba7acf59cc29f4,"Massively multithreaded GPUs achieve high throughput by running thousands of threads in parallel. To fully utilize the their hardware, contemporary workloads spawn work to the GPU in bulk by launching large tasks, where each task is a kernel that contains thousands of threads that occupy the entire GPU. GPUs face severe underutilization and their performance benefits vanish if the tasks are narrow, i.e., they contain less than 512 threads. Latency-sensitive applications in network, signal, and image processing that generate a large number of tasks with relatively small inputs are examples of such limited parallelism. This article presents Pagoda, a runtime system that virtualizes GPU resources, using an OS-like daemon kernel called MasterKernel. Tasks are spawned from the CPU onto Pagoda as they become available, and are scheduled by the MasterKernel at the warp granularity. This level of control enables the GPU to keep scheduling and executing tasks as long as free warps are found, dramatically reducing underutilization. Experimental results on real hardware demonstrate that Pagoda achieves a geometric mean speedup of 5.52X over PThreads running on a 20-core CPU, 1.76X over CUDA-HyperQ, and 1.44X over GeMTC, the state-of-the-art runtime GPU task scheduling system. © 2019 Association for Computing Machinery. All rights reserved.",GPU runtime system; Task parallelism; Utilization,Graphics processing unit; Image processing; Program processors; Towers; Waste utilization; High throughput; Limited parallelism; Performance benefits; Runtime systems; Sensitive application; State of the art; Task parallelism; Task-scheduling; Computer hardware
Hyperqueues: Design and implementation of deterministic concurrent queues,2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075637028&doi=10.1145%2f3365660&partnerID=40&md5=3aacf35bfeef6ce2807ec9da18ce6010,"The hyperqueue is a programming abstraction for queues that results in deterministic and scale-free parallel programs. Hyperqueues extend the concept of Cilk++ hyperobjects to provide thread-local views on a shared data structure. While hyperobjects are organized around private local views, hyperqueues provide a shared view on a queue data structure. Hereby, hyperqueues guarantee determinism for programs using concurrent queues. We define the programming API and semantics of two instances of the hyperqueue concept. These hyperqueues differ in their API and the degree of concurrency that is extracted. We describe the implementation of the hyperqueues in a work-stealing scheduler and demonstrate scalable performance on pipeline-parallel benchmarks from PARSEC and StreamIt. © 2019 Association for Computing Machinery. All rights reserved.",Hyperqueue,Application programming interfaces (API); Benchmarking; Computer programming; Data structures; Semantics; Design and implementations; Hyperqueue; Parallel benchmarks; Parallel program; Programming abstractions; Scalable performance; Scale-free; Shared data structure; Queueing theory
Using butterfly-patterned partial sums to draw from discrete distributions,2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075637634&doi=10.1145%2f3365662&partnerID=40&md5=3ce783b1b4c1521bbfb10096117a914e,"We describe a simd technique for drawing values from multiple discrete distributions, such as sampling from the random variables of a mixture model, that avoids computing a complete table of partial sums of the relative probabilities. A table of alternate (""butterfly-patterned"") form is faster to compute, making better use of coalesced memory accesses; from this table, complete partial sums are computed on the fly during a binary search.Measurements using CUDA 7.5 on an nvidia Titan Black GPU show that this technique makes an entire machine-learning application that uses a Latent Dirichlet Allocation topic model with 1,024 topics about 13% faster (when using single-precision floating-point data) or about 35% faster (when using double-precision floating-point data) than doing a straightforward matrix transposition after using coalesced accesses. © 2019 Association for Computing Machinery. All rights reserved.",Butterfly; Coalesced memory access; Discrete distribution; GPU; Latent Dirichlet allocation; Lda; Machine learning; Memory bottleneck; Multithreading; Parallel computing; Random sampling; Simd; Transposed memory access,Computer aided instruction; Digital arithmetic; Graphics processing unit; Learning systems; Machine learning; Memory architecture; Parallel processing systems; Probability distributions; Statistics; Butterfly; Coalesced memory access; Discrete distribution; Latent Dirichlet allocation; Memory access; Memory bottleneck; Multi-threading; Random sampling; Simd; Random access storage
Introduction to the special issue for SPAA'17,2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075599963&doi=10.1145%2f3363417&partnerID=40&md5=2d42a7acc6c7dec4cb07fbeb560ad5d3,[No abstract available],,
Near optimal parallel algorithms for dynamic DFS in undirected graphs,2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075609704&doi=10.1145%2f3364212&partnerID=40&md5=6da6c837d5262d97e35ffd0ed9ddfefb,"Depth first search (DFS) tree is a fundamental data structure for solving various graph problems. The classical algorithm [54] for building a DFS tree requiresO(m + n) time for a given undirected graphG having n vertices and m edges. Recently, Baswana et al. [5] presented a simple algorithm for updating the DFS tree of an undirected graph after an edge/vertex update in O (n)1 time. However, their algorithm is strictly sequential. We present an algorithm achieving similar bounds that can be easily adopted to the parallel environment. In the parallel environment, a DFS tree can be computed from scratch in expected O (1) time [2] on an EREW PRAM, whereas the best deterministic algorithm takes O ( √ n) time [2, 27] on a CRCW PRAM. Our algorithm can be used to develop optimal time (to poly logn factors) deterministic parallel algorithms for maintaining fully dynamic DFS and fault tolerant DFS of an undirected graph. (1) Parallel Fully Dynamic DFS: Given an arbitrary online sequence of vertex or edge updates, we can maintain a DFS tree of an undirected graph in O (1) time per update usingm processors on an EREW PRAM. (2) Parallel Fault tolerant DFS: An undirected graph can be preprocessed to build a data structure of size O(m), such that for any set of k updates (where k is constant) in the graph, a DFS tree of the updated graph can be computed in O (1) time using n processors on an EREW PRAM. For constant k, this is also work optimal (to poly logn factors). Moreover, our fully dynamic DFS algorithm provides, in a seamless manner, nearly optimal (to poly logn factors) algorithms for maintaining a DFS tree in the semi-streaming environment and a restricted distributed model. These are the first parallel, semi-streaming, and distributed algorithms for maintaining a DFS tree in the dynamic setting. © 2019 Copyright held by the owner/author(s).",Algorithm; DFS; Distributed; Dynamic; Graph; Parallel; Streaming,Acoustic streaming; Algorithms; Data structures; Dynamics; Fault tolerance; Forestry; Parallel algorithms; Parallel processing systems; Depth-First-Search (DFS); Deterministic algorithms; Distributed; Distributed modeling; Graph; Parallel; Parallel environment; SIMPLE algorithm; Trees (mathematics)
New cover time bounds for the coalescing-branching random walk on graphs,2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075615376&doi=10.1145%2f3364206&partnerID=40&md5=3ecc35c1288b003702fccd6a0873de08,"We present new bounds on the cover time of the coalescing-branching random walk process COBRA. The COBRA process, introduced in Dutta et al. [9], can be viewed as spreading a single item of information throughout an undirected graph in synchronised rounds. In each round, each vertex that has received the information in the previous round (possibly simultaneously from more than one neighbour and possibly not for the first time), ""pushes"" the information to k randomly selected neighbours. The COBRA process is typically studied for integer branching rates k ≥ 2 (with the case k = 1 corresponding to a random walk). The aim of the process is to propagate the information quickly, but with a limited number of transmissions per vertex per round. The COBRA cover time is the expected number of rounds until all vertices have received the information at least once. Our main results are bounds of O(m + (dmax)2 logn) and O(m logn) on the COBRA cover time for arbitrary connected graphs with n vertices, m edges and maximum graph degree dmax, and bounds of O((r 2 + r/(1 - λ)) logn) and O((1/(1 - λ)2) logn) for r -regular connected graphs with the second largest eigenvalue λ in absolute value. Our bounds for general graphs are always O(n2 logn), decreasing to O(n) for constant degree graphs, while the best previous bound is O(n2.75 logn). Our first bound for regular graphs applied to the lazy variant of the COBRA process is O((r 2 + r/φ2) logn), where φ is the conductance of the graph. The best previous results for the COBRA process imply for this case only a bound of O((r 4/φ2) log2 n). To derive our bounds, we develop the following new approach to analysing the performance of the COBRA process. We introduce a type of infection process, which we call the Biased Infection with Persistent Source (BIPS) process, show that BIPS can be viewed as dual to COBRA, and obtain bounds for COBRA by analysing the convergence of BIPS. © 2019 Copyright held by the owner/author(s).",COBRA process; Cover time; Epidemic processes; Random processes on graphs,Eigenvalues and eigenfunctions; Flocculation; Graphic methods; Random processes; Branching random walks; Constant degree graphs; Cover time; Epidemic process; Infection process; Number of transmissions; Second largest eigenvalue; Undirected graph; Graph theory
On energy conservation in data centers,2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075617925&doi=10.1145%2f3364210&partnerID=40&md5=91cac1a2a35526a413a03d6f0fa708f3,"We formulate and study an optimization problem that arises in the energy management of data centers and, more generally, multiprocessor environments. Data centers host a large number of heterogeneous servers. Each server has an active state and several standby/sleep states with individual power consumption rates. The demand for computing capacity varies over time. Idle servers may be transitioned to low-power modes so as to rightsize the pool of active servers. The goal is to find a state transition schedule for the servers that minimizes the total energy consumed. On a small scale, the same problem arises in multicore architectures with heterogeneous processors on a chip. One has to determine active and idle periods for the cores so as to guarantee a certain service and minimize the consumed energy. For this power/capacity management problem, we develop two main results. We use the terminology of the data center setting. First, we investigate the scenario that each server has two states: An active state and a sleep state. We show that an optimal solution, minimizing energy consumption, can be computed in polynomial time by a combinatorial algorithm. The algorithm resorts to a single-commodity minimum-cost flow computation. Second, we study the general scenario that each server has an active state and multiple standby/sleep states. We devise a τ -approximation algorithm that relies on a two-commodity minimum-cost flow computation. Here, τ is the number of different server types. A data center has a large collection of machines but only a relatively small number of different server architectures. Moreover, in the optimization, one can assign servers with comparable energy consumption to the same class. Technically, both of our algorithms involve nontrivial flow modification procedures. In particular, given a fractional two-commodity flow, our algorithm executes advanced rounding and flow packing routines. © 2019 Copyright held by the owner/author(s).",Approximation algorithms; Efficient algorithms; Heterogeneousmachines; Minimum-cost flow,Approximation algorithms; Cost accounting; Energy utilization; Polynomial approximation; Software architecture; Combinatorial algorithm; Heterogeneous processors; Heterogeneous servers; Heterogeneousmachines; Minimum cost flows; Multicore architectures; Optimization problems; Power consumption rates; Green computing
Distributed graph clustering and sparsification,2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075612242&doi=10.1145%2f3364208&partnerID=40&md5=e74e079914ba62d90a1f302f38f6446b,"Graph clustering is a fundamental computational problem with a number of applications in algorithm design, machine learning, data mining, and analysis of social networks. Over the past decades, researchers have proposed a number of algorithmic design methods for graph clustering. Most of these methods, however, are based on complicated spectral techniques or convex optimisation and cannot be directly applied for clustering many networks that occur in practice, whose information is often collected on different sites. Designing a simple and distributed clustering algorithm is of great interest and has comprehensive applications for processing big datasets. In this article, we present a simple and distributed algorithm for graph clustering: For a wide class of graphs that are characterised by a strong cluster-structure, our algorithm finishes in a poly-logarithmic number of rounds and recovers a partition of the graph close to optimal. One of the main procedures behind our algorithm is a sampling scheme that, given a dense graph as input, produces a sparse subgraph that provably preserves the cluster-structure of the input. Compared with previous sparsification algorithms that require Laplacian solvers or involve combinatorial constructions, this procedure is easy to implement in a distributed setting and runs fast in practice. © 2019 Association for Computing Machinery.",Distributed computing; Graph clustering; Graph sparsification,Convex optimization; Data mining; Distributed computer systems; Machine learning; Object oriented programming; Algorithmic design; Cluster structure; Computational problem; Convex optimisation; Distributed clustering algorithm; Graph clustering; Graph sparsification; Spectral techniques; Clustering algorithms
Tight bounds for clairvoyant dynamic bin packing,2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073714084&doi=10.1145%2f3364214&partnerID=40&md5=1cf778e2a034cbe1ffce74d9cffa081f,"In this article, we focus on the Clairvoyant Dynamic Bin Packing (DBP) problem, which extends the Classical Online Bin Packing problem in that items arrive and depart over time and the departure time of an item is known upon its arrival. The problem naturally arises when handling cloud-based networks. We focus specifically on the MinUsageTime objective function, which aims to minimize the overall usage time of all bins that are opened during the packing process. Earlier work has shown a O( log μ log log μ ) upper bound on the algorithm's competitiveness, where μ is defined as the ratio between the maximal and minimal durations of all items. We improve the upper bound by giving a O( √ log μ)-competitive algorithm. We then provide a matching lower bound of Ω( √ log μ) on the competitive ratio of any online algorithm, thus closing the gap with regard to this problem.We then focus on what we call the class of aligned inputs and give aO(log log μ)- competitive algorithm for this case, beating the lower bound of the general case by an exponential factor. Surprisingly enough, the analysis of our algorithm that we present is closely related to various properties of binary strings. © 2019 Association for Computing Machinery.",Analysis of algorithms; Clairvoyant setting; Competitive ratio; Dynamic bin packing; Online algorithms,Software engineering; Analysis of algorithms; Competitive algorithms; Competitive ratio; Dynamic bin packing; Exponential factors; Objective functions; On-line algorithms; Online bin packing; Computer networks
The mobile server problem,2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073751869&doi=10.1145%2f3364204&partnerID=40&md5=d75c145742d5cb8ddb9a56cef061007a,"We introduce the Mobile Server problem, inspired by current trends to move computational tasks from cloud structures to multiple devices close to the end user. An example of this is embedded systems in autonomous cars that communicate to coordinate their actions. Our model is a variant of the classical Page Migration problem. More formally, we consider a mobile server holding a data page. The server can move in the Euclidean space (of arbitrary dimension). In every round, requests for data items from the page pop up at arbitrary points in the space. The requests are served, each at a cost of the distance from the requesting point and the server, and the mobile server may move, at a cost D times the distance traveled for some constant D.We assume a maximum distancem that the server is allowed to move per round. We show that no online algorithm can achieve a competitive ratio independent of the length of the input sequence in this setting. Hence, we augment the maximum movement distance of the online algorithms to (1 + δ ) times the maximum distance of the offline solution.We provide a deterministic algorithm that is simple to describe and works for multiple variants of our problem. The algorithm achieves almost tight competitive ratios independent of the length of the input sequence. Our algorithm also achieves a constant competitive ratio without resource augmentation in a variantwhere the movement of clients is also restricted. © 2019 Association for Computing Machinery.",Competitive analysis; Online algorithms; Page migration; Resource augmentation,Computer networks; Software engineering; Arbitrary dimension; Competitive analysis; Computational task; Deterministic algorithms; Multiple variants; On-line algorithms; Page migration; Resource augmentation; Embedded systems
Distributed partial clustering,2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073758152&doi=10.1145%2f3322808&partnerID=40&md5=85db224d030238d194cd37c0c79762a5,"Recent years have witnessed an increasing popularity of algorithm design for distributed data, largely due to the fact that massive datasets are often collected and stored in different locations. In the distributed setting, communication typically dominates the query processing time. Thus, it becomes crucial to design communication-efficient algorithms for queries on distributed data. Simultaneously, it has been widely recognized that partial optimizations, where we are allowed to disregard a small part of the data, provide us significantly better solutions. The motivation for disregarded points often arises from noise and other phenomena that are pervasive in large data scenarios. In this article, we focus on partial clustering problems, k-center, k-median, and k-means objectives in the distributed model, and provide algorithms with communication sublinear of the input size. As a consequence, we develop the first algorithms for the partial k-median and means objectives that run in subquadratic running time. We also initiate the study of distributed algorithms for clustering uncertain data, where each data point can possibly fall into multiple locations under certain probability distribution. © 2019 Association for Computing Machinery.",Clustering; Distributed computing; K-centers; K-means; K-medians,Distributed computer systems; Probability distributions; Query processing; Clustering; Design communication; Distributed modeling; K-center; K-means; K-median; Partial clustering; Partial optimizations; K-means clustering
Distributed detection of cycles,2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073708898&doi=10.1145%2f3322811&partnerID=40&md5=5e1c60e6699f1b8e9edb84dac410dfa5,"Distributed property testing in networks has been introduced by Brakerski and Patt-Shamir [6], with the objective of detecting the presence of large dense sub-networks in a distributed manner. Recently, Censor- Hillel et al. [7] have revisited this notion and formalized it in a broader context. In particular, they have shown how to detect 3-cycles in a constant number of rounds by a distributed algorithm. In a follow-up work, Fraigniaud et al. [21] have shown how to detect 4-cycles in a constant number of rounds as well. However, the techniques in these latter works were shown not to generalize to larger cycles Ck with k ≥ 5. In this article, we completely settle the problem of cycle detection by establishing the following result: For every k ≥ 3, there exists a distributed property testing algorithm for Ck -freeness, performing in a constant number of rounds. All these results hold in the classical congest model for distributed network computing. Our algorithm is 1-sided error. Its round-complexity is O(1/ϵ ) where ϵ ϵ (0, 1) is the property-testing parameter measuring the gap between legal and illegal instances. © 2019 Association for Computing Machinery.",Congest model; Cycle detection; Distributed computing; Distributed decision; Distributed property testing,Computer networks; Software engineering; Cycle detection; Distributed decision; Distributed detection; Distributed network computing; In networks; Property-testing; Round complexity; Sub-network; Distributed computer systems
Scheduling mutual exclusion accesses in equal-length jobs,2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075740029&doi=10.1145%2f3342562&partnerID=40&md5=7b67e9d7679e01eb32e0a61542902f16,"A fundamental problem in parallel and distributed processing is the partial serialization that is imposed due to the need for mutually exclusive access to common resources. In this article, we investigate the problem of optimally scheduling (in terms of makespan) a set of jobs, where each job consists of the same number L of unit-duration tasks, and each task either accesses exclusively one resource from a given set of resources or accesses a fully shareable resource. We develop and establish the optimality of a fast polynomial-time algorithm to find a schedule with the shortest makespan for any number of jobs and for any number of resources for the case of L = 2. In the notation commonly used for job-shop scheduling problems, this result means that the problem J |di j = 1,nj = 2|Cmax is polynomially solvable, adding to the polynomial solutions known for the problems J2|nj . 2|Cmax and J2|di j = 1|Cmax (whereas other closely related versions such as J2|nj . 3|Cmax, J2|di j {1, 2}|Cmax, J3|nj . 2|Cmax, J3|di j = 1|Cmax, and J |di j = 1,nj . 3|Cmax are all known to be NPcomplete). For the general case L > 2 (i.e., for the job-shop problem J |di j = 1,nj = L > 2|Cmax), we present a competitive heuristic and provide experimental comparisons with other heuristic versions and, when possible, with the ideal integer linear programming formulation. © 2019 Association for Computing Machinery.",Critical resources; Job-shop scheduling; Mutual exclusion; Parallel programming; Polynomial-time algorithm,Heuristic programming; Integer programming; Parallel programming; Polynomial approximation; Scheduling; Competitive heuristic; Critical resources; Experimental comparison; Integer linear programming formulation; Job shop scheduling problems; Mutual exclusions; Parallel and distributed processing; Polynomial-time algorithms; Job shop scheduling
Modeling universal globally adaptive load-balanced routing,2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075647328&doi=10.1145%2f3349620&partnerID=40&md5=87b14c1f4228a60f8015b75731031621,"Universal globally adaptive load-balanced (UGAL) routing has been proposed for various interconnection networks and has been deployed in a number of current-generation supercomputers. Although UGAL-based schemes have been extensively studied, most existing results are based on either simulation or measurement. Without a theoretical understanding of UGAL, multiple questions remain: For which traffic patterns is UGAL most suited? In addition, what determines the performance of the UGAL-based scheme on a particular network configuration? In this work, we develop a set of throughput models for UGALbased on linear programming. We show that the throughput models are valid across the torus, Dragonfly, and Slim Fly network topologies. Finally, we identify a robust model that can accurately and efficiently predict UGAL throughput for a set of representative traffic patterns across different topologies. Our models not only provide a mechanism to predict UGAL performance on large-scale interconnection networks but also reveal the innerworking of UGAL and further our understanding of this type of routing. © 2019 Association for Computing Machinery.",Adaptive routing; High performance computing; UGAL routing,Interconnection networks (circuit switching); Linear programming; Supercomputers; Topology; Adaptive routing; Current generation; High performance computing; Large-scale interconnections; Network configuration; Network topology; Throughput models; UGAL routing; Network routing
I/O scheduling strategy for periodic applications,2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075636909&doi=10.1145%2f3338510&partnerID=40&md5=33d77f1c332f8f9ca1d469959d8f0bf6,"With the ever-growing need of data in HPC applications, the congestion at the I/O level becomes critical in supercomputers. Architectural enhancement such as burst buffers and pre-fetching are added to machines but are not sufficient to prevent congestion. Recent online I/O scheduling strategies have been put in place, but they add an additional congestion point and overheads in the computation of applications. In this work, we show how to take advantage of the periodic nature of HPC applications to develop efficient periodic scheduling strategies for their I/O transfers. Our strategy computes once during the job scheduling phase a pattern that defines the I/O behavior for each application, after which the applications run independently, performing their I/O at the specified times. Our strategy limits the amount of congestion at the I/O node level and can be easily integrated into current job schedulers.We validate this model through extensive simulations and experiments on an HPC cluster by comparing it to state-of-The-Art online solutions, showing that not only does our scheduler have the advantage of being de-centralized and thus overcoming the overhead of online schedulers, but also that it performs better than the other solutions, improving the application dilation up to 16% and the maximum system efficiency up to 18%. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",HPC; I/O; Periodicity; Scheduling; Supercomputers,Supercomputers; Architectural enhancement; Congestion points; Extensive simulations; On-line schedulers; Periodic scheduling; Periodicity; Scheduling strategies; System efficiency; Scheduling
Scalable deep learning via I/O analysis and optimization,2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068433990&doi=10.1145%2f3331526&partnerID=40&md5=f5ef7c6d1672333bb9ca50041b31c962,"Scalable deep neural network training has been gaining prominence because of the increasing importance of deep learning in a multitude of scientific and commercial domains. Consequently, a number of researchers have investigated techniques to optimize deep learning systems. Much of the prior work has focused on runtime and algorithmic enhancements to optimize the computation and communication. Despite these enhancements, however, deep learning systems still suffer from scalability limitations, particularly with respect to data I/O. This situation is especially true for training models where the computation can be effectively parallelized, leaving I/O as the major bottleneck. In fact, our analysis shows that I/O can take up to 90% of the total training time. Thus, in this article, we first analyze LMDB, the most widely used I/O subsystem of deep learning frameworks, to understand the causes of this I/O inefficiency. Based on our analysis, we propose LMDBIO—an optimized I/O plugin for scalable deep learning. LMDBIO includes six novel optimizations that together address the various shortcomings in existing I/O for deep learning. Our experimental results show that LMDBIO significantly outperforms LMDB in all cases and improves overall application performance by up to 65-fold on a 9,216-core system. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Caffe; I/O bottleneck; I/O in deep learning; LMDB; LMDBIO; Parallel I/O; Scalable deep learning,Neural networks; Application performance; Caffe; Learning frameworks; LMDB; LMDBIO; Neural network training; Parallel I/O; Training model; Deep neural networks
A generalized parallel task model for recurrent real-time processes,2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068151584&doi=10.1145%2f3322809&partnerID=40&md5=3f258bbc90f3a6f0d7563f05b290cfc8,"A model is considered for representing recurrent precedence-constrained tasks that are to execute on multiprocessor platforms. A recurrent task is specified as a directed acyclic graph (DAG), a period, and a relative deadline. Each vertex of the DAG represents a sequential job, while the edges of the DAG represent precedence constraints between these jobs. All the jobs of the DAG are released simultaneously and need to complete execution within the specified relative deadline of their release. Each task may release jobs in this manner an unbounded number of times, with successive releases occurring at least the specified period apart. Conditional control structures are also allowed. The scheduling problem is to determine whether a set of such recurrent tasks can be scheduled to always meet all deadlines upon a specified number of identical processors. This problem is shown to be computationally intractable, but amenable to efficient approximate solutions. Earliest Deadline First (EDF) and Deadline Monotonic (DM) are shown to be good approximate global scheduling algorithms. Polynomial and pseudo-polynomial time schedulability tests, of differing effectiveness, are presented for determining whether a given task set can be scheduled by EDF or DM to always meet deadlines on a specified number of processors. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Approximation algorithm; Conditional control-flow; Directed acyclic graph; Multiprocessor platform; Precedence constraints; Schedulability test,Approximation algorithms; Flow graphs; Multiprocessing systems; Polynomial approximation; Scheduling algorithms; Control flows; Directed acyclic graph (DAG); Multi-processor platforms; Precedence constraints; Schedulability test; Response time (computer systems)
Massively Parallel Polar Decomposition on Distributed-memory Systems,2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068168559&doi=10.1145%2f3328723&partnerID=40&md5=a3b7196ed6aff7921d03656505436b64,"We present a high-performance implementation of the Polar Decomposition (PD) on distributed-memory systems. Building upon on the QR-based Dynamically Weighted Halley (QDWH) algorithm, the key idea lies in finding the best rational approximation for the scalar sign function, which also corresponds to the polar factor for symmetric matrices, to further accelerate the QDWH convergence. Based on the Zolotarev rational functions-introduced by Zolotarev (ZOLO) in 1877-this new PD algorithm ZOLO-PD converges within two iterations even for ill-conditioned matrices, instead of the original six iterations needed for QDWH. ZOLO-PD uses the property of Zolotarev functions that optimality is maintained when two functions are composed in an appropriate manner. The resulting ZOLO-PD has a convergence rate up to 17, in contrast to the cubic convergence rate for QDWH. This comes at the price of higher arithmetic costs and memory footprint. These extra floating-point operations can, however, be processed in an embarrassingly parallel fashion. We demonstrate performance using up to 102,400 cores on two supercomputers. We demonstrate that, in the presence of a large number of processing units, ZOLO-PD is able to outperform QDWH by up to 2.3× speedup, especially in situations where QDWH runs out of work, for instance, in the strong scaling mode of operation. © 2019 Association for Computing Machinery.",Distributed-memory systems; Parallel algorithms; Polar decomposition; Strong scaling; Zolotarev functions,Approximation algorithms; Digital arithmetic; Memory architecture; Parallel algorithms; Supercomputers; Distributed memory systems; Floating point operations; High performance implementations; Ill-conditioned matrices; Massively parallels; Polar decompositions; Rational approximations; Strong scaling; Rational functions
Optimal schedule for all-to-all personalized communication in multiprocessor systems,2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068188061&doi=10.1145%2f3329867&partnerID=40&md5=de06b08863efef75a88a8db890d47050,"In this article, we address the problem of finding an optimal schedule for all-to-all personalized message communication among the processors in a multiprocessor system where every processor has a unique message for every other processor. When there are n processors and n2 parallel databus or channels for message communications, there exist algorithms that require O(n2) time for assigning the databus/channels to the processor-pairs to obtain a schedule with minimum number of time slots. However, in recent massively parallel processing systems with a huge number of processors, the number k of available databus/channels is usually much smaller than n2 . Thus, in each round of communication, only k processor-pairs (k < n2 ) can exchange their messages in parallel. We address this general case of all-to-all personalized communication and present a new technique for scheduling the channels among processor-pairs where only k < n2 databus/channels are available. We show that the proposed technique is optimal for all values of n other than 2k < n < 3k. For 2k < n < 3k, we show that the required number of rounds may be more than the lower bound of n(n2k−1) by at most 11%, and proving the optimality of our technique remains as an open problem in this case. © 2019 Association for Computing Machinery.",All-to-all personalized communication; Interconnection-networks; Message transfer; Multi-port processors; Multiprocessor systems; Optimal-schedule,Interconnection networks (circuit switching); All-to-all personalized communication; Message transfers; Multi processor systems; Multi-port; Optimal schedule; Parallel processing systems
Editorial from the editor-in-chief,2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068180213&doi=10.1145%2f3325883&partnerID=40&md5=d523bff0e98e9dcf0ce4ab51b3864ee7,[No abstract available],,
Optimizing I/O performance of HPC applications with autotuning,2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065757355&doi=10.1145%2f3309205&partnerID=40&md5=ddba457f90e960c337b7c4871cfd071b,"Parallel Input output is an essential component of modern high-performance computing (HPC). Obtaining good I/O performance for a broad range of applications on diverse HPC platforms is a major challenge, in part, because of complex inter dependencies between I/O middleware and hardware. The parallel file system and I/O middleware layers all offer optimization parameters that can, in theory, result in better I/O performance. Unfortunately, the right combination of parameters is highly dependent on the application, HPC platform, problem size, and concurrency. Scientific application developers do not have the time or expertise to take on the substantial burden of identifying good parameters for each problem configuration. They resort to using system defaults, a choice that frequently results in poor I/O performance. We expect this problem to be compounded on exascale-class machines, which will likely have a deeper software stack with hierarchically arranged hardware resources. We present as a solution to this problem an autotuning system for optimizing I/O performance, I/O performance modeling, I/O tuning, and I/O patterns. We demonstrate the value of this framework across several HPC platforms and applications at scale. © 2019 Association for Computing Machinery.",Autotuning; HPC; I/O; Parallel file systems; Performance optimization; Storage,Computation theory; Energy storage; Middleware; Autotuning; Hardware resources; Inter-dependencies; Modern high performance; Optimization parameter; Parallel file system; Performance optimizations; Scientific applications; File organization
EagerMap: A task mapping algorithm to improve communication and load balancing in clusters of multicore systems,2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065770209&doi=10.1145%2f3309711&partnerID=40&md5=b38ce8e475ef108fd7c7f6f74bd173e3,"Communication between tasks and load imbalance have been identified as a major challenge for the performance and energy efficiency of parallel applications. A common way to improve communication is to increase its locality, that is, to reduce the distances of data transfers, prioritizing the usage of faster and more efficient local interconnections over remote ones. Regarding load imbalance, cores should execute a similar amount of work. An important problem to be solved in this context is how to determine an optimized mapping of tasks to cluster nodes and cores that increases the overall locality and load balancing. In this article, we propose the EagerMap algorithm to determine task mappings, which is based on a greedy heuristic to match application communication patterns to hardware hierarchies and which can also consider the task load. Compared to previous algorithms, EagerMap is faster, scales better, and supports more types of computer systems, while maintaining the same or better quality of the determined task mapping. EagerMap is therefore an interesting choice for task mapping on a variety of modern parallel architectures. © 2019 Association for Computing Machinery.",Clusters; Communication; Locality; Task mapping,Communication; Conformal mapping; Data transfer; Energy efficiency; Parallel architectures; Clusters; Communication pattern; Greedy heuristics; Load imbalance; Locality; Multi-core systems; Parallel application; Task mapping; Clustering algorithms
Concurrent hash tables: Fast and General(?)!,2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062322422&doi=10.1145%2f3309206&partnerID=40&md5=3844c50a2391c1988274bbf89bb6f553,"Concurrent hash tables are one of the most important concurrent data structures, which are used in numerous applications. For some applications, it is common that hash table accesses dominate the execution time. To efficiently solve these problems in parallel, we need implementations that achieve speedups in highly concurrent scenarios. Unfortunately, currently available concurrent hashing libraries are far away from this requirement, in particular, when adaptively sized tables are necessary or contention on some elements occurs. Our starting point for better performing data structures is a fast and simple lock-free concurrent hash table based on linear probing that is, however, limited to word-sized key-value types and does not support dynamic size adaptation. We explain how to lift these limitations in a provably scalable way and demonstrate that dynamic growing has a performance overhead comparable to the same generalization in sequential hash tables. We perform extensive experiments comparing the performance of our implementations with six of the most widely used concurrent hash tables. Ours are considerably faster than the best algorithms with similar restrictions and an order of magnitude faster than the best more general tables. In some extreme cases, the difference even approaches four orders of magnitude. All our implementations discussed in this publication can be found on github [17]. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Concurrency; Dynamic data structures; Experimental analysis; Hash table; Lock-freedom; Transactional memory,Data structures; Locks (fasteners); Concurrency; Dynamic data structure; Experimental analysis; Hash table; Lock freedoms; Transactional memory; Concurrency control
Group mutual exclusion by fetch-and-increment,2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062322147&doi=10.1145%2f3309202&partnerID=40&md5=eb8ee748c8861fe7be9bc1a013077918,"The group mutual exclusion (GME) problem (also called the room synchronization problem) arises in various practical applications that require concurrent data sharing. Group mutual exclusion aims to achieve exclusive access to a shared resource (a shared room) while facilitating concurrency among non-conflicting requests. The problem is that threads with distinct interests are not allowed to access the shared resource concurrently, but multiple threads with same interest can. In Blelloch et al. (2003), the authors presented a simple solution to the room synchronization problem using fetch&add (F&A) and test-and-set (T&S) atomic operations. This algorithm has O(m) remote memory references (RMRs) in the cache coherent (CC) model, where m is the number of forums. In Bhatt and Huang (2010), an open problem was posed: “Is it possible to design a GME algorithm with constant RMR for the CC model using fetch&addinstructions?” This question is partially answered in this article by presenting a group mutual exclusion algorithm using fetch-and-increment instructions. The algorithm is simple and scalable. © 2019 Association for Computing Machinery.",Algorithms; Concurrency; Concurrent date structure; Constant remote memory access; Fetch-and-increment; Group mutual exclusion; Synchronization,Algorithms; Synchronization; Atomic operation; Concurrency; Fetch-and-increment; Group mutual exclusion; Multiple threads; Remote memory access; Remote memory references; Synchronization problem; Cache memory
Multigrid for matrix-free high-order finite element computations on graphics processors,2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065760623&doi=10.1145%2f3322813&partnerID=40&md5=a3201e2c92a318b32bb8b58077d210d2,"This article presents matrix-free finite-element techniques for efficiently solving partial differential equations on modern many-core processors, such as graphics cards. We develop a GPU parallelization of a matrix-free geometric multigrid iterative solver targeting moderate and high polynomial degrees, with support for general curved and adaptively refined hexahedral meshes with hanging nodes. The central algorithmic component is the matrix-free operator evaluation with sum factorization. We compare the node-level performance of our implementation running on an Nvidia Pascal P100 GPU to a highly optimized multicore implementation running on comparable Intel Broadwell CPUs and an Intel Xeon Phi. Our experiments show that the GPU implementation is approximately 1.5 to 2 times faster across four different scenarios of the Poisson equation and a variety of element degrees in 2D and 3D. The lowest time to solution per degree of freedom is recorded for moderate polynomial degrees between 3 and 5. A detailed performance analysis highlights the capabilities of the GPU architecture and the chosen execution model with threading within the element, particularly with respect to the evaluation of the matrix-vector product. Atomic intrinsics are shown to provide a fast way for avoiding the possible race conditions in summing the elemental residuals into the global vector associated to shared vertices, edges, and surfaces. In addition, the solver infrastructure allows for using mixed-precision arithmetic that performs the multigrid V-cycle in single precision with an outer correction in double precision, increasing throughput by up to 83%. © 2019 Association for Computing Machinery.",CUDA; Finite element method; Geometric multigrid; Matrix-free method; Sum factorization,Computer graphics equipment; Degrees of freedom (mechanics); Factorization; Finite element method; Iterative methods; Matrix algebra; Poisson equation; Polynomials; Program processors; CUDA; Finite element techniques; Geometric multigrids; High-order finite elements; Many-core processors; Matrix-free methods; Matrix-vector products; Performance analysis; Graphics processing unit
Lock contention management in multithreaded MPI,2019,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059887270&doi=10.1145%2f3275443&partnerID=40&md5=385da0a0f5b639a38e99b1f06e27d479,"In this article, we investigate contention management in lock-based thread-safe MPI libraries. Specifically, we make two assumptions: (1) locks are the only form of synchronization when protecting communication paths; and (2) contention occurs, and thus serialization is unavoidable. Our work distinguishes between lock acquisitions with respect to work being performed inside a critical section; productive vs. unproductive. Waiting for message reception without doing anything else inside a critical section is an example of unproductive lock acquisition. We show that the high-throughput nature of modern scalable locking protocols translates into better communication progress for throughput-intensive MPI communication but negatively impacts latency-sensitive communication because of overzealous unproductive lock acquisition. To reduce unproductive lock acquisitions, we devised a method that promotes threads with productive work using a generic two-level priority locking protocol. Our results show that using a high-throughput protocol for productive work and a fair protocol for less productive code paths ensures the best tradeoff for fine-grained communication, whereas a fair protocol is sufficient for more coarse-grained communication. Although these efforts have been rewarding, scalability degradation remains significant. We discuss techniques that diverge from the pure locking model and offer the potential to further improve scalability. © 2019 ACM. All rights reserved.",Critical section; MPI; Runtime contention; Threads,Mergers and acquisitions; Scalability; Throughput; Communication path; Contention managements; Critical sections; High-throughput protocols; Message reception; MPI communications; Runtimes; Threads; Locks (fasteners)
Robust and probabilistic failure-Aware placement,2018,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056340036&doi=10.1145%2f3210367&partnerID=40&md5=7fb4a23db2dec536f9b757fd8ca801d6,"Motivated by the growing complexity and heterogeneity of modern data centers, and the prevalence of commodity component failures, this article studies the failure-Aware placement problem of placing tasks of a parallel job on machines in the data center with the goal of increasing availability. We consider two models of failures: Adversarial and probabilistic. In the adversarial model, each node has a weight (higher weight implying higher reliability) and the adversary can remove any subset of nodes of total weight at most a given boundW and our goal is to find a placement that incurs the least disruption against such an adversary. In the probabilistic model, each node has a probability of failure and we need to find a placement that maximizes the probability that at least K out of N tasks survive at any time. For adversarial failures, we first show that (i) the problems are in Σ2, the second level of the polynomial hierarchy; (ii) a variant of the problem that we call RobustFAP (for Robust Failure-Aware Placement) is co-NP-hard; and (iii) an all-or-nothing version of RobustFAP is σ2-complete. We then give a polynomialtime approximation scheme (PTAS) for RobustFAP, a key ingredient of which is a solution that we design for a fractional version of RobustFAP. We then study HierRobustFAP, which is the fractional RobustFAP problem over a hierarchical network, in which failures can occur at any subset of nodes in the hierarchy, and a failure at a node can adversely impact all of its descendants in the hierarchy. To solve HierRobustFAP, we introduce a notion of hierarchical max-min fairness and a novel Generalized Spreading algorithm, which is simultaneously optimal for every upper bound W on the total weight of nodes that an adversary can fail. These generalize the classical notion of max-min fairness to work with nodes of differing capacities, differing reliability weights, and hierarchical structures. Using randomized rounding, we extend this to give an algorithm for integral HierRobustFAP. For the probabilistic version, we first give an algorithm that achieves an additive ϵ approximation in the failure probability for the single level version, called ProbFAP, while giving up a (1 + ϵ ) multiplicative factor in the number of failures. We then extend the result to the hierarchical version, HierProbFAP, achieving an ϵ additive approximation in failure probability while giving up an (L + ϵ ) multiplicative factor in the number of failures, where L is the number of levels in the hierarchy. © 2018 Association for Computing Machinery. All rights reserved.",Datacenter networks; Failure-Aware placement; Max-min fairness,Failure (mechanical); Failure analysis; Probability; Data center networks; Hierarchical structures; Max-min fairness; Multiplicative factors; Polynomial hierarchies; Polynomial time approximation schemes; Probabilistic modeling; Probability of failure; Approximation algorithms
Access to data and number of iterations: Dual primal algorithms for maximum matching under resource constraints,2018,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054895037&doi=10.1145%2f3154855&partnerID=40&md5=5665bb64d0df9300fb22724b909b6549,"In this article, we consider graph algorithms in models of computation where the space usage (random accessible storage, in addition to the read-only input) is sublinear in the number of edges m and the access to input is constrained. These questions arise in many natural settings, and in particular in the analysis of streaming algorithms, MapReduce or similar algorithms, or message passing distributed computing that model constrained parallelism with sublinear central processing. We focus on weighted nonbipartite maximum matching in this article. For any constant p > 1, we provide an iterative sampling-based algorithm for computing a (1 − ϵ)-approximation of the weighted nonbipartite maximum matching that uses O(p/ϵ) rounds of sampling, and O(n1+1/p) space. The results extend to b-Matching with small changes. This article combines adaptive sketching literature and fast primal-dual algorithms based on relaxed Dantzig-Wolfe decision procedures. Each round of sampling is implemented through linear sketches and can be executed in a single round of streaming or two rounds of MapReduce. The article also proves that nonstandard linear relaxations of a problem, in particular penalty-based formulations, are helpful in reducing the adaptive dependence of the iterations. © 2018 ACM.",Maximum matching; Primal dual algorithms,Iterative methods; Message passing; Random access storage; Decision procedure; Maximum matchings; Models of computation; Number of iterations; Primal dual algorithms; Resource Constraint; Sampling-based algorithms; Streaming algorithm; Approximation algorithms
Better Bounds for Coalescing-Branching RandomWalks,2018,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056324809&doi=10.1145%2f3209688&partnerID=40&md5=23a35fc5c982351fb2ed070cce04f04a,"Coalescing-branching random walks, or cobra walks for short, are a natural variant of random walks on graphs that can model the spread of disease through contacts or the spread of information in networks. In a k-cobra walk, at each timestep, a subset of the vertices are active; each active vertex chooses k random neighbors (sampled independently and uniformly with replacement) that become active at the next step, and these are the only active vertices at the next step. A natural quantity to study for cobra walks is the cover time, which corresponds to the expected time when all nodes have become infected or received the disseminated information. In this article, we extend previous results for cobra walks in multiple ways. We show that the cover time for the 2-cobra walk on [0,n]d is O(n) (where the order notation hides constant factors that depend on d); previous work had shown the cover time was O(n ?polylog(n)). We show that the cover time for a 2-cobra walk on an n-vertex d-regular graph with conductance ψG is O(d4ψ-2                             G log2 n), significantly generalizing a previous result that held only for expander graphs with sufficiently high expansion. And, finally, we show that the cover time for a 2-cobra walk on a graph with n vertices andm edges is always O(mn3/4 logn); this is the first result showing that the bound of θ(n3) for the worst-case cover time for random walks can be beaten using 2-cobra walks. © 2018 Association for Computing Machinery. All rights reserved.",Cover time; Epidemic processes; Information spreading; Networks; Random walks,Flocculation; Networks (circuits); Random processes; Branching random walks; Constant factors; Cover time; Epidemic process; Information spreading; Random Walk; Spread of disease; Spread of informations; Graph theory
Race detection in two dimensions,2018,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054898460&doi=10.1145%2f3264618&partnerID=40&md5=a3b21331195be8bd9feae654c931c7c3,"Dynamic race detection is a program analysis technique for detecting errors caused by undesired interleavings of concurrent tasks. A primary challenge when designing efficient race detection algorithms is to achieve manageable space requirements. State-of-the-art algorithms for unstructured parallelism require Θ(n) space per monitored memory location, where n is the total number of tasks. This is a serious drawback when analyzing programs with many tasks. In contrast, algorithms for programs with a series-parallel (SP) structure require only Θ(1) space. Unfortunately, it is currently not well understood if there are classes of parallelism beyond SP that can also benefit from and be analyzed with Θ(1) space complexity. In this work, we show that structures richer than SP graphs, namely, that of two-dimensional (2D) lattices, can also be analyzed in Θ(1) space. Toward that (a) we extend Tarjan’s algorithm for finding lowest common ancestors to handle 2D lattices; (b) from that extension we derive a serial algorithm for race detection that can analyze arbitrary task graphs with a 2D lattice structure; (c) we present a restriction to fork-join that admits precisely the 2D lattices as task graphs (e.g., it can express pipeline parallelism). Our work generalizes prior work on structured race detection and aims to provide a deeper understanding of the interplay between structured parallelism and program analysis efficiency. © 2018 Association for Computing Machinery.",Lowest common ancestors; Parallel language constructs; Race conditions; Race detection; Structured parallelism; Two-dimensional lattices,Computer networks; Hazards and race conditions; Software engineering; Lowest common ancestors; Parallel languages; Race detection; Structured parallelism; Two-dimensional lattices; Crystal lattices
Efficient race detection for reducer hyperobjects,2018,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054893039&doi=10.1145%2f3205914&partnerID=40&md5=97c1e8811d03ef6f12756fe5e3806341,"A multithreaded Cilk program that is ostensibly deterministic May nevertheless behave nondeterministically due to programming errors in the code. For a Cilk program that uses reducers—a general reduction mechanism supported in various Cilk dialects—such programming errors are especially challenging to debug, because the errors can expose the nondeterminism in how the Cilk runtime system manages reducers. We identify two unique types of races that arise from incorrect use of reducers in a Cilk program, and we present two algorithms to catch these races. The first algorithm, called the Peer-Set algorithm, detects view-read races, which occur when the program attempts to retrieve a value out of a reducer when the read May result in a nondeterministic value, such as before all previously spawned subcomputations that might update the reducer have necessarily returned. The second algorithm, called the SP+ algorithm, detects determinacy races—instances where a write to a memory location occurs logically in parallel with another access to that location—even when the raced-on memory locations relate to reducers. Both algorithms are provably correct, asymptotically efficient, and can be implemented efficiently in practice. We have implemented both algorithms in our prototype race detector, Rader. When running Peer-Set, Rader incurs a geometric-mean multiplicative overhead of 2.56 over running the benchmark without instrumentation. When running SP+, Rader incurs a geometric-mean multiplicative overhead of 16.94. © 2018 ACM.",Cilk; Determinacy race; Nondeterminism; Reducers; View-read race,Errors; Location; Cilk; Determinacy race; Non-determinism; Reducers; View-read race; Program debugging
Introduction to special issue on SPAA’15,2018,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054897513&doi=10.1145%2f3226041&partnerID=40&md5=a243f94850fd45b172596437b848c68e,[No abstract available],,
Introduction to the Special Issue for SPAA 2016,2018,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056301167&doi=10.1145%2f3230677&partnerID=40&md5=6b61080289354ecc5dfaffc0d9e38d81,[No abstract available],,
Lock-Free transactional transformation for linked data structures,2018,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056310796&doi=10.1145%2f3209690&partnerID=40&md5=f1e0001b87a5377213c0eff2fa638f9e,"Nonblocking data structures allow scalable and thread-safe access to shared data. They provide individual operations that appear to execute atomically. However, it is often desirable to execute multiple operations atomically in a transactional manner. Previous solutions, such as Software Transactional Memory (STM) and transactional boosting, manage transaction synchronization separately from the underlying data structure's thread synchronization. Although this reduces programming effort, it leads to overhead associated with additional synchronization and the need to rollback aborted transactions. In this work, we present a newmethodology for transforming high-performance lock-free linked data structures into high-performance lock-free transactional linked data structures without revamping the data structures' original synchronization design. Our approach leverages the semantic knowledge of the data structure to eliminate the overhead of false conflicts and rollbacks. We encapsulate all operations, operands, and transaction status in a transaction descriptor, which is shared among the nodes accessed by the same transaction.We coordinate threads to help finish the remaining operations of delayed transactions based on their transaction descriptors. When a transaction fails,we recover the correct abstract state by reversely interpreting the logical status of a node.We also present an obstruction-free version of our algorithm that can be applied to dynamic execution scenarios and an example of our approach applied to a hash map. In our experimental evaluation using transactions with randomly generated operations, our lock-free transactional data structures outperform the transactional boosted ones by 70% on average. They also outperform the alternative STM-based approaches by a factor of 2 to 13 across all scenarios. More importantly, we achieve 4,700 to 915,000 times fewer spurious aborts than the alternatives. © 2018 Association for Computing Machinery. All rights reserved.",Lock-free; Transactional boosting; Transactional data structure; Transactional memory,Data handling; Data structures; Linked data; Locks (fasteners); Metadata; Semantics; Storage allocation (computer); Synchronization; Experimental evaluation; Lock-free; Nonblocking data structures; Software transactional memory; Thread synchronization; Transactional boosting; Transactional data; Transactional memory; Information management
Fast distributed algorithms for connectivity and MST in Large Graphs,2018,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051102207&doi=10.1145%2f3209689&partnerID=40&md5=5a8302800f32eaf6b3adf05773692472,"Motivated by the increasing need to understand the algorithmic foundations of distributed large-scale graph computations, we study a number of fundamental graph problems in amessage-passing model for distributed computing where k ≥ 2 machines jointly perform computations on graphs with n nodes (typically, n > k). The input graph is assumed to be initially randomly partitioned among the k machines, a common implementation in many real-world systems. Communication is point-To-point, and the goal is to minimize the number of communication rounds of the computation. Our main result is an (almost) optimal distributed randomized algorithm for graph connectivity. Our algorithm runs in Õ (n/k2) rounds ( Õ notation hides a polylog(n) factor and an additive polylog(n) term). This improves over the best previously known bound of Õ (n/k) [Klauck et al., SODA 2015] and is optimal (up to a polylogarithmic factor) in light of an existing lower bound of ω(n/k2). Our improved algorithm uses a bunch of techniques, including linear graph sketching, that prove useful in the design of efficient distributed graph algorithms. Using the connectivity algorithm as a building block, we then present fast randomized algorithms for computing minimum spanning trees, (approximate) min-cuts, and for many graph verification problems. All these algorithms take Õ (n/k2) rounds and are optimal up to polylogarithmic factors. We also show an almost matching lower bound of ω (n/k2) rounds for many graph verification problems by leveraging lower bounds in random-partition communication complexity. © 2018 Association for Computing Machinery. All rights reserved.",Distributed graph algorithms; Graph connectivity; Graph sketching; Minimum spanning trees,Distributed computer systems; Algorithmic foundations; Communication complexity; Connectivity algorithms; Distributed graph algorithms; Graph connectivity; Graph sketching; Minimum spanning trees; Poly-logarithmic factors; Trees (mathematics)
ThreadScan: Automatic and scalable memory reclamation,2018,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054890115&doi=10.1145%2f3201897&partnerID=40&md5=cde0f8933679f4ee675ead47f3fc5bea,"The concurrent memory reclamation problem is that of devising a way for a deallocating thread to verify that no other concurrent threads hold references to a memory block being deallocated. To date, in the absence of automatic garbage collection, there is no satisfactory solution to this problem; existing tracking methods like hazard pointers, reference counters, or epoch-based techniques like RCU are either prohibitively expensive or require significant programming expertise to the extent that implementing them efficiently can be worthy of a publication. None of the existing techniques are automatic or even semi-automated. In this article, we take a new approach to concurrent memory reclamation. Instead of manually tracking access to memory locations as done in techniques like hazard pointers, or restricting shared accesses to specific epoch boundaries as in RCU, our algorithm, called ThreadScan, leverages operating system signaling to automatically detect which memory locations are being accessed by concurrent threads. Initial empirical evidence shows that ThreadScan scales surprisingly well and requires negligible programming effort beyond the standard use of Malloc and Free. © 2018 ACM.",Concurrent data structures; Lock-based data structures; Lock-free data structures; Memory reclamation,Concurrency control; Data structures; Hazards; Locks (fasteners); Automatic garbage collection; Concurrent data structures; Concurrent threads; Lock-free data structures; Memory locations; Satisfactory solutions; Scalable memory; Tracking method; Reclamation
Randomized approximate nearest neighbor search with limited adaptivity,2018,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056311573&doi=10.1145%2f3209884&partnerID=40&md5=a2f2d83c85f77112fadba22a00b7dfef,"We study the complexity of parallel data structures for approximate nearest neighbor search in d-dimensional Hamming space {0, 1}d . A classic model for static data structures is the cell-probe model [27]. We consider a cell-probe model with limited adaptivity, where given a k ≥ 1, a query is resolved by making at most k rounds of parallel memory accesses to the data structure. We give two randomized algorithms that solve the approximate nearest neighbor search using k rounds of parallel memory accesses: -a simple algorithm with O(k(logd)1/k ) total number of memory accesses for all k ≥ 1; -an algorithm withO(k + ( 1/k logd)O(1/k)) total number of memory accesses for all sufficiently large k. Both algorithms use data structures of polynomial size. We prove an ω( 1/k (logd)1/k ) lower bound for the total number of memory accesses for any randomized algorithm solving the approximate nearest neighbor search within k ≤ log log d/2 log log log d rounds of parallel memory accesses on any data structures of polynomial size. This lower bound shows that our first algorithm is asymptotically optimal when k = O(1). And our second algorithm achieves the asymptotically optimal tradeoff between number of rounds and total number ofmemory accesses. In the extremal case, when k = O( log log d/log log log d ) is big enough, our second algorithm matches the θ( log log d/log log log d ) tight bound for fully adaptive algorithms for approximate nearest neighbor search in [11]. © 2018 Association for Computing Machinery. All rights reserved.",Cell-probe model; Communication complexity; Data structures; Nearest neighbor search,Adaptive algorithms; Data structures; Memory architecture; Probes; Asymptotically optimal; Cell probes; Communication complexity; Parallel data structures; Parallel memory; Polynomial size; Randomized Algorithms; SIMPLE algorithm; Nearest neighbor search
PowerLyra: Differentiated graph computation and partitioning on skewed graphs,2018,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061273684&doi=10.1145%2f3298989&partnerID=40&md5=73d6f6f640128439ee026e1eb20805a4,"Natural graphs with skewed distributions raise unique challenges to distributed graph computation and partitioning. Existing graph-parallel systems usually use a “one-size-fits-all” design that uniformly processes all vertices, which either suffer from notable load imbalance and high contention for high-degree vertices (e.g., Pregel and GraphLab) or incur high communication cost and memory consumption even for low-degree vertices (e.g., PowerGraph and GraphX). In this article, we argue that skewed distributions in natural graphs also necessitate differentiated processing on high-degree and low-degree vertices. We then introduce PowerLyra, a new distributed graph processing system that embraces the best of both worlds of existing graph-parallel systems. Specifically, PowerLyra uses centralized computation for low-degree vertices to avoid frequent communications and distributes the computation for high-degree vertices to balance workloads. PowerLyra further provides an efficient hybrid graph partitioning algorithm (i.e., hybrid-cut) that combines edge-cut (for low-degree vertices) and vertex-cut (for high-degree vertices) with heuristics. To improve cache locality of inter-node graph accesses, PowerLyra further provides a locality-conscious data layout optimization. PowerLyra is implemented based on the latest GraphLab and can seamlessly support various graph algorithms running in both synchronous and asynchronous execution modes. A detailed evaluation on three clusters using various graph-analytics and MLDM (Machine Learning and Data Mining) applications shows that PowerLyra outperforms PowerGraph by up to 5.53X (from 1.24X) and 3.26X (from 1.49X) for real-world and synthetic graphs, respectively, and is much faster than other systems like GraphX and Giraph, yet with much less memory consumption. A porting of hybrid-cut to GraphX further confirms the efficiency and generality of PowerLyra. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Graph computation; Graph partitioning; Locality-conscious data layout; Power-law degree distribution; Skewed graph,Data mining; Graphic methods; Learning systems; Parallel processing systems; Asynchronous executions; Centralized computation; Data layout optimization; Data layouts; Graph Partitioning; Power law degree distribution; Skewed distribution; Skewed graph; Graph theory
BARAN: Bimodal adaptive reconfigurable-allocator network-on-chip,2018,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061224504&doi=10.1145%2f3294049&partnerID=40&md5=86dddd03ff7efca8f7ecfe01bc334271,"Virtual channels are employed to improve the throughput under high traffic loads in Networks-on-Chips (NoCs). However, they can impose non-negligible overheads on performance by prolonging clock cycle time, especially under low traffic loads where the impact of virtual channels on performance is trivial. In this article, we propose a novel architecture, called BARAN, that can either improve on-chip network performance or reduce its power consumption (depending on the specific implementation chosen), not both at the same time, when virtual channels are underutilized; that is, the average number of virtual channel allocation requests per cycle is lower than the number of total virtual channels. We also introduce a reconfigurable arbitration logic within the BARAN architecture that can be configured to have multiple latencies and, hence, multiple slack times. The increased slack times are then used to reduce the supply voltage of the routers or increase their clock frequency in order to reduce power consumption or improve the performance of the whole NoC system. The power-centric design of BARAN reduces NoC power consumption by 43.4% and 40.6% under CMP and GPU workloads, on average, respectively, compared to a baseline architecture while imposing negligible area and performance overheads. The performance-centric design of BARAN reduces the average packet latency by 45.4% and 42.1%, on average, under CMP and GPU workloads, respectively, compared to the baseline architecture while increasing power consumption by 39.7% and 43.7%, on average. Moreover, the performance-centric BARAN postpones the network saturation rate by 11.5% under uniform random traffic compared to the baseline architecture. © 2019 Association for Computing Machinery.",Low-power design; Network-on-chip; Performance optimization; Reconfigurable logic,Clocks; Communication channels (information theory); Computer circuits; Electric power supplies to apparatus; Electric power utilization; Low power electronics; Network architecture; Network-on-chip; Reconfigurable architectures; Reconfigurable hardware; Routers; Servers; Average packet latencies; Base-line architecture; Low-power design; Network saturation; Novel architecture; Performance optimizations; Reconfigurable logic; Virtual channels; Integrated circuit design
Scheduling dynamic parallel workload of mobile devices with access guarantees,2018,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061201347&doi=10.1145%2f3291529&partnerID=40&md5=c7b9c63a373924286d92f4fd121abd13,"We study a dynamic resource-allocation problem that arises in various parallel computing scenarios, such as mobile cloud computing, cloud computing systems, Internet of Things systems, and others. Generically, we model the architecture as client mobile devices and static base stations. Each client ""arrives"" to the system to upload data to base stations by radio transmissions and then ""leaves."" The problem, called Station Assignment, is to assign clients to stations so that every client uploads their data under some restrictions, including a target subset of stations, a maximum delay between transmissions, a volume of data to upload, and a maximum bandwidth for each station. We study the solvability of Station Assignment under an adversary that controls the arrival and departure of clients, limited to maximum rate and burstiness of such arrivals. We show upper and lower bounds on the rate and burstiness for various client arrival schedules and protocol classes. To the best of our knowledge, this is the first time that Station Assignment is studied under adversarial arrivals and departures. © 2018 Association for Computing Machinery.",Continuous adversarial dynamics; Health monitoring systems; Internet of things; Mobile cloud computing; Radio networks; Station assignment,Base stations; Internet of things; Mobile radio systems; Radio transmission; Dynamic resource allocations; Health monitoring system; Maximum delay; Parallel workloads; Radio networks; Scheduling dynamics; Station assignment; Upper and lower bounds; Mobile cloud computing
A high-qality and fast maximal independent set implementation for GPUs,2018,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061191705&doi=10.1145%2f3291525&partnerID=40&md5=9de1664e26605b087b0bf0ad843fded4,"Computing a maximal independent set is an important step in many parallel graph algorithms. This article introduces ECL-MIS, a maximal independent set implementation that works well on GPUs. It includes key optimizations to speed up computation, reduce the memory footprint, and increase the set size. Its CUDA implementation requires fewer than 30 kernel statements, runs asynchronously, and produces a deterministic result. It outperforms the maximal independent set implementations of Pannotia, CUSP, and IrGL on each of the 16 tested graphs of various types and sizes. On a Titan X GPU, ECL-MIS is between 3.9 and 100 times faster (11.5 times, on average). ECL-MIS running on the GPU is also faster than the parallel CPU codes Ligra, Ligra+, and PBBS running on 20 Xeon cores, which it outperforms by 4.1 times, on average. At the same time, ECL-MIS produces maximal independent sets that are up to 52% larger (over 10%, on average) compared to these preexisting CPU and GPU implementations. Whereas these codes produce maximal independent sets that are, on average, about 15% smaller than the largest possible such sets, ECL-MIS sets are less than 6% smaller than the maximum independent sets. © 2018 Association for Computing Machinery.",Code optimization; GPU; Maximal independent set; Parallel programming,Graphics processing unit; Parallel programming; Program processors; Code optimization; GPU implementation; Maximal independent set; Maximum independent sets; Memory footprint; Parallel graph algorithms; Speed up; Codes (symbols)
New high performance GPGPU code transformation framework applied to large production weather prediction code,2018,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061209419&doi=10.1145%2f3291523&partnerID=40&md5=5ed8d271030521d3d59a7648d13eece3,"We introduce ""Hybrid Fortran,"" a new approach that allows a high-performance GPGPU port for structured grid Fortran codes. This technique only requires minimal changes for a CPU targeted codebase, which is a significant advancement in terms of productivity. It has been successfully applied to both dynamical core and physical processes of ASUCA, a Japanese mesoscale weather prediction model with more than 150k lines of code. By means of a minimal weather application that resembles ASUCA's code structure, Hybrid Fortran is compared to both a performance model as well as today's commonly used method, OpenACC. As a result, the Hybrid Fortran implementation is shown to deliver the same or better performance than OpenACC, and its performance agrees with the model both on CPU and GPU. In a full-scale production run, using an ASUCA grid with 1581 × 1301 × 58 cells and real-world weather data in 2km resolution, 24 NVIDIA Tesla P100 running the Hybrid Fortran-based GPU port are shown to replace more than fifty 18-core Intel Xeon Broadwell E5-2695 v4 running the reference implementation - an achievement comparable to more invasive GPGPU rewrites of other weather models. © 2018 Copyright held by the owner/author(s).",CUDA; Fortran; GPGPU; OpenACC; Performance models; Weather prediction,Codes (symbols); Cosine transforms; FORTRAN (programming language); Program processors; CUDA; GPGPU; Openacc; Performance Model; Weather prediction; Weather forecasting
A lower bound technique for communication in BSP,2018,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054891846&doi=10.1145%2f3181776&partnerID=40&md5=a236694d37e36df6ebaa58f81396ab94,"Communication is a major factor determining the performance of algorithms on current computing systems; it is therefore valuable to provide tight lower bounds on the communication complexity of computations. This article presents a lower bound technique for the communication complexity in the bulk-synchronous parallel (BSP) model of a given class of DAG computations. The derived bound is expressed in terms of the switching potential of a DAG, that is, the number of permutations that the DAG can realize when viewed as a switching network. The proposed technique yields tight lower bounds for the fast Fourier transform (FFT), and for any sorting and permutation network. A stronger bound is also derived for the periodic balanced sorting network, by applying this technique to suitable subnetworks. Finally, we demonstrate that the switching potential captures communication requirements even in computational models different from BSP, such as the I/O model and the LPRAM. © 2017 ACM.",Communication; FFT; Lower bounds; Parallel computing; Sorting networks; Switching networks,Communication; Computational complexity; Fast Fourier transforms; Parallel processing systems; Switching; Switching networks; Balanced sorting network; Bulk synchronous parallel models; Communication complexity; Lower bound techniques; Lower bounds; Performance of algorithm; Permutation network; Sorting network; Complex networks
An autotuning protocol to rapidly build autotuners,2018,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053472502&doi=10.1145%2f3291527&partnerID=40&md5=358cb3eddc35bb6bef8e0f22ec4da510,"Automatic performance tuning (Autotuning) is an increasingly critical tuning technique for the high portable performance of Exascale applications. However, constructing an autotuner from scratch remains a challenge, even for domain experts. In this work, we propose a performance tuning and knowledge management suite (PAK) to help rapidly build autotuners. In order to accommodate existing autotuning techniques, we present an autotuning protocol that is composed of an extractor, producer, optimizer, evaluator, and learner. To achieve modularity and reusability, we also define programming interfaces for each protocol component as the fundamental infrastructure, which provides a customizable mechanism to deploy knowledge mining in the performance database. PAK's usability is demonstrated by studying two important computational kernels: stencil computation and sparse matrix-vector multiplication (SpMV). Our proposed autotuner based on PAK shows comparable performance and higher productivity than traditional autotuners by writing just a few tens of code using our autotuning protocol. © 2019 Association for Computing Machinery.",Autotuner; Knowledge database; Protocol; SpMV; Stencil,Network protocols; Reusability; Automatic performance tuning; Autotuner; Computational kernels; Knowledge database; Programming interface; Sparse matrix-vector multiplication; SpMV; Stencil; Knowledge management
ESTIMA: Extrapolating scalability of in-memory applications,2017,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056333189&doi=10.1145%2f3108137&partnerID=40&md5=7a02a68ff4f639dd6563005e390852c5,"This article presents estima, an easy-To-use tool for extrapolating the scalability of in-memory applications. estima is designed to perform a simple yet important task: Given the performance of an application on a small machine with a handful of cores, estima extrapolates its scalability to a larger machine with more cores, while requiring minimum input from the user. The key idea underlying estima is the use of stalled cycles (e.g., cycles that the processor spends waiting for missed cache line fetches or busy locks). estima measures stalled cycles on a few cores and extrapolates them to more cores, estimating the amount of waiting in the system. estima can be effectively used to predict the scalability of in-memory applications for bigger execution machines. For instance, using measurements of memcached and SQLite on a desktop machine, we obtain accurate predictions of their scalability on a server. Our extensive evaluation shows the effectiveness of estima on a large number of in-memory benchmarks. © 2017 ACM.",ESTIMA; Extrapolation; in-memory; Prediction; Scalability; Stalled cycles,Cache memory; Extrapolation; Forecasting; Accurate prediction; ESTIMA; Measurements of; Memory applications; Stalled cycles; Scalability
"Guest editor introduction ppopp 2016, special issue 2 of 2",2017,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056341416&doi=10.1145%2f3108142&partnerID=40&md5=ac042e9bf65910d1d925bbc915022c19,[No abstract available],,
"Guest editor introduction PPoPP 2016, Special Issue 2 of 2",2017,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056316665&doi=10.1145%2f3108141&partnerID=40&md5=27b02aaa4fa9fac9be698f1ef885565f,[No abstract available],,
Gunrock: GPU graph analytics,2017,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054866259&doi=10.1145%2f3108140&partnerID=40&md5=6afd271514777909e9aa5691c7da774c,"For large-scale graph analytics on the GPU, the irregularity of data access and control flow, and the complexity of programming GPUs, have presented two significant challenges to developing a programmable high-performance graph library. ""Gunrock,"" our graph-processing system designed specifically for the GPU, uses a high-level, bulk-synchronous, data-centric abstraction focused on operations on a vertex or edge frontier. Gunrock achieves a balance between performance and expressiveness by coupling high-performance GPU computing primitives and optimization strategies with a high-level programming model that allows programmers to quickly develop new graph primitives with small code size and minimal GPU programming knowledge.We characterize the performance of various optimization strategies and evaluate Gunrock's overall performance on different GPU architectures on a wide range of graph primitives that span from traversalbased algorithms and ranking algorithms, to triangle counting and bipartite-graph-based algorithms. The results show that on a single GPU, Gunrock has on average at least an order of magnitude speedup over Boost and PowerGraph, comparable performance to the fastest GPU hardwired primitives and CPU sharedmemory graph libraries, such as Ligra and Galois, and better performance than any other GPU high-level graph library. © 2017 ACM.",GPU; Graph processing; Runtime framework,Access control; Flow graphs; Graph theory; Graphic methods; Program processors; Bipartite graphs; GPU programming; Graph analytics; Graph processing; High-level programming models; Optimization strategy; Ranking algorithm; Runtime frameworks; Graphics processing unit
Lease/release: Architectural support for scaling contended data structures,2017,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056339489&doi=10.1145%2f3132168&partnerID=40&md5=1451722412a0a9ad5461745b2efc1fce,"High memory contention is generally agreed to be aworst-case scenario for concurrent data structures. There has been a significant amount of research effort spent investigating designs that minimize contention, and several programming techniques have been proposed to mitigate its effects. However, there are currently few architectural mechanisms to allow scaling contended data structures at high thread counts. In this article, we investigate hardware support for scalable contended data structures. We propose Lease/Release, a simple addition to standard directory-based MESI cache coherence protocols, allowing participants to lease memory, at the granularity of cache lines, by delaying coherence messages for a short, bounded period of time. Our analysis shows that Lease/Release can significantly reduce the overheads of contention for both non-blocking (lock-free) and lock-based data structure implementations while ensuring that no deadlocks are introduced.We validate Lease/Release empirically on the Graphite multiprocessor simulator on a range of data structures, including queue, stack, and priority queue implementations, as well as on transactional applications. Results show that Lease/Release consistently improves both throughput and energy usage, by up to 5x, both for lock-free and lock-based data structure designs. © 2017 ACM.",Concurrent data structures; Hardware mechanisms; lock-based data structures; Lock-free data structures,Cache memory; Data structures; Hardware; Locks (fasteners); Multiprocessing systems; Architectural support; Cache coherence protocols; Concurrent data structures; Data structure design; Hardware mechanism; Lock-free data structures; Multiprocessor simulators; Programming technique; Concurrency control
Adding Approximate Counters,2017,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056326181&doi=10.1145%2f3132167&partnerID=40&md5=03ebae8e370ac24450909b30abee6dc9,"We describe a general framework for adding the values of two approximate counters to produce a new approximate counter value whose expected estimated value is equal to the sum of the expected estimated values of the given approximate counters. (To the best of our knowledge, this is the first published description of any algorithm for adding two approximate counters.) We then work out implementation details for five different kinds of approximate counter and provide optimized pseudocode. For three of them, we present proofs that the variance of a counter value produced by adding two counter values in this way is bounded, and in fact is no worse, or not much worse, than the variance of the value of a single counter to which the same total number of increment operations have been applied. Addition of approximate counters is useful in massively parallel divide-And-conquer algorithms that use a distributed representation for large arrays of counters.We describe two machine-learning algorithms for topic modeling that use millions of integer counters and confirm that replacing the integer counters with approximate counters is effective, speeding up a GPU-based implementation by over 65% and a CPU-based implementation by nearly 50%, as well as reducing memory requirements, without degrading their statistical effectiveness. © 2017 ACM.",Approximate counters; Distributed computing; Divide and conquer; Multithreading; Parallel computing; Statistical counters,Distributed computer systems; Learning algorithms; Parallel processing systems; Distributed representation; Divide and conquer; Divide-and-conquer algorithm; Massively parallels; Memory requirements; Multi-threading; Topic Modeling; Two machines; Learning systems
Hybridizing and relaxing dependence tracking for efficient parallel runtime support,2017,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056337753&doi=10.1145%2f3108138&partnerID=40&md5=4700e7572a01e4129d624f8666e45797,"It is notoriously challenging to develop parallel software systems that are both scalable and correct. Runtime support for parallelism-such as multithreaded record and replay, data race detectors, transactional memory, and enforcement of stronger memory models-helps achieve these goals, but existing commodity solutions slow programs substantially to track (i.e., detect or control) an execution's cross-Thread dependencies accurately. Prior work tracks cross-Thread dependencies either ""pessimistically,"" slowing every program access, or ""optimistically,"" allowing for lightweight instrumentation of most accesses but dramatically slowing accesses that are conflicting (i.e., involved in cross-Thread dependencies). This article presents two novel approaches that seek to improve the performance of dependence tracking. Hybrid tracking (HT) hybridizes pessimistic and optimistic tracking by overcoming a fundamental mismatch between these two kinds of tracking. HT uses an adaptive, profile-based policy to make runtime decisions about switching between pessimistic and optimistic tracking. Relaxed tracking (RT) attempts to reduce optimistic tracking's overhead on conflicting accesses by tracking dependencies in a ""relaxed"" way-meaning that not all dependencies are tracked accurately-while still preserving both program semantics and runtime support's correctness. To demonstrate the usefulness and potential of HT and RT, we build runtime support based on the two approaches. Our evaluation shows that both approaches offer performance advantages over existing approaches, but there exist challenges and opportunities for further improvement. HT and RT are distinct solutions to the same problem. It is easier to build runtime support based onHT than on RT, although RT does not incur the overhead of online profiling. This article presents the two approaches together to inform and inspire future designs for efficient parallel runtime support. © 2017 ACM.",Concurrency correctness; Data races; Dependence tracking; Dynamic analysis; Runtime support for parallelism; Synchronization,Computer networks; Dynamic analysis; Software engineering; Synchronization; Concurrency correctness; Data races; Data-race detectors; Parallel software; Program semantics; Record-and-replay; Runtime support; Transactional memory; Semantics
Efficient data streaming multiway aggregation through concurrent algorithmic designs and new abstract data types,2017,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048385319&doi=10.1145%2f3131272&partnerID=40&md5=e69c583f86a2b7552ba251396e7d0117,"Data streaming relies on continuous queries to process unbounded streams of data in a real-Time fashion. It is commonly demanding in computation capacity, given that the relevant applications involve very large volumes of data. Data structures act as articulation points and maintain the state of data streaming operators, potentially supporting high parallelism and balancing the work among them. Prompted by this fact, in this work we study and analyze parallelization needs of these articulation points, focusing on the problem of streaming multiway aggregation, where large data volumes are received from multiple input streams. The analysis of the parallelization needs, as well as of the use and limitations of existing aggregate designs and their data structures, leads us to identify needs for appropriate shared objects that can achieve low-latency and high-Throughput multiway aggregation. We present the requirements of such objects as abstract data types and we provide efficient lock-free linearizable algorithmic implementations of them, along with new multiway aggregate algorithmic designs that leverage them, supporting both deterministic order-sensitive and order-insensitive aggregate functions. Furthermore, we point out future directions that open through these contributions. The article includes an extensive experimental study, based on a variety of continuous aggregation queries on two large datasets extracted from SoundCloud, a music social network, and from a Smart Grid network. In all the experiments, the proposed data structures and the enhanced aggregate operators improved the processing performance significantly, up to one order of magnitude, in terms of both throughput and latency, over the commonly used techniques based on queues. © 2017 ACM.",Data streaming; data structures; lock-free synchronization,Abstracting; Aggregates; Data reduction; Data structures; Locks (fasteners); Aggregation queries; Articulation points; Computation capacity; Data streaming; Large data volumes; Lock-free synchronization; Processing performance; Smart grid networks; Abstract data types
Autogen: Automatic discovery of efficient recursive divide-&-conquer algorithms for solving dynamic programming problems,2017,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056311970&doi=10.1145%2f3125632&partnerID=40&md5=e50558da3f275a7c8d4f1797533baa3e,"We present Autogen-an algorithm that for a wide class of dynamic programming (DP) problems automatically discovers highly efficient cache-oblivious parallel recursive divide-And-conquer algorithms from inefficient iterative descriptions of DP recurrences. Autogen analyzes the set of DP table locations accessed by the iterative algorithm when run on a DP table of small size and automatically identifies a recursive access pattern and a corresponding provably correct recursive algorithm for solving the DP recurrence.We use Autogen to autodiscover efficient algorithms for several well-known problems. Our experimental results show that several autodiscovered algorithms significantly outperform parallel looping and tiled loop-based algorithms. Also, these algorithms are less sensitive to fluctuations of memory and bandwidth compared with their looping counterparts, and their running times and energy profiles remain relatively more stable. To the best of our knowledge, Autogen is the first algorithm that can automatically discover new nontrivial divide-And-conquer algorithms. © 2017 ACM.",Autogen; Automatic discovery; Cache-efficient; Cache-oblivious; Divideand-conquer; Dynamic programming; Energy-efficient; Parallel; Recursive,Cache memory; Energy efficiency; Iterative methods; Problem solving; Autogen; Automatic discovery; Cache-efficient; Cache-oblivious; Divide and conquer; Energy efficient; Parallel; Recursive; Dynamic programming
Domlock: A new multi-granularity locking technique for hierarchies,2017,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052953014&doi=10.1145%2f3127584&partnerID=40&md5=c9a2a35a5eadcb6e94e27ab2610af1e6,"We present efficient locking mechanisms for hierarchical data structures. Several applications work on an abstract hierarchy of objects, and a parallel execution on this hierarchy necessitates synchronization across workers operating on different parts of the hierarchy. Existing synchronization mechanisms are too coarse, too inefficient, or too ad hoc, resulting in reduced or unpredictable amount of concurrency.We propose a new locking approach based on the structural properties of the underlying hierarchy.We show that the developed techniques are efficient evenwhen the hierarchy is an arbitrary graph. Theoretically,we present our approach as a locking-cost-minimizing instance of a generic algebraic model of synchronization for hierarchies. Using STMBench7, we illustrate considerable reduction in the locking cost, resulting in an average throughput improvement of 42%. © 2017 ACM.",Dominators; Graphs; Hierarchical data structure; Locking; Object graphs; Synchronization; Trees,Data structures; Synchronization; Trees (mathematics); Dominators; Graphs; Hierarchical data structure; Locking; Object graphs; Trees; Locks (fasteners)
GPU multisplit: An extended study of a parallel algorithm,2017,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054831524&doi=10.1145%2f3108139&partnerID=40&md5=6c42f27927dfac0ad0429c2e570478df,"Multisplit is a broadly useful parallel primitive that permutes its input data into contiguous buckets or bins, where the function that categorizes an element into a bucket is provided by the programmer. Due to the lack of an efficient multisplit on Graphics Processing Units (GPUs), programmers often choose to implement multisplit with a sort. One way is to first generate an auxiliary array of bucket IDs and then sort input data based on it. In case smaller indexed buckets possess smaller valued keys, another way for multisplit is to directly sort input data. Both methods are inefficient and require more work than necessary: The former requires more expensive data movements while the latter spends unnecessary effort in sorting elements within each bucket. In this work, we provide a parallel model and multiple implementations for the multisplit problem. Our principal focus is multisplit for a small (up to 256) number of buckets. We use warp-synchronous programming models and emphasize warpwide communications to avoid branch divergence and reduce memory usage. We also hierarchically reorder input elements to achieve better coalescing of global memory accesses. On a GeForce GTX 1080 GPU, we can reach a peak throughput of 18.93Gkeys/s (or 11.68Gpairs/s) for a key-only (or key-value) multisplit. Finally, we demonstrate how multisplit can be used as a building block for radix sort. In our multisplit-based sort implementation, we achieve comparable performance to the fastest GPU sort routines, sorting 32-bit keys (and key-value pairs) with a throughput of 3.0Gkeys/s (and 2.1Gpair/s). © 2017 ACM.",Ballot; Bucketing; Graphics processing unit (GPU); Histogram; Multisplit; Radix sort; Shuffle; Warp-synchronous programming,Computer graphics; Computer graphics equipment; Flocculation; Image coding; Input output programs; Program processors; Sorting; Ballot; Bucketing; Graphics Processing Unit (GPU); Histogram; Multisplit; Radix sort; Shuffle; Synchronous programming; Graphics processing unit
Resource oblivious sorting on multicores,2017,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054897853&doi=10.1145%2f3040221&partnerID=40&md5=49dd8c059ccf778cb962cf108999b24a,"We present a deterministic sorting algorithm, Sample, Partition, and Merge Sort (SPMS), that interleaves the partitioning of a sample sort with merging. Sequentially, it sorts n elements in O(nlog n) time cacheobliviously with an optimal number of cache misses. The parallel complexity (or critical path length) of the algorithm is O(log nlog log n), which improves on previous bounds for deterministic sample sort. The algorithm also has low false sharing costs. When scheduled by a work-stealing scheduler in a multicore computing environment with a global shared memory and p cores, each having a cache of size M organized in blocks of size B, the costs of the additional cache misses and false sharing misses due to this parallel execution are bounded by the cost of O(S M/B) and O(S B) cache misses, respectively, where S is the number of steals performed during the execution. Finally, SPMS is resource oblivious in that the dependence on machine parameters appear only in the analysis of its performance and not within the algorithm itself. © 2017 ACM.",Cache oblivious; merge sort; Sample sort; Sorting,Software engineering; Sorting; Cache-oblivious; Critical path lengths; Multi-core computing; Oblivious sorting; Parallel complexity; Parallel executions; Sample sort; Sorting algorithm; Computer networks
Introduction to the Special Section on PPoPP'15,2017,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056461092&doi=10.1145%2f3040224&partnerID=40&md5=a5394abcfd856c0e9563d0240de7b84f,[No abstract available],,
Generality and Speed in Nonblocking Dual Containers,2017,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056487728&doi=10.1145%2f3040220&partnerID=40&md5=4d47253a03a382b1237b00926583c11c,"Nonblocking dual data structures extend traditional notions of nonblocking progress to accommodate partial methods, both by bounding the number of steps that a thread can execute after its preconditions have been satisfied and by ensuring that a waiting thread performs no remote memory accesses that could interfere with the execution of other threads. A nonblocking dual container, in particular, is designed to hold either data or requests. An insert operation either adds data to the container or removes and satisfies a request; a remove operation either takes data out of the container or inserts a request. We present the first general-purpose construction for nonblocking dual containers, allowing any nonblocking container for data to be paired with almost any nonblocking container for requests. We also present new custom algorithms, based on the LCRQ of Morrison and Afek, that outperform the fastest previously known dual containers by factors of four to six. © 2017 ACM.",Dual containers; Nonblocking data structures; Synchronization,Data structures; Synchronization; Non-blocking; Nonblocking data structures; Remote memory access; Containers
A Library for Portable and Composable Data Locality Optimizations for NUMA Systems,2017,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050214484&doi=10.1145%2f3040222&partnerID=40&md5=296d59157976e7c158e6474bf5c63a97,"Many recent multiprocessor systems are realized with a nonuniform memory architecture (NUMA) and accesses to remote memory locations take more time than local memory accesses. Optimizing NUMA memory system performance is difficult and costly for three principal reasons: (1) Today's programming languages/ libraries have no explicit support for NUMA systems, (2) NUMA optimizations are not portable, and (3) optimizations are not composable (i.e., they can become ineffective or worsen performance in environments that support composable parallel software). This article presents TBB-NUMA, a parallel programming library based on Intel Threading Building Blocks (TBB) that supports portable and composable NUMA-Aware programming. TBB-NUMA provides a model of task affinity that captures a programmer's insights on mapping tasks to resources. NUMAawareness affects all layers of the library (i.e., resource management, task scheduling, and high-level parallel algorithm templates) and requires close coupling between all these layers. Optimizations implemented with TBB-NUMA (for a set of standard benchmark programs) result in up to 44% performance improvement over standard TBB. But more important, optimized programs are portable across different NUMA architectures and preserve data locality also when composed with other parallel computations sharing the same resource management layer. © 2017 ACM.",Data placement; NUMA; Scheduling,Benchmarking; Memory architecture; Natural resources management; Parallel programming; Resource allocation; Scheduling; Data locality optimization; Data placement; Multi processor systems; Non-uniform memory architecture; NUMA; Performance improvements; Remote memory location; Threading building blocks; Information management
Automatic scalable atomicity via semantic locking,2017,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051146144&doi=10.1145%2f3040223&partnerID=40&md5=e8a9a0a2ce9b0f35aead96fc6889cf4b,"In this article, we consider concurrent programs in which the shared state consists of instances of linearizable abstract data types (ADTs). We present an automated approach to concurrency control that addresses a common need: The need to atomically execute a code fragment, which may contain multiple ADT operations on multiple ADT instances. We present a synthesis algorithm that automatically enforces atomicity of given code fragments (in a client program) by inserting pessimistic synchronization that guarantees atomicity and deadlock-freedom (without using any rollback mechanism). Our algorithm takes a commutativity specification as an extra input. This specification indicates for every pair of ADT operations the conditions under which the operations commute. Our algorithm enables greater parallelism by permitting commuting operations to execute concurrently. We have implemented the synthesis algorithm in a Java compiler and applied it to several Java programs. Our results show that our approach produces efficient and scalable synchronization. © 2017 ACM.",Abstract data types; Automatic locking; Concurrency; Semantics; Transactions,Abstract data types; Abstracting; Java programming language; Locks (fasteners); Program compilers; Semantics; Specifications; Automated approach; Automatic locking; Client programs; Concurrency; Concurrent program; Deadlock freedom; Synthesis algorithms; Transactions; Concurrency control
C-Stream: A co-routine-based elastic stream processing engine,2017,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054894126&doi=10.1145%2f3184120&partnerID=40&md5=6e5feaabd58d82a437c213ae9b4afc96,"Stream processing is a computational paradigm for on-the-fly processing of live data. This paradigm lends itself to implementations that can provide high throughput and low latency by taking advantage of various forms of parallelism that are naturally captured by the stream processing model of computation, such as pipeline, task, and data parallelism. In this article, we describe the design and implementation of C-Stream, which is an elastic stream processing engine. C-Stream encompasses three unique properties. First, in contrast to the widely adopted event-based interface for developing streaming operators, C-Stream provides an interface wherein each operator has its own driver loop and relies on data availability application programming interfaces (APIs) to decide when to perform its computations. This self-control-based model significantly simplifies the development of operators that require multiport synchronization. Second, C-Stream contains a dynamic scheduler that manages the multithreaded execution of the operators. The scheduler, which is customizable via plug-ins, enables the execution of the operators as co-routines, using any number of threads. The base scheduler implements back-pressure, provides data availability APIs, and manages preemption and termination handling. Last, C-Stream varies the degree of parallelism to resolve bottlenecks by both dynamically changing the number of threads used to execute an application and adjusting the number of replicas of data-parallel operators.We provide an experimental evaluation of C-Stream. The results show that C-Stream is scalable, highly customizable, and can resolve bottlenecks by dynamically adjusting the level of data parallelism used. © 2017 ACM.",C-Stream; Elastic stream processing engine,Application programming interfaces (API); Data handling; Engines; Pipeline processing systems; Scheduling; Computational paradigm; Degree of parallelism; Design and implementations; Dynamic schedulers; Experimental evaluation; Multithreaded executions; Stream processing; Stream processing engines; C (programming language)
Partitioning models for scaling parallel sparse matrix-matrix multiplication,2017,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054889020&doi=10.1145%2f3155292&partnerID=40&md5=75b17b9f6cf4886aa0e96d6af886c112,"We investigate outer-product-parallel, inner-product-parallel, and row-by-row-product-parallel formulations of sparse matrix-matrix multiplication (SpGEMM) on distributed memory architectures. For each of these three formulations, we propose a hypergraph model and a bipartite graph model for distributing SpGEMM computations based on one-dimensional (1D) partitioning of input matrices. We also propose a communication hypergraph model for each formulation for distributing communication operations. The computational graph and hypergraph models adopted in the first phase aim at minimizing the total message volume and balancing the computational loads of processors, whereas the communication hypergraph models adopted in the second phase aim at minimizing the total message count and balancing the message volume loads of processors. That is, the computational partitioning models reduce the bandwidth cost and the communication hypergraph models reduce the latency cost. Our extensive parallel experiments on up to 2048 processors for a wide range of realistic SpGEMM instances show that although the outer-product-parallel formulation scales better, the row-by-row-product-parallel formulation is more viable due to its significantly lower partitioning overhead and competitive scalability. For computational partitioning models, our experimental findings indicate that the proposed bipartite graph models are attractive alternatives to their hypergraph counterparts because of their lower partitioning overhead. Finally, we show that by reducing the latency cost besides the bandwidth cost through using the communication hypergraph models, the parallel SpGEMM time can be further improved up to 32%. © 2017 ACM.",Bandwidth; Communication cost; Graph partitioning; Hypergraph partitioning; Latency; Sparse matrix-matrix multiplication; SpGEMM,Bandwidth; Cost reduction; Matrix algebra; Memory architecture; One dimensional; Parallel processing systems; Communication cost; Graph Partitioning; Hypergraph partitioning; Latency; Sparse matrix-matrix multiplications; SpGEMM; Graph theory
Multidimensional intratile parallelization for memory-starved stencil computations,2017,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053396616&doi=10.1145%2f3155290&partnerID=40&md5=9b9e582288112b6f0e2d26aa6e27cd6b,"Optimizing the performance of stencil algorithms has been the subject of intense research over the last two decades. Since many stencil schemes have low arithmetic intensity, most optimizations focus on increasing the temporal data access locality, thus reducing the data traffic through the main memory interface with the ultimate goal of decoupling from this bottleneck. There are, however, only a few approaches that explicitly leverage the shared cache feature of modern multicore chips. If every thread works on its private, separate cache block, the available cache space can become too small, and sufficient temporal locality may not be achieved. We propose a flexible multidimensional intratile parallelization method for stencil algorithms on multicore CPUs with a shared outer-level cache. This method leads to a significant reduction in the required cache space without adverse effects from hardware prefetching or TLB shortage. Our Girih framework includes an autotuner to select optimal parameter configurations on the target hardware. We conduct performance experiments on two contemporary Intel processors and compare with the state-of-the-art stencil frameworks Pluto and Pochoir, using four corner-case stencil schemes and a wide range of problem sizes. Girih shows substantial performance advantages and best arithmetic intensity at almost all problem sizes, especially on low-intensity stencils with variable coefficients. We study in detail the performance behavior at varying grid sizes using phenomenological performance modeling. Our analysis of energy consumption reveals that our method can save energy through reduced DRAM bandwidth usage even at a marginal performance gain. It is thus well suited for future architectures that will be strongly challenged by the cost of data movement, be it in terms of performance or energy consumption. © 2017 ACM.",Media access control; Multi-channel; Radio interference; Time synchronization; Wireless sensor networks,Dynamic random access storage; Energy utilization; Hardware; Medium access control; Program processors; Radio interference; Wireless sensor networks; Future architectures; Media access control; Multi channel; Performance experiment; Performance Model; Stencil computations; Time synchronization; Variable coefficients; Data reduction
Transparently space sharing a multicore among multiple processes,2016,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041443477&doi=10.1145%2f3001910&partnerID=40&md5=4c608aa2577f4262689724235905cd5a,"As hardware becomes increasingly parallel and the availability of scalable parallel software improves, the problem of managing multiple multithreaded applications (processes) becomes important. Malleable processes, which can vary the number of threads used as they run, enable sophisticated and flexible resource management. Although many existing applications parallelized for SMPs with parallel runtimes are in fact already malleable, deployed runtime environments provide no interface nor any strategy for intelligently allocating hardware threads or even preventing oversubscription. Prior research methods either depend on profiling applications ahead of time tomake good decisions about allocations or do not account for process efficiency at all, leading to poor performance. None of these prior methods have been adapted widely in practice. This article presents the Scheduling and Allocation with Feedback (SCAF) system: A drop-in runtime solution that supports existing malleable applications in making intelligent allocation decisions based on observed efficiency without any changes to semantics, program modification, offline profiling, or even recompilation. Our existing implementation can control most unmodified OpenMP applications. Other malleable threading libraries can also easily be supported with small modifications without requiring application modification or recompilation. In this work, we present the SCAF daemon and a SCAF-Aware port of the GNU OpenMP runtime. We present a new technique for estimating process efficiency purely at runtime using available hardware counters and demonstrate its effectiveness in aiding allocation decisions. We evaluated SCAF using NAS NPB parallel benchmarks on five commodity parallel platforms, enumerating architectural features and their effects on our scheme. We measured the benefit of SCAF in terms of sum of speedups improvement (a common metric for multiprogrammed environments) when running all benchmark pairs concurrently compared to equipartitioning-the best existing competing scheme in the literature. We found that SCAF improves on equipartitioning on four out of five machines, showing a mean improvement factor in sum of speedups of 1.04 to 1.11x for benchmark pairs, depending on the machine, and 1.09x on average. Since we are not aware of any widely available tool for equipartitioning, we also compare SCAF against multiprogramming using unmodified OpenMP, which is the only environment available to end users today. SCAF improves on the unmodified OpenMP runtimes for all five machines, with a mean improvement of 1.08 to 2.07x, depending on the machine, and 1.59x on average. © 2016 ACM.",Multithreaded programming; Oversubscription; Parallelism; Resource management; User-level scheduling,Application programming interfaces (API); Efficiency; Multiprogramming; Natural resources management; Open source software; Resource allocation; Scheduling; Semantics; Multithreaded programming; Oversubscription; Parallelism; Resource management; User-level scheduling; Application programs
Hypergraph partitioning for sparse matrix-matrix multiplication,2016,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041192786&doi=10.1145%2f3015144&partnerID=40&md5=ee5f96f6214410ddfe6800e8d83c2ff4,"We propose a fine-grained hypergraph model for sparse matrix-matrix multiplication (SpGEMM), a key computational kernel in scientific computing and data analysis whose performance is often communication bound. This model correctly describes both the interprocessor communication volume along a critical path in a parallel computation and also the volume of data moving through the memory hierarchy in a sequential computation. We show that identifying a communication-optimal algorithm for particular input matrices is equivalent to solving a hypergraph partitioning problem. Our approach is nonzero structure dependent, meaning that we seek the best algorithm for the given input matrices. In addition to our three-dimensional fine-grained model, we also propose coarse-grained one-dimensional and two-dimensional models that correspond to simpler SpGEMM algorithms. We explore the relations between our models theoretically, and we study their performance experimentally in the context of three applications that use SpGEMM as a key computation. For each application, we find that at least one coarse-grained model is as communication efficient as the fine-grained model. We also observe that different applications have affinities for different algorithms. Our results demonstrate that hypergraphs are an accurate model for reasoning about the communication costs of SpGEMM as well as a practical tool for exploring the SpGEMM algorithm design space. © 2016 ACM.",Hypergraph partitioning; Sparse matrix-matrix multiplication,Graph theory; One dimensional; Three dimensional computer graphics; Coarse grained models; Communication optimal algorithms; Computational kernels; Hypergraph partitioning; Inter processor communication; Sequential computations; Sparse matrix-matrix multiplications; Two dimensional model; Matrix algebra
Adaptive optimization modeling of preconditioned conjugate gradient on Multi-GPUs,2016,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026260269&doi=10.1145%2f2990849&partnerID=40&md5=8e32528424932f872cd896abcad02cb5,"The preconditioned conjugate gradient (PCG) algorithm is a well-known iterative method for solving sparse linear systems in scientific computations. GPU-Accelerated PCG algorithms for large-sized problems have attracted considerable attention recently. However, on a specific multi-GPU platform, producing a highly parallel PCG implementation for any large-sized problem requires significant time because several manual steps are involved in adjusting the related parameters and selecting an appropriate storage format for the matrix block that is assigned to each GPU. This motivates us to propose adaptive optimization modeling of PCG on multi-GPUs, which mainly involves the following parts: (1) an optimization multi-GPU parallel framework of PCG and (2) the profile-based optimization modeling for each one of the main components of the PCG algorithm, including vector operation, inner product, and sparse matrix-vector multiplication (SpMV). Our model does not construct a new storage format or kernel but automatically and rapidly generates an optimal parallel PCG algorithm for any problem on a specific multi-GPU platform by integrating existing storage formats and kernels. We take a vector operation kernel, an inner-product kernel, and five popular SpMV kernels for an example to present the idea of constructing the model. Given that our model is general, independent of the problems, and dependent on the resources of devices, this model is constructed only once for each type of GPU. The experiments validate the high efficiency of our proposed model. © 2016 ACM.",CUDA; Multiple GPUs; Optimization model; Preconditioned conjugate gradient; Sparse matrixvector multiplication,Linear systems; Matrix algebra; Optimization; Program processors; CUDA; Multiple GPUs; Optimization modeling; Preconditioned conjugate gradient; Sparse matrix-vector multiplication; Conjugate gradient method
Damaris: Addressing performance variability in data management for post-petascale simulations,2016,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048537754&doi=10.1145%2f2987371&partnerID=40&md5=ed3ba150d8c9446d399eb8d1d372cd23,"With exascale computing on the horizon, reducing performance variability in data management tasks (storage, visualization, analysis, etc.) is becoming a key challenge in sustaining high performance. This variability significantly impacts the overall application performance at scale and its predictability over time. In this article, we present Damaris, a system that leverages dedicated cores in multicore nodes to offload data management tasks, including I/O, data compression, scheduling of datamovements, in situ analysis, and visualization. We evaluate Damaris with the CM1 atmospheric simulation and the Nek5000 computational fluid dynamic simulation on four platforms, including NICS's Kraken and NCSA's Blue Waters. Our results show that (1) Damaris fully hides the I/O variability as well as all I/O-related costs, thus making simulation performance predictable; (2) it increases the sustained write throughput by a factor of up to 15 compared with standard I/O approaches; (3) it allows almost perfect scalability of the simulation up to over 9,000 cores, as opposed to state-of-The-Art approaches that fail to scale; and (4) it enables a seamless connection to the VisIt visualization software to perform in situ analysis and visualization in a way that impacts neither the performance of the simulation nor its variability. In addition, we extended our implementation of Damaris to also support the use of dedicated nodes and conducted a thorough comparison of the two approaches-dedicated cores and dedicated nodes-for I/O tasks with the aforementioned applications. © 2016 ACM.",Damaris; Dedicated cores; Dedicated nodes; Exascale computing; I/O; In situ visualization,Computational fluid dynamics; Computer software; Data visualization; Digital storage; Scheduling; Visualization; Damaris; Dedicated cores; Dedicated nodes; Exascale computing; Situ visualization; Information management
"Sixteen heuristics for joint optimization of performance, energy, and temperature in allocating tasks to multi-cores",2016,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053624524&doi=10.1145%2f2948973&partnerID=40&md5=bc56c119af1a8fbaf0462d242efadb5f,"Three-way joint optimization of performance (P), energy (E), and temperature (T) in scheduling parallel tasks to multiple cores poses a challenge that is staggering in its computational complexity. The goal of the PET optimized scheduling (PETOS) problem is to minimize three quantities: the completion time of a task graph, the total energy consumption, and the peak temperature of the system. Algorithms based on conventional multi-objective optimization techniques can be designed for solving the PETOS problem. But their execution times are exceedingly high and hence their applicability is restricted merely to problems of modest size. Exacerbating the problem is the solution space that is typically a Pareto front since no single solution can be strictly best along all three objectives. Thus, not only is the absolute quality of the solutions important but ""the spread of the solutions"" along each objective and the distribution of solutions within the generated tradeoff front are also desired. A natural alternative is to design efficient heuristic algorithms that can generate good solutions as well as good spreads - note that most of the prior work in energy-efficient task allocation is predominantly single- or dual-objective oriented. Given a directed acyclic graph (DAG) representing a parallel program, a heuristic encompasses policies as to what tasks should go to what cores and at what frequency should that core operate. Various policies, such as greedy, iterative, and probabilistic, can be employed. However, the choice and usage of these policies can influence a heuristic towards a particular objective and can also profoundly impact its performance. This article proposes 16 heuristics that utilize various methods for task-to-core allocation and frequency selection. This article also presents a methodical classification scheme which not only categorizes the proposed heuristics but can also accommodate additional heuristics. Extensive simulation experiments compare these algorithms while shedding light on their strengths and tradeoffs. © 2016 ACM.",Energy-efficient computing; Multi-core systems; Multi-objective optimization; Scheduling; Task assignment; Task graphs; Thermal-efficient computing,Directed graphs; Embedded systems; Energy utilization; Heuristic algorithms; Heuristic methods; Iterative methods; Multiobjective optimization; Problem solving; Scheduling; Energy efficient computing; Multi-core systems; Task assignment; Task graph; Thermal-efficient computing; Energy efficiency
Compiling affine loop nests for a dynamic scheduling runtime on shared and distributed memory,2016,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055515412&doi=10.1145%2f2948975&partnerID=40&md5=d547436a2e10d26f07ee62985c14ca59,"Current de-facto parallel programming models like OpenMP and MPI make it difficult to extract task-level dataflow parallelism as opposed to bulk-synchronous parallelism. Task parallel approaches that use point-topoint synchronization between dependent tasks in conjunction with dynamic scheduling dataflow runtimes are thus becoming attractive. Although good performance can be extracted for both shared and distributed memory using these approaches, there is little compiler support for them. In this article, we describe the design of compiler-runtime interaction to automatically extract coarsegrained dataflow parallelism in affine loop nests for both shared and distributed-memory architectures. We use techniques from the polyhedral compiler framework to extract tasks and generate components of the runtime that are used to dynamically schedule the generated tasks. The runtime includes a distributed decentralized scheduler that dynamically schedules tasks on a node. The schedulers on different nodes cooperate with each other through asynchronous point-to-point communication, and all of this is achieved by code automatically generated by the compiler. On a set of six representative affine loop nest benchmarks, while running on 32 nodes with 8 threads each, our compiler-assisted runtime yields a geometric mean speedup of 143.6× (70.3× to 474.7×) over the sequential version and a geometric mean speedup of 1.64× (1.04× to 2.42×) over the state-of-the-art automatic parallelization approach that uses bulk synchronization. We also compare our system with past work that addresses some of these challenges on shared memory, and an emerging runtime (Intel Concurrent Collections) that demands higher programmer input and effort in parallelizing. To the best of our knowledge, ours is also the first automatic scheme that allows for dynamic scheduling of affine loop nests on a cluster of multicores. © 2016 ACM.",Compiler-runtime framework; Dataflow runtime; Distributed-memory architectures; Dynamic scheduling; Parallelization; Polyhedral model; Task parallelism,Application programming interfaces (API); Data flow analysis; Parallel programming; Program compilers; Scheduling; Distributed memory architecture; Dynamic scheduling; Parallelizations; Polyhedral modeling; Runtime frameworks; Runtimes; Task parallelism; Memory architecture
Selecting multiple order statistics with a graphics processing unit,2016,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056297062&doi=10.1145%2f2948974&partnerID=40&md5=e3909acce2cfee7e3b9cdad45549b4bc,"Extracting a set of multiple order statistics from a huge data set provides important information about the distribution of the values in the full set of data. This article introduces an algorithm, bucketMultiSelect, for simultaneously selecting multiple order statistics with a graphics processing unit (GPU). Typically, when a large set of order statistics is desired, the vector is sorted.When the sorted version of the vector is not needed, bucketMultiSelect significantly reduces computation time by eliminating a large portion of the unnecessary operations involved in sorting. For large vectors, bucketMultiSelect returns thousands of order statistics in less time than sorting the vector while typically using less memory. For vectors containing 228 values of type double, bucketMultiSelect selects the 101 percentile order statistics in less than 95ms and is more than 8× faster than sorting the vector with a GPU optimized merge sort. © 2016 ACM.",CUDA; GPGPU; Graphics processing units; Multi-core; Order statistics; Selection,Computer graphics; Computer graphics equipment; Image coding; Program processors; Sorting; Statistics; Vectors; CUDA; GPGPU; Multi core; Order statistics; Selection; Graphics processing unit
Identifying the root causes of wait states in large-scale parallel applications,2016,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052762376&doi=10.1145%2f2934661&partnerID=40&md5=68a1231e61c9afc62e218cac847de0f8,"Driven by growing application requirements and accelerated by current trends in microprocessor design, the number of processor cores on modern supercomputers is increasing from generation to generation. However, load or communication imbalance prevents many codes from taking advantage of the available parallelism, as delays of single processes may spread wait states across the entire machine. Moreover, when employing complex point-to-point communication patterns, wait states may propagate along far-reaching cause-effect chains that are hard to track manually and that complicate an assessment of the actual costs of an imbalance. Building on earlier work by Meira, Jr., et al., we present a scalable approach that identifies program wait states and attributes their costs in terms of resource waste to their original cause. By replaying event traces in parallel both forward and backward, we can identify the processes and call paths responsible for the most severe imbalances, even for runs with hundreds of thousands of processes. © 2016 ACM.",Event tracing; Load imbalance; MPI; OpenMP; Performance analysis; Root-cause analysis,Application programming interfaces (API); Supercomputers; Event tracing; Load imbalance; OpenMP; Performance analysis; Root cause analysis; Parallel processing systems
Assessing general-purpose algorithms to cope with fail-stop and silent errors,2016,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032645910&doi=10.1145%2f2897189&partnerID=40&md5=6a6579ed837c22a855c0b2efeb965dcb,"In this article, we combine the traditional checkpointing and rollback recovery strategies with verification mechanisms to cope with both fail-stop and silent errors. The objective is to minimize makespan and/or energy consumption. For divisible load applications, we use first-order approximations to find the optimal checkpointing period to minimize execution time, with an additional verification mechanism to detect silent errors before each checkpoint, hence extending the classical formula by Young and Daly for fail-stop errors only. We further extend the approach to include intermediate verifications, and to consider a bicriteria problem involving both time and energy (linear combination of execution time and energy consumption). Then, we focus on application workflows whose dependence graph is a linear chain of tasks. Here, we determine the optimal checkpointing and verification locations, with or without intermediate verifications, for the bicriteria problem. Rather than using a single speed during the whole execution, we further introduce a new execution scenario, which allows for changing the execution speed via Dynamic Voltage and Frequency Scaling (DVFS). In this latter scenario, we determine the optimal checkpointing and verification locations, as well as the optimal speed pairs for each task segment between any two consecutive checkpoints. Finally, we conduct an extensive set of simulations to support the theoretical study, and to assess the performance of each algorithm, showing that the best overall performance is achieved under the most flexible scenario using intermediate verifications and different speeds. © 2016 ACM.",Checkpoint; Fail-stop error; Failure; HPC; Resilience; Silent data corruption; Silent error; Verification,Computer system recovery; Dynamic frequency scaling; Energy utilization; Verification; Voltage scaling; Checkpoint; Divisible load applications; Dynamic voltage and frequency scaling; First-order approximations; Optimal checkpointing; Resilience; Roll-back recoveries; Silent data corruptions; Errors
Simple parallel and distributed algorithms for spectral graph sparsification,2016,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045559580&doi=10.1145%2f2948062&partnerID=40&md5=7eda8eef591b0faf4d0b5f27cab6e4b6,"We describe simple algorithms for spectral graph sparsification, based on iterative computations of weighted spanners and sampling. Leveraging the algorithms of Baswana and Sen for computing spanners, we obtain the first distributed spectral sparsification algorithm in the CONGEST model.We also obtain a parallel algorithm with improved work and time guarantees, as well as other natural distributed implementations. Combining this algorithm with the parallel framework of Peng and Spielman for solving symmetric diagonally dominant linear systems, we get a parallel solver that is significantlymore efficient in terms of the total work. © 2016 ACM.",SDD linear systems; Sparsest cut; Spectral sparsification,Linear systems; Diagonally dominant linear systems; Distributed implementation; Graph sparsification; Iterative computation; Parallel and distributed algorithms; Parallel framework; Sparsest cut; Sparsification; Iterative methods
Locality-based network creation games,2016,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056772584&doi=10.1145%2f2612669.261280&partnerID=40&md5=ef1ebb6f702be30efe396399b62dffba,"Network creation games have been extensively studied, both by economists and computer scientists, due to their versatility in modeling individual-based community formation processes. These processes, in turn, are the theoretical counterpart of several economics, social, and computational applications on the Internet. In their several variants, these games model the tension of a player between the player's two antagonistic goals: to be as close as possible to the other players and to activate a cheapest possible set of links. However, the generally adopted assumption is that players have a common and complete information about the ongoing network, which is quite unrealistic in practice. In this article, we consider a more compelling scenario in which players have only limited information about the network in whicy they are embedded. More precisely, we explore the game-theoretic and computational implications of assuming that players have a complete knowledge of the network structure only up to a given radius k, which is one of the most qualified local-knowledge models used in distributed computing. In this respect, we define a suitable equilibrium concept, and we provide a comprehensive set of upper and lower bounds to the price of anarchy for the entire range of values of k and for the two classic variants of the game, namely, those in which a player's cost-besides the activation cost of the owned links-depends on the maximum/sum of all distances to the other nodes in the network, respectively. These bounds are assessed through an extensive set of experiments. © 2016 ACM.",Game theory; Local knowledge; Network creation games; Price of anarchy,Artificial intelligence; Computation theory; Computer games; Economics; Game theory; Complete information; Computational applications; Computer scientists; Limited information; Local knowledge; Network creation; Price of anarchy; Upper and lower bounds; Distributed computer systems
Executing dynamic data-graph computations deterministically using chromatic scheduling,2016,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053484880&doi=10.1145%2f2896850&partnerID=40&md5=09ac5885185dbfcfa8057a0eaa2b247d,"A data-graph computation-popularized by such programming systems as Galois, Pregel, GraphLab, PowerGraph, and GraphChi-is an algorithm that performs local updates on the vertices of a graph. During each round of a data-graph computation, an update function atomically modifies the data associated with a vertex as a function of the vertex's prior data and that of adjacent vertices. A dynamic data-graph computation updates only an active subset of the vertices during a round, and those updates determine the set of active vertices for the next round. This article introduces PRISM, a chromatic-scheduling algorithm for executing dynamic data-graph computations. PRISM uses a vertex coloring of the graph to coordinate updates performed in a round, precluding the need for mutual-exclusion locks or other nondeterministic data synchronization. A multibag data structure is used by PRISM to maintain a dynamic set of active vertices as an unordered set partitioned by color. We analyze PRISM using work-span analysis. Let G = (V, E) be a degree-ϵ graph colored with ? colors, and suppose that Q ? V is the set of active vertices in a round. Define size(Q) = |Q| +ϵv?Q deg(v), which is proportional to the space required to store the vertices of Q using a sparse-graph layout. We show that a P-processor execution of PRISM performs updates in Q using O(?(lg(Q/?) + lgϵ) + lg P) span and (size(Q) + P) work. These theoretical guarantees are matched by good empirical performance. To isolate the effect of the scheduling algorithm on performance, we modified GraphLab to incorporate PRISM and studied seven application benchmarks on a 12-core multicore machine. PRISM executes the benchmarks 1.2 to 2.1 times faster than GraphLab's nondeterministic lock-based scheduler while providing deterministic behavior. This article also presents PRISM-R, a variation of PRISM that executes dynamic data-graph computations deterministically even when updates modify global variables with associative operations. PRISM-R satisfies the same theoretical bounds as PRISM, but its implementation is more involved, incorporating a multivector data structure to maintain a deterministically ordered set of vertices partitioned by color. Despite its additional complexity, PRISM-R is only marginally slower than PRISM. On the seven application benchmarks studied, PRISM-R incurs a 7% geometric mean overhead relative to PRISM. © 2016 ACM.",Chromatic scheduling; Data-graph computations; Determinism; Multicore; Multithreading; Parallel programming; Scheduling; Work stealing,Benchmarking; Color; Data structures; Locks (fasteners); Multicore programming; Multitasking; Parallel programming; Prisms; Scheduling; Scheduling algorithms; Chromatic scheduling; Data graph; Determinism; Multi core; Multi-threading; Work-stealing; Graph theory
On computing maximal independent sets of hypergraphs in parallel,2016,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054021584&doi=10.1145%2f2938436&partnerID=40&md5=8311fad5809956edeb70fcd14d387622,"Whether or not the problem of finding maximal independent sets (MIS) in hypergraphs is in (R)NC is one of the fundamental problems in the theory of parallel computing. Essentially, the challenge is to design (randomized) algorithms in which the number of processors used is polynomial and the (expected) runtime is polylogarithmic in the size of the input. Unlike the well-understood case of MIS in graphs, for the hypergraph problem, our knowledge is quite limited despite considerable work. It is known that the problem is in RNC when the edges of the hypergraph have constant size. For general hypergraphs with n vertices and medges, the fastest previously known algorithm works in time O(√n) n) with poly(m, n) processors. In this article, we give an EREW PRAM randomized algorithm that works in time no(1) with O(n + mlog n) processors on general hypergraphs satisfying m= no(1) log log n log log log n .We also give an EREW PRAM deterministic algorithm that runs in time nϵ on a graph with m= n1/d edges, for any constants δ, ϵ; the number of processors is polynomial in m, n for a fixed choice of d, ϵ. Our algorithms are based on a sampling idea that reduces the dimension of the hypergraph and employs the algorithm for constant dimension hypergraphs as a subroutine. © 2016 ACM.",Hypergraphs; Independent sets; Parallel algorithms; Randomized algorithms,Computation theory; Parallel algorithms; Parallel processing systems; Constant sizes; Deterministic algorithms; Hyper graph; Hypergraph problem; Independent set; Maximal independent set; Polylogarithmic; Randomized Algorithms; Graph theory
Experimental analysis of space-bounded schedulers,2016,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979750912&doi=10.1145%2f2938389&partnerID=40&md5=7883a6f0ba463537b0ec452d9731ab9c,"The running time of nested parallel programs on shared-memory machines depends in significant part on how well the scheduler mapping the program to the machine is optimized for the organization of caches and processor cores on the machine. Recent work proposed ""space-bounded schedulers"" for scheduling such programs on themultilevel cache hierarchies of current machines. The main benefit of this class of schedulers is that they provably preserve locality of the program at every level in the hierarchy, which can result in fewer cache misses and better use of bandwidth than the popular work-stealing scheduler. On the other hand, compared to work stealing, space-bounded schedulers are inferior at load balancing and may have greater scheduling overheads, raising the question as to the relative effectiveness of the two schedulers in practice. In this article, we provide the first experimental study aimed at addressing this question. To facilitate this study, we built a flexible experimental framework with separate interfaces for programs and schedulers. This enables a head-to-head comparison of the relative strengths of schedulers in terms of running times and cache miss counts across a range of benchmarks. (The framework is validated by comparisons with the IntelR CilkTM Plus work-stealing scheduler.) We present experimental results on a 32-core XeonR 7560 comparing work stealing, hierarchy-minded work stealing, and two variants of space-bounded schedulers on both divide-and-conquer microbenchmarks and some popular algorithmic kernels. Our results indicate that space-bounded schedulers reduce the number of L3 cache misses compared to work-stealing schedulers by 25% to 65% for most of the benchmarks, but incur up to 27% additional scheduler and load-imbalance overhead. Only for memory-intensive benchmarks can the reduction in cache misses overcome the added overhead, resulting in up to a 25% improvement in running time for synthetic benchmarks and about 20% improvement for algorithmic kernels.We also quantify runtime improvements varying the available bandwidth per core (the ""bandwidth gap"") and show up to 50% improvements in the running times of kernels as this gap increases fourfold. As part of our study, we generalize prior definitions of space-bounded schedulers to allow for more practical variants (while still preserving their guarantees) and explore implementation tradeoffs. © 2016 ACM.",Cache misses; Memory bandwidth; Multicores; Space-bounded schedulers; Thread schedulers; Work stealing,Balancing; Cache memory; Cost reduction; Program processors; Scheduling; Cache Miss; Memory bandwidths; Multi-cores; Space-bounded schedulers; Thread schedulers; Work-stealing; Bandwidth
Competitively scheduling tasks with intermediate parallelizability,2016,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056790973&doi=10.1145%2f2938378&partnerID=40&md5=a57865640b24d28112b797e18edc1031,"We introduce a scheduling algorithm Intermediate-SRPT, and show that it is O(log P)-competitive with respect to average flow time when scheduling jobs whose parallelizability is intermediate between being fully parallelizable and sequential. Here, the parameter P denotes the ratio between the maximum job size to the minimum.We also show a general matching lower bound on the competitive ratio. Our analysis builds on an interesting combination of potential function and local competitiveness arguments. © 2016 ACM.",Parallelization; Scheduling; Speedup curves,Scheduling; Average flows; Competitive ratio; Lower bounds; Parallelizations; Potential function; Scheduling jobs; Scheduling tasks; Speedup curves; Scheduling algorithms
Introduction to the special issue on SPAA 2014,2016,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056791611&doi=10.1145%2f2936716&partnerID=40&md5=bdd1fea7002edacac03d6ed48ea1a28d,[No abstract available],,
Parallel peeling algorithms,2016,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027872488&doi=10.1145%2f2938412&partnerID=40&md5=7d0c0d5ade25984686154a919b478f1e,"The analysis of several algorithms and data structures can be framed as a peeling process on a random hypergraph: vertices with degree less than k are removed until there are no vertices of degree less than k left. The remaining hypergraph is known as the k-core. In this article, we analyze parallel peeling processes, in which in each round, all vertices of degree less than k are removed. It is known that, below a specific edge-density threshold, the k-core is empty with high probability. We show that, with high probability, below this threshold, only 1 log((k-1)(r-1)) log log n+ O(1) rounds of peeling are needed to obtain the empty k-core for r-uniform hypergraphs. This bound is tight up to an additive constant. Interestingly, we show that, above this threshold, Ω(log n) rounds of peeling are required to find the nonempty k-core. Since most algorithms and data structures aim to peel to an empty k-core, this asymmetry appears fortunate. We verify the theoretical results both with simulation and with a parallel implementation using graphics processing units (GPUS). Our implementation provides insights into how to structure parallel peeling algorithms for efficiency in practice. © 2016 ACM.",Invertible Bloom lookup tables; Low-density parity check codes; Parallel algorithms; Peeling algorithms,Computer graphics; Data structures; Graphics processing unit; Parallel algorithms; Program processors; Table lookup; Algorithms and data structures; Edge densities; High probability; Low-density parity-check (LDPC) codes; Parallel implementations; Peeling algorithm; R-uniform hypergraphs; Random hypergraph; Graph theory
Low-Rank methods for parallelizing dynamic programming algorithms,2016,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984798604&doi=10.1145%2f2884065&partnerID=40&md5=0e7d54f1706ee08f075a3e17d4224d7f,"This article proposes efficient parallel methods for an important class of dynamic programming problems that includes Viterbi, Needleman-Wunsch, Smith-Waterman, and Longest Common Subsequence. In dynamic programming, the subproblems that do not depend on each other, and thus can be computed in parallel, form stages or wavefronts. The methods presented in this article provide additional parallelism allowing multiple stages to be computed in parallel despite dependencies among them. The correctness and the performance of the algorithm relies on rank convergence properties of matrix multiplication in the tropical semiring, formed with plus as the multiplicative operation and max as the additive operation. This article demonstrates the efficiency of the parallel algorithm by showing significant speedups on a variety of important dynamic programming problems. In particular, the parallel Viterbi decoder is up to 24× faster (with 64 processors) than a highly optimized commercial baseline. © 2016 ACM.",Dynamic programming; Longest common subsequence; Needleman-wunsch; Parallelism; Tropical semiring,Computer networks; Software engineering; Convergence properties; Dynamic programming algorithm; Dynamic programming problems; Longest common subsequences; MAtrix multiplication; Needleman-Wunsch; Parallelism; Tropical semiring; Dynamic programming
On folded-Clos networks with deterministic single-path routing,2016,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056582816&doi=10.1145%2f2858654&partnerID=40&md5=f709ff4c0437a256fffc764c4c4c6c69,"Folded-Clos networks, also known as fat-trees, have been widely used as interconnects in large-scale high-performance computing clusters. Although users often treat such interconnects as replacements of nonblocking crossbar switches that can carry out any permutation communication without contention, the networking capability of such interconnects without a centralized controller in computer communication environments is not well understood. In this article, we investigate nonblocking two-level folded-Clos networks with deterministic single-path routing, but no centralized controller, and establish the nonblocking condition. The results indicate that nonblocking two-level folded-Clos networks without a centralized controller are much more expensive to construct than the traditional nonblocking networks in the telecommunication environment. Practical two-level folded-Clos based interconnects are blocking. For such interconnects, we establish the lower bound for worst-case contention for permutations with any deterministic single-path routing scheme, show that existing routing schemes perform poorly in terms of worst-case contention for permutations, present a routing scheme that achieves the theoretical optimal, and empirically compare the performance of existing schemes with the optimal routing scheme. The techniques developed for two-level folded-Clos networks are further extended for the general fat-trees of any heights. © 2016 ACM",Clos networks; Computer communications; Fat-tree; Nonblocking; Single-path deterministic routing,Controllers; Forestry; Integrated control; Routing protocols; Switching networks; Clos networks; Computer Communications; Deterministic routing; Fat trees; Non-blocking; Network routing
Well-structured futures and cache locality,2016,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054891950&doi=10.1145%2f2858650&partnerID=40&md5=8d022be5c46b7a0144d8c2d2db24b71b,"In fork-join parallelism, a sequential program is split into a directed acyclic graph of tasks linked by directed dependency edges, and the tasks are executed, possibly in parallel, in an order consistent with their dependencies. A popular and effective way to extend fork-join parallelism is to allow threads to create futures. A thread creates a future to hold the results of a computation, which May or May not be executed in parallel. That result is returned when some thread touches that future, blocking if necessary until the result is ready. Recent research has shown that although futures can, of course, enhance parallelism in a structured way, they can have a deleterious effect on cache locality. In the worst case, futures can incur (PT∞ + tT∞) deviations, which implies (CPT∞ +CtT∞) additional cache misses, where C is the number of cache lines, P is the number of processors, t is the number of touches, and T∞ is the computation span. Since cache locality has a large impact on software performance on modern multicores, this result is troubling. In this article, we show that if futures are used in a simple, disciplined way, then the situation is much better: if each future is touched only once, either by the thread that created it or by a later descendant of the thread that created it, then parallel executions with work stealing can incur at most O(CPT2                             ∞) additional cache misses—a substantial improvement. This structured use of futures is characteristic of many (but not all) parallel applications. © 2016 ACM.",Cache locality; Futures; Parallel programming; Performance bounds; Scheduling; Work stealing,Cache memory; Parallel programming; Scheduling; Cache locality; Directed acyclic graph (DAG); Futures; Parallel application; Performance bounds; Sequential programs; Software performance; Work-stealing; Directed graphs
Leveraging hardware message passing for efficient thread synchronization,2016,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041994715&doi=10.1145%2f2858652&partnerID=40&md5=cb1f839fd5189732b413fd7fcb3fc316,"As the level of parallelism in manycore processors keeps increasing, providing efficient mechanisms for thread synchronization in concurrent programs is becoming a major concern. On cache-coherent shared-memory processors, synchronization efficiency is ultimately limited by the performance of the underlying cache coherence protocol. This article studies how hardware support for message passing can improve synchronization performance. Considering the ubiquitous problem of mutual exclusion, we devise novel algorithms for (i) classic locking, where application threads obtain exclusive access to a shared resource prior to executing their critical sections (CSes), and (ii) delegation, where CSes are executed by special threads. For classic locking, our HYBLOCK algorithm uses a mix of shared memory and hardware message passing, which introduces the idea of hybrid synchronization algorithms. For delegation, we propose MP-SERVER and HYBCOMB: the former is a straightforward adaptation of the server approach to hardware message passing, whereas the latter is a novel hybrid combining algorithm. Evaluation on Tilera’s TILE-Gx processor shows that HYBLOCK outperforms the best known classic locks. Furthermore, MP-SERVER can execute contended CSes with unprecedented throughput, as stalls related to cache coherence are removed from the critical path. HYBCOMB can achieve comparable performance while avoiding the need to dedicate server cores. Consequently, our queue and stack implementations, based on the new synchronization algorithms, largely outperform their most efficient shared-memory-only counterparts. © 2016 ACM.",Concurrent objects; Message passing; Mutual exclusion,Cache memory; Hardware; Locks (fasteners); Memory architecture; Synchronization; Cache coherence protocols; Concurrent objects; Hybrid synchronizations; Mutual exclusions; Shared memory processors; Synchronization algorithm; Synchronization performance; Thread synchronization; Message passing
Concurrency testing using controlled schedulers: An empirical study,2016,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041446437&doi=10.1145%2f2858651&partnerID=40&md5=884f2f506809d6550d09a9afad40f1ee,"We present an independent empirical study on concurrency testing using controlled schedulers. We have gathered 49 buggy concurrent software benchmarks, drawn from public code bases, which we call SCTBench. We applied a modified version of an existing concurrency testing tool to SCTBench, testing five controlled scheduling techniques: depth-first search, preemption bounding, delay bounding, a controlled random scheduler, and probabilistic concurrency testing (PCT). We attempt to answer several research questions: Which technique performs the best, in terms of bug finding ability? How effective are the two main schedule bounding techniques—preemption bounding and delay bounding—at finding bugs? What challenges are associated with applying concurrency testing techniques to existing code? Can we classify certain benchmarks as trivial or nontrivial? Overall, we found that PCT (with parameter d = 3) was the most effective technique in terms of bug finding; it found all bugs found by the other techniques, plus an additional three, and it missed only one bug. Surprisingly, we found that the naive controlled random scheduler, which randomly chooses one thread to execute at each scheduling point, performs well, finding more bugs than preemption bounding and just two fewer bugs than delay bounding. Our findings confirm that delay bounding is superior to preemption bounding and that schedule bounding is superior to an unbounded depth-first search. The majority of bugs in SCTBench can be exposed using a small schedule bound (1–2), supporting previous claims, although one benchmark requires five preemptions. We found that the need to remove nondeterminism and control all synchronization (as is required for systematic concurrency testing) can be nontrivial. There were eight distinct programs that could not easily be included in out study, such as those that perform network and interprocess communication. We report various properties about the benchmarks tested, such as the fact that the bugs in 18 benchmarks were exposed 50% of the time when using random scheduling. We note that future work should not use the benchmarks that we classify as trivial when presenting new techniques, other than as a minimum baseline. We have made SCTBench and our tools publicly available for reproducibility and use in future work. © 2016 ACM.",Concurrency; Context bounding; Stateless model checking; Systematic concurrency testing,Model checking; Scheduling; Testing; Bounding techniques; Concurrency; Concurrent software; Context bounding; Depth first search; Interprocess communication; Probabilistic concurrency testing; Scheduling techniques; Concurrency control
Introduction to the special issue on PPoPP’14,2016,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054889302&doi=10.1145%2f2856513&partnerID=40&md5=838a81337ae45bae8893f045a6809ace,[No abstract available],,
X10 and APGAS at petascale,2016,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044081208&doi=10.1145%2f2894746&partnerID=40&md5=993afda7fdab82af0e8cdcce46b8c98d,"X10 is a high-performance, high-productivity programming language aimed at large-scale distributed and shared-memory parallel applications. It is based on the Asynchronous Partitioned Global Address Space (APGAS) programming model, supporting the same fine-grained concurrency mechanisms within and across shared-memory nodes. We demonstrate that X10 delivers solid performance at petascale by running (weak scaling) eight application kernels on an IBM Power 775 supercomputer utilizing up to 55,680 Power7 cores (for 1.7Pflop/s of theoretical peak performance). For the four HPC Class 2 Challenge benchmarks, X10 achieves 41% to 87% of the system’s potential at scale (as measured by IBM’s HPCC Class 1 optimized runs). We also implement K-Means, Smith-Waterman, Betweenness Centrality, and Unbalanced Tree Search (UTS) for geometric trees. Our UTS implementation is the first to scale to petaflop systems. We describe the advances in distributed termination detection, distributed load balancing, and use of high-performance interconnects that enable X10 to scale out to tens of thousands of cores. We discuss how this work is driving the evolution of the X10 language, core class libraries, and runtime systems. © 2016 ACM.",APGAS; Performance; Scalability; X10,Scalability; Supercomputers; APGAS; Betweenness centrality; Distributed load balancing; Distributed termination; Fine-grained concurrency; Partitioned Global Address Space; Performance; Shared-memory parallels; Memory architecture
Masa: A multiplatform architecture for sequence aligners with block pruning,2016,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048707775&doi=10.1145%2f2858656&partnerID=40&md5=fe3cd1af24a4b1dac60d2385c048cd3b,"Biological sequence alignment is a very popular application in Bioinformatics, used routinely worldwide. Many implementations of biological sequence alignment algorithms have been proposed for multicores, GPUs, FPGAs and CellBEs. These implementations are platform-specific; porting them to other systems requires considerable programming effort. This article proposes and evaluates MASA, a flexible and customizable software architecture that enables the execution of biological sequence alignment applications with three variants (local, global, and semiglobal) in multiple hardware/software platforms with block pruning, which is able to reduce significantly the amount of data processed. To attain our flexibility goals, we also propose a generic version of block pruning and developed multiple parallelization strategies as building blocks, including a new asynchronous dataflow-based parallelization, which May be combined to implement efficient aligners in different platforms. We provide four MASA aligner implementations for multicores (OmpSs and OpenMP), GPU (CUDA), and Intel Phi (OpenMP), showing that MASA is very flexible. The evaluation of our generic block pruning strategy shows that it significantly outperforms the previously proposed block pruning, being able to prune up to 66.5% of the cells when using the new dataflow-based parallelization strategy. © 2016 ACM.",Biological sequence alignment; GPU; Intel phi; Multicores; Parallel algorithms,Application programming interfaces (API); Application programs; Graphics processing unit; Multiprocessing systems; Parallel algorithms; Program processors; Biological sequence alignment; Building blockes; Hardware/software; Intel phi; Multi-cores; Parallelization strategies; Parallelizations; Pruning strategy; Bioinformatics
"Trade-offs between synchronization, communication, and computation in parallel linear algebra computations",2016,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054842014&doi=10.1145%2f2897188&partnerID=40&md5=8849668b821e60621677cee6bf9c03d5,"This article derives trade-offs between three basic costs of a parallel algorithm: synchronization, data movement, and computational cost. These trade-offs are lower bounds on the execution time of the algorithm that are independent of the number of processors but dependent on the problem size. Therefore, they provide lower bounds on the execution time of any parallel schedule of an algorithm computed by a system composed of any number of homogeneous processors, each with associated computational, communication, and synchronization costs. We employ a theoretical model that measures the amount of work and data movement as a maximum over that incurred along any execution path during the parallel computation. By considering this metric rather than the total communication volume over the whole machine, we obtain new insights into the characteristics of parallel schedules for algorithms with nontrivial dependency structures. We also present reductions from BSP and LogGP algorithms to our execution model, extending our lower bounds to these two models of parallel computation. We first develop our results for general dependency graphs and hypergraphs based on their expansion properties, and then we apply the theorem to a number of specific algorithms in numerical linear algebra, namely triangular substitution, Cholesky factorization, and stencil computations. We represent some of these algorithms as families of dependency graphs. We derive their communication lower bounds by studying the communication requirements of the hypergraph structures shared by these dependency graphs. In addition to these lower bounds, we introduce a new communication-efficient parallelization for stencil computation algorithms, which is motivated by results of our lower bound analysis and the properties of previously existing parallelizations of the algorithms. © 2016 ACM.",Communication lower bounds; Graph expansion; Numerical linear algebra; Stencil computations,Commerce; Computation theory; Economic and social effects; Expansion; Graph theory; Linear algebra; Parallel processing systems; Scheduling algorithms; Synchronization; Cholesky factorizations; Dependency structures; Graph expansions; Homogeneous processors; Lower bounds; Numerical Linear Algebra; Parallel linear algebras; Stencil computations; Computational efficiency
CuFasterTucker: A Stochastic Optimization Strategy for Parallel Sparse FastTucker Decomposition on GPU Platform,2024,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196098626&doi=10.1145%2f3648094&partnerID=40&md5=ee220dc80b85287a7aacc3a88a590fef,"The amount of scientific data is currently growing at an unprecedented pace, with tensors being a common form of data that display high-order, high-dimensional, and sparse features. While tensor-based analysis methods are effective, the vast increase in data size has made processing the original tensor infeasible. Tensor decomposition offers a solution by decomposing the tensor into multiple low-rank matrices or tensors that can be efficiently utilized by tensor-based analysis methods. One such algorithm is the Tucker decomposition, which decomposes an N-order tensor into N low-rank factor matrices and a low-rank core tensor. However, many Tucker decomposition techniques generate large intermediate variables and require significant computational resources, rendering them inadequate for processing high-order and high-dimensional tensors. This article introduces FasterTucker decomposition, a novel approach to tensor decomposition that builds on the FastTucker decomposition, a variant of the Tucker decomposition. We propose an efficient parallel FasterTucker decomposition algorithm, called cuFasterTucker, designed to run on a GPU platform. Our algorithm has low storage and computational requirements and provides an effective solution for high-order and high-dimensional sparse tensor decomposition. Compared to state-of-the-art algorithms, our approach achieves a speedup of approximately 7 to 23 times.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesGPU CUDA parallelization; Kruskal approximation; sparse tensor decomposition; stochastic strategy; tensor computation,Digital storage; Graphics processing unit; Matrix algebra; Optimization; Additional key word and phrasesgpu CUDA parallelization; Key words; Kruskal; Kruskal approximation; Parallelizations; Sparse tensor decomposition; Sparse tensors; Stochastic strategy; Stochastics; Tensor computation; Tensor decomposition; Tensors
Machine Learning-Based Kernel Selector for SpMV Optimization in Graph Analysis,2024,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196196178&doi=10.1145%2f3652579&partnerID=40&md5=a7c79db2ca9672733838dd99fd7e75ed,"Sparse Matrix and Vector multiplication (SpMV) is one of the core algorithms in various large-scale scientific computing and real-world applications. With the rapid development of AI and big data, the input vector in SpMV becomes sparse in many application fields. Especially in some graph analysis calculations, the sparsity of the input vector will change with the running of the program, and the non-zero element distribution of the adjacency matrix of some graph data has the power law property, leading to serious load imbalance, which requires additional optimization means. Therefore, the optimal SpMV kernel may be different, and a single SpMV kernel can no longer meet the acceleration requirements. In this article, we propose a decision tree-based adaptive SpMV framework, named DTSpMV, that can automatically select appropriate SpMV kernels according to different input data in iterations of graph computation. Based on the analysis of computing patterns, bit-array compression algorithms, and serial and parallel algorithms, we encapsulate nine SpMV kernels within the framework. We explore machine learning-based kernel selectors in terms of both accuracy and runtime overhead. Experimental results on NVIDIA Tesla T4 GPU show that our adaptive framework achieves the arithmetic average performance improvement of 152× compared to the SpMV kernel in cuSPARSE.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesAdaptive framework; GPU computing; graph analysis; machine learning; SpMV,Acceleration; Decision trees; Graphics processing unit; Matrix algebra; Additional key word and phrasesadaptive framework; GPU computing; Graph analysis; Key words; Machine-learning; MAtrix multiplication; Sparse matrices; Sparse matrix and vector multiplication; Sparse vectors; Vector multiplication; Machine learning
CuFastTucker: A Novel Sparse FastTucker Decomposition for HHLST on Multi-GPUs,2024,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196121682&doi=10.1145%2f3661450&partnerID=40&md5=d7afa0f125febfdc32b2a83d32dc6ac0,"High-order, high-dimension, and large-scale sparse tensors (HHLST) have found their origin in various real industrial applications, such as social networks, recommender systems, bioinformatics, and traffic information. To handle these complex tensors, sparse tensor decomposition techniques are employed to project the HHLST into a low-rank space. In this article, we propose a novel sparse tensor decomposition model called Sparse FastTucker Decomposition (SFTD), which is a variant of Sparse Tucker Decomposition (STD). The SFTD utilizes Kruskal approximation for the core tensor, and we present a theorem that reduces the exponential space and computational overhead to a polynomial one. Additionally, we reduce the space overhead of intermediate parameters in the algorithmic process by sampling the intermediate matrix. Furthermore, this method guarantees convergence. To enhance the speed of SFTD, we leverage the compactness of matrix multiplication and parallel access through a stochastic strategy, resulting in GPU-accelerated cuFastTucker. Moreover, we propose a data division and communication strategy for cuFastTucker to accommodate data on Multi-GPU setups. Our proposed cuFastTucker demonstrates faster calculation and convergence speeds, as well as significantly lower space and computational overhead compared to state-of-the-art (SOTA) algorithms such as P-Tucker, Vest, GTA, Bigtensor, and SGD_Tucker.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesGPU CUDA parallelization; Kruskal approximation; sparse tucker decomposition; stochastic strategy,Computation theory; Cost reduction; Matrix algebra; Program processors; Stochastic systems; Tensors; Additional key word and phrasesgpu CUDA parallelization; Key words; Kruskal; Kruskal approximation; Parallelizations; Sparse tensors; Sparse tuck decomposition; Stochastic strategy; Stochastics; Tucker decompositions; Polynomial approximation
TLPGNN: A Lightweight Two-level Parallelism Paradigm for Graph Neural Network Computation on Single and Multiple GPUs,2024,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196163892&doi=10.1145%2f3644712&partnerID=40&md5=230338a52df721fd0933b9d3c70d8e31,"Graph Neural Networks (GNNs) are an emerging class of deep learning models specifically designed for graph-structured data. They have been effectively employed in a variety of real-world applications, including recommendation systems, drug development, and analysis of social networks. The GNN computation includes regular neural network operations and general graph convolution operations, which take most of the total computation time. Though several recent works have been proposed to accelerate the computation for GNNs, they face the limitations of heavy pre-processing, low efficiency atomic operations, and unnecessary kernel launches. In this article, we design TLPGNN, a lightweight two-level parallelism paradigm for GNN computation. First, we conduct a systematic analysis of the hardware resource usage of GNN workloads to understand the characteristics of GNN workloads deeply. With the insightful observations, we then divide the GNN computation into two levels, i.e., vertex parallelism for the first level and feature parallelism for the second. Next, we employ a novel hybrid dynamic workload assignment to address the imbalanced workload distribution. Furthermore, we fuse the kernels to reduce the number of kernel launches and cache the frequently accessed data into registers to avoid unnecessary memory traffic. To scale TLPGNN to multi-GPU environments, we propose an edge-aware row-wise 1-D partition method to ensure a balanced workload distribution across different GPU devices. Experimental results on various benchmark datasets demonstrate the superiority of our approach, achieving substantial performance improvement over state-of-the-art GNN computation systems, including Deep Graph Library (DGL), GNNAdvisor, and FeatGraph, with speedups of 6.1×, 7.7×, and 3.0×, respectively, on average. Evaluations of multiple-GPU TLPGNN also demonstrate that our solution achieves both linear scalability and a well-balanced workload distribution.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesGraph neural networks; GPU; performance,Benchmarking; Deep learning; Graph neural networks; Program processors; Additional key word and phrasesgraph neural network; Balanced work-load; Graph neural networks; Key words; Learning models; Multiple GPUs; Network computations; Neural-networks; Performance; Work-load distribution; Graphics processing unit
Low-Overhead Trace Collection and Profiling on GPU Compute Kernels,2024,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196258695&doi=10.1145%2f3649510&partnerID=40&md5=d7a2a5f7d88498639af306dc47b35bbe,"While GPUs can bring substantial speedup to compute-intensive tasks, their programming is notoriously hard. From their programming model, to microarchitectural particularities, the programmer may encounter many pitfalls which may hinder performance in obscure ways. Numerous performance analysis tools provide helpful data on the efficiency of the compute kernels, but few allow the programmer to efficiently gather runtime information directly on the device and pinpoint the sections to optimize.We propose in this article an instrumentation method to collect traces while executing the compute kernel, with a reduced overhead compared with other approaches, by exploiting the inherently parallel behavior of GPUs and compartmentalizing tracing phases. The reference implementation is freely available and induces an average overhead of 1.6 × on a popular scientific computing benchmark and 1.5 × over the kernel execution time. This represents an improvement of an order of magnitude compared with similar work, and proves useful for timing-guided optimizations. The tool generates insightful execution traces and timestamps which can be analyzed to better understand performance issues in the kernel. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",GPU programming; performance analysis; software tracing,Program processors; Analysis tools; Compute-intensive tasks; GPU-programming; Low overhead; Performance; Performances analysis; Programming models; Run-time information; Software tracing; Trace collection; Graphics processing unit
Decentralized Scheduling for Data-Parallel Tasks in the Cloud,2024,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196073817&doi=10.1145%2f3651858&partnerID=40&md5=0a6a9391e36d4247e9ac795684e51d65,"For latency-sensitive data processing applications in the cloud, concurrent data-parallel tasks need to be scheduled and processed quickly. A data-parallel task usually consists of a set of sub-tasks, generating a set of flows that are collectively referred to as coflows. The state-of-the-art schedulers collect coflow information in the cloud to optimize coflow-level performance. However, most of the coflows, classified as small coflows because they consist of only short flows, have been largely overlooked. This article presents OptaX, a decentralized network scheduling service that collaboratively schedules data-parallel tasks' small coflows. OptaX adopts a cross-layer, commercial off-the-shelf switch-compatible design that leverages the sendbuffer information in the kernel to adaptively optimize flow scheduling in the network. Specifically, OptaX (i) monitors the system calls (syscalls) in the hosts to obtain their sendbuffer footprints, and (ii) recognizes small coflows and assigns high priorities to their flows. OptaX transfers these flows in a FIFO manner by adjusting TCP's two attributes: window size and round-trip time. We have implemented OptaX as a Linux kernel module. The evaluation shows that OptaX is at least 2.2× faster than fair sharing and 1.2× faster than only assigning small coflows with the highest priority. We further apply OptaX to improve the small I/O performance of Ursa, a distributed block storage system that provides virtual disks where small I/O is dominant. Ursa with OptaX achieves significant improvement compared to the original Ursa for small I/O latency.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesDecentralized scheduling; coflows; cross-layer scheduling; data-parallel tasks,Computer operating systems; Multiprocessing systems; Sensitive data; Additional key word and phrasesdecentralized scheduling; Co-flow; Cross layer scheduling; Data parallel; Data-parallel task; Decentralized scheduling; Key words; Parallel task; Performance; Sensitive datas; Digital storage
A Conflict-Resilient Lock-Free Linearizable Calendar Queue,2024,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187917453&doi=10.1145%2f3635163&partnerID=40&md5=6618a2cac9e6d9400c29d4e0f2ef73f7,"In the last two decades, great attention has been devoted to the design of non-blocking and linearizable data structures, which enable exploiting the scaled-up degree of parallelism in off-the-shelf shared-memory multi-core machines. In this context, priority queues are highly challenging. Indeed, concurrent attempts to extract the highest-priority item are prone to create detrimental thread conflicts that lead to abort/retry of the operations. In this article, we present the first priority queue that jointly provides: (i) lock-freedom and linearizability; (ii) conflict resiliency against concurrent extractions; (iii) adaptiveness to different contention profiles; and (iv) amortized constant-time access for both insertions and extractions. Beyond presenting our solution, we also provide proof of its correctness based on an assertional approach. Also, we present an experimental study on a 64-CPU machine, showing that our proposal provides performance improvements over state-of-the-art non-blocking priority queues. © 2024 Association for Computing Machinery. All rights reserved.",,Locks (fasteners); Calendar queues; Degree of parallelism; Linearizability; Lock freedoms; Lock-free; Multi-core machines; Non-blocking; Priority queues; Scaled-up; Shared memory; Queueing theory
Modeling and Analyzing Evaluation Cost of CUDA Kernels,2024,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187796433&doi=10.1145%2f3639403&partnerID=40&md5=29648dbe2b207f72ea9d5e3546e8face,"Motivated by the increasing importance of general-purpose Graphic Processing Units (GPGPU) programming, exemplified by NVIDIA’s CUDA framework, as well as the difficulty, especially for novice programmers, of reasoning about performance in GPGPU kernels, we introduce a novel quantitative program logic for CUDA kernels. The logic allows programmers to reason about both functional correctness and resource usage of CUDA kernels, paying particular attention to a set of common but CUDA-specific performance bottlenecks: warp divergences, uncoalesced memory accesses, and bank conflicts. The logic is proved sound with respect to a novel operational cost semantics for CUDA kernels. The semantics, logic, and soundness proofs are formalized in Coq. An inference algorithm based on LP solving automatically synthesizes symbolic resource bounds by generating derivations in the logic. This algorithm is the basis of RaCUDA, an end-to-end resource-analysis tool for kernels, which has been implemented using an existing resource-analysis tool for imperative programs. An experimental evaluation on a suite of benchmarks shows that the analysis is effective in aiding the detection of performance bugs in CUDA kernels. © 2024 Copyright held by the owner/author(s).",CUDA; performance analysis; program logics; Resource-aware type system; thread-level parallelism,Benchmarking; Computer circuits; Cost benefit analysis; Inference engines; Program debugging; Program processors; Semantics; Analysis tools; CUDA; Evaluation cost; Performances analysis; Program logic; Resource analysis; Resource aware; Resource-aware type system; Thread level parallelism; Type systems; Graphics processing unit
Checkpointing Strategies to Tolerate Non-Memoryless Failures on HPC Platforms,2024,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187796212&doi=10.1145%2f3624560&partnerID=40&md5=39ee9902e3cb7aa7afb25f67a49254b7,"This article studies checkpointing strategies for parallel applications subject to failures. The optimal strategy to minimize total execution time, or makespan, is well known when failure IATs obey an Exponential distribution, but it is unknown for non-memoryless failure distributions. We explain why the latter fact is misunderstood in recent literature. We propose a general strategy that maximizes the expected efficiency until the next failure, and we show that this strategy achieves an asymptotically optimal makespan, thereby establishing the first optimality result for arbitrary failure distributions. Through extensive simulations, we show that the new strategy is always at least as good as the Young/Daly strategy for various failure distributions. For distributions with high infant mortality (such as LogNormal with shape parameter k = 2.51 or Weibull with shape parameter 0.5), the execution time is divided by a factor of 1.9 on average, and up to a factor 4.2 for recently deployed platforms.  Copyright © 2024 held by the owner/author(s).",Checkpoint; failure; fault-tolerance; non-memoryless; Young/Daly formula,Weibull distribution; Check pointing; Checkpoint; Failure distributions; Makespan; Memoryless; Non-memoryless; Optimal strategies; Parallel application; Shape parameters; Young/daly formula; Fault tolerance
ABSS: An Adaptive Batch-Stream Scheduling Module for Dynamic Task Parallelism on Chiplet-based Multi-Chip Systems,2024,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187794741&doi=10.1145%2f3643597&partnerID=40&md5=84133a471665817e3cedcffc5d7e9ade,"Thanks to the recognition and promotion of chiplet-based High-Performance Computing (HPC) system design technology by semiconductor industry/market leaders, chiplet-based multi-chip systems have gradually become the mainstream. Unfortunately, programming such systems to achieve efficient computing is a challenge, especially when considering dynamic task parallelism. This paper presents an Adaptive Batch-Stream Scheduling (ABSS) module for dynamic task parallelism on chiplet-based multi-chip systems. To this end, we propose an adaptive batch-stream scheduling method based on Graph Convolution Network (GCN) classifier to select the appropriate scheduling scheme. We further design a chiplet-based core-cluster binding mechanism, which establishes the affinity between threads and core-clusters on CPU-compute die. Moreover, to achieve dynamic workload balance, we propose a chiplet-based nearest task stealing method. We implement our ABSS module on the HiSilicon Kunpeng-920 chiplet-based multi-chip system. Experiments show that it outperforms state-of-the-art parallelism solutions, such as Intel Threading Building Blocks. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Adaptive batch-stream scheduling; chiplet-based core-cluster binding; chiplet-based multi-chip system; chiplet-based nearest task stealing; task parallelism,Scheduling algorithms; Semiconductor device manufacture; Adaptive batch-stream scheduling; Chiplet-based core-cluster binding; Chiplet-based multi-chip system; Chiplet-based near task stealing; Dynamic tasks; High performance computing systems; Multi chip system; Scheduling module; Stream Scheduling; Task parallelism; Microprocessor chips
HPS Cholesky: Hierarchical Parallelized Supernodal Cholesky with Adaptive Parameters,2024,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187775084&doi=10.1145%2f3630051&partnerID=40&md5=fd39eb4e99167868b05f36244d5114f6,"Sparse supernodal Cholesky on multi-NUMAs is challenging due to the supernode relaxation and load balancing. In this work, we propose a novel approach to improve the performance of sparse Cholesky by combining deep learning with a relaxation parameter and a hierarchical parallelization strategy with NUMA affinity. Specifically, our relaxed supernodal algorithm utilizes a well-trained GCN model to adaptively adjust relaxation parameters based on the sparse matrix's structure, achieving a proper balance between task-level parallelism and dense computational granularity. Additionally, the hierarchical parallelization maps supernodal tasks to the local NUMA parallel queue and updates contribution blocks in pipeline mode. Furthermore, the stream scheduling with NUMA affinity can further enhance the efficiency of memory access during the numerical factorization. The experimental results show that HPS Cholesky can outperform state-of-the-art libraries, such as EigenLLT, CHOLMOD, PaStiX and SuiteSparse on 79.78%, 79.60%, 82.09% and 74.47% of 1,128 datasets. It achieves an average speedup of 1.41x over the current optimal relaxation algorithm. Moreover, 70.83% of matrices have surpassed MKL sparse Cholesky on Xeon Gold 6248.  Copyright © 2024 held by the owner/author(s).",graph convolutional network; hierarchical parallelization; multi-NUMA architecture; Supernodal Cholesky factorization; task stream processing,Data mining; Deep learning; Matrix algebra; Memory architecture; Cholesky; Cholesky factorizations; Convolutional networks; Graph convolutional network; Hierarchical parallelization; Multi-NUMA architecture; Relaxation parameter; Stream processing; Supernodal cholesky factorization; Task stream processing; Factorization
Improved Online Scheduling of Moldable Task Graphs under Common Speedup Models,2024,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187777808&doi=10.1145%2f3630052&partnerID=40&md5=d826f62e09dce73b1e8be9531d60e1b1,"We consider the online scheduling problem of moldable task graphs onmultiprocessor systems forminimizing the overall completion time (or makespan). Moldable job scheduling has been widely studied in the literature, in particular when tasks have dependencies (i.e., task graphs) or when tasks are released on-the-fly (i.e., online). However, few studies have focused on both (i.e., online scheduling of moldable task graphs). In this article, we design a new online scheduling algorithm for this problem and derive constant competitive ratios under several common yet realistic speedup models (i.e., roofline, communication, Amdahl, and a general combination). These results improve the oneswe have shown in the preliminary version of the article. We also prove, for each speedup model, a lower bound on the competitiveness of any online list scheduling algorithm that allocates processors to a task based only on the task's parameters and not on its position in the graph. This lower bound matches exactly the competitive ratio of our algorithm for the roofline, communication, and Amdahl's model, and is close to the ratio for the general model. Finally, we provide a lower bound on the competitive ratio of any deterministic online algorithm for the arbitrary speedup model, which is not constant but depends on the number of tasks in the longest path of the graph.  © 2024 Copyright held by the owner/author(s).",competitive ratio; moldable task; online scheduling; Task graph,Online systems; Scheduling algorithms; Competitive ratio; Completion time; Jobs scheduling; Low bound; Makespan; Moldable tasks; Online scheduling; Rooflines; Scheduling problem; Tasks graph; Graphic methods
Parallel Minimum Cuts in O(m log2 n) Work and Low Depth,2023,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181468580&doi=10.1145%2f3565557&partnerID=40&md5=f2c8ec50ff9b4a2798aa5551ae2eae01,"We present a randomized O(m log2 n) work, O(polylog n) depth parallel algorithm for minimum cut. This algorithm matches the work bounds of a recent sequential algorithm by Gawrychowski, Mozes, and Weimann [ICALP’20], and improves on the previously best parallel algorithm by Geissmann and Gianinazzi [SPAA’18], which performs O(m log4 n) work in O(polylog n) depth. Our algorithm makes use of three components that might be of independent interest. First, we design a parallel data structure that efficiently supports batched mixed queries and updates on trees. It generalizes and improves the work bounds of a previous data structure of Geissmann and Gianinazzi and is work efficient with respect to the best sequential algorithm. Second, we design a parallel algorithm for approximate minimum cut that improves on previous results by Karger and Motwani. We use this algorithm to give a work-efficient procedure to produce a tree packing, as in Karger’s sequential algorithm for minimum cuts. Last, we design an efficient parallel algorithm for solving the minimum 2-respecting cut problem. © 2023 Copyright held by the owner/author(s)",dynamic trees; graph algorithms; Minimum cut; parallel algorithms,Data structures; Trees (mathematics); Algorithm for solving; Dynamic trees; Graph algorithms; Low depths; Minimum cut; Parallel data structures; Polylogs; Sequential algorithm; Three-component; Tree packing; Parallel algorithms
Non-clairvoyant Scheduling with Predictions,2023,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181847035&doi=10.1145%2f3593969&partnerID=40&md5=a0f080b014ae134b0180ac8f50697b8a,"In the single-machine non-clairvoyant scheduling problem, the goal is to minimize the total completion time of jobs whose processing times are unknown a priori. We revisit this well-studied problem and consider the question of how to effectively use (possibly erroneous) predictions of the processing times. We study this question from ground zero by first asking what constitutes a good prediction; we then propose a new measure to gauge prediction quality and design scheduling algorithms with strong guarantees under this measure. Our approach to derive a prediction error measure based on natural desiderata could find applications for other online problems. © 2023 Copyright held by the owner/author(s)",competitive ratio; non-clairvoyance; prediction; Scheduling,Scheduling algorithms; Competitive ratio; Error measures; Non-clairvoyance; Prediction errors; Prediction quality; Processing time; Scheduling; Scheduling problem; Single- machines; Total completion time; Forecasting
Distributed Graph Coloring Made Easy,2023,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181849299&doi=10.1145%2f3605896&partnerID=40&md5=38e27c407f47ba5b2751e731571d41d2,"In this article, we present a deterministic CONGEST algorithm to compute an O(kΔ)-vertex coloring in O(Δ/k) + log∗ n rounds, where Δ is the maximum degree of the network graph and k ≥ 1 can be freely chosen. The algorithm is extremely simple: each node locally computes a sequence of colors and then it tries colors from the sequence in batches of size k. Our algorithm subsumes many important results in the history of distributed graph coloring as special cases, including Linial’s color reduction [Linial, FOCS’87], the celebrated locally iterative algorithm from [Barenboim, Elkin, Goldenberg, PODC’18], and various algorithms to compute defective and arbdefective colorings. Our algorithm can smoothly scale between several of these previous results and also simplifies the state-of-the-art (Δ + 1)-coloring algorithm. At the cost of losing some of the algorithm’s simplicity we also provide a O(kΔ)-coloring algorithm in O(√Δ/k) + log∗ n rounds. We also provide improved deterministic algorithms for ruling sets, and, additionally, we provide a tight characterization for one-round color reduction algorithms. © 2023 Copyright held by the owner/author(s)",CONGEST model; distributed graph coloring; LOCAL model,Coloring; Graph theory; Iterative methods; Coloring algorithms; Colour reductions; CONGEST model; Deterministics; Distributed graph coloring; Graph colorings; LOCAL model; Maximum degree; Networks/graphs; Vertex coloring; Color
Algorithms for Right-sizing Heterogeneous Data Centers,2023,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181849191&doi=10.1145%2f3595286&partnerID=40&md5=fe929db8e951cdac41df213b31369934,"Power consumption is a dominant and still growing cost factor in data centers. In time periods with low load, the energy consumption can be reduced by powering down unused servers. We resort to a model introduced by Lin, Wierman, Andrew, and Thereska [23, 24] that considers data centers with identical machines and generalize it to heterogeneous data centers with d different server types. The operating cost of a server depends on its load and is modeled by an increasing, convex function for each server type. In contrast to earlier work, we consider the discrete setting, where the number of active servers must be integral. Thereby, we seek truly feasible solutions. For homogeneous data centers (d = 1), both the offline and the online problem were solved optimally in References [3, 4]. In this article, we study heterogeneous data centers with general time-dependent operating cost functions. We develop an online algorithm based on a work function approach that achieves a competitive ratio of 2d + 1 + ϵ for any ϵ > 0. For time-independent operating cost functions, the competitive ratio can be reduced to 2d + 1. There is a lower bound of 2d shown in Reference [5], so our algorithm is nearly optimal. For the offline version, we give a graph-based (1 + ϵ)-approximation algorithm. Additionally, our offline algorithm is able to handle time-variable data-center sizes. © 2023 Copyright held by the owner/author(s)",approximation algorithm; competitive analysis; discrete setting; energy conservation; Heterogeneous machines; online algorithm,Cost functions; Electric loads; Energy utilization; Graphic methods; Green computing; Operating costs; Competitive analysis; Competitive ratio; Cost-factors; Cost-function; Datacenter; Discrete settings; Heterogeneous data centers; Heterogeneous machine; Offline; On-line algorithms; Approximation algorithms
A Fast Algorithm for Aperiodic Linear Stencil Computation using Fast Fourier Transforms,2023,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181848452&doi=10.1145%2f3606338&partnerID=40&md5=1748c3b496bab65b2f3c8793b470bf32,"Stencil computations are widely used to simulate the change of state of physical systems across a multidimensional grid over multiple timesteps. The state-of-the-art techniques in this area fall into three groups: cache-aware tiled looping algorithms, cache-oblivious divide-and-conquer trapezoidal algorithms, and Krylov subspace methods. In this article, we present two efficient parallel algorithms for performing linear stencil computations. Current direct solvers in this domain are computationally inefficient, and Krylov methods require manual labor and mathematical training. We solve these problems for linear stencils by using discrete Fourier transforms preconditioning on a Krylov method to achieve a direct solver that is both fast and general. Indeed, while all currently available algorithms for solving general linear stencils perform Θ (NT) work, where N is the size of the spatial grid and T is the number of timesteps, our algorithms perform o (NT) work. To the best of our knowledge, we give the first algorithms that use fast Fourier transforms to compute final grid data by evolving the initial data for many timesteps at once. Our algorithms handle both periodic and aperiodic boundary conditions and achieve polynomially better performance bounds (i.e., computational complexity and parallel runtime) than all other existing solutions. Initial experimental results show that implementations of our algorithms that evolve grids of roughly 107 cells for around 105 timesteps run orders of magnitude faster than state-of-the-art implementations for periodic stencil problems, and 1.3× to 8.5× faster for aperiodic stencil problems. © 2023 Copyright held by the owner/author(s)",divide-and-conquer; fast fourier transform; parallel stencil solver; Stencil computation,Cache memory; Computational efficiency; Change of state; Direct solvers; Divide-and-conquer; Fast algorithms; Krylov method; Multi-dimensional grids; Parallel stencil solv; Physical systems; Stencil computations; Time step; Discrete Fourier transforms
Introduction to the Special Issue for SPAA’21,2023,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181936516&doi=10.1145%2f3630608&partnerID=40&md5=2c82237c035a7b56be0fc8203b9adfd7,[No abstract available],,
Orthogonal Layers of Parallelism in Large-Scale Eigenvalue Computations,2023,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173283282&doi=10.1145%2f3614444&partnerID=40&md5=9b0436365edd492efb5d2a6629926818,"We address the communication overhead of distributed sparse matrix-(multiple)-vector multiplication in the context of large-scale eigensolvers, using filter diagonalization as an example. The basis of our study is a performance model, which includes a communication metric that is computed directly from the matrix sparsity pattern without running any code. The performance model quantifies to which extent scalability and parallel efficiency are lost due to communication overhead.To restore scalability, we identify two orthogonal layers of parallelism in the filter diagonalization technique. In the horizontal layer the rows of the sparse matrix are distributed across individual processes. In the vertical layer bundles of multiple vectors are distributed across separate process groups. An analysis in terms of the communication metric predicts that scalability can be restored if, and only if, one implements the two orthogonal layers of parallelism via different distributed vector layouts.Our theoretical analysis is corroborated by benchmarks for application matrices from quantum and solid state physics, road networks, and nonlinear programming. We finally demonstrate the benefits of using orthogonal layers of parallelism with two exemplary application cases - an exciton and a strongly correlated electron system - which incur either small or large communication overhead. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",communication-avoiding algorithms; Distributed computing; sparse matrix-vector multiplication,Benchmarking; Distributed computer systems; Eigenvalues and eigenfunctions; Matrix algebra; Nonlinear programming; Quantum theory; Restoration; Vectors; Communication avoiding algorithms; Communication overheads; Eigenvalue computation; Filter diagonalization; Large-scales; Multiple vectors; Performance Modeling; Sparse matrices; Sparse matrix-vector multiplication; Sparse-Matrix Vector multiplications; Scalability
"Fast Parallel Algorithms for Enumeration of Simple, Temporal, and Hop-constrained Cycles",2023,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173215215&doi=10.1145%2f3611642&partnerID=40&md5=35327cd2fcbc7946512aefacc3d0acdf,"Cycles are one of the fundamental subgraph patterns and being able to enumerate them in graphs enables important applications in a wide variety of fields, including finance, biology, chemistry, and network science. However, to enable cycle enumeration in real-world applications, efficient parallel algorithms are required. In this work, we propose scalable parallelisation of state-of-the-art sequential algorithms for enumerating simple, temporal, and hop-constrained cycles. First, we focus on the simple cycle enumeration problem and parallelise the algorithms by Johnson and by Read and Tarjan in a fine-grained manner. We theoretically show that our resulting fine-grained parallel algorithms are scalable, with the fine-grained parallel Read-Tarjan algorithm being strongly scalable. In contrast, we show that straightforward coarse-grained parallel versions of these simple cycle enumeration algorithms that exploit edge- or vertex-level parallelism are not scalable. Next, we adapt our fine-grained approach to enable the enumeration of cycles under time-window, temporal, and hop constraints. Our evaluation on a cluster with 256 CPU cores that can execute up to 1,024 simultaneous threads demonstrates a near-linear scalability of our fine-grained parallel algorithms when enumerating cycles under the aforementioned constraints. On the same cluster, our fine-grained parallel algorithms achieve, on average, one order of magnitude speedup compared to the respective coarse-grained parallel versions of the state-of-the-art algorithms for cycle enumeration. The performance gap between the fine-grained and the coarse-grained parallel algorithms increases as we use more CPU cores.  © 2023 Copyright held by the owner/author(s).",Cycle enumeration; graph pattern mining; parallel graph algorithms,Clustering algorithms; Graph theory; Coarse-grained parallels; CPU cores; Cycle enumeration; Fine grained; Graph pattern mining; Graph patterns; Parallel graph algorithms; Parallel version; Pattern mining; Simple++; Parallel algorithms
The Computational Complexity of Feasibility Analysis for Conditional DAG Tasks,2023,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173242967&doi=10.1145%2f3606342&partnerID=40&md5=278c60703d4d8322d25ef9254330f0f6,"The Conditional DAG (CDAG) task model is used for modeling multiprocessor real-time systems containing conditional expressions for which outcomes are not known prior to their evaluation. Feasibility analysis for CDAG tasks upon multiprocessor platforms is shown to be complete for the complexity class pspace; assuming np g pspace, this result rules out the use of Integer Linear Programming solvers for solving this problem efficiently. It is further shown that there can be no pseudo-polynomial time algorithm that solves this problem unless p = pspace. © 2023 Copyright held by the owner/author(s).",global scheduling; Multiprocessor feasibility analysis; pspace complete,Computational complexity; Integer programming; Interactive computer systems; Multiprocessing systems; Polynomial approximation; Complexity class; Conditional expressions; Feasibility analysis; Global scheduling; Integer Linear Programming; Multi-processor platforms; Multiprocessor feasibility analyse; PSPACE-complete; Real - Time system; Task modelling; Real time systems
Investigation and Implementation of Parallelism Resources of Numerical Algorithms,2023,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164246136&doi=10.1145%2f3583755&partnerID=40&md5=aed3181df9f89b61233fcee4937cbf4a,"This article is devoted to an approach to solving a problem of the efficiency of parallel computing. The theoretical basis of this approach is the concept of a Q-determinant. Any numerical algorithm has a Q-determinant. The Q-determinant of the algorithm has clear structure and is convenient for implementation. The Q-determinant consists of Q-terms. Their number is equal to the number of output data items. Each Q-term describes all possible ways to compute one of the output data items based on the input data. We also describe a software Q-system for studying the parallelism resources of numerical algorithms. This system enables to compute and compare the parallelism resources of numerical algorithms. The application of the Q-system is shown on the example of numerical algorithms with different structures of Q-determinants. Furthermore, we suggest a method for designing of parallel programs for numerical algorithms. This method is based on a representation of a numerical algorithm in the form of a Q-determinant. As a result, we can obtain the program using the parallelism resource of the algorithm completely. Such programs are called Q-effective. The results of this research can be applied to increase the implementation efficiency of numerical algorithms, methods, as well as algorithmic problems on parallel computing systems.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesQ-term of algorithm; parallelism resource of algorithm; Q-determinant of algorithm; Q-effective implementation of algorithm; Q-effective program; Q-effective programming; representation of algorithm in form of Q-determinant; software Q-system,Numerical methods; Additional key word and phrasesq-term of algorithm; Effective programs; Key words; Parallelism resource of algorithm; Q-determinant of algorithm; Q-effective implementation of algorithm; Q-effective program; Q-effective programming; Representation of algorithm in form of Q-determinant; Software Q-system; Efficiency
Parallel Peeling of Bipartite Networks for Hierarchical Dense Subgraph Discovery,2023,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164236328&doi=10.1145%2f3583084&partnerID=40&md5=489f32f75f850f5e66a640e0377ba681,"Wing and Tip decomposition are motif-based analytics for bipartite graphs that construct a hierarchy of butterfly (2,2-biclique) dense edge and vertex induced subgraphs, respectively. They have applications in several domains, including e-commerce, recommendation systems, document analysis, and others.Existing decomposition algorithms use a bottom-up approach that constructs the hierarchy in an increasing order of the subgraph density. They iteratively select the edges or vertices with minimum butterfly count peel, i.e., remove them along with their butterflies. The amount of butterflies in real-world bipartite graphs makes bottom-up peeling computationally demanding. Furthermore, the strict order of peeling entities results in a large number of sequentially dependent iterations. Consequently, parallel algorithms based on bottom up peeling incur heavy synchronization and poor scalability.In this article, we propose a novel Parallel Bipartite Network peelinG (PBNG) framework that adopts a two-phased peeling approach to relax the order of peeling, and in turn, dramatically reduce synchronization. The first phase divides the decomposition hierarchy into few partitions and requires little synchronization. The second phase concurrently processes all partitions to generate individual levels of the hierarchy and requires no global synchronization. The two-phased peeling further enables batching optimizations that dramatically improve the computational efficiency of PBNG.We empirically evaluate PBNG using several real-world bipartite graphs and demonstrate radical improvements over the existing approaches. On a shared-memory 36 core server, PBNG achieves up to 19.7× self-relative parallel speedup. Compared to the state-of-the-art parallel framework ParButterfly, PBNG reduces synchronization by up to 15,260× and execution time by up to 295×. Furthermore, it achieves up to 38.5× speedup over state-of-the-art algorithms specifically tuned for wing decomposition. Our source code is made available at https://github.com/kartiklakhotia/RECEIPT.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesGraph algorithms; bipartite graphs; dense graph mining; parallel graph analytics,Computational efficiency; Graph theory; Graphic methods; Iterative methods; Additional key word and phrasesgraph algorithm; Bipartite graphs; Bipartite network; Dense graph mining; Dense graphs; Graph mining; Graph-analytic; Key words; Parallel graph analytic; Real-world; Synchronization
Faster Supervised Average Consensus in Adversarial and Stochastic Anonymous Dynamic Networks,2023,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164245734&doi=10.1145%2f3593426&partnerID=40&md5=5279b5cc9685eb88902aef3483c5c324,"How do we reach consensus on an average value in a dynamic crowd without revealing identity? In this work, we study the problem of average network consensus in Anonymous Dynamic Networks (ADN). Network dynamicity is specified by the sequence of topology-graph isoperimetric numbers occurring over time, which we call the isoperimetric dynamicity of the network. The consensus variable is the average of values initially held by nodes, which is customary in the network-consensus literature. Given that having an algorithm to compute the average one can compute the network size (i.e., the counting problem) and vice versa, we focus on the latter.We present a deterministic distributed average network consensus algorithm for ADNs that we call isoperimetric Scalable Coordinated Anonymous Local Aggregation, and we analyze its performance for different scenarios, including worst-case (adversarial) and stochastic dynamic topologies. Our solution utilizes supervisor nodes, which have been shown to be necessary for computations in ADNs. The algorithm uses the isoperimetric dynamicity of the network as an input, meaning that only the isoperimetric number parameters (or their lower bound) must be given, but topologies may occur arbitrarily or stochastically as long as they comply with those parameters.Previous work for adversarial ADNs overestimates the running time to deal with worst-case scenarios. For ADNs with given isoperimetric dynamicity, our analysis shows improved performance for some practical dynamic topologies, with cubic time or better for stochastic ADNs, and our experimental evaluation indicates that our theoretical bounds could not be substantially improved for some models of dynamic networks.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesAnonymous Dynamic Networks; algebraic computations; counting; network consensus; stochastic dynamic networks,Stochastic models; Stochastic systems; Additional key word and phrasesanonymous dynamic network; Algebraic computations; Counting; Dynamic network; Isoperimetric number; Key words; Network consensus; Performance; Stochastic dynamic networks; Stochastics; Topology
POETS: An Event-driven Approach to Dissipative Particle Dynamics,2023,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164245580&doi=10.1145%2f3580372&partnerID=40&md5=c33a84d66c8a2c482498fb45e363e656,"HPC clusters have become ever more expensive, both in terms of capital cost and energy consumption; some estimates suggest that competitive installations at the end of the next decade will require their own power station. One way around this looming problem is to design bespoke computing engines, but while the performance benefits are good, the design costs are huge and cannot easily be amortized. Partially Ordered Event Triggered System (POETS) - the focus of this article - seeks to exploit a middle way: The architecture is tuned to a specific algorithmic pattern but, within that constraint, is fully programmable. POETS software is quasi-imperative: The user defines a set of sequential event handlers, defines the topology of a (typically large) concurrent ensemble of these, and lets them interact. The ""solution""may be exfiltrated from the emergent behaviour of the ensemble. In this article, we describe (briefly) the architecture, and an example computational chemistry application, dissipative particle dynamics (DPD). The DPD algorithm is traditionally implemented using parallel computational techniques, but we re-cast it as a concurrent compute problem that is then ideally suited to POETS. Our prototype system is realised on a cluster of 48 FPGAs providing 50K concurrent hardware threads, and we report performance speedups of over two orders of magnitude better than a single thread baseline comparator and scaling behaviour that is almost constant. The results are validated against a ""conventional""implementation.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesParallel architectures; event-driven processing; massively micro-parallel systems,Computational chemistry; Cost benefit analysis; Energy utilization; Parallel architectures; Additional key word and phrasesparallel architecture; Dissipative particle dynamics; Event-driven; Event-driven approach; Event-driven processing; Event-triggered system; Key words; Massively micro-parallel system; Parallel system; Partially ordered events; Dissipative particle dynamics
A Distributed-GPU Deep Reinforcement Learning System for Solving Large Graph Optimization Problems,2023,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164241019&doi=10.1145%2f3589188&partnerID=40&md5=c83a8dced0b3f9e583adb9db164b7cea,"Graph optimization problems (such as minimum vertex cover, maximum cut, traveling salesman problems) appear in many fields including social sciences, power systems, chemistry, and bioinformatics. Recently, deep reinforcement learning (DRL) has shown success in automatically learning good heuristics to solve graph optimization problems. However, the existing RL systems either do not support graph RL environments or do not support multiple or many GPUs in a distributed setting. This has compromised the ability of reinforcement learning in solving large-scale graph optimization problems due to lack of parallelization and high scalability. To address the challenges of parallelization and scalability, we develop RL4GO, a high-performance distributed-GPU DRL framework for solving graph optimization problems. RL4GO focuses on a class of computationally demanding RL problems, where both the RL environment and policy model are highly computation intensive. Traditional reinforcement learning systems often assume either the RL environment is of low time complexity or the policy model is small. In this work, we distribute large-scale graphs across distributed GPUs and use the spatial parallelism and data parallelism to achieve scalable performance. We compare and analyze the performance of the spatial parallelism and data parallelism and show their differences. To support graph neural network (GNN) layers that take as input data samples partitioned across distributed GPUs, we design parallel mathematical kernels to perform operations on distributed 3D sparse and 3D dense tensors. To handle costly RL environments, we design a parallel graph environment to scale up all RL-environment-related operations. By combining the scalable GNN layers with the scalable RL environment, we are able to develop high-performance RL4GO training and inference algorithms in parallel. Furthermore, we propose two optimization techniques - replay buffer on-the-fly graph generation and adaptive multiple-node selection - to minimize the spatial cost and accelerate reinforcement learning. This work also conducts in-depth analyses of parallel efficiency and memory cost and shows that the designed RL4GO algorithms are scalable on numerous distributed GPUs. Evaluations on large-scale graphs show that (1) RL4GO training and inference can achieve good parallel efficiency on 192 GPUs, (2) its training time can be 18 times faster than the state-of-the-art Gorila distributed RL framework [34], and (3) its inference performance achieves a 26 times improvement over Gorila.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesParallel machine learning system; high performance computing,Deep learning; Efficiency; Graph neural networks; Graphics processing unit; Inference engines; Learning algorithms; Learning systems; Multilayer neural networks; Optimization; Scalability; Additional key word and phrasesparallel machine learning system; Graph optimization problems; High performance computing; Key words; Large-scales; Machine learning systems; Performance; Performance computing; Reinforcement learning systems; Reinforcement learnings; Reinforcement learning
A Heterogeneous Parallel Computing Approach Optimizing SpTTM on CPU-GPU via GCN,2023,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164243651&doi=10.1145%2f3584373&partnerID=40&md5=14be7cd3c2368d9b325bcbc9ae813ca3,"Sparse Tensor-Times-Matrix (SpTTM) is the core calculation in tensor analysis. The sparse distributions of different tensors vary greatly, which poses a big challenge to designing efficient and general SpTTM. In this paper, we describe SpTTM on CPU-GPU heterogeneous hybrid systems and give a parallel execution strategy for SpTTM in different sparse formats. We analyze the theoretical computer powers and estimate the number of tasks to achieve the load balancing between the CPU and the GPU of the heterogeneous systems. We discuss a method to describe tensor sparse structure by graph structure and design a new graph neural network SPT-GCN to select a suitable tensor sparse format. Furthermore, we perform extensive experiments using real datasets to demonstrate the advantages and efficiency of our proposed input-aware slice-wise SpTTM. The experimental results show that our input-aware slice-wise SpTTM can achieve an average speedup of 1.310 × compared to ParTI! library on a CPU-GPU heterogeneous system.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesCPU-GPU heterogeneous systems; format selection; GCN; parallel computing; SpTTM,Graph neural networks; Graphics processing unit; Tensors; Additional key word and phrasescpu-GPU heterogeneous system; Format selection; GCN; Heterogeneous parallel computing; Heterogeneous systems; Key words; Parallel com- puting; Sparse tensor-time-matrix; Sparse tensors; Time matrix; Hybrid systems
"MCSH, a Lock with the Standard Interface",2023,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164246075&doi=10.1145%2f3584696&partnerID=40&md5=a583aa23cdadc6f712b2f94d6a755a0a,"The MCS lock of Mellor-Crummey and Scott (1991), 23 pages. is a very efficient first-come first-served mutual-exclusion algorithm that uses the atomic hardware primitives fetch-and-store and compare-and-swap. However, it has the disadvantage that the calling thread must provide a pointer to an allocated record. This additional parameter violates the standard locking interface, which has only the lock as a parameter. Hence, it is impossible to switch to MCS without editing and recompiling an application that uses locks. This article provides a variation of MCS with the standard interface, which remains FCFS, called MCSH. One key ingredient is to stack allocate the necessary record in the acquire procedure of the lock, so its life-time only spans the delay to enter a critical section. A second key ingredient is communicating the allocated record between the acquire and release procedures through the lock to maintain the standard locking interface. Both of these practices are known to practitioners, but our solution combines them in a unique way. Furthermore, when these practices are used in prior papers, their correctness is often argued informally. The correctness of MCSH is verified rigorously with the proof assistant PVS, and experiments are run to compare its performance with MCS and similar locks.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesConcurrency; atomicity; critical section; efficiency; mutual exclusion,Additional key word and phrasesconcurrency; Atomicity; Compare and swaps; Critical sections; First come first served; Key words; Mutual exclusion algorithms; Mutual exclusions; Recompiling; Standard interface
Performance Implication of Tensor Irregularity and Optimization for Distributed Tensor Decomposition,2023,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164245981&doi=10.1145%2f3580315&partnerID=40&md5=2cce951e8fe8034d71a33126f6e8f5f3,"Tensors are used by a wide variety of applications to represent multi-dimensional data; tensor decompositions are a class of methods for latent data analytics, data compression, and so on. Many of these applications generate large tensors with irregular dimension sizes and nonzero distribution. CANDECOMP/PARAFAC decomposition (Cpd) is a popular low-rank tensor decomposition for discovering latent features. The increasing overhead on memory and execution time of Cpd for large tensors requires distributed memory implementations as the only feasible solution. The sparsity and irregularity of tensors hinder the improvement of performance and scalability of distributed memory implementations. While previous works have been proved successful in Cpd for tensors with relatively regular dimension sizes and nonzero distribution, they either deliver unsatisfactory performance and scalability for irregular tensors or require significant time overhead in preprocessing. In this work, we focus on medium-grained tensor distribution to address their limitation for irregular tensors. We first thoroughly investigate through theoretical and experimental analysis. We disclose that the main cause of poor Cpd performance and scalability is the imbalance of multiple types of computations and communications and their tradeoffs; and sparsity and irregularity make it challenging to achieve their balances and tradeoffs. Irregularity of a sparse tensor is categorized based on two aspects: very different dimension sizes and a non-uniform nonzero distribution. Typically, focusing on optimizing one type of load imbalance causes other ones more severe for irregular tensors. To address such challenges, we propose irregularity-aware distributed Cpd that leverages the sparsity and irregularity information to identify the best tradeoff between different imbalances with low time overhead. We materialize the idea with two optimization methods: the prediction-based grid configuration and matrix-oriented distribution policy, where the former forms the global balance among computations and communications, and the latter further adjusts the balances among computations. The experimental results show that our proposed irregularity-aware distributed Cpd is more scalable and outperforms the medium- and fine-grained distributed implementations by up to 4.4 × and 11.4 × on 1,536 processors, respectively. Our optimizations support different sparse tensor formats, such as compressed sparse fiber (CSF), coordinate (COO), and Hierarchical Coordinate (HiCOO), and gain good scalability for all of them.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSparse tensor; CPD; irregularity; tensor decomposition,Commerce; Data Analytics; Memory architecture; Scalability; Additional key word and phrasessparse tensor; CANDECOMP/PARAFAC; CPD; Distributed Memory; Irregularity; Key words; Optimisations; PARAFAC decomposition; Performance and scalabilities; Tensor decomposition; Tensors
GreenMD: Energy-efficient Matrix Decomposition on Heterogeneous Multi-GPU Systems,2023,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164244936&doi=10.1145%2f3583590&partnerID=40&md5=cc088824f22a439cb70c4a798a11e98c,"The current trend of performance growth in HPC systems is accompanied by a massive increase in energy consumption. In this article, we introduce GreenMD, an energy-efficient framework for heterogeneous systems for LU factorization utilizing multi-GPUs. LU factorization is a crucial kernel from the MAGMA library, which is highly optimized. Our aim is to apply DVFS to this application by leveraging slacks intelligently on both CPUs and multiple GPUs. To predict the slack times, accurate performance models are developed separately for both CPUs and GPUs based on the algorithmic knowledge and manufacturer's specifications. Since DVFS does not reduce static energy consumption, we also develop undervolting techniques for both CPUs and GPUs. Reducing voltage below threshold values may give rise to errors; hence, we extract the minimum safe voltages (VsafeMin) for the CPUs and GPUs utilizing a low overhead profiling phase and apply them before execution. It is shown that GreenMD improves the CPU, GPU, and total energy about 59%, 21%, and 31%, respectively, while delivering similar performance to the state-of-the-art linear algebra MAGMA library.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesParallel computation; energy efficiency; heterogeneous multi GPUs systems; high performance applications,Energy efficiency; Energy utilization; Factorization; Graphics processing unit; Lower-upper decomposition; Additional key word and phrasesparallel computation; Energy efficient; Energy-consumption; Heterogeneous multi GPU system; High performance applications; Key words; LU factorization; Matrix decomposition; Multi-GPU Systems; Performance; Program processors
Non-overlapping High-accuracy Parallel Closure for Compact Schemes: Application in Multiphysics and Complex Geometry,2023,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153200791&doi=10.1145%2f3580005&partnerID=40&md5=5bfcdc138dd90bb0f34b48216065f764,"Compact schemes are often preferred in performing scientific computing for their superior spectral resolution. Error-free parallelization of a compact scheme is a challenging task due to the requirement of additional closures at the inter-processor boundaries. Here, sources of the error due to sub-domain boundary closures for the compact schemes are analyzed with global spectral analysis. A high-accuracy parallel computing strategy devised in ""A high-accuracy preserving parallel algorithm for compact schemes for DNS. ACM Trans. Parallel Comput. 7, 4, 1-32 (2020)""systematically eliminates error due to parallelization and does not require overlapping points at the sub-domain boundaries. This closure is applicable for any compact scheme and is termed here as non-overlapping high-accuracy parallel (NOHAP) sub-domain boundary closure. In the present work, the advantages of the NOHAP closure are shown with the model convection equation and by solving the compressible Navier-Stokes equation for three-dimensional Rayleigh-Taylor instability simulations involving multiphysics dynamics and high Reynolds number flow past a natural laminar flow airfoil using a body-conforming curvilinear coordinate system. Linear scalability of the NOHAP closure is shown for the large-scale simulations using up to 19,200 processors.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",DNS; Global spectral analysis; natural laminar flow airfoil; parallel compact schemes; Rayleigh-Taylor instability; sub-domain closure schemes,Errors; Laminar flow; Navier Stokes equations; Parallel flow; Parallel processing systems; Rayleigh scattering; Reynolds number; Spectrum analysis; Compact schemes; DNS; Global spectral analyse; Laminar flow airfoils; Natural laminar flow airfoil; Natural-laminar flow; Parallel compact scheme; Rayleigh-Taylor instabilities; Sub-domain closure scheme; Subdomain; Multiphysics
Performance Analysis and Optimal Node-aware Communication for Enlarged Conjugate Gradient Methods,2023,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152632301&doi=10.1145%2f3580003&partnerID=40&md5=8ee75de2bb8781e7a97050ca0c72c03e,"Krylov methods are a key way of solving large sparse linear systems of equations but suffer from poor strong scalability on distributed memory machines. This is due to high synchronization costs from large numbers of collective communication calls alongside a low computational workload. Enlarged Krylov methods address this issue by decreasing the total iterations to convergence, an artifact of splitting the initial residual and resulting in operations on block vectors. In this article, we present a performance study of an enlarged Krylov method, Enlarged Conjugate Gradients (ECG), noting the impact of block vectors on parallel performance at scale. Most notably, we observe the increased overhead of point-to-point communication as a result of denser messages in the sparse matrix-block vector multiplication kernel. Additionally, we present models to analyze expected performance of ECG, as well as motivate design decisions. Most importantly, we introduce a new point-to-point communication approach based on node-aware communication techniques that increases efficiency of the method at scale.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",collectives; communication; node-aware; Parallel; sparse matrix,Electrocardiography; Linear systems; Matrix algebra; Block vectors; Collective; Conjugate-gradient method; Krylov method; Large sparse linear systems; Node-aware; Parallel; Performances analysis; Point-to-point communication; Sparse matrices; Conjugate gradient method
Efficient Distributed Matrix-free Multigrid Methods on Locally Refined Meshes for FEM Computations,2023,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152632538&doi=10.1145%2f3580314&partnerID=40&md5=d1b78f087168190a8b6360ec348bdc9e,"This work studies three multigrid variants for matrix-free finite-element computations on locally refined meshes: geometric local smoothing, geometric global coarsening (both h-multigrid), and polynomial global coarsening (a variant of p-multigrid). We have integrated the algorithms into the same framework - the open source finite-element library deal.II - , which allows us to make fair comparisons regarding their implementation complexity, computational efficiency, and parallel scalability as well as to compare the measurements with theoretically derived performance metrics. Serial simulations and parallel weak and strong scaling on up to 147,456 CPU cores on 3,072 compute nodes are presented. The results obtained indicate that global-coarsening algorithms show a better parallel behavior for comparable smoothers due to the better load balance, particularly on the expensive fine levels. In the serial case, the costs of applying hanging-node constraints might be significant, leading to advantages of local smoothing, even though the number of solver iterations needed is slightly higher. When using p- and h-multigrid in sequence (hp-multigrid), the results indicate that it makes sense to decrease the degree of the elements first from a performance point of view due to the cheaper transfer.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",finite-element computations; linear solvers; matrix-free methods; Multigrid,Computational efficiency; Finite element method; Matrix algebra; Ostwald ripening; Coarsenings; Distributed matrix; Finite element computations; Linear solver; Local smoothing; Matrix free; Matrix-free methods; Multigrid methods; Multigrids; Work study; Coarsening
Tridigpu: A GPU Library for Block Tridiagonal and Banded Linear Equation Systems,2023,ACM Transactions on Parallel Computing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152630144&doi=10.1145%2f3580373&partnerID=40&md5=432abd19e89249c4316aa2822beade40,"In this article, we present a CUDA library with a C API for solving block cyclic tridiagonal and banded systems on one GPU. The library can process block tridiagonal systems with block sizes from 1 × 1 (scalar) to 4 × 4 and banded systems with up to four sub- and superdiagonals. For the compute-intensive block size cases and cases with many right-hand sides, we write out an explicit factorization to memory; however, for the scalar case, the fastest approach is to only output the coarse system and recompute the factorization. Prominent features of the library are (scaled) partial pivoting for improved numeric stability; highest-performance kernels, which completely utilize GPU memory bandwidth; and support for multiple sparse or dense right-hand side and solution vectors. The additional memory consumption is only 5% of the original tridiagonal system, which enables the solution of systems up to GPU memory size. The performance of the state-of-the-art scalar tridiagonal solver of cuSPARSE is outperformed by factor 5 for large problem sizes of 225 unknowns, on a GeForce RTX 2080 Ti.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesTridiagonal solver; banded solver; CUDA; gbsv; GPUs; gtsv,Factorization; Linear equations; Program processors; Additional key word and phrasestridiagonal solv; Banded solv; Banded systems; Block sizes; CUDA; Gbsv; Gtsv; Key words; Performance; Tridiagonal; Graphics processing unit
