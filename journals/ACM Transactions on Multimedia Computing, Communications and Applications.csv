Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
Improving concept-based image retrieval with training weights computed from tags,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954323150&doi=10.1145%2f2790230&partnerID=40&md5=c6918f61b2f53a8f3c4a0a6464b066fc,"This article presents a novel approach to training classifiers for concept detection using tags and a variant of Support Vector Machine that enables the usage of training weights per sample. Combined with an appropriate tag weighting mechanism, more relevant samples play a more important role in the calibration of the final concept-detector model. We propose a complete, automated framework that (i) calculates relevance scores for each image-concept pair based on image tags, (ii) transforms the scores into relevance probabilities and automatically annotates each image according to this probability, (iii) transforms either the relevance scores or the probabilities into appropriate training weights and finally, (iv) incorporates the training weights and the visual features into a Fuzzy Support Vector Machine classifier to build the concept-detector model. The framework can be applied to online public collections, by gathering a large pool of diverse images, and using the calculated probability to select a training set and the associated training weights. To evaluate our argument, we experiment on two large annotated datasets. Experiments highlight the retrieval effectiveness of the proposed approach. Furthermore, experiments with various levels of annotation error show that using weights derived from tags significantly increases the robustness of the resulting concept detectors. Copyright © 2015 ACM.",Concept-based image retrieval; Fuzzy support vector machine; Weighted training sample,Probability; Support vector machines; Annotated datasets; Annotation errors; Concept detection; Concept-based; Detector modeling; Fuzzy support vector machines; Retrieval effectiveness; Training sample; Image retrieval
Opinion question answering by sentiment clip localization,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954312828&doi=10.1145%2f2818711&partnerID=40&md5=4411fd9c7f65661a474d797d141a369d,"This article considers multimedia question answering beyond factoid and how-to questions. We are interested in searching videos for answering opinion-oriented questions that are controversial and hotly debated. Examples of questions include ""Should Edward Snowden be pardoned?"" and ""Obamacare-unconstitutional or not?"". These questions often invoke emotional response, either positively or negatively, hence are likely to be better answered by videos than texts, due to the vivid display of emotional signals visible through facial expression and speaking tone. Nevertheless, a potential answer of duration 60s may be embedded in a video of 10min, resulting in degraded user experience compared to reading the answer in text only. Furthermore, a text-based opinion question may be short and vague, while the video answers could be verbal, less structured grammatically, and noisy because of errors in speech transcription. Direct matching of words or syntactic analysis of sentence structure, such as adopted by factoid and how-to question-answering, is unlikely to find video answers. The first problem, the answer localization, is addressed by audiovisual analysis of the emotional signals in videos for locating video segments likely expressing opinions. The second problem, questions and answers matching, is tackled by a deep architecture that nonlinearly matches text words in questions and speeches in videos. Experiments are conducted on eight controversial topics based on questions crawled from Yahoo! Answers and Internet videos from YouTube. Copyright © 2015 ACM.",Multimedia question answering; Multimodality sentiment analysis; Opinion clip localization,Speech transmission; Syntactics; Transcription; Audiovisual analysis; Controversial topics; Emotional response; Opinion clip localization; Question Answering; Sentence structures; Sentiment analysis; Speech transcriptions; Natural language processing systems
Visual comfort for stereoscopic 3D by using motion sensors on 3D mobile devices,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946547789&doi=10.1145%2f2808211&partnerID=40&md5=759435d51640d1c2f062fc77bfb942fd,"Advanced 3D mobile devices attract a lot of attentions for 3D visualization nowadays. Stereoscopic images and video taken from the 3D mobile devices are uncomfortable for 3D viewing experiences due to the limited hardware for stereoscopic 3D stabilization. The existing stereoscopic 3D stabilization methods are computationally inefficient for the 3D mobile devices. In this article, we point out that this critical issue deteriorates the 3D viewing experiences on the 3D mobile devices. To improve visual comfort, we propose an efficient and effective algorithm to stabilize the stereoscopic images and video for the 3D mobile devices. To rectify the video jitter, we use the gyroscope and accelerometer embedded on the mobile devices to obtain the geometry information of the cameras. Using a different method than video-content-based motion estimation, our algorithm based on the gyroscope and acceleration data can achieve higher accuracy to effectively stabilize the video. Therefore, our approach is robust in video stabilization even under poor lighting and substantial foreground motion. Our algorithm outperforms previous approaches in not only smaller running time but also the better comfort of the stereoscopic 3D visualization for the 3D mobile devices. © 2015 ACM.",Mobile devices; Stereoscopic 3d,Gyroscopes; Image enhancement; Mobile devices; Motion estimation; Stabilization; Stereo image processing; Visualization; Smartphones; Acceleration data; Effective algorithms; Foreground motions; Geometry information; Stereoscopic 3d; Stereoscopic 3D visualization; Stereoscopic image; Video stabilization; 3D Visualization; Three dimensional computer graphics
ShotVis: Smartphone-Based visualization of OCR information from images,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946544711&doi=10.1145%2f2808210&partnerID=40&md5=044d25bbae85eacffb659a2a7a66c908,"While visualization has been widely used as a data presentation tool in both desktop and mobile devices, the rapid visualization of information from images is still underexplored. In this work, we present a smartphone image acquisition and visualization approach for text-based data. Our prototype, ShotVis, takes images of text captured from mobile devices and extracts information for visualization. First, scattered characters in the text are processed and interactively reformulated to be stored as structured data (i.e., tables of numbers, lists of words, sentences). From there, ShotVis allows users to interactively bind visual forms to the underlying data and produce visualizations of the selected forms through touch-based interactions. In this manner, ShotVis can quickly summarize text from images into word clouds, scatterplots, and various other visualizations all through a simple click of the camera. In this way, ShotVis facilitates the interactive exploration of text data captured via cameras in smartphone devices. To demonstrate our prototype, several case studies are presented along with one user study to demonstrate the effectiveness of our approach. © 2015 ACM.",Cyber-physical interaction; Data visualization; Interaction; Smartphone; Touch-based interface; Wearable computing,Cameras; Smartphones; Visualization; Cyber physicals; Data presentation; Interaction; Interactive exploration; Touch based interactions; Touch-based interface; Visualization of information; Wearable computing; Data visualization
Correspondence autoencoders for cross-modal retrieval,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946574915&doi=10.1145%2f2808205&partnerID=40&md5=3d06a9374946ac904990650f3d670d86,"This article considers the problem of cross-modal retrieval, such as using a text query to search for images and vice-versa. Based on different autoencoders, several novel models are proposed here for solving this problem. These models are constructed by correlating hidden representations of a pair of autoencoders. A novel optimal objective, which minimizes a linear combination of the representation learning errors for each modality and the correlation learning error between hidden representations of two modalities, is used to train the model as a whole. Minimizing the correlation learning error forces the model to learn hidden representations with only common information in different modalities, while minimizing the representation learning error makes hidden representations good enough to reconstruct inputs of each modality. To balance the two kind of errors induced by representation learning and correlation learning,we set a specific parameter in our models. Furthermore, according to the modalities the models attempt to reconstruct they are divided into two groups. One group including three models is named multimodal reconstruction correspondence autoencoder since it reconstructs both modalities. The other group including two models is named unimodal reconstruction correspondence autoencoder since it reconstructs a single modality. The proposed models are evaluated on three publicly available datasets. And our experiments demonstrate that our proposed correspondence autoencoders perform significantly better than three canonical correlation analysis based models and two popular multimodal deep models on cross-modal retrieval tasks. © 2015 ACM.",Autoencoder; Cross-modal; Deep learning; Image and text; Retrieval,Deep learning; Errors; Information retrieval; Smartphones; Auto encoders; Canonical correlation analysis; Cross-modal; Image and text; Learning error; Linear combinations; Retrieval; Three models; Learning systems
From 3D sensing to printing: A survey,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946043444&doi=10.1145%2f2818710&partnerID=40&md5=00c987bc796f9dead2ac295791aa4acd,"Three-dimensional (3D) sensing and printing technologies have reshaped our worldinrecent years.Inthis article, a comprehensive overview of techniques related to the pipeline from 3D sensing to printing is provided. We compare the latest 3D sensors and 3D printers and introduce several sensing, postprocessing, and printing techniques available from both commercial deployments and published research. In addition, we demonstrate several devices, software, and experimental results of our related projects to further elaborate details of this process. A case study is conducted to further illustrate the possible tradeoffs during the process of this pipeline. Current progress, future research trends, and potential risks of 3D technologies are also discussed. © 2015 ACM 1551-6857/2015/10-ART27 $15.00.",3D model reconstruction; 3D printers; 3D sensing technologies,3D modeling; Pipelines; Printing presses; Three dimensional computer graphics; 3-D sensing; 3D model reconstruction; Commercial deployment; Potential risks; Printing techniques; Printing technologies; Research trends; Threedimensional (3-d); 3D printers
SAfeDJ: A crowd-cloud codesign approach to situation-aware music delivery for drivers,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946557163&doi=10.1145%2f2808201&partnerID=40&md5=38bb857243b9f3d5e59444198c70ba73,"Driving is an integral part of our everyday lives, but it is also a time when people are uniquely vulnerable. Previous research has demonstrated that not only does listening to suitable music while driving not impair driving performance, but it could lead to an improved mood and a more relaxed body state, which could improve driving performance and promote safe driving significantly. In this article, we propose SAfeDJ, a smartphone-based situation-aware music recommendation system, which is designed to turn driving into a safe and enjoyable experience. SAfeDJ aims at helping drivers to diminish fatigue and negative emotion. Its design is based on novel interactive methods, which enable in-car smartphones to orchestrate multiple sources of sensing data and the drivers' social context, in collaboration with cloud computing to form a seamless crowdsensing solution. This solution enables different smartphones to collaboratively recommend preferable music to drivers according to each driver's specific situations in an automated and intelligent manner. Practical experiments of SAfeDJ have proved its effectiveness in music-mood analysis, and moodfatigue detections of drivers with reasonable computation and communication overheads on smartphones. Also, our user studies have demonstrated that SAfeDJ helps to decrease fatigue degree and negative mood degree of drivers by 49.09% and 36.35%, respectively, compared to traditional smartphone-based music player under similar driving situations. © 2015 ACM.",Cloud; Context; Crowdsensing; Driving; Music mood; Smartphones,Automobile drivers; Clouds; Communication overheads; Context; Crowdsensing; Driving; Driving performance; Music mood; Music mood analysis; Music Recommendation System; Smartphones
Emotion recognition during speech using dynamics of multiple regions of the face,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946550788&doi=10.1145%2f2808204&partnerID=40&md5=2c29e66879ffab99c248c7b29922057d,"The need for human-centered, affective multimedia interfaces has motivated research in automatic emotion recognition. In this article, we focus on facial emotion recognition. Specifically, we target a domain in which speakers produce emotional facial expressions while speaking. The main challenge of this domain is the presence of modulations due to both emotion and speech. For example, an individual's mouth movement may be similar when he smiles and when he pronounces the phoneme /IY/, as in ""cheese"". The result of this confusion is a decrease in performance of facial emotion recognition systems. In our previous work, we investigated the joint effects of emotion and speech on facial movement. We found that it is critical to employ proper temporal segmentation and to leverage knowledge of spoken content to improve classification performance. In the current work, we investigate the temporal characteristics of specific regions of the face, such as the forehead, eyebrow, cheek, and mouth. We present methodology that uses the temporal patterns of specific regions of the face in the context of a facial emotion recognition system. We test our proposed approaches on two emotion datasets, the IEMOCAP and SAVEE datasets. Our results demonstrate that the combination of emotion recognition systems based on different facial regions improves overall accuracy compared to systems that do not leverage different characteristics of individual regions. © 2015 ACM.",Emotion; Emotion recognition; Facial movement; Segmentation,Face recognition; Image segmentation; Knowledge management; Pattern recognition systems; Smartphones; Automatic emotion recognition; Classification performance; Emotion; Emotion recognition; Facial movements; Multimedia interfaces; Temporal characteristics; Temporal segmentations; Speech recognition
Enabling context-aware indoor augmented reality via Smartphone sensing and vision tracking,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946546896&doi=10.1145%2f2808208&partnerID=40&md5=0dbbed9ea465417acb683ded8e1d6ea4,"Augmented reality (AR) aims to render the world that users see and overlay information that reflects the real physical dynamics. The digital view could be potentially projected near the Point-of-Interest (POI) in a way that makes the virtual view attached to the POI even when the camera moves. Achieving smooth support for movements is a subject of extensive studies. One of the key problems is where the augmented information should be added to the field of vision in real time. Existing solutions either leverage GPS location for rendering outdoor AR views (hundreds of kilometers away) or rely on image markers for small-scale presentation (only for the marker region). To realize AR applications under various scales and dynamics, we propose a suite of algorithms for fine-grained AR view tracking to improve the accuracy of attitude and displacement estimation, reduce the drift, eliminate the marker, and lower the computation cost. Instead of requiring extremely high, accurate, absolute locations, we propose multimodal solutions according to mobility levels without additional hardware requirement. Experimental results demonstrate significantly less error in projecting and tracking the AR view. These results are expected to make users excited to explore their surroundings with enriched content. © 2015 ACM.",Attitude; Augmented reality; Displacement; INS; Smartphone,Indium; Rendering (computer graphics); Smartphones; Augmented reality; Attitude; Computation costs; Displacement; Displacement estimation; Physical dynamics; Point of interest; Smartphone sensing; Vision tracking; Augmented reality; Smartphones
PSNController: An unwanted content control system in pervasive social networking based on trust management,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946561244&doi=10.1145%2f2808206&partnerID=40&md5=08c512a16f081fcbc11d9ed582fd3bd9,"Pervasive social networking (PSN) supports online and instant social activities and communications in a universal and pervasive manner on the basis of heterogeneous networks. However, at the same time, when mobile users expect useful and valuable contents via PSN, they may also receive unwanted, unexpected, or even malicious contents. These contents may intrude user devices, occupy device memories, and irritate mobile users. Unwanted content control in PSN has become a crucial issue that impacts the success of PSN usage. Nowadays, the literature still lacks a robust and generic unwanted content control system that can be practically applied. In this article, we present the design and implementation of PSNController, an unwanted content control system in PSN based on trust management. We evaluate the system performance under a variety of intrusions and attacks. The result shows the system is effective with regard to accuracy, efficiency, and robustness. It can control unwanted contents in PSN according to trust evaluation. We further study user acceptance on PSNController prototype system based on a small-scale user study. We receive sound user feedback on PSNController with regard to perceived ease of use, perceived usefulness, interface design, playfulness, and acceptance attitude. © 2015 ACM.",Performance evaluation; Social networking; Trust management; Unwanted content control,Control systems; Heterogeneous networks; Smartphones; Content control; Design and implementations; Perceived ease of use; Perceived usefulness; Performance evaluations; Pervasive social networkings; Social activities; Trust management; Performance evaluation; Social networking (online)
Investigating on-screen gamepad designs for smartphone-controlled video games,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946565615&doi=10.1145%2f2808202&partnerID=40&md5=04c7ae060adbeec0ecd207df49d1609b,"On-screen gamepads are increasingly used as controllers for video games on distant screens, yet lack the typical tactile feedback known from hardware controllers. We conducted a comparative lab study to investigate four smartphone gamepads inspired by traditional game controllers and mobile game controls (directional buttons, directional pad, floating joystick, tilt control). The study consisted of both completing a formal control test as well as controlling two popular video games of different genres (Pac-Man and Super Mario Bros.). The results indicate that the directional buttons require the most attention of the user, however, work precisely for direction-restricted navigational tasks. Directional pad and joystick showed a similar performance, yet they encourage drifting and unintended operations when the user is focused on the remote screen. While currently unfamiliar to many users, the floating joystick can reduce the glances at the device. Tilt turned out to be not sufficiently precise and quick for the investigated tasks. The article concludes with derived design guidelines with easily realizable measures for typical contexts such as casual gaming at home or spontaneous gaming on public displays. © 2015 ACM.",Game controller; Gamepad; Smartphone; Touchscreen,Fasteners; Human computer interaction; Smartphones; Touch screens; Formal controls; Game controller; Gamepad; Mobile games; Public display; Tactile feedback; Tilt control; Video game; Controllers
Launching an efficient participatory sensing campaign: A smart mobile device-based approach,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946553463&doi=10.1145%2f2808198&partnerID=40&md5=57a793a32b960014f25812bcce4f25c8,"Participatory sensing is a promising sensing paradigm that enables collection, processing, dissemination and analysis of the phenomena of interest by ordinary citizens through their handheld sensing devices. Participatory sensing has huge potential in many applications, such as smart transportation and air quality monitoring. However, participants may submit low-quality, misleading, inaccurate, or even malicious data if a participatory sensing campaign is not launched effectively. Therefore, it has become a significant issue to establish an efficient participatory sensing campaign for improving the data quality. This article proposes a novel five-tier framework of participatory sensing and addresses several technical challenges in this proposed framework including: (1) optimized deployment of data collection points (DC-points); and (2) efficient recruitment strategy of participants. Toward this end, the deployment of DC-points is formulated as an optimization problem with maximum utilization of sensor and then a Wise-Dynamic DC-points Deployment (WD3) algorithm is designed for high-quality sensing. Furthermore, to guarantee the reliable sensing data collection and communication, a trajectory-based strategy for participant recruitment is proposed to enable campaign organizers to identify well-suited participants for data sensing based on a joint consideration of temporal availability, trust, and energy. Extensive experiments and performance analysis of the proposed framework and associated algorithms are conducted. The results demonstrate that the proposed algorithm can achieve a good sensing coverage with a smaller number of DC-points, and the participants that are termed as social sensors are easily selected, to evaluate the feasibility and extensibility of the proposed recruitment strategies. © 2015 ACM.",Deployment; Dta; Participatory sensing; Recruitment; Tensor; Trajectory,Air quality; Differential thermal analysis; Human computer interaction; Mobile devices; Tensors; Trajectories; Optimization; Smartphones; Air quality monitoring; Deployment; Optimized deployments; Participatory Sensing; Recruitment; Recruitment strategies; Smart transportations; Temporal availability; Optimization problems; Data acquisition
Introduction to: Special issue on extended best papers from ACM multimedia 2014,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946559619&doi=10.1145%2f2820400&partnerID=40&md5=eefc98ad8fb1c6ca347a3ec12c4336f5,[No abstract available],,
Context-Aware photography learning for smart mobile devices,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946569487&doi=10.1145%2f2808199&partnerID=40&md5=240569252d6ccb84218541426424dcd0,"In this work we have developed a photography model based on machine learning which can assist a user in capturing high quality photographs. As scene composition and camera parameters play a vital role in aesthetics of a captured image, the proposed method addresses the problem of learning photographic composition and camera parameters. Further, we observe that context is an important factor from a photography perspective, we therefore augment the learning with associated contextual information. The proposed method utilizes publicly available photographs along with social media cues and associated metainformation in photography learning. We define context features based on factors such as time, geolocation, environmental conditions and type of image, which have an impact on photography. We also propose the idea of computing the photographic composition basis, eigenrules and baserules, to support our composition learning. The proposed system can be used to provide feedback to the user regarding scene composition and camera parameters while the scene is being captured. It can also recommend position in the frame where people should stand for better composition. Moreover, it also provides camera motion guidance for pan, tilt and zoom to the user for improving scene composition. © 2015 ACM.",Aesthetics; Camera parameters; Composition learning; Context; Photography; Social media,Cameras; Photography; Social networking (online); Learning systems; Smartphones; Aesthetics; Camera parameter; Context; Contextual information; Environmental conditions; High-quality photographs; Photographic composition; Social media; Learning systems; Photography
Automated LINK generation for sensor-enriched Smartphone images,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946554962&doi=10.1145%2f2808209&partnerID=40&md5=5d6c4351c8c0d909415d876b5470f27d,"The ubiquity of the smartphonesmakes them ideal platforms for generating in-situ content. In well-attended events, photos captured by attendees have diverse views that could be subjected to occlusion and abnormal lighting effects that could obscure the view. Such unstructured photo collections also have significant redundancy. Thus, a scene that is partially occluded or has bad contrast in one photo may be captured in another photo, possibly with higher details. We propose an application called Autolink that automatically establishes content-based links between sensor-annotated photos in unstructured photo collections captured using smartphones, such that users could navigate between high-context and high-detail images. This hierarchically structured image collection facilitates the design of applications for navigation and discovery, analytics about user photography patterns, user taste, and content/event popularity. Autolink includes a framework that constructs this hierarchy efficiently and with little content-specific training data by combining photo content processing with associated sensor logs obtained from multiple participants. We evaluated the performance of Autolink on two real-world sensor tagged photo datasets. The result shows that Autolink is able to efficiently cluster photos at 20 times faster than candidate algorithms, into the appropriate hierarchy with at least 70% precision and 37% better recall than candidate algorithms. © 2015 ACM.",Collaboration; Hyperlinking; Images; Smartphone,Computer networks; Hardware; Automated link; Collaboration; Content processing; Hyperlinking; Images; Lighting effects; Photo collections; Structured images; Smartphones
Scalable object retrieval with compact image representation from generic object regions,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946049902&doi=10.1145%2f2818708&partnerID=40&md5=4a6c315b6de6d30334c2ee4bc66625d4,"In content-based visual object retrieval, image representation is one of the fundamental issues in improving retrieval performance. Existing works adopt either local SIFT-like features or holistic features, and may suffer sensitivity to noise or poor discrimination power. In this article, we propose a compact representation for scalable object retrieval from few generic object regions. The regions are identified with a general object detector and are described with a fusion of learning-based features and aggregated SIFT features. Further, we compress feature representation in large-scale image retrieval scenarios. We evaluate the performance of the proposed method on two public ground-truth datasets, with promising results. Experimental results on a million-scale image database demonstrate superior retrieval accuracy with efficiency gain in both computation and memory usage. © 2015 ACM 1551-6857/2015/10-ART29 $15.00.",Compact image representation; Image retrieval,Image enhancement; Knowledge representation; Object detection; Compact representation; Feature representation; Holistic features; Image representations; Object detectors; Object retrieval; Retrieval accuracy; Retrieval performance; Image retrieval
Multiview image block compressive sensing with joint multiphase decoding for visual sensor network,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946038682&doi=10.1145%2f2818712&partnerID=40&md5=06c83319830b57782d0325fa5381201c,"In this article,a multiview image compression framework, which involves the use of Block-based Compressive Sensing (BCS) and Joint Multiphase Decoding (JMD), is proposed for a Visual Sensor Network (VSN). In the proposed framework, one of the sensor nodes is configured to serve as the reference node, the others as nonreference nodes. The images are encoded independently using the BCS to produce two observed measurements that are transmitted to the host workstation. In this case, the nonreference nodes always encoded the images (INR) at a lower subrate when compared with the images from the reference nodes (IR). The idea is to improve the reconstruction of INRusing IR. After the two observed measurements are received by the host workstation, they are first decoded independently, then image registration is applied to align IR onto the same plane of INR. The aligned IRis then fused with INR, using wavelets to produce the projected image IP. Subsequently, the difference between the measurements of the IPand INRis calculated. The difference is then decoded and added to IPto produce the final reconstructed INRThe simulation results show that the proposed framework is able to improve the quality of INRon average by 2dB to 3dB at lower subrates when compared with other Compressive Sensing (CS)-based multiview image compression frameworks. © 2015 ACM 1551-6857/2015/10-ART30 $15.00.",Image compression; Wireless sensor networks,Compressed sensing; Decoding; Image enhancement; Sensor nodes; Wireless sensor networks; Block based; Compressive sensing; Measurements of; Multi-view image; Reference nodes; Visual sensor networks; Image compression
QoE-driven rate adaptation heuristic for fair adaptive video streaming,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946065884&doi=10.1145%2f2818361&partnerID=40&md5=40c25882f90add8727645e420438617d,"HTTP Adaptive Streaming (HAS) is quickly becoming the de facto standard for video streaming services. In HAS, each video is temporally segmented and stored in different quality levels. Rate adaptation heuristics, deployed at the video player, allow the most appropriate level to be dynamically requested, based on the current network conditions. It has been shown that today's heuristics underperform when multiple clients consume video at the same time, due to fairness issues among clients. Concretely, this means that different clients negatively influence each other as they compete for shared network resources. In this article, we propose a novel rate adaptation algorithm called FINEAS (Fair In-Network Enhanced Adaptive Streaming), capable of increasing clients' Quality of Experience (QoE) and achieving fairness in a multiclient setting. A key element of this approach is an in-network system of coordination proxies in charge of facilitating fair resource sharing among clients. The strength of this approach is threefold. First, fairness is achieved without explicit communication among clients and thus no significant overhead is introduced into the network. Second, the system of coordination proxies is transparent to the clients, that is, the clients do not need to be aware of its presence. Third, the HAS principle is maintained, as the in-network components only provide the clients with new information and suggestions, while the rate adaptation decision remains the sole responsibility of the clients themselves. We evaluate this novel approach through simulations, under highly variable bandwidth conditions and in several multiclient scenarios. We show how the proposed approach can improve fairness up to 80% compared to state-of-the-art HAS heuristics in a scenario with three networks, each containing 30 clients streaming video at the same time. © 2015 ACM 1551-6857/2015/10-ART28 $15.00.",Experimental evaluation; Fairness; HTTP Adaptive Streaming; Microsoft smooth streaming; Mobile; Quality of experience; Rate adaptation,Bandwidth; HTTP; Quality control; Quality of service; Experimental evaluation; Fairness; Http adaptive streaming; Microsoft smooth streaming; Mobile; Quality of experience (QoE); Rate adaptation; Video streaming
Supporting healthy grocery shopping via mobile augmented reality,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946565786&doi=10.1145%2f2808207&partnerID=40&md5=0e36294642199c8906bba5159d65643f,"Augmented reality (AR) applications have recently become popular on modern smartphones. We explore the effectiveness of this mobile AR technology in the context of grocery shopping, in particular as a means to assist shoppers in making healthier decisions as they decide which grocery products to buy. We construct an AR-assisted mobile grocery-shopping application that makes real-time, customized recommendations of healthy products to users and also highlights products to avoid for various types of health concerns, such as allergies to milk or nut products, low-sodium or low-fat diets, and general caloric intake. We have implemented a prototype of this AR-assisted mobile grocery shopping application and evaluated its effectiveness in grocery store aisles. Our application's evaluation with typical grocery shoppers demonstrates that AR overlay tagging of products reduces the search time to find healthy food items, and that coloring the tags helps to improve the user's ability to quickly and easily identify recommended products, as well as products to avoid. We have evaluated our application's functionality by analyzing the data we collected from 15 in-person actual grocery-shopping subjects and 104 online application survey participants. © 2015 ACM.",Augmented reality; Grocery shopping; Mobile health; Nutrition; Recommendation,mHealth; Nutrition; Petroleum reservoir evaluation; Surveys; Smartphones; Grocery shopping; Grocery stores; Health concerns; Healthy foods; Mobile augmented reality; On-line applications; Recommendation; Search time; Augmented reality
Smartening up the student learning experience with ubiquitous media,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946562561&doi=10.1145%2f2808203&partnerID=40&md5=a78206b552463488a0004de62afcb380,"This article describes how an experimental platform for social, mobile and ubiquitous computing has been used in a wide-ranging longitudinal ""in the wild"" case study of the platform with a set of third-party services. The article outlines some of the relevant aspects of the platform, including built-in support for community formation, for context sensitivity, automated learning and adaptation to the user, and formanagement of privacy and trust relationships. The platform architecture is based on the notion of Cooperating Smart Spaces (CSSs), where a CSS is a partition of the platform corresponding to a single user and distributed over the devices belonging to that user. Three of the case study services were intended for use in a physical environment specifically created to support ubiquitous intelligence; they were highly interactive and used shared screens, voice input and gestural interaction. Another three ubiquitous services were available throughout the university environment as mobile and desktop services. The case study exploited this architecture's ability to integrate multiple novel applications and interface devices and to deliver them flexibly in these different environments. The platform proved to be stable and reliable and the study shows that treating a provider of services and resources (the University) as a CSS is instrumental in enabling the platform to provide this range of services across differing environments. © 2015 ACM.",Mobile computing; Social networks; Ubiquitous computing,Mobile computing; Network architecture; Social networking (online); Social sciences computing; Smartphones; Experimental platform; Mobile and ubiquitous computing; Physical environments; Platform architecture; Student learning experiences; Third party services; Ubiquitous intelligence; University environment; Ubiquitous computing
"Introduction to: Special issue on Smartphone-based interactive technologies, systems, and applications",2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946558068&doi=10.1145%2f2820398&partnerID=40&md5=69d73916469bea3f21f948346f958fa1,[No abstract available],,
Accessing tape music documents on mobile devices,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946571783&doi=10.1145%2f2808200&partnerID=40&md5=fdbccc7880d6a5d2683210b4159d28cc,"The aim of this article is to present and discuss an innovative methodology aimed at accessing digitized copies of historical tape music audio documents; the methodology leverages on the multimedia and multisensory capabilities of mobile devices to provide an unprecedented level of fruition. In addition to the methodology, and stemming from it, we present an actual software application for Android tablet devices. This novel piece of software was designed and developed in a multidisciplinary team involving engineers as well asmusicians, composers, and archivists. The strongest element in our work is the fact that it follows a rigorous process and it is based on the principles of philological awareness; thus, it also takes into consideration the critical points in the musicologist's domain such as (i) the definition of preservation (i.e., master) copy, (ii) the importance of secondary information, (iii) the history of production and transmission of audio documents. © 2015 ACM.",Audio archives; Audio document preservation and access; Design; Mobile computing,Application programs; Audio acoustics; Design; Mobile computing; Smartphones; Audio archives; Audio document preservation and access; Innovative methodologies; Multi-disciplinary teams; Multisensory; Software applications; Audio equipment
CelebrityNet: A social network constructed from large-scale online celebrity images,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940434901&doi=10.1145%2f2801125&partnerID=40&md5=9f6974c14751ba4f4b20b56c2be81de5,"Photos are an important information carrier for implicit relationships. In this article, we introduce an image based social network, called CelebrityNet, built from implicit relationships encoded in a collection of celebrity images. We analyze the social properties reflected in this image-based social network and automatically infer communities among the celebrities. We demonstrate the interesting discoveries of the CelebrityNet. We particularly compare the inferred communities with human manually labeled ones and show quantitatively that the automatically detected communities are highly aligned with that of human interpretation. Inspired by the uniqueness of visual content and tag concepts within each community of the CelebrityNet, we further demonstrate that the constructed social network can serve as a knowledge base for high-level visual recognition tasks. In particular, this social network is capable of significantly improving the performance of automatic image annotation and classification of unknown images. © 2015 ACM.","E.1 [data]: data structure-graphs and networks; Human factors; I.2.10 [artificial intelligence]: vision and scene understanding-representations, data structures, and transforms; I.4.8 [image processing and computer vision]: scene analysis-object recognition; I.5.4 [pattern recognition]: applications-computer vision; J.4 [social and behavioral sciences]: sociology; Multimedia; Photos; Social networks",Artificial intelligence; Behavioral research; Computer vision; Data structures; Human engineering; Image analysis; Image classification; Image enhancement; Knowledge based systems; Object recognition; Sociology; Graphs and networks; Multimedia; Photos; Scene analysis; Social and behavioral science; Vision and scene understanding; Social networking (online)
QoE-based SVC layer dropping in LTE networks using content-aware layer priorities,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940426573&doi=10.1145%2f2754167&partnerID=40&md5=6534c1ba0f867cb723a78e9252f0ca89,"The increasing popularity of mobile video streaming applications has led to a high volume of video traffic in mobile networks. As the base station, for instance, the eNB in LTE networks, has limited physical resources, it can be overloaded by this traffic. This problem can be addressed by using Scalable Video Coding (SVC), which allows the eNB to drop layers of the video streams to dynamically adapt the bitrate. The impact of bitrate adaptation on the Quality of Experience (QoE) for the users depends on the content characteristics of videos. As the current mobile network architectures do not support the eNB in obtaining video content information, QoE optimization schemes with explicit signaling of content information have been proposed. These schemes, however, require the eNB or a specific optimization module to process the video content on the fly in order to extract the required information. This increases the computation and signaling overhead significantly, raising the OPEX for mobile operators. To address this issue, in this article, a content-aware (CA) priority marking and layer dropping scheme is proposed. The CA priority indicates a transmission order for the layers of all transmitted videos across all users, resulting from a comparison of their utility versus rate characteristics. The CA priority values can be determined at the P-GW on the fly, allowing mobile operators to control the priority marking process. Alternatively, they can be determined offline at the video servers, avoiding real-time computation in the core network. The eNB can perform content-aware SVC layer dropping using only the priority values. No additional content processing is required. The proposed scheme is lightweight both in terms of architecture and computation. The improvement in QoE is substantial and very close to the performance obtained with the computation and signaling-intensive QoE optimization schemes. © 2015 ACM.",LTE; Mobile video streaming; Priority marking; QoE; SVC,Mobile telecommunication systems; Network architecture; Quality of service; Static Var compensators; Video recording; Video streaming; Wireless networks; Content information; Mobile video streaming; Optimization module; Optimization scheme; Priority marking; Quality of experience (QoE); Real-time computations; Signaling overheads; Scalable video coding
Combining acoustic and multilevel visual features for music genre classification,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940398869&doi=10.1145%2f2801127&partnerID=40&md5=874b689ebb63e72a7434ab52fac187c2,"Most music genre classification approaches extract acoustic features from frames to capture timbre information, leading to the common framework of bag-of-frames analysis. However, time-frequency analysis is also vital for modeling music genres. This article proposes multilevel visual features for extracting spectrogram textures and their temporal variations. A confidence-based late fusion is proposed for combining the acoustic and visual features. The experimental results indicated that the proposed method achieved an accuracy improvement of approximately 14% and 2% in the world's largest benchmark dataset (MASD) and Unique dataset, respectively. In particular, the proposed approach won the Music Information Retrieval Evaluation eXchange (MIREX) music genre classification contests from 2011 to 2013, demonstrating the feasibility and necessity of combining acoustic and visual features for classifying music genres. © 2015 ACM.",Algorithms; H.3.3 [information storage and retrieval]: information search and retrieval; H.5.5 [information interfaces and presentation]: sound and music computing; Music genre classification; Performance,Algorithms; Computer music; Textures; Accuracy Improvement; Benchmark datasets; H.3.3 [information storage and retrieval]: information search and retrievals; Music genre classification; Music information retrieval; Performance; Sound and music computing; Time frequency analysis; Classification (of information)
Link-aware reconfigurable point-to-point video streaming for mobile devices,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940416798&doi=10.1145%2f2771438&partnerID=40&md5=8ba97dcedca7034cb19d99c7dcb3bbfc,"Even though people of all social standings use current mobile devices in the wide spectrum of purpose from entertainment tools to communication means, some issues with real-time video streaming in hostile wireless environment still exist. In this article, we introduce CoSA, a link-aware real-time video streaming system for mobile devices. The proposed system utilizes a 3D camera to distinguish the region of importance (ROI) and non-ROI region within the video frame. Based on the link-state feedback from the receiver, the proposed system allocates a higher bandwidth for the region that is classified as ROI and a lower bandwidth for non-ROI in the video stream by reducing the video's bit rate. We implemented CoSA in a real test-bed where the IEEE 802.11 is employed as a medium for wireless networking. Furthermore, we verified the effectiveness of the proposed system by conducting a thorough empirical study. The results indicate that the proposed system enables real-time video streaming while maintaining a consistent visual quality by dynamically reconfiguring video coding parameters according to the link quality. © 2015 ACM.",Experimentation; H.5.1 [information interfaces and presentation]: multimedia information systems - video; Link-state feedback; Measurement; Mobile multimedia system; Performance; Reconfigurable video coding,Bandwidth; Cameras; IEEE Standards; Interface states; Measurement; Multimedia systems; State feedback; Experimentation; Mobile multimedia systems; MultiMedia Information Systems; Performance; Reconfigurable; Video streaming
Segmentation of discriminative patches in human activity video,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940416799&doi=10.1145%2f2750780&partnerID=40&md5=ff33ce0150aa05a3bbad338f828b0e5f,"In this article, we present a novel approach to segment discriminative patches in human activity videos. First, we adopt the spatio-temporal interest points (STIPs) to represent significant motion patterns in the video sequence. Then, nonnegative sparse coding is exploited to generate a sparse representation of each STIP descriptor. We construct the feature vector for each video by applying a two-stage sum-pooling and l2-normalization operation. After training a multi-class classifier through the error-correcting code SVM, the discriminative portion of each video is determined as the patch that has the highest confidence while also being correctly classified according to the video category. Experimental results show that the video patches extracted by our method are more separable, while preserving the perceptually relevant portion of each activity. © 2015 ACM.",Algorithms; Discriminative patches; Error-correcting code SVM; Human activity; I.2.10 [artificial intelligence]: vision and scene understanding-video analysis; I.4.9 [image processing and computer vision]: applications; I.5.4 [pattern recognition]: applications-computer vision; Nonnegative sparse coding; Performance,Algorithms; Artificial intelligence; Image coding; Discriminative patches; Error correcting code; Human activities; I.2.10 [artificial intelligence]: vision and scene understanding - video analysis; Image processing and computer vision; Non-negative sparse coding; Performance; Computer vision
Wireless multicast for zoomable video streaming,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940434899&doi=10.1145%2f2801123&partnerID=40&md5=a1f787a47f2d4827604f6ca27af5105d,"Zoomable video streaming refers to a new class of interactive video applications, where users can zoom into a video stream to view a selected region of interest in higher resolutions and pan around to move the region of interest. The zoom and pan effects are typically achieved by breaking the source video into a grid of independently decodable tiles. Streaming the tiles to a set of heterogeneous users using broadcast is challenging, as users have different link rates and different regions of interest at different resolution levels. In this article, we consider the following problem: Given the subset of tiles that each user requested, the link rate of each user, and the available time slots, at which resolution should each tile be sent, to maximize the overall video quality received by all users. We design an efficient algorithm to solve this problem and evaluate the solution on a testbed using 10 mobile devices. Our method is able to achieve up to 12dB improvements over other heuristic methods. 2015 Copyright held by the owner/author(s).","Algorithms; C.2.1 [computer-communication networks]: network architecture and design-wireless communication; Design; Dynamic programming; Experimentation; H.4.3 [information systems applications]: communications applications; H.5.1 [information interfaces and presentation]: multimedia information systems-video (e.g., tape, disk, DVI); Measurement; Optimal multicast algorithm; Performance; Wireless multicast; Zoomable video",Algorithms; Computer systems programming; Design; Dynamic programming; Heuristic methods; Image segmentation; Information systems; Information use; Measurement; Multicasting; Multimedia systems; Network architecture; Communications applications; Experimentation; Multicast algorithms; MultiMedia Information Systems; Performance; Wireless communications; Wireless multicast; Zoomable video; Video streaming
User preferences modeling and learning for pleasing photo collage generation,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940404101&doi=10.1145%2f2801126&partnerID=40&md5=0ef242a2362ca423285a00b80db1e628,"In this article, we consider how to automatically create pleasing photo collages created by placing a set of images on a limited canvas area. The task is formulated as an optimization problem. Differently from existing state-of-the-art approaches, we here exploit subjective experiments to model and learn pleasantness from user preferences. To this end, we design an experimental framework for the identification of the criteria that need to be taken into account to generate a pleasing photo collage. Five different thematic photo datasets are used to create collages using state-of-the-art criteria. A first subjective experiment where several subjects evaluated the collages, emphasizes that different criteria are involved in the subjective definition of pleasantness. We then identify new global and local criteria and design algorithms to quantify them. The relative importance of these criteria are automatically learned by exploiting the user preferences, and new collages are generated. To validate our framework, we performed several psycho-visual experiments involving different users. The results shows that the proposed framework allows to learn a novel computational model which effectively encodes an inter-user definition of pleasantness. The learned definition of pleasantness generalizes well to new photo datasets of different themes and sizes not used in the learning. Moreover, compared with two state-of-the-art approaches, the collages created using our framework are preferred by the majority of the users. © 2015 ACM.",Algorithms; Automatic collage creation; Design; Experimentation; G.1.6 [mathematics of computing]: optimization; I.2.6 [artificial intelligence]: learning - parameter learning; I.4.0 [image processing and computer vision]: general; I.4.9 [image processing and computer vision]: applications; Optimization algorithm; Performance; Preference modeling; Subjective experiment; User modeling; Visual features extraction,Algorithms; Artificial intelligence; Computer vision; Design; Optimization; Automatic collage creation; Experimentation; Image processing and computer vision; Learning parameters; Mathematics of computing; Optimization algorithms; Performance; Preference modeling; Subjective experiments; User Modeling; Visual features extraction; Learning systems
QoE-based cross-layer optimization for uplink video transmission,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940404102&doi=10.1145%2f2801124&partnerID=40&md5=c2051176d148a5cda0aafa5a2e03fc73,"We study the problem of resource-efficient uplink distribution of user-generated video content over fourth-generation mobile networks. This is challenged by (1) the capacity-limited and time-variant uplink channel, (2) the resource-hungry upstreamed videos and their dynamically changing complexity, and (3) the different playout times of the video consumers. To address these issues, we propose a systematic approach for quality-of-experience (QoE)-based resource optimization and uplink transmission of multiuser generated video content. More specifically, we present an analytical model for distributed scalable video transmission at the mobile producers which considers these constraints. This is complemented by a multiuser cross-layer optimizer in the mobile network which determines the transmission capacity for each mobile terminal under current cell load and radio conditions. Both optimal and low-complexity solutions are presented. Simulation results for LTE uplink transmission show that significant gains in perceived video quality can be achieved by our cross-layer resource optimization scheme. In addition, the distributed optimization at the mobile producers can further improve the user experience across the different types of video consumers. © 2015 ACM.",Algorithms; Design; H.5.1 [information interfaces and presentation]: multimedia information systems - evaluation/methodology; Live and VoD; LTE uplink; Quality of experience; Resource allocation; Scalable video transmission,Algorithms; Complex networks; Design; Image communication systems; Mobile telecommunication systems; Multimedia systems; Radio transmission; Resource allocation; Scalable video coding; User experience; Video recording; Wireless networks; Live and VoD; LTE uplink; MultiMedia Information Systems; Quality of experience (QoE); Scalable video; Quality of service
Using paired distances of signal peaks in stereo channels as fingerprints for copy identification,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940398872&doi=10.1145%2f2742059&partnerID=40&md5=5f6ac7a37105b8a616ed54e35dee7047,"This article proposes to use the relative distances between adjacent envelope peaks detected in stereo audio as fingerprints for copy identification. The matching algorithm used is the rough longest common subsequence (RLCS) algorithm. The experimental results show that the proposed approach has better identification accuracy than an MPEG-7 based scheme for distorted and noisy audio. When compared with other schemes, the proposed scheme uses fewer bits with comparable performance. The proposed fingerprints can also be used in conjunction with the MPEG-7 based scheme for lower computational burden. © 2015 ACM.",Design; Envelope peak; H.2.4 [database management]: systems-multimedia database; H.5.1 [information interfaces and presentation]: multimedia information systems-audio input/output; I.5.2 [pattern recognition]: design methodology-classifier design and evaluation; MPEG-7 audio signature descriptor; Music fingerprint; Rough longest common subsequence; Stereo music,Audio acoustics; Audio systems; Classification (of information); Design; Information management; Palmprint recognition; Pattern recognition systems; Audio signature; Design Methodology; Envelope peak; Longest common subsequences; Multimedia database; MultiMedia Information Systems; Music fingerprint; Stereo music; Motion Picture Experts Group standards
Area of simulation: Mechanism and architecture for Multi-Avatar Virtual Environments,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940398870&doi=10.1145%2f2764463&partnerID=40&md5=0ad239beaa73c5dfbd48db76671a9f7e,"Although Multi-Avatar Distributed Virtual Environments (MAVEs) such as Real-Time Strategy (RTS) games entertain daily hundreds of millions of online players, their current designs do not scale. For example, even popular RTS games such as the StarCraft series support in a single game instance only up to 16 players and only a few hundreds of avatars loosely controlled by these players, which is a consequence of the Event-Based Lockstep Simulation (EBLS) scalability mechanism they employ. Through empirical analysis, we show that a single Area of Interest (AoI), which is a scalability mechanism that is sufficient for single-avatar virtual environments (such as Role-Playing Games), also cannot meet the scalability demands of MAVEs. To enable scalable MAVEs, in this work we propose Area of Simulation (AoS), a new scalability mechanism, which combines and extends the mechanisms of AoI and EBLS. Unlike traditional AoI approaches, which employ only update-based operational models, our AoS mechanism uses both event-based and update-based operational models to manage not single, but multiple areas of interest. Unlike EBLS, which is traditionally used to synchronize the entire virtual world, our AoS mechanism synchronizes only selected areas of the virtual world. We further design an AoS-based architecture, which is able to use both our AoS and traditional AoI mechanisms simultaneously, dynamically trading-off consistency guarantees for scalability. We implement and deploy this architecture and we demonstrate that it can operate with an order of magnitude more avatars and a larger virtual world without exceeding the resource capacity of players' computers. © 2015 ACM.","Area of interest; C.2.4 [computer-communication networks]: distributed systems-client/server; C.5.1 [information interfaces and presentation]: multimedia information systems-artificial, augmented, and virtual realities; Design; Distributed virtual environments; Real time strategy games",Computer games; Computer software; Design; Distributed database systems; Network architecture; Real time systems; Scalability; Virtual reality; Area of interest; Distributed systems; Distributed Virtual Environments; Multimedia information systems-artificial; Real-time strategy games; Distributed computer systems
Parallel massive clustering of discrete distributions,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930858981&doi=10.1145%2f2700293&partnerID=40&md5=9c673d3e064d4e26cccefef5c2d01137,"The trend of analyzing big data in artificial intelligence demands highly-scalable machine learning algorithms, among which clustering is a fundamental and arguably the most widely applied method. To extend the applications of regular vector-based clustering algorithms, the Discrete Distribution (D2) clustering algorithm has been developed, aiming at clustering data represented by bags of weighted vectors which are well adopted data signatures in many emerging information retrieval and multimedia learning applications. However, the high computational complexity of D2-clustering limits its impact in solving massive learning problems. Here we present the parallel D2-clustering (PD2-clustering) algorithm with substantially improved scalability. We developed a hierarchical multipass algorithm structure for parallel computing in order to achieve a balance between the individual-node computation and the integration process of the algorithm. Experiments and extensive comparisons between PD2-clustering and other clustering algorithms are conducted on synthetic datasets. The results show that the proposed parallel algorithm achieves significant speed-up with minor accuracy loss. We apply PD2-clustering to image concept learning. In addition, by extending D2-clustering to symbolic data, we apply PD2-clustering to protein sequence clustering. For both applications, we demonstrate the high competitiveness of our new algorithm in comparison with other state-of-the-art methods. © 2015 ACM.",Algorithms; Clustering; D.1.3 [programming techniques]: concurrent programming-distributed programming; Discrete distribution; I.5.3 [pattern recognitions]: clustering-algorithms; Image annotation; Large-scale learning; Parallel computing; Protein clustering,Algorithms; Cluster analysis; Computer programming; Image annotation; Learning algorithms; Machine learning; Parallel processing systems; Pattern recognition; Proteins; Clustering; Concurrent programming; Discrete distribution; Large-scale learning; Multi-media learning; Protein sequence clustering; Scalable machine learning; State-of-the-art methods; Clustering algorithms
Multi-camera coordination and control in surveillance systems: A survey,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930851551&doi=10.1145%2f2710128&partnerID=40&md5=af51b4fe03f86d5e711ae67b87c00338,"The use of multiple heterogeneous cameras is becoming more common in today's surveillance systems. In order to perform surveillance tasks, effective coordination and control in multi-camera systems is very important, and is catching significant research attention these days. This survey aims to provide researchers with a state-of-the-art overview of various techniques for multi-camera coordination and control (MC3) that have been adopted in surveillance systems. The existing literature on MC3 is presented through several classifications based on the applicable architectures, frameworks and the associated surveillance tasks. Finally, a discussion on the open problems in surveillance area that can be solved effectively using MC3 and the future directions in MC3 research is presented. © 2015 ACM.","A.1 [general literature]: introductory and survey; Active camera coordination and control; Algorithms; Cyber-physical system; Human factors; I.2.11 [artificial intelligence]: distributed artificial intelligence-multiagent systems; I.2.9 [artificial intelligence]: robotics-commercial robots and applications, sensors; I.4.8 [image processing and computer vision]: scene analysis-tracking; Multi-camera surveillance system; PTZ camera network; Security",Algorithms; Artificial intelligence; Cameras; Computer control systems; Computer vision; Distributed computer systems; Embedded systems; Human engineering; Image processing; Intelligent robots; Monitoring; Multi agent systems; Robotics; Surveys; Video cameras; Commercial robots; Coordination and Control; Cyber physical systems (CPSs); Distributed Artificial Intelligence; I.4.8 [Image Processing and Computer Vision]: Scene Analysis - Tracking; Introductory and Survey; Multi-camera surveillance systems; PTZ camera; Security; Security systems
Cross-platform emerging topic detection and elaboration from multimedia streams,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930828141&doi=10.1145%2f2730889&partnerID=40&md5=6bb0786381505e82a0a43594834d1b84,"With the explosive growth of online media platforms in recent years, it becomes more and more attractive to provide users a solution of emerging topic detection and elaboration. And this posts a real challenge to both industrial and academic researchers because of the overwhelming information available in multiple modalities and with large outlier noises. This article provides a method on emerging topic detection and elaboration using multimedia streams cross different online platforms. Specifically, Twitter, New York Times and Flickr are selected for the work to represent the microblog, news portal and imaging sharing platforms. The emerging keywords of Twitter are firstly extracted using aging theory. Then, to overcome the nature of short length message in microblog, Robust Cross-Platform Multimedia Co-Clustering (RCPMM-CC) is proposed to detect emerging topics with three novelties: 1) The data from different media platforms are in multimodalities; 2) The coclustering is processed based on a pairwise correlated structure, in which the involved three media platforms are pairwise dependent; 3) The noninformative samples are automatically pruned away at the same time of coclustering. In the last step of cross-platform elaboration, we enrich each emerging topic with the samples from New York Times and Flickr by computing the implicit links between social topics and samples from selected news and Flickr image clusters, which are obtained by RCPMM-CC. Qualitative and quantitative evaluation results demonstrate the effectiveness of our method. © 2015 ACM.",Algorithms; Coclustering; Cross-media; Cross-platform; H.3.3 [information storage and retrieval]: information search and retrieval-clustering; Topic detection,Algorithms; Computation theory; Digital storage; Media streaming; Social networking (online); Co-clustering; Cross-media; Cross-platform; H.3.3 [information storage and retrieval]: information search and retrievals; Topic detection; Explosives detection
Similarity search over the cloud based on image descriptors' dimensions value cardinalities,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930831832&doi=10.1145%2f2716315&partnerID=40&md5=0558f961be77532c9a3540e3582bc78b,"In recognition that in modern applications billions of images are stored into distributed databases in different logical or physical locations, we propose a similarity search strategy over the cloud based on the dimensions value cardinalities of image . Our strategy has low preprocessing requirements by dividing the computational cost of the preprocessing steps into several nodes over the cloud and locating the with similar dimensions value cardinalities logically close. New images are inserted into the distributed databases over the cloud efficiently, by supporting dynamical update in real-time. The proposed insertion algorithm has low computational complexity, depending exclusively on the dimensionality of and a small subset of with similar dimensions value cardinalities. Finally, an efficient query processing algorithm is proposed, where the dimensions of image are prioritized in the searching strategy, assuming that dimensions of high value cardinalities have more discriminative power than the dimensions of low ones. The computation effort of the query processing algorithm is divided into several nodes over the cloud infrastructure. In our experiments with seven publicly available datasets of image , we show that the proposed similarity search strategy outperforms competitive methods of single node, parallel and cloud-based architectures, in terms of preprocessing cost, search time and accuracy. © 2015 Association for Computing Machinery.",Algorithms; Content-based image retrieval; Distributed databases; Experimentation; H.3.3 [information storage and retrieval]: information search and retrieval; Large-scale similarity search; Measurement; Multimedia cloud computing,Algorithms; Content based retrieval; Image processing; Measurement; Query processing; Content based image retrieval; Distributed database; Experimentation; H.3.3 [information storage and retrieval]: information search and retrievals; Scale similarity; Distributed database systems
QuGu: A quality guaranteed video dissemination protocol over urban vehicular ad hoc networks,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930847507&doi=10.1145%2f2725469&partnerID=40&md5=5b557252912c1aae26c851e87107a376,"Video dissemination over Vehicular Ad Hoc Networks is an attractive technology that supports many novel applications. The merit of this work lies in the design of an efficient video dissemination protocol that provides high video quality at different data rates for urban scenarios. Our objective is to improve received video quality while meeting delay and packet loss. In this work, we first employ a reliable scheme known as connected dominating set, which is an efficient receiver-based routing scheme for broadcasting video content. To avoid repeated computing of the connected dominating set, we add three statuses to each node. In nonscalable video coding, the distribution of lost frames can cause a major impact on video quality at the receiver's end. Therefore, for the second step, we employ Interleaving to spread out the burst losses and to reduce the influence of loss distributions. Although Interleaving can reduce the influence of cluster frame loss, single packet loss is also a concern due to collisions, and to intermittent disconnection in the topology. In order to fix these single packet losses, we propose a store-carry-forward scheme for the nodes in order to retransmit the local buffer stored packets. The results, when compared to the selected base protocols, show that our proposed protocol is an efficient solution for video dissemination over urban Vehicular Ad Hoc Networks. © 2015 ACM.",Algorithms; C.2.2 [computer-communication networks]: network protocols; Connected dominating set; Experimentation; H.5.1 [information interfaces and presentation]: multimedia information systems-video; Interleaving; Network coding; Performance; QoS; Routing protocols; Vehicular network; Video streaming,Algorithms; Internet protocols; Mobile telecommunication systems; Network coding; Packet loss; Quality of service; Routing protocols; Signal receivers; Video signal processing; Video streaming; Computer communication networks; Connected Dominating Set; Experimentation; Interleaving; MultiMedia Information Systems; Performance; Vehicular networks; Vehicular ad hoc networks
The cameraman operating my virtual camera is artificial: Can the machine be as good as a human?,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930851072&doi=10.1145%2f2744411&partnerID=40&md5=7a580d8e8b6ac9dc9eca95d07e1f2562,"In this article, we argue that the energy spent in designing autonomous camera control systems is not spent in vain. We present a real-time virtual camera system that can create smooth camera motion. Similar systems are frequently benchmarked with the human operator as the best possible reference; however, we avoid a priori assumptions in our evaluations. Our main question is simply whether we can design algorithms to steer a virtual camera that can compete with the user experience for recordings from an expert operator with several years of experience? In this respect, we present two low-complexity servoing methods that are explored in two user studies. The results from the user studies give a promising answer to the question pursued. Furthermore, all components of the system meet the real-time requirements on commodity hardware. The growing capabilities of both hardware and network in mobile devices give us hope that this system can be deployed to mobile users in the near future. Moreover, the design of the presented system takes into account that services to concurrent users must be supported. © 2015 Copyright held by the owner/author(s).",Experimentation; H.5.2 [information interfaces and presentation]: multimedia information systems-video; Interactive immersion; Measurement; Panning; Panorama video; Performance; Quality of experience; Real-time; User studies; Virtual camera; Visual servoing; Zoom,Cameras; Man machine systems; Measurement; Mobile telecommunication systems; Quality of service; Real time systems; Visual servoing; Experimentation; Interactive immersion; MultiMedia Information Systems; Panning; Panorama video; Performance; Quality of experience (QoE); Real time; User study; Virtual camera; Zoom; User experience
Efficient MAC for real-time video streaming over wireless LAN,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930847249&doi=10.1145%2f2744412&partnerID=40&md5=c271195b49d89489c990b1a756b565aa,"Wireless communication systems are highly prone to channel errors. With video being a major player in Internet traffic and undergoing exponential growth in wireless domain, we argue for the need of a Video-aware MAC (VMAC) to significantly improve the throughput and delay performance of real-time video streaming service. VMAC makes two changes to optimize wireless LAN for video traffic: (a) It incorporates a Perceptual-Error-Tolerance (PET) to the MAC frames by reducing MAC retransmissions while minimizing any impact on perceptual video quality; and (b) It uses a group NACK-based Adaptive Window (NAW) of MAC frames to improve both throughput and delay performance in varying channel conditions. Through simulations and experiments, we observe 56-89% improvement in throughput and 34-48% improvement in delay performance over legacy DCF and 802.11e schemes. VMAC also shows 15-78% improvement over legacy schemes with multiple clients. © 2015 ACM.",Design; H.5.1 [information interfaces and presentation]: multimedia information systems-video; IEEE 802.11 standards; Performance; Video transmission; Wireless LAN,Design; IEEE Standards; Image communication systems; Video streaming; Exponential growth; IEEE 802.11 standards; MultiMedia Information Systems; Perceptual video quality; Performance; Real-time videostreaming; Video transmissions; Wireless communication system; Wireless local area networks (WLAN)
Cache-centric video recommendation: An approach to improve the efficiency of YouTube caches,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930827096&doi=10.1145%2f2716310&partnerID=40&md5=18fbb9545dbfd0ad1bc7accf20f254d1,"In this article, we take advantage of the user behavior of requesting videos from the top of the related list provided by YouTube to improve the performance of YouTube caches. We recommend that local caches reorder the related lists associated with YouTube videos, presenting the cached content above noncached content. We argue that the likelihood that viewers select content from the top of the related list is higher than selection from the bottom, and pushing contents already in the cache to the top of the related list would increase the likelihood of choosing cached content. To verify that the position on the list really is the selection criterion more dominant than the content itself, we conduct a user study with 40 YouTube-using volunteers who were presented with random related lists in their everyday YouTube use. After confirming our assumption, we analyze the benefits of our approach by an investigation that is based on two traces collected from a university campus. Our analysis shows that the proposed reordering approach for related lists would lead to a 2 to 5 times increase in cache hit rate compared to an approach without reordering the related list. This increase in hit rate would lead to reduction in server load and backend bandwidth usage, which in turn reduces the latency in streaming the video requested by the viewer and has the potential to improve the overall performance of YouTube's content distribution system. An analysis of YouTube's recommendation system reveals that related lists are created from a small pool of videos, which increases the potential for caching content from related lists and reordering based on the content in the cache. © 2015 ACM.",Caching; H.5.1 [information interfaces and presentation]: multimedia information system-video; Measurement; Performance; Recommendation; YouTube,Measurement; Caching; MultiMedia Information Systems; Performance; Recommendation; YouTube; Behavioral research
An advanced visibility restoration algorithm for single hazy images,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930843580&doi=10.1145%2f2726947&partnerID=40&md5=e3f3ca60007f070bad23d4173b5b8078,"Haze removal is the process by which horizontal obscuration is eliminated from hazy images captured during inclement weather. Images captured in natural environments with varied weather conditions frequently exhibit localized light sources or color-shift effects. The occurrence of these effects presents a difficult challenge for hazy image restoration, with which many traditional restoration methods cannot adequately contend. In this article, we present a new image haze removal approach based on Fisher's linear discriminant-based dual dark channel prior scheme in order to solve the problems associated with the presence of localized light sources and color shifts, and thereby achieve effective restoration. Experimental restoration results via qualitative and quantitative evaluations show that our proposed approach can provide higher haze-removal efficacy for images captured in varied weather conditions than can the other state-of-the-art approaches. © 2015 ACM.",Algorithms; Dehaze; Design; I.4.5 [image processing and computer vision]: reconstruction-transform methods; Image haze removal; Performance; Visibility restoration,Algorithms; Design; Discriminant analysis; Light sources; Meteorology; Restoration; Visibility; Dehaze; Fisher's linear discriminant; Haze removal; Performance; Quantitative evaluation; Restoration algorithm; State-of-the-art approach; Transform methods; Image reconstruction
Audio musical dice game: A user-preference-aware medley generating system,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930851860&doi=10.1145%2f2710015&partnerID=40&md5=ff362415649a1f9db9dbf20197f0f16c,"This article proposes a framework for creating user-preference-aware music medleys from users' music collections. We treat the medley generation process as an audio version of a musical dice game. Once the user's collection has been analyzed, the system is able to generate various pleasing medleys. This flexibility allows users to create medleys according to the specified conditions, such as the medley structure or the must-use clips. Even users without musical knowledge can compose medley songs from their favorite tracks. The effectiveness of the system has been evaluated through both objective and subjective experiments on individual components in the system. © 2015 ACM.",Algorithms; Concatenating music; Design; Experimentation; H.5.1 [information interfaces and presentation]: multimedia information systems; H.5.5 [information interfaces and presentation]: sound and music computing-systems; Music editing; Musical medley,Algorithms; Computer networks; Design; Concatenating music; Experimentation; MultiMedia Information Systems; Music editing; Musical medley; Sound and music computing; Audio acoustics
3DTI Amphitheater: Towards 3DTI broadcasting,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924087582&doi=10.1145%2f2700297&partnerID=40&md5=b535bb5c041c34d3b9493d9c9a4fea7c,"3DTI Amphitheater is a live broadcasting system for dissemination of 3DTI (3D Tele-immersive) content. The virtual environment constructed by the system mimics an amphitheater in the real world, where performers interact with each other in the central circular stage, and the audience is placed in virtual seats that surround the stage. Users of the Amphitheater can be geographically dispersed and the streams created by the performer sites are disseminated in a P2P network among the participants. To deal with the high bandwidth demand and strict latency bound of the service, we identify the hierarchical priority of streams in construction of the content dissemination forest. Result shows that the Amphitheater outperforms prior 3DTI systems by boosting the application QoS by a factor of 2.8 while sustaining the same hundred-scale audience group. © 2015 ACM.",3D tele-immersion; Broadcasting; Content dissemination,Broadcasting; 3d tele-immersion; Application QoS; Broadcasting systems; Content dissemination; High bandwidth; Latency bounds; P2P network; Real-world; Peer to peer networks
Introduction to the special issue on MM Sys 2014 and NOSSDAV 2014,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924087346&doi=10.1145%2f2717509&partnerID=40&md5=059ead547c85f567f9bd1cc10c6f6198,[No abstract available],,
Optimal selection of adaptive streaming representations,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930975637&doi=10.1145%2f2700294&partnerID=40&md5=b4c486accc9dd6ddb004c6dea77b458c,"Adaptive streaming addresses the increasing and heterogeneous demand of multimedia content over the Internet by offering several encoded versions for each video sequence. Each version (or representation) is characterized by a resolution and a bit rate, and it is aimed at a specific set of users, like TV or mobile phone clients. While most existing works on adaptive streaming deal with effective playout-buffer control strategies on the client side, in this article we take a providers' perspective and propose solutions to improve user satisfaction by optimizing the set of available representations. We formulate an integer linear program that maximizes users' average satisfaction, taking into account network dynamics, type of video content, and user population characteristics. The solution of the optimization is a set of encoding parameters corresponding to the representations set that maximizes user satisfaction. We evaluate this solution by simulating multiple adaptive streaming sessions characterized by realistic network statistics, showing that the proposed solution outperforms commonly used vendor recommendations, in terms of user satisfaction but also in terms of fairness and outage probability. The simulation results show that video content information as well as network constraints and users' statistics play a crucial role in selecting proper encoding parameters to provide fairness among users and to reduce network resource usage. We finally propose a few theoretical guidelines that can be used, in realistic settings, to choose the encoding parameters based on the user characteristics, the network capacity and the type of video content. © 2015 ACM.",Content distribution; Dynamic adaptive streaming over http; Integer linear program; Video streaming,Encoding (symbols); HTTP; Motion compensation; Signal encoding; Video recording; Video signal processing; Video streaming; Content distribution; Dynamic Adaptive Streaming over HTTP; Heterogeneous demand; Integer linear programs; Multimedia contents; Network constraints; Population characteristics; User characteristics; Encoding parameters; Integer programming
Decoder-complexity-aware encoding of motion compensation for multiple heterogeneous receivers,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924072605&doi=10.1145%2f2700300&partnerID=40&md5=d72ad4d48cf6ed6f752d3f9bc725fd25,"For mobile multimedia systems, advances in battery technology have been much slower than those in memory, graphics, and processing power, making power consumption a major concern in mobile systems. The computational complexity of video codecs, which consists of CPU operations and memory accesses, is one of the main factors affecting power consumption. In this article, we propose a method that achieves near-optimal video quality while respecting user-defined bounds on the complexity needed to decode a video. We specifically focus on the motion compensation process, including motion vector prediction and interpolation, because it is the single largest component of computation-based power consumption. We start by formulating a scenario with a single receiver as a rate-distortion optimization problem and we develop an efficient decoder-complexity-aware video encoding method to solve it. Then we extend our approach to handle multiple heterogeneous receivers, each with a different complexity requirement. We test our method experimentally using the H.264 standard for the single receiver scenario and the H.264 SVC extension for the multiple receiver scenario. Our experimental results show that our method can achieve up to 97% of the optimal solution value in the single receiver scenario, and an average of 97% of the optimal solution value in the multiple receiver scenario. Furthermore, our tests with actual power measurements show a power saving of up to 23% at the decoder when the complexity threshold is halved in the encoder. © 2015 ACM.",Decoder complexity modeling; H.264/avc decoding; H.264/svc decoding; Motion compensation,Decoding; Electric distortion; Electric power utilization; Encoding (symbols); Image coding; Motion compensation; Multimedia systems; Optimal systems; Signal distortion; Signal encoding; Compensation process; Decoder complexity modeling; H.264/AVC-decoding; H.264/SVC; Mobile multimedia systems; Motion vector prediction; Multiple receivers; Rate-distortion optimization; Video signal processing
"Cam mark: Analyzing, modeling, and simulating artifacts in camcorder copies",2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924072218&doi=10.1145%2f2700295&partnerID=40&md5=befb03d8eef773e353099e80bbda4f3f,"To support the development of any system that includes the generation and evaluation of camcorder copies, as well as to provide a common benchmark for robustness against camcorder copies, we present a tool to simulate digital video re-acquisition using a digital video camera. By resampling each video frame, we simulate the typical artifacts occurring in a camcorder copy: geometric modifications (aspect ratio changes, cropping, perspective and lens distortion), temporal sampling artifacts (due to different frame rates, shutter speeds, rolling shutters, or playback), spatial and color subsampling (rescaling, filtering, Bayer color filter array), and processing steps (automatic gain control, automatic white balance). We also support the simulation of camera movement (e.g., a hand-held camera) and background insertion. Furthermore, we allow for an easy setup and calibration of all the simulated artifacts, using sample/reference pairs of images and videos. Specifically temporal subsampling effects are analyzed in detail to create realistic frame blending artifacts in the simulated copies. We carefully evaluated our entire camcorder simulation system and found that the models we developed describe and match the real artifacts quite well. © 2015 ACM.",Automatic gain control; Automatic white balance; Bayer cfa; Camcorder copy; Digital watermarking; Display synchronization; Frame blending; Lens distortion; Moire; Perspective distortion; Rolling shutter; Spatial re-sampling; Temporal re-sampling; Video watermarking benchmark,Aspect ratio; Computer graphics; Digital watermarking; Gain control; Multimedia systems; Petroleum reservoir evaluation; Automatic gain control; Automatic white balance; Bayer cfa; Lens distortion; Moire; Perspective distortion; Resampling; Rolling shutters; Video watermarking; Video cameras
Scheduling a video transcoding server to save energy,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924086680&doi=10.1145%2f2700282&partnerID=40&md5=510e1274870d7f2cd4c9f207a7b98fbe,"Recent popular streaming services such as TV Everywhere, N-Screen, and dynamic adaptive streaming over HTTP (DASH) need to deliver content to the wide range of devices, requiring video content to be transcoded into different versions. Transcoding tasks require a lot of computation, and each task typically has its own real-time constraint. These make it difficult to manage transcoding, but the more efficient use of energy in servers is an imperative. We characterize transcoding workloads in terms of deadlines and computation times, and propose a new dynamic voltage and frequency scaling (DVFS) scheme that allocates a frequency and a workload to each CPU with the aim of minimizing power consumption while meeting all transcoding deadlines. This scheme has been simulated, and also implemented in a Linux transcoding server, in which a frontend node distributes transcoding requests to heterogeneous backend nodes. This required a new protocol for communication between nodes, a DVFS management scheme to reduce power consumption and thread management and scheduling schemes which ensure that transcoding deadlines are met. Power measurements show that this approach can reduce system-wide energy consumption by 17% to 31%, compared with the Linux Ondemand governor. © 2015 ACM.",Dynamic voltage and frequency scaling; Low-power systems; Multimedia systems,Dynamic frequency scaling; Electric power utilization; Energy utilization; Linux; Multimedia systems; Scheduling; Voltage scaling; Dynamic Adaptive Streaming over HTTP; Dynamic voltage and frequency scaling; Efficient use of energy; Low-power systems; Real time constraints; Scheduling schemes; System-wide energy consumption; Video-transcoding; Video signal processing
Analysis and detection of fake views in online video services,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924068518&doi=10.1145%2f2700290&partnerID=40&md5=7e100ca271c5239de1f88153a930f048,"Online video-on-demand(VoD) services invariably maintain a view count for each video they serve, and it has become an important currency for various stakeholders, from viewers, to content owners, advertizers, and the online service providers themselves. There is often significant financial incentive to use a robot (or a botnet) to artificially create fake views. How can we detect fake views? Can we detect them (and stop them) efficiently? What is the extent of fake views with current VoD service providers? These are the questions we study in this article. We develop some algorithms and show that they are quite effective for this problem. © 2015 ACM.",Fake view; Online video service,Computer networks; Advertizers; Fake view; Financial incentives; On-line service; Online video; Service provider; Video on demand
Progressive motion vector clustering for motion estimation and auxiliary tracking,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923360211&doi=10.1145%2f2700296&partnerID=40&md5=4f9ca2a816a143181413afb832f0c59b,"The motion vector similarity between neighboring blocks is widely used in motion estimation algorithms. However, for nonneighboring blocks, they may also have similar motions due to close depths or belonging to the same object inside the scene. Therefore, the motion vectors usually have several kinds of patterns, which reveal a clustering structure. In this article,we propose a progressive clustering algorithm, which periodically counts the motion vectors of the past blocks to make incremental clustering statistics. These statistics are used as the motion vector predictors for the following blocks. It is proved to be much more efficient for one block to find the best-matching candidate with the predictors. We also design the clustering based search with CUDA for GPU acceleration. Another interesting application of the clustering statistics is persistent static object tracking. Based on the statistics, several auxiliary tracking areas are created to guide the object tracking. Even when the target object has significant changes in appearance or it disappears occasionally, its position still can be predicted. The experiments on Xiph.org Video Test Media dataset illustrate that our clustering based search algorithm outperforms the mainstream and some state-of-the-art motion estimation algorithms. It is 33 times faster on average than the full search algorithm with only slightly higher meansquare error values in the experiments. The tracking results show that the auxiliary tracking areas help to locate the target object effectively. © 2015 ACM 1551-6857/2015/01-ART33 $15.00.",Auxiliary tracking; Block matching; Motion estimation; Progressive clustering,Algorithms; Color image processing; Image compression; Learning algorithms; Motion estimation; Statistical tests; Target tracking; Tracking (position); Vectors; Block Matching; Full Search algorithm; GPU accelerations; Incremental clustering; Motion estimation algorithm; Progressive clustering; Progressive motion; Search Algorithms; Clustering algorithms
Double verification secret sharing mechanism based on adaptive pixel pair matching,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923329838&doi=10.1145%2f2700291&partnerID=40&md5=6c2409ed4b37a9ab3be0666c140f25ef,"Verifiability is essential for the secret sharing approach, which allows the involved participants to detect cheaters during the secret retrieval process. In this article, we propose a double verification secret sharing (DVSS) mechanism that can not only prevent fraudulent participants but also satisfy the requirements of secret payload, camouflage, image fidelity and lossless revealed secret. DVSS offers double verification process to enhance the cheater detectability; experimental results reveal that the designed scheme can share larger secret capacity and retain superior image quality than the related secret sharing methods. © 2015 ACM 1551-6857/2015/01-ART33 $15.00.",Adaptive pixel pair matching; Camouflage; Secret sharing,Camouflage; Computer networks; Hardware; Detectability; Image fidelity; Lossless; Pair matching; Retrieval process; Secret sharing; Verifiability; Verification process; Pixels
Image enhancement in encrypted domain over cloud,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923357157&doi=10.1145%2f2656205&partnerID=40&md5=9c5646fde142369dca616cc0b6c981bd,"Cloud-based multimedia systems are becoming increasingly common. These systems offer not only storage facility, but also high-end computing infrastructure which can be used to process data for various analysis tasks ranging from low-level data quality enhancement to high-level activity and behavior identification operations. However, cloud data centers, being third party servers, are often prone to information leakage, raising security and privacy concerns. In this article, we present a Shamir's secret sharing based method to enhance the quality of encrypted image data over cloud. Using the proposed method we show that several image enhancement operations such as noise removal, antialiasing, edge and contrast enhancement, and dehazing can be performed in encrypted domain with near-zero loss in accuracy and minimal computation and data overhead. Moreover, the proposed method is proven to be information theoretically secure. © 2015 ACM 1551-6857/2015/01-ART33 $15.00.",Cloud computing; Encrypted domain processing; Image enhancement; Secret sharing,Cloud computing; Cryptography; Demulsification; Digital storage; Image enhancement; Information theory; Mobile security; Multimedia systems; Behavior identifications; Contrast Enhancement; Domain processing; Enhancement operations; High end computing; Information leakage; Secret sharing; Security and privacy; Image processing
Content vs. Context: Visual and geographic information use in video landmark retrieval,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923328962&doi=10.1145%2f2700287&partnerID=40&md5=f568d1e9f1f0d4b292b2a74e5758c685,"Due to the ubiquity of sensor-equipped smartphones, it has become increasingly feasible for users to capture videos together with associated geographic metadata, for example the location and the orientation of the camera. Such contextual information creates new opportunities for the organization and retrieval of georeferenced videos. In this study we explore the task of landmark retrieval through the analysis of two types of state-of-the-art techniques, namely media-content-based and geocontext-based retrievals. For the contentbased method, we choose the Spatial Pyramid Matching (SPM) approach combined with two advanced coding methods: Sparse Coding (SC) and Locality-Constrained Linear Coding (LLC). For the geo-based method, we present the Geo Landmark Visibility Determination (GeoLVD) approach which computes the visibility of a landmark based on intersections of a camera's field-of-view (FOV) and the landmark's geometric information available from Geographic Information Systems (GIS) and services. We first compare the retrieval results of the two methods, and discuss the strengths and weaknesses of each approach in terms of precision, recall and execution time. Next we analyze the factors that affect the effectiveness for the content-based and the geo-based methods, respectively. Finally we propose a hybrid retrievalmethod based on the integration of the visual (content) and geographic (context) information, which is shown to achieve significant improvements in our experiments. We believe that the results and observations in this work will enlighten the design of future geo-referenced video retrieval systems, improve our understanding of selecting the most appropriate visual features for indexing and searching, and help in selecting between the most suitable methods for retrieval based on different conditions. © 2015 ACM 1551-6857/2015/01-ART33 $15.00.",Content-based analysis; Geo-referenced videos; Landmark retrieval,Cameras; Codes (symbols); Geographic information systems; Visibility; Content-based analysis; Contextual information; Geo-referenced videos; Landmark retrieval; Spatial Pyramid Matching; State-of-the-art techniques; Video retrieval system; Visibility determination; Information use
A 3D-HEVC fast mode decision algorithm for real-time applications,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923369021&doi=10.1145%2f2700298&partnerID=40&md5=27d582c2a30d8f9e4796e843d750975f,"3D High Efficiency Video Coding (3D-HEVC) is an extension of the HEVC standard for coding of multiview videos and depth maps. It inherits the same quadtree coding structure as HEVC for both components, which allows recursively splitting into four equal-sized coding units (CU). One of 11 different prediction modes is chosen to code a CUin inter-frames. Similar to the joint model ofH.264/AVC, themode decision process inHM (reference software of HEVC) is performed using all the possible depth levels and prediction modes to find the one with the least rate distortion cost using a Lagrange multiplier. Furthermore, both motion estimation and disparity estimation need to be performed in the encoding process of 3D-HEVC. Those tools achieve high coding efficiency, but lead to a significant computational complexity. In this article, we propose a fast mode decision algorithm for 3D-HEVC. Since multiview videos and their associated depth maps represent the same scene, at the same time instant, their prediction modes are closely linked. Furthermore, the prediction information of a CU at the depth level X is strongly related to that of its parent CU at the depth level X-1 in the quadtree coding structure of HEVC since two corresponding CUs from two neighboring depth levels share similar video characteristics. The proposed algorithm jointly exploits the inter-view coding mode correlation, the inter-component (texture-depth) correlation and the inter-level correlation in the quadtree structure of 3D-HEVC. Experimental results show that our algorithm saves 66% encoder runtime on average with only a 0.2% BD-Rate increase on coded views and 1.3% BD-Rate increase on synthesized views. © 2015 ACM 1551-6857/2015/01-ART33 $15.00.",3D-HEVC; Computational complexity; Mode decision; Motion estimation; Real-time applications,Algorithms; Codes (symbols); Computational complexity; Electric distortion; Forecasting; Lagrange multipliers; Motion estimation; Signal distortion; Video signal processing; 3D-HEVC; Disparity estimations; Fast mode decision algorithm; High coding efficiency; High-efficiency video coding; Mode Decision; Prediction informations; Real-time application; Image coding
Robust color image watermarking using geometric invariant quaternion polar harmonic transform,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923325769&doi=10.1145%2f2700299&partnerID=40&md5=64c35ca87277148a4841a9f410f39233,"It is a challenging work to design a robust color image watermarking scheme against geometric distortions. Moments and moment invariants have become a powerful tool in robust image watermarking owing to their image description capability and geometric invariance property. However, the existing moment-based watermarking schemes were mainly designed for gray images but not for color images, and detection quality and robustness will be lowered when watermark is directly embedded into the luminance component or three color channels of color images. Furthermore, the imperceptibility of the embedded watermark is not well guaranteed. Based on algebra of quaternions and polar harmonic transform (PHT), we introduced the quaternion polar harmonic transform (QPHT) for invariant color image watermarking in this article, which can be seen as the generalization of PHT for gray-level images. It is shown that the QPHT can be obtained from the PHT of each color channel. We derived and analyzed the rotation, scaling, and translation (RST) invariant property of QPHT. We also discussed the problem of color image watermarking using QPHT. Experimental results are provided to illustrate the efficiency of the proposed color image watermarking against geometric distortions and common image processing operations (including color attacks). © 2015 ACM 1551-6857/2015/01-ART33 $15.00.",Color image watermarking; Geometric invariance; Imperceptibility; Quaternion polar harmonic transform; Robustness,"Color; Color image processing; Geometry; Harmonic analysis; Image processing; Mathematical transformations; Robustness (control systems); Watermarking; Color image watermarking; Geometric invariance; Imperceptibility; Invariant properties; Polar harmonic transforms; Robust image watermarking; Rotation , scaling , and translations; Watermarking schemes; Image watermarking"
INSTRE: A new benchmark for instance-level object retrieval and recognition,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923340591&doi=10.1145%2f2700292&partnerID=40&md5=be9f1660d1439840c7c70ecd7ad2e607,"Over the last several decades, researches on visual object retrieval and recognition have achieved fast and remarkable success. However, while the category-level tasks prevail in the community, the instance-level tasks (especially recognition) have not yet received adequate focuses. Applications such as content-based search engine and robot vision systems have alerted the awareness to bring instance-level tasks into a more realistic and challenging scenario. Motivated by the limited scope of existing instance-level datasets, in this article we propose a new benchmark for INSTance-level visual object REtrieval and REcognition (INSTRE). Compared with existing datasets, INSTRE has the following major properties: (1) balanced data scale, (2) more diverse intraclass instance variations, (3) cluttered and less contextual backgrounds, (4) object localization annotation for each image, (5) well-manipulated double-labelled images for measuring multiple object (within one image) case. We will quantify and visualize the merits of INSTRE data, and extensively compare them against existing datasets. Then on INSTRE, we comprehensively evaluate several popular algorithms to large-scale object retrieval problem with multiple evaluation metrics. Experimental results show that all the methods suffer a performance drop on INSTRE, proving that this field still remains a challenging problem. Finally we integrate these algorithms into a simple yet efficient scheme for recognition and compare it with classification-based methods. Importantly, we introduce the realistic multiobjects recognition problem. All experiments are conducted in both single object case and multiple objects case. © 2015 ACM 1551-6857/2015/01-ART33 $15.00.",Annotation; Dataset; Evaluation; Instance-level; Multiple object; Object recognition; Object retrieval,Computer vision; Search engines; Annotation; Dataset; Evaluation; Instance-level; Multiple objects; Object retrieval; Object recognition
Boosted multifeature learning for cross-domain transfer,2015,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923341321&doi=10.1145%2f2700286&partnerID=40&md5=b8de4d1dc035c682f417d34465811420,"Conventional learning algorithm assumes that the training data and test data share a common distribution. However, this assumption will greatly hinder the practical application of the learned model for cross-domain data analysis in multimedia. To deal with this issue, transfer learning based technology should be adopted. As a typical version of transfer learning, domain adaption has been extensively studied recently due to its theoretical value and practical interest. In this article, we propose a boosted multifeature learning (BMFL) approach to iteratively learn multiple representations within a boosting procedure for unsupervised domain adaption. The proposed BMFL method has a number of properties. (1) It reuses all instances with different weights assigned by the previous boosting iteration and avoids discarding labeled instances as in conventional methods. (2) It models the instance weight distribution effectively by considering the classification error and the domain similarity, which facilitates learning new feature representation to correct the previously misclassified instances. (3) It learns multiple different feature representations to effectively bridge the source and target domains. We evaluate the BMFL by comparing its performance on three applications: image classification, sentiment classification and spam filtering. Extensive experimental results demonstrate that the proposed BMFL algorithm performs favorably against state-of-the-art domain adaption methods. © 2015 ACM 1551-6857/2015/01-ART33 $15.00.",Boosting; Denoising auto-encoder; Domain adaptation; Multifeature,Algorithms; Image classification; Auto encoders; Boosting; Classification errors; Domain adaptation; Feature representation; Multi features; Multiple representation; Sentiment classification; Iterative methods
Twitter is faster: Personalized time-aware video recommendation from Twitter to YouTube,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920941758&doi=10.1145%2f2637285&partnerID=40&md5=df202c06f0b0286cee96157b7190bcb3,"Traditional personalized video recommendation methods focus on utilizing user profile or user history behaviors to model user interests, which follows a static strategy and fails to capture the swift shift of the short-term interests of users. According to our cross-platform data analysis, the information emergence and propagation is faster in social textual stream-based platforms than that in multimedia sharing platforms at micro user level. Inspired by this, we propose a dynamic user modeling strategy to tackle personalized video recommendation issues in the multimedia sharing platform YouTube, by transferring knowledge from the social textual stream-based platform Twitter. In particular, the cross-platform video recommendation strategy is divided into two steps. (1) Real-time hot topic detection: the hot topics that users are currently following are extracted from users' tweets, which are utilized to obtain the related videos in YouTube. (2) Time-aware video recommendation: for the target user in YouTube, the obtained videos are ranked by considering the user profile in YouTube, time factor, and quality factor to generate the final recommendation list. In this way, the short-term (hot topics) and long-term (user profile) interests of users are jointly considered. Carefully designed experiments have demonstrated the advantages of the proposed method. © 2014 ACM.",Cross-platform; Personalization; Short-term interest; Time-aware; Video recommendation,Data flow analysis; Data streams; Social networking (online); Cross-platform; Personalizations; Short-term interests; Time-aware; Video recommendation; Behavioral research
Aesthetics-guided summarization from multiple user generated videos,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920984006&doi=10.1145%2f2659520&partnerID=40&md5=ab322d6f21275b24e126847cf1cbf493,"In recent years, with the rapid development of camera technology and portable devices, we have witnessed a flourish of user generated videos, which are gradually reshaping the traditional professional video oriented media market. The volume of user generated videos in repositories is increasing at a rapid rate. In today's video retrieval systems, a simple query will return many videos which seriously increase the viewing burden. To manage these video retrievals and provide viewers with an efficient way to browse, we introduce a system to automatically generate a summarization from multiple user generated videos and present their salience to viewers in an enjoyable manner. Among multiple consumer videos, we find their qualities to be highly diverse due to various factors such as a photographer's experience or environmental conditions at the time of capture. Such quality inspires us to include a video quality evaluation component into the video summarization since videos with poor qualities can seriously degrade the viewing experience. We first propose a probabilistic model to evaluate the aesthetic quality of each user generated video. This model compares the rich aesthetics information from several well-known photo databases with generic unlabeled consumer videos, under a human perception component indicating the correlation between a video and its constituting frames. Subjective studies were carried out with the results indicating that our method is reliable. Then a novel graph-based formulation is proposed for the multi-video summarization task. Desirable summarization criteria is incorporated as the graph attributes and the problem is solved through a dynamic programming framework. Comparisons with several state-of-the-art methods demonstrate that our algorithm performs better than other methods in generating a skimming video in preserving the essential scenes from the original multiple input videos, with smooth transitions among consecutive segments and appealing aesthetics overall. © 2014 ACM.",Quality assess; User generated videos; Video quality; Video summary,Dynamic programming; Graphic methods; Video recording; Environmental conditions; Multi-video summarizations; Probabilistic modeling; State-of-the-art methods; User-generated video; Video quality; Video retrieval system; Video summaries; Quality control
Beyond 1Mbps global overlay live streaming: The case of proxy helpers,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920986772&doi=10.1145%2f2652485&partnerID=40&md5=2f689ac16a2b668943861e5c7ba6bfc6,"In order to provide live streaming over the global Internet, a content provider often deploys an overlay network consisting of distributed proxies placed close to user pools. Streaming of multi-Mbps video over such an overlay is challenging because of bandwidth bottlenecks in paths. To effectively overcome these bottlenecks, we consider employing proxy helpers in the overlay to provide rich path diversity. The helpers do not have any attached users, and hence may forward partial video streams (or not at all) if necessary. In this way, the helpers serve as stepping stones to supply full streams to the servers. The issue is how to involve the helpers in the overlay to achieve low streaming delay meeting a certain high streaming bitrate requirement. To address the issue, we first formulate the problem which captures various delay and bandwidth components, and show that it is NP-hard. We then propose an efficient algorithm called Stepping-Stones (SS) which can be efficiently implemented in a controller. Given the encouraging simulation results, we develop a novel streaming testbed for SS and explore, through sets of Internet experiments, the effectiveness of helpers to achieve high bitrate (multi-Mbps) global live streaming. In our experiments, proxies are deployed with a reasonably wide global footprint. We collect more than a hundred hours of streaming traces with bitrate ranging from 500kbps to a few Mbps. Our experimental data validates that helpers indeed play an important role in achieving high bitrate in today's Internet. Global multi-Mbps streaming is possible due to their multihop and multipath advantages. Our experimental trials and data also provide valuable insights on the design of a global push-based streaming network. There are strong benefits of using proxy helpers to achieve high bitrate and low delay. © 2014 ACM.",Global live streaming; Multi-Mbps video; Proxy helpers; Testbed experiment,Bandwidth; Testbeds; Bandwidth bottlenecks; Distributed proxies; Experimental trials; Internet experiments; Live streaming; Multi-Mbps video; Proxy helpers; Streaming networks; Video streaming
Social event classification via boosted multimodal supervised Latent Dirichlet Allocation,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920938850&doi=10.1145%2f2659521&partnerID=40&md5=65eb87c3cee6f162eec17642879c56be,"With the rapidly increasing popularity of social media sites (e.g., Flickr, YouTube, and Facebook), it is convenient for users to share their own comments on many social events, which successfully facilitates social event generation, sharing and propagation and results in a large amount of user-contributed media data (e.g., images, videos, and text) for a wide variety of real-world events of different types and scales. As a consequence, it has become more and more difficult to exactly find the interesting events from massive social media data, which is useful to browse, search and monitor social events by users or governments. To deal with these issues, we propose a novel boosted multimodal supervised Latent Dirichlet Allocation (BMM-SLDA) for social event classification by integrating a supervised topic model, denoted as multi-modal supervised Latent Dirichlet Allocation (mm-SLDA), in the boosting framework. Our proposed BMM-SLDA has a number of advantages. (1) Our mm-SLDA can effectively exploit the multimodality and the multiclass property of social events jointly, and make use of the supervised category label information to classify multiclass social event directly. (2) It is suitable for large-scale data analysis by utilizing boosting weighted sampling strategy to iteratively select a small subset of data to efficiently train the corresponding topic models. (3) It effectively exploits social event structure by the document weight distribution with classification error and can iteratively learn new topic model to correct the previously misclassified event documents. We evaluate our BMM-SLDA on a real world dataset and show extensive experimental results, which demonstrate that our model outperforms state-of-the-art methods. © 2014 ACM.",AdaBoost; Multimodality; Social event classification; Social media; Supervised LDA,Adaptive boosting; Information retrieval systems; Iterative methods; Scales (weighing instruments); Social networking (online); Statistics; Classification errors; Large-scale data analysis; Latent Dirichlet allocation; Multi-modality; Social events; Social media; State-of-the-art methods; Supervised LDA; Classification (of information)
Octree-based 3D logic and computation of spatial relationships in live video query processing,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920946574&doi=10.1145%2f2645864&partnerID=40&md5=a6c0b593c356661e96ae66b5e7c88af9,"Live video computing (LVC) on distributed smart cameras has many important applications; and a database approach based on a Live Video DataBase Management System (LVDBMS) has shown to be effective for general LVC application development. The performance of such a database system relies on accurate interpretation of spatial relationships among objects in the live video. With the popularity of affordable depth cameras, 3D spatial computation techniques have been applied. However, the 3D object models currently used are expensive to compute, and offer limited scalability. We address this drawback in this article by proposing an octree-based 3D spatial logic and presenting algorithms for computing 3D spatial relationships using depth cameras. To support continuous query processing on live video streams, we also develop a GPU-based implementation of the proposed technique to further enhance scalability for real-time applications. Extensive performance studies based on a public RGB-D dataset as well as the LVDBMS prototype demonstrates the correctness and efficiency of our techniques. © 2014 ACM.",3D reconstruction; Live video computing; Live Video DataBase; Spatial relationships,Cameras; Computation theory; Computer circuits; Distributed database systems; Query processing; Scalability; Video recording; Video signal processing; 3D reconstruction; Application development; Continuous query processing; Distributed Smart Cameras; Live video; Real-time application; Spatial computations; Spatial relationships; Three dimensional computer graphics
"Large-area, multilayered, and high-resolution visual monitoring using a dual-camera system",2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920991800&doi=10.1145%2f2645862&partnerID=40&md5=d69b4f46482c3758db5ed26230ca4817,"Large-area, high-resolution visual monitoring systems are indispensable in surveillance applications. To construct such systems, high-quality image capture and display devices are required. Whereas high-quality displays have rapidly developed, as exemplified by the announcement of the 85-inch 4K ultrahigh-definition TV by Samsung at the 2013 Consumer Electronics Show (CES), high-resolution surveillance cameras have progressed slowly and remain not widely used compared with displays. In this study, we designed an innovative framework, using a dual-camera system comprising a wide-angle fixed camera and a high-resolution pan-tilt-zoom (PTZ) camera to construct a large-area, multilayered, and high-resolution visual monitoring system that features multiresolution monitoring of moving objects. First, we developed a novel calibration approach to estimate the relationship between the two cameras and calibrate the PTZ camera. The PTZ camera was calibrated based on the consistent property of distinct pan-tilt angle at various zooming factors, accelerating the calibration process without affecting accuracy; this calibration process has not been reported previously. After calibrating the dual-camera system, we used the PTZ camera and synthesized a large-area and high-resolution background image. When foreground targets were detected in the images captured by the wide-angle camera, the PTZ camera was controlled to continuously track the user-selected target. Last, we integrated preconstructed high-resolution background and low-resolution foreground images captured using the wide-angle camera and the high-resolution foreground image captured using the PTZ camera to generate a large-area, multilayered, and high-resolution view of the scene. © 2014 ACM.","Dual-camera system; Large-area, multilayered, and high-resolution system; PTZ camera; Visual monitoring; Wide-angle fixed camera",Calibration; Cameras; Display devices; High definition television; Monitoring; Space telescopes; Dual-camera systems; Fixed cameras; High-resolution systems; PTZ camera; Visual monitoring; Security systems
ALP: Adaptive loss protection scheme with constant overhead for interactive video applications,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920946282&doi=10.1145%2f2656203&partnerID=40&md5=bdc1b1b1401715abd02986bffc555ea3,"There has been an increasing demand for interactive video transmission over the Internet for applications such as video conferencing, video calls, and telepresence applications. These applications are increasingly moving towards providing High Definition (HD) video quality to users. A key challenge in these applications is to preserve the quality of video when it is transported over best-effort networks that do not guarantee lossless transport of video packets. In such conditions, it is important to protect the transmitted video by using intelligent and adaptive protection schemes. Applications such as HD video conferencing require live interaction among participants, which limits the overall delay the system can tolerate. Therefore, the protection scheme should add little or no extra delay to video transport. We propose a novel Adaptive Loss Protection (ALP) scheme for interactive HD video applications such as video conferencing and video chats. This scheme adds negligible delay to the transmission process and is shown to achieve better quality than other schemes in lossy networks. The proposed ALP scheme adaptively applies four different protection modes to cope with the dynamic network conditions, which results in high video quality in all network conditions. Our ALP scheme consists of four protection modes; each of these modes utilizes a protection method. Two of the modes rely on the state-of-the-art protection methods, and we propose a new Integrated Loss Protection (ILP) method for the other two modes. In the ILP method we integrate three factors for distributing the protection among packets. These three factors are error propagation, region of interest and header information. In order to decide when to switch between the protection modes, a new metric is proposed based on the effectiveness of each mode in performing protection, rather than just considering network statistics such as packet loss rate. Results show that by using this metric not only the overall quality will be improved but also the variance of quality will decrease. One of the main advantages of the proposed ALP scheme is that it does not increase the bit rate overhead in poor network conditions. Our results show a significant gain in video quality, up to 3dB PSNR improvement is achieved using our scheme, compared to protecting all packets equally with the same amount of overhead. © 2014 ACM.",Adaptive video protection; Data partitioning; Error propagation; FEC; FMO; H.264/AVC; HD video; Interactive video; ROI; Slicing; ULP,Digital television; Image communication systems; Image segmentation; Packet networks; Visual communication; Data partitioning; Error propagation; H.264/AVC; HD videos; Interactive video; Slicing; Video protection; Video conferencing
Speaker-following video subtitles,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920943045&doi=10.1145%2f2632111&partnerID=40&md5=1685133fda15f2669bb523906ae177c9,"We propose a new method for improving the presentation of subtitles in video (e.g., TV and movies). With conventional subtitles, the viewer has to constantly look away from the main viewing area to read the subtitles at the bottom of the screen, which disrupts the viewing experience and causes unnecessary eyestrain. Our method places on-screen subtitles next to the respective speakers to allow the viewer to follow the visual content while simultaneously reading the subtitles. We use novel identification algorithms to detect the speakers based on audio and visual information. Then the placement of the subtitles is determined using global optimization. A comprehensive usability study indicated that our subtitle placement method outperformed both conventional fixed-position subtitling and another previous dynamic subtitling method in terms of enhancing the overall viewing experience and reducing eyestrain. © 2014 ACM.",Speaker detection; Speaker-following subtitle placement; Video viewing experience,Global optimization; Audio and visual information; Identification algorithms; Placement methods; Speaker detection; Speaker-following subtitle placement; Usability studies; Video viewing experience; Visual content; Speech recognition
Spatial-temporal tag mining for automatic geospatial video annotation,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920949659&doi=10.1145%2f2658981&partnerID=40&md5=5ff51cafa1cbab1fe88e81f67644e6e0,"Videos are increasingly geotagged and used in practical and powerful GIS applications.However, video search and management operations are typically supported by manual textual annotations, which are subjective and laborious. Therefore, research has been conducted to automate or semi-automate this process. Since a diverse vocabulary for video annotations is of paramount importance towards good search results, this article proposes to leverage crowdsourced data from social multimedia applications that host tags of diverse semantics to build a spatio-temporal tag repository, consequently acting as input to our auto-annotation approach. In particular, to build the tag store, we retrieve the necessary data from several social multimedia applications, mine both the spatial and temporal features of the tags, and then refine and index them accordingly. To better integrate the tag repository, we extend our previous approach by leveraging the temporal characteristics of videos as well. Moreover, we set up additional ranking criteria on the basis of tag similarity, popularity and location bias. Experimental results demonstrate that, by making use of such a tag repository, the generated tags have a wide range of semantics, and the resulting rankings are more consistent with human perception. © 2014 ACM.",Clustering; Geospatial; Location sensors; Mobile videos; Social media; Video tags,Computer networks; Clustering; Geo-spatial; Location sensors; Mobile video; Social media; Video tags; Semantics
"Mulsemedia: State of the art, perspectives, and challenges",2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907855558&doi=10.1145%2f2617994&partnerID=40&md5=5c9491df26ec7ba2f4ec4d0774dd9c45,"Mulsemedia-multiple sensorial media-captures a wide variety of research efforts and applications. This article presents a historic perspective on mulsemedia work and reviews current developments in the area. These take place across the traditional multimedia spectrum-from virtual reality applications to computer games-as well as efforts in the arts, gastronomy, and therapy, to mention a few. We also describe standardization efforts, via the MPEG-V standard, and identify future developments and exciting challenges the community needs to overcome. © 2014 ACM.",Contour perception; Flow visualization; Mulsemedia; Multisensory; Perceptual theory; Visual cortex; Visualization,Arts computing; Flow visualization; Motion Picture Experts Group standards; Visualization; Contour perception; Mulsemedia; Multisensory; Perceptual theories; Visual cortexes; Computer games
"Erratum: A framework for network aware caching for video on demand systems (ACM Transactions on Multimedia Computing, Communications and Applications (2013) 9:4 (22))",2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907816909&doi=10.1145%2f2501652&partnerID=40&md5=d089519a4560d486ea019a932ecb2bc3,[No abstract available],,
User quality of experience of mulsemedia applications,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907813889&doi=10.1145%2f2661329&partnerID=40&md5=e782c109a08ce7b7b1fcc64412593db7,"User Quality of Experience (QoE) is of fundamental importance in multimedia applications and has been extensively studied for decades. However, user QoE in the context of the emerging multiple-sensorial media (mulsemedia) services, which involve different media components than the traditional multimedia applications, have not been comprehensively studied. This article presents the results of subjective tests which have investigated user perception of mulsemedia content. In particular, the impact of intensity of certain mulsemedia components including haptic and airflow on user-perceived experience are studied. Results demonstrate that by making use of mulsemedia the overall user enjoyment levels increased by up to 77%. © 2014 ACM.",Air flow; Cross-modality; Feeling; Haptic; Mulsemedia; Olfaction; Perception,Multimedia services; Sensory perception; Air flow; Cross modality; Feeling; Haptic; Mulsemedia; Olfaction; Quality of service
Attribute-augmented semantic hierarchy: Towards a unified framework for content-based image retrieval,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907812437&doi=10.1145%2f2637291&partnerID=40&md5=93c6fe65f1c963eb31b1f810a5f227d2,"This article presents a novel attribute-augmented semantic hierarchy (A2SH) and demonstrates its effectiveness in bridging both the semantic and intention gaps in content-based image retrieval (CBIR). A2SH organizes semantic concepts into multiple semantic levels and augments each concept with a set of related attributes. The attributes are used to describe the multiple facets of the concept and act as the intermediate bridge connecting the concept and low-level visual content. An hierarchical semantic similarity function is learned to characterize the semantic similarities among images for retrieval. To better capture user search intent, a hybrid feedback mechanism is developed, which collects hybrid feedback on attributes and images. This feedback is then used to refine the search results based on A2SH. We use A2SH as a basis to develop a unified content-based image retrieval system. We conduct extensive experiments on a large-scale dataset of over one million Web images. Experimental results show that the proposed A2SH can characterize the semantic affinities among images accurately and can shape user search intent quickly, leading to more accurate search results as compared to state-of-the-art CBIR solutions. © 2014 ACM.",Attribute; Image retrieval; Semantic hierarchy,Content based retrieval; Search engines; Semantics; Attribute; Content based image retrieval; Content based image retrieval systems; Large-scale dataset; Semantic affinity; Semantic hierarchies; Semantic similarity; Unified framework; Image retrieval
Designing vibrotactile codes to communicate verb phrases,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907859001&doi=10.1145%2f2637289&partnerID=40&md5=0cd6a0219228e0b2a8ef991784382c16,"Soldiers, to guard themselves from enemy assault, have to maintain visual and auditory awareness of their environment. Their visual and auditory senses are thus saturated. This makes these channels less usable for communication. The tactile medium of communication with users is appropriate for displaying information in such situations. Research in interpersonal communication among soldiers shows that the most common form of communication between soldiers involves the use of verb phrases. In this article, we have developed a three-by-three tactile display and proposed a method for mapping the components of a verb phrase to two dimensions of tactile codes-shape and waveform. Perception of tactile codes by users depends on the ability of users to distinguish shape and waveform of the code. We have proposed a measure to rate the distinguish-ability of any two shapes and created a graph-based user-centric model using this measure to select distinguishable shapes from a set of all presentable shapes. We conducted two user studies to evaluate the ability of users to perceive tactile information. The results from our first study showed users' ability to perceive tactile shapes, tactile waveforms, and form verb phrases from tactile codes. The recognition accuracy and time taken to distinguish were better when the shapes were selected from the graph model than when shapes were chosen based on intuition. The second user study was conducted to test the performance of users while performing a primary visual task simultaneously with a secondary audio or haptic task. Users were more familiar with perceiving information from an auditory medium than from a haptic medium, which was reflected in their performance. Thus the performance of users in the primary visual task was better while using an audio medium of communication than while using a haptic medium of communication. © 2014 ACM.",Communication; Graph model; Perception model; Tactile code; Tactile interface; User centric design; Vibrotactile pattern perception,Communication; Graph theory; Graphic methods; Haptic interfaces; Graph model; Perception Modeling; Tactile code; User-centric designs; Vibro-tactile patterns; Codes (symbols)
"Errata for: A framework for network aware caching for video on demand systems (ACM Transactions on Multimedia Computing, Communications and Applications (2013) 9:4 (22) DOI: 10.1145/2501643.2501652)",2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995617475&doi=10.1145%2f2661298&partnerID=40&md5=af48233cff823d9ed0819d2bea1ef485,"Some errors were introduced into this article in the preparation of the final source files. The errors are summarized in the following text and revised pages with the corrected elements indicated in red are provided. The full corrected article can be accessed in the ACM DL, DOI http://dx.doi.org/10.1145/2501643.2501652 - Page 8: New Figure 6(a) - Page 16: New Figures 8(a), 8(b), and 9(a) - Page 17: New Figure 10(b) - Page 18: New Figures 11 and 12; corrected text reference - Page 19: Final sentence deleted. © 2014 ACM.",Algorithms; B.3.2 [memory structures]: design styles - cache memories; Caching; Content distribution networks; Measurement; Video on demand,
EEG correlates of pleasant and unpleasant odor perception,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907858920&doi=10.1145%2f2637287&partnerID=40&md5=0b34a7440129b492ba5e6455e9b60f1a,"Olfaction-enhanced multimedia experience is becoming vital for strengthening the sensation of reality and the quality of user experience. One approach to investigate olfactory perception is to analyze the alterations in brain activity during stimulation with different odors. In this article, the changes in the electroencephalogram (EEG) when perceiving hedonically-different odors are studied. Results of within and across-subject analysis are presented. We show that EEG-based odor classification using brain activity is possible and can be used to automatically recognize odor pleasantness when a subject-specific classifier is trained. However, it is a challenging problem to design a generic classifier. © 2014 ACM.",Brain; Classification; Electroencephalogram; Odors; Olfaction,Brain; Classification (of information); Multimedia systems; Neurophysiology; Odors; Brain activity; Electro-encephalogram (EEG); Generic classifier; Odor classification; Olfaction; Subject-specific; User experience; Electroencephalography
Wow! you are so beautiful today,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907809967&doi=10.1145%2f2659234&partnerID=40&md5=2c9f31558e38fdee5e6bc2d8be58ae48,"Beauty e-Experts, a fully automatic system for makeover recommendation and synthesis, is developed in this work. The makeover recommendation and synthesis system simultaneously considers many kinds of makeover items on hairstyle and makeup. Given a user-provided frontal face image with short/bound hair and no/light makeup, the Beauty e-Experts system not only recommends the most suitable hairdo and makeup, but also synthesizes the virtual hairdo and makeup effects. To acquire enough knowledge for beauty modeling, we built the Beauty e-Experts Database, which contains 1,505 female photos with a variety of attributes annotated with different discrete values.We organize these attributes into two different categories, beauty attributes and beauty-related attributes. Beauty attributes refer to those values that are changeable during the makeover process and thus need to be recommended by the system. Beauty-related attributes are those values that cannot be changed during the makeup process but can help the system to perform recommendation. Based on this Beauty e-Experts Dataset, two problems are addressed for the Beauty e-Experts system: what to recommend and how to wear it, which describes a similar process of selecting hairstyle and cosmetics in daily life. For the what-to-recommend problem, we propose a multiple tree-structured supergraph model to explore the complex relationships among high-level beauty attributes, mid-level beauty-related attributes, and low-level image features. Based on this model, the most compatible beauty attributes for a given facial image can be efficiently inferred. For the how-to-wear-it problem, an effective and efficient facial image synthesis module is designed to seamlessly synthesize the recommended makeovers into the user facial image. We have conducted extensive experiments on testing images of various conditions to evaluate and analyze the proposed system. The experimental results well demonstrate the effectiveness and efficiency of the proposed system. © 2014 ACM.",Beauty recommendation; Beauty synthesis; Multiple tree-structured super-graphs model,Forestry; Recommender systems; Trees (mathematics); Wear of materials; Automatic systems; Beauty recommendation; Complex relationships; Effectiveness and efficiencies; Facial-image synthesis modules; Low-level image features; Multiple trees; Super graph; Image processing
Structured streaming skeleton - A new feature for online human gesture recognition,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907815333&doi=10.1145%2f2648583&partnerID=40&md5=306243865b42f10d07b504c7cd1e5323,"Online human gesture recognition has a wide range of applications in computer vision, especially in humancomputer interaction applications. The recent introduction of cost-effective depth cameras brings a new trend of research on body-movement gesture recognition. However, there are two major challenges: (i) how to continuously detect gestures from unsegmented streams, and (ii) how to differentiate different styles of the same gesture from other types of gestures. In this article, we solve these two problems with a new effective and efficient feature extractionmethod-Structured Streaming Skeleton (SSS)-which uses a dynamicmatching approach to construct a feature vector for each frame. Our comprehensive experiments on MSRC-12 Kinect Gesture, Huawei/3DLife-2013, and MSR-Action3D datasets have demonstrated superior performances than the state-of-the-art approaches. We also demonstrate model selection based on the proposed SSS feature, where the classifier of squared loss regression with l2,1 norm regularization is a recommended classifier for best performance. © 2014 ACM.",Depth camera; Feature extraction; Gesture recognition,Cameras; Cost effectiveness; Feature extraction; Human computer interaction; Musculoskeletal system; 1-norm regularizations; Cost effective; Depth camera; Feature vectors; Human gesture recognition; Model Selection; On-body; State-of-the-art approach; Gesture recognition
A generic utility model representing the quality of sensory experience,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907856682&doi=10.1145%2f2648429&partnerID=40&md5=96f586eeb9c64112dc79a2e3ee2c0451,"Current QoE research is mainly focusing on single modalities (audio, visual) or combinations thereof. In our research, we propose annotating traditional multimedia content with additional sensory effects, such as ambient light, vibration, wind, and olfaction, which could potentially stimulate all human senses. Investigating the influence of individual sensory effects and combinations thereof is important in order to understand how these individual sensory effects influence the Quality of Experience (QoE) as a whole. In this article, we describe the results of such a subjective quality assessment of audio-visual sequences which are annotated with additional sensory effects such as ambient light, wind, and vibration using the MPEG-V standard. The results of this assessment allow us to derive a utility model representing the Quality of Sensory Experience (QuaSE) complementary to existing QoE models described in terms of Quality of Service (QoS) parameters. For validating our proposed utilitymodel, we provide an example instantiation and validate it against results of subjective quality assessments. © 2014 ACM.",MPEG-V; Quality of experience; Sensory experience; Subjective quality assessment,Motion Picture Experts Group standards; Ambient light; Audio-visual; Multimedia contents; Quality of experience (QoE); Quality of Service parameters; Sensory experiences; Subjective quality assessments; Utility model; Quality of service
Discovering geo-informative attributes for location recognition and exploration,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907859601&doi=10.1145%2f2648581&partnerID=40&md5=0518126a2797c48257e93620055b1c40,"This article considers the problem of automatically discovering geo-informative attributes for location recognition and exploration. The attributes are expected to be both discriminative and representative, which correspond to certain distinctive visual patterns and associate with semantic interpretations. For our solution, we analyze the attribute at the region level. Each segmented region in the training set is assigned a binary latent variable indicating its discriminative capability. A latent learning framework is proposed for discriminative region detection and geo-informative attribute discovery. Moreover, we use user-generated content to obtain the semantic interpretation for the discovered visual attributes. Discriminative and searchbased attribute annotation methods are developed for geo-informative attribute interpretation. The proposed approach is evaluated on one challenging dataset including GoogleStreetView and Flickr photos. Experimental results show that (1) geo-informative attributes are discriminative and useful for location recognition; (2) the discovered semantic interpretation is meaningful and can be exploited for further location exploration. © 2014 ACM.",Geo-informative attributes; Latent model; Location recognition,Semantics; Annotation methods; Informative attributes; Latent models; Location recognition; Segmented regions; Semantic interpretation; User-generated content; Visual attributes; Location
Introduction to the special issue best papers of ACM multimedia 2013,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907860009&doi=10.1145%2f2661331&partnerID=40&md5=74b07b9ff2901a53fd851454f70e67ac,[No abstract available],,
Introduction to special issue on multiple sensorial (MulSeMedia) multimodal media: Advances and applications,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907809597&doi=10.1145%2f2661333&partnerID=40&md5=ecdd60c66421ea4cb43b0e8da396756c,[No abstract available],,
Multimodal hand and foot gesture interaction for handheld devices,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907808678&doi=10.1145%2f2645860&partnerID=40&md5=acd46b785f41647c318ae91bcb01fedc,"We present a hand-and-foot-based multimodal interaction approach for handheld devices. Our method combines input modalities (i.e., hand and foot) and provides a coordinated output to both modalities along with audio and video. Human foot gesture is detected and tracked using contour-based template detection (CTD) and Tracking-Learning-Detection (TLD) algorithm. 3D foot pose is estimated from passive homography matrix of the camera. 3D stereoscopic and vibrotactile are used to enhance the immersive feeling.We developed a multimodal football game based on the multimodal approach as a proof-of-concept.We confirm our systems user satisfaction through a user study. © 2014 ACM.",Gesture estimation; HCI; Mobile; Multimodal interaction; Smartphone games; Vibrotactile,Human computer interaction; Interactive computer systems; Stereo image processing; Gesture interaction; Homography matrices; Mobile; Multi-modal approach; Multi-Modal Interactions; Template detection; User satisfaction; Vibrotactile; Hand held computers
Integration of multisensorial stimuli and multimodal interaction in a hybrid 3DTV system,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907817548&doi=10.1145%2f2617992&partnerID=40&md5=35961560a553e2a8ac7ced3aa1bef6d0,"This article proposes the integration of multisensorial stimuli and multimodal interaction components into a sports multimedia asset under two dimensions: immersion and interaction. The first dimension comprises a binaural audio system and a set of sensory effects synchronized with the audiovisual content, whereas the second explores interaction through the insertion of interactive 3D objects into the main screen and on-demand presentation of additional information in a second touchscreen. We present an end-to-end solution integrating these components into a hybrid (internet-broadcast) television system using current 3DTV standards. Results from an experimental study analyzing the perceived quality of these stimuli and their influence on the Quality of Experience are presented. © 2014 ACM.",binaural audio; Hybrid-based 3DTV; Immersive media; interactive 3D objects integrated into the video scene; Interactive media; Multimedia information systems; Multisensorial multimodal media; quality of experience; second screen; sensory effects,Audio systems; Interactive computer systems; Television systems; binaural audio; Hybrid-based 3DTV; Immersive media; Interactive media; Multi-modal; MultiMedia Information Systems; Quality of experience (QoE); Second screens; sensory effects; Video scene; Quality of service
Multiple-scent enhanced multimedia synchronization,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907842836&doi=10.1145%2f2637293&partnerID=40&md5=625dd0001ea0fdc314bc65d070f34407,"This study looked at users' perception of interstream synchronization between audiovisual media and two olfactory streams. The ability to detect skews and the perception and impact of skews on user Quality of Experience (QoE) is analyzed. The olfactory streams are presented with the same skews (i.e., delay) and with variable skews (i.e., jitter and mix of scents). This article reports the limits beyond which desynchronization reduces user-perceived quality levels. Also, a minimum gap between the presentations of consecutive scents is identified, necessary to ensuring enhanced user-perceived quality. There is no evidence (not considering scent type) that overlapping or mixing of scents increases user QoE levels for olfaction-enhanced multimedia. © 2014 ACM.",Multimedia synchronization; Olfaction; Quality of experience; Subjective quality assessment,Odors; Synchronization; Audio-visual media; Desynchronization; Interstream synchronization; Multimedia synchronization; Olfaction; Quality of experience (QoE); Subjective quality assessments; Users' perception; Quality of service
GamingAnywhere: The first open source cloud gaming system,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893227972&doi=10.1145%2f2537855&partnerID=40&md5=0b0b44002d3aa9f525095fee0bf911d4,"We present the first open source cloud gaming system, called GamingAnywhere. In addition to its openness, we have designed, GamingAnywhere for high extensibility, portability, and reconfigurability. We implemented it on Windows, Linux, OS X, and Android. We conducted extensive experiments to evaluate its performance. Our experimental results indicate that GamingAnywhere is efficient, scalable, adaptable to network conditions, and achieves high responsiveness and streaming quality. Gaming-Anywhere can be employed by researchers, game developers, service providers, and end users for setting up cloud gaming testbeds, which we believe, will stimulate more research into innovations for cloud gaming systems and applications. © 2014 ACM.",Cloud games; Live video streaming; Performance evaluation; Performance optimization; Real-time encoding; Remote rendering,Computer operating systems; Live video streaming; Performance evaluation; Performance optimizations; Real-time encoding; Remote rendering; Open systems
Saving disk energy in video servers by combining caching and prefetching,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893287928&doi=10.1145%2f2537856&partnerID=40&md5=910d025afdd5d80989548c2358d10330,"Maintenance and upgrades to the significant storage infrastructure in a video server often create a heterogenous disk array. We show how to manage the energy consumption of such an array by combining caching and prefetching techniques. We first examine how seek operations affect disk energy consumption, and then analyze the relationship between the amount of prefetched data and the number of seeks, and the effect of the size of the prefetching buffer on energy consumption. Based on this, we propose a new data prefetching scheme in which the amount of data prefetched for each video stream is dynamically adjusted to allow for the bit-rates of streams and the power characteristics of different disks. We next examine the impact of caching on disk power consumption and propose a new caching scheme that prioritizes each stream based on the ratio of the amount of energy that can be saved to its cache requirement, so as to make effective use of limited caching space. We address the trade-off between caching and prefetching and propose an algorithm that dynamically divides the entire buffer space into prefetching and caching regions, with the aim of minimizing overall disk energy consumption. Experimental results show that our scheme can reduce disk energy consumption between 26% and 31%, compared to a server without prefetching and caching. © 2014 ACM.",Low-Power Systems; Multimedia Storage Systems; Power Management,Digital storage; Video streaming; Caching and prefetching; Data pre-fetching; Disk Power Consumption; Low-power systems; Multimedia storage; Power characteristic; Power managements; Storage infrastructure; Energy utilization
OSM: Prioritized evolutionary QoS optimization for interactive 3D teleimmersion,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893247123&doi=10.1145%2f2543899&partnerID=40&md5=67b72e623f600e25f2b64b7f2ba1b66d,"Different 3D tele-immersive (3DTI) activities pose different prioritized requirements for application and network-level quality of service (QoS) to ensure a strong quality of experience (QoE) for participants. Some applications put heavy weight on audio quality, some consider higher quality for upper body video streams, and some seek very low end-to-end interactivity delay. In addition, a variation in streaming content may arise in the 3DTI space due to the participants' change in interests (e.g., view change). Therefore, there is a need for an adaptive multistream, multisite 3DTI session management strategy that is not only unobtrusive, but also optimizes prioritized QoS parameters in 3DTI content distribution based on user activity and content variation. To address this next generation session management problem, we revisit the design space of multistream and multisite 3DTI session layer. We present an evolutionary 3DTI session optimization approach using Open Session Management (OSM) architecture that uses a global view of participants and overlays network conditions to optimize QoS parameters. Experimental results with PlanetLab traces show that our optimization process is unobtrusive, and the optimized TI sessions provide higher satisfaction to the participants (in some cases up to 50% higher) compared to the current solutions in the 3DTI space. © 2014 ACM.",3D Tele-immersion; Content distribution; Distributed application; Optimization; Session management,Computer software maintenance; Optimization; Tensors; Three dimensional; Video streaming; 3d tele-immersion; Content distribution; Content variation; Distributed applications; Network condition; Optimization approach; Quality of experience (QoE); Session management; Quality of service
Introduction to the special issue of best papers of ACM MMSys 2013 and ACM NOSSDAV 2013,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893300186&doi=10.1145%2f2557424&partnerID=40&md5=3138d23eb88ca39c6f658de444c5c2f1,[No abstract available],,
User-profile-based perceived olfactory and visual media synchronization,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893253704&doi=10.1145%2f2540994&partnerID=40&md5=42e03bba6ae69b227cc73e2ba8fb588c,"As a step towards enhancing users' perceived multimedia quality levels, this article presents the results of a study which looked at user's perception of inter-stream synchronization between scent and video. The ability to detect and the perception of and impact of skew on user's quality of experience is analyzed considering user's age, sex, and culture (user profile). The results indicate that skews beyond a certain level between olfaction and video have a negative impact on user-perceived experience. Olfaction before video is more noticeable to users than olfaction after video, and assessors are more tolerable of olfactory data presented after video. © 2014 ACM.",Multimedia synchronization; Olfaction; Quality of experience; Subjective quality assessment,Computer networks; Hardware; Interstream synchronization; Multimedia quality; Multimedia synchronization; Olfaction; Quality of experience (QoE); Subjective quality assessments; User's perceptions; Visual media; Quality of service
Bandwidth adaptation for 3D mesh preview streaming,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893306229&doi=10.1145%2f2537854&partnerID=40&md5=1b2b3229e7237d83bc3606f5d710dbd4,"Online galleries of 3D models typically provide two ways to preview a model before the model is downloaded and viewed by the user: (i) by showing a set of thumbnail images of the 3D model taken from representative views (or keyviews); (ii) by showing a video of the 3D model as viewed from a moving virtual camera along a path determined by the content provider. We propose a third approach called preview streaming for mesh-based 3D objects: by streaming and showing parts of the mesh surfaces visible along the virtual camera path. This article focuses on the preview streaming architecture and framework and presents our investigation into how such a system would best handle network congestion effectively. We present three basic methods: (a) STOP-AND-WAIT, where the camera pauses until sufficient data is buffered; (b) REDUCE-SPEED, where the camera slows down in accordance to reduce network bandwidth; and (c) REDUCE-QUALITY, where the camera continues to move at the same speed but fewer vertices are sent and displayed, leading to lower mesh quality. We further propose two advanced methods: (d) KEYVIEWAWARE, which trades off mesh quality and camera speed appropriately depending on how close the current view is to the keyviews, and (e) ADAPTIVE-ZOOM, which improves visual quality by moving the virtual camera away from the original path. A user study reveals that our KEYVIEW-AWARE method is preferred over the basic methods. Moreover, the ADAPTIVE-ZOOM scheme compares favorably to the KEYVIEW-AWARE method, showing that path adaptation is a viable approach to handling bandwidth variation. © 2014 ACM.",3D preview; Camera path; Progressive mesh streaming,Content based retrieval; Three dimensional; 3D preview; Bandwidth adaptation; Bandwidth variation; Content providers; Network congestions; Progressive Mesh; Representative views; Streaming architecture; Cameras
Bagadus: An integrated real-time system for soccer analytics,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893226662&doi=10.1145%2f2541011&partnerID=40&md5=f1d1898988ab0c2199fb24a8990a82be,"The importance of winning has increased the role of performance analysis in the sports industry, and this underscores how statistics and technology keep changing the way sports are played. Thus, this is a growing area of interest, both from a computer system view in managing the technical challenges and from a sport performance view in aiding the development of athletes. In this respect, Bagadus is a real-time prototype of a sports analytics application using soccer as a case study. Bagadus integrates a sensor system, a soccer analytics annotations system, and a video processing system using a video camera array. A prototype is currently installed at Alfheim Stadium in Norway, and in this article, we describe how the system can be used in real-time to playback events. The system supports both stitched panorama video and camera switching modes and creates video summaries based on queries to the sensor system. Moreover, we evaluate the system from a systems point of view, benchmarking different approaches, algorithms, and trade-offs, and show how the system runs in real time. © 2014 ACM.",Camera array; Real-time panorama video; Sensor tracking; Soccer system; Sport analytics; System integration; Video annotation,Cameras; Real time systems; Sensors; Video cameras; Video signal processing; Camera arrays; Real-time panorama; Soccer system; Sport analytics; System integration; Video annotations; SportS
High performance many-to-many intranet screen sharing with DisplayCast,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893959277&doi=10.1145%2f2534328&partnerID=40&md5=e594e5202ff2b03a3a210f61adb9509f,"DisplayCast is a many to many Intranet screen sharing system. Its screen capture mechanism creates a sequence of pixmap images of the screen updates. Prior systems that used a similar approach were designed to operate over constrained wide-area networks and did not exploit the Intranet network conditions to achieve high capture rates. First we empirically analyzed the screen contents for a variety of scenarios. We showed that screen updates were sporadic with long periods of inactivity. When active, screens were updated at far higher rates than was supported by earlier systems. The mismatch was pronounced for interactive scenarios. Even during active screen updates, the number of updated pixels were frequently small. We showed that crucial information can be lost if individual updates were merged. When the available system resources could not support high capture rates, we showed ways in which updates can be effectively collapsed. Next, we investigate compression mechanisms for streaming these updates. Even while using a hardware encoder, lossy compressors such as H.264 were unable to sustain high frame rates. Though Zlib lossless compression operated within the latency and compression rate requirements, the compression efficiency was poor. By analyzing the screen pixels, we developed a practical transformation that significantly improved compression rates. DisplayCast incorporates these observations. It shares the processor and network resources required for screen capture, compression and transmission with host applications whose output needs to be shared. DisplayCast is agile and uses faster processing capability to achieve even higher performance. Our system components operate natively in Windows 7, Mac OS X and iOS and is deployed in a production setting. DisplayCast is released under a New BSD License. © 2014 ACM.",,
A New Data Hiding Method via Revision History Records on Collaborative Writing Platforms,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893953333&doi=10.1145%2f2534408&partnerID=40&md5=7532111fd612a688ec9dda5d7a4983e6,"A new data hiding method via collaboratively-written articles with forged revision history records on collaborative writing platforms is proposed. The hidden message is camouflaged as a stego-document consisting of a stego-article and a revision history created through a simulated process of collaborative writing. The revisions are forged using a database constructed by mining word sequences used in real cases from an English Wikipedia XML dump. Four characteristics of article revisions are identified and utilized to embed secret messages, including the author of each revision, the number of corrected word sequences, the content of the corrected word sequences, and the word sequences replacing the corrected ones. Related problems arising in utilizing these characteristics for data hiding are identified and solved skillfully, resulting in an effective multiway method for hiding secret messages into the revision history. To create more realistic revisions, Huffman coding based on the word sequence frequencies collected from Wikipedia is applied to encode the word sequences. Good experimental results show the feasibility of the proposed method. © 2014 ACM.",,
Memory recall based video search: Finding videos you have seen before based on your memory,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893919725&doi=10.1145%2f2534409&partnerID=40&md5=e5cb845c4089c708d8aef2706c565a81,"We often remember images and videos that we have seen or recorded before but cannot quite recall the exact venues or details of the contents. We typically have vague memories of the contents, which can often be expressed as a textual description and/or rough visual descriptions of the scenes. Using these vague memories, we then want to search for the corresponding videos of interest. We call this ""Memory Recall based Video Search"" (MRVS). To tackle this problem, we propose a video search system that permits a user to input his/her vague and incomplete query as a combination of text query, a sequence of visual queries, and/or concept queries. Here, a visual query is often in the form of a visual sketch depicting the outline of scenes within the desired video, while each corresponding concept query depicts a list of visual concepts that appears in that scene. As the query specified by users is generally approximate or incomplete, we need to develop techniques to handle this inexact and incomplete specification by also leveraging on user feedback to refine the specification. We utilize several innovative approaches to enhance the automatic search. First, we employ a visual query suggestion model to automatically suggest potential visual features to users as better queries. Second, we utilize a color similarity matrix to help compensate for inexact color specification in visual queries. Third, we leverage on the ordering of visual queries and/or concept queries to rerank the results by using a greedy algorithm. Moreover, as the query is inexact and there is likely to be only one or few possible answers, we incorporate an interactive feedback loop to permit the users to label related samples which are visually similar or semantically close to the relevant sample. Based on the labeled samples, we then propose optimization algorithms to update visual queries and concept weights to refine the search results. We conduct experiments on two large-scale video datasets: TRECVID 2010 and YouTube. The experimental results demonstrate that our proposed system is effective for MRVS tasks. © 2014 ACM.",,
Convergence of interactive displays with smart mobile devices for effective advertising: A survey,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893932494&doi=10.1145%2f2557450&partnerID=40&md5=f683518189a2aeb05b8642de665c8656,"The trend of replacing public static signages with digital displays creates opportunities for interactive display systems, which can be used in collaborative workspaces, social gaming platforms and advertising. Based on marketing communication concepts and existing models for consumer behavior, three stages, namely attraction, interaction and conation, are defined in this article to analyze the effectiveness of interactive display advertising. By reviewing various methods and strategies employed by existing systems with attraction, interaction and conation stages, this article concludes that smart mobile devices should be integrated as a component to increase the effectiveness of interactive displays as advertising tools. Future research challenges related to this topic are also discussed. © 2014 ACM.",,
Dynamic load balancing in distributed virtual environments using heat diffusion,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893964250&doi=10.1145%2f2499906&partnerID=40&md5=9d5586cf05747f9c08511b86cb5c4969,"Distributed virtual environments (DVEs) are attracting a lot of attention in recent years, due to the increasing popularity of online gaming and social networks. As the number of concurrent users of a DVE increases, a critical problem is on how the workload among multiple servers can be balanced in order to maintain real-time performance. Although a number of load balancing methods have been proposed, they either try to produce high quality load balancing results and become too slow or emphasize on efficiency and the load balancing results become less effective. In this article, we propose a new approach to address this problem based on heat diffusion. Our work has two main contributions. First, we propose a local and a global load balancing methods for DVEs based on heat diffusion. Second, we investigate two performance factors of the proposed methods, the convergence threshold and the load balancing interval. We have conducted a number of experiments to extensively evaluate the performance of the proposed methods. Our experimental results show that the proposed methods outperform existing methods in that our methods are effective in reducing server overloading while at the same time being efficient. © 2014 ACM.",,
Scalable multimedia content analysis on parallel platforms using python,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893907072&doi=10.1145%2f2517151&partnerID=40&md5=8393f6df36c5e877bc9a43adb028573b,"In this new era dominated by consumer-produced media there is a high demand for web-scalable solutions to multimedia content analysis. A compelling approach to making applications scalable is to explicitly map their computation onto parallel platforms. However, developing efficient parallel implementations and fully utilizing the available resources remains a challenge due to the increased code complexity, limited portability and required low-level knowledge of the underlying hardware. In this article, we present PyCASP, a Python-based framework that automatically maps computation onto parallel platforms from Python application code to a variety of parallel platforms. PyCASP is designed using a systematic, pattern-oriented approach to offer a single software development environment for multimedia content analysis applications. Using PyCASP, applications can be prototyped in a couple hundred lines of Python code and automatically scale to modern parallel processors. Applications written with PyCASP are portable to a variety of parallel platforms and efficiently scale from a single desktop Graphics Processing Unit (GPU) to an entire cluster with a small change to application code. To illustrate our approach, we present three multimedia content analysis applications that use our framework: a state-of-the-art speaker diarization application, a content-based music recommendation system based on the Million Song Dataset, and a video event detection system for consumer-produced videos. We show that across this wide range of applications, our approach achieves the goal of automatic portability and scalability while at the same time allowing easy prototyping in a high-level language and efficient performance of low-level optimized code. © 2014 ACM.",,
Mixed image-keyword query adaptive hashing over multilabel images,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893976012&doi=10.1145%2f2540990&partnerID=40&md5=cc3cfdcf55dc2b6e7d13b138295acc64,"This article defines a new hashing task motivated by real-world applications in content-based image retrieval, that is, effective data indexing and retrieval given mixed query (query image together with user-provided keywords). Our work is distinguished from state-of-the-art hashing research by two unique features: (1) Unlike conventional image retrieval systems, the input query is a combination of an exemplar image and several descriptive keywords, and (2) the input image data are often associated with multiple labels. It is an assumption that is more consistent with the realistic scenarios. The mixed image-keyword query significantly extends traditional image-based query and better explicates the user intention. Meanwhile it complicates semantics-based indexing on the multilabel data. Though several existing hashing methods can be adapted to solve the indexing task, unfortunately they all prove to suffer from low effectiveness. To enhance the hashing efficiency, we propose a novel scheme ""boosted shared hashing"". Unlike prior works that learn the hashing functions on either all image labels or a single label, we observe that the hashing function can be more effective if it is designed to index over an optimal label subset. In other words, the association between labels and hash bits are moderately sparse. The sparsity of the bit-label association indicates greatly reduced computation and storage complexities for indexing a new sample, since only limited number of hashing functions will become active for the specific sample. We develop a Boosting style algorithm for simultaneously optimizing both the optimal label subsets and hashing functions in a unified formulation, and further propose a query-adaptive retrieval mechanism based on hash bit selection for mixed queries, no matter whether or not the query words exist in the training data. Moreover, we show that the proposed method can be easily extended to the case where the data similarity is gauged by nonlinear kernel functions. Extensive experiments are conducted on standard image benchmarks like CIFAR-10, NUS-WIDE and a-TRECVID. The results validate both the sparsity of the bit-label association and the convergence of the proposed algorithm, and demonstrate that the proposed hashing scheme achieves substantially superior performances over state-of-the-art methods under the same hash bit budget. © 2014 ACM.",,
Editorial note,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906888783&doi=10.1145%2f2634234&partnerID=40&md5=afcf2365c1092a3ca495fc75265012a5,[No abstract available],,
Online estimation of evolving human visual interest,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906890895&doi=10.1145%2f2632284&partnerID=40&md5=1e8ac7455165c488660d3361ae97b2d7,"Regions in video streams attracting human interest contribute significantly to human understanding of the video. Being able to predict salient and informative Regions of Interest (ROIs) through a sequence of eye movements is a challenging problem. Applications such as content-aware retargeting of videos to different aspect ratios while preserving informative regions and smart insertion of dialog (closed-caption text)1 into the video stream can significantly be improved using the predicted ROIs. We propose an interactive humanin-the-loop framework to model eye movements and predict visual saliency into yet-unseen frames. Eye tracking and video content are used to model visual attention in a manner that accounts for important eyegaze characteristics such as temporal discontinuities due to sudden eye movements, noise, and behavioral artifacts. A novel statistical- and algorithm-based method gaze buffering is proposed for eye-gaze analysis and its fusion with content-based features. Our robust saliency prediction is instantiated for two challenging and exciting applications. The first application alters video aspect ratios on-the-fly using content-aware video retargeting, thus making them suitable for a variety of display sizes. The second application dynamically localizes active speakers and places dialog captions on-the-fly in the video stream. Our method ensures that dialogs are faithful to active speaker locations and do not interfere with salient content in the video stream. Our framework naturally accommodates personalisation of the application to suit biases and preferences of individual users. © 2014 ACM.",Gaze; Video captioning; Video retargeting; Visual attention,Aspect ratio; Forecasting; Video streaming; Content-aware video retargeting; Content-based features; Gaze; Human understanding; Regions of interest; Video captioning; Video retargeting; Visual Attention; Eye movements
Circle & search: Attribute-aware shoe retrieval,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906853416&doi=10.1145%2f2632165&partnerID=40&md5=80cd974ddc644d1c3c349c35a8721ccf,"Taking the shoe as a concrete example, we present an innovative product retrieval system that leverages object detection and retrieval techniques to support a brand-new online shopping experience in this article. The system, called Circle & Search, enables users to naturally indicate any preferred product by simply circling the product in images as the visual query, and then returns visually and semantically similar products to the users. The system is characterized by introducing attributes in both the detection and retrieval of the shoe. Specifically, we first develop an attribute-aware part-based shoe detection model. By maintaining the consistency between shoe parts and attributes, this shoe detector has the ability to model high-order relations between parts and thus the detection performance can be enhanced. Meanwhile, the attributes of this detected shoe can also be predicted as the semantic relations between parts. Based on the result of shoe detection, the system ranks all the shoes in the repository using an attribute refinement retrieval model that takes advantage of query-specific information and attribute correlation to provide an accurate and robust shoe retrieval. To evaluate this retrieval system, we build a large dataset with 17,151 shoe images, in which each shoe is annotated with 10 shoe attributes e.g., heel height, heel shape, sole shape, etc.). According to the experimental result and the user study, our Circle & Search system achieves promising shoe retrieval performance and thus significantly improves the users' online shopping experience. © 2014 ACM.",Attribute learning; Object detection; Shoe retrieval,Electronic commerce; Image retrieval; Object recognition; Semantics; Attribute learning; Detection performance; Innovative product; Object Detection; Retrieval performance; Retrieval techniques; Semantic relations; Shoe retrieval; Search engines
A sketch-based approach for interactive organization of video clips,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906860044&doi=10.1145%2f2645643&partnerID=40&md5=0a4e5e3b5892f741899eeabbbae96d56,"With the rapid growth of video resources, techniques for efficient organization of video clips are becoming appealing in the multimedia domain. In this article, a sketch-based approach is proposed to intuitively organize video clips by: (1) enhancing their narrations using sketch annotations and (2) structurizing the organization process by gesture-based free-form sketching on touch devices. There are two main contributions of this work. The first is a sketch graph, a novel representation for the narrative structure of video clips to facilitate content organization. The second is a method to perform context-aware sketch recommendation scalable to large video collections, enabling common users to easily organize sketch annotations. A prototype system integrating the proposed approach was evaluated on the basis of five different aspects concerning its performance and usability. Two sketch searching experiments showed that the proposed context-aware sketch recommendation outperforms, in terms of accuracy and scalability, two state-of-the-art sketch searching methods. Moreover, a user study showed that the sketch graph is consistently preferred over traditional representations such as keywords and keyframes. The second user study showed that the proposed approach is applicable in those scenarios where the video annotator and organizer were the same person. The third user study showed that, for video content organization, using sketch graph users took on average 1/3 less time than using a mass-market tool Movie Maker and took on average 1/4 less time than using a state-of-theart sketch alternative. These results demonstrated that the proposed sketch graph approach is a promising video organization tool. © 2014 ACM.",Contextaware recommendation; Sketch annotation; Sketching interface; Video organization,Computer networks; Hardware; Context-aware recommendations; Narrative structures; Searching methods; Sketch annotation; Sketching interface; Video collections; Video content organization; Video organization; Video cameras
Exploration in interactive personalized music recommendation: A reinforcement learning approach,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906856108&doi=10.1145%2f2623372&partnerID=40&md5=da52cbcf39aaff70ff8fd51e7294ca75,"Current music recommender systems typically act in a greedymanner by recommending songs with the highest user ratings. Greedy recommendation, however, is suboptimal over the long term: it does not actively gather information on user preferences and fails to recommend novel songs that are potentially interesting. A successful recommender system must balance the needs to explore user preferences and to exploit this information for recommendation. This article presents a new approach to music recommendation by formulating this exploration-exploitation trade-off as a reinforcement learning task. To learn user preferences, it uses Bayesian model that accounts for both audio content and the novelty of recommendations. A piecewise-linear approximation to the model and a variational inference algorithm help to speed up Bayesian inference. One additional benefit of our approach is a single unified model for both music recommendation and playlist generation. We demonstrate the strong potential of the proposed approach with simulation results and a user study. © 2014 ACM.",Application; Machine learning; Model; Music; Recommender systems,Applications; Approximation algorithms; Artificial intelligence; Bayesian networks; Inference engines; Learning systems; Models; Piecewise linear techniques; Recommender systems; Reinforcement learning; Bayesian inference; Exploration exploitations; Music; Music recommendation; Music recommender systems; Piecewise linear approximations; Reinforcement learning approach; Variational inference; Quality of service
A top-down approach for video summarization,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906857262&doi=10.1145%2f2632267&partnerID=40&md5=984d6bc6c8771e83b2423521e55aae48,"While most existing video summarization approaches aim to identify important frames of a video from either a global or local perspective, we propose a top-down approach consisting of scene identification and scene summarization. For scene identification, we represent each frame with global features and utilize a scalable clustering method.We then formulate scene summarization as choosing those frames that best cover a set of local descriptors with minimal redundancy. In addition, we develop a visual word-based approach to make our approach more computationally scalable. Experimental results on two benchmark datasets demonstrate that our proposed approach clearly outperforms the state-of-the-art. © 2014 ACM.",Clustering; Keyframe extraction; Keypoint; Local visual word; Scene identification,Computer networks; Hardware; Clustering; Key-frame extraction; Keypoint; Scene identifications; Visual word; Video recording
Up-fusion: An evolving multimedia fusion method,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906849887&doi=10.1145%2f2611777&partnerID=40&md5=37c8985b4d42a728864c32af194c9591,"The amount of multimedia data on the Internet has increased exponentially in the past few decades and this trend is likely to continue. Multimedia content inherently has multiple information sources, therefore effective fusion methods are critical for data analysis and understanding. So far, most of the existing fusion methods are static with respect to time, making it difficult for them to handle the evolving multimedia content. To address this issue, in recent years, several evolving fusion methods were proposed, however, their requirements are difficult to meet, making them useful only in limited applications. In this article, we propose a novel evolving fusion method based on the online portfolio selection theory. The proposed method takes into account the correlation among different information sources and evolves the fusion model when new multimedia data is added. It performs effectively on both crisp and soft decisions without requiring additional context information. Extensive experiments on concept detection and human detection tasks over the TRECVID dataset and surveillance data have been conducted and significantly better performance has been obtained. © 2014 ACM.",Fusion; Portfolio theory; Up-fusion,Fusion reactions; Network security; Better performance; Context information; Information sources; Multimedia contents; Online portfolios; Portfolio theories; Surveillance data; Up-fusion; Content based retrieval
PROPANE: A progressive panorama streaming protocol to support interactive 3d virtual environment exploration on graphics-constrained devices,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906883480&doi=10.1145%2f2602222&partnerID=40&md5=4fd0cb9faebe287cef7ac88bfb78b700,"Image-Based Rendering (IBR) has become widely known by its relatively low requirements for generating new scenes based on a sequence of reference images. This characteristic of IBR shows a remarkable potential impact in rendering complex 3D virtual environments on graphics-constrained devices, such as head-mounted displays, set-top boxes, media streaming devices, and so on. If well exploited, IBR coupled with remote rendering would enable the exploration of complex virtual environments on these devices. However, remote rendering requires the transmission of a large volume of images. In addition, existing solutions consider limited and/or deterministic navigation schemes as a means of decreasing the volume of streamed data. This article proposes the PROgressive PANorama StrEaming protocol (PROPANE) to offer users a smoother virtual navigation experience by prestreaming the imagery data required to generate new views as the user wanders within a 3D environment. PROPANE is based on a very simple yet effective trigonometry model and uses a strafe (lateral movement) technique to minimize the delay between image updates at the client end. This article introduces the concept of key partial panoramas, namely panorama segments that cover movements in any direction by simply strafing from an appropriate key partial panorama and streaming the amount of lost pixels. Therefore, PROPANE can provide a constrained device with sufficient imagery data to cover a future user's viewpoints, thereby minimizing the impact of transmission delay and jitter. PROPANE has been implemented and compared to two baseline remote rendering schemes. The evaluation results show that the proposed technique outperforms the selected and closely related existing schemes by minimizing the response time while not limiting the user to predefined paths as opposed to previous protocols. © 2014 ACM.",Image-based rendering; Mobile devices; Multimedia communications; Remote rendering and streaming; Streaming protocol; Wireless networks,Image reconstruction; Mobile devices; Multimedia systems; Propane; Three dimensional computer graphics; Virtual reality; Wireless networks; 3-D virtual environment; Constrained devices; Head mounted displays; Image-Based Rendering; Multi-media communications; Remote rendering; STreaming protocols; Transmission delays; Rendering (computer graphics)
Understanding video sharing propagation in social networks: Measurement and analysis,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905221160&doi=10.1145%2f2594440&partnerID=40&md5=af907aead25889264fdee0148aa33e02,"Modern online social networking has drastically changed the information distribution landscape. Recently, video has become one of the most important types of objects spreading among social networking service users. The sheer and ever-increasing data volume, the broader coverage, and the longer access durations of video objects, however, present significantly more challenges than other types of objects. This article takes an initial step toward understanding the unique characteristics of video sharing propagation in social networks. Based on realworld data traces from a large-scale online social network, we examine the user behavior from diverse aspects and identify different types of users involved in video propagation. We closely investigate the temporal distribution during propagation as well as the typical propagation structures, revealing more details beyond stationary coverage. We further extend the conventional epidemic models to accommodate diverse types of users and their probabilistic viewing and sharing behaviors. The model, effectively capturing the essentials of the propagation process, serves as a valuable basis for such applications as workload synthesis, traffic prediction, and resource provision of video servers.",Information propagation; Measurement; Social network; Video sharing,Behavioral research; Measurements; Information distributions; Information propagation; Measurement and analysis; On-line social networks; Online social networkings; Social networking services; Temporal distribution; Video sharing; Social networking (online)
Cross-domain multi-event tracking via CO-PMHT,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905258215&doi=10.1145%2f2602633&partnerID=40&md5=600e8fbf79425fddda4e969d9f440032,"With the massive growth of events on the Internet, efficient organization and monitoring of events becomes a practical challenge. To deal with this problem, we propose a novel CO-PMHT (CO-Probabilistic Multi-Hypothesis Tracking) algorithm for crossdomain multi-event tracking to obtain their informative summary details and evolutionary trends over time. We collect a largescale dataset by searching keywords on two domains (Gooogle News and Flickr) and downloading both images and textual content for an event. Given the input data, our algorithm can track multiple events in the two domains collaboratively and boost the tracking performance. Specifically, the bridge between two domains is a semantic posterior probability, that avoids the domain gap. After tracking, we can visualize the whole evolutionary process of the event over time and mine the semantic topics of each event for deep understanding and event prediction. The extensive experimental evaluations on the collected dataset well demonstrate the effectiveness of the proposed algorithm for cross-domain multi-event tracking.",CO-PMHT; Cross-domain; Multi-event tracking; Multi-modality; PMHT,Semantics; CO-PMHT; Cross-domain; Evolutionary process; Experimental evaluation; Multi-hypothesis tracking; Multi-modality; PMHT; Posterior probability; Evolutionary algorithms
Personalized video recommendation through graph propagation,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905239309&doi=10.1145%2f2598779&partnerID=40&md5=a0eea147864867fd832f1010209b35ad,"The rapid growth of the number of videos on the Internet provides enormous potential for users to find content of interest. However, the vast quantity of videos also turns the finding process into a difficult task. In this article, we address the problem of providing personalized video recommendation for users. Rather than only exploring the user-video bipartite graph that is formulated using click information, we first combine the clicks and queries information to build a tripartite graph. In the tripartite graph, the query nodes act as bridges to connect user nodes and video nodes. Then, to further enrich the connections between users and videos, three subgraphs between the same kinds of nodes are added to the tripartite graph by exploring content-based information (video tags and textual queries). We propose an iterative propagation algorithm over the enhanced graph to compute the preference information of each user. Experiments conducted on a dataset with 1, 369 users, 8, 765 queries, and 17, 712 videos collected from a commercial video search engine demonstrate the effectiveness of the proposed method. © 2014 ACM.",Graph propagation; Personalized recommendation; Video recommendation,Iterative methods; Search engines; Telecommunication networks; Commercial video; Content-based information; Personalized recommendation; Personalized video; Preference information; Propagation algorithm; Tripartite graphs; Video recommendation; Graph theory
Personalized photograph ranking and selection system considering positive and negative user feedback,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905254283&doi=10.1145%2f2584105&partnerID=40&md5=449c5fe8416361fddc9c15287e7413d7,"In this article, we propose a novel personalized ranking system for amateur photographs. The proposed framework treats the photograph assessment as a ranking problem and we introduce the idea of personalized ranking, which ranks photographs considering both their aesthetic qualities and personal preferences. Photographs are described using three types of features: photo composition, color and intensity distribution, and personalized features. An aesthetic prediction model is learned from labeled photographs by using the proposed image features and RBF-ListNet learning algorithm. The experimental results show that the proposed framework outperforms in the ranking performance: a Kendall's tau value of 0.432 is significantly higher than those obtained by the features proposed in one of the state-of-the-art approaches (0.365) and by learning based on support vector regression (0.384). To realize personalization in ranking, three approaches are proposed: the feature-based approach allows users to select photographs with specific rules, the example-based approach takes the positive feedback from users to rerank the photograph, and the list-based approach takes both positive and negative feedback from users into consideration. User studies indicate that all three approaches are effective in both aesthetic and personalized ranking.",Aesthetic rules; Example-based reranking; Personalized ranking; Photograph; Photograph ranking,Feedback; Aesthetic rules; Personalized ranking; Photograph; Photograph ranking; Re-ranking; Photography
Fast near-duplicate image detection using uniform randomized trees,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905254848&doi=10.1145%2f2602186&partnerID=40&md5=e0f4c6f4de01bc938c6ad3d2388d404f,"Indexing structure plays an important role in the application of fast near-duplicate image detection, since it can narrow down the search space. In this article, we develop a cluster of uniform randomized trees (URTs) as an efficient indexing structure to perform fast near-duplicate image detection. The main contribution in this article is that we introduce ""uniformity"" and ""randomness"" into the indexing construction. The uniformity requires classifying the object images into the same scale subsets. Such a decision makes good use of the two facts in near-duplicate image detection, namely: (1) the number of categories is huge; (2) a single category usually contains only a small number of images. Therefore, the uniform distribution is very beneficial to narrow down the search space and does not significantly degrade the detection accuracy. The randomness is embedded into the generation of feature subspace and projection direction, improveing the flexibility of indexing construction. The experimental results show that the proposed method is more efficient than the popular locality-sensitive hashing and more stable and flexible than the traditional KD-tree.",Fast near-duplicate image detection; Indexing structure; Uniform randomized tree,Indexing (of information); Random processes; Trees (mathematics); Detection accuracy; Feature subspace; Indexing structures; Locality sensitive hashing; Near-duplicate image detection; Projection direction; Randomized trees; Uniform distribution; Content based retrieval
Bilateral correspondence model for words-and-pictures association in multimedia-rich microblogs,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905222346&doi=10.1145%2f2611388&partnerID=40&md5=5327314b8d6e092d1c77814a18a4187b,"Nowadays, the amount of multimedia contents in microblogs is growing significantly. More than 20% of microblogs link to a picture or video in certain large systems. The rich semantics in microblogs provides an opportunity to endow images with higher-level semantics beyond object labels. However, this raises new challenges for understanding the association between multimodal multimedia contents in multimedia-rich microblogs. Disobeying the fundamental assumptions of traditional annotation, tagging, and retrieval systems, pictures and words in multimedia-rich microblogs are loosely associated and a correspondence between pictures and words cannot be established. To address the aforementioned challenges, we present the first study analyzing and modeling the associations between multimodal contents in microblog streams, aiming to discover multimodal topics from microblogs by establishing correspondences between pictures and words in microblogs. We first use a data-driven approach to analyze the new characteristics of the words, pictures, and their association types in microblogs. We then propose a novel generative model called the Bilateral Correspondence Latent Dirichlet Allocation (BC-LDA) model. Our BC-LDA model can assign flexible associations between pictures and words and is able to not only allow picture-word co-occurrence with bilateral directions, but also single modal association. This flexible association can best fit the data distribution, so that the model can discover various types of joint topics and generate pictures and words with the topics accordingly. We evaluate this model extensively on a large-scale real multimedia-rich microblogs dataset.We demonstrate the advantages of the proposed model in several application scenarios, including image tagging, text illustration, and topic discovery. The experimental results demonstrate that our.",Image analysis; Social media; Topic models,Image analysis; Semantics; Statistics; Application scenario; Data distribution; Data-driven approach; Latent Dirichlet allocation; Multimedia contents; Retrieval systems; Social media; Topic model; Indexing (of information)
Placing videos on a semantic hierarchy for search result navigation,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905235707&doi=10.1145%2f2578394&partnerID=40&md5=ee8fd932b82fa4fef0c2a2f8aa425709,"Organizing video search results in a list view is widely adopted by current commercial search engines, which cannot support efficient browsing for complex search topics that have multiple semantic facets. In this article, we propose to organize video search results in a highly structured way. Specifically, videos are placed on a semantic hierarchy that accurately organizes various facets of a given search topic. To pick the most suitable videos for each node of the hierarchy, we define and utilize three important criteria: relevance, uniqueness, and diversity. Extensive evaluations on a large YouTube video dataset demonstrate the effectiveness of our approach.",Video organization; Visualization; Web video browsing,Flow visualization; Multimedia systems; Search engines; Complex searches; Semantic hierarchies; Video dataset; Video organization; Video search; Web video; YouTube; Semantics
Protecting the content integrity of digital imagery with fidelity preservation: An improved version,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898875501&doi=10.1145%2f2568224&partnerID=40&md5=037d34ab0aed625a82b661d96389640d,"Fragile watermarking has attracted a lot of attention in the last decade. An interesting approach, presented in 2011 by Lin et al., results in very high quality of the watermarked images. However, after a thorough examination of the paper, a few improvements are proposed in our revised version of the algorithm in order to overcome some shortcomings. In particular, changes to the pseudocode and modifications to deal with pixel saturation are suggested, along with a way to improve the scheme security. Finally, a deeper analysis of the security is presented. © 2014 ACM 1551-6857/2014/04-ART23 $15.00.",Content integrity; Fragile watermarking; Information hiding,Computer networks; Hardware; Content integrity; Digital imagery; Fragile watermarking; High quality; Information hiding; Pseudo-code; Watermarked images; Watermarking
A quality of experience model for haptic virtual environments,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898893876&doi=10.1145%2f2540991&partnerID=40&md5=3e9b806f0f5bf6e22eb178451fe93256,"Haptic-based Virtual Reality (VR) applications have many merits. What is still obscure, from the designer's perspective of these applications, is the experience the users will undergo when they use the VR system. Quality of Experience (QoE) is an evaluation metric from the user's perspective that unfortunately has received limited attention from the research community. Assessing the QoE of VR applications reflects the amount of overall satisfaction and benefits gained from the application in addition to laying the foundation for ideal user-centric design in the future. In this article, we propose a taxonomy for the evaluation of QoE for multimedia applications and in particular VR applications. We model this taxonomy using a Fuzzy logic Inference System (FIS) to quantitatively measure the QoE of haptic virtual environments. We build and test our FIS by conducting a users'study analysis to evaluate the QoE of a haptic game application. Our results demonstrate that the proposed FIS model reflects the user's estimation of the application's quality significantly with low error and hence is suited for QoE evaluation. © 2014 ACM 1551-6857/2014/04-ART23 $15.00.",Fuzzy logic evaluation; Haptic I/O; Haptic multimedia application; QoE; Quality of experience; Virtual reality,Fuzzy logic; Quality control; Taxonomies; Virtual reality; Fuzzy logic inference; Haptic I/O; Haptic virtual environments; Multimedia applications; QoE; Quality of experience (QoE); Research communities; User-centric designs; Quality of service
Identifying compression history of wave audio and its applications,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898870328&doi=10.1145%2f2575978&partnerID=40&md5=49bef2ec8c3e3cba78921e200d10c13e,"Audio signal is sometimes stored and/or processed inWAV (waveform) format without any knowledge of its previous compression operations. To perform some subsequent processing, such as digital audio forensics, audio enhancement and blind audio quality assessment, it is necessary to identify its compression history. In this article, we will investigate how to identify a decompressed wave audio that went through one of three popular compression schemes, includingMP3,WMA (windows media audio) and AAC (advanced audio coding). By analyzing the corresponding frequency coefficients, including modified discrete cosine transform (MDCT) and Mel-frequency cepstral coefficients (MFCCs), of those original audio clips and their decompressed versions with different compression schemes and bit rates, we propose several statistics to identify the compression scheme as well as the corresponding bit rate previously used for a given WAV signal. The experimental results evaluated on 8,800 audio clips with various contents have shown the effectiveness of the proposed method. In addition, some potential applications of the proposed method are discussed. © 2014 ACM 1551-6857/2014/04-ART23 $15.00.",Audio compression history identification; Mel-frequency cepstral coefficients; Modified discrete cosine transform,Audio recordings; Discrete cosine transforms; Advanced Audio Coding; Audio compression history; Audio quality assessments; Compression operations; Frequency coefficient; Mel-frequency cepstral coefficients; Modified discrete cosine transforms; Windows Media Audio; Audio signal processing
Dissecting user behaviors for a simultaneous live and VoD IPTV system,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898883508&doi=10.1145%2f2568194&partnerID=40&md5=e42d9c9a51e99edabb6758c23362dcab,"IPTV services deployed nowadays often consist of both live TV and Video-on-Demand (VoD), offered by the same service provider to the same pool of users over the same managed network. Understanding user behaviors in such a setting is hence an important step for system modelling and optimization. Previous studies on user behavior on video services were on either live TV or VoD. For the first time, we conduct an in-depth large-scale behavior study for IPTV users offering simultaneously live TV and VoD choices at the same time. Our data is from the largest IPTV service provider in China, offering hundreds of live channels and hundreds of thousands of VoD files, with traces covering more than 1.9 million users over a period of 5 months. This large dataset provides us a unique opportunity to cross-compare user viewing behaviors for these services on the same platform, and sheds valuable insights on how users interact with such a simultaneous system. Our results lead to new understanding on IPTV user behaviors which have strong implications on system design. For example, we find that the average holding time for VoD is significantly longer than live TV. live TV users tend to surf more. However, if such channel surfing is discounted, the holding times of both services are not much different. While users in VoD tend to view HD longer, channel popularity for live TV is much less dependent on its video quality. In contrast to some popular assumptions on user interactivity, the transitions among live TV, VoD, and offline modes are far from a Markov model. © 2014 ACM 1551-6857/2014/04-ART23 $15.00.",IPTV; Live TV; User behavior; VoD,IPTV; Markov processes; Television broadcasting; Video on demand; Behavior studies; Channel Surfing; Managed networks; Service provider; System modelling; User behaviors; Video-on-Demand (VoD); VoD; Behavioral research
Saving energy in mobile devices for on-demand multimedia streaming a cross-layer approach,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898879063&doi=10.1145%2f2556942&partnerID=40&md5=f84324ed5d5bd3f3d89859960676769e,"This article proposes a novel energy-efficient multimedia delivery system called EStreamer. First, we study the relationship between buffer size at the client, burst-shaped TCP-based multimedia traffic, and energy consumption of wireless network interfaces in smartphones. Based on the study, we design and implement EStreamer for constant bit rate and rate-adaptive streaming. EStreamer can improve battery lifetime by 3x, 1.5x, and 2x while streaming over Wi-Fi, 3G, and 4G, respectively. © 2014 ACM 1551-6857/2014/04-ART23 $15.00.",Constant bit rate; Cross-layer; DASH; Energy efficiency; Multimedia streaming; Radio signaling; Rate-adaptive streaming; Video streaming; Wireless network,Bits; Energy efficiency; Energy utilization; Video streaming; Wireless networks; Constant bit rate; Cross-layer; Cross-layer approach; DASH; Design and implements; Multimedia delivery systems; Multimedia streaming; On-demand multimedia streaming; Mobile devices
A hamming embedding kernel with informative bag-of-visual words for video semantic indexing,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898920556&doi=10.1145%2f2535938&partnerID=40&md5=d0490e938ac49f7a74c76ffccc342dbd,"In this article, we propose a novel Hamming embedding kernel with informative bag-of-visual words to address two main problems existing in traditional BoW approaches for video semantic indexing. First, Hamming embedding is employed to alleviate the information loss caused by SIFT quantization. The Hamming distances between keypoints in the same cell are calculated and integrated into the SVM kernel to better discriminate different image samples. Second, to highlight the concept-specific visual information, we propose to weight the visual words according to their informativeness for detecting specific concepts. We show that our proposed kernels can significantly improve the performance of concept detection. © 2014 ACM 1551-6857/2014/04-ART23 $15.00.",Bag-of-visual word; Hamming embedding; Kernel optimization; Video semantic indexing,Hamming distance; Bag-of-visual word; Concept detection; Hamming embedding; Information loss; Informative ness; Kernel optimizations; Video semantics; Visual information; Indexing (of information)
DIP: Distributed identification of polluters in P2P live streaming,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898908554&doi=10.1145%2f2568223&partnerID=40&md5=1fb75954c2a799857c4e7f83109eea24,"Peer-to-peer live streaming applications are vulnerable to malicious actions of peers that deliberately modify data to decrease or prevent the fruition of the media (pollution attack). In this article we propose DIP, a fully distributed, accurate, and robust algorithm for the identification of polluters. DIP relies on checks that are computed by peers upon completing reception of all blocks composing a data chunk. A check is a special message that contains the set of peer identifiers that provided blocks of the chunk as well as a bit to signal if the chunk has been corrupted. Checks are periodically transmitted by peers to their neighbors in the overlay network; peers receiving checks use them to maintain a factor graph. This graph is bipartite and an incremental belief propagation algorithm is run on it to compute the probability of a peer being a polluter. Using a prototype deployed over PlanetLab we show by extensive experimentation that DIP allows honest peers to identify polluters with very high accuracy and completeness, even when polluters collude to deceive them. Furthermore, we show that DIP is efficient, requiring low computational, communication, and storage overhead at each peer. © 2014 ACM 1551-6857/2014/04-ART23 $15.00.",Belief propagation; Malicious node identification; P2P streaming; Peer-to-peer; PlanetLab; Pollution attack; Statistical inference,Algorithms; Digital storage; Distributed computer systems; Overlay networks; Pollution; Video streaming; Belief propagation; Malicious nodes; P2P streaming; Peer to peer; PlanetLab; Pollution attack; Statistical inference; Peer to peer networks
Mesh discriminative features for 3D steganalysis,2014,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898898069&doi=10.1145%2f2535555&partnerID=40&md5=a6280049153b11d032bf1be184e10cc2,"We propose a steganalytic algorithm for triangle meshes, based on the supervised training of a classifier by discriminative feature vectors. After a normalization step, the triangle mesh is calibrated by one step of Laplacian smoothing and then a feature vector is computed, encoding geometric information corresponding to vertices, edges and faces. For a given steganographic or watermarking algorithm, we create a training set containing unmarkedmeshes and meshes marked by that algorithm, and train a classifier using Quadratic Discriminant Analysis. The performance of the proposed method was evaluated on six well-known watermarking/steganographic schemes with satisfactory accuracy rates. © 2014 ACM 1551-6857/2014/04-ART23 $15.00.",Data embedding; Discriminative feature vector; Steganalysis; Triangle meshes,Algorithms; Digital watermarking; Discriminant analysis; Mesh generation; Data embedding; Discriminative features; Geometric information; Quadratic discriminant analysis; Steganalysis; Steganalytic algorithms; Triangle mesh; Watermarking algorithms; Steganography
Annotation propagation in image databases using similarity graphs,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891794862&doi=10.1145%2f2487736&partnerID=40&md5=d018cfea2076f77a51a4efa6204bd79e,"The practicality of large-scale image indexing and querying methods depends crucially upon the availability of semantic information. The manual tagging of images with semantic information is in general very labor intensive, and existing methods for automated image annotation may not always yield accurate results. The aim of this paper is to reduce to a minimum the amount of human intervention required in the semantic annotation of images, while preserving a high degree of accuracy. Ideally, only one copy of each object of interest would be labeled manually, and the labels would then be propagated automatically to all other occurrences of the objects in the database. To this end, we propose an influence propagation strategy, SW-KProp, that requires no human intervention beyond the initial labeling of a subset of the images. SW-KProp distributes semantic information within a similarity graph defined on all images in the database: each image iteratively transmits its current label information to its neighbors, and then readjusts its own label according to the combined influences of its neighbors. SW-KProp influence propagation can be efficiently performed by means of matrix computations, provided that pairwise similarities of images are available. We also propose a variant of SW-KProp which enhances the quality of the similarity graph by selecting a reduced feature set for each prelabeled image and rebuilding its neighborhood. The performances of the SW-KProp method and its variant were evaluated against several competing methods on classification tasks for three image datasets: a handwritten digit dataset, a face dataset and a web image dataset. For the digit images, SW-KProp and its variant performed consistently better than the other methods tested. For the face and web images, SW-KProp outperformed its competitors for the case when the number of prelabeled images was relatively small. The performance was seen to improve significantly when the feature selection strategy was applied. © 2013 ACM.",Classification; Feature selection; Image annotation; Iterative method; Linear system; Neighborhood,Classification (of information); Database systems; Feature extraction; Image analysis; Image retrieval; Linear systems; Matrix algebra; Pattern recognition; Query processing; Semantics; Annotation propagation; Automated image annotations; High degree of accuracy; Image annotation; Large-scale image indexing; Neighborhood; Propagation strategies; Semantic annotations; Iterative methods
Enhancing news organization for convenient retrieval and browsing,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891756003&doi=10.1145%2f2488732&partnerID=40&md5=70902cf942bbbdd64a51200c2d541f5b,"To facilitate users to access news quickly and comprehensively, we design a news search and browsing system named GeoVisNews, in which the news elements of ""Where"", ""Who"", ""What"" and ""When"" are enhanced via news geo-localization, image enrichment and joint ranking, respectively. For news geo-localization, an Ordinal Correlation Consistent Matrix Factorization (OCCMF) model is proposed to maintain the relevance rankings of locations to a specific news document and simultaneously capture intra-relations among locations and documents. To visualize news, we develop a novel method to enrich news documents with appropriate web images. Specifically, multiple queries are first generated from news documents for image search, and then the appropriate images are selected from the collected web images by an intelligent fusion approach based on multiple features. Obtaining the geo-localized and image enriched news resources, we further employ a joint ranking strategy to provide relevant, timely and popular news items as the answer of user searching queries. Extensive experiments on a large-scale news dataset collected from the web demonstrate the superior performance of the proposed approaches over related methods. © 2013 ACM.",Geo-location; GeoVisNews; Image enrichment; Matrix factorization; News organization,Query processing; Consistent matrices; Geolocations; GeoVisNews; Intelligent fusion; Matrix factorizations; Multiple features; Ordinal correlation; Relevance ranking; Search engines
Secure randomized image watermarking based on singular value decomposition,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891791933&doi=10.1145%2f2542205.2542207&partnerID=40&md5=d2b14efdc1cc38f4d1a4b802620d5190,"In this article, a novel logo watermarking scheme is proposed based on wavelet frame transform, singular value decomposition and automatic thresholding. The proposed scheme essentially rectifies the ambiguity problem in the SVD-based watermarking. The core idea is to randomly upscale the size of host image using reversible random extension transform followed by the embedding of logo watermark in the wavelet frame domain. After embedding, a verification phase is casted with the help of a binary watermark and toral automorphism. At the extraction end, the binary watermark is first extracted followed by the verification of watermarked image. The logo watermark is extracted if and only if the watermarked image is verified. The security, attack and comparative analysis confirm high security, efficiency and robustness of the proposed watermarking system. © 2013 ACM.",Automatic thresholding; Chaotic map; Digital watermarking; Reversible random extension transform; Singular value decomposition; Wavelet frame transform,Chaotic systems; Digital watermarking; Image processing; Image watermarking; Automatic thresholding; Chaotic map; Comparative analysis; Extension transforms; Toral automorphism; Watermarked images; Watermarking systems; Wavelet frame; Singular value decomposition
Large-scale multilabel propagation based on efficient sparse graph construction,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891820392&doi=10.1145%2f2542205.2542209&partnerID=40&md5=693a0c48b7f5e856a2132fed604a8f41,"With the popularity of photo-sharing websites, the number of web images has exploded into unseen magnitude. Annotating such large-scale data will cost huge amount of human resources and is thus unaffordable. Motivated by this challenging problem, we propose a novel sparse graph based multilabel propagation (SGMP) scheme for super large scale datasets. Both the efficacy and accuracy of the image annotation are further investigated under different graph construction strategies, where Gaussian noise and non-Gaussian sparse noise are simultaneously considered in the formulations of these strategies. Our proposed approach outperforms the state-of-the-art algorithms by focusing on: (1) For large-scale graph construction, a simple yet efficient LSH (Locality Sensitive Hashing)-based sparse graph construction scheme is proposed to speed up the construction. We perform the multilabel propagation on this hashing-based graph construction, which is derived with LSH approach followed by sparse graph construction within the individual hashing buckets; (2) To further improve the accuracy, we propose a novel sparsity induced scalable graph construction scheme, which is based on a general sparse optimization framework. Sparsity essentially implies a very strong prior: for large scale optimization, the values of most variables shall be zeros when the solution reaches the optimum. By utilizing this prior, the solutions of large-scale sparse optimization problems can be derived by solving a series of much smaller scale subproblems; (3) For multilabel propagation, different from the traditional algorithms that propagate over individual label independently, our proposed propagation first encodes the label information of an image as a unit label confidence vector and naturally imposes inter-label constraints and manipulates labels interactively. Then, the entire propagation problem is formulated on the concept of Kullback-Leibler divergence defined on probabilistic distributions, which guides the propagation of the supervision information. Extensive experiments on the benchmark dataset NUS-WIDE with 270k images and its lite version NUS-WIDE-LITE with 56k images well demonstrate the effectiveness and scalability of the proposed multi-label propagation scheme. © 2013 ACM.",Collaborative multilabel propagation; Image annotation; Sparsity induced graph construction,Algorithms; Gaussian noise (electronic); Image analysis; Image retrieval; Optimization; Probability distributions; Graph construction; Image annotation; Kullback Leibler divergence; Large-scale optimization; Locality sensitive hashing; Multi-label propagation; Probabilistic distribution; State-of-the-art algorithms; Graph theory
Detecting profilable and overlapping communities with user-generated multimedia contents in LBSNs,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891769520&doi=10.1145%2f2502415&partnerID=40&md5=8d7a6e47f0b0ab67e148e7ce931795f9,"In location-based social networks (LBSNs), users implicitly interact with each other by visiting places, issuing comments and/or uploading photos. These heterogeneous interactions convey the latent information for identifying meaningful user groups, namely social communities, which exhibit unique location-oriented characteristics. In this work, we aim to detect and profile social communities in LBSNs by representing the heterogeneous interactions with a multimodality nonuniform hypergraph. Here, the vertices of the hypergraph are users, venues, textual comments or photos and the hyperedges characterize the k-partite heterogeneous interactions such as posting certain comments or uploading certain photos while visiting certain places. We then view each detected social community as a dense subgraph within the heterogeneous hypergraph, where the user community is constructed by the vertices and edges in the dense subgraph and the profile of the community is characterized by the vertices related with venues, comments and photos and their inter-relations. We present an efficient algorithm to detect the overlapped dense subgraphs, where the profile of each social community is guaranteed to be available by constraining the minimal number of vertices in each modality. Extensive experiments on Foursquare data well validated the effectiveness of the proposed framework in terms of detecting meaningful social communities and uncovering their underlying profiles in LBSNs. © 2013 ACM.",Community detection; Community Profiling; Heterogeneous hypergraph; Location-based social networks,Algorithms; Social networking (online); Community detection; Community Profiling; Heterogeneous interactions; Hypergraph; Location-based social networks; Multimedia contents; Overlapping communities; Social communities; Graph theory
MOWL: An ontology representation language for web-based multimedia applications,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891770211&doi=10.1145%2f2542205.2542210&partnerID=40&md5=c6c90cf88ffabd95652068228e85ea75,"Several multimedia applications need to reason with concepts and their media properties in specific domain contexts. Media properties of concepts exhibit some unique characteristics that cannot be dealt with conceptual modeling schemes followed in the existing ontology representation and reasoning schemes. We have proposed a new perceptual modeling technique for reasoning with media properties observed in multimedia instances and the latent concepts. Our knowledge representation scheme uses a causal model of the world where concepts manifest in media properties with uncertainties. We introduce a probabilistic reasoning scheme for belief propagation across domain concepts through observation of media properties. In order to support the perceptual modeling and reasoning paradigm, we propose a new ontology language, Multimedia Web Ontology Language (MOWL). Our primary contribution in this article is to establish the need for the new ontology language and to introduce the semantics of its novel language constructs. We establish the generality of our approach with two disperate knowledge-intensive applications involving reasoning with media properties of concepts. © 2013 ACM.",Bayesian networks; Concept recognition; Digital heritage; Multimedia ontology; Multimedia web ontology language; Recommendation engine,Bayesian networks; Knowledge representation; Recommender systems; Semantics; Concept recognition; Digital heritage; Multimedia applications; Multimedia ontology; Ontology representation language; Ontology representations; Probabilistic reasoning; Web ontology language; Ontology
Content-based copy detection through multimodal feature representation and temporal pyramid matching,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891752464&doi=10.1145%2f2542205.2542208&partnerID=40&md5=c11145547577aa7933b59f2dec318a6d,"Content-based copy detection (CBCD) is drawing increasing attention as an alternative technology to watermarking for video identification and copyright protection. In this article, we present a comprehensive method to detect copies that are subjected to complicated transformations. A multimodal feature representation scheme is designed to exploit the complementarity of audio features, global and local visual features so that optimal overall robustness to a wide range of complicated modifications can be achieved. Meanwhile, a temporal pyramid matching algorithm is proposed to assemble frame-level similarity search results into sequence-level matching results through similarity evaluation over multiple temporal granularities. Additionally, inverted indexing and locality sensitive hashing (LSH) are also adopted to speed up similarity search. Experimental results over benchmarking datasets of TRECVID 2010 and 2009 demonstrate that the proposed method outperforms other methods for most transformations in terms of copy detection accuracy. The evaluation results also suggest that our method can achieve competitive copy localization preciseness. © 2013 ACM.",Content-based copy detection; Feature representation; Temporal pyramid matching,Image retrieval; Alternative technologies; Content-based copy detections; Copyright protections; Feature representation; Locality sensitive hashing; Similarity evaluation; Temporal pyramid matching; Video identification; Copyrights
A survey of music similarity and recommendation from music context data,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891804431&doi=10.1145%2f2542205.2542206&partnerID=40&md5=55a977a57a7fb72e52736f1edab4a460,"In this survey article, we give an overview of methods for music similarity estimation and music recommendation based on music context data. Unlike approaches that rely on music content and have been researched for almost two decades, music-contextbased (or contextual) approaches to music retrieval are a quite recent field of research within music information retrieval (MIR). Contextual data refers to all music-relevant information that is not included in the audio signal itself. In this article, we focus on contextual aspects of music primarily accessible through web technology. We discuss different sources of context-based data for individual music pieces and for music artists. We summarize various approaches for constructing similarity measures based on the collaborative or cultural knowledge incorporated into these data sources. In particular, we identify and review three main types of context-based similarity approaches: text-retrieval-based approaches (relying on web-texts, tags, or lyrics), cooccurrence-based approaches (relying on playlists, page counts, microblogs, or peer-to-peer-networks), and approaches based on user ratings or listening habits. This article elaborates the characteristics of the presented context-based measures and discusses their strengths as well as their weaknesses. © 2013 ACM.",Music context; Music information retrieval; Music recommendation; Music similarity; Survey,Distributed computer systems; Information retrieval; Peer to peer networks; Surveying; Context-based similarity; Cultural knowledge; Music context; Music information retrieval; Music recommendation; Music similarity; Similarity measure; Web technologies; Surveys
Care and scale: Fifteen years of music retrieval,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886911910&doi=10.1145%2f2492703&partnerID=40&md5=8dc4aa94e24683213d855da655493cd5,"The co-founder of The Echo Nest, a music intelligence company that now powers recommendation and discovery for most music services, discusses the notion of care and scale, cultural analysis of music, a brief history of music retrieval, and how and why The Echo Nest got started.",Corporate research factors; Cultural perception; Information retrieval; Music retrieval,Computer networks; Hardware; Information retrieval; Corporate research; Cultural analysis; Music retrieval; History
How far we've come: Impact of 20 years of multimedia information retrieval,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886926683&doi=10.1145%2f2491844&partnerID=40&md5=d9c4f31ceb969ff7d4cd1c18757834c3,"This article reviews the major research trends that emerged in the last two decades within the broad area of multimedia information retrieval, with a focus on the ACM Multimedia community. Trends are defined (nonscientifically) to be topics that appeared in ACM multimedia publications and have had a significant number of citations. The article also assesses the impacts of these trends on real-world applications. The views expressed are subjective and likely biased but hopefully useful for understanding the heritage of the community and stimulating new research direction.",Content-based image retrieval; Multimedia information retrieval; Music retrieval; Video retrieval,Content based retrieval; Broad areas; Content based image retrieval; Multimedia community; Multimedia information retrieval; Music retrieval; Real-world; Research trends; Video retrieval; Information retrieval
A personal look back at twenty years of research in multimedia content analysis,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886936821&doi=10.1145%2f2502434&partnerID=40&md5=f336017d7a908382611b8493e64d6ceb,"This paper is a personal look back at twenty years of research in multimedia content analysis. It addresses the areas of audio, photo and video analysis for the purpose of indexing and retrieval from the perspective of a multimedia researcher. Whereas a general analysis of content is impossible due to the personal bias of the user, significant progress was made in the recognition of specific objects or events. The paper concludes with a brief outlook on the future.",Brain-computer interface; Face recognition; Multimedia content analysis; Music analysis; Query by example; Text recognition; User feedback; Video analysis,Character recognition; Face recognition; Multimedia content analysis; Music analysis; Query-by example; Text recognition; User feedback; Video analysis; Research
Robust and accurate mobile visual localization and its applications,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886903458&doi=10.1145%2f2491735&partnerID=40&md5=3ecb3f46f218464ad7f6a479e4753188,"Mobile applications are becoming increasingly popular. More and more people are using their phones to enjoy ubiquitous location-based services (LBS). The increasing popularity of LBS creates a fundamental problem: mobile localization. Besides traditional localization methods that use GPS or wireless signals, using phone-captured images for localization has drawn significant interest from researchers. Photos contain more scene context information than the embedded sensors, leading to a more precise location description. With the goal being to accurately sense real geographic scene contexts, this article presents a novel approach to mobile visual localization according to a given image (typically associated with a rough GPS position). The proposed approach is capable of providing a complete set of more accurate parameters about the scene geo-context including the real locations of both the mobile user and perhaps more importantly the captured scene, as well as the viewing direction. To figure out how to make image localization quick and accurate, we investigate various techniques for large-scale image retrieval and 2D-to- 3D matching. Specifically, we first generate scene clusters using joint geo-visual clustering, with each scene being represented by a reconstructed 3D model from a set of images. The 3D models are then indexed using a visual vocabulary tree structure. Taking geo-tags of the database image as prior knowledge, a novel location-based codebook weighting scheme proposed to embed this additional information into the codebook. The discriminative power of the codebook is enhanced, thus leading to better image retrieval performance. The query image is aligned with the models obtained from the image retrieval results, and eventually registered to a real-world map. We evaluate the effectiveness of our approach using several large-scale datasets and achieving estimation accuracy of a user's location within 13 meters, viewing direction within 12 degrees, and viewing distance within 26 meters. Of particular note is our showcase of three novel applications based on localization results: (1) an on-the-spot tour guide, (2) collaborative routing, and (3) a sight-seeing guide. The evaluations through user studies demonstrate that these applications are effective in facilitating the ideal rendezvous for mobile users.",Geo-tagging; Location-based services; Mobile visual localization; Scene reconstruction,Data processing; Global positioning system; Location based services; Maps; Query processing; Telephone sets; Trees (mathematics); Context information; Discriminative power; Geo-tagging; Large-scale datasets; Localization method; Retrieval performance; Scene reconstruction; Visual localization; Image retrieval
Multimedia Retrieval that Matters,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886925503&doi=10.1145%2f2490827&partnerID=40&md5=fd4acf4519eb1f3f4aa04a466fbe75da,"This article emphasizes the need to refocus multimedia information retrieval (MIR) research towards bridging the utility gap, the gap between the expected and defacto usefulness of MIR solutions. This requires us to revisit the notion of relevance, but also to consider other criteria for assessing MIR solutions, like the informativeness of the retrieved results and how helpful they are for the users. The article also states that this focus shift cannot be realized incrementally, but by revisiting the foundations of MIR solutions, that is, by a utility-by-design approach. In this respect, a number of research challenges are proposed.",Multimedia indexing; Multimedia information retrieval; Multimedia search; Utility by design,Computer networks; Hardware; Informative ness; Multimedia indexing; Multimedia information retrieval; Multimedia Retrieval; Multimedia search; Research challenges; Information retrieval
Propagation-based social-aware multimedia content distribution,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886907534&doi=10.1145%2f2523001.2523005&partnerID=40&md5=7be65397f427bd107732cb8ebf6312ab,"Online social networks have reshaped how multimedia contents are generated, distributed, and consumed on today's Internet. Given the massive number of user-generated contents shared in online social networks, users are moving to directly access these contents in their preferred social network services. It is intriguing to study the service provision of social contents for global users with satisfactory quality of experience. In this article, we conduct large-scale measurement of a real-world online social network system to study the social content propagation. We have observed important propagation patterns, including social locality, geographical locality, and temporal locality. Motivated by the measurement insights, we propose a propagationbased social-aware delivery framework using a hybrid edge-cloud and peer-assisted architecture. We also design replication strategies for the architecture based on three propagation predictors designed by jointly considering user, content, and context information. In particular, we design a propagation region predictor and a global audience predictor to guide how the edgecloud servers backup the contents, and a local audience predictor to guide how peers cache the contents for their friends. Our trace-driven experiments further demonstrate the effectiveness and superiority of our design.",Social network; Video service,Design; Online systems; Quality of service; Large-scale measurement; Multimedia content distribution; On-line social networks; Quality of experience (QoE); Replication strategies; Social network services; User-generated content; Video services; Social networking (online)
Two decades of internet video streaming: A retrospective view,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886929065&doi=10.1145%2f2505805&partnerID=40&md5=11f938f561d301d7ca789476317bb9e2,"For over two decades, video streaming over the Internet has received a substantial amount of attention from both academia and industry. Starting from the design of transport protocols for streaming video, research interests have later shifted to the peer-to-peer paradigm of designing streaming protocols at the application layer. More recent research has focused on building more practical and scalable systems, using Dynamic Adaptive Streaming over HTTP. In this article, we provide a retrospective view of the research results over the past two decades, with a focus on peer-to-peer streaming protocols and the effects of cloud computing and social media.","Cloud computing; HTTP streaming; Multicast; P2P streaming; Socialmedia, multimedia streaming; Video streaming",Cloud computing; HTTP; Multicasting; Research; Video streaming; Dynamic Adaptive Streaming over HTTP; Http streaming; Multimedia streaming; P2P streaming; Peer-to-peer paradigm; Peer-to-peer streaming; STreaming protocols; Transport protocols; Peer to peer networks
Navigating the worldwide community of photos,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886942640&doi=10.1145%2f2492208&partnerID=40&md5=bc9e9583d866f4c6580a7af0146d9c8d,"The last decade has seen an explosion in the number of photographs available on the Internet. The sheer volume of interesting photos makes it a challenge to explore this space. VariousWeb and social media sites, along with search and indexing techniques, have been developed in response. One natural way to navigate these images in a 3D geo-located context. In this article, we reflect on our work in this area, with a focus on techniques that build partial 3D scene models to help find and navigate interesting photographs in an interactive, immersive 3D setting. We also discuss how finding such relationships among photographs opens up exciting new possibilities for multimedia authoring, visualization, and editing.",Image-based modeling; Image-based rendering; Visualization,Flow visualization; Image reconstruction; Navigation; Photography; Visualization; 3D scenes; Image-based modeling; Image-Based Rendering; Immersive; Indexing techniques; Multi-Media authoring; Social media; Three dimensional
Multimedia systems research: The first twenty years and lessons for the next twenty,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886908641&doi=10.1145%2f2490859&partnerID=40&md5=d6a102a268d104b18f1780f5c3b3dcc7,This retrospective article examines the past two decades of multimedia systems research through the lens of three research topics that were in vogue in the early days of the field and offers perspectives on the evolution of these research topics. We discuss the eventual impact of each line of research and offer lessons for future research in the field.,Multimedia systems,Computer supported cooperative work; Multimedia systems; Research topics; Through the lens; Research
Editorial note,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886932930&doi=10.1145%2f2523001.2523002&partnerID=40&md5=e9bba0b719f275c2aa47a1cb02519973,[No abstract available],,
Evolution of temporal multimedia synchronization principles: A historical viewpoint,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886934721&doi=10.1145%2f2490821&partnerID=40&md5=3fb93e1f75fb15d94021907899d5c54c,"The evolution of multimedia applications has drastically changed human life and behaviors. New communication technologies lead to new requirements for multimedia synchronization. This article presents a historical view of temporal synchronization studies focusing on continuous multimedia. We demonstrate how the development of multimedia systems has created new challenges for synchronization technologies. We conclude with a new application-dependent, multilocation, multirequirement synchronization framework to address these new challenges.",Multimedia synchronization; Survey,Behavioral research; Multimedia systems; Surveying; Communication technologies; Historical view; Human lives; Multilocation; Multimedia applications; Multimedia synchronization; Temporal multimedia; Temporal synchronization; Synchronization
Image search-from thousands to billions in 20 years,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886906128&doi=10.1145%2f2490823&partnerID=40&md5=71040f7661bbb8fbc3ebcac2e928fbd0,"This article presents a comprehensive review and analysis on image search in the past 20 years, emphasizing the challenges and opportunities brought by the astonishing increase of dataset scales from thousands to billions in the same time period, which was witnessed first-hand by the authors as active participants in this research area. Starting with a retrospective review of three stages of image search in the history, the article highlights major breakthroughs around the year 2000 in image search features, indexing methods, and commercial systems, which marked the transition from stage two to stage three. Subsequent sections describe the image search research from four important aspects: system framework, feature extraction and image representation, indexing, and big data's potential. Based on the review, the concluding section discusses open research challenges and suggests future research directions in effective visual representation, image knowledge base construction, implicit user feedback and crowdsourcing, mobile image search, and creative multimedia interfaces.",Big data; Content-based; Global feature; Image feature; Image knowledge base; Image retrieval; Indexing; Local feature; Review; Visual representation; Web image search,Data processing; Feature extraction; Image retrieval; Indexing (of information); Knowledge based systems; Research; Reviews; Big datum; Content-based; Global feature; Image features; Knowledge base; Local feature; Visual representations; Web image search; Search engines
Exploiting unconscious user signals in multimodal human-computer interaction,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886920722&doi=10.1145%2f2502433&partnerID=40&md5=ce8056882bab6e0ef4c84d23b5a7ffaa,"This article presents the idea of empathic stimulation that relies on the power and potential of unconsciously conveyed attentive and emotional information to facilitate human-machine interaction. Starting from a historical review of related work presented at past ACM Multimedia conferences, we discuss challenges that arise when exploiting unconscious human signals for empathic stimulation, such as the real-time analysis of psychological user states and the smooth adaptation of the human-machine interface based on this analysis. A classical application field that might benefit from the idea of unconscious human-computer interaction is the exploration of massive datasets.",Emotion recognition; Multimodal interfaces; Social signal processing,Man machine systems; Signal processing; Emotion recognition; Emotional information; Human machine interaction; Human Machine Interface; Multi-modal interfaces; Multimedia conferences; Multimodal human computer interaction; Social signal processing; Human computer interaction
Are we in the middle of a video streaming revolution?,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886939284&doi=10.1145%2f2490826&partnerID=40&md5=4e97473c8d90156d9e9c3e13f6589af1,"It has been roughly 20 years since the beginning of video streaming over the Internet. Until very recently, video streaming experiences left much to be desired. Over the last few years, this has significantly improved making monetization of streaming, possible. Recently, there has been an explosion of commercial video delivery services over the Internet, sometimes referred to as over-the-top (OTT) delivery. All these services invariably use streaming technologies. Initially, streaming had all the promise, then for a long time, it was download and play, later progressive download for short content, and now it is streaming again. Did streaming win the download versus streaming contest? Did the best technology win? The improvement in streaming experience has been possible through a variety of new streaming technologies, some proprietary and others extensions to standard protocols. The primary delivery mechanism for entertainment video, both premium content like movies and user generated content (UGC), tends to be HTTP streaming. Is HTTP streaming the panacea for all problems? The goal of this article is to give an industry perspective of what fundamentally changed in video streaming that makes it commercially viable now. This article outlines how a blend of technology choices between download and streaming makes the current wave of ubiquitous streaming possible for entertainment video delivery. After identifying problems that still need to be solved, the article concludes with the lessons learnt from the video streaming evolution.",HTTP streaming; Multimedia systems; Video delivery; Video streaming,Internet; Multimedia systems; Video streaming; Commercial video; Delivery mechanism; Http streaming; Standard protocols; Streaming technology; Technology choices; User generated content (UGC); Video delivery; HTTP
Over twenty years of eigenfaces,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886872060&doi=10.1145%2f2490824&partnerID=40&md5=8011fbb4f5331507ec938e984ef19877,"The inaugural ACM Multimedia Conference coincided with a surge of interest in computer vision technologies for detecting and recognizing people and their activities in images and video. Face recognition was the first of these topics to broadly engage the vision and multimedia research communities. The Eigenfaces approach was, deservedly or not, the method that captured much of the initial attention, and it continues to be taught and used as a benchmark over 20 years later. This article is a brief personal view of the genesis of Eigenfaces for face recognition and its relevance to the multimedia community.",Biometrics; Face recognition; Vision-based interaction,Biometrics; Computer vision technology; Eigenfaces; Multimedia community; Multimedia conferences; Multimedia research; Vision-based interaction; Face recognition
Experiential media systems,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886896301&doi=10.1145%2f2502432&partnerID=40&md5=b5e67189897d2e92bf8f4dba8c05abbc,"This article presents a personalized narrative on the early discussions within the Multimedia community and the subsequent research on experiential media systems. I discuss two different research initiatives-design of real-time, immersive multimedia feedback environments for stroke rehabilitation; exploratory environments for events that exploited the user's ability to make connections. I discuss the issue of foundations: the question of multisensory integration and superadditivity; the need for identification of first-class Multimedia problems; expanding the scope of Multimedia research.",Experiential systems; Foundations; Multimedia,Computer networks; Foundations; Hardware; Feedback environments; Media systems; Multimedia; Multimedia community; Multimedia research; Multisensory integration; Stroke rehabilitation; Superadditivity; Research
"Online video delivery: Past, present, and future",2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886877801&doi=10.1145%2f2502435&partnerID=40&md5=357fa37939c87b6cdf8451bf9520399c,"Video streaming is the core technology for online video delivery systems. Initial research on this technology faced many challenges. In this article, lessons learned from beginning trials are discussed; some pioneering works that provided early solutions and inspired subsequent research are presented; and new techniques required for emerging applications are examined.",Caching; Video on demand; Video streaming,Video on demand; Video streaming; Caching; Core technology; Emerging applications; Online video; Online systems
"Advances in immersive communication: (1) telephone, (2) television, (3) teleportation",2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886876674&doi=10.1145%2f2492704&partnerID=40&md5=d1f8e4438cf78a9ce54ca7d3c1f8710b,"The last great advances in immersive communication were the invention of the telephone over 137 years ago and the invention of the video telephone (ne television) over 86 years ago. However, a perfect storm is brewing for the next advance in immersive communication, thanks to the convergence of massive amounts of computation, bandwidth, resolution, new sensors, and new displays. It could well be the Multimedia community that turns this brew into the next great advance in immersive communication, something akin to teleportation.",Depth cameras; Immersion; Telepresence; Virtual reality,Communication; Patents and inventions; Telephone; Telephone sets; Virtual reality; Visual communication; Depth camera; Immersion; Immersive; Multimedia community; Perfect storm; Telepresence; Video telephone; Telecommunication systems
Introduction to the special section on the 20th anniversary of the acm international conference on multimedia,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886874189&doi=10.1145%2f2523001.2523003&partnerID=40&md5=494144e275632537ca87ca6100e4b5a8,[No abstract available],,
Social influence analysis and application on multimedia sharing websites,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886870261&doi=10.1145%2f2502436&partnerID=40&md5=6c24ffb6fa552df8caa9501d93e45dca,"Social media is becoming popular these days, where users necessarily interact with each other to form social networks. Influence network, as one special case of social network, has been recognized as significantly impacting social activities and user decisions. We emphasize in this article that the inter-user influence is essentially topic-sensitive, as for different tasks users tend to trust different influencers and be influenced most by them. While existing research focuses on global influence modeling and applies to text-based networks, this work investigates the problem of topic-sensitive influence modeling in the multimedia domain. According to temporal data justification, we propose a multimodal probabilistic model, considering both users' textual annotation and uploaded visual images. This model is capable of simultaneously extracting user topic distributions and topic-sensitive influence strengths. By identifying the topic-sensitive influencer, we are able to conduct applications, like collective search and collaborative recommendation. A risk minimization-based general framework for personalized image search is further presented, where the image search task is transferred to measure the distance of image and personalized query language models. The framework considers the noisy tag issue and enables easy incorporation of social influence. We have conducted experiments on a large-scale Flickr dataset. Qualitative as well as quantitative evaluation results have validated the effectiveness of the topic-sensitive influencer mining model, and demonstrated the advantage of incorporating topic-sensitive influence in personalized image search and topic-based image recommendation.",Influence analysis; Social media; Social relation analysis; Topic model,Computational linguistics; Query languages; Risk assessment; Social networking (online); Collaborative recommendation; Influence analysis; Probabilistic modeling; Quantitative evaluation; Query language model; Social media; Social relations; Topic Modeling; Economic and social effects
"Socially-aware multimedia authoring: Past, present, and future",2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886885878&doi=10.1145%2f2491893&partnerID=40&md5=a4599c35eff7fdae61fe5ede2a51091e,"Creating compelling multimedia productions is a nontrivial task. This is as true for creating professional content as it is for nonprofessional editors. During the past 20 years, authoring networked content has been a part of the research agenda of the multimedia community. Unfortunately, authoring has been seen as an initial enterprise that occurs before 'real' content processing takes place. This limits the options open to authors and to viewers of rich multimedia content for creating and receiving focused, highly personal media presentations. This article reflects on the history of multimedia authoring. We focus on the particular task of supporting socially-aware multimedia, in which the relationships within particular social groups among authors and viewers can be exploited to create highly personal media experiences. We provide an overview of the requirements and characteristics of socially-aware multimedia authoring within the context of exploiting community content. We continue with a short historical perspective on authoring support for these types of situations. We then present an overview of a current system for supporting socially-aware multimedia authoring within the community content. We conclude with a discussion of the issues that we feel can provide a fruitful basis for future multimedia authoring support. We argue that providing support for socially-aware multimedia authoring can have a profound impact on the nature and architecture of the entire multimedia information processing pipeline.",Authoring systems; Personalization; Socially-aware multimedia,Computer networks; Hardware; Authoring systems; Historical perspective; Multi-Media authoring; Multimedia community; Multimedia information processing; Multimedia productions; Personalizations; Socially-aware multimedia; Data processing
Looking forward 10 years to multimedia successes,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886889096&doi=10.1145%2f2490825&partnerID=40&md5=38aafae02ddb0582b8754e7b815d66de,"A panel at ACM Multimedia 2012 addressed research successes in the past 20 years. While the panel focused on the past, this article discusses successes since the ACM SIGMM 2003 Retreat and suggests research directions in the next ten years. While significant progress has been made, more research is required to allow multimedia to impact our everyday computing environment. The importance of hardware changes on future research directions is discussed. We believe ubiquitous computing- meaning abundant computation and network bandwidth-should be applied in novel ways to solve multimedia grand challenges and continue the IT revolution of the past century.",Multimedia research directions,Ubiquitous computing; Computing environments; Future research directions; Grand Challenge; IT revolution; Multimedia research; Research
Introduction to the special section of best papers of acm multimedia 2012,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886884918&doi=10.1145%2f2523001.2523004&partnerID=40&md5=d0d08174cb1f5970865035bdfd347d74,[No abstract available],,
Editorial: Reviewers,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024248152&doi=10.1145%2f2501643.2501644&partnerID=40&md5=d20f1a17bfd3e89ee894749bb938be9c,[No abstract available],,
A self-similarity approach to repairing large dropouts of streamed music,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880341809&doi=10.1145%2f2487268.2487273&partnerID=40&md5=8855da041bb1865e1d1e94032166e08d,"Enjoyment of audio has now become about flexibility and personal freedom. Digital audio content can be acquired from many sources and wireless networking allows digital media devices and associated peripherals to be unencumbered by wires. However, despite recent improvements in capacity and quality of service, wireless networks are inherently unreliable communications channels for the streaming of audio, being susceptible to the effects of range, interference, and occlusion. This time-varying reliability of wireless audio transfer introduces data corruption and loss, with unpleasant audible effects that can be profound and prolonged in duration. Traditional communications techniques for error mitigation perform poorly and in a bandwidth inefficient manner in the presence of such large-scale defects in a digital audio stream. A novel solution that can complement existing techniques takes account of the semantics and natural repetition of music. Through the use of self-similarity metadata, missing or damaged audio segments can be seamlessly replaced with similar undamaged segments that have already been successfully received. We propose a technology to generate relevant self-similarity metadata for arbitrary audio material and to utilize this metadata within a wireless audio receiver to provide sophisticated and real-time correction of large-scale errors. The primary objectives are to match the current section of a song being received with previous sections while identifying incomplete sections and determining replacements based on previously received portions of the song. This article outlines our approach to Forward Error Correction (FEC) technology that is used to ""repair"" a bursty dropout when listening to time-dependent media on a wireless network. Using self-similarity analysis on a music file, we can ""automatically"" repair the dropout with a similar portion of the music already received thereby minimizing a listener's discomfort. © 2013 ACM.",Audio repair; Data compaction and compression; Forward error correction; Streaming audio; Streaming media,Compaction; Data compression; Digital devices; Digital storage; Forward error correction; Metadata; Quality of service; Repair; Semantics; Telecommunication systems; Wireless networks; Communications channels; Data compaction; Primary objective; Real-time corrections; Self-similarities; Streaming media; Time-varying reliabilities; Wireless networking; Audio streaming
A conditional access system with efficient key distribution and revocation for mobile pay-TV systems,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880341481&doi=10.1145%2f2487268.2487271&partnerID=40&md5=16e0ccdb84d5247f87349005d1e97418,"Current mobile pay-TV systems have two types of Conditional Access Systems (CAS): group-key-based and public-key systems. The best feature of group-key-based systems is the ability to enjoy the broadcast nature in delivery multimedia contents, while the major advantage of public-key systems is consolidating the security foundation to withstand various attacks, such as collusion attacks. However, the problems of group-key-based systems include collusion attacks, lack of nonrepudiation, and troublesome key distribution. Even worse, the benefit of broadcast efficiency is confined to a group size of no more than 512 subscribers. For public-key systems, the poor delivery scalability is the major shortcoming because the unique private key feature is only suitable for one-to-one delivery. In this article, we introduce a scalable access control scheme to integrate the merits of broadcasting regardless of group size and sound security assurance, including fine-grained access control and collusion attack resistance. For subscriber revocation, a single message is broadcast to the other subscribers to get the updated key, thus significantly boosting subscriber revocation scalability. Due to mobile subscribers' dynamic movements, this article also analyzes the benefit of retransmission cases in our system. Through the performance evaluation and functionality comparison, the proposed scheme should be a decent candidate to enhance the security strength and transmission efficiency in a mobile pay-TV system. © 2013 ACM.",Digital video broadcasting; Multimedia delivery; Nonrepudiation; Sophisticated access control,Digital video broadcasting (DVB); Network security; Scalability; Access control schemes; Broadcast efficiency; Conditional access systems; Functionality comparisons; Multimedia contents; Multimedia delivery; Non-repudiation; Transmission efficiency; Access control
Information recall task impact in olfaction-enhanced multimedia,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880358551&doi=10.1145%2f2487268.2487270&partnerID=40&md5=f3e4aebf13aadcca84595ae29ad155b3,"Enhancing multimedia applications with olfactory sensations is one of the last challenges in the area. While there is evidence, both scientific and anecdotal, that olfactory cues help users in information recall tasks, there is a lack of work when the targeted information is one contained in a multimedia presentation, which is precisely the focus of this article. Accordingly, we present the results of two experimental studies. The first study measured the impact of olfactory media variation on the user's ability to perceive, synthesize, and analyze the informational content of olfactory-enhanced multimedia videos; the second study measured the impact of information content, and an information recall task in respect of user perception of the relevance, sense of reality, and acceptability of the olfactory media content, as well as the overall enjoyment of the experience. Results show that the use of olfactory media content, both pleasant and unpleasant, in multimedia displays does not significantly impact on information assimilation in a negative way. Moreover, the addition of a performance task may enhance the user's understanding of the correlation between the characteristic odor(s) and the scenario under consideration, as well as enable users to consciously learn the odors. © 2013 ACM.",Information recall; Multimedia; Olfaction,Computer networks; Hardware; Experimental studies; Information contents; Information recalls; Multimedia; Multimedia applications; Multimedia presentation; Olfaction; Olfactory sensation; Multimedia services
Call for Papers,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024270701&doi=10.1145%2f2487268.2500818&partnerID=40&md5=2c6ce52bdc74e9ab9df856a6c190198e,[No abstract available],,
Near-lossless semantic video summarization and its applications to video analysis,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880369508&doi=10.1145%2f2487268.2487269&partnerID=40&md5=15866bd621b7ab3f171c004a34c56647,"The ever increasing volume of video content on the Web has created profound challenges for developing efficient indexing and search techniques to manage video data. Conventional techniques such as video compression and summarization strive for the two commonly conflicting goals of low storage and high visual and semantic fidelity. With the goal of balancing both video compression and summarization, this article presents a novel approach, called Near-Lossless Semantic Summarization (NLSS), to summarize a video stream with the least high-level semantic information loss by using an extremely small piece of metadata. The summary consists of compressed image and audio streams, as well as the metadata for temporal structure and motion information. Although at a very low compression rate (around 1/40 of H.264 baseline, where traditional compression techniques can hardly preserve an acceptable visual fidelity), the proposed NLSS still can be applied to many video-oriented tasks, such as visualization, indexing and browsing, duplicate detection, concept detection, and so on. We evaluate the NLSS on TRECVID and other video collections, and demonstrate that it is a powerful tool for significantly reducing storage consumption, while keeping high-level semantic fidelity. © 2013 ACM.",Video applications; Video content analysis; Video storage; Video summarization,Image compression; Indexing (of information); Metadata; Semantics; Video streaming; Compression techniques; Conventional techniques; Duplicate detection; High level semantics; Video applications; Video storage; Video summarization; Video-content analysis; Video recording
Interactive partner control in close interactions for real-time applications,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880337401&doi=10.1145%2f2487268.2487274&partnerID=40&md5=681f29bd4fb54b93edf60427f28af6d3,"This article presents a new framework for synthesizing motion of a virtual character in response to the actions performed by a user-controlled character in real time. In particular, the proposed method can handle scenes in which the characters are closely interacting with each other such as those in partner dancing and fighting. In such interactions, coordinating the virtual characters with the human player automatically is extremely difficult because the system has to predict the intention of the player character. In addition, the style variations from different users affect the accuracy in recognizing the movements of the player character when determining the responses of the virtual character. To solve these problems, our framework makes use of the spatial relationship-based representation of the body parts called interaction mesh, which has been proven effective for motion adaptation. The method is computationally efficient, enabling real-time character control for interactive applications. We demonstrate its effectiveness and versatility in synthesizing a wide variety of motions with close interactions. © 2013 ACM.",Character animation; Close interactions; Motion capture; Virtual partner,Hardware; Character animation; Computationally efficient; Interactive applications; Motion adaptation; Motion capture; Real-time application; Virtual character; Virtual partner; Computer networks
A generalized tamper localization approach for reversible watermarking algorithms,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880379093&doi=10.1145%2f2487268.2487272&partnerID=40&md5=ebae91d24b6121d99dd67271a883fbf8,"In general reversible watermarking algorithms, the convention is to reject the entire cover image at the receiver end if it fails authentication, since there is no way to detect the exact locations of tampering. This feature may be exploited by an adversary to bring about a form of DoS attack. Here we provide a solution to this problem in form of a tamper localization mechanism for reversible watermarking algorithms, which allows selective rejection of distorted cover image regions in case of authentication failure, thus avoiding rejection of the complete image. Additionally it minimizes the bandwidth requirement of the communication channel. © 2013 ACM.",Authentication; Digital image forensics; Reversible watermarking; Tamper localization,Authentication; Bandwidth requirement; Cover-image; Digital image forensics; DoS attacks; Reversible watermarking; Reversible watermarking algorithm; Tamper localization; Digital watermarking
Identity verification based on handwritten signatures with haptic information using genetic programming,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878501094&doi=10.1145%2f2457450.2457453&partnerID=40&md5=d0b9089924b9275ef9e834d56bc765ab,"In this article, haptic-based handwritten signature verification using Genetic Programming (GP) classification is presented. A comparison of GP-based classification with classical classifiers including support vector machine, k-nearest neighbors, naive Bayes, and random forest is conducted. In addition, the use of GP in discovering small knowledge-preserving subsets of features in high-dimensional datasets of haptic-based signatures is investigated and several approaches are explored. Subsets of features extracted from GP-generated models (analytic functions) are also exploited to determine the importance and relevance of different haptic data types (e.g., force, position, torque, and orientation) in user identity verification. The results revealed that GP classifiers compare favorably with the classical methods and use a much fewer number of attributes (with simple function sets). © 2013 ACM.",Biometrics; Classification; Genetic Programming; Haptics; User verification,Biometrics; Classification (of information); Decision trees; Analytic functions; Classical methods; Handwritten signature verification; Handwritten signatures; Haptics; Identity verification; K-nearest neighbors; User verification; Genetic programming
APRICOD: An access-pattern-driven distributed caching middleware for fast content discovery of noncontinuous media access,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878477618&doi=10.1145%2f2457450.2457457&partnerID=40&md5=7dd21a9cca08fcc442f430a7a8663c5f,"Content discovery is a major source of latency in peer-to-peer (P2P) media streaming systems, especially in the presence of noncontinuous user access, such as random seek in Video-on-Demand (VoD) streaming and teleportation in a Networked Virtual Environment (NVE). After the aforementioned user interactions, streaming systems often need to initiate the content discovery process to identify where to retrieve the requested media objects. Short content lookup latency is demanded to ensure smooth user experience. Existing content discovery systems based on either a Distributed Hash Table (DHT) or gossip mechanism cannot cope with noncontinuous access efficiently due to their long lookup latency. In this work, we propose an access-pattern-driven distributed caching middleware named APRICOD, which caters for fast and scalable content discovery in peer-to-peer media streaming systems, especially when user interactions are present. APRICOD exploits correlations among media objects accessed by users, and adapts to shift in the user access pattern automatically. We first present a general APRICOD design that can be used with any existing content discovery system. We then present an implementation of APRICOD on top of Pastry, which we use to evaluate APRICOD. Our evaluation in a 1024-node system, using a Second Life trace with 5, 735 users and a VoD trace with 54 users, shows that APRICOD can effectively resolve all continuous access queries with a single hop deterministically with node failure as an exception, and resolve noncontinuous access queries with a single hop with high probability. © 2013 ACM.",Access pattern; Caching; Content discovery; Noncontinuous media access; Peer-to-peer,Peer to peer networks; Video on demand; Virtual reality; Access patterns; Caching; Content discoveries; Media access; Peer to peer; Media streaming
Multifeature analysis and semantic context learning for image classification,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878480055&doi=10.1145%2f2457450.2457454&partnerID=40&md5=2ad1a2ffacb7babf0fdb2455e6b3a7a1,"This article introduces an image classification approach in which the semantic context of images and multiple low-level visual features are jointly exploited. The context consists of a set of semantic terms defining the classes to be associated to unclassified images. Initially, a multiobjective optimization technique is used to define a multifeature fusion model for each semantic class. Then, a Bayesian learning procedure is applied to derive a context model representing relationships among semantic classes. Finally, this context model is used to infer object classes within images. Selected results from a comprehensive experimental evaluation are reported to show the effectiveness of the proposed approaches. © 2013 ACM.",Image classification; Multifeature fusion; Object detection; Semantic context modeling,Multiobjective optimization; Object recognition; Semantic Web; Semantics; Bayesian learning; Classification approach; Experimental evaluation; Multi-feature fusion; Multi-objective optimization techniques; Object Detection; Semantic context; Visual feature; Image classification
Modeling the effect of user interactions on mesh-based P2P vod streaming systems,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878494606&doi=10.1145%2f2457450.2457455&partnerID=40&md5=7b94e754c4783cd9133c0c74710fef08,"User interactions such as seeks and pauses are widely supported by existing Peer-to-Peer Video-on-Demand (P2P VoD) streaming systems. Their effect on the streaming system, however, has not been well studied. Seeks cause peers to skip part of the video, making them stay in the system for shorter time, and thus contribute less. On the other hand, only part of the video is downloaded due to seeks, reducing peers' demand from the system. It is unclear which factor dominates the effect of seeks on the streaming system. Pauses during playback, on one hand, allow peers to stay longer in the system and upload more content. When interleaved with seeks, however, long pauses may increase peers' demand unnecessarily as peers may download content that will eventually be skipped by subsequent forward seeks. The collective effect of seeks and pauses, together with the known random peer departure, is unintuitive and needs to be addressed properly so as to understand the effect of human factors on the streaming system performance. In this article, we develop an analytical model to both qualitatively and quantitatively study the effect of seeks and pauses on mesh-based P2P VoD streaming systems, in particular, the effect on the server cost. Our model can help in understanding how human factors such as seeks and pauses affect the streaming system performance, tuning a P2P VoD system towards better system performance and stability, and providing a framework for capacity planning. © 2013 ACM.",Analytical modeling; Pause; Peer-to-peer; Random departure; Seek; Streaming; User interaction; Video-on-demand,Acoustic streaming; Analytical models; Human engineering; Video on demand; Pause; Peer to peer; Random departure; Seek; User interaction; Peer to peer networks
A reward-and-punishment-based approach for concept detection using adaptive ontology rules,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878517814&doi=10.1145%2f2457450.2457452&partnerID=40&md5=b2c03e81af680a3a1e087b256ddc3e0c,"Despite the fact that performance improvements have been reported in the last years, semantic concept detection in video remains a challenging problem. Existing concept detection techniques, with ontology rules, exploit the static correlations among primitive concepts but not the dynamic spatiotemporal correlations. The proposed method rewards (or punishes) detected primitive concepts using dynamic spatiotemporal correlations of the given ontology rules and updates these ontology rules based on the accuracy of detection. Adaptively learned ontology rules significantly help in improving the overall accuracy of concept detection as shown in the experimental result. © 2013 ACM.",Adaptive ontology rules; Concept detection; Dynamic correlation; Model; Multimedia data mining,Hardware; Models; Adaptive ontology rules; Concept detection; Dynamic correlation; Multimedia data mining; Overall accuracies; Performance improvements; Semantic concept detection; Spatiotemporal correlation; Computer networks
Human perception of haptic-to-video and haptic-to-audio skew in multimedia applications,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878476292&doi=10.1145%2f2457450.2457451&partnerID=40&md5=288448583151c99376c49eb9e02e48d0,"The purpose of this research is to assess the sensitivity of humans to perceive asynchrony among media signals coming from a computer application. Particularly we examine haptic-to-video and haptic-to-audio skew. For this purpose we have designed an experimental setup, where users are exposed to a basic multimedia presentation resembling a ping-pong game. For every collision between a ball and a racket, the user is able to perceive auditory, visual, and haptic cues about the collision event. We artificially introduce negative and positive delay to the auditory and visual cues with respect to the haptic stream. We subjectively evaluate the perception of inter-stream asynchrony perceived by the users using two types of haptic devices. The statistical results of our evaluation show perception rates of around 100 ms regardless of modality and type of device. © 2013 ACM.",Haptics; Synchronization,Computer applications; Synchronization; Collision events; Haptic devices; Haptics; Human perception; Media signals; Multimedia applications; Multimedia presentation; Visual cues; Multimedia services
Effective transfer tagging from image to video,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878475233&doi=10.1145%2f2457450.2457456&partnerID=40&md5=e1b6823a2e5b80ebbcee23087c1d6ca2,"Recent years have witnessed a great explosion of user-generated videos on the Web. In order to achieve an effective and efficient video search, it is critical for modern video search engines to associate videos with semantic keywords automatically. Most of the existing video tagging methods can hardly achieve reliable performance due to deficiency of training data. It is noticed that abundant well-tagged data are available in other relevant types of media (e.g., images). In this article, we propose a novel video tagging framework, termed as Cross-Media Tag Transfer (CMTT), which utilizes the abundance of well-tagged images to facilitate video tagging. Specifically, we build a cross-media tunnel to transfer knowledge from images to videos. To this end, an optimal kernel space, in which distribution distance between images and video is minimized, is found to tackle the domainshift problem. A novel cross-media video tagging model is proposed to infer tags by exploring the intrinsic local structures of both labeled and unlabeled data, and learn reliable video classifiers. An efficient algorithm is designed to optimize the proposed model in an iterative and alternative way. Extensive experiments illustrate the superiority of our proposal compared to the state-of-the-art algorithms. © 2013 ACM.",Cross media; Semi-supervised learning; Transfer learning; Video tagging,Algorithms; Optimization; Semantics; Supervised learning; Cross-media; Labeled and unlabeled data; Reliable performance; Semi-supervised learning; State-of-the-art algorithms; Transfer learning; User-generated video; Video tagging; Iterative methods
Web-accessible geographic integration and calibration of webcams,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874912530&doi=10.1145%2f2422956.2422964&partnerID=40&md5=fdeb7e8d2ead8baecc5cb5df4a68adea,"A global network of webcams offers unique viewpoints from tens of thousands of locations. Understanding the geographic context of this imagery is vital in using these cameras for quantitative environmental monitoring or surveillance applications. We derive robust geo-calibration constraints that allow users to geo-register static or pan-tilt-zoom cameras by specifying a few corresponding points, and describe our Web interface suitable for novices. We discuss design decisions that support our scalable, publicly accessible Web service that allows webcam textures to be displayed live on 3D geographic models. Finally, we demonstrate several multimedia applications for geo-calibrated cameras. © 2013 ACM.",Algorithms,Algorithms; Calibration; Cameras; Web services; Design decisions; Environmental Monitoring; Geographic contexts; Geographic model; Multimedia applications; Pan-tilt-zoom camera; Publicly accessible; Surveillance applications; Security systems
GPSView: A scenic driving route planner,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874917844&doi=10.1145%2f2422956.2422959&partnerID=40&md5=8bf28dbfec720a4249eb7dc1e9c6f1d5,"GPS devices have been widely used in automobiles to compute navigation routes to destinations. The generated driving route targets the minimal traveling distance, but neglects the sightseeing experience of the route. In this study, we propose an augmented GPS navigation system, GPSView, to incorporate a scenic factor into the routing. The goal of GPSView is to plan a driving route with scenery and sightseeing qualities, and therefore allow travelers to enjoy sightseeing on the drive. To do so, we first build a database of scenic roadways with vistas of landscapes and sights along the roadside. Specifically, we adapt an attention-based approach to exploit community-contributed GPS-tagged photos on the Internet to discover scenic roadways. The premise is: a multitude of photos taken along a roadway imply that this roadway is probably appealing and catches the public's attention. By analyzing the geospatial distribution of photos, the proposed approach discovers the roadside sight spots, or Points-Of-Interest (POIs), which have good scenic qualities and visibility to travelers on the roadway. Finally, we formulate scenic driving route planning as an optimization task towards the best trade-off between sightseeing experience and traveling distance. Testing in the northern California area shows that the proposed system can deliver promising results. © 2013 ACM.",Algorithm; Experimentation,Algorithms; Navigation systems; California; Experimentation; Geo-spatial; Gps-tagged photos; Navigation routes; Optimization task; Route planner; Route planning; Roadsides
Image retrieval with query-adaptive hashing,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874818499&doi=10.1145%2f2422956.2422958&partnerID=40&md5=a4062649a2f54f880b9694b3142c18d3,"Hashing-based approximate nearest-neighbor search may well realize scalable content-based image retrieval. The existing semantic-preserving hashing methods leverage the labeled data to learn a fixed set of semantic-aware hash functions. However, a fixed hash function set is unable to well encode all semantic information simultaneously, and ignores the specific user's search intention conveyed by the query. In this article, we propose a query-adaptive hashing method which is able to generate the most appropriate binary codes for different queries. Specifically, a set of semantic-biased discriminant projection matrices are first learnt for each of the semantic concepts, through which a semantic-adaptable hash function set is learnt via a joint sparsity variable selection model. At query time, we further use the sparsity representation procedure to select the most appropriate hash function subset that is informative to the semantic information conveyed by the query. Extensive experiments over three benchmark image datasets well demonstrate the superiority of our proposed query-adaptive hashing method over the state-of-the-art ones in terms of retrieval accuracy. © 2013 ACM.",Algorithms,Algorithms; Content based retrieval; Hash functions; Approximate nearest-neighbor searches; Content based image retrieval; Projection matrix; Retrieval accuracy; Search intentions; Semantic concept; Semantic information; Variable selection; Semantics
Region- and action-aware virtual world clients,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874919761&doi=10.1145%2f2422956.2422962&partnerID=40&md5=6b9f2f9aef2b24c754846fe064a2b7b2,"We propose region- and action-aware virtual world clients. To develop such clients, we present a parameterized network traffic model, based on a large collection of Second Life traces gathered by us. Our methodology is also applicable to virtual worlds other than Second Life. With the traffic model, various optimization criteria can be adopted, including visual quality, response time, and energy consumption.We use energy consumption as the show case, and demonstrate via trace-driven simulations that, compared to two existing schemes, a mobile client can save up to 36% and 41% communication energy by selectively turning on its WiFi network interface. © 2013 ACM.",Performance,Energy utilization; Optimization; Virtual reality; Communication energy; Network traffic models; Optimization criteria; Performance; Trace driven simulation; Traffic model; Virtual worlds; Visual qualities; Interactive computer graphics
Identification of scene locations from geotagged images,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874848138&doi=10.1145%2f2422956.2422961&partnerID=40&md5=e4e4d384c577ef86dd819401a3ed8ca3,"Due to geotagging capabilities of consumer cameras, it has become easy to capture the exact geometric location where a picture is taken. However, the location is not the whereabouts of the scene taken by the photographer but the whereabouts of the photographer himself. To determine the actual location of an object seen in a photo some sophisticated and tiresome steps are required on a special camera rig, which are generally not available in common digital cameras. This article proposes a novel method to determine the geometric location corresponding to a specific image pixel. A new technique of stereo triangulation is introduced to compute the relative depth of a pixel position. Geographical metadata embedded in images are utilized to convert relative depths to absolute coordinates. When a geographic database is available we can also infer the semantically meaningful description of a scene object from where the specified pixel is projected onto the photo. Experimental results demonstrate the effectiveness of the proposed approach in accurately identifying actual locations. © 2013 ACM.",Algorithms; Design; Experimentation,Algorithms; Design; Metadata; Absolute coordinate; Experimentation; Geographic database; Geometric locations; Geotagged images; Pixel position; Scene object; Stereo triangulation; Pixels
Spider: A system for finding 3D video copies,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874890577&doi=10.1145%2f2422956.2422963&partnerID=40&md5=e5ca2a97fbc11a6c30df03b24b73a5bd,"This article presents a novel content-based copy detection system for 3D videos. The system creates compact and robust depth and visual signatures from the 3D videos. Then, signature of a query video is compared against an indexed database of reference videos' signatures. The system returns a score, using both spatial and temporal characteristics of videos, indicating whether the query video matches any video in the reference video database, and in case of matching, which portion of the reference video matches the query video. Analysis shows that the system is efficient, both computationally and storage-wise. The system can be used, for example, by video content owners, video hosting sites, and third-party companies to find illegally copied 3D videos. We implemented Spider, a complete realization of the proposed system, and conducted rigorous experiments on it. Our experimental results show that the proposed system can achieve high accuracy in terms of precision and recall even if the 3D videos are subjected to several transformations at the same time. For example, the proposed system yields 100% precision and recall when copied videos are parts of original videos, and more than 90% precision and recall when copied videos are subjected to different individual transformations. © 2013 ACM.",Algorithms; Experimentation,Algorithms; Experiments; Query processing; Video recording; Content-based copy detections; Experimentation; Precision and recall; Query video; Temporal characteristics; Video contents; Video database; Visual signatures; Three dimensional computer graphics
Privacy preserving continuous multimedia streaming in MANETs,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897016441&doi=10.1145%2f2501643.2501645&partnerID=40&md5=2b05d07de4a89b9a4b8b332aefcdb92f,"At present, mobile devices are prevalent with end users and continuous media streaming services in mobile ad-hoc networks (MANETs) support popular applications. It is required for applications that stream isochronous media that the network link be continuously available. In this study, we introduce two group-server scheduling schemes to improve link continuity: static group-server scheduling and dynamic group-server scheduling. With our solution, if one of the current links between a client and a server instance breaks, the client can still download the multimedia content from another scheduled server peer. In addition, we incorporate the data link layer constraints as well as privacy concerns into our protocol design. The simulation results show that the proposed schemes significantly improve the effective link duration, overall system performance, and degree of privacy in MANETs. © 2014 ACM.",Link availability; Mobile ad-hoc networks; Mobility models; Privacy; Streaming,Acoustic streaming; Data privacy; Media streaming; Mobile devices; Scheduling; Telecommunication links; Continuous media; Link availability; Mobile adhoc network (MANETs); Mobility model; Multimedia contents; Multimedia streaming; Privacy preserving; Scheduling schemes; Mobile ad hoc networks
SIFT match verification by geometric coding for large-scale partial-duplicate web image search,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874848071&doi=10.1145%2f2422956.2422960&partnerID=40&md5=2b27edd6adfd23eef3103499475f4fce,"Most large-scale image retrieval systems are based on the bag-of-visual-words model. However, the traditional bag-of-visualwords model does not capture the geometric context among local features in images well, which plays an important role in image retrieval. In order to fully explore geometric context of all visual words in images, efficient global geometric verification methods have been attracting lots of attention. Unfortunately, current existing methods on global geometric verification are either computationally expensive to ensure real-time response, or cannot handle rotation well. To solve the preceding problems, in this article, we propose a novel geometric coding algorithm, to encode the spatial context among local features for largescale partial-duplicate Web image retrieval. Our geometric coding consists of geometric square coding and geometric fan coding, which describe the spatial relationships of SIFT features into three geo-maps for global verification to remove geometrically inconsistent SIFT matches. Our approach is not only computationally efficient, but also effective in detecting partial-duplicate images with rotation, scale changes, partial-occlusion, and background clutter. Experiments in partial-duplicate Web image search, using two datasets with one million Web images as distractors, reveal that our approach outperforms the baseline bag-of-visual-words approach even following a RANSAC verification in mean average precision. Besides, our approach achieves comparable performance to other state-of-the-art global geometric verification methods, for example, spatial coding scheme, but is more computationally efficient. © 2013 ACM.",Algorithms; Experimentation; Verification,Algorithms; Computational efficiency; Experiments; Geometry; Image matching; Verification; Bag-of-visual-words; Computationally efficient; Experimentation; Geometric verifications; Image retrieval systems; Real-time response; Spatial relationships; Web image retrieval; Image retrieval
Procedural content generation for games: A survey,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874922448&doi=10.1145%2f2422956.2422957&partnerID=40&md5=b5392c218d6ea45f55c7987e20636f32,"Hundreds of millions of people play computer games every day. For them, game content-from 3D objects to abstract puzzles- plays a major entertainment role. Manual labor has so far ensured that the quality and quantity of game content matched the demands of the playing community, but is facing new scalability challenges due to the exponential growth over the last decade of both the gamer population and the production costs. Procedural Content Generation for Games (PCG-G) may address these challenges by automating, or aiding in, game content generation. PCG-G is difficult, since the generator has to create the content, satisfy constraints imposed by the artist, and return interesting instances for gamers. Despite a large body of research focusing on PCG-G, particularly over the past decade, ours is the first comprehensive survey of the field of PCG-G. We first introduce a comprehensive, six-layered taxonomy of game content: bits, space, systems, scenarios, design, and derived. Second, we survey the methods used across the whole field of PCG-G from a large research body. Third, we map PCG-G methods to game content layers; it turns out that many of the methods used to generate game content from one layer can be used to generate content from another. We also survey the use of methods in practice, that is, in commercial or prototype games. Fourth and last, we discuss several directions for future research in PCG-G, which we believe deserve close attention in the near future. © 2013 ACM.",Algorithms; Design; Standardization; Theory,Algorithms; Design; Population statistics; Research; Standardization; 3D object; Exponential growth; Game contents; Manual labors; Procedural content generations; Production cost; Theory; Surveys
Robust image annotation via simultaneous feature and sample outlier pursuit,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896926339&doi=10.1145%2f2501643.2501646&partnerID=40&md5=71afcbb7f75165c02c8496843c05b145,"Graph-based semi-supervised image annotation has achieved great success in a variety of studies, yet it essentially and intuitively suffers from both the irrelevant/noisy features (referred to as feature outliers) and the unusual/corrupted samples (referred to as sample outliers). In this work, we investigate how to derive robust sample affinity matrix via simultaneous feature and sample outlier pursuit. This task is formulated as a Dual-outlier and Prior-driven Low-Rank Representation (DP-LRR) problem, which possesses convexity in objective function. In DP-LRR, the clean data are assumed to be self-reconstructible with low-rank coefficient matrix as in LRR; while the error matrix is decomposed as the sum of a row-wise sparse matrix and a column-wise sparse matrix, the l2,1-norm minimization of which encourages the pursuit of feature and sample outliers respectively. The DP-LRR is further regularized by the priors from side information, that is, the inhomogeneous data pairs. An efficient iterative procedure based on linearized alternating direction method is presented to solve the DP-LRR problem, with closed-form solutions within each iteration. The derived low-rank reconstruction coefficient matrix is then fed into any graph based semi-supervised label propagation algorithm for image annotation, and as a by-product, the cleaned data from DP-LRR can also be utilized as a better image representation to generally boost image annotation performance. Extensive experiments on MIRFlickr, Corel30K, NUS-WIDE-LITE and NUS-WIDE databases well demonstrate the effectiveness of the proposed formulation for robust image annotation.. © 2014 ACM.",Low-Rank Representation; Sample and feature outlier removal,Graphic methods; Image analysis; Iterative methods; Matrix algebra; 1-norm minimizations; Alternating direction methods; Closed form solutions; Coefficient matrix; Image representations; Low-rank representations; Objective functions; Outlier removals; Statistics
Towards decrypting attractiveness via multi-modality cues,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896936092&doi=10.1145%2f2501643.2501650&partnerID=40&md5=41634039878d02364199bf337193abdf,"Decrypting the secret of beauty or attractiveness has been the pursuit of artists and philosophers for centuries. To date, the computational model for attractiveness estimation has been actively explored in computer vision and multimedia community, yet with the focus mainly on facial features. In this article, we conduct a comprehensive study on female attractiveness conveyed y single/multiplemodalities of cues, that is, face, dressing and/or voice, and aim to discover how different modalities individually and collectively affect the human sense of beauty. To extensively investigate the problem, we collect the Multi-Modality Beauty (M2B) dataset, which is annotated with attractiveness levels converted from manual k-wise ratings and semantic attributes of different modalities. Inspired by the common consensus that middle-level attribute prediction can assist higher-level computer vision tasks, we manually labeled many attributes for each modality. Next, a tri-layer Dual-supervised Feature-Attribute-Task (DFAT) network is proposed to jointly learn the attribute model and attractiveness model of single/multiple modalities. To remedy possible loss of information caused by incomplete manual attributes, we also propose a novel Latent Dual-supervised Feature-Attribute- Task (LDFAT) network, where latent attributes are combined with manual attributes to contribute to the final attractiveness estimation. The extensive experimental evaluations on the collected M2B dataset well demonstrate the effectiveness of the proposed DFAT and LDFAT networks for female attractiveness prediction.. © 2014 ACM.",Dressing; Face; Latent attributes; Voice attractiveness,Philosophical aspects; Semantics; Wheel dressing; Computational model; Experimental evaluation; Face; Facial feature; Latent attributes; Multi-modality; Multimedia community; Semantic attribute; Computer vision
Hybrid method based on topography for robust detection of iris center and eye corners,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896913563&doi=10.1145%2f2501643.2501647&partnerID=40&md5=cc6cbd82aa72e41a7e152570142c7048,A multistage procedure to detect eye features is presented. Multiresolution and topographic classification are used to detect the iris center. The eye corner is calculated combining valley detection and eyelid curve extraction. The algorithm is tested in the BioID database and in a proprietary database containing more than 1200 images. The results show that the suggested algorithm is robust and accurate. Regarding the iris center our method obtains the best average behavior for the BioID database compared to other available algorithms. Additional contributions are that our algorithm functions in real time and does not require complex post processing stages.. © 2014 ACM.,Corner detection; Eye tracking; Iris detection; Topography,Database systems; Topography; Average behavior; Corner detection; Eye-tracking; Iris detection; Multiresolution; Post-processing stages; Robust detection; Valley detections; Algorithms
Towards optimizing human labeling for interactive image tagging,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893853416&doi=10.1145%2f2501643.2501651&partnerID=40&md5=0eda407e2eed87b00f2b7aa140c4e218,"Interactive tagging is an approach that combines human and computer to assign descriptive keywords to image contents in a semi-Automatic way. It can avoid the problems in automatic tagging and pure manual tagging by achieving a compromise between tagging performance and manual cost. However, conventional research efforts on interactive tagging mainly focus on sample selection and models for tag prediction. In this work, we investigate interactive tagging from a different aspect. We introduce an interactive image tagging framework that can more fully make use of human's labeling efforts. That means, it can achieve a specified tagging performance by taking less manual labeling effort or achieve better tagging performance with a specified labeling cost. In the framework, hashing is used to enable a quick clustering of image regions and a dynamic multiscale clustering labeling strategy is proposed such that users can label a large group of similar regions each time. We also employ a tag refinement method such that several inappropriate tags can be automatically corrected. Experiments on a large dataset demonstrate the effectiveness of our approach. © 2014 ACM.",Interactive image tagging; Multiscale cluster labeling,Hardware; Automatic tagging; Cluster labeling; Interactive images; Interactive tagging; Labeling strategy; Multi-scale clustering; Research efforts; Sample selection; Computer networks
Exploiting content relevance and social relevance for personalized ad recommendation on internet TV,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896923727&doi=10.1145%2f2501643.2501648&partnerID=40&md5=0293e73427bc01356eab2ef32456fb62,"There have been not many interactions between the two dominant forms of mass communication: television and the Internet, while nowadays the appearance of Internet television makes them more closely. Different with traditional TV in a passive mode of transmission, Internet TV makes it more possible to make personalized service recommendation because of the interactivity between users and the Internet. In this article, we introduce a scheme to provide targeted ad recommendation to Internet TV users by exploiting the content relevance and social relevance. First, we annotate TV videos in terms of visual content analysis and textual analysis by aligning visual and textual information. Second, with user-user, video-video and user-video relationships, we employ Multi-Relationship based Probabilistic Matrix Factorization (MRPMF) to learn representative tags for modeling user preference. And then semantic content relevance (between product/ad and TV video) and social relevance (between product/ad and user interest) are calculated by projecting the corresponding tags into our advertising concept space. Finally, with relevancy scores we make ranking for relevant product/ads to effectively provide users personalized recommendation. The experimental results demonstrate attractiveness and effectiveness of our proposed approach.. © 2014 ACM.",Internet TV; Personalized recommendation; Video advertising; Video alignment,Marketing; Semantics; Television broadcasting; Internet television; Internet tv; Personalized recommendation; Personalized service; Probabilistic matrix factorizations; Textual information; Video advertisings; Video alignment; Internet
Mobile Haptic E-Book system to support 3D immersive reading in ubiquitous environments,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896924778&doi=10.1145%2f2501643.2501649&partnerID=40&md5=cccbf5af2d8b57f597ad3b32a38b79f1,"In order to leverage the use of various modalities such as audio-visual materials in instilling effective learning behavior we present an intuitive approach of annotation based hapto-Audio-visual interaction with the traditional digital learning materials such as e-books. By integrating the home entertainment system in the user's reading experience combined with haptic interfaces we want to examine whether such augmentation of modalities influence the user's learning patterns. The proposed Haptic E- Book (HE-Book) system leverages the haptic jacket, haptic arm band as well as haptic sofa interfaces to receive haptic emotive signals wirelessly in the form of patterned vibrations of the actuators and expresses the learning material by incorporating image, video, 3D environment based augmented display in order to pave ways for intimate reading experience in the popular mobile e-book platform.. © 2014 ACM.",Edutainment system; Haptic modality; Mobile learning; Multimodal interface,Electronic document exchange; Electronic publishing; Interfaces (materials); Three dimensional; Audio-visual material; Digital learning materials; Edutainment systems; Haptic modality; Home entertainment system; Mobile Learning; Multi-modal interfaces; Ubiquitous environments; Haptic interfaces
A framework for network aware caching for video on demand systems,2013,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896939180&doi=10.1145%2f2501643.2501652&partnerID=40&md5=4ed346b60fdace8bf3c869b7fbf57b74,"Video on Demand (VoD) services allow users to select and locally consume remotely stored content. We investigate the use of caching to solve the scalability issues of several existing VoD providers. We propose metrics and goals that define the requirements of a caching framework for CDNs of VoD systems. Using data logs collected from Motorola equipment from Comcast VoD deployments we show that several classic caching solutions do not satisfy the proposed goals. We address this issue by developing novel techniques for predicting future values of several metrics of interest. We rely on computed predictions to define the penalty imposed on the system, both network and caching sites, when not storing individual items. We use item penalties to devise novel caching and static content placement strategies. We use the previously mentioned data logs to validate our solutions and show that they satisfy all the defined goals.. © 2014 ACM.",Caching; Content distribution networks; Video on demand,Information retrieval; Caching; Content distribution networks; Network-aware; Novel techniques; Placement strategy; Scalability issue; Video on demand services; Video-on-demand system; Video on demand
User perception of media content association in olfaction-enhanced multimedia,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871193695&doi=10.1145%2f2379790.2379794&partnerID=40&md5=c3177b6120d56d519263daddc389752b,Olfaction is an exciting challenge facing multimedia applications. In this article we have investigated user perception of the association between olfactory media content and video media content in olfactory-enhanced multimedia. Results show that the association between scent and content has a significant impact on the user-perceived experience of olfactory-enhanced multimedia.,Human-computer interaction; Multimedia quality; Olfaction; Quality of perception,Hardware; Human computer interaction; Media content; Multimedia applications; Multimedia quality; Olfaction; Quality of perception; Significant impacts; User perceptions; Video media; Computer networks
Object-based image retrieval with kernel on adjacency atrix and local combined features,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871223446&doi=10.1145%2f2379790.2379796&partnerID=40&md5=e95f13bb800c6e3725c0f0a5437dcf3b,"In object-based image retrieval, there are two important issues: an effective image representation method for representing image content and an effective image classification method for processing user feedback to find more images containing the user-desired object categories. In the image representation method, the local-based representation is the best selection for object-based image retrieval. As a kernel-based classification method, Support Vector Machine (SVM) has shown impressive performance on image classification. But SVM cannot work on the local-based representation unless there is an appropriate kernel. To address this problem, some representative kernels are proposed in literatures. However, these kernels cannot work effectively in object-based image retrieval due to ignoring the spatial context and the combination of local features. In this article, we present Adjacent Matrix (AM) and the Local Combined Features (LCF) to incorporate the spatial context and the combination of local features into the kernel. We propose the AM-LCF feature vector to represent image content and the AM-LCF kernel to measure the similarities between AM-LCF feature vectors. According to the detailed analysis, we show that the proposed kernel can overcome the deficiencies of existing kernels. Moreover, we evaluate the proposed kernel through experiments of object-based image retrieval on two public image sets. The experimental results show that the performance of object-based image retrieval can be improved by the proposed kernel.",Feedback processing; Kernel; Local combined features; Object-based image retrieval,Image classification; Partial discharges; Support vector machines; Adjacent matrix; Classification methods; Combined features; Feature vectors; Image content; Image representations; Kernel; Kernel-based classification methods; Local feature; Object categories; Object-based image retrieval; Public image; Spatial context; User feedback; Image retrieval
Label-to-region with continuity-biased bi-layer sparsity Priors,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871256346&doi=10.1145%2f2379790.2379792&partnerID=40&md5=783bc5265157f2d9bc23acada91a8f40,"In this work, we investigate how to reassign the fully annotated labels at image level to those contextually derived semantic regions, namely Label-to-Region (L2R), in a collective manner. Given a set of input images with label annotations, the basic idea of our approach to L2R is to first discover the patch correspondence across images, and then propagate the common labels shared in image pairs to these correlated patches. Specially, our approach consists of following aspects. First, each of the input images is encoded as a Bag-of-Hierarchical-Patch (BOP) for capturing the rich cues at variant scales, and the individual patches are expressed by patch-level feature descriptors. Second, we present a sparse representation formulation for discovering how well an image or a semantic region can be robustly reconstructed by all the other image patches from the input image set. The underlying philosophy of our formulation is that an image region can be sparsely reconstructed with the image patches belonging to the other images with common labels, while the robustness in label propagation across images requires that these selected patches come from very few images. This preference of being sparse at both patch and image level is named bi-layer sparsity prior. Meanwhile, we enforce the preference of choosing larger-size patches in reconstruction, referred to as continuitybiased prior in this work, which may further enhance the reliability of L2R assignment. Finally, we harness the reconstruction coefficients to propagate the image labels to the matched patches, and fuse the propagation results over all patches to finalize the L2R task. As a by-product, the proposed continuity-biased bi-layer sparse representation formulation can be naturally applied to perform image annotation on new testing images. Extensive experiments on three public image datasets clearly demonstrate the effectiveness of our proposed framework in both L2R assignment and image annotation. © 2012 ACM.",Bag-of-hierarchical-patch; Image annotation; Label-to-Region; Sparse representation,Image analysis; Semantics; Bag-of-hierarchical-patch; Bi-layer; Feature descriptors; Image annotation; Image pairs; Image patches; Image regions; Input image; Label propagation; Label-to-Region; Public image; Sparse representation; Image reconstruction
Editorial,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871187930&doi=10.1145%2f2379790.2379791&partnerID=40&md5=bbd3ade303e8b8d0a25f8df55edc6898,[No abstract available],,
Algorithms for stochastic optimization of multicast content delivery with network coding,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871197333&doi=10.1145%2f2379790.2379798&partnerID=40&md5=71ee81c27d11e239eb89f303aeaf57d8,"The usage of network resources by content providers is commonly governed by Service-Level Agreements (SLA) between the content provider and the network service provider. Resource usage exceeding the limits specified in the SLA incurs the content provider additional charges, usually at a higher cost. Hence, the content provider's goal is to provision adequate resources in the SLA based on forecasts of future demand. We study capacity purchasing strategies when the content provider employs network coded multicast as the media delivery mechanism, with uncertainty in its future customer set explicitly taken into consideration. The latter requires the content provider to make capacity provisioning decisions based on market predictions and historical customer usage patterns. The probabilistic element suggests a stochastic optimization approach. We model this problem as a two-stage stochastic optimization problem with recourse. Such optimizations are #P-hard to solve directly, and we design two approximation algorithms for them. The first is a heuristic algorithm that exploits properties unique to network coding, so that only polynomial-time operations are needed. It performs well in general scenarios, but the gap from the optimal solution is not bounded by any constant in the worst case. This motivates our second approach, a sampling algorithm partly inspired from the work of Gupta et al. [2004a]. We employ techniques from duality theory in linear optimization to prove that the sampling algorithm provides a 3-approximation to the stochastic multicast problem. We conduct extensive simulations to illustrate the efficacy of both algorithms, and show that the performance of both is usually within 10% of the optimal solution in practice.",Linear programming; Multicast; Network coding; Stochastic optimization,Approximation algorithms; Content based retrieval; Heuristic algorithms; Learning algorithms; Linear programming; Multicasting; Network coding; Optimal systems; Sales; Stochastic models; Stochastic systems; Content delivery; Content providers; Duality theory; Extensive simulations; Linear optimization; Market prediction; Media delivery; Multicasts; Network resource; Network service providers; Optimal solutions; Polynomial-time; Resource usage; Sampling algorithm; Service Level Agreements; Stochastic optimization approach; Stochastic optimization problems; Stochastic optimizations; Usage patterns; Optimization
In-video product annotation with web information mining,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871253650&doi=10.1145%2f2379790.2379797&partnerID=40&md5=252d39e13365954f2d3051658efcf45e,"Product annotation in videos is of great importance for video browsing, search, and advertisement. However, most of the existing automatic video annotation research focuses on the annotation of high-level concepts, such as events, scenes, and object categories. This article presents a novel solution to the annotation of specific products in videos by mining information from the Web. It collects a set of high-quality training data for each product by simultaneously leveraging Amazon and Google image search engine. A visual signature for each product is then built based on the bag-of-visual-words representation of the training images. A correlative sparsification approach is employed to remove noisy bins in the visual signatures. These signatures are used to annotate video frames. We conduct experiments on more than 1,000 videos and the results demonstrate the feasibility and effectiveness of our approach.",Product Annotation; Video search; Web mining,Internet; Automatic video annotation; Bag-of-visual-words; High quality; Image search engine; Object categories; Product Annotation; Sparsification; Training data; Training image; Video browsing; Video frame; Video search; Visual signatures; Web information; Web Mining; Data mining
NextSlidePlease: Authoring and delivering agile multimedia presentations,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871217141&doi=10.1145%2f2379790.2379795&partnerID=40&md5=21a3580c1ca9844fc38b3be68d573554,"Presentation support tools, such as Microsoft PowerPoint, pose challenges both in terms of creating linear presentations from complex data and fluidly navigating such linear structures when presenting to diverse audiences. NextSlidePlease is a slideware application that addresses these challenges using a directed graph structure approach for authoring and delivering multimedia presentations. The application combines novel approaches for searching and analyzing presentation datasets, composing meaningfully structured presentations, and efficiently delivering material under a variety of time constraints. We introduce and evaluate a presentation analysis algorithm intended to simplify the process of authoring dynamic presentations, and a time management and path selection algorithm that assists users in prioritizing content during the presentation process. Results from two comparative user studies indicate that the directed graph approach promotes the creation of hyperlinks, the consideration of connections between content items, and a richer understanding of the time management consequences of including and selecting presentation material.",Authoring; Presentations; Slideware,Algorithms; Hypertext systems; Management; Analysis algorithms; Authoring; Complex data; Data sets; Directed graph structures; Dynamic presentation; Hyperlinks; Linear structures; Microsoft PowerPoint; Multimedia presentation; Path selection algorithms; Presentations; Slideware; Structured presentations; Support tool; Time constraints; Time management; User study; Directed graphs
Efficient targeted search using a focus and context video browser,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871240807&doi=10.1145%2f2379790.2379793&partnerID=40&md5=d9bf77f41d3bda13935736b0da2d4bf7,"Currently there are several interactive content-based video retrieval techniques and systems available. However, retrieval performance depends heavily on the means of interaction. We argue that effective CBVR requires efficient, specialized user interfaces. In this article we propose guidelines for such an interface, and we propose an effective CBVR engine: the ForkBrowser, which builds upon the principle of focus and context. This browser is evaluated using a combination of user simulation and real user evaluation. Results indicate that the ideas have merit, and that the browser performs very well when compared to the state-of-the-art in video retrieval.",Conceptual similarity; Information visualization; Interactive search; Multidimensional browsing; Semantic threads,Information systems; Semantics; User interfaces; Conceptual similarity; Content-based video retrieval; Information visualization; Interactive search; Multi-dimensional browsing; Retrieval performance; User evaluations; User simulation; Video retrieval; Image retrieval
A collusion attack optimization strategy for digital fingerprinting,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871287849&doi=10.1145%2f2344436.2344442&partnerID=40&md5=81184f0197b20bb5eb1aae00949f1357,"Collusion attack is a cost-efficient attack for digital fingerprinting. In this article, we propose a novel collusion attack strategy, Iterative Optimization Collusion Attack (IOCA), which is based upon the gradient attack and the principle of informed watermark embedding. We evaluate the performance of the proposed collusion attack strategy in defeating four typical fingerprinting schemes under a well-constructed evaluation framework. The simulation results show that the proposed strategy performs more effectively than the gradient attack, and adopting no more than three fingerprinted copies can sufficiently collapse examined fingerprinting schemes. Meanwhile, the content resulted from the proposed attack still preserves high perceptual quality. © 2012 ACM.",Collusion attack; Digital fingerprinting; Multimedia security; Optimization,Optimization; Collusion attack; Cost-efficient; Digital fingerprinting; Evaluation framework; Fingerprinting schemes; Iterative Optimization; Multimedia security; Optimization strategy; Perceptual quality; Watermark embedding; Network security
Image hatching for visual cryptography,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871295102&doi=10.1145%2f2344436.2344438&partnerID=40&md5=cd4986706fb3b2e200a40c2b9dd76acd,"Image hatching (or nonphotorealistic line-art) is a technique widely used in the printing or engraving of currency. Diverse styles of brush strokes have previously been adopted for different areas of an image to create aesthetically pleasing textures and shading. Because there is no continuous tone within these types of images, a multilevel scheme is proposed, which uses different textures based on a threshold level. These textures are then applied to the different levels and are then combined to build up the final hatched image. The proposed technique allows a secret to be hidden using Visual Cryptography (VC) within the hatched images. Visual cryptography provides a very powerful means by which one secret can be distributed into two or more pieces known as shares. When the shares are superimposed exactly together, the original secret can be recovered without computation. Also provided is a comparison between the original grayscale images and the resulting hatched images that are generated by the proposed algorithm. This reinforces that the overall quality of the hatched scheme is sufficient. The Structural SIMilarity index (SSIM) is used to perform this comparison. © 2012 ACM.",Cryptography; Data encryption; Image hatching; Structural SIMilarity; Visual Cryptography,Cryptography; Textures; Brush stroke; Data encryption; Gray-scale images; Image hatching; Line-art; Overall quality; Structural similarity; Threshold levels; Visual cryptography; Image texture
A novel 3D video transcoding scheme for adaptive 3D video transmission to heterogeneous terminals,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870211819&doi=10.1145%2f2348816.2348822&partnerID=40&md5=35affbb6829be4c7841edd6032cf741a,"Three-dimensional video (3DV) is attracting many interests with its enhanced viewing experience and more user driven features. 3DV has several unique characteristics different from 2D video: (1) It has a much larger amount of data captured and compressed, and corresponding video compression techniques can be much more complicated in order to explore data redundancy. This will lead to more constraints on users' network access and computational capability, (2) Most users only need part of the 3DV data at any given time, while the users' requirements exhibit large diversity, (3) Only a limited number of views are captured and transmitted for 3DV. View rendering is thus necessary to generate virtual views based on the received 3DV data. However, many terminal devices do not have the functionality to generate virtual views. To enable 3DV experience for the majority of users with limited capabilities, adaptive 3DV transmission is necessary to extract/generate the required data content and represent it with supported formats and bitrates for heterogeneous terminal devices. 3DV transcoding is an emerging and effective technique to achieve desired adaptive 3DV transmission. In this article, we propose the first efficient 3DV transcoding scheme that can obtain any desired view, either an encoded one or a virtual one, and compress it with more universal H.264/AVC. The key idea of the proposed scheme is to appropriately utilize motion information contained in the bitstream to generate candidate motion information. Original information of both the desired view and reference views are used to obtain this candidate information and a proper motion refinement process is carried out for certain blocks. Simulation results show that, compared to the straightforward cascade algorithm, the proposed scheme is able to output compressed bitstream of the required view with significantly reduced complexity while incurring negligible performance loss. Such a 3DV transcoding can be applied to most gateways that usually have constraints on computational complexity and time delay. © 2012 ACM 1551-6857/2012/09-ART43 $15.00.",3D video; Adaptive 3D video; Multiview video; Transcoding,Digital storage; Image communication systems; Image compression; Motion Picture Experts Group standards; Video signal processing; 2D video; 3D video; Bit stream; Bitrates; Cascade algorithm; Compression techniques; Computational capability; Data contents; Data redundancy; Heterogeneous terminals; Motion information; Multiview video; Network access; Performance loss; Proper motion; Reduced complexity; Refinement process; Terminal devices; Transcoding; View rendering; Virtual view; Three dimensional computer graphics
CZLoD: A psychophysical approach for 3D tele-immersive video,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870220067&doi=10.1145%2f2348816.2348818&partnerID=40&md5=33bd4481bd1c95b7729036a25d7edd3d,"This article presents a psychophysical study that measures the perceptual thresholds of a new factor called Color-plus-Depth Level-of-Details (CZLoD) peculiar to polygon-based 3D tele-immersive video. The results demonstrate the existence of Just Noticeable Degradation and Just Unacceptable Degradation thresholds on the factor. In light of the results, we design and implement a real-time perception-based quality adaptor for 3D tele-immersive video. Our experimental results show that the adaptation scheme can reduce resource usage (e.g., CPU cycles) while considerably enhancing the overall perceived visual quality. Our analysis confirms the potential temporal and spatial performance benefits achievable with CZLoD adaptation. © 2012 ACM.",Adaptation; Color-plus-Depth; Level-of-Details; Psychophysics; Tele-immersive video,Computer networks; Hardware; Adaptation; Color-plus-Depth; Level of detail; Psychophysics; Tele-immersive video; Three dimensional computer graphics
Aggregate licenses validation for digital rights violation detection,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871267195&doi=10.1145%2f2344436.2344443&partnerID=40&md5=bbb695b6fa0acf9aa8092fb5623c39a7,"Digital Rights Management (DRM) is the term associated with the set of technologies to prevent illegal multimedia content distribution and consumption. DRM systems generally involve multiple parties such as owner, distributors, and consumers. The owner issues redistribution licenses to its distributors. The distributors in turn using their received redistribution licenses can generate and issue new redistribution licenses to other distributors and new usage licenses to consumers. As a part of rights violation detection, these newly generated licenses must be validated by a validation authority against the redistribution license used to generate them. The validation of these newly generated licenses becomes quite complex when there exist multiple redistribution licenses for a media with the distributors. In such cases, the validation process requires validation using an exponential number (to the number of redistribution licenses) of validation inequalities and each validation inequality may contain up to an exponential number of summation terms. This makes the validation process computationally intensive and necessitates to do the validation efficiently. To overcome this, we propose validation tree, a prefix-tree-based validation method to do the validation efficiently. Theoretical analysis and experimental results show that our proposed technique reduces the validation time significantly. © 2012 ACM.",Digital Rights Management (DRM); License organization; License validation,Computation; Copy Rights; Forestry; Forestry; Digital rights; Digital Rights Management; DRM system; Exponential numbers; License validation; Multimedia content distribution; Validation process; Copyrights
Exposing MP3 audio forgeries using frame offsets,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871277299&doi=10.1145%2f2344436.2344441&partnerID=40&md5=578f4b5b3185b8dd27e7148890235b71,"Audio recordings should be authenticated before they are used as evidence. Although audio watermarking and signature are widely applied for authentication, these two techniques require accessing the original audio before it is published. Passive authentication is necessary for digital audio, especially for the most popular audio format: MP3. In this article, we propose a passive approach to detect forgeries of MP3 audio. During the process of MP3 encoding the audio samples are divided into frames, and thus each frame has its own frame offset after encoding. Forgeries lead to the breaking of framing grids. So the frame offset is a good indication for locating forgeries, and it can be retrieved by the identification of the quantization characteristic. In this way, the doctored positions can be automatically located. Experimental results demonstrate that the proposed approach is effective in detecting some common forgeries, such as deletion, insertion, substitution, and splicing. Even when the bit rate is as low as 32 kbps, the detection rate is above 99%. © 2012 ACM.",Audio authentication; Forgery detection; MP3 audio forgery,Encoding (symbols); Audio authentication; Audio samples; Bit rates; Detection rates; Digital audio; Forgery detections; MP3 audio forgery; Authentication
A real-time remote rendering system for interactive mobile graphics,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870232628&doi=10.1145%2f2348816.2348825&partnerID=40&md5=4ea472422592606771f94b5fe382112f,"Mobile devices are gradually changing people's computing behaviors. However, due to the limitations of physical size and power consumption, they are not capable of delivering a 3D graphics rendering experience comparable to desktops. Many applications with intensive graphics rendering workloads are unable to run on mobile platforms directly. This issue can be addressed with the idea of remote rendering: the heavy 3D graphics rendering computation runs on a powerful server and the rendering results are transmitted to the mobile client for display. However, the simple remote rendering solution inevitably suffers from the large interaction latency caused by wireless networks, and is not acceptable for many applications that have very strict latency requirements. In this article, we present an advanced low-latency remote rendering system that assists mobile devices to render interactive 3D graphics in real-time. Our design takes advantage of an image based rendering technique: 3D image warping, to synthesize the mobile display from the depth images generated on the server. The research indicates that the system can successfully reduce the interaction latency while maintaining the high rendering quality by generating multiple depth images at the carefully selected viewpoints. We study the problem of viewpoint selection, propose a real-time reference viewpoint prediction algorithm, and evaluate the algorithm performance with real-device experiments. © 2012 ACM.",3D image warping; Interaction latency; Mobile devices; Remote rendering,Algorithms; Mobile devices; Three dimensional; 3-D image; 3D graphics; Algorithm performance; Computing behavior; Depth image; Graphics rendering; Image based rendering; Interaction latency; Low-latency; Mobile client; Mobile Displays; Mobile graphics; Mobile platform; Prediction algorithms; Remote rendering; Remote rendering systems; Rendering quality; Viewpoint selection; Three dimensional computer graphics
A robust high-capacity affine-transformation-invariant scheme for watermarking 3D geometric models,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871263568&doi=10.1145%2f2344436.2344440&partnerID=40&md5=171e742fe92816da0f0ed155446440dc,"In this article we propose a novel, robust, and high-capacity watermarking method for 3D meshes with arbitrary connectivities in the spatial domain based on affine invariants. Given a 3D mesh model, a watermark is embedded as affine-invariant length ratios of one diagonal segment to the residing diagonal intersected by the other one in a coplanar convex quadrilateral. In the extraction process, a watermark is recovered by combining all the watermark pieces embedded in length ratios through majority voting. Extensive experimental results demonstrate the robustness, high computational efficiency, high capacity, and affine-transformation-invariant characteristics of the proposed approach. © 2012 ACM.",3D model authentication; 3D model watermarking; Affine transformation invariant; Copyright protection; High capacity,Copyrights; Three dimensional; Three dimensional computer graphics; 3D model watermarking; 3D models; Affine transformations; Copyright protections; High capacity; Watermarking
Introduction to special issue on multimedia security,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871281307&doi=10.1145%2f2344436.2344437&partnerID=40&md5=880698b19e71836b56afd7b41f010c18,[No abstract available],,
Reference index-based H.264 video watermarking scheme,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871328857&doi=10.1145%2f2344436.2344439&partnerID=40&md5=dd106ecd2e3a6220b38145c99d48815d,"Video watermarking has received much attention over the past years as a promising solution to copy protection. Watermark robustness is still a key issue of research, especially when a watermark is embedded in the compressed video domain. In this article, a robust watermarking scheme for H.264 video is proposed. During video encoding, the watermark is embedded in the index of the reference frame, referred to as reference index, a bitstream syntax element newly proposed in the H.264 standard. Furthermore, the video content (current coded blocks) is modified based on an optimization model, aiming at improving watermark robustness without unacceptably degrading the video's visual quality or increasing the video's bit rate. Compared with the existing schemes, our method has the following three advantages: (1) The bit rate of the watermarked video is adjustable; (2) the robustness against common video operations can be achieved; (3) the watermark embedding and extraction are simple. Extensive experiments have verified the good performance of the proposed watermarking scheme. © 2012 ACM.",H.264; Reference index; Video; Watermarking,Digital watermarking; Image compression; Bit rates; Bitstream syntax; Compressed video; Copy protection; H.264; H.264 standards; H.264 video; Optimization models; Reference frame; Reference index; Robust watermarking scheme; Video; Video contents; Video encodings; Video watermarking; Visual qualities; Watermark embedding; Watermark robustness; Watermarking schemes; Watermarking
Introduction to special section on 3D mobile multimedia,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870228890&doi=10.1145%2f2348816.2348820&partnerID=40&md5=b108c501b1da38a6c16af585fe328058,[No abstract available],,
Introduction to the special section of best papers of ACM multimedia 2011,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870200681&doi=10.1145%2f2348816.2348817&partnerID=40&md5=e0a1a5b3c47aa4376a11ee857a8e12ad,[No abstract available],,
Active query sensing: Suggesting the best query view for mobile visual search,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870160727&doi=10.1145%2f2348816.2348819&partnerID=40&md5=7fc4380706f5a6e56f5d538c4366d4b5,"While much exciting progress is being made in mobile visual search, one important question has been left unexplored in all current systems. When searching objects or scenes in the 3D world, which viewing angle is more likely to be successful? More particularly, if the first query fails to find the right target, how should the user control the mobile camera to form the second query? In this article, we propose a novel Active Query Sensing system for mobile location search, which actively suggests the best subsequent query view to recognize the physical location in the mobile environment. The proposed system includes two unique components: (1) an offline process for analyzing the saliencies of different views associated with each geographical location, which predicts the location search precisions of individual views by modeling their self-retrieval score distributions. (2) an online process for estimating the view of an unseen query, and suggesting the best subsequent view change. Specifically, the optimal viewing angle change for the next query can be formulated as an online information theoretic approach. Using a scalable visual search system implemented over a NYC street view dataset (0.3 million images), we show a performance gain by reducing the failure rate of mobile location search to only 12% after the second query. We have also implemented an end-to-end functional system, including user interfaces on iPhones, client-server communication, and a remote search server. This work may open up an exciting new direction for developing interactive mobile media applications through the innovative exploitation of active sensing and query formulation. © 2012 ACM 1551-6857/2012/09-ART40 $15.00.",Active Query Sensing; Content-based image retrieval; Mobile location recognition; Mobile visual search,Content based retrieval; Information theory; Sensors; User interfaces; Active Query Sensing; Active Sensing; Client-server communication; Content based image retrieval; Data sets; Failure rate; Functional systems; Geographical locations; Mobile camera; Mobile environments; Mobile location; Mobile media; Mobile visual searches; Off-line process; On-line information; On-line process; Performance Gain; Physical locations; Query formulation; Sensing systems; User control; Viewing angle; Visual search; Search engines
A new methodology to derive objective quality assessment metrics for scalable multiview 3D video coding,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870180523&doi=10.1145%2f2348816.2348823&partnerID=40&md5=a8c3bdcf5c87d91ec21c614dab343e8e,"With the growing demand for 3D video, efforts are underway to incorporate it in the next generation of broadcast and streaming applications and standards. 3D video is currently available in games, entertainment, education, security, and surveillance applications. A typical scenario for multiview 3D consists of several 3D video sequences captured simultaneously from the same scene with the help of multiple cameras from different positions and through different angles. Multiview video coding provides a compact representation of these multiple views by exploiting the large amount of inter-view statistical dependencies. One of the major challenges in this field is how to transmit the large amount of data of a multiview sequence over error prone channels to heterogeneous mobile devices with different bandwidth, resolution, and processing/battery power, while maintaining a high visual quality. Scalable Multiview 3D Video Coding (SMVC) is one of the methods to address this challenge; however, the evaluation of the overall visual quality of the resulting scaled-down video requires a new objective perceptual quality measure specifically designed for scalable multiview 3D video. Although several subjective and objective quality assessment methods have been proposed for multiview 3D sequences, no comparable attempt has been made for quality assessment of scalable multiview 3D video. In this article, we propose a new methodology to build suitable objective quality assessment metrics for different scalable modalities in multiview 3D video. Our proposed methodology considers the importance of each layer and its content as a quality of experience factor in the overall quality. Furthermore, in addition to the quality of each layer, the concept of disparity between layers (inter-layer disparity) and disparity between the units of each layer (intra-layer disparity) is considered as an effective feature to evaluate overall perceived quality more accurately. Simulation results indicate that by using this methodology, more efficient objective quality assessment metrics can be introduced for each multiview 3D video scalable modalities. © 2012 ACM.",Mobile 3D video; Multiview 3D video; Objective quality assessment; Scalable modalities,Image coding; Mobile devices; Quality control; Quality of service; Security systems; 3D video; 3D video coding; Compact representation; Error-prone channel; Growing demand; Heterogeneous mobile devices; Intra-layer; Mobile 3D; Multi-views; Multiple cameras; Multiple views; Multiview video coding; Objective quality assessment; Overall quality; Overall visual qualities; Perceived quality; Perceptual quality measures; Quality assessment; Quality of experience (QoE); Scalable modalities; Statistical dependencies; Streaming applications; Surveillance applications; Visual qualities; Three dimensional computer graphics
QoE-oriented 3D video transcoding for mobile streaming,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870168263&doi=10.1145%2f2348816.2348821&partnerID=40&md5=b07029777a1ba2fcbad09df0652ee423,"With advance in mobile 3D display, mobile 3D video is already enabled by the wireless multimedia networking, and it will be gradually popular since it can make people enjoy the natural 3D experience anywhere and anytime. In current stage, mobile 3D video is generally delivered over the heterogeneous network combined by wired and wireless channels. How to guarantee the optimal 3D visual quality of experience (QoE) for the mobile 3D video streaming is one of the important topics concerned by the service provider. In this article, we propose a QoE-oriented transcoding approach to enhance the quality of mobile 3D video service. By learning the pre-controlled QoE patterns of 3D contents, the proposed 3D visual QoE inferring model can be utilized to regulate the transcoding configurations in real-time according to the feedbacks of network and user-end device information. In the learning stage, we propose a piecewise linear mean opinion score (MOS) interpolation method to further reduce the cumbersome manual work of preparing QoE patterns. Experimental results show that the proposed transcoding approach can provide the adapted 3D stream to the heterogeneous network, and further provide superior QoE performance to the fixed quantization parameter (QP) transcoding and mean squared error (MSE) optimized transcoding for mobile 3D video streaming. © 2012 ACM 1551-6857/2012/09- ART42 $15.00.",3D video streaming; Mobile 3D video; QoE; Transcoding,Heterogeneous networks; Optimization; Piecewise linear techniques; Quality of service; Video streaming; 3D content; 3D video; Device information; Interpolation method; Manual work; Mean opinion scores; Mean squared error; Mobile 3D; Piecewise linear; QoE; Quantization parameters; Service provider; Transcoding; Visual qualities; Wired and wireless; Wireless multimedia; Three dimensional computer graphics
Efficient matchings and mobile augmented reality,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870184620&doi=10.1145%2f2348816.2348826&partnerID=40&md5=035537b3a974d136e6cd674501a2f80f,"With the fast-growing popularity of smart phones in recent years, augmented reality (AR) on mobile devices is gaining more attention and becomes more demanding than ever before. However, the limited processors in mobile devices are not quite promising for AR applications that require real-time processing speed. The challenge exists due to the fact that, while fast features are usually not robust enough in matchings, robust features like SIFT or SURF are not computationally efficient. There is always a tradeoff between robustness and efficiency and it seems that we have to sacrifice one for the other. While this is true for most existing features, researchers have been working on designing new features with both robustness and efficiency. In this article, we are not trying to present a completely new feature. Instead, we propose an efficient matching method for robust features. An adaptive scoring scheme and a more distinctive descriptor are also proposed for performance improvements. Besides, we have developed an outdoor augmented reality system that is based on our proposed methods. The system demonstrates that not only it can achieve robust matchings efficiently, it is also capable to handle large occlusions such as passengers and moving vehicles, which is another challenge for many AR applications. © 2012 ACM.",Efficient matching; Image retrieval; Mixed reality; Mobile image matching,Image matching; Image retrieval; Mobile devices; Virtual reality; AR application; Computationally efficient; Descriptors; Efficient matching; Large occlusion; Matching methods; Matchings; Mixed reality; Mobile augmented reality; Moving vehicles; Outdoor augmented reality; Performance improvements; Real-time processing speed; Scoring schemes; Augmented reality
Energy-efficient multicasting of multiview 3D videos to mobile devices,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870194140&doi=10.1145%2f2348816.2348824&partnerID=40&md5=cae6698fd004305421753825b56e40d6,"Multicasting multiple video streams over wireless broadband access networks enables the delivery of multimedia content to large-scale user communities in a cost-efficient manner. Three dimensional (3D) videos are the next natural step in the evolution of digital media technologies. In order to provide 3D perception, 3D video streams contain one or more views that greatly increase their bandwidth requirements. Due to the limited channel capacity and variable bit rate of the videos, multicasting multiple 3D videos over wireless broadband networks is a challenging problem. In this article, we consider a 4G wireless access network in which a number of 3D videos represented in two-view plus depth format and encoded using scalable video coders are multicast. We formulate the optimal 3D video multicasting problem to maximize the quality of rendered virtual views on the receivers' displays. We show that this problem is NP-complete and present a polynomial time approximation algorithm to solve it. We then extend the proposed algorithm to efficiently schedule the transmission of the chosen substreams from each video in order to maximize the power saving on the mobile receivers. Our simulation-based experimental results show that our algorithm provides solutions that are within 0.3 dB of the optimal solutions while satisfying real-time requirements of multicast systems. In addition, our algorithm results in an average power consumption reduction of 86%. © 2012 ACM.",3D video; Energy saving; LTE; Multimedia networking; WiMAX; Wireless networks,Channel capacity; Digital storage; Energy conservation; Mobile devices; Mobile telecommunication systems; Scheduling algorithms; Three dimensional computer graphics; Video streaming; Wimax; Wireless networks; 3D perception; 3D video; 4G wireless; Average power; Bandwidth requirement; Cost-efficient; Digital media technologies; Energy efficient; LTE; Mobile receiver; Multi-views; Multicast systems; Multicasts; Multimedia contents; Multimedia networking; Multiple video streams; Natural steps; NP Complete; Optimal solutions; Polynomial time approximation algorithms; Power savings; Real time requirement; Scalable video coders; Sub-streams; Three-dimensional (3D) video; User communities; Variable bit rate; Virtual view; Wireless broadband access; Wireless broadband networks; Multicasting
A Unified Context Model for Web Image Retrieval,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922054198&doi=10.1145%2f2240136.2240141&partnerID=40&md5=1adad6a47cd13bd6914c1f11b97e3210,"Content-based web image retrieval based on the query-by-example (QBE) principle remains a challenging problem due to the semantic gap as well as the gap between a user's intent and the representativeness of a typical image query. In this article, we propose to address this problem by integrating query-related contextual information into an advanced query model to improve the performance of QBE-based web image retrieval. We consider both the local and global context of the query image. The local context can be inferred from the web pages and the click-through log associated with the query image, while the global context is derived from the entire corpus comprising all web images and the associated web pages. To effectively incorporate the local query context we propose a language modeling based approach to deal with the combined structured query representation from the contextual and visual information. The global query context is integrated by the multi-modal relevance model to “reconstruct” the query from the document models indexed in the corpus. In this way, the global query context is employed to address the noise or missing information in the query and its local context, so that a comprehensive and robust query model can be obtained. We evaluated the proposed approach on a representative product image dataset collected from the web and demonstrated that the inclusion of the local and global query contexts significantly improves the performance of QBE-based web image retrieval. © 2012, ACM. All rights reserved.",Algorithms; context-based web image retrieval; Experimentation; Image retrieval,
An Information-Based Dynamic Extrapolation Model for Networked Virtual Environments,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893626285&doi=10.1145%2f2240136.2240140&partnerID=40&md5=96c46a363c4e17838d7e5af091394c4c,"Various Information Management techniques have been developed to help maintain a consistent shared virtual world in a Networked Virtual Environment. However, such techniques have to be carefully adapted to the application state dynamics and the underlying network. This work presents a novel framework that minimizes inconsistency by optimizing bandwidth usage to deliver useful information. This framework measures the state evolution using an information model and dynamically switches extrapolation models and the packet rate to make the most information-efficient usage of the available bandwidth. The results shown demonstrate that this approach can help optimize consistency under constrained and time-varying network conditions. © 2012, ACM. All rights reserved.",collaborative virtual environments; Consistency; distributed interactive applications; distributed interactive simulation; information management techniques; Measurements; networked multi-player computer games; networked virtual environments; Performance; Theory,
Video Streaming Using a Location-Based Bandwidth-Lookup Service for Bitrate Planning,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010273910&doi=10.1145%2f2240136.2240137&partnerID=40&md5=b720e80649da1de7eb2f6ca3de88b09f,"A lot of people around the world commute using public transportation and would like to spend this time viewing streamed video content such as news or sports updates. However, mobile wireless networks typically suffer from severe bandwidth fluctuations, and the networks are often completely unresponsive for several seconds, sometimes minutes. Today, there are several ways of adapting the video bitrate and thus the video quality to such fluctuations, for example, using scalable video codecs or segmented adaptive HTTP streaming that switches between nonscalable video streams encoded in different bitrates. Still, for a better long-term video playout experience that avoids disruptions and frequent quality changes while using existing video adaptation technology, it is desirable to perform bandwidth prediction and planned quality adaptation. This article describes a video streaming system for receivers equipped with a GPS. A receiver's download rate is constantly monitored, and periodically reported back to a central database along with associated GPS positional data. Thus, based on the current location, a streaming device can use a GPS-based bandwidth-lookup service in order to better predict the near-future bandwidth availability and create a schedule for the video playout that takes likely future availability into account. To create a prototype and perform initial tests, we conducted several field trials while commuting using public transportation. We show how our database has been used to predict bandwidth fluctuations and network outages, and how this information helps maintain uninterrupted playback with less compromise on video quality than possible without prediction. © 2012, ACM. All rights reserved.",Adaptive streaming; bandwidth prediction; bitrate planning; Experimentation; fluctuating bandwidth; GPS; Measurement; mobile internet; Performance; wireless,
Sparse Transfer Learning for Interactive Video Search Reranking,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016672386&doi=10.1145%2f2240136.2240139&partnerID=40&md5=5057b9360a92ee96cdf3c78842263c39,"Visual reranking is effective to improve the performance of the text-based video search. However, existing reranking algorithms can only achieve limited improvement because of the well-known semantic gap between low-level visual features and highlevel semantic concepts. In this article, we adopt interactive video search reranking to bridge the semantic gap by introducing user's labeling effort. We propose a novel dimension reduction tool, termed sparse transfer learning (STL), to effectively and efficiently encode user's labeling information. STL is particularly designed for interactive video search reranking. Technically, it (a) considers the pair-wise discriminative information to maximally separate labeled query relevant samples from labeled query irrelevant ones, (b) achieves a sparse representation for the subspace to encodes user's intention by applying the elastic net penalty, and (c) propagates user's labeling information from labeled samples to unlabeled samples by using the data distribution knowledge. We conducted extensive experiments on the TRECVID 2005, 2006 and 2007 benchmark datasets and compared STL with popular dimension reduction algorithms. We report superior performance by using the proposed STL-based interactive video search reranking. © 2012, ACM. All rights reserved.",Algorithm; dimension reduction; Experimentation; Interactive video search reranking; Performance; sparsity; transfer learning,
A Joint Layered Scheme for Reliable and Secure Mobile JPEG-2000 Streaming,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885614617&doi=10.1145%2f2240136.2240143&partnerID=40&md5=912dba6793a3248b9c474fbdfefeeb3f,"This article presents a novel joint layered approach to simultaneously achieve both reliable and secure mobile JPEG-2000 image streaming. With a priori knowledge of JPEG-2000 source coding and channel coding, the proposed joint system integrates authentication into the media error protection components to ensure that every source-decodable media unit is authenticated. By such a dedicated design, the proposed scheme protects both compressed JPEG-2000 codestream and the authentication data from wireless channel impairments. It is fundamentally different from many existing systems that consider the problem of media authentication separately from the other operations in the media transmission system. By utilizing the contextual relationship, such as coding dependency and content importance between media slices for authentication hash appending, the proposed scheme generates an extremely low authentication overhead. Under this joint layered coding framework, an optimal rate allocation algorithm for source coding, channel coding, and media authentication is developed to guarantee end-to-end media quality. Experiment results on JPEG-2000 images validate the proposed scheme and demonstrate that the performance of the proposed scheme is approaching its upper bound, in which case no authentication is applied to the media stream. © 2012, ACM. All rights reserved.",Algorithms; authentication schemes; communication system security; error correction coding; mobile media communication; Multimedia authentication; Performance; Security; streaming authentication,
Automatic Evaluation of Video Summaries,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986242216&doi=10.1145%2f2240136.2240138&partnerID=40&md5=cee7cbb43e42705fdd98b1bd35b8c995,"This article describes a method for the automatic evaluation of video summaries based on the training of individual predictors for different quality measures from the TRECVid 2008 BBC Rushes Summarization Task. The obtained results demonstrate that, with a large set of evaluation data, it is possible to train fully automatic evaluation systems based on visual features automatically extracted from the summaries. The proposed approach will enable faster and easier estimation of the results of newly developed abstraction algorithms and the study of which summary characteristics influence their perceived quality. © 2012, ACM. All rights reserved.",Algorithms; Automatic evaluation; skimming; summarization; summarization algorithms evaluation,
Interactive television news,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861638400&doi=10.1145%2f2168996.2168999&partnerID=40&md5=9cbec53e4c399a69fca2ffa30387d330,"A new interactive television experience has been created for watching television news. The goal is to create a news experience that is similar to the way people watch television in their living rooms while giving viewers the power to make choices about what they see. We partnered with existing news organizations to create tools consistent with current news production practices. The viewer experience allows selection of the order of news content, skipping unwanted content and exploring stories in more depth. These tools were used to produce seven days of interactive commercial news that were viewed in ten homes. © 2012 ACM.",Interactive news; Interactive television,Digital television; Interactive news; Living room; News content; Production practice; Interactive television
Collaborative video reindexing via matrix factorization,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861607626&doi=10.1145%2f2168996.2169003&partnerID=40&md5=7a5a679aba7eef11cf6d9c44a2280255,"Concept-based video indexing generates a matrix of scores predicting the possibilities of concepts occurring in video shots. Based on the idea of collaborative filtering, this article presents unsupervised methods to refine the initial scores generated by concept classifiers by taking into account the concept-to-concept correlation and shot-to-shot similarity embedded within the score matrix. Given a noisy matrix, we refine the inaccurate scores via matrix factorization. This method is further improved by learning multiple local models and incorporating contextual-temporal structures. Experiments on the TRECVID 2006-2008 datasets demonstrate relative performance gains ranging from 13% to 52% without using any user annotations or external knowledge resources. © 2012 ACM.",Concept detection; Multimedia content analysis; Semantic video indexing; TRECVID; Unsupervised learning,Indexing (of information); Semantics; Unsupervised learning; Collaborative filtering; Concept detection; Concept-based; Data sets; External knowledge; Local model; Matrix factorizations; Multimedia content analysis; Noisy matrix; Re-indexing; Relative performance; Semantic video indexing; TRECVID; Unsupervised method; User annotations; Video indexing; Video shots; Refining
Sprite generation using sprite fusion,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861595330&doi=10.1145%2f2168996.2169002&partnerID=40&md5=1a9fb4162a2f1328839dd86e3987741c,"There has been related research for sprite or mosaic generation for over 15 years. In this article, we try to understand the methodologies for sprite generation and identify what has not actually been covered for sprite generation. We first identify issues and focus on the domain of videos for sprite generation. We introduce a novel sprite fusion method that blends two sprites. Sprite fusion method produces good results for tracking videos and does not require object segmentation. We present sample results of our experiments. © 2012 ACM.",Sprite generation; Video processing; Video standards,Computer networks; Hardware; Fusion methods; Mosaic generation; Object segmentation; Sprite generation; Video processing; Video standard; Video signal processing
Image label completion by pursuing contextual decomposability,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861612612&doi=10.1145%2f2168996.2169001&partnerID=40&md5=27f214a9a801bc98ca5efbae66105f4a,"This article investigates how to automatically complete the missing labels for the partially annotated images, without image segmentation. The label completion procedure is formulated as a nonnegative data factorization problem, to decompose the global image representations that are used for describing the entire images, for instance, various image feature descriptors, into their corresponding label representations, that are used for describing the local semantic regions within images. The solution provided in this work is motivated by following observations. First, label representations of the regions with the same label often share certain commonness, yet may be essentially different due to the large intraclass variations. Thus, each label or concept should be represented by using a subspace spanned by an ensemble of basis, instead of a single one, to characterize the intralabel diversities. Second, the subspaces for different labels are different from each other. Third, while two images are similar with each other, the corresponding label representations should be similar. We formulate this cross-image context as well as the given partial label annotations in the framework of nonnegative data factorization and then propose an efficient multiplicative nonnegative update rules to alternately optimize the subspaces and the reconstruction coefficients. We also provide the theoretic proof of algorithmic convergence and correctness. Extensive experiments over several challenging image datasets clearly demonstrate the effectiveness of our proposed solution in boosting the quality of image label completion and image annotation accuracy. Based on the same formulation, we further develop a label ranking algorithms, to refine the noised image labels without any manual supervision. We compare the proposed label ranking algorithm with the state-of-the-arts over the popular evaluation databases and achieve encouragingly improvements. © 2012 ACM.",Image annotation; Image label completion; Label ranking; Multilabel classification,Factorization; Image analysis; Semantics; Decomposability; Image annotation; Image datasets; Image features; Image representations; Intra-class variation; Multi-label; Ranking algorithm; Algorithms
REED: Optimizing First Person Shooter game server discovery using network coordinates,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861636507&doi=10.1145%2f2168996.2169000&partnerID=40&md5=ef8d293c89e14b774ec10fef2053f04d,"Online First Person Shooter (FPS) games typically use a client-server communication model, with thousands of enthusiasthosted game servers active at any time. Traditional FPS server discovery may take minutes, as clients create thousands of short-lived packet flows while probing all available servers to find a selection of game servers with tolerable round trip time (RTT). REED reduces a client's probing time and network traffic to 1% of traditional server discovery. REED game servers participate in a centralized, incremental calculation of their network coordinates, and clients use these coordinates to expedite the discovery of servers with low RTTs. © 2012 ACM.",First Person Shooter; Home networks; Internet protocol; Latency estimation; Network coordinates; Online games; Search optimization; Server discovery,Information theory; Internet protocols; Optimization; Personal communication systems; First person shooter; Home networks; Latency estimation; Network coordinates; On-line games; Search optimization; Internet
Image registration for foveated panoramic sensing,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861637282&doi=10.1145%2f2168996.2168997&partnerID=40&md5=57e025f9df01304b6cee196593904ca3,"This article addresses the problem of registering high-resolution, small field-of-view images with low-resolution panoramic images provided by a panoramic catadioptric video sensor. Such systems may find application in surveillance and telepresence systems that require a large field of view and high resolution at selected locations. Although image registration has been studied in more conventional applications, the problem of registering panoramic and conventional video has not previously been addressed, and this problem presents unique challenges due to (i) the extreme differences in resolution between the sensors (more than a 16:1 linear resolution ratio in our application), and (ii) the resolution inhomogeneity of panoramic images. The main contributions of this article are as follows. First, we introduce our foveated panoramic sensor design. Second, we show how a coarse registration can be computed from the raw images using parametric template matching techniques. Third, we propose two refinement methods allowing automatic and near real-time registration between the two image streams. The first registration method is based on matching extracted interest points using a closed form method. The second registration method is featureless and based on minimizing the intensity discrepancy allowing the direct recovery of both the geometric and the photometric transforms. Fourth, a comparison between the two registration methods is carried out, which shows that the featureless method is superior in accuracy. Registration examples using the developed methods are presented. © 2012 ACM.",Attention; Foveated sensing; Matching; Omnidirectional sensing; Registration,Mathematical transformations; Sensors; Template matching; Visual communication; Attention; Foveated sensing; Matching; Omnidirectional sensing; Registration; Image registration
Comparison of Predictive Contract Mechanisms from an information theory perspective,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861687976&doi=10.1145%2f2168996.2168998&partnerID=40&md5=3b814450d9869fa35dfb5faa0add36dd,"Inconsistency arises across a Distributed Virtual Environment due to network latency induced by state changes communications. Predictive Contract Mechanisms (PCMs) combat this problem through reducing the amount of messages transmitted in return for perceptually tolerable inconsistency. To date there are no methods to quantify the efficiency of PCMs in communicating this reduced state information. This article presents an approach derived from concepts in information theory for a deeper understanding of PCMs. Through a comparison of representative PCMs, the worked analysis illustrates interesting aspects of PCMs operation and demonstrates how they can be interpreted as a form of lossy information compression. © 2012 ACM.",Collaborative virtual environments; Consistency; Dead reckoning; Distributed interactive applications; Distributed interactive simulation; Distributed virtual environments; Networked multi-player computer games; Networked virtual environments; Predictive contract mechanisms,Adaptive filtering; Distributed computer systems; Collaborative virtual environment; Computer game; Consistency; Dead reckoning; Distributed interactive applications; Distributed interactive simulation; Distributed Virtual Environments; Networked virtual environments; Information theory
Towards an automatic music arrangement framework using score reduction,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863290889&doi=10.1145%2f2071396.2071404&partnerID=40&md5=bbad50aea1400304b38899427be0c1c4,"Score reduction is a process that arranges music for a target instrument by reducing original music. In this study we present a music arrangement framework that uses score reduction to automatically arrange music for a target instrument. The original music is first analyzed to determine the type of arrangement element of each section, then the phrases are identified and each is assigned a utility according to its type of arrangement element. For a set of utility-assigned phrases, we transform the music arrangement into an optimization problem and propose a phrase selection algorithm. The music is arranged by selecting appropriate phrases satisfying the playability constraints of a target instrument. Using the proposed framework, we implement a music arrangement system for the piano. An approach similar to Turing test is used to evaluate the quality of the music arranged by our system. The experiment results show that our system is able to create viable music for the piano. © 2012 ACM.",Automatic music arrangement; Phrase selection; Piano reduction; Score reduction,Artificial intelligence; Automatic music arrangement; Optimization problems; Phrase selection; Playability; Selection algorithm; Turing tests; Instruments
ISP-Friendly P2P live streaming: A roadmap to realization,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878326705&doi=10.1145%2f2089085.2089088&partnerID=40&md5=d91b8b98d608e052542eee9ce77e80ae,"Peer-to-Peer (P2P) applications generate large amounts of Internet network traffic. The wide-reaching connectivity of P2P systems is creating resource inefficiencies for network providers. Recent studies have demonstrated that localizing cross-ISP (Internet service provider) traffic can mitigate this challenge. However, bandwidth sensitivity and display quality requirements complicate the ISP-friendly design for live streaming systems. To this date, although some prior techniques focusing on live streaming systems exist, the correlation between traffic localization and streaming quality guarantee has not been well explored. Additionally, the proposed solutions are often not easy to apply in practice. In our presented work, we demonstrate that the cross-ISP traffic of P2P live streaming systems can be significantly reduced with little impact on the streaming quality. First, we analytically investigate and quantify the tradeoff between traffic localization and streaming quality guarantee, determining the lower bound of the inter-AS (autonomous system) streaming rate below which streaming quality cannot be preserved. Based on the analysis, we further propose a practical ISP-friendly solution, termed IFPS, which requires only minor changes to the peer selection mechanism and can easily be integrated into both new and existing systems. Additionally, the significant opportunity for localizing traffic is underscored by our collected traces from PPLive, which also enabled us to derive realistic parameters to guide our simulations. The experimental results demonstrate that IFPS reduces cross-ISP traffic from 81% up to 98% while keeping streaming quality virtually unaffected. © 2012 ACM.",ISP-friendly; P2P; Streaming; Traffic locality,Acoustic streaming; Distributed computer systems; Internet service providers; Video streaming; Autonomous systems; Existing systems; ISP-friendly; ISP-friendly P2P; Network provider; P2P; Peer-to-peer application; Traffic localities; Peer to peer networks
Providing hierarchical lookup service for P2P-VoD systems,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878281343&doi=10.1145%2f2089085.2089092&partnerID=40&md5=afcf1ddf408a5917e1760f7fc61bdbe6,"Supporting random jump in P2P-VoD systems requires efficient lookup for the ""best"" suppliers, where ""best"" means the suppliers should meet two requirements: content match and network quality match. Most studies use a DHT-based method to provide content lookup; however, these methods are neither able to meet the network quality requirements nor suitable for VoD streaming due to the large overhead. In this paper, we propose Mediacoop, a novel hierarchical lookup scheme combining both content and quality match to provide random jumps for P2P-VoD systems. It exploits the play position to efficiently locate the candidate suppliers with required data (content match), and performs refined lookup within the candidates to meet quality match. Theoretical analysis and simulation results show that Mediacoop is able to achieve lower jump latency and control overhead than the typical DHT-based method. Moreover, we implement Mediacoop in a BitTorrent-like P2P-VoD system called CoolFish and make optimizations for such ""total cache"" applications. The implementation and evaluation in CoolFish show that Mediacoop is able to improve user experiences, especially the jump latency, which verifies the practicability of our design. © 2012 ACM.",Distributed lookup; Hierarchical overlay; Peer-to-peer; Video-on-demand,Distributed computer systems; Hierarchical systems; Video on demand; Analysis and simulation; Control overhead; Hierarchical overlay; Lookup services; Lookups; Network qualities; Peer to peer; User experience; Peer to peer networks
Auction-based P2P VoD streaming: Incentives and optimal scheduling,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874024033&doi=10.1145%2f2089085.2089091&partnerID=40&md5=c5755eb4a7d4538d25a7415807750e90,"Real-world large-scale Peer-to-Peer (P2P) Video-on-Demand (VoD) streaming applications face more design challenges as compared to P2P live streaming, due to higher peer dynamics and less buffer overlap. The situation is further complicated when we consider the selfish nature of peers, who in general wish to download more and upload less, unless otherwise motivated. Taking a new perspective of distributed dynamic auctions, we design efficient P2P VoD streaming algorithms with simultaneous consideration of peer incentives and streaming optimality. In our solution, media block exchanges among peers are carried out through local auctions, in which budget-constrained peers bid for desired blocks from their neighbors, which in turn deliver blocks to the winning bidders and collect revenue. With strategic design of a discriminative second price auction with seller reservation, a supplying peer has full incentive to maximally contribute its bandwidth to increase its budget; requesting peers are also motivated to bid in such a way that optimal media block scheduling is achieved effectively in a fully decentralized fashion. Applying techniques from convex optimization and mechanism design, we prove (a) the incentive compatibility at the selling and buying peers, and (b) the optimality of the induced media block scheduling in terms of social welfare maximization. Large-scale empirical studies are conducted to investigate the behavior of the proposed auction mechanisms in dynamic P2P VoD systems based on real-world settings. © 2012 ACM.",Auction; Incentive; Optimal scheduling; Peer-to-peer streaming,Budget control; Commerce; Convex optimization; Machine design; Optimization; Scheduling; Video on demand; Video streaming; Auction; Incentive; Incentive compatibility; Optimal scheduling; Peer-to-peer streaming; Social welfare maximization; Streaming applications; Video-on-Demand (VoD); Peer to peer networks
Editorial note,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878300362&doi=10.1145%2f2089085.2089086&partnerID=40&md5=48fa825fe6f70c91fbcd3e5d1a765922,[No abstract available],,
Diagnosing network-Wide P2P live streaming inefficiencies,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878292267&doi=10.1145%2f2089085.2089090&partnerID=40&md5=56900be63dd57a043adcebc9cc6870c8,"Large-scale live peer-to-peer (P2P) streaming applications have been successfully deployed in today's Internet. While they can accommodate hundreds of thousands of users simultaneously with hundreds of channels of programming, there still commonly exist channels and times where and when the streaming quality is unsatisfactory. In this paper, based on more than two terabytes and one year worth of live traces from UUSee, a large-scale commercial P2P live streaming system, we show an in-depth network-wide diagnosis of streaming inefficiencies, commonly present in typical mesh-based P2P live streaming systems. As the first highlight of our work, we identify an evolutionary pattern of low streaming quality in the system, and the distribution of streaming inefficiencies across various streaming channels and in different geographical regions. We then carry out an extensive investigation to explore the causes to such streaming inefficiencies over different times and across different channels/regions at specific times, by investigating the impact of factors such as the number of peers, peer upload bandwidth, inter-peer bandwidth availability, server bandwidth consumption, and many more. The original discoveries we have brought forward include the two-sided effects of peer population on the streaming quality in a streaming channel, the significant impact of inter-peer bandwidth bottlenecks at peak times, and the inefficient utilization of server capacities across concurrent channels. Based on these insights, we identify problems within the existing P2P live streaming design and discuss a number of suggestions to improve real-world streaming protocols operating at a large scale. © 2012 ACM.",Peer-to-peer streaming; Streaming inefficiency,Bandwidth; Internet protocols; Video streaming; Bandwidth availability; Bandwidth bottlenecks; Bandwidth consumption; Peer-to-peer streaming; Server capacity; Streaming applications; STreaming protocols; Upload bandwidths; Peer to peer networks
Exploring interest correlation for peer-to-peer socialized video sharing,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863251877&doi=10.1145%2f2071396.2071401&partnerID=40&md5=d82aab1ed50b2ea4811da0d2b0115ff3,"The last five years have witnessed an explosion of networked video sharing, represented by YouTube, as a new killer Internet application. Their sustainable development however is severely hindered by the intrinsic limit of their client/server architecture. A shift to the peer-to-peer paradigm has been widely suggested with success already shown in live video streaming and movieon- demand. Unfortunately, our latest measurement demonstrates that short video clips exhibit drastically different statistics, which would simply render these existing solutions suboptimal, if not entirely inapplicable. Our long-term measurement over five million YouTube videos, on the other hand, reveals interesting social networks with strong correlation among the videos, thus opening new opportunities to explore. In this article, we present NetTube, a novel peerto-peer assisted delivering framework that explores the user interest correlation for short video sharing. We address a series of key design issues to realize the system, including a bi-layer overlay, an efficient indexing scheme, a delay-aware scheduling mechanism, and a prefetching strategy leveraging interest correlation. We evaluate NetTube through both simulations and prototype experiments, which show that it greatly reduces the server workload, improves the playback quality and scales well. © 2012 ACM.",Peer-to-peer; Social network; Video on demand; YouTube,Hardware; Video on demand; Bi-layer; Client/server architecture; Design issues; Indexing scheme; Internet application; Intrinsic limit; Live video streaming; Long-term measurements; Networked video; Peer to peer; Peer-to-peer paradigm; Prefetching; Prototype experiment; Scheduling mechanism; Social Networks; Strong correlation; User interests; Video clips; Video sharing; YouTube; Computer networks
Quality of data delivery in peer-to-peer video streaming,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878284005&doi=10.1145%2f2089085.2089089&partnerID=40&md5=fb5169e4537132b42313238de237e157,"QoS in a P2P video streaming system is evaluated in three stages: content generation, data delivery and video playback. We use jitter-free probability as the main performance metric to study Quality of Data delivery (QoD). A new model that incorporates both bandwidth and data availability of P2P network is proposed. Our model relies on a sharing factor that models data availability among all peers. We simulate on a minimalistic network to demonstrate how to apply the analytical model to design a P2P video streaming system with a very low jitter rate. Our simulation experimental results reveal that the lower bound on jitter-free probability is indeed effective to reflect the QoD of the entire system. Our model captures the impact of many design choices, including upload bandwidth limit, peer selection strategies, and video stream chunking schemes. © 2012 ACM.",Peer-to-peer; Quality of service; Queuing Model; Video streaming,Bandwidth; Distributed computer systems; Jitter; Quality of service; Queueing theory; Video streaming; Data availability; Peer to peer; Peer-to-peer video streaming; Performance metrices; Quality of data; Queuing models; Study qualities; Upload bandwidths; Peer to peer networks
Building an efficient transcoding overlay for P2P streaming to heterogeneous devices,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878302125&doi=10.1145%2f2089085.2089087&partnerID=40&md5=bac5554206dc0e0d78f7fcd3a33283e7,"With the increasing deployment of Internet P2P/overlay streaming systems, more and more clients use mobile devices, such as smart phones and PDAs, to access these Internet streaming services. Compared to wired desktops, mobile devices normally have a smaller screen size, a less color depth, and lower bandwidth and thus cannot correctly and effectively render and display the data streamed to desktops. To address this problem, in this paper, we propose PAT (Peer-Assisted Transcoding) to enable effective online transcoding in P2P/overlay streaming. PAT has the following unique features. First, it leverages active peer cooperation without demanding infrastructure support such as transcoding servers. Second, as online transcoding is computationally intensive while the various devices used by participating clients may have limited computing power and related resources (e.g., battery, bandwidth), an additional overlay, called metadata overlay, is constructed to instantly share the intermediate transcoding result of a transcoding procedure with other transcoding nodes to minimize the total computing overhead in the system. The experimental results collected within a realistically simulated testbed show that by consuming 6% extra bandwidth, PAT could save up to 58% CPU cycles for online transcoding. © 2012 ACM.",Heterogeneity; Meta-transcoding; P2P/overlay streaming,Bandwidth; Internet; Metadata; Mobile devices; Computing power; Heterogeneity; Heterogeneous devices; Internet streaming; Meta-transcoding; P2P streaming; Streaming systems; Unique features; Peer to peer networks
ImageSense: Towards contextual image advertising,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863283549&doi=10.1145%2f2071396.2071402&partnerID=40&md5=6845be3bd86748b70dbf5925a517cd41,"The daunting volumes of community-contributed media contents on the Internet have become one of the primary sources for online advertising. However, conventional advertising treats image and video advertising as general text advertising by displaying relevant ads based on the contents of the Web page, without considering the inherent characteristics of visual contents. This article presents a contextual advertising system driven by images, which automatically associates relevant ads with an image rather than the entire text in aWeb page and seamlessly inserts the ads in the nonintrusive areas within each individual image. The proposed system, called ImageSense, supports scalable advertising of, from root to node, Web sites, pages, and images. In ImageSense, the ads are selected based on not only textual relevance but also visual similarity, so that the ads yield contextual relevance to both the text in the Web page and the image content. The ad insertion positions are detected based on image salience, as well as face and text detection, to minimize intrusiveness to the user. We evaluate ImageSense on a large-scale real-world images and Web pages, and demonstrate the effectiveness of ImageSense for online image advertising. © 2012 ACM.",,Websites; Contextual advertisings; Image content; Inherent characteristics; Insertion position; Media content; Non-intrusive; Online advertising; Online images; Primary sources; Real-world image; Text detection; Visual content; Visual similarity; Web page; Marketing
The sweet smell of success: Enhancing multimedia applications with olfaction,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857776610&doi=10.1145%2f2071396.2071398&partnerID=40&md5=fc06d6b44c73739e862f76d3865a1640,"Olfaction, or smell, is one of the last challenges which multimedia applications have to conquer. As far as computerized smell is concerned, there are several difficulties to overcome, particularly those associated with the ambient nature of smell. In this article, we present results from an empirical study exploring users' perception of olfaction-enhanced multimedia displays. Findings show that olfaction significantly adds to the user multimedia experience. Moreover, use of olfaction leads to an increased sense of reality and relevance. Our results also show that users are tolerant of the interference and distortion effects caused by olfactory effect in multimedia. © 2012 ACM.",Human-computer interaction; Multimedia quality; Olfaction; Quality of perception,Computer networks; Hardware; Distortion effects; Empirical studies; Multimedia applications; Multimedia displays; Multimedia quality; Olfaction; Olfactory effect; Quality of perception; Users' perception; Multimedia systems
Design and evaluation of a testbed for mobile TV networks,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857769857&doi=10.1145%2f2071396.2071399&partnerID=40&md5=3335ef11b5439bad6774086bd2502fe8,"This article presents the design of a complete, open-source, testbed for broadcast networks that offer mobile TV services. Although basic architectures and protocols have been developed for such networks, detailed performance tuning and analysis are still needed, especially when these networks scale to serve many diverse TV channels to numerous subscribers. The detailed performance analysis could also motivate designing new protocols and algorithms for enhancing future mobile TV networks. Currently, many researchers evaluate the performance of mobile TV networks using simulation and/or theoretical modeling methods. These methods, while useful for early assessment, typically abstract away many necessary details of actual, fairly complex, networks. Therefore, an open-source platform for evaluating new ideas in a real mobile TV network is needed. This platform is currently not possible with commercial products, because they are sold as black boxes without the source code. In this article, we summarize our experiences in designing and implementing a testbed for mobile TV networks. We integrate off-the-shelf hardware components with carefully designed software modules to realize a scalable testbed that covers almost all aspects of real networks. We use our testbed to empirically analyze various performance aspects of mobile TV networks and validate/refute several claims made in the literature as well as discover/quantify multiple important performance tradeoffs. © 2012 ACM.",Broadcast networks; Channel switching delay; DVB-H; Energy saving; Mobile multimedia; Mobile TV; Testbed; Wireless streaming,Computer simulation; Energy conservation; Network architecture; Television broadcasting; Testbeds; Broadcast Networks; Channel switching delay; DVB-H; Mobile multimedia; Mobile TV; Digital television
Discovering multirelational structure in social media streams,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863238828&doi=10.1145%2f2071396.2071400&partnerID=40&md5=828e082c4525cbbaae886bb21cecb81a,"In this article, we present a novel algorithm to discover multirelational structures from social media streams. A media item such as a photograph exists as part of a meaningful interrelationship among several attributes, including time, visual content, users, and actions. Discovery of such relational structures enables us to understand the semantics of human activity and has applications in content organization, recommendation algorithms, and exploratory social network analysis. We are proposing a novel nonnegative matrix factorization framework to characterize relational structures of group photo streams. The factorization incorporates image content features and contextual information. The idea is to consider a cluster as having similar relational patterns; each cluster consists of photos relating to similar content or context. Relations represent different aspects of the photo stream data, including visual content, associated tags, photo owners, and post times. The extracted structures minimize the mutual information of the predicted joint distribution. We also introduce a relational modularity function to determine the structure cost penalty, and hence determine the number of clusters. Extensive experiments on a large Flickr dataset suggest that our approach is able to extract meaningful relational patterns from group photo streams. We evaluate the utility of the discovered structures through a tag prediction task and through a user study. Our results show that our method based on relational structures, outperforms baseline methods, including feature and tag frequency based techniques, by 35%-420%. We have conducted a qualitative user study to evaluate the benefits of our framework in exploring group photo streams. The study indicates that users found the extracted clustering results clearly represent major themes in a group; the clustering results not only reflect how users describe the group data but often lead the users to discover the evolution of the group activity. © 2012 ACM.",Multirelational learning; Nonnegative matrix factorization; Social media; Social network analysis; Structure mining,Clustering algorithms; Media streaming; Photography; Semantics; Social networking (online); Multirelational learning; Nonnegative matrix factorization; Social media; Social Network Analysis; Structure mining; Quality control
Fusing multiple video sensors for surveillance,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857746171&doi=10.1145%2f2071396.2071403&partnerID=40&md5=425d0429c5acd735579a199c9bb73f5e,"Real-time detection, tracking, recognition, and activity understanding of moving objects from multiple sensors represent fundamental issues to be solved in order to develop surveillance systems that are able to autonomously monitor wide and complex environments. The algorithms that are needed span therefore from image processing to event detection and behaviour understanding, and each of them requires dedicated study and research. In this context, sensor fusion plays a pivotal role in managing the information and improving system performance. Here we present a novel fusion framework for combining the data coming from multiple and possibly heterogeneous sensors observing a surveillance area. © 2012 ACM.",Multicamera video surveillance,Image processing; Monitoring; Sensors; Complex environments; Event detection; Heterogeneous sensors; Improving systems; Moving objects; Multi-camera video; Multiple sensors; Multiple video sensors; Real-time detection; Sensor fusion; Surveillance systems; Security systems
A control theoretic scheme for efficient video transmission over IEEE 802.11e EDCA WLANs,2012,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983348490&doi=10.1145%2f2240136.2240142&partnerID=40&md5=0e771e0aa858c5556008eec07317382a,"The EDCA mechanism of the IEEE 802.11 standard has been designed to support, among others, video traffic. This mechanism relies on a number of parameters whose configuration is left open by the standard. Although there are some recommended values for these parameters, they are fixed independent of the WLAN conditions, which results in suboptimal performance. Following this observation, a number of approaches in the literature have been devised to set the EDCA parameters based on an estimation of the WLAN conditions. However, these previous approaches are based on heuristics and hence do not guarantee optimized performance. In this article we propose a novel algorithm to adjust the EDCA parameters to carry video traffic which, in contrast to previous approaches, is sustained on mathematical foundations that guarantee optimal performance. In particular, our approach builds upon (i) an analytical model of the WLAN performance under video traffic, used to derive the optimal point of operation of EDCA, and (ii) a control theoretic designed mechanism which drives the WLAN to this point of operation. Via extensive simulations, we show that the proposed approach performs optimally and substantially outperforms the standard recommended configuration as well as previous adaptive proposals. © 2012 ACM.",Control theory; EDCA; IEEE 802.11; Video transmission,Control theory; Image communication systems; Optimization; Parameter estimation; Wireless local area networks (WLAN); EDCA; Extensive simulations; IEEE 802.11 standards; IEEE 802.11s; Mathematical foundations; Optimized performance; Sub-optimal performance; Video transmissions; IEEE Standards
Traffic prediction and QoS transmission of real-time live VBR videos in WLANs,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855222621&doi=10.1145%2f2043612.2043614&partnerID=40&md5=820b36cd27fb344b8647f3b5df7d2c7f,"As the demand for broadband multimedia wireless services is increasing, improving quality of service (QoS) of the widely deployed IEEE 802.11 wireless LANs (WLANs) has become crucial. To support the QoS required by a wide range of applications, the IEEE 802.11 working group has defined a new standard - the IEEE 802.11e. Substantial studies have been performed on traffic scheduling for variable bit rate (VBR) video transport over 802.11e WLANs. However, within those studies, relatively little attention has been devoted to the QoS transmission of real-time live VBR videos. In this paper, we present a novel traffic scheduling algorithm for IEEE 802.11e that aims at achieving high channel utilization while still guaranteeing QoS requirements for real-time live VBR videos. The novel characteristic of this algorithm, compared to published literatures, is that it predicts the bandwidth requirements for future traffic using a novel traffic predictor designed to provide simple yet accurate online prediction. Analyses using real life MPEG video traces indicate that the proposed traffic predictor significantly outperforms previously published technique with respect to the prediction error. The proposed traffic predictor can also be used independently to estimate any MPEG traffic. The performance of the proposed traffic scheduling algorithm is also investigated by comparing several existing scheduling algorithms. Simulation results demonstrate that the proposed traffic scheduling algorithm surpasses other mechanisms in terms of channel utilization, buffer usage, video quality and packet loss rate. © 2011 ACM.",Traffic prediction; VBR video; WLAN,Forecasting; Motion Picture Experts Group standards; Multimedia services; Quality of service; Bandwidth requirement; Broadband multimedia; Buffer usage; Channel utilization; IEEE 802.11 wireless LAN; IEEE 802.11s; IEEE802.11E; MPEG traffic; MPEG video; Online prediction; Packet loss rates; Prediction errors; QoS requirements; Traffic prediction; Traffic scheduling; Variable bit rate video; VBR video; Video quality; WLAN; Working groups; Scheduling algorithms
Beyond search: Event-driven summarization for web videos,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855251176&doi=10.1145%2f2043612.2043613&partnerID=40&md5=bfbbabd63cec37244f5659094056aeeb,"The explosive growth of Web videos brings out the challenge of how to efficiently browse hundreds or even thousands of videos at a glance. Given an event-driven query, social media Web sites usually return a large number of videos that are diverse and noisy in a ranking list. Exploring such results will be time-consuming and thus degrades user experience. This article presents a novel scheme that is able to summarize the content of video search results by mining and threading ""key"" shots, such that users can get an overview of main content of these videos at a glance. The proposed framework mainly comprises four stages. First, given an event query, a set of Web videos is collected associated with their ranking order and tags. Second, key-shots are established and ranked based on near-duplicate keyframe detection and they are threaded in a chronological order. Third, we analyze the tags associated with key-shots. Irrelevant tags are filtered out via a representativeness and descriptiveness analysis, whereas the remaining tags are propagated among key-shots by random walk. Finally, summarization is formulated as an optimization framework that compromises relevance of key-shots and user-defined skimming ratio. We provide two types of summarization: video skimming and visual-textual storyboard.We conduct user studies on twenty event queries for over hundred hours of videos crawled from YouTube. The evaluation demonstrates the feasibility and effectiveness of the proposed solution. © 2011 ACM.",Event evolution; Key-shot tagging; Key-shot threading; Web video summarization,Multimedia systems; Websites; Chronological order; Event evolution; Explosive growth; Key frames; Key-shot tagging; Key-shot threading; Optimization framework; Random Walk; Social media; User experience; User study; Video search; Video skimming; Web video; YouTube; File editors
Efficient computation of queries on feature streams,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855217373&doi=10.1145%2f2043612.2043616&partnerID=40&md5=fbb189f491bc0880c13a17fc90daf789,"This article introduces the notion of virtual feature stream, a feature stream defined from a primary data stream, in which at any time only the features that are needed to compute the queries that are currently running in the system are computed. Virtual feature streams are, in general, impossible to determine a priori, but the paper introduces an algorithm that stops the computation of features as soon as it can be proved that they are no longer needed thus generating, albeit in a roundabout and more expensive than the ideal way, a feature stream that is less expensive than the complete one to compute and safe: the queries that accept the virtual feature stream are those (and only those) that would accept the original feature stream. © 2011 ACM.",Multimedia databases; Query optimization,Hardware; Efficient computation; Multimedia database; Primary data; Query optimization; Running-in; Computer networks
Interactive films and coconstruction,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855235693&doi=10.1145%2f2043612.2043617&partnerID=40&md5=dbba099b743bfe39808e766ea8e41623,"Interactive Filmmaking is both an aesthetic and technological challenge. Steerable plots, where audiences are not passive viewers but active participants of the narrative experience, require an engaging narrative model as well as a technologically feasible structure. This article discusses the connection between aesthetics, cinema, and interactivity and presents a model for interactive narration that is based on the audience's ability to read and interpret footage differently according to its context. Through a detour narrative model it is possible to engage audiences in a coconstructive hypermedia experience while at the same time minimizing the amount of footage required. An interface model that allows seamless hypervideo navigation through graphic interaction is also discussed, and the interactive short film The Crime or Revenge of Fernando Moreno is presented, along with user experience and usability studies that experimentally prove our hypothesis. © 2011 ACM.",Coconstruction; Filmmaking; Interactive film; Storytelling,Motion pictures; Co-construction; Filmmaking; Hypermedia; Hypervideo; Interactive film; Interactivity; Interface model; Storytelling; Technological challenges; Usability studies; User experience; Interfaces (materials)
Browse by chunks: Topic mining and organizing on web-scale social media,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863609548&doi=10.1145%2f2037676.2037687&partnerID=40&md5=1cc61785c5f6c4334f6c40ec5d0317a3,"The overwhelming amount of Web videos returned from search engines makes effective browsing and search a challenging task. Rather than conventional ranked list, it becomes necessary to organize the retrieved videos in alternative ways. In this article, we explore the issue of topic mining and organizing of the retrieved web videos in semantic clusters. We present a framework for clustering-based video retrieval and build a visualization user interface. A hierarchical topic structure is exploited to encode the characteristics of the retrieved video collection and a semi-supervised hierarchical topic model is proposed to guide the topic hierarchy discovery. Carefully designed experiments on web-scale video dataset collected from video sharing websites validate the proposed method and demonstrate that clustering-based video retrieval is practical to facilitate users for effective browsing. © 2011 ACM.",Hierarchical topic model; Search result clustering; Semisupervised learning; Social media; Topic mining; Video retrieval,Search engines; Semantics; User interfaces; Visualization; Search results; Semi-supervised learning; Social media; Topic model; Video retrieval; Semantic Web
"Recognition of adult images, videos, and web page bags",2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863620291&doi=10.1145%2f2037676.2037685&partnerID=40&md5=3ab10ad11b778d7b8190bcd12f9c0088,"In this article, we develop an integrated adult-content recognition system which can detect adult images, adult videos, and adult Web page bags, where a Web page bag consists of a Web page and a predefined number of Web pages linked to it through hyperlinks. In our adult image-recognition algorithm, we model skin patches rather than skin pixels, resulting in better results than state-of-the-art algorithms which model skin pixels. In our adult video-recognition algorithm, information from the accompanying audio section around an image in an adult video is used to obtain a prior classification of the image. The algorithm achieves a better performance than the ones which use image information alone or audio information alone. The adult Web page bag recognition is carried out using multi-instance learning based on the combination of classifying texts, images and videos in Web pages. Both the speed and the accuracy for recognizing the Web adult content are increased, in contrast to recognizing Web pages one-by-one. © 2011 ACM.",Recognition of adult images; Recognition of adult videos; Recognition of adult web page bags; Skin patch modeling,Algorithms; Hypertext systems; Adult images; Audio information; Hyperlinks; Image information; Multi-instance learning; Recognition of adult videos; Recognition of adult web page bags; Recognition systems; Skin patch; State-of-the-art algorithms; Websites
Mining Flickr landmarks by modeling reconstruction sparsity,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863614070&doi=10.1145%2f2037676.2037688&partnerID=40&md5=72c6043e5c6b03fa740f6ed395341216,"In recent years, there have been ever-growing geographical tagged photos on the community Web sites such as Flickr. Discovering touristic landmarks from these photos can help us to make better sense of our visual world. In this article, we report our work on mining landmarks from geotagged Flickr photos for city scene summarization and touristic recommendations. We begin by exploring the geographical and visual statistics of the Web users' photographing manner, based on which we conduct landmark mining in two steps: First, we propose to partition each city into geographical regions based on spectral clustering over the geotags of Flickr photos. Second, in each landmark region, we present a representative photo mining scheme based on sparse representation. Our main idea is to regard the landmark mining problem as a process to find photos whose visual signatures can be reconstructed using other photos of this landmark region with a minimal coding length. This sparse reconstruction scheme offers a general perspective to mine the representative photos. Indeed, by simplifying the data correlation constraints in our scheme, several previous works in representative photo discovery and landmark mining can be derived. Finally, we introduce a Hyperlink-Induced Topic Search model to refine our landmark ranking, which incorporates the community knowledge to simulate the landmark ranking problem as a dynamic page ranking problem. We have deployed our proposed landmark mining framework on a city scene summarization and navigation system, which works on one million geotagged Flickr photos coming from twenty worldwide metropolises. We have also quantitatively compared our scheme with several state-of-the-art works. © 2011 ACM.",Image analysis; Internet; Knowledge representation; Multimedia systems,Image analysis; Internet; Knowledge representation; Multimedia systems; Navigation systems; Data correlations; Dynamic pages; Geo-tags; Mining problems; Ranking problems; Scene summarizations; Search models; Sparse reconstruction; Sparse representation; Spectral clustering; Visual signatures; Visual world; Web users; Websites
A holistic approach to aesthetic enhancement of photographs,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863612290&doi=10.1145%2f2037676.2037678&partnerID=40&md5=55bdb8094f17dc570731d31872df95b8,"This article presents an interactive application that enables users to improve the visual aesthetics of their digital photographs using several novel spatial recompositing techniques. This work differs from earlier efforts in two important aspects: (1) it focuses on both photo quality assessment and improvement in an integrated fashion, (2) it enables the user to make informed decisions about improving the composition of a photograph. The tool facilitates interactive selection of one or more than one foreground objects present in a given composition, and the system presents recommendations for where it can be relocated in a manner that optimizes a learned aesthetic metric while obeying semantic constraints. For photographic compositions that lack a distinct foreground object, the tool provides the user with crop or expansion recommendations that improve the aesthetic appeal by equalizing the distribution of visual weights between semantically different regions. The recomposition techniques presented in the article emphasize learning support vector regression models that capture visual aesthetics from user data and seek to optimize this metric iteratively to increase the image appeal. The tool demonstrates promising aesthetic assessment and enhancement results on variety of images and provides insightful directions towards future research. © 2011 ACM.",Interactive photo tools; Quality enhancement; Spatial recomposition,Semantics; Aesthetic enhancements; Digital photographs; Foreground objects; Holistic approach; Informed decision; Integrated fashion; Interactive applications; Learning support; Photo quality; Photographic composition; Quality enhancement; Semantic constraints; Spatial recomposition; User data; Photography
Using rich social media information for music recommendation via hypergraph model,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863622813&doi=10.1145%2f2037676.2037679&partnerID=40&md5=d5cf1e80d71e2391331676feae717e2c,"There are various kinds of social media information, including different types of objects and relations among these objects, in music social communities such as Last.fm and Pandora. This information is valuable for music recommendation. However, there are two main challenges to exploit this rich social media information: (a) There are many different types of objects and relations in music social communities, which makes it difficult to develop a unified framework taking into account all objects and relations. (b) In these communities, some relations are much more sophisticated than pairwise relation, and thus cannot be simply modeled by a graph. We propose a novel music recommendation algorithm by using both multiple kinds of social media information and music acoustic-based content. Instead of graph, we use hypergraph to model the various objects and relations, and consider music recommendation as a ranking problem on this hypergraph. While an edge of an ordinary graph connects only two objects, a hyperedge represents a set of objects. In this way, hypergraph can be naturally used to model high-order relations. © 2011 ACM.",Hypergraph; Music recommendation; Recommender system; Social media information,Hardware; Recommender systems; High-order; Hypergraph; Hypergraph model; Last.fm; Music recommendation; Ranking problems; Social communities; Social media; Unified framework; Computer networks
VlogSense: Conversational behavior and social attention in YouTube,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863609324&doi=10.1145%2f2037676.2037690&partnerID=40&md5=696a97b8cf59e126f84c188fb9bd3e95,"We introduce the automatic analysis of conversational vlogs (VlogSense, for short) as a new research domain in social media. Conversational vlogs are inherently multimodal, depict natural behavior, and are suitable for large-scale analysis. Given their diversity in terms of content, VlogSense requires the integration of robust methods for multimodal analysis and for social media understanding. We present an original study on the automatic characterization of vloggers' audiovisual nonverbal behavior, grounded in work from social psychology and behavioral computing. Our study on 2,269 vlogs from YouTube shows that several nonverbal cues are significantly correlated with the social attention received by videos. © 2011 ACM.",Nonverbal behavior; Social media; Vlogging; YouTube,Data mining; Websites; Automatic analysis; Large-scale analysis; Multi-modal; Multimodal analysis; Nonverbal behavior; Research domains; Robust methods; Social media; Social psychology; Vlogging; YouTube; Behavioral research
A cognitive approach for effective coding and transmission of 3D video,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863608191&doi=10.1145%2f2037676.2037680&partnerID=40&md5=e173b8a37bba7c8ca194a72c580f1e16,"Future multimedia applications will rely on the transmission of 3D video contents within heterogeneous fruition scenarios, and as a matter of fact, the reliable delivery of 3D video signals proves to be a crucial issue in such communications. To this purpose, multimedia communication experts have been designing cross-layer strategies to improve the quality of the perceived 3D experience. This article presents a new cross-layer strategy, called Cognitive Source Coding (CSC), that defines a new 3D video system able to identify the different elements of the 3D scene and choose the most appropriate coding strategy. © 2011 ACM.",3D video; Cognitive Source Coding; Cross-layer optimization; Joint source-channel coding; Source Coding,Multimedia systems; Three dimensional; 3D scenes; 3D video; Coding strategy; Cross layer optimization; Cross-layer strategy; Joint source-channel coding; Multimedia applications; Multimedia communication; Reliable delivery; Source-coding; Three dimensional computer graphics
Video accessibility enhancement for hearing-impaired users,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863615833&doi=10.1145%2f2037676.2037681&partnerID=40&md5=3a8902ec762403858916babd2983edc4,"There are more than 66 million people suffering from hearing impairment and this disability brings them difficulty in video content understanding due to the loss of audio information. If the scripts are available, captioning technology can help them in a certain degree by synchronously illustrating the scripts during the playing of videos. However, we show that the existing captioning techniques are far from satisfactory in assisting the hearing-impaired audience to enjoy videos. In this article, we introduce a scheme to enhance video accessibility using a Dynamic Captioning approach, which explores a rich set of technologies including face detection and recognition, visual saliency analysis, text-speech alignment, etc. Different from the existing methods that are categorized as static captioning, dynamic captioning puts scripts at suitable positions to help the hearing-impaired audience better recognize the speaking characters. In addition, it progressively highlights the scripts word-by-word via aligning them with the speech signal and illustrates the variation of voice volume. In this way, the special audience can better track the scripts and perceive the moods that are conveyed by the variation of volume. We implemented the technology on 20 video clips and conducted an in-depth study with 60 real hearing-impaired users. The results demonstrated the effectiveness and usefulness of the video accessibility enhancement scheme. © 2011 ACM.",Accessibility; Dynamic Captioning; Hearing impairment,Character recognition; Accessibility; Audio information; Face detection and recognition; Hearing impairments; Hearing-impaired users; In-depth study; Speech signals; Video clips; Video contents; Visual saliency; Audition
Introduction to special issue on social media,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863618560&doi=10.1145%2f2037676.2037682&partnerID=40&md5=2a6fac5bf88a1052fc6d7273c3af9fc7,[No abstract available],,
Contextual tag inference,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859446368&doi=10.1145%2f2037676.2037689&partnerID=40&md5=b1171292fa4ce6305279a67c73b835a5,"This article examines the use of two kinds of context to improve the results of content-based music taggers: the relationships between tags and between the clips of songs that are tagged. We show that users agree more on tags applied to clips temporally ""closer"" to one another; that conditional restricted Boltzmann machine models of tags can more accurately predict related tags when they take context into account; and that when training data is ""smoothed"" using context, support vector machines can better rank these clips according to the original, unsmoothed tags and do this more accurately than three standard multi-label classifiers. © 2011 ACM.",Autotagging; Clips; Context; Music; Smoothing; Tags,Computer networks; Hardware; Autotagging; Clips; Context; Music; Smoothing; Tags; Classification (of information)
Exploiting online music tags for music emotion classification,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863610435&doi=10.1145%2f2037676.2037683&partnerID=40&md5=17570a017457da2c2dd6d67cad9ca02f,"The online repository of music tags provides a rich source of semantic descriptions useful for training emotion-based music classifier. However, the imbalance of the online tags affects the performance of emotion classification. In this paper, we present a novel data-sampling method that eliminates the imbalance but still takes the prior probability of each emotion class into account. In addition, a two-layer emotion classification structure is proposed to harness the genre information available in the online repository of music tags. We show that genre-based grouping as a precursor greatly improves the performance of emotion classification. On the average, the incorporation of online genre tags improves the performance of emotion classification by a factor of 55% over the conventional single-layer system. The performance of our algorithm for classifying 183 emotion classes reaches 0.36 in example-based f-score. © 2011 ACM.",Class imbalance; Multi-label classification; Music emotion classification; Music genre; Online music tags; Social media,Semantics; Class imbalance; Emotion classification; Multi-label; Music genre; Online music; Social media; Classification (of information)
Automatic creation of photo books from stories in social media,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863615348&doi=10.1145%2f2037676.2037684&partnerID=40&md5=092cbfdf27320254944a11c934328bfb,"Photos are a special way to tell stories of our best memories and moments. The representation of those photos in appealing physical photo books is highly appreciated by many people. Today, many photos are shared via social networking sites, where people upload their photos and share their stories with their friends. The members of social networks comment on each other's photos, add tags or descriptions and upload new photos of the same events to their albums. While the media of different personal events are available on the social network, there is no easy way to collect and bundle them into a story and print this story as a photo book. We propose an approach to automatically detect media elements that match a query (where, when, what, who) in the user's social network and intelligently arrange and compose them into a printable photo book. We combine content analysis of text and images to automatically and semi-automatically select photos of a specific story. We calculate the probabilities of each two photos to belong to the same event using an Expectation-Maximization algorithm that we propose in order to be able to retrieve them easily when receiving the user queries, and we address the differences between our model and other models that use similar proposed algorithms. People's tags and the interaction between the users and the photos as well as other semantic information are exploited to select important photos that are suitable to create the photo book. The selected photos and derived semantics are then employed to automatically create an appealing layout for the photo book. © 2011 ACM.",Expectation-Maximization; Information retrieval; Layout; Multimedia retrieval; Photo books; Social media; Social networks,Algorithms; Information retrieval; Semantics; Expectation Maximization; Layout; Multimedia Retrieval; Photo books; Social media; Social Networks; Social networking (online)
Introduction to ACM multimedia 2010 best paper candidates,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863624864&doi=10.1145%2f2037676.2037677&partnerID=40&md5=512813b4cfa21c8f1681c938a434b814,[No abstract available],,
SCENT: Scalable compressed monitoring of evolving multirelational social networks,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863634824&doi=10.1145%2f2037676.2037686&partnerID=40&md5=1cd6775477ef764cb4204f78c663ccd9,"We propose SCENT, an innovative, scalable spectral analysis framework for internet scale monitoring of multirelational social media data, encoded in the form of tensor streams. In particular, a significant challenge is to detect key changes in the social media data, which could reflect important events in the real world, sufficiently quickly. Social media data have three challenging characteristics. First, data sizes are enormous; recent technological advances allow hundreds of millions of users to create and share content within online social networks. Second, social data are often multifaceted (i.e., have many dimensions of potential interest, from the textual content to user metadata). Finally, the data is dynamic; structural changes can occur at multiple time scales and be localized to a subset of users. Consequently, a framework for extracting useful information from social media data needs to scale with data volume, and also with the number and diversity of the facets of the data. In SCENT, we focus on the computational cost of structural change detection in tensor streams. We extend compressed sensing (CS) to tensor data. We show that, through the use of randomized tensor ensembles, SCENT is able to encode the observed tensor streams in the form of compact descriptors. We show that the descriptors allow very fast detection of significant spectral changes in the tensor stream, which also reduce data collection, storage, and processing costs. Experiments over synthetic and real data show that SCENT is faster (17.7x-159x for change detection) and more accurate (above 0.9 F-score) than baseline methods. © 2011 ACM.",Multirelational learning; Social media; Social network analysis; Stream mining; Tensor analysis,Metadata; Signal detection; Social networking (online); Spectrum analysis; Multirelational learning; Social media; Social Network Analysis; Stream mining; Tensor analysis; Tensors
Game-on-demand: An online game engine based on geometry streaming,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053001356&doi=10.1145%2f2000486.2000493&partnerID=40&md5=572104b9882866707a9fe3f97d5576d0,"In recent years, online gaming has become very popular. In contrast to stand-alone games, online games tend to be large-scale and typically support interactions among users. However, due to the high network latency of the Internet, smooth interactions among the users are often difficult. The huge and dynamic geometry data sets also make it difficult for some machines, such as handheld devices, to run those games. These constraints have stimulated some research interests on online gaming, which may be broadly categorized into two areas: technological support and user-perceived visual quality. Technological support concerns the performance issues while user-perceived visual quality concerns the presentation quality and accuracy of the game. In this article, we propose a game-on-demand engine that addresses both research areas. The engine distributes game content progressively to each client based on the player's location in the game scene. It comprises a two-level content management scheme and a prioritized content delivery scheme to help identify and deliver relevant game content at appropriate quality to each client dynamically. To improve the effectiveness of the prioritized content delivery scheme, it also includes a synchronization scheme to minimize the location discrepancy of avatars (game players). We demonstrate the performance of the proposed engine through numerous experiments. © 2011 ACM.",Design; Performance,Design; Knowledge engineering; Online systems; Content delivery; Content management; Data sets; Dynamic geometry; Game players; Hand held device; Network latencies; On-line games; On-line gaming; Performance; Performance issues; Research areas; Support interaction; Synchronization scheme; Technological supports; Visual qualities; Internet
Protecting the content integrity of digital imagery with fidelity preservation,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053006913&doi=10.1145%2f2000486.2000489&partnerID=40&md5=42a91da268948509535782a287a33989,"Fragile watermarking is applied to protect the content integrity of digital images. The main concerns related to watermarking include retaining the quality of the watermarked image and retaining the ability to detect whether any manipulation has occurred. Because recent watermarking techniques seriously distort the quality of the protected image after embedding the authentication code into the image content, attention has been drawn to how to satisfy both the need for image fidelity and detection ability. To account for the influence from both essentials, a novel algorithm is proposed in this article. The new scheme utilizes a weighted-sum function to embed (n+1) authentication bits into a block with 2n pixels by modifying only one original pixel with (±1). With fewer authentication codes, the new process can protect the content of the image. The experimental results demonstrate that the approach can guarantee the fidelity of the watermarked image while retaining tamper-proof functionality. © 2011 ACM.",Security; Verification,Algorithms; Authentication; Pixels; Verification; Authentication bits; Authentication codes; Content integrity; Detection ability; Digital image; Digital imagery; Fragile watermarking; Image content; Image fidelity; Novel algorithm; Security; Tamperproof; Watermarked images; Watermarking techniques; Weighted-sum; Watermarking
Selecting vantage objects for similarity indexing,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052972306&doi=10.1145%2f2000486.2000490&partnerID=40&md5=b3a49ded75f955f540b4dba44f1ebf0d,"Indexing has become a key element in the pipeline of a multimedia retrieval system, due to continuous increases in database size, data complexity, and complexity of similarity measures. The primary goal of any indexing algorithm is to overcome high computational costs involved with comparing the query to every object in the database. This is achieved by efficient pruning in order to select only a small set of candidate matches. Vantage indexing is an indexing technique that belongs to the category of embedding or mapping approaches, because it maps a dissimilarity space onto a vector space such that traditional access methods can be used for querying. Each object is represented by a vector of dissimilarities to a small set of m reference objects, called vantage objects. Querying takes place within this vector space. The retrieval performance of a system based on this technique can be improved significantly through a proper choice of vantage objects. We propose a new technique for selecting vantage objects that addresses the retrieval performance directly, and present extensive experimental results based on three data sets of different size and modality, including a comparison with other selection strategies. The results clearly demonstrate both the efficacy and scalability of the proposed approach. © 2011 ACM.",Algorithms; Experimentation; Performance,Algorithms; Indexing (of information); Information retrieval; Vector spaces; Access methods; Computational costs; Data complexity; Data sets; Database size; Different sizes; Experimentation; Indexing algorithms; Indexing techniques; Key elements; Multimedia Retrieval; O-r mappings; Performance; Reference objects; Retrieval performance; Similarity measure; System-based; Search engines
"Video quality for face detection, recognition, and tracking",2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053024796&doi=10.1145%2f2000486.2000488&partnerID=40&md5=133f14c241f405b42632dfe6648cf2d2,"Many distributed multimedia applications rely on video analysis algorithms for automated video and image processing. Little is known, however, about the minimum video quality required to ensure an accurate performance of these algorithms. In an attempt to understand these requirements, we focus on a set of commonly used face analysis algorithms. Using standard datasets and live videos, we conducted experiments demonstrating that the algorithms show almost no decrease in accuracy until the input video is reduced to a certain critical quality, which amounts to significantly lower bitrate compared to the quality commonly acceptable for human vision. Since computer vision percepts video differently than human vision, existing video quality metrics, designed for human perception, cannot be used to reason about the effects of video quality reduction on accuracy of video analysis algorithms. We therefore investigate two alternate video quality metrics, blockiness and mutual information, and show how they can be used to estimate the critical video qualities for face analysis algorithms. © 2011 ACM.",Experimentation; Measurement; Performance,Algorithms; Computer vision; Experiments; Face recognition; Measurements; Accurate performance; Automated video; Bit rates; Blockiness; Critical quality; Data sets; Distributed multimedia; Experimentation; Face analysis; Human perception; Human vision; Input videos; Live video; Mutual informations; Performance; Video analysis; Video quality; Quality control
Derivative-based audio steganalysis,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053029734&doi=10.1145%2f2000486.2000492&partnerID=40&md5=b28bcc954b8660e727e75608e78dba5c,"This article presents a second-order derivative-based audio steganalysis. First, Mel-cepstrum coefficients and Markov transition features from the second-order derivative of the audio signal are extracted; a support vector machine is then applied to the features for discovering the existence of hidden data in digital audio streams. Also, the relation between audio signal complexity and steganography detection accuracy, which is an issue relevant to audio steganalysis performance evaluation but so far has not been explored, is analyzed experimentally. Results demonstrate that, in comparison with a recently proposed signal stream-based Mel-cepstrum method, the second-order derivative-based audio steganalysis method gains a considerable advantage under all categories of signal complexity-especially for audio streams with high signal complexity, which are generally the most challenging for steganalysis-and thereby significantly improves the state of the art in audio steganalysis. © 2011 ACM.",Algorithms; Design; Security,Algorithms; Design; Audio signal; Audio steganalysis; Audio stream; Detection accuracy; Digital audio; Performance evaluation; Second order derivatives; Second orders; Security; Signal complexity; State of the art; Transition features; Steganography
Editorial notice,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052992023&doi=10.1145%2f2000486.2000487&partnerID=40&md5=b5f9c83ee933e834efcc7a2ff2186679,[No abstract available],,
Supporting region-of-interest cropping through constrained compression,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052999699&doi=10.1145%2f2000486.2000491&partnerID=40&md5=399e8f125e325d9a675a661f3ec4d51c,"The ability to create super high-resolution video is becoming relative easy to do either through a single high-definition video camera or panoramic video that automatically stitches multiple views together. As an example of the former, the motion picture industry now has 6000 × 4000 pixel full-rate video cameras available. This means that supporting region-of-interest cropping will become more important in the future. In this article, we propose a mechanism to support region-of-interest adaptation of stored video. The proposed approach creates a compression-compliant stream (e.g., MPEG-2), while still allowing it to be cropped. Fortunately, video standards like MPEG-2 specify the format of a compliant stream, and not the algorithm to get there. As a result, there is an opportunity to allow system researchers and implementers ways to optimize for applications. We show various fundamental tradeoffs that are made in order to support region-of-interest cropping with super high-resolution video which we received from a local motion-picture firm. © 2011 ACM.",Algorithms; Design; Experimentation; Human factors; Management; Measurement; Performance; Standardization,Algorithms; Cameras; Design; Human engineering; Management; Measurements; Motion Picture Experts Group standards; Motion pictures; Standardization; Experimentation; High resolution; High-definition videos; Multiple views; Panoramic video; Performance; Region of interest; Video standard; Video cameras
Knowledge discovery from 3D human motion streams through semantic dimensional reduction,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952505115&doi=10.1145%2f1925101.1925104&partnerID=40&md5=7dc50cd9c7391f9ef053c431d89b33ad,"3D human motion capture is a form of multimedia data that is widely used in entertainment as well as medical fields (such as orthopedics, physical medicine, and rehabilitation where gait analysis is needed). These applications typically create large repositories of motion capture data and need efficient and accurate content-based retrieval techniques. 3D motion capture data is in the form of multidimensional time-series data. To reduce the dimensions of human motion data while maintaining semantically important features, we quantize human motion data by extracting spatio-temporal features through SVD and translate them onto a symbolic sequential representation through our proposed sGMMEM (semantic Gaussian Mixture Modeling with EM). In order to handle variations in motion capture data due to human body characteristics and speed of motion, we transform the semantically quantized values into a histogram representation. This representation is used as a signature for classification and similarity-based retrieval.We achieved good classification accuracies for ""coarse"" human motion categories (such as walking 92.85%, run 91.42%, and jump 94.11%) and even for subtle categories (such as dance 89.47%, laugh 83.33%, basketball signal 85.71%, golf putting 80.00%). Experiments also demonstrated that the proposed approach outperforms earlier techniques such as the wMSV (weighted Motion Singular Vector) approach and LB Keogh method. © 2011 ACM.",3D motion capture; EM; Gaussian mixture model; Singular value decomposition; String matching,Gaussian distribution; Image segmentation; Medicine; Metadata; Semantics; Singular value decomposition; 3D motion capture; EM; Gaussian mixture model; Singular values; String matching; Three dimensional
A fuzzy algorithm for dynamically adaptive multimedia streaming,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952531920&doi=10.1145%2f1925101.1925106&partnerID=40&md5=14325aaab787c5a34cc2a32a22fb2900,"The QoS-aware delivery model of multimedia is an interesting research area. The wireless networking systems connecting mobile clients and media servers have created the paradigm of mobile multimedia. In mobile multimedia systems, the media delivery model has to maintain two diagonally opposite objectives, such as maintaining QoS of playback and saving energy consumption of the mobile devices. The traditional pull, push, and the hybrid push-pull models of media delivery are not completely suitable to offer consistent QoS of playback while saving the energy consumptions at mobile devices. This article proposes a novel multimedia delivery system based on the Fuzzy Adaptive Buffering (FAB) algorithm using pull model. The FAB algorithm employs a fuzzy inference technique and dynamically adapts to the execution environments. The experimental results illustrate that the FAB algorithm successfully adapts to dynamic execution contexts while maintaining playback-QoS and saving the energy consumption of the mobile clients by keeping the data prefetching thread in the sleeping mode from 31.44% to 97.4% of streaming time depending on the execution environments. © 2011 ACM.",Buffering; Data prefetch; Fuzzy logic; Multimedia streaming; Multithread; QoS; VoD,Adaptive algorithms; Energy utilization; Fuzzy inference; Fuzzy sets; Media streaming; Mobile devices; Multimedia systems; Portable equipment; Video on demand; Wireless networks; Buffering; Data prefetch; Multi-thread; Multimedia streaming; QoS; VoD; Mathematical models
Optimal layered multicast,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952518426&doi=10.1145%2f1925101.1925102&partnerID=40&md5=2d66d9fe71b3ca2f6fb00ed7504c9510,"Recent advances in network coding research dramatically changed the underlying structure of optimal multicast routing algorithms and made them efficiently computable. While most such algorithm design assumes a single file/layer being multicast, layered coding introduces new challenges into the paradigm due to its cumulative decoding nature. Layered coding is designed to handle heterogeneity in receiver capacities, and a node may decode layer k only if it successfully receives all layers in 1..k. We show that recently proposed optimization models for layered multicast do not correctly address this challenge. We argue that in order to achieve the absolute maximum throughput (or minimum cost), it is necessary to decouple the application-layer throughput from network-layer throughput. In particular, a node should be able to receive a nonconsecutive layer or a partial layer even if it cannot decode and utilize it (e.g., for playback in media streaming applications). The rationale is that nodes at critical network locations need to receive data just for helping other peers. We present a mathematical programming model that addresses these challenges and achieves absolute optimal performance. Simulation results show considerable throughput gain (cost reduction) compared with previous models, in a broad range of network scenarios. We then provide a formal proof that the layered multicast problem is NP-complete. We design a randomized rounding algorithm to approximate the optimal layered multicast, and show the efficacy of our technique using simulations. We then proceed to further generalize our model by studying the optimal progression of layer sizes. We show that such optimization is nonconvex, and apply a simulated annealing algorithm to solve it, with flexible trade-off between solution quality and running time. We verify the effectiveness of the new model and the simulated annealing algorithm through extensive simulations, and point out insights on the connection between optimal layer size progression and node capacity distribution. © 2011 ACM.",Multicast routing; Network coding; Optimization,Computer simulation; Cost reduction; Encoding (symbols); Fiber optic networks; Information theory; Mathematical programming; Media streaming; Multicasting; Network layers; Peer to peer networks; Routing algorithms; Simulated annealing; Structural optimization; Throughput; Algorithm design; Extensive simulations; Formal proofs; Layered coding; Layered multicasts; Mathematical programming models; Maximum through-put; Minimum cost; Multicast routing; Multicast routing algorithms; Multicasts; Network location; Network scenario; New model; Node capacity; Nonconvex; NP Complete; Optimal performance; Optimization models; Randomized rounding; Running time; Simulated annealing algorithms; Simulation result; Solution quality; Streaming applications; Network coding
Modeling progressive mesh streaming: Does data dependency matter?,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952522227&doi=10.1145%2f1925101.1925105&partnerID=40&md5=545e7a378410ef283a1cf054faa44df8,"3D triangular meshes are becoming an increasingly prevalent data type in networked applications such as digital museums, online games, and virtual worlds. In these applications, a 3D mesh is typically coded progressively, yielding a multiresolution representation suitable for streaming. While such progressive coding allows incremental rendering for users while data is being transmitted, it introduces dependencies between data, causing delay in rendering when packets are lost. This article quantitatively analyzes the effects of such dependency by modeling the distribution of decoding time as a function of mesh properties and network parameters. We apply our model to study two extreme cases of dependency in progressive meshes and show that the effect of dependencies on decoded mesh quality diminishes with time. Our model provides the expected decoded mesh quality at the receiver at a given time. Based on this expected value, we propose a packetization strategy that improves the decoded mesh quality during the initial stage of streaming. We validate the accuracy of our model under a variety of network conditions, including bursty losses, fluctuating RTT, and varying sending rate. The values predicted from our model match the measured value reasonably well in all cases except when losses are too bursty. © 2011 ACM.",3D data; Packetization; Progressive meshes; Streaming,Virtual reality; 3D data; 3D meshes; Bursty loss; Data dependencies; Data type; Digital museums; Expected values; Extreme case; Initial stages; Mesh quality; Multi resolution representation; Network condition; Network parameters; Networked applications; On-line games; Packetization; Progressive Coding; Progressive Mesh; Progressive meshes; Sending rate; Streaming; Triangular meshes; Virtual worlds; Three dimensional
Using simulcast and scalable video coding to efficiently control channel switching delay in mobile tv broadcast networks,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952520436&doi=10.1145%2f1925101.1925103&partnerID=40&md5=24fdafc26409537debee1c5001f411bb,"Many mobile TV standards dictate using energy saving schemes to increase the viewing time on mobile devices, since mobile receivers are battery powered. The most common scheme for saving energy is to make the base station broadcast the video data of a TV channel in bursts with a bit-rate much higher than the encoding rate of the video stream, which enables mobile devices to turn off their radio frequency circuits when not receiving bursts. Broadcasting TV channels in bursts, however, increases channel switching delay. The switching delay is important, because long and variable switching delays are annoying to users and may turn them away from the mobile TV service. In this article, we first analyze the burst broadcasting scheme currently used in many deployed mobile TV networks, and we show that it is not efficient in terms of controlling the channel switching delay. We then propose new schemes to guarantee that a given maximum switching delay is not exceeded and that the energy consumption of mobile devices is minimized. We prove the correctness of the proposed schemes and analytically analyze the achieved energy saving. We also use scalable video coding to generalize the proposed schemes in order to support mobile devices with heterogeneous resources. We implement the proposed schemes in a mobile TV testbed to show their practicality and to validate our theoretical analysis. The experimental results show that the proposed schemes: (i) significantly increase the energy saving achieved on mobile devices: up to 95% saving is observed, and (ii) support both homogeneous and heterogeneous mobile devices. © 2011 ACM.",Broadcast networks; Channel switching delay; DVB-H; Energy saving; Mobile TV; Scalable video coding; Video simulcast,Digital television; Energy conservation; Energy utilization; Mobile devices; Mobile telecommunication systems; Portable equipment; Radio broadcasting; Switching; Television broadcasting; Broadcast Networks; Channel switching; DVB-H; Energy saving; Mobile TV; Scalable video coding; Video simulcast; Wireless networks
Statistical multiplexing of variable-bit-rate videos streamed to mobile devices,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952526225&doi=10.1145%2f1925101.1925107&partnerID=40&md5=cf538f8fbff882025ae3b8b1b7b697c5,"We address the problem of broadcasting multiple video streams over a broadcast network to many mobile devices, so that: (i) streaming quality of mobile devices is maximized, (ii) energy consumption of mobile devices is minimized, and (iii) goodput in the network is maximized. We consider two types of broadcast networks: closed-loop networks, in which all video streams are jointly encoded to ensure their total bit rate does not exceed the broadcast network bandwidth, and open-loop networks, in which videos are encoded using standalone coders, and thus must be carefully broadcast to avoid playout glitches. We first show that the problem of optimally broadcasting multiple videos is NP-complete. We then propose an approximation algorithm to construct burst schedules for multiple VBR (Variable-Bit-Rate) streams. The proposed algorithm frees network operators from the manual and error-prone bandwidth reservation process which is currently used in practice. We prove that the proposed algorithm achieves optimal goodput and near-optimal energy saving. We show that it produces glitch-free schedules in closedloop networks, and it minimizes number of glitches in open-loop networks. We implement the proposed algorithm in a tracedriven simulator, and conduct extensive simulations for both open- and closed-loop networks. The simulation results show that the proposed algorithm outperforms the existing algorithms in many aspects, including number of late frames, number of concurrently broadcast video streams, and energy saving of mobile devices. To show the practicality and efficiency of the proposed algorithm, we also implement it in a real mobile TV testbed as a proof of concept. The results from the testbed confirm that the proposed algorithm: (i) does not result in playout glitches, (ii) achieves high energy saving, and (iii) runs in real time. © 2011 ACM.",Broadcast networks; DVB-H; Energy saving; Goodput; Mobile TV; Variable-bit-rate streams,Approximation algorithms; Bandwidth; Computer simulation; Energy conservation; Energy utilization; Mathematical operators; Mobile devices; Mobile telecommunication systems; Optimization; Portable equipment; Television broadcasting; Test facilities; Testbeds; Video streaming; Broadcast Networks; DVB-H; Energy saving; Good put; Mobile TV; Variable-bit-rate streams; Digital television
Multigranularity reuse of learning resources,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-79551674145&doi=10.1145%2f1870121.1870122&partnerID=40&md5=40b3c1fdaecda4cf79ea481f8d368da0,"This article investigates a scenario of reuse in which existing learning resources serve as preliminary products for the creation of new learning resources. Authors should be able to reuse learning resources and also parts of them at different levels of granularity in a modular way. The requirements of multigranularity reuse are analyzed and compared to existing solutions. A concept for modular, multigranularity reuse is presented in this article. It is also shown how this kind of reuse can be achieved in practise.© 2011 ACM.",E-learning; Granularity; Learning resources; Reusability; Reuse,Reusability; Granularity; Learning resource; Learning resources; Multi-granularity; Reuse; E-learning
SMIL builder: An incremental authoring tool for SMIL documents,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-79551672997&doi=10.1145%2f1870121.1870123&partnerID=40&md5=15273513377b535f6a5c009a4f042afc,"We present in this article a temporal SMIL editor with incremental verification capabilities, based on a formal Petri Net-based model. Our authoring tool, named SMIL Builder, allows the author to ""build"" his document step by step, while insuring at every stage the validity of the current state of the document. These incremental authoring and consistency checking features are based on the H-SMIL-Net model (Hierarchical SMIL Petri Net), a temporal extension of Petri Nets. Our aim is to propose an easy-to-use temporal environment which can satisfy a wide range of users; so we opted for an interface combining simplicity and ergonomics.© 2011 ACM.",Authoring; Modeling; Multimedia; Petri Nets; SMIL; Verification,Ergonomics; Graph theory; Model checking; Authoring; Authoring tool; Consistency checking; Incremental authoring; Incremental verification; Modeling; Multimedia; Net model; SMIL; SMIL documents; Step-by-step; Temporal extensions; Petri nets
Call for Papers,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024247888&doi=10.1145%2f2043612.2043619&partnerID=40&md5=49b6c4a0553ac99a2f235138f65190fa,[No abstract available],,
An empirical analysis of serendipitous media sharing among campus-wide wireless users,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-79551671927&doi=10.1145%2f1870121.1870127&partnerID=40&md5=8543994f70b2b38100018a0d92904fb5,"Contemporary systems use centralized as well as peer-to-peer mechanisms for the large scale distribution of media objects. In this work, we investigate a serendipitous mechanism for directly sharing media objects among a local community of wireless users. This localized sharing is attractive when wide area network connectivity is undesirable, expensive or unavailable; especially when the shared media objects are large. With some restrictions, such localized sharing of media objects is also acceptable to content owners. However, localized sharing has to contend with far fewer media providers who may also not offer the variety of objects available from wide-area services. We collected empirical data from the widely deployed Apple iTunes application for our analysis. We showed that users are already making a significant amount of media objects available for serendipitous sharing. Our analysis showed that the shared object annotations exhibited a Zipfian long tail distribution. The availability patterns of wireless iTunes users and the object annotations makes serendipitous sharing inappropriate for scenarios that require access to a specific object. Instead, mechanisms that allow the user to specify classes of interesting objects are better suited for such users. Also, given the smaller scale of these systems, serendipitous sharing can benefit from approaches that allow users to disseminate a compact representation of their shared objects. Though the wireless user availability rates was not as high as what was observed in a corporate desktop setting, a large fraction of the users showed high temporal consistency. This allows for high availability with reasonable replication during weekday daytime hours. We answer important questions regarding the viability of a campus-wide media sharing system.© 2011 ACM.",Local media sharing; Peer-to-peer,Wide area networks; Compact representation; Empirical analysis; Empirical data; High availability; Large-scale distribution; Local community; Local media sharing; Long-tail distribution; Media objects; Peer to peer; Shared objects; Sharing systems; Temporal consistency; Wide area; Wireless users; Peer to peer networks
Beat space segmentation and octave scale cepstral feature for sung language recognition in pop music,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855247962&doi=10.1145%2f2043612.2043615&partnerID=40&md5=919717f9fdebbe07458e4afb243e8af1,"Sung language recognition relies on both effective feature extraction and acoustic modeling. In this paper, we study rhythm based music segmentation with the frame size being the duration of the smallest note in the music, as opposed to fixed length segmentation in spoken language recognition. It is found that acoustic features extracted from the rhythm based segmentation scheme outperform those from fixed length segmentation. We also study the effectiveness of a musically motivated acoustic feature. Octave scale cepstral coefficients (OSCCs) by comparing with the other acoustic features: Log frequency cepstral coefficients, Linear prediction coefficients (LPC) and LPC-derived cepstral coefficients. Finally, we examine the modeling capabilities of Gaussian mixture models and support vector machines in sung language recognition experiments. Experiments conducted on a corpus of 400 popular songs sung in English, Chinese, German, and Indonesian, showed that the OSCC feature outperforms other features. A sung language recognition accuracy of 64.9% was achieved when Gaussian mixture models were trained on shifted-delta-OSCC acoustic features, extracted via rhythm based music segmentation. © 2011 ACM.",Feature extraction; Language characterization; Sung language recognition,Extraction; Feature extraction; Speech recognition; Cepstral coefficients; Gaussian Mixture Model; Language characterizations; Linear prediction coefficients; Music segmentations; Segmentation scheme; Spoken language recognition; Sung language recognition; Modeling languages
Domical Cooperative Caching for Streaming Media in Wireless Home Networks,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908146415&doi=10.1145%2f2043612.2043618&partnerID=40&md5=b66aca972edb27e293a2b93640ddfce6,"Wireless home networks are widely deployed due to their low cost, ease of installation, and plug-and-play capabilities with consumer electronic devices. A challenge of these environments is how to manage data across devices. This is specially true for continuous media (audio and video clips) which are large in size and delay sensitive. Caching of clips across wireless devices may improve user experience, measurable by different Quality of Service (QoS) metrics such as throughput and startup latency. Moreover, caching at the edge of the network reduces the demand for the infrastructure outside the home. In this study, we present Domical, a novel cooperative caching technique designed for streaming media in wireless home networks consisting of a handful of devices. Domical is novel because it considers both asymmetry of the available wireless link bandwidth and heterogeneity of available cache space. We provide a comprehensive description of Domical, presenting its key knobs, and the behavior of the algorithm with different granularity of data caching (block versus clip). © 2011, ACM. All rights reserved.",Algorithms; Cooperative caching; Design; Experimentation; Performance; Streaming media; Wireless home networks,
A framework for cross-layer optimization of video streaming in wireless networks,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-79551668695&doi=10.1145%2f1870121.1870126&partnerID=40&md5=57538b9c30e79bfefaac2c486d1e1a8a,"We present a general framework for optimizing the quality of video streaming in wireless networks that are composed of multiple wireless stations. The framework is general because: (I) it can be applied to different wireless networks, such as IEEE 802.11e WLAN and IEEE 802.16 WiMAX, (II) it can employ different objective functions for the optimization, and (III) it can adopt various models for the wireless channel, the link layer, and the distortion of the video streams in the application layer. The optimization framework controls parameters in different layers to optimally allocate the wireless network resources among all stations. More specifically, we address this video optimization problem in two steps. First, we formulate an abstract optimization problem for video streaming in wireless networks in general. This formulation exposes the important interaction between parameters belonging to different layers in the network stack. Then, we instantiate and solve the general problem for the recent IEEE 802.11e WLANs, which support prioritized traffic classes. We show how the calculated optimal solutions can efficiently be implemented in the distributed mode of the IEEE 802.11e standard. We evaluate our proposed solution using extensive simulations in the OPNET simulator, which captures most features of realistic wireless networks. In addition, to show the practicality of our solution, we have implemented it in the driver of an off-the-shelf wireless adapter that complies with the IEEE 802.11e standard. Our experimental and simulation results show that significant quality improvement in video streams can be achieved using our solution, without incurring any significant communication or computational overhead. We also explain how the general video optimization problem can be applied to other wireless networks, in particular, to the IEEE 802.16 WiMAX networks, which are becoming very popular.© 2011 ACM.",Cross layer design; Effective airtime; Video optimization; WiMAX; Wireless networks; WLAN,Acoustic streaming; Computer simulation; Network layers; Optimization; Standards; Video streaming; Videotex; Wimax; Application layers; Computational overheads; Cross layer optimization; Cross-layer design; Distributed mode; Effective airtime; Extensive simulations; IEEE 802.16; IEEE802.11E; Link layers; Network resource; Network stack; Objective functions; OPNET simulator; Optimal solutions; Optimization framework; Optimization problems; Quality improvement; Simulation result; Traffic class; Video optimization; Video streams; WiMAX networks; Wireless channel; Wireless stations; WLAN; Wireless networks
Modeling and assessing quality of information in multisensor multimedia monitoring systems,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049462391&doi=10.1145%2f1870121.1870124&partnerID=40&md5=df4af448e85684bd5ab7a65db6136af7,"Current sensor-based monitoring systems use multiple sensors in order to identify high-level information based on the events that take place in the monitored environment. This information is obtained through low-level processing of sensory media streams, which are usually noisy and imprecise, leading to many undesired consequences such as false alarms, service interruptions, and often violation of privacy. Therefore, we need a mechanism to compute the quality of sensor-driven information that would help a user or a system in making an informed decision and improve the automated monitoring process. In this article, we propose a model to characterize such quality of information in a multisensor multimedia monitoring system in terms of certainty, accuracy/confidence and timeliness. Our model adopts a multimodal fusion approach to obtain the target information and dynamically compute these attributes based on the observations of the participating sensors. We consider the environment context, the agreement/disagreement among the sensors, and their prior confidence in the fusion process in determining the information of interest. The proposed method is demonstrated by developing and deploying a real-time monitoring system in a simulated smart environment. The effectiveness and suitability of the method has been demonstrated by dynamically assessing the value of the three quality attributes with respect to the detection and identification of human presence in the environment.© 2011 ACM.",Event detection; Multimedia monitoring systems; Multimodal fusion; Quality of information,Sensors; Automated monitoring; Current sensors; Event detection; False alarms; Fusion process; High-level information; Informed decision; Media streams; Monitoring system; Multi sensor; Multimedia monitoring systems; Multimodal fusion; Multiple sensors; Quality attributes; Quality of information; Real-time monitoring systems; Service interruption; Smart environment; Target information; Monitoring
Near-duplicate keyframe retrieval by semi-supervised learning and Nonrigid image matching,2011,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-79551668192&doi=10.1145%2f1870121.1870125&partnerID=40&md5=86424c5e49995f430710832fc6c9e844,"Near-duplicate keyframe (NDK) retrieval techniques are critical to many real-world multimedia applications. Over the last few years, we have witnessed a surge of attention on studying near-duplicate image/keyframe retrieval in the multimedia community. To facilitate an effective approach to NDK retrieval on large-scale data, we suggest an effective Multi-Level Ranking (MLR) scheme that effectively retrieves NDKs in a coarse-to-fine manner. One key stage of the MLR ranking scheme is how to learn an effective ranking function with extremely small training examples in a near-duplicate detection task. To attack this challenge, we employ a semi-supervised learning method, semi-supervised support vector machines, which is able to significantly improve the retrieval performance by exploiting unlabeled data. Another key stage of the MLR scheme is to perform a fine matching among a subset of keyframe candidates retrieved from the previous coarse ranking stage. In contrast to previous approaches based on either simple heuristics or rigid matching models, we propose a novel Nonrigid Image Matching (NIM) approach to tackle near-duplicate keyframe retrieval from real-world video corpora in order to conduct an effective fine matching. Compared with the conventional methods, the proposed NIM approach can recover explicit mapping between two near-duplicate images with a few deformation parameters and find out the correct correspondences from noisy data simultaneously. To evaluate the effectiveness of our proposed approach, we performed extensive experiments on two benchmark testbeds extracted from the TRECVID2003 and TRECVID2004 corpora. The promising results indicate that our proposed method is more effective than other state-of-the-art approaches for near-duplicate keyframe retrieval.© 2011 ACM.",Image copy detection; Near-duplicate keyframe; Nonrigid Image Matching; Semi-supervised learning,Image matching; Supervised learning; Deformation parameter; Image copy detection; Multimedia applications; Near-duplicate detection; Near-duplicate keyframe; Semi- supervised learning; Semi-supervised learning methods; State-of-the-art approach; Information retrieval
Understanding Overlay Characteristics of a Large-Scale Peer-to-Peer IPTV System,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985899352&doi=10.1145%2f1865106.1865115&partnerID=40&md5=805abf707679dd7377f2107be2fd4568,"This article presents results from our measurement and modeling efforts on the large-scale peer-to-peer (p2p) overlay graphs spanned by the PPLive system, the most popular and largest p2p IPTV (Internet Protocol Television) system today. Unlike other previous studies on PPLive, which focused on either network-centric or user-centric measurements of the system, our study is unique in (a) focusing on PPLive overlay-specific characteristics, and (b) being the first to derive mathematical models for its distributions of node degree, session length, and peer participation in simultaneous overlays. Our studies reveal characteristics ofmultimedia streaming p2p overlays that aremarkedly different from existing file-sharing p2p overlays. Specifically, we find that: (1) PPLive overlays are similar to random graphs in structure and thus more robust and resilient to the massive failure of nodes, (2) Average degree of a peer in the overlay is independent of the channel population size and the node degree distribution can be fitted by a piecewise function, (3) The availability correlation between PPLive peer pairs is bimodal, that is, some pairs have highly correlated availability, while others have no correlation, (4) Unlike p2p file-sharing peers, PPLive peers are impatient and session lengths (discretized, per channel) are typically geometrically distributed, (5) Channel population size is time-sensitive, self-repeated, event-dependent, and varies more than in p2p file-sharing networks, (6) Peering relationships are slightly locality-aware, and (7) Peer participation in simultaneous overlays follows a Zipf distribution. We believe that our findings can be used to understand current large-scale p2p streaming systems for future planning of resource usage, and to provide useful and practical hints for future design of large-scale p2p streaming systems. © This article presents results from our measurement and modeling efforts on the large-scale peer-to-peer (p2p) overlay graphs spanned by the PPLive system, the most popular and largest p2p IPTV (Internet Protocol Television) system today. Unlike other previous studies on PPLive, which focused on either network-centric or user-centric measurements of the system, our study is unique in (a) focusing on PPLive overlay-specific characteristics, and (b) being the first to derive mathematical models for its distributions of node degree, session length, and peer participation in simultaneous overlays. Our studies reveal characteristics ofmultimedia streaming p2p overlays that aremarkedly different from existing file-sharing p2p overlays. Specifically, we find that: (1) PPLive overlays are similar to random graphs in structure and thus more robust and resilient to the massive failure of nodes, (2) Average degree of a peer in the overlay is independent of the channel population size and the node degree distribution can be fitted by a piecewise function, (3) The availability correlation between PPLive peer pairs is bimodal, that is, some pairs have highly correlated availability, while others have no correlation, (4) Unlike p2p file-sharing peers, PPLive peers are impatient and session lengths (discretized, per channel) are typically geometrically distributed, (5) Channel population size is time-sensitive, self-repeated, event-dependent, and varies more than in p2p file-sharing networks, (6) Peering relationships are slightly locality-aware, and (7) Peer participation in simultaneous overlays follows a Zipf distribution. We believe that our findings can be used to understand current large-scale p2p streaming systems for future planning of resource usage, and to provide useful and practical hints for future design of large-scale p2p streaming systems. © 2010, ACM. All rights reserved.",IPTV; Measurement; multimedia; overlay; Peer-to-peer; Performance; PPLive; streaming,
Multimedia sensor fusion for retrieving identity in biometric access control systems,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649551619&doi=10.1145%2f1865106.1865110&partnerID=40&md5=04d855f7453e60c5d1641266579bbfad,"In this article, we propose a novelmultimedia sensor fusion approach based on heterogeneous sensors for biometric access control applications. The proposed fusion technique uses multiple acoustic and visual sensors for extracting dominant biometric cues, and combines them with nondominant cues. The performance evaluation of the proposed fusion protocol and a novel cascaded authentication approach using a 3D stereovision database shows a significant improvement in performance and robustness, with equal error rates of 42.9% (audio only), 32% (audio + 3D face+ 2D lip features), 15% (audio + 3D face+ 2D eye features), and 7.3% (audio-3D face+2D lip +2D eye-eyebrows) respectively. © 2010 ACM.",,Biometrics; Face recognition; Security systems; Sensors; Three dimensional; 3D faces; Control applications; Equal error rate; Fusion protocol; Fusion techniques; Heterogeneous sensors; Lip features; Performance evaluation; Sensor fusion; Visual sensor; Access control
Dialocalization: Acoustic speaker diarization and visual localization as joint optimization problem,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649623318&doi=10.1145%2f1865106.1865111&partnerID=40&md5=62d437788526f200e55ed2aa07f688f2,"The following article presents a novel audio-visual approach for unsupervised speaker localization in both time and space and systematically analyzes its unique properties. Using recordings from a single, low-resolution room overview camera and a single far-field microphone, a state-of-the-art audio-only speaker diarization system (speaker localization in time) is extended so that both acoustic and visual models are estimated as part of a joint unsupervised optimization problem. The speaker diarization system first automatically determines the speech regions and estimates ""who spoke when,"" then, in a second step, the visual models are used to infer the location of the speakers in the video. We call this process "" dialocalization."" The experiments were performed on real-world meetings using 4.5 hours of the publicly available AMI meeting corpus. The proposed system is able to exploit audio-visual integration to not only improve the accuracy of a state-of-the-art (audio-only) speaker diarization, but also adds visual speaker localization at little incremental engineering and computation costs. The combined algorithm has different properties, such as increased robustness, that cannot be observed in algorithms based on single modalities. The article describes the algorithm, presents benchmarking results, explains its properties, and systematically discusses the contributions of each modality. © 2010 ACM.",Multimodal integration; Speaker diarization; Speech; Visual localization,Algorithms; Optimization; Audio-visual; Audio-visual integration; Combined algorithms; Computation costs; Far-field; Joint optimization; Multimodal integration; Optimization problems; Real-world; Speaker diarization; Speaker localization; Time and space; Visual localization; Visual model; Audio acoustics
Optimizing consistency by maximizing bandwidth usage in distributed interactive applications,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649570799&doi=10.1145%2f1865106.1865114&partnerID=40&md5=12a892b81c1808f2e0110e0466926f0c,"A key factor determining the success of a Distributed Interactive Application (DIA) is the maintenance of a consistent shared virtual world. To help maintain consistency, a number of Information Management techniques have been developed. However, unless carefully tuned to the underlying network, they can negatively impact on consistency. This work presents a novel adaptive algorithm for optimizing consistency by maximizing available bandwidth usage in DIAs. This algorithm operates by estimating bandwidth from trends in network latency, and modifying data transmission rates to match the estimated value. Results presented within demonstrate that this approach can help optimise consistency levels in a DIA. © ACM.",Adaptive algorithms; Consistency; Information management techniques; Multiplayer games,Bandwidth; Data communication systems; Industrial management; Information management; Interactive devices; Optimization; Virtual reality; Available bandwidth; Bandwidth usage; Consistency; Consistency level; Data transmission rates; Distributed interactive applications; Key factors; Management techniques; Multiplayer games; Network latencies; Underlying networks; Virtual worlds; Adaptive algorithms
Spatial-geometric approach to physical mobile interaction based on accelerometer and IR sensory data fusion,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649627436&doi=10.1145%2f1865106.1865112&partnerID=40&md5=fb05c8f555b638a2e81954460554a32a,"Interaction with the physical environment using mobile phones has become increasingly desirable and feasible. Nowadays mobile phones are being used to control different devices and access information/services related to those devices. To facilitate such interaction, devices are usually marked with RFID tags or visual markers, which are read by a mobile phone equipped with an integrated RFID reader or camera to fetch related information about those objects and initiate further actions. This article contributes in this domain of mobile physical interaction; however, using a spatial-geometric approach for interacting with indoor physical objects and artifacts instead of RFID based solutions. Using this approach, a mobile phone can point from a distance to an annotated object or a spatial subregion of that object for the purpose of interaction. The pointing direction and location is determined based on the fusion of IR camera and accelerometer data, where the IR cameras are used to calculate the 3D position of the mobile phone users and the accelerometer in the phone provides its tilting and orientation information. The annotation of objects and their subregions with which the mobile phone interacts is performed by specifying their geometric coordinates and associating related information or services with them.We perform experiment in a technology-augmented smart space and show the applicability and potential of the proposed approach. © 2010 ACM.",Indoor position; Multimedia objects tagging; Multisensor fusion; Orientation; Physical browsing; Physical mobile interaction,Accelerometers; Cameras; Geometry; Mobile devices; Mobile telecommunication systems; Radio navigation; Telephone; Telephone sets; Telephone systems; Indoor position; Mobile interaction; Multimedia objects tagging; Multisensor fusion; Orientation; Physical browsing; Mobile phones
Understanding overlay characteristics of a large-scale peer-to-peer IPTV system,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649571833&doi=10.1145%2f1865106.186&partnerID=40&md5=e9329d3721fb0b7ac28cac7f0822d222,"This article presents results from our measurement and modeling efforts on the large-scale peer-to-peer (p2p) overlay graphs spanned by the PPLive system, the most popular and largest p2p IPTV (Internet Protocol Television) system today. Unlike other previous studies on PPLive, which focused on either network-centric or user-centric measurements of the system, our study is unique in (a) focusing on PPLive overlay-specific characteristics, and (b) being the first to derive mathematical models for its distributions of node degree, session length, and peer participation in simultaneous overlays. Our studies reveal characteristics ofmultimedia streaming p2p overlays that aremarkedly different from existing file-sharing p2p overlays. Specifically, we find that: (1) PPLive overlays are similar to random graphs in structure and thus more robust and resilient to the massive failure of nodes, (2) Average degree of a peer in the overlay is independent of the channel population size and the node degree distribution can be fitted by a piecewise function, (3) The availability correlation between PPLive peer pairs is bimodal, that is, some pairs have highly correlated availability, while others have no correlation, (4) Unlike p2p file-sharing peers, PPLive peers are impatient and session lengths (discretized, per channel) are typically geometrically distributed, (5) Channel population size is time-sensitive, self-repeated, event-dependent, and varies more than in p2p file-sharing networks, (6) Peering relationships are slightly locality-aware, and (7) Peer participation in simultaneous overlays follows a Zipf distribution. We believe that our findings can be used to understand current large-scale p2p streaming systems for future planning of resource usage, and to provide useful and practical hints for future design of large-scale p2p streaming systems. © ACM.",IPTV; Multimedia; Overlay; Peer-to-peer; PPLive; Streaming,Distributed computer systems; Internet protocols; IPTV; Mathematical models; Population statistics; Television broadcasting; Multimedia; Overlay; Peer to peer; PPLive; Streaming; Peer to peer networks
Foreword to the special issue on multimedia sensor fusion,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649542093&doi=10.1145%2f1865106.1865108&partnerID=40&md5=84f4962fc64c56c0c6cded7770ddb63b,[No abstract available],,
Enabling multiparty 3D tele-immersive environments with ViewCast,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649536306&doi=10.1145%2f1865106.1865113&partnerID=40&md5=5797cb9cd4cb4a883d2034da581d9a50,"Three-dimensional tele-immersive (3DTI) environments have great potential to promote collaborative work among geographically distributed users. However, most existing 3DTI systems work with only two sites due to the huge demand of resources and the lack of a simple yet powerful networking model to handle connectivity, scalability, and quality-of-service (Q0S) guarantees. In this article, we explore the design space from the angle of multistream management to enable multi-party 3DTI communication. Multiple correlated 3D video streams are employed to provide a comprehensive representation of the physical scene in each 3DTI environment, and rendered together to establish a common cyberspace among all participating 3DTI environments. The existence of multistream correlation provides the unique opportunity for new approaches in QoS provisioning. Previous work mostly concentrated on compression and adaptation techniques on the per-stream basis while ignoring the application layer semantics and the coordination required among streams. We propose an innovative and generalized ViewCast model to coordinate the multistream content dissemination over an overlay network. ViewCast leverages view semantics in 3D freeviewpoint video systems to fill the gap between the high-level user interest and the low-level stream management. In ViewCast, only the view information is specified by the user/application, while the underlying control dynamically performs stream differentiation, selection, coordination and dissemination.We present the details of ViewCast and evaluate it through both simulation and 3DTI sessions among tele-immersive environments residing in different institutes across the Internet2. Our experimental results demonstrate the implementation feasibility and performance enhancement of ViewCast in supporting the multiparty3DTI collaboration. © 2010 ACM.",3D tele-immersion; Application level multicast; Distributed multimedia system; Multi-stream coordination; Networking protocol; Q<sub>0</sub>S adaptation,Hydraulics; Multicasting; Multimedia systems; Overlay networks; Quality of service; Tensors; Three dimensional; 3d tele-immersion; Application level multicast; Distributed multimedia system; Multi-stream; Networking protocols; S adaptation; Network protocols
MultiFusion: A boosting approach for multimedia fusion,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649582690&doi=10.1145%2f1865106.1865109&partnerID=40&md5=058407b4f6770e13763570974b4cb661,"The multimodal data usually contain complementary, correlated and redundant information. Thus, multimodal fusion is useful for many multisensor applications. Here, a novel multimodal fusion algorithm is proposed, which is referred to as ""MultiFusion."" The approach adopts a boosting structure where the atomic event is considered as the fusion unit. The correlation of multimodal data is used to form an overall classifier in each iteration. Moreover, by adopting the Adaboost-like structure, the overall fusion performance is improved. Both the simulation experiment and the real application show the effectiveness of the MultiFusion approach. Our approach can be applied in different multimodal applications to exploit the multimedia data characteristics and improve the performance. © 2010 ACM.",Adaboost; Atomic event multimodal fusion; Boosting; Decision fusion,Atoms; AdaBoost; Atomic event multimodal fusion; Boosting; Boosting approach; Decision fusion; Fusion performance; Fusion units; Multi-modal data; Multi-modal fusion; Multi-sensor applications; Multifusion; Multimedia data; Multimodal application; Real applications; Redundant informations; Simulation experiments; Adaptive boosting
LiveSky: Enhancing CDN with P2P,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956390618&doi=10.1145%2f1823746.1823750&partnerID=40&md5=76213dec345957dd31b324c221a112b4,"We present the design and deployment experiences with LiveSky, a commercial hybrid CDN-P2P live streaming system, which inherits the best of both CDN and P2P. We address several key challenges, including: 1) ease of integration with existing CDN infrastructure, 2) dynamic resource scaling while guaranteeing quality-of-service, 3) providing good user experience, ensuring network friendliness and upload fairness. LiveSky has been used for several large-scale live streaming events in China. Our evaluation results from real-world indicate that such a hybrid CDN-P2P system provides quality and performance comparable to a CDN and effectively scales the system capacity. © 2010 ACM.",Content delivery networks; Live streaming; Peer-to-peer,Multimedia services; Video streaming; Commercial hybrids; Content delivery network; Evaluation results; Live streaming; P2P system; Peer to peer; Real-world; System Capacity; User experience; Peer to peer networks
Audio-visual atoms for generic video concept classification,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956370559&doi=10.1145%2f1823746.1823748&partnerID=40&md5=e6b30104ed7fce48a0fc13617875aa01,"We investigate the challenging issue of joint audio-visual analysis of generic videos targeting at concept detection. We extract a novel local representation, Audio-Visual Atom (AVA), which is defined as a region track associated with regional visual features and audio onset features. We develop a hierarchical algorithm to extract visual atoms from generic videos, and locate energy onsets from the corresponding soundtrack by time-frequency analysis. Audio atoms are extracted around energy onsets. Visual and audio atoms form AVAs, based on which discriminative audio-visual codebooks are constructed for concept detection. Experiments over Kodak's consumer benchmark videos confirm the effectiveness of our approach. © 2010 ACM.",Audio-Visual Atom; Audio-visual codebook; Joint audio-visual analysis; Semantic concept detection,Feature extraction; Semantics; Audio-visual; Audio-Visual Atom; Audiovisual analysis; Codebooks; Concept classification; Concept detection; Energy onset; Hierarchical algorithm; Semantic concept detection; Time frequency analysis; Visual feature; Atoms
Call For Papers,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025421058&doi=10.1145%2f1823746.1837254&partnerID=40&md5=50ecbecc8892ffbfcbcb0c67a8effd1b,[No abstract available],,
Word level automatic alignment of music and lyrics using vocal synthesis,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956388808&doi=10.1145%2f1823746.1823753&partnerID=40&md5=410feb5c860aa83e8ce50268d4b9da60,"We propose a signal-based approach instead of the commonly used model-based approach, to automatically align vocal music with text lyrics at the word level. In this approach, we use a text-to-speech system to synthesize the singing voice according to the lyrics. In this way, aligning the music signal with the corresponding text lyrics becomes the alignment of two audio signals. This study uses the results of music information modeling and singing voice synthesis. In music information modeling, we study different music representation strategies for music segmentation, music region indexing and region content descriptions; in singing voice synthesis, we generate singing voice by making use of music knowledge to approximate the target vocal line in terms of tempo. The experimental results on a 20-song database show 26.3% and 36.1% word level alignment error rates at eighth note and sixteenth note alignment tolerances respectively. The proposed approach presents an alternative and effective solution to music-lyrics alignment which may require less training dataset. © 2010 ACM.",Music indexing; Music information modeling; Music-lyrics alignment; Rhythm analysis; Singing voice synthesis,Alignment; Audio acoustics; Indexing (of information); Knowledge representation; Music indexing; Music information; Music-lyrics alignment; Rhythm analysis; Singing-voice synthesis; Computer music
Visual Query Suggestion: Towards capturing user intent in internet image search,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956354481&doi=10.1145%2f1823746.1823747&partnerID=40&md5=7abbf5281f2dd5c3622f50790cdba21f,"Query suggestion is an effective approach to bridge the Intention Gap between the users' search intents and queries. Most existing search engines are able to automatically suggest a list of textual query terms based on users' current query input, which can be called Textual Query Suggestion. This article proposes a new query suggestion scheme named Visual Query Suggestion (VQS) which is dedicated to image search. VQS provides a more effective query interface to help users to precisely express their search intents by joint text and image suggestions. When a user submits a textual query, VQS first provides a list of suggestions, each containing a keyword and a collection of representative images in a dropdown menu. Once the user selects one of the suggestions, the corresponding keyword will be added to complement the initial query as the new textual query, while the image collection will be used as the visual query to further represent the search intent. VQS then performs image search based on the new textual query using text search techniques, as well as content-based visual retrieval to refine the search results by using the corresponding images as query examples. We compare VQS against three popular image search engines, and show that VQS outperforms these engines in terms of both the quality of query suggestion and the search performance. © 2010 ACM.",Image search; Intention Gap; Query suggestion,Information retrieval; Search engines; As content; Image collections; Image search; Image search engine; Image suggestion; Intention Gap; Internet images; Query interfaces; Query suggestion; Search performance; Search results; Text search; Textual query; Visual query; Visual retrieval; Content based retrieval
Semi-Supervised Distance Metric Learning for Collaborative Image Retrieval and Clustering,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956374577&doi=10.1145%2f1823746.1823752&partnerID=40&md5=ec5d3ba7f1e9090a223dabab731b8a05,"Learning a good distance metric plays a vital role in many multimedia retrieval and data mining tasks. For example, a typical content-based image retrieval (CBIR) system often relies on an effective distance metric to measure similarity between any two images. Conventional CBIR systems simply adopting Euclidean distance metric often fail to return satisfactory results mainly due to the well-known semantic gap challenge. In this article, we present a novel framework of Semi-Supervised Distance Metric Learning for learning effective distance metrics by exploring the historical relevance feedback log data of a CBIR system and utilizing unlabeled data when log data are limited and noisy. We formally formulate the learning problem into a convex optimization task and then present a new technique, named as ""Laplacian Regularized Metric Learning"" (LRML). Two efficient algorithms are then proposed to solve the LRML task. Further, we apply the proposed technique to two applications. One direct application is for Collaborative Image Retrieval (CIR), which aims to explore the CBIR log data for improving the retrieval performance of CBIR systems. The other application is for Collaborative Image Clustering (CIC), which aims to explore the CBIR log data for enhancing the clustering performance of image pattern clustering tasks. We conduct extensive evaluation to compare the proposed LRML method with a number of competing methods, including 2 standard metrics, 3 unsupervised metrics, and 4 supervised metrics with side information. Encouraging results validate the effectiveness of the proposed technique. © 2010 ACM.",Content-based image retrieval; Distance Metric Learning; Multimedia data clustering,Clustering algorithms; Content based retrieval; Convex optimization; Feedback; CBIR system; Collaborative image retrieval; Content based image retrieval; Data mining tasks; Distance Metric Learning; Distance metrics; Effective distance; Efficient algorithm; Euclidean distance; Image clustering; Image patterns; Laplacians; Learning problem; Log data; Metric learning; Multimedia data; Multimedia Retrieval; Other applications; Relevance feedback; Retrieval performance; Semantic gap; Semi-supervised; Side information; Standard metrics; Unlabeled data; Information retrieval
Introduction to the best papers of ACM multimedia 2009,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956365877&doi=10.1145%2f1823746.1830482&partnerID=40&md5=fcc9630aeb3d95d06328dc1a1bbc0411,[No abstract available],,
Looking at near-duplicate videos from a human-centric perspective,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956357142&doi=10.1145%2f1823746.1823749&partnerID=40&md5=a0afc730a27c68d3c2c8c1dc99e74049,"Popular content in video sharing websites (e.g., YouTube) is usually replicated via identical copies or near-duplicates. These duplicates are usually studied because they pose a threat to site owners in terms of wasted disk space, or privacy infringements. Furthermore, this content might potentially hinder the users' experience in these websites. The research presented in this article focuses around the central argument that there is no agreement on the technical definition of what these near-duplicates are, and, more importantly, there is no strong evidence that users of video sharing websites would like this content to be removed. Most scholars define near-duplicate video clips (NDVC) by means of non-semantic features (e.g., different image/audio quality), while a few also include semantic features (i.e., different videos of similar content). However, it is unclear what features contribute to the human perception of near-duplicate videos. The findings of four large scale online surveys that were carried out in the context of our research confirm the relevance of both types of features. Some of our findings confirm the adopted definitions of NDVC whereas other findings are surprising: Near-duplicate videos with different image quality, audio quality, or with/without overlays were perceived as NDVC. However, the same could not be verified when videos differed by more than one of these features at the same time. With respect to semantics, it is yet unclear the exact role that it plays in relation to the features that make videos alike. From a user's perspective, participants preferred in most cases to see only one of the NDVC in the search results of a video search query and they were more tolerant to changes in the audio than in the video tracks. Based on all these findings, we propose a new user-centric NDVC definition and present implications for how duplicate content should be dealt with by video sharing Web sites. © 2010 ACM.",Near-duplicate; Psychophysical experiment; Similarity; User study; Video sharing; YouTube NDVC,Color photography; Image quality; Semantics; Websites; Near-duplicate; Psychophysical experiments; Similarity; User study; Video sharing; YouTube; Experiments
Efficient delivery of on-demand video streams to heterogeneous receivers,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956362216&doi=10.1145%2f1823746.1823754&partnerID=40&md5=2786fa633814c124ffb2502ef14bc401,"The number of video streams that can be serviced concurrently is highly constrained by the required real-time and high-rate transfers of multimedia data. Resource sharing techniques, such as Batching, Patching, and Earliest Reachable Merge Target (ERMT), can be used to address this problem by utilizing the multicast facility, which allows multiple requests to share the same set of server and network resources. They assume, however, that all clients have the same available download bandwidth and buffer space. We study how to efficiently support clients with varying available download bandwidth and buffer space, while delivering data in a client-pull fashion using enhanced resource sharing. In particular, we propose three hybrid solutions to address the variability in the download bandwidth among clients: Simple Hybrid Solution (SHS), Adaptive Hybrid Solution (AHS), and Enhanced Hybrid Solution (EHS). SHS simply combines Batching with either Patching or ERMT, leading to two alternatives: SHS-P and SHS-E, respectively. Batching is used for clients with bandwidth lower than double the video playback rate, and Patching/ERMT is used for the rest. In contrast, AHS and EHS classify clients into multiple bandwidth classes and service them accordingly. AHS employs a new stream type, called adaptive stream, and EHS employs an enhanced adaptive stream type to serve clients with bandwidth capacities ranging between the video playback rate and double that rate. AHS and EHS employ adaptive streams or enhanced adaptive streams in conjunction with Batching and Patching or ERMT, leading to four possible schemes: AHS-P, AHS-E, EHS-P, and EHS-E. Moreover, we consider the variability of the available buffer space among clients. Furthermore, we study how the waiting playback requests for different videos can be scheduled for service in the heterogeneous environment, capturing the variations in both the client bandwidth and buffer space. We evaluate the effectiveness of the proposed solutions and analyze various scheduling policies through extensive simulation. © 2010 ACM.",Adaptive stream merging; Client heterogeneity; Multimedia servers; Video streaming; Video-on-demand (VOD),Acoustic streaming; Bandwidth; Hydraulics; Merging; Servers; Video on demand; Videotex; Bandwidth capacity; Buffer space; Client heterogeneity; Extensive simulations; Heterogeneous environments; High rate; Hybrid solution; Multicasts; Multimedia data; Multimedia servers; Network resource; Resource sharing; Scheduling policies; Stream merging; Video Playback; Video streams; Video streaming
ELVIS: Entertainment-Led Video Summaries,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956352791&doi=10.1145%2f1823746.1823751&partnerID=40&md5=80ba0113de65965fe0b97501f80c3570,"Video summaries present the user with a condensed and succinct representation of the content of a video stream. Usually this is achieved by attaching degrees of importance to low-level image, audio and text features. However, video content elicits strong and measurable physiological responses in the user, which are potentially rich indicators of what video content is memorable to or emotionally engaging for an individual user. This article proposes a technique that exploits such physiological responses to a given video stream by a given user to produce Entertainment-Led VIdeo Summaries (ELVIS). ELVIS is made up of five analysis phases which correspond to the analyses of five physiological response measures: electro-dermal response (EDR), heart rate (HR), blood volume pulse (BVP), respiration rate (RR), and respiration amplitude (RA). Through these analyses, the temporal locations of the most entertaining video subsegments, as they occur within the video stream as a whole, are automatically identified. The effectiveness of the ELVIS technique is verified through a statistical analysis of data collected during a set of user trials. Our results show that ELVIS is more consistent than RANDOM, EDR, HR, BVP, RR and RA selections in identifying the most entertaining video subsegments for content in the comedy, horror/comedy, and horror genres. Subjective user reports also reveal that ELVIS video summaries are comparatively easy to understand, enjoyable, and informative. © 2010 ACM.",Affect; Emotion; Personalization; Physiological response; Semantics; Video content; Video summarization,Hydraulics; Physiology; Semantics; Video recording; Video streaming; Affect; Emotion; Personalizations; Physiological response; Video contents; Video summarization; Physiological models
Effect of compressed offline foveated video on viewing behavior and subjective quality,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-78149334236&doi=10.1145%2f1671954.1671958&partnerID=40&md5=a084271c5785c641574b6cbbd510744c,"Offline foveation is a technique to improve the compression efficiency of digitized video. The general idea behind offline foveation is to blur video regions where no or a small number of previewers look without decreasing the subjective quality for later viewers. It relies on the fact that peripheral vision is reduced compared to central vision, and the observation that during free-viewing humans' gaze positions generally coincide when watching video. In this article, we conduct two experiments to assess how offline foveation affects viewing behavior and subjective quality. In the first experiment, 15 subjects free-viewed six video clips before and after offline foveation whereas in the second experiment we had 17 subjects assessing the quality of these videos after one, two, and three consecutive viewings. Eye movements were measured during the experiments. Results showed that, although offline foveation prior to encoding with H.264 yielded data reductions up to 52% (20% average) on the tested videos, it had little or no effect on where people looked, their inter subject dispersion, fixation duration, saccade amplitude, or the experienced quality during first-time viewing. However, seeing the videos more than once increased the inter subject dispersion and decreased the subjective quality. In view of these results, we discuss the usage of offline foveated video in practical applications. © 2010 ACM.",,
DSI: A model for distributed multimedia semantic indexing and content integration,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-78149320924&doi=10.1145%2f1671954.1671957&partnerID=40&md5=ebc3800b68b78d3266c447f79ef20684,"Considerable research has been done on the content-based multimedia delivery and access in distributed data repositories. As noted in the literature, there is always a trade-off between multimedia quality and access speed. In addition, the overall performance is greatly determined by the distribution of the multimedia data. In this article, an unsupervised multimedia semantic integration approach for a distributed infrastructure, the Distributed Semantic Indexing (DSI), is presented that addresses both the data quality and search performance. With the ability of summarizing content information and guiding data distribution, the proposed approach is distinguished by: (1) logic-based representation and concise abstraction of the semantic contents of multimedia data, which are further integrated to form a general overview of a multimedia data repository-content signature: (2) application of linguistic relationships to construct a hierarchical metadata based on the content signatures allowing imprecise queries; and (3) achieving the optimal performance in terms of search cost. The fundamental structure of the proposed model is presented. The proposed scheme has been simulated and the simulation results are analyzed and compared against several other approaches that have been advocated in the literature. © 2010 ACM.",,
Blind robust watermarking of 3D motion data,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-78149329461&doi=10.1145%2f1671954.1671956&partnerID=40&md5=a3d8219348d692f3fc5acbe74e9e112d,"The article addresses the problem of copyright protection for 3D motion-captured data by designing a robust blind watermarking mechanism. The mechanism segments motion capture data and identifies clusters of 3D points per segment. A watermark can be embedded and extracted within these clusters by using a proposed extension of 3D quantization index modulation. The watermarking scheme is blind in nature and the encoded watermarks are shown to be imperceptible, and secure. The resulting hiding capacity has bounds based on cluster size. The watermarks are shown to be robust against attacks such as uniform affine transformations (scaling, rotation, and translation), cropping, reordering, and noise addition. The time complexity for watermark embedding and extraction is estimated as O(n log n) and O(n2log n), respectively. © 2010 ACM.",,
Waiting-time prediction in scalable on-demand video streaming,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950112071&doi=10.1145%2f1671962.1671967&partnerID=40&md5=cb6b9909af609b18cb14324dc6fc2e54,"Providing video streaming users with expected waiting times enhances their perceived quality-of-service (QoS) and encourages them to wait. In the absence of any waiting-time feedback, users are more likely to defect because of the uncertainty as to when their services will start. We analyze waiting-time predictability in scalable video streaming. We propose two prediction schemes and study their effectiveness when applied with various stream merging techniques and scheduling policies. The results demonstrate that the waiting time can be predicted accurately, especially when enhanced cost-based scheduling is applied. The combination of waiting-time prediction and cost-based scheduling leads to outstanding performance benefits. © 2010 ACM.",Scheduling; Stream merging; Time-of-service guarantees; Video streaming; Waiting-time prediction,Acoustic streaming; Forecasting; Hydraulics; Merging; Scheduling; Videotex; Cost-based scheduling; On-demand video streaming; Perceived quality; Performance benefits; Prediction schemes; Scalable video streaming; Scheduling policies; Service guarantees; Stream merging; Time-of-service guarantees; Waiting-time; Waiting-time prediction; Video streaming
Real-time H.264 video encoding in software with fast mode decision and dynamic complexity control,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-78149305861&doi=10.1145%2f1671954.1671959&partnerID=40&md5=a8b756c2f3fdf73a5a97dec4eddb6518,"This article presents a novel real-time algorithm for reducing and dynamically controlling the computational complexity of an H.264 video encoder implemented in software. A fast mode decision algorithm, based on a Pareto-optimal macroblock classification scheme, is combined with a dynamic complexity control algorithm that adjusts the MB class decisions such that a constant frame rate is achieved. The average coding efficiency of the proposed algorithm was found to be similar to that of conventional encoding operating at half the frame rate. The proposed algorithm was found to provide lower average bitrate and distortion than static complexity scaling. © 2010 ACM.",,
Fast min-hashing indexing and robust spatio-temporal matching for detecting video copies,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950173645&doi=10.1145%2f1671962.1671966&partnerID=40&md5=b74a210805da8d660597aedcc8a35b55,"The increase in the number of video copies, both legal and illegal, has become a major problem in the multimedia and Internet era. In this article, we propose a novel method for detecting various video copies in a video sequence. To achieve fast and robust detection, the method fully integrates several components, namely the min-hashing signature to compactly represent a video sequence, a spatio-temporal matching scheme to accurately evaluate video similarity compiled from the spatial and temporal aspects, and some speedup techniques to expedite both min-hashing indexing and spatio-temporal matching. The results of experiments demonstrate that, compared to several baseline methods with different feature descriptors and matching schemes, the proposed method which combines both global and local feature descriptors yields the best performance when encountering a variety of video transformations. The method is very fast, requiring approximately 0.06 seconds to search for copies of a thirty-second video clip in a six-hour video sequence. © 2010 ACM.",Content-based copy detection; Histogram pruning; Near-duplicate,Graphic methods; Indexing (of information); Baseline methods; Content-based; Copy detection; Feature descriptors; Local feature; Matching scheme; Novel methods; Robust detection; Spatio-temporal; Temporal aspects; Video clips; Video sequences; Video similarity; Video recording
"An eye localization, tracking and blink pattern recognition system: Algorithm and evaluation",2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950163425&doi=10.1145%2f1671962.1671964&partnerID=40&md5=058cc85b9e4c9a6ed91aa0009e62db1f,"This study is to investigate the fundamental problems of, (1) facial feature detection and localization, especially eye features; and (2) eye dynamics, including tracking and blink detection. We first describe our contribution to eye localization. Following that, we discuss a simultaneous eye tracking and blink detection system. Facial feature detection is solved in a general object detection framework and its performance for eye localization is presented. A binary tree representation based on feature dependency partitions the object feature space in a coarse to fine manner. In each compact feature subspace, independent component analysis (ICA) is used to get the independent sources, whose probability density functions (PDFs) are modeled by Gaussian mixtures. When applying this representation for the task of eye detection, a subwindow is used to scan the entire image and each obtained image patch is examined using Bayesian criteria to determine the presence of an eye subject. After the eyes are automatically located with binary tree-based probability learning, interactive particle filters are used for simultaneously tracking the eyes and detecting the blinks. The particle filters use classification-based observation models, in which the posterior probabilities are evaluated by logistic regressions in tensor subspaces. Extensive experiments are used to evaluate the performance from two aspects, (1) blink detection rate and the accuracy of blink duration in terms of the frame numbers; (2) eye tracking accuracy. We also present an experimental setup for obtaining the benchmark data in tracking accuracy evaluation. The experimental evaluation demonstrates the capability of this approach. © 2010 ACM.",Eye blink detection; Human computer interface; Particle filtering; Video processing,Binary trees; Evolutionary algorithms; Eye protection; Independent component analysis; Interfaces (computer); Nonlinear filtering; Pattern recognition systems; Probability; Video signal processing; Bayesian criterion; Benchmark data; Blink detection system; Blink duration; Coarse to fine; Detection rates; Experimental evaluation; Experimental setup; Eye blink; Eye detection; Eye localization; Eye-tracking; Facial feature detection; Feature space; Feature subspace; Fundamental problem; Gaussian mixtures; Human computer interfaces; Image patches; Interactive particles; Logistic regressions; Object Detection; Observation model; Particle filter; Particle Filtering; Posterior probability; Probability learning; Tracking accuracy; Video processing; Probability density function
Server selection in large-scale video-on-demand systems,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049519675&doi=10.1145%2f1671954.1671955&partnerID=40&md5=45a19ff68dc53ef92d639a99f5401378,"Video on demand, particularly with user-generated content, is emerging as one of the most bandwidth-intensive applications on the Internet. Owing to content control and other issues, some video-on-demand systems attempt to prevent downloading and peer-to-peer content delivery. Instead, such systems rely on server replication, such as via third-party content distribution networks, to support video streaming (or pseudostreaming) to their clients. A major issue with such systems is the cost of the required server resources. By synchronizing the video streams for clients that make closely spaced requests for the same video from the same server, server costs (such as for retrieval of the video data from disk) can be amortized over multiple requests. A fundamental trade-off then arises, however, with respect to server selection. Network delivery cost is minimized by selecting the nearest server, while server cost is minimized by directing closely spaced requests for the same video to a common server. This article compares classes of server selection policies within the context of a simple system model. We conclude that: (i) server selection using dynamic system state information (rather than only proximities and average loads) can yield large improvements in performance, (ii) deferring server selection for a request as late as possible (i.e., until just before streaming is to begin) can yield additional large improvements, and (iii) within the class of policies using dynamic state information and deferred selection, policies using only ""local"" (rather than global) request information are able to achieve most of the potential performance gains. © 2010 ACM.",,
Authentication schemes for multimedia streams: Quantitative analysis and comparison,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-78149338937&doi=10.1145%2f1671954.1671960&partnerID=40&md5=5ec28966f82b24b5e9e3999d36890973,"With the rapid increase in the demand for multimedia services, securing the delivery of multimedia content has become an important issue. Accordingly, the problem of multimedia stream authentication has received considerable attention by previous research and various solutions have been proposed. However, these solutions have not been rigorously analyzed and contrasted to each other, and thus their relative suitability for different streaming environments is not clear. This article presents comprehensive analysis and comparison among different schemes proposed in the literature to authenticate multimedia streams. Authentication schemes for nonscalable and scalable multimedia streams are analyzed. To conduct this analysis, we define five important performance metrics, which are computation cost, communication overhead, receiver buffer size, delay, and tolerance to packet losses. We derive analytic formulas for these metrics for all considered authentication schemes to numerically analyze their performance. In addition, we implement all schemes in a simulator to study and compare their performance in different environments. The parameters for the simulator are carefully chosen to mimic realistic settings. We draw several conclusions on the advantages and disadvantages of each scheme. We extend our analysis to authentication techniques for scalable streams. We pay careful attention to the flexibility of scalable streams and analyze its impacts on the authentication schemes. Our analysis and comparison reveal the merits and shortcomings of each scheme, provide guidelines on choosing the most appropriate scheme for a given multimedia streaming application, and could stimulate designing new authentication schemes or improving existing ones. For example, our detailed analysis has led us to design a new authentication scheme that combines the best features of two previous schemes. © 2010 ACM.",,
Detecting malicious nodes in peer-to-peer streaming by peer-based monitoring,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950103044&doi=10.1145%2f1671962.1671965&partnerID=40&md5=c56bb2e3faa524de45ffd1ca807ed264,"Current peer-to-peer (P2P) streaming systems often assume that nodes cooperate to upload and download data. However, in the open environment of the Internet, this is not necessarily true and there exist malicious nodes in the system. In this article, we study malicious actions of nodes that can be detected through peer-based monitoring. We require each node to monitor the data received and to periodically send monitoring messages about its neighbors to some trustworthy nodes. To efficiently store and search messages among multiple trustworthy nodes, we organize trustworthy nodes into a threaded binary tree. Trustworthy nodes also dynamically redistribute monitoring messages among themselves to achieve load balancing. Our simulation results show that this scheme can efficiently detect malicious nodes with high accuracy, and that the dynamic redistribution method can achieve good load balancing among trustworthy nodes. © 2010 ACM.",Malicious nodes; Peer monitoring; Peer-to-peer streaming,Binary trees; Monitoring; Dynamic redistribution; Load-Balancing; Malicious nodes; Open environment; Peer monitoring; Peer to peer; Peer-to-peer streaming; Simulation result; Peer to peer networks
Enabling multi-party 3D tele-immersive environments with ViewCast,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950169572&doi=10.1145%2f1671962.1671963&partnerID=40&md5=617e94a98e8f77cb00411d708d42d953,"Three-dimensional tele-immersive (3DTI) environments have great potential to promote collaborative work among geographically distributed users. However, most existing 3DTI systems only work with two sites due to the huge demand of resources and the lack of a simple yet powerful networking model to handle connectivity, scalability, and quality-of-service (QoS) guarantees. In this article, we explore the design space from the angle of multi-stream management to enable multi-party 3DTI communication. Multiple correlated 3D video streams are employed to provide a comprehensive representation of the physical scene in each 3DTI environment, and are rendered together to establish a common cyberspace among all participating 3DTI environments. The existence of multi-stream correlation provides the unique opportunity for new approaches in QoS provisioning. Previous work mostly concentrated on compression and adaptation techniques on the per-stream basis while ignoring the application layer semantics and the coordination required among streams. We propose an innovative and generalized ViewCast model to coordinate the multi-stream content dissemination over an overlay network. ViewCast leverages view semantics in 3D free-viewpoint video systems to fill the gap between high-level user interest and low-level stream management. In ViewCast, only the view information is specified by the user/application, while the underlying control dynamically performs stream differentiation, selection, coordination, and dissemination. We present the details of ViewCast and evaluate it through both simulation and 3DTI sessions among tele-immersive environments residing in different institutes across the Internet. Our experimental results demonstrate the implementation feasibility and performance enhancement of ViewCast in supporting multi-party 3DTI collaboration. © 2010 ACM.",3D tele-immersion; Application level multicast; Distributed multimedia system; Multi-stream coordination; Networking protocol; QoS adaptation,Internet protocols; Multicasting; Multimedia systems; Overlay networks; Quality of service; Semantics; Tensors; Three dimensional; 3d tele-immersion; Application-level multicast; Distributed multimedia systems; Multi-stream; Networking protocols; QoS adaptation; Hydraulics
Source traffic analysis,2010,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956384246&doi=10.1145%2f1823746.1823755&partnerID=40&md5=898adaf8beeb56c04b6cb2a6394a8fc4,"Traffic modeling and simulation plays an important role in the area of Network Monitoring and Analysis, for it provides practitioners with efficient tools to evaluate the performance of networks and of their elements. This article focus on the traffic generated by a single source, providing an overview of what was done in the field and studying the statistical properties of the traffic produced by a personal computer, including analysis of the autocorrelation structure. Different distributions were fitted to the interarrival times, packet sizes, and byte count processes with the goal of singling out the ones most suitable for traffic generation. © 2010 ACM.",Modeling; Self-similarity; Simulation; Source traffic analysis,Computer networks; Hardware; Models; Autocorrelation structures; Different distributions; Network Monitoring; Self-similarities; Simulation; Source traffic; Statistical properties; Traffic generation; Personal computers
Improving the extraction of bilingual terminology from Wikipedia,2009,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-75649131103&doi=10.1145%2f1596990.1596995&partnerID=40&md5=496d080a34697543a756354a019b5098,"Research on the automatic construction of bilingual dictionaries has achieved impressive results. Bilingual dictionaries are usually constructed from parallel corpora, but since these corpora are available only for selected text domains and language pairs, the potential of other resources is being explored as well. In this article, we want to further pursue the idea of using Wikipedia as a corpus for bilingual terminology extraction. We propose a method that extracts term-translation pairs from different types of Wikipedia link information. After that, an SVM classifier trained on the features of manually labeled training data determines the correctness of unseen term-translation pairs. © 2009 ACM.",Bilingual dictionary; Link analysis; Wikipedia mining,Learning algorithms; Terminology; Automatic construction; Bilingual dictionary; Labeled training data; Language pairs; Link analysis; Parallel corpora; SVM classifiers; Terminology extraction; Translation pair; Wikipedia; Wikipedia mining; Software agents
Touchable 3D video system,2009,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-75649147702&doi=10.1145%2f1596990.1596993&partnerID=40&md5=e08e808a49f1a1cb0d2f40571f12a671,"Multimedia technologies are reaching the limits of providing audio-visual media that viewers consume passively. An important factor, which will ultimately enhance the user's experience in terms of impressiveness and immersion, is interaction. Among daily life interactions, haptic interaction plays a prominent role in enhancing the quality of experience of users, and in promoting physical and emotional development. Therefore, a critical step in multimedia research is expected to bring the sense of touch, or haptics, into multimedia systems and applications. This article proposes a touchable 3D video system where viewers can actively touch a video scene through a force-feedback device, and presents the underlying technologies in three functional components: (1) contents generation, (2) contents transmission, and (3) viewing and interaction. First of all, we introduce a depth image-based haptic representation (DIBHR) method that adds haptic and heightmap images, in addition to the traditional depth image-based representation (DIBR), to encode the haptic surface properties of the video media. In this representation, the haptic image contains the stiffness, static friction, and dynamic friction, whereas the heightmap image contains roughness of the video contents. Based on this representation method, we discuss how to generate synthetic and natural (real) video media through a 3D modeling tool and a depth camera, respectively. Next, we introduce a transmission mechanism based on the MPEG-4 framework where new MPEG-4 BIFS nodes are designed to describe the haptic scene. Finally, a haptic rendering algorithm to compute the interaction force between the scene and the viewer is described. As a result, the performance of the haptic rendering algorithm is evaluated in terms of computational time and smooth contact force. It operates marginally within a 1 kHz update rate that is required to provide stable interaction force and provide smoother contact force with the depth image that has high frequency geometrical noise using a median filter. © 2009 ACM.",Haptic rendering algorithm; Haptic surface properties; Video representation,Algorithms; Dielectric properties; Motion Picture Experts Group standards; Multimedia systems; Stiction; Surface properties; Trace analysis; Video recording; Video signal processing; Visual communication; 3D modeling tools; 3D video; Audio-visual media; Computational time; Contact forces; Critical steps; Daily lives; Depth camera; Depth image; Dynamic friction; Force feedback devices; Functional components; Haptic interactions; Haptic rendering algorithm; Haptic rendering algorithms; Haptics; High frequency; Interaction forces; Median filter; Multimedia research; Multimedia technologies; Quality of experiences; Representation method; Sense of touch; Static friction; Transmission mechanisms; Video contents; Video media; Video representations; Video scene; Three dimensional
Video interactions in online video social networks,2009,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-75649092947&doi=10.1145%2f1596990.1596994&partnerID=40&md5=45e4fc1efffaf16d0d1d4fc694f94239,"This article characterizes video-based interactions that emerge from YouTube's video response feature, which allows users to discuss themes and to provide reviews for products or places using much richer media than text. Based on crawled data covering a representative subset of videos and users, we present a characterization from two perspectives: the video response view and the interaction network view. In addition to providing valuable statistical models for various characteristics, our study uncovers typical user behavioral patterns in video-based environments and shows evidence of opportunistic behavior. © 2009 ACM.",Opportunistic behavior; Promotion; Social media; Social networks; Video communication; Video interactions; Video spam; YouTube,Internet; Behavioral patterns; Interaction networks; Online video; Social media; Social Networks; Statistical models; Video communications; Video interactions; Video response; YouTube; Social networking (online)
Music-driven character animation,2009,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-75649098616&doi=10.1145%2f1596990.1596991&partnerID=40&md5=ddb1847545bbf3f44622cd5a6778bd85,"Music-driven character animation extracts musical features from a song and uses them to create an animation. This article presents a system that builds a new animation directly from musical attributes, rather than simply synchronizing it to the music like similar systems. Using a simple script that identifies the movements involved in the performance and their timing, the user can easily control the animation of characters. Another unique feature of the system is its ability to incorporate multiple characters into the same animation, both with synchronized and unsynchronized movements. A system that integrates Celtic dance movements is developed in this article. An evaluation of the results shows that the majority of animations are found to be appealing to viewers and that altering the music can change the attractiveness of the final result. © 2009 ACM.",Character animation; Motion synthesis; Music analysis; Primitive movements,Character animation; Dance movement; Motion synthesis; Music analysis; Musical features; Unique features; Animation
"A study of content authentication in proxy-enabled multimedia delivery systems: Model, techniques, and applications",2009,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-75649113338&doi=10.1145%2f1596990.1596992&partnerID=40&md5=dd61f03b1e348cbb2bcb94734c2204b3,"Compared with the direct server-user approach, the server-proxy-user architecture for multimedia delivery promises significantly improved system scalability. The introduction of the intermediary transcoding proxies between content servers and end users in this architecture, however, brings unprecedented challenges to content security. In this article, we present a systematic study on the end-to-end content authentication problem in the server-proxy-user context, where intermediary proxies transcode multimedia content dynamically. We present a formal model for the authentication problem, propose a concrete construction for authenticating generic data modality and formally prove its security. We then apply the generic construction to authenticating specific multimedia formats, for example, JPEG2000 code-streams and MPEG-4 video streams. The prototype implementation shows that our scheme is suitable for practical applications. © 2009 ACM.",End-to-end authentication; Multimedia content delivery; Security,Concrete construction; Image coding; Knowledge based systems; Motion Picture Experts Group standards; Multimedia systems; Network security; Servers; Software prototyping; Content authentication; Content security; Content servers; End users; Formal model; Generic construction; Generic data; JPEG 2000; MPEG-4 video streams; Multimedia content delivery; Multimedia contents; Multimedia delivery; Multimedia delivery systems; Multimedia format; Prototype implementations; System scalability; Systematic study; Transcode; Transcoding proxy; User context; Authentication
"Fragment, tag, enrich, and send: Enhancing social sharing of video",2009,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-69149109427&doi=10.1145%2f1556134.1556136&partnerID=40&md5=3fa6d72ae9751b664aa8851c0d43c3c7,"The migration of media consumption to personal computers retains distributed social viewing, but only via nonsocial, strictly personal interfaces. This article presents an architecture, and implementation for media sharing that allows for enhanced social interactions among users. Using a mixed-device model, our work allows targeted, personalized enrichment of content. All recipients see common content, while differentiated content is delivered to individuals via their personal secondary screens. We describe the goals, architecture, and implementation of our system in this article. In order to validate our results, we also present results from two user studies involving disjoint sets of test participants. © 2009 ACM.",Asynchronous media sharing; Differentiated content enrichment; Secondary screens,Architectural design; Asynchronous media sharing; Device models; Differentiated content enrichment; Disjoint sets; Media consumption; Secondary screens; Social interactions; Social sharing; User study; Personal computers
The big picture on small screens delivering acceptable video quality in mobile TV,2009,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-69149102872&doi=10.1145%2f1556134.1556137&partnerID=40&md5=2c16a65f9baa7e94286baf7f49f4ce89,"Mobile TV viewers can change the viewing distance and (on some devices) scale the picture to their preferred viewing ratio, trading off size for angular resolution. We investigated optimal trade-offs between size and resolution through a series of studies. Participants selected their preferred size and rated the acceptability of the visual experience on a 200ppi device at a 4:3 aspect ratio. They preferred viewing ratios similar to living room TV setups regardless of the much lower resolution: at a minimum 14 pixels per degree. While traveling on trains people required videos with a height larger than 35mm. © 2009 ACM.",Mobile multimedia consumption; Resolution; Size; Trade-off,Aspect ratio; Locomotives; Multimedia systems; Railroad cars; Angular resolution; Living room; Lower resolution; Mobile multimedia consumption; Mobile TV; Preferred size; Resolution; Size; Small screens; Trade-off; Video quality; Viewing distance; Commerce
Introduction to the special section for the best papers of ACM multimedia 2008,2009,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-69149106944&doi=10.1145%2f1556134.1556135&partnerID=40&md5=b09c9355fe081d4f01271abbb3d3e06a,[No abstract available],,
Playout buffer and rate optimization for streaming over IEEE 802.11 wireless networks,2009,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-69149106026&doi=10.1145%2f1556134.1556143&partnerID=40&md5=6ba4f4cc18a629507e8e7b86021ff45b,"Most streaming rate selection and buffer optimization algorithms are developed for wired networks and can perform poorly over wireless networks. Wireless MAC layer behavior, such as rate adaptation, retransmissions, and medium sharing, can significantly degrade the effectiveness of current streaming algorithms. This article presents the Buffer and Rate Optimization for Streaming (BROS) algorithm to improve streaming performance. BROS uses a bandwidth estimation tool designed specifically for wireless networks and models the relationship between buffer size, streaming data rate, and available bandwidth distribution. BROS optimizes the streaming data rate and initial buffer size, resulting in a high data rate but with few frame losses and buffer underflow events, while still keeping a small initial buffer delay. BROS is implemented in the Emulated Streaming (EmuS) client-server system and evaluated on an IEEE 802.11 wireless testbed with various wireless conditions. The evaluation shows that BROS can effectively optimize the streaming rate and initial buffer size based on wireless network bandwidth conditions, thus achieving better performance than static rate or buffer selection and jitter removal buffers. © 2009 ACM.",Multimedia networking; Playout buffer; Streaming rate; Wireless networks,Bandwidth; Jitter; Multimedia systems; Optimization; Standards; Telecommunication traffic; Available bandwidth; Bandwidth estimation; Buffer delay; Buffer sizes; Client-server systems; Frame loss; High data rate; IEEE 802.11 wireless networks; IEEE 802.11s; Jitter removal; Multimedia networking; Optimization algorithms; Playout buffer; Rate adaptation; Retransmissions; Streaming algorithm; Streaming data; Streaming rate; Underflows; Wired networks; Wireless MAC; Wireless testbed; Wireless networks
Client-centered multimedia content adaptation,2009,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-69149090529&doi=10.1145%2f1556134.1556139&partnerID=40&md5=b3d7fbf6e3b165d816d09968fff01d7c,"The design and implementation of a client-centered multimedia content adaptation system suitable for a mobile environment comprising of resource-constrained handheld devices or clients is described. The primary contributions of this work are: (1) the overall architecture of the client-centered content adaptation system, (2) a data-driven multi-level Hidden Markov model (HMM)-based approach to perform both video segmentation and video indexing in a single pass, and (3) the formulation and implementation of a Multiple-choice Multidimensional Knapsack Problem (MMKP)-based video personalization strategy. In order to segment and index video data, a video stream is modeled at both the semantic unit level and video program level. These models are learned entirely from training data and no domain-dependent knowledge about the structure of video programs is used. This makes the system capable of handling various kinds of videos without having to manually redefine the program model. The proposed MMKP-based personalization strategy is shown to include more relevant video content in response to the client's request than the existing 0/1 knapsack problem and fractional knapsack problem-based strategies, and is capable of satisfying multiple client-side constraints simultaneously. Experimental results on CNN news videos and Major League Soccer (MLS) videos are presented and analyzed. © 2009 ACM.",Hidden Markov models; Multiple choice multidimensional knapsack problem; Video indexing; Video personalization,Computational grammars; Indexing (of information); Integer programming; Multimedia systems; Object recognition; Video recording; Visual communication; 0/1 knapsack problems; Content adaptation; Data-driven; Hand held device; Knapsack problems; Mobile environments; Multi-level; Multidimensional knapsack problems; Multimedia content adaptation; Multiple choice multidimensional knapsack problem; Multiple clients; News video; Personalizations; Primary contribution; Program models; Resource-constrained; Semantic units; Side constraints; Single pass; Training data; Video contents; Video data; Video indexing; Video personalization; Video programs; Video segmentation; Video streams; Hidden Markov models
Design of multimedia surveillance systems,2009,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-69149089957&doi=10.1145%2f1556134.1556140&partnerID=40&md5=fef5c3ab8941ff0e30cdaaac254c56ba,"This article addresses the problem of how to select the optimal combination of sensors and how to determine their optimal placement in a surveillance region in order to meet the given performance requirements at a minimal cost for a multimedia surveillance system. We propose to solve this problem by obtaining a performance vector, with its elements representing the performances of subtasks, for a given input combination of sensors and their placement. Then we show that the optimal sensor selection problem can be converted into the form of Integer Linear Programming problem (ILP) by using a linear model for computing the optimal performance vector corresponding to a sensor combination. Optimal performance vector corresponding to a sensor combination refers to the performance vector corresponding to the optimal placement of a sensor combination. To demonstrate the utility of our technique, we design and build a surveillance system consisting of PTZ (Pan-Tilt-Zoom) cameras and active motion sensors for capturing faces. Finally, we show experimentally that optimal placement of sensors based on the design maximizes the system performance. © 2009 ACM.",Performance vector; Sensor selection and placement,Content based retrieval; Integer programming; Linearization; Monitoring; Multimedia systems; Optimization; Problem solving; Security systems; Vectors; Active motion; Design and build; Integer Linear Programming; Linear model; Minimal cost; Optimal combination; Optimal performance; Optimal placements; Optimal sensor; Performance requirements; Performance vector; Performance vectors; Sensor combinations; Sensor selection and placement; Subtasks; Surveillance systems; Sensors
SEVA: Sensor-enhanced video annotation,2009,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-69149083050&doi=10.1145%2f1556134.1556141&partnerID=40&md5=45239a14888c984bc40f5209c43f8fe6,"In this article, we study how a sensor-rich world can be exploited by digital recording devices such as cameras and camcorders to improve a user's ability to search through a large repository of image and video files. We design and implement a digital recording system that records identities and locations of objects (as advertised by their sensors) along with visual images (as recorded by a camera). The process, which we refer to as Sensor-Enhanced Video Annotation (SEVA), combines a series of correlation, interpolation, and extrapolation techniques. It produces a tagged stream that later can be used to efficiently search for videos or frames containing particular objects or people. We present detailed experiments with a prototype of our system using both stationary and mobile objects as well as GPS and ultrasound. Our experiments show that: (i) SEVA has zero error rates for static objects, except very close to the boundary of the viewable area; (ii) for moving objects or a moving camera, SEVA only misses objects leaving or entering the viewable area by 1 - 2 frames; (iii) SEVA can scale to 10 fast-moving objects using current sensor technology; and (iv) SEVA runs online using relatively inexpensive hardware. © 2009 ACM.",Context-based retrieval; Location-based services; Sensor-enhanced; Video annotation,Approximation theory; Cameras; Digital devices; Encoding (symbols); Image recording; Location; Sensors; Ultrasonic imaging; Context-based retrieval; Current sensors; Digital recording; Digital recording system; Error rate; Extrapolation techniques; IMPROVE-A; Location-based services; Mobile objects; Moving cameras; Moving objects; Static objects; Video annotation; Video annotations; Video files; Visual image; Computer graphics
Compact and progressive plant models for streaming in networked virtual environments,2009,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-69149105121&doi=10.1145%2f1556134.1556138&partnerID=40&md5=e9cc77cbd6dbda0630a2c42c5377dd64,"Just as in the real world, plants are important objects in virtual worlds for creating pleasant and realistic environments, especially those involving natural scenes. As such, much effort has been made in realistic modeling of plants. As the trend moves towards networked and distributed virtual environments, however, the current models are inadequate as they are not designed for progressive transmissions. In this article, we fill in this gap by proposing a progressive representation for plants based on generalized cylinders. We model the shape and thickness of branches in a plant as Bézier curves, group the curves according to the similarity, and differentially code the curves to represent the plant in a compact and progressive manner. To facilitate the transmission of the plants, we quantify the visual contribution of each branch and use this weight in packet scheduling. We show the efficiency of our representations and the effectiveness of our packet scheduler through experiments over a wide area network. © 2009 ACM.",Multiresolution; Networked virtual environment; Plant models; Progressive coding; Progressive transmission; Streaming,Light transmission; Multiresolution analysis; Packet networks; Scheduling; Transmissions; Multiresolution; Networked virtual environment; Plant models; Progressive coding; Progressive transmission; Streaming; Virtual reality
"Multipath live streaming via TCP: Scheme, performance and benefits",2009,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-69149092792&doi=10.1145%2f1556134.1556142&partnerID=40&md5=c7f40daa6e2dfcb8be2a480d1ed9455a,"Motivated by the wide use of TCP for multimedia streaming in practice and the increasing availability of multipath between end hosts, we study multipath live streaming via TCP in this article. We first design a simple and practical TCP-based multipath streaming scheme, named Dynamic MPath-streaming (DMP-streaming), which dynamically distributes packets over multiple paths by implicitly inferring the available bandwidths on these paths. To allow systematic performance study, we develop an analytical model for DMP-streaming and validate the model using extensive ns simulation and Internet experiments. We explore the parameter space of this model and find that DMP-streaming generally provides satisfactory performance when the aggregate achievable TCP throughput is 1.6 times the video bitrate, when allowing a few seconds of startup delay. Last, we comment on the benefits of using multipath versus single path for TCP-based streaming. © 2009 ACM.",Multimedia streaming; Performance modeling,Bandwidth; Multimedia systems; Video streaming; Analytical model; Available bandwidth; Internet experiments; Live streaming; Multi-path; Multi-path streaming; Multimedia streaming; Multiple-path; Parameter spaces; Performance modeling; Performance study; Single path; TCP throughput; Video bitrate; Transmission control protocol
"Authoring, viewing, and generating hypervideo: An overview of hyper-hitchcock",2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-69149092744&doi=10.1145%2f1413862.1413868&partnerID=40&md5=11ab00a21119e1f7688464da3db5bed7,"Hyper-Hitchcock consists of three components for creating and viewing a form of interactive video called detail-on-demand video: a hypervideo editor, a hypervideo player, and algorithms for automatically generating hypervideo summaries. Detail-on-demand video is a form of hypervideo that supports one hyperlink at a time for navigating between video sequences. The Hyper-Hitchcock editor enables authoring of detail-on-demand video without programming and uses video processing to aid in the authoring process. The Hyper-Hitchcock player uses labels and keyframes to support navigation through and back hyperlinks. HyperHitchcock includes techniques for automatically generating hypervideo summaries of one or more videos that take the form of multiple linear summaries of different lengths with links from the shorter to the longer summaries. User studies on authoring and viewing provided insight into the various roles of links in hypervideo and found that player interface design greatly affects people's understanding of hypervideo structure and the video they access. © 2008 ACM.",Hypervideo; Link generation; Video editing; Video summarization,Hypertext systems; Video amplifiers; Authoring process; Hyperlinks; Hypervideo; Hypervideo players; Interactive video; Interface designs; Key-frames; Link generation; On-Demand; Three component; User study; Video editing; Video processing; Video sequences; Video summarization; Video recording
Dynamic privacy assessment in a smart house environment using multimodal sensing,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350395089&doi=10.1145%2f1413862.1413863&partnerID=40&md5=642c123c546df9987c20479ba353038e,"Surveillance applications in private environments such as smart houses require a privacy management policy if such systems are to be accepted by the occupants of the environment. This is due to the invasive nature of surveillance, and the private nature of the home. In this article, we propose a framework for dynamically altering the privacy policy applied to the monitoring of a smart house based on the situation within the environment. Initially the situation, or context, within the environment is determined; we identify several factors for determining environmental context, and propose methods to quantify the context using audio and binary sensor data. The context is then mapped to an appropriate privacy policy, which is implemented by applying data hiding techniques to control access to data gathered from various information sources. The significance of this work lies in the examination of privacy issues related to assisted-living smart house environments. A single privacy policy in such applications would be either too restrictive for an observer, for example, a carer, or too invasive for the occupants. We address this by proposing a dynamic method, with the aim of decreasing the invasiveness of the technology, while retaining the purpose of the system. © 2008 ACM.",Assisted living; Audio; Context aware; Privacy; Surveillance and monitoring,Access control; Houses; Assisted living; Audio; Context aware; Privacy; Surveillance and monitoring; Monitoring
Sensing and using social context,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-64549086866&doi=10.1145%2f1413862.1413864&partnerID=40&md5=7d21db87c1691ce820acc1301ac10d03,"We present online algorithms to extract social context: Social spheres are labeled locations of significance, represented as convex hulls extracted from GPS traces. Colocation is determined from Bluetooth and GPS to extract social rhythms, patterns in time, duration, place, and people corresponding to real-world activities. Social ties are formulated from proximity and shared spheres and rhythms. Quantitative evaluation is performed for 10+ million samples over 45 man-months. Applications are presented with assessment of perceived utility: Socio-Graph, a video and photo browser with filters for social metadata, and Jive, a blog browser that uses rhythms to discover similarity between entries automatically © 2008 ACM.",Multimedia browsing; Social context,Cellular telephone systems; Global positioning system; Metadata; Colocations; Convex hull; Multimedia browsing; On-line algorithms; Perceived utility; Photo browsers; Quantitative evaluation; Real-world activities; Social context; Social ties; Multimedia systems
Invisible watermarking based on creation and robust insertion-extraction of image adaptive watermarks,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350509599&doi=10.1145%2f1413862.1413865&partnerID=40&md5=64869ef80c5ec2025e05c366ade143e8,"This article presents a novel invisible robust watermarking scheme for embedding and extracting a digital watermark in an image. The novelty lies in determining a perceptually important subimage in the host image. Invisible insertion of the watermark is performed in the most significant region of the host image such that tampering of that portion with an intention to remove or destroy will degrade the esthetic quality and value of the image. One feature of the algorithm is that this subimage is used as a region of interest for the watermarking process and eliminates the chance of watermark removal. Another feature of the algorithm is the creation of a compound watermark using the input user watermark (logo) and attributes of the host image. This facilitates the homogeneous fusion of a watermark with the cover image, preserves the quality of the host image, and allows robust insertion-extraction. Watermark creation consists of two distinct phases. During the first phase, a statistical image is synthesized from a perceptually important subimage of the image. A compound watermark is created by embedding a watermark (logo) into the statistical synthetic image by using a visible watermarking technique. This compound watermark is invisibly embedded into the important block of the host image. The authentication process involves extraction of the perceptive logo as well statistical testing for two-layer evidence. Results of the experimentation using standard benchmarks demonstrates the robustness and efficacy of the proposed watermarking approach. Ownership proof could be established under various hostile attacks. © 2008 ACM.",Content protection; Copyright protection; Image; Invisible watermarking; Watermarking,Copyrights; Digital watermarking; Image processing; Military photography; Adaptive watermarks; Content protection; Copyright protection; Cover-image; Digital water-marks; Embedding and extracting; Host images; Image; Invisible watermarking; Region of interest; Robust watermarking scheme; Statistical images; Statistical testing; Synthetic images; Two layers; Visible watermarking; Watermarking
End-to-end delay control of multimedia applications over multihop wireless links,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350490551&doi=10.1145%2f1413862.1413869&partnerID=40&md5=20d204a9a2d8b30b58cb9aeb858e109f,"The proliferation of multimedia applications over mobile, resource-constrained wireless networks has raised the need for techniques that adapt these applications both to clients' Quality of Service (QoS) requirements and to network resource constraints. This article investigates the upper-layer adaptation mechanisms to achieve end-to-end delay control for multimedia applications. The proposed adaptation approach spans application layer, middleware layer and network layer. In application layer, the requirement adaptor dynamically changes the requirement levels according to end-to-end delay measurement and acceptable QoS requirements for the end-users. In middleware layer, the priority adaptor is used to dynamically adjust the service classes for applications using feedback control theory. In network layer, the service differentiation scheduler assigns different network resources (e.g., bandwidth) to different service classes. With the coordination of these three layers, our approach can adaptively assign resources to multimedia applications. To evaluate the impact of our adaptation scheme, we built a real IEEE 802.11 ad hoc network testbed. The test-bed experiments show that the proposed upper-layer adaptation for end-to-end delay control successfully adjusts multimedia applications to meet delay requirements in many scenarios. © 2008 ACM.",End-to-end delay qos; Wireless ad hoc networks,Control theory; Feedback; Middleware; Multimedia systems; Network layers; Quality of service; Standards; Test facilities; Wireless ad hoc networks; Wireless telecommunication systems; Adaptation scheme; Application layers; Different services; End to end delay; End-to-end delay qos; End-users; IEEE 802.11s; In-network; Layer adaptation; Middleware layer; Multihop wireless; Multimedia applications; Network resource; QoS requirements; Resource-constrained; Service class; Service differentiation; Three-layer; Ad hoc networks
Estimation of certainty for responses to multiple-choice questionnaires using eye movements,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350519389&doi=10.1145%2f1413862.1413867&partnerID=40&md5=161223ab09f1d7605542f29bb90ba90c,"To examine the feasibility of estimating the degree of strength of belief (SOB) of responses using eye movements, the scan paths of eye movements were analyzed while subjects reviewed their own responses to multiple choice tasks. All fixation points of eye movements were classified into visual areas, or cells, which corresponded with the positions of answers. Two estimation procedures are proposed using eye-movement data. The first one is identifying SOB using scan-path transitions. By comparing subject's reports of high and low SOB and eye-movement estimations, a significant correct rate of discrimination of SOB was observed. When the threshold of discrimination was controlled, a high rate of correct responses was obtained if it was set at a low level. The second procedure is conducting SOB discrimination using support vector machines (SVM) trained with features of fixations. Subject's gazing features were analyzed while they reviewed their own responses. A discrimination model for SOB was trained with several combinations of features to see whether performance of a significant level could be obtained. As a result, a trained model with 3 features (which consist of interval time, vertical difference, and length between fixations) can provide significant discrimination performance for SOB. These results provide evidence that strength of belief can be estimated using eye movements. © 2008 ACM.",Certainty; Eye-movements; Scan-path analysis; Support vector machines,Estimation; Image retrieval; Law enforcement; Maximum likelihood estimation; Regression analysis; Support vector machines; Certainty; Degree of strength; Discrimination model; Estimation procedures; Fixation point; High rate; Interval time; Low level; Multiple choice; Scan path; Scan-path analysis; Second procedures; Vertical difference; Visual areas; Eye movements
A criterion-based multilayer access control approach for multimedia applications and the implementation considerations,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350515462&doi=10.1145%2f1413862.1413870&partnerID=40&md5=d408bcb7df074e326363c89b3eb6efe3,"In this article, a novel criterion-based multilayer access control (CBMAC) approach is presented to enhance existing access control models such as Role-Based, Mandatory, and Discretionary Access Control models to support multilayer (multilevel) access control. The proposed approach is based on a set of predefined security criteria which are extracted from authorization rules. The security attributes of objects and users are specified by security criterion expressions (serving as locks) and the elements (serving as keys) of security criterion subsets respectively. An object embedded with a number of security criterion expressions becomes a secure object while a user associated with a security criterion subset is called a secure user. The multilayer access control is achieved by evaluating the embedded security criterion expressions (actuating locks) by the elements (keys) in a user's security criterion subset. The paper also provides the details of integrating the proposed approach with existing access control models and presents the implementation considerations of Criterion-Based Role-Based Multilayer Access Control, the integration of CBMAC and Role-Based Access Control © 2008 ACM.",Multilayer access control; Secure object; Secure permission; Secure user; Security criterion,Integration; Locks (fasteners); Multilayers; Multimedia systems; Security systems; Access control models; Authorization rules; Discretionary access control models; Embedded security; Multimedia applications; Novel criterion; Role-based; Role-based Access Control; Secure object; Secure permission; Secure user; Security attributes; Security criterion; Access control
Offering data confidentiality for multimedia overlay multicast: Design and analysis,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350506595&doi=10.1145%2f1413862.1413866&partnerID=40&md5=1e1b9f50e4f5752ecb99d72d75804903,"Application layer multicast (ALM) has been proposed to overcome current limitations in IP multicast for large-group multimedia communication. We address offering data confidentiality tailored for ALM. To achieve confidentiality, a node may need to continuously re-encrypt packets before forwarding them downstream. Furthermore, keys have to be changed whenever there is a membership change, leading to rekey processing overhead at the nodes. For a large and dynamic group, these reencryption and rekeying operations incur high processing overhead at the nodes. We propose and analyze a scalable scheme called Secure Overlay Multicast (SOM) which clusters ALM peers so as to localize rekeying within a cluster and to limit re-encryption at cluster boundaries, thereby minimizing the total nodal processing overhead. We describe the operations of SOM and compare its nodal processing overhead with two other basic approaches, namely, host-to-host encryption and whole group encryption. We also present a simplified analytic model for SOM and show that there exists an optimal cluster size to minimize the total nodal processing overhead. By comparing with a recently proposed ALM scheme (DT protocol), SOM achieves a substantial reduction in nodal processing overhead with similar network performance in terms of network stress and delay. © 2008 ACM.",Key management; Multicast security; Overlay multicast; Performance analysis,Internet protocols; Multicasting; Multimedia systems; Network performance; Network protocols; Network security; Strength of materials; Systems engineering; Analytic models; Application-layer multicast; Cluster boundaries; Cluster sizes; Current limitation; Data confidentiality; Design and analysis; Dynamic groups; IP Multicast; Key management; Multicast security; Multimedia communication; Overlay multicast; Performance analysis; Processing overhead; Re-encryption; Re-keying; Secure overlay; Substantial reduction; Cryptography
Constructing visual phrases for effective and efficient object-based image retrieval,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349122079&doi=10.1145%2f1404880.1404887&partnerID=40&md5=28f7691a614830951ecc25f9e7df3c91,"The explosion of multimedia data necessitates effective and efficient ways for us to get access to our desired ones. In this article, we draw an analogy between image retrieval and text retrieval and propose a visual phrase-based approach to retrieve images containing desired objects (object-based image retrieval). The visual phrase is defined as a pair of frequently co-occurred adjacent local image patches and is constructed using data mining. We design methods on how to construct visual phrase and how to index/search images based on visual phrase. We demonstrate experiments to show our visual phrase-based approach can be very efficient and more effective than current visual word-based approach. © 2008 ACM.",Content-based image retrieval; Inverted index; Local image descriptor; Object-based image retrieval; SIFT; Visual phrase,Content based retrieval; Decision support systems; Feature extraction; Image enhancement; Information management; Information retrieval; Ketones; Translation (languages); Content-based image retrieval; Inverted index; Local image descriptor; Object-based image retrieval; SIFT; Visual phrase; Image retrieval
Scalable on-demand media streaming for heterogeneous clients,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349117608&doi=10.1145%2f1404880.1404888&partnerID=40&md5=67964befd94791889668ac42fd36573a,"Periodic broadcast protocols enable efficient streaming of highly popular media files to large numbers of concurrent clients. Most previous periodic broadcast protocols, however, assume that all clients can receive at the same rate, and also assume that reception bandwidth is not time-varying. In this article, we first develop a new periodic broadcast protocol, Optimized Heterogeneous Periodic Broadcast (OHPB), that can be optimized for a given population of clients with heterogeneous reception bandwidths and quality-of-service requirements. The OHPB protocol utilizes an optimized segment size progression determined by solving a linear optimization model that takes as input the client population characteristics and an objective function such as mean client startup delay. We then develop a generalization of the OHPB linear optimization model that allows optimal server bandwidth allocation among multiple concurrent OHPB broadcasts, wherein each media file and its clients may have different characteristics. Finally, we propose complementary client protocols employing work-ahead buffering of data during playback, so as to enable more uniform playback quality when the reception bandwidth is time-varying. © 2008 ACM.",Linear programming; Periodic broadcasts; Quality-of-service; Scalable streaming,Bandwidth; Broadcasting; Computer networks; Linear programming; Linearization; Optimization; Population statistics; Telecommunication systems; Time varying systems; Video streaming; Bandwidth allocations; Linear optimization models; Media files; Objective functions; On demands; Periodic broadcasts; Population characteristics; Scalable streaming; Start-up; Quality of service
Nuisance level of a voice call,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349144261&doi=10.1145%2f1404880.1404886&partnerID=40&md5=853db1cd980f34c240606c8d7ee42331,"In our everyday life, we communicate with many people such as family, friends, neighbors, and colleagues. We communicate with them using different communication media such as email, telephone calls, and face-to-face interactions. While email is not real-time and face-to-face communications require geographic proximity, voice and video communications are preferred over other modes of communication. However, real-time voice/video calls may create nuisance to the receiver. In this article, we describe a mathematical model for computing nuisance level of incoming voice/video calls. We computed the closeness and nuisance level using the calling patterns between the caller and the callee. To validate the nuisance model, we collected cell phone call records of real-life people at our university and computed the nuisance value for all voice calls. We validated the nuisance levels using the feedback from those real-life people. Such a nuisance model is useful for predicting unwanted voice and video sessions in an IP communication network. © 2008 ACM.",Behavior; Multimedia communications; Nuisance; Presence; Security; Tolerance; Unwantedness,Human computer interaction; Multimedia systems; Telecommunication equipment; Telecommunication networks; Telephone; Telephone sets; Voice/data communication systems; Behavior; Multimedia communications; Nuisance; Presence; Security; Tolerance; Unwantedness; Communication
A dynamic decision network framework for online media adaptation in stroke rehabilitation,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349115432&doi=10.1145%2f1404880.1404884&partnerID=40&md5=f46dc5b8a8c1c2acce16c1e899a00134,"In this article, we present a media adaptation framework for an immersive biofeedback system for stroke patient rehabilitation. In our biofeedback system, media adaptation refers to changes in audio/visual feedback as well as changes in physical environment. Effective media adaptation frameworks help patients recover generative plans for arm movement with potential for significantly shortened therapeutic time. The media adaptation problem has significant challenges(a) high dimensionality of adaptation parameter space; (b) variability in the patient performance across and within sessions; (c) the actual rehabilitation plan is typically a non-first-order Markov process, making the learning task hard. Our key insight is to understand media adaptation as a real-time feedback control problem. We use a mixture-of-experts based Dynamic Decision Network (DDN) for online media adaptation. We train DDN mixtures per patient, per session. The mixture models address two basic questions(a) given a specific adaptation suggested by the domain experts, predict the patient performance, and (b) given the expected performance, determine the optimal adaptation decision. The questions are answered through an optimality criterion based search on DDN models trained in previous sessions. We have also developed new validation metrics and have very good results for both questions on actual stroke rehabilitation data. © 2008 ACM.",Biofeedback; Dynamic decision network; Media adaptation; Mixture of experts,Biofeedback; Feedback control; Markov processes; Mixtures; Neural networks; Neuromuscular rehabilitation; Adaptation parameters; Arm movements; Biofeedback systems; Domain experts; Dynamic decision network; Dynamic decision networks; Effective medias; First orders; High dimensionalities; Immersive; Learning tasks; Media adaptation; Media adaptations; Mixture models; Mixture of experts; Online medias; Optimal adaptations; Optimality criterions; Physical environments; Rehabilitation plans; Specific adaptations; Stroke rehabilitations; Time feedbacks; Patient rehabilitation
Re-cinematography: Improving the camerawork of casual video,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349116768&doi=10.1145%2f1404880.1404882&partnerID=40&md5=c46f2dc5c3d95ef4f4398a6b1d2ada31,"This article presents an approach to postprocessing casually captured videos to improve apparent camera movement. Re-cinematography transforms each frame of a video such that the video better follows cinematic conventions. The approach breaks a video into shorter segments. Segments of the source video where there is no intentional camera movement are made to appear as if the camera is completely static. For segments with camera motions, camera paths are keyframed automatically and interpolated with matrix logarithms to give velocity-profiled movements that appear intentional and directed. Closeups are inserted to provide compositional variety in otherwise uniform segments. The approach automatically balances the tradeoff between motion smoothness and distortion to the original imagery. Results from our prototype show improvements to poor quality home videos. © 2008 ACM.",Casual video; Cinematography; Image stabilization,Algebra; Photography; Rhenium; Camera motions; Camera movements; Casual video; Cinematography; Home videos; Image stabilization; Matrix logarithms; Post-processing; Cameras
"Interactive TV narratives: Opportunities, progress, and challenges",2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349163076&doi=10.1145%2f1412196.1412198&partnerID=40&md5=872acec4032766ac209993aab5aa530e,"This article is motivated by the question whether television should do more than simply offer interactive services alongside (and separately from) traditional linear programs, in the context of its dominance being seriously challenged and threatened by interactive forms of screen media entertainment. It suggests: yes. Interactive narrativity, that is, the ability to interact with (and influence) stories whilst they are being told, represents one clear development path for interactive television. The capabilities of computing technology are ripe for exploring this new form of storytelling, from creation to commercial distribution. The article starts by looking at the relationship between narrativity and interactivity in the current context of screen media, and identifies clear signs of interest from certain European public broadcasters in interactive TV narratives. It then presents in detail four recent experimental interactive TV productions in the genres of drama, news, and documentary, developed in collaboration with public broadcasters, which illustrate the potential and richness of this new form of storytelling, but also highlight new technological capabilities necessary for such productions. A number of essential technological requirements are then discussed in more detail in the final part. The article suggests that the ShapeShifting Media Technology, employed in the implementation of the four productions, has made significant advances both at the technological and the creative ends in supporting the development of interactive TV narrativity, but, however, that further developments are required before being able to answer questions such as Would end users want such a form of screen media entertainment and Would it be effective for both end users and producers. © 2008 ACM.",Computational narrativity; Digital storytelling; Entertainment; Interactive; Media; Narrativity; Nonlinear; Screen media; Shapeshifting; Television,Linear programming; Computational narrativity; Digital storytelling; Entertainment; Interactive; Media; Narrativity; Nonlinear; Screen media; Shapeshifting; Digital television
Watch-and-comment as a paradigm toward ubiquitous interactive video editing,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349126440&doi=10.1145%2f1412196.1412201&partnerID=40&md5=27bdd18babfd7ded5c00dd84be89a690,"The literature reports research efforts allowing the editing of interactive TV multimedia documents by end-users. In this article we propose complementary contributions relative to end-user generated interactive video, video tagging, and collaboration. In earlier work we proposed the watch-and-comment (WaC) paradigm as the seamless capture of an individual's comments so that corresponding annotated interactive videos be automatically generated. As a proof of concept, we implemented a prototype application, the WaCTool, that supports the capture of digital ink and voice comments over individual frames and segments of the video, producing a declarative document that specifies both: different media stream structure and synchronization. In this article, we extend the WaC paradigm in two ways. First, user-video interactions are associated with edit commands and digital ink operations. Second, focusing on collaboration and distribution issues, we employ annotations as simple containers for context information by using them as tags in order to organize, store and distribute information in a P2P-based multimedia capture platform. We highlight the design principles of the watch-and-comment paradigm, and demonstrate related results including the current version of the WaCTool and its architecture. We also illustrate how an interactive video produced by the WaCTool can be rendered in an interactive video environment, the Ginga-NCL player, and include results from a preliminary evaluation. © 2008 ACM.",Annotation; Ginga-NCL; Interactive digital video; P2P collaboration,Architectural design; Digital arithmetic; Digital television; Ink; Multimedia systems; Software prototyping; Annotation; Automatically generated; Complementary contributions; Context informations; Design principles; Digital inks; Ginga-NCL; Interactive digital video; Interactive TV; Interactive video environments; Interactive videos; Its architectures; Media streams; Multimedia documents; P2P collaboration; Proof of concepts; Research efforts; Two ways; Video interactions; Video tagging; Computer graphics
Equipment allocation in video-on-demand network deployments,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349086025&doi=10.1145%2f1404880.1404885&partnerID=40&md5=df94e7faebacb2d885e96029125346b6,"Video-on-Demand (VoD) services are very user-friendly, but also complex and resource demanding. Deployments involve careful design of many mechanisms where content attributes and usage models should be taken into account. We define, and propose a methodology to solve, the VoD Equipment Allocation Problem of determining the number and type of streaming servers with directly attached storage (VoD servers) to install at each potential location in a metropolitan area network topology such that deployment costs are minimized. We develop a cost model for VoD deployments based on streaming, storage and transport costs and train a parametric function that maps the amount of available storage to a worst-case hit ratio. We observe the impact of having to determine the amount of storage and streaming cojointly, and determine the minimum demand required to deploy replicas as well as the average hit ratio at each location. We observe that common video-on-demand server configurations lead to the installation of excessive storage, because a relatively high hit-ratio can be achieved with small amounts of storage so streaming requirements dominate. © 2008 ACM.",Equipment allocation; Optimization; Resource allocation; Video-on-demand,Electric network topology; Internet; Large scale systems; Machine design; Metropolitan area networks; Planning; Resource allocation; Servers; Video streaming; Cost models; Equipment allocation; Equipment allocations; Hit ratios; Network deployments; Parametric functions; Streaming servers; Transport costs; Usage models; VOD servers; Video on demand
GridCast: Improving peer sharing for P2P VoD,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349106060&doi=10.1145%2f1412196.1412199&partnerID=40&md5=6c812fd2b89168016ca8574eba017966,"Video-on-Demand (VoD) is a compelling application, but costly. VoD is costly due to the load it places on video source servers. Many have proposed using peer-to-peer (P2P) techniques to shift load from servers to peers. Yet, nobody has implemented and deployed a system to openly and systematically evaluate how these techniques work. This article describes the design, implementation and evaluation of GridCast, a real deployed P2P VoD system. GridCast has been live on CERNET since May of 2006. It provides seek, pause, and play operations, and employs peer sharing to improve system scalability. In peak months, GridCast has served videos to 23,000 unique users. From the first deployment, we have gathered information to understand the system and evaluate how to further improve peer sharing through caching and replication. We first show that GridCast with single video caching (SVC) can decrease load on source servers by an average of 22% from a client-server architecture. We analyze the net effect on system resources and determine that peer upload is largely idle. This leads us to changing the caching algorithm to cache multiple videos (MVC). MVC decreases source load by an average of 51% over the client-server. The improvement is greater as user load increases. This bodes well for peer-assistance at larger scales. A detailed analysis of MVC shows that departure misses become a major issue in a P2P VoD system with caching optimization. Motivated by this observation, we examine how to use replication to eliminate departure misses and further reduce server load. A framework for lazy replication is presented and evaluated in this article. In this framework, two predictors are plugged in to create the working replication algorithm. With these two simple predictors, lazy replication can decrease server load by 15% from MVC with only a minor increase in network traffic. © 2008 ACM.",Caching; Peer-to-peer; Replication; Video-on-demand,Client server computer systems; Computer networks; Distributed computer systems; Internet; Large scale systems; Video on demand; Video streaming; Caching; Caching algorithms; Lazy replications; Net effects; Network traffics; Peer to peers; Peer-to-peer; Replication; Replication algorithms; Server architectures; Server loads; System resources; System scalabilities; Video caching; Video sources; VOD systems; Servers
Introduction to special issue: Human-centered televisiondirections in interactive digital television research,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349159660&doi=10.1145%2f1412196.1412197&partnerID=40&md5=11ba6aee7c2a4da7e76ed19eb3a49ecc,"The research area of interactive digital TV is in the midst of a significant revival. Unlike the first generation of digital TV, which focused on producer concerns that effectively limited (re)distribution, the current generation of research is closely linked to the role of the user in selecting, producing, and distributing content. The research field of interactive digital television is being transformed into a study of human-centered television. Our guest editorial reviews relevant aspects of this transformation in the three main stages of the content lifecycle: content production, content delivery, and content consumption. While past research on content production tools focused on full-fledged authoring tools for professional editors, current research studies lightweight, often informal end-user authoring systems. In terms of content delivery, user-oriented infrastructures such as peer-to-peer are being seen as alternatives to more traditional broadcast solutions. Moreover, end-user interaction is no longer limited to content selection, but now facilitates nonlinear participatory television productions. Finally, user-to-user communication technologies have allowed television to become a central component of an interconnected social experience. The background context given in this article provides a framework for appreciating the significance of four detailed contributions that highlight important directions in transforming interactive television research. © 2008 ACM.",Interactive television; Shared experiences; Standards; Survey,Communication; Digital arithmetic; Interactive computer graphics; Learning systems; Production; Systems analysis; Television; Television broadcasting; Television standards; Authoring systems; Authoring tools; Central components; Content consumptions; Content deliveries; Content productions; Current generations; Digital TV; First generations; Important directions; Infra structures; Interactive digital televisions; Interactive digital TV; Interactive television; Interactive televisions; Life-Cycle; Peer to peers; Research areas; Research fields; Research studies; Shared experiences; User communications; User interactions; Digital television
ScaleFFS: A scalable log-structured flash file system for mobile multimedia systems,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349140243&doi=10.1145%2f1404880.1404889&partnerID=40&md5=70d42e99670e636fec1031ca8cc552c1,"NAND flash memory has become one of the most popular storage media for mobile multimedia systems. A key issue in designing storage systems for mobile multimedia systems is handling large-capacity storage media and numerous large files with limited resources such as memory. However, existing flash file systems, including JFFS2 and YAFFS in particular, exhibit many limitations in addressing the storage capacity of mobile multimedia systems. In this article, we design and implement a scalable flash file system, called ScaleFFS, for mobile multimedia systems. ScaleFFS is designed to require only a small fixed amount of memory space and to provide fast mount time, even if the file system size grows to more than tens of gigabytes. The measurement results show that ScaleFFS can be instantly mounted regardless of the file system size, while achieving the same write bandwidth and up to 22% higher read bandwidth compared to JFFS2. © 2008 ACM.",File system; Flash memory; NAND; Storage system,Computer systems; Data storage equipment; Flash memory; Telecommunication systems; File system; File systems; Flash file systems; Limited resources; Measurement results; Memory spaces; NAND; NAND FLASH memories; Storage capacities; Storage medias; Storage system; Storage systems; Multimedia systems
Examining presence and lightweight messaging in a social television experience,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349168984&doi=10.1145%2f1412196.1412200&partnerID=40&md5=b6ef103d48e351ff4355876c2ff19360,"We report on a field evaluation of a prototype social television system (Social TV) that incorporates lightweight messaging as well as ambient awareness of user presence on the system. This evaluation was conducted over a two-week period and involved the participation of ten households. Participants appreciated the ability to see their buddies' presence on the system, the ability to see or suggest the programs they were currently watching, and the ability to send short messages to one another. The presence facilities available in Social TV also allowed participants to learn more about one another's TV viewing habits and preferences, and fostered a sense of connectedness between them. However, they also felt constrained by the limitations of the communication options available to them and demanded free-form text or voice chat to be able to fully express themselves. © 2008 ACM.",Ambient displays; Awareness displays; Computer-mediated communication; Social television,Ambient awarenesses; Ambient displays; Awareness displays; Computer-mediated communication; Field evaluations; Short messages; Social television; Social televisions; Social tv; Voice chats; Television systems
Special section from the ACM multimedia conference 2007,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349157984&doi=10.1145%2f1404880.1404881&partnerID=40&md5=d4cec0814ba43d09662f7685d6f48330,[No abstract available],,
Correlative multilabel video annotation with temporal kernels,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349133674&doi=10.1145%2f1404880.1404883&partnerID=40&md5=016fcc459df5830f5caf07ce499067f2,"Automatic video annotation is an important ingredient for semantic-level video browsing, search and navigation. Much attention has been paid to this topic in recent years. These researches have evolved through two paradigms. In the first paradigm, each concept is individually annotated by a pre-trained binary classifier. However, this method ignores the rich information between the video concepts and only achieves limited success. Evolved from the first paradigm, the methods in the second paradigm add an extra step on the top of the first individual classifiers to fuse the multiple detections of the concepts. However, the performance of these methods can be degraded by the error propagation incurred in the first step to the second fusion one. In this article, another paradigm of the video annotation method is proposed to address these problems. It simultaneously annotates the concepts as well as model correlations between them in one step by the proposed Correlative Multilabel (CML) method, which benefits from the compensation of complementary information between different labels. Furthermore, since the video clips are composed by temporally ordered frame sequences, we extend the proposed method to exploit the rich temporal information in the videos. Specifically, a temporal-kernel is incorporated into the CML method based on the discriminative information between Hidden Markov Models (HMMs) that are learned from the videos. We compare the performance between the proposed approach and the state-of-the-art approaches in the first and second paradigms on the widely used TRECVID data set. As to be shown, superior performance of the proposed method is gained. © 2008 ACM.",Concept correlation; Multilabeling; Temporal kernel; Video annotation,Classifiers; Information theory; Learning systems; Markov processes; AS models; Automatic video annotations; Binary classifiers; Complementary informations; Concept correlation; Data sets; Error propagations; Frame sequences; Individual classifiers; Multilabel; Multilabeling; Multiple detections; One steps; Second fusions; Superior performances; Temporal informations; Temporal kernel; Trecvid; Video annotation; Video annotations; Video browsing; Video clips; Hidden Markov models
Exploring large-scale peer-to-peer live streaming topologies,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-52949123571&doi=10.1145%2f1386109.1386112&partnerID=40&md5=c2fbb585d20b7e4be37c9c13a5ce07f5,"Real-world live peer-to-peer (P2P) streaming applications have been successfully deployed in the Internet, delivering live multimedia content to millions of users at any given time. With relative simplicity in design with respect to peer selection and topology construction protocols and without much algorithmic sophistication, current-generation live P2P streaming applications are able to provide users with adequately satisfying viewing experiences. That said, little existing research has provided sufficient insights on the time-varying internal characteristics of peer-to-peer topologies in live streaming. This article presents Magellan, our collaborative work with UUSee Inc., Beijing, China, for exploring and charting graph theoretical properties of practical P2P streaming topologies, gaining important insights in their topological dynamics over a long period of time. With more than 120 GB worth of traces starting September 2006 from a commercially deployed P2P live streaming system that represents UUSee's core product, we have completed a thorough and in-depth investigation of the topological properties in large-scale live P2P streaming, as well as their evolutionary behavior over time, for example, at different times of the day and in flash crowd scenarios. We seek to explore real-world P2P streaming topologies with respect to their graph theoretical metrics, such as the degree, clustering coefficient, and reciprocity. In addition, we compare our findings with results from existing studies on topological properties of P2P file sharing applications, and present new and unique observations specific to streaming. We have observed that live P2P streaming sessions demonstrate excellent scalability, a high level of reciprocity, a clustering phenomenon in each ISP, and a degree distribution that does not follow the power-law distribution. © 2008 ACM.",Peer-to-peer streaming; Topology characterization,"Distributed computer systems; Flow of solids; Graph theory; Internet; Internet protocols; Internet service providers; Natural resources exploration; Video streaming; Beijing , China; Clustering coefficient; Collaborative works; Degree distributions; Flash crowds; In-depth investigation; Internal characteristics; Live streaming; Magellan; Multi-media content; P2p file sharing; P2P streaming; Peer selection; Peer-to-Peer; Peer-to-peer streaming; Peer-to-peer topologies; Power-law distribution; Real-world; Streaming applications; Time-varying; Topological dynamics; Topological properties; Topology characterization; Topology construction; Topology"
Predictive real-time perceptual compression based on eye-gaze-position analysis,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-52949094227&doi=10.1145%2f1386109.1386116&partnerID=40&md5=35a8e08303baddb5d6fcc6436d70e961,This article designs a real-time perceptual compression system (RTPCS) based on eye-gaze-position analysis. Our results indicate that the eye-gaze-position containment metric provides more efficient and effective evaluation of an RTPCS than the eye fixation containment. The presented RTPCS is designed for a network communication scenario with a feedback loop delay. The proposed RTPCS uses human visual system properties to compensate for the delay and to provide high ratios of multimedia compression. © 2008 ACM.,Human visual system; Real-time multimedia compression,Data compression; Topology; Video signal processing; Feed back loops; Human visual system; Multimedia compression; Network communications; Perceptual compression; Position analysis; Real-time multimedia compression; Multimedia systems
Towards attention-centered interfaces: An aesthetic evaluation of perspective with eye tracking,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-52949143798&doi=10.1145%2f1386109.1386111&partnerID=40&md5=cf4c5dd9ab00f76570b0599f0e2c4238,"The established method of representing three-dimensional space on a two-dimensional surface involves camera based, point of regard systems, comparable in design to the early ""camera obscura"". However, geometrical limitations of such models lead to distortions of perspective when projected. This research investigated the influence of single- versus multi-perspectives on aesthetic choices within one image. A clear perceptual bias towards multi-perspective images was found, additionally supported by an eye tracking study. We propose that human users are more attracted by multi-perspective images, which emphasise the ""semantic foci"" of the scene, than by those being synthesized statically with only one geometrical prospect. © 2008 ACM.",Eye tracking; Perspective projection; Scene perception; Subjective evaluation,Cameras; Image enhancement; Image segmentation; Information theory; Three dimensional; Camera obscura; Eye tracking; Human users; Multi-perspective; Perspective projection; Scene perception; Subjective evaluation; Three-dimensional space; Two-dimensional surfaces; Two dimensional
Low-latency adaptive streaming over TCP,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-52949125566&doi=10.1145%2f1386109.1386113&partnerID=40&md5=2d2bc347c803940676303e2ad3dcc4cf,"Media streaming over TCP has become increasingly popular because TCP's congestion control provides remarkable stability to the Internet. Streaming over TCP requires adapting to bandwidth availability, but unforunately, TCP can introduce significant latency at the application level, which causes unresponsive and poor adaptation. This article shows that this latency is not inherent in TCP but occurs as a result of throughput-optimized TCP implementations. We show that this latency can be minimized by dynamically tuning TCP's send buffer. Our evaluation shows that this approach leads to better application-level adaptation and it allows supporting interactive and other low-latency applications over TCP. © 2008 ACM.",Low latency streaming; Multimedia applications; TCP,Real time systems; Application level; Bandwidth availability; Congestion control; Low latency streaming; Low-latency; Media streaming; Multimedia applications; TCP; Transmission control protocol
Video game design using an eye-movement-dependent model of visual attention,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-52949135490&doi=10.1145%2f1386109.1386115&partnerID=40&md5=1a36f5fb10859337d2209807d1ebf12c,"Eye movements can be used to infer the allocation of covert attention. In this article, we propose to model the allocation of attention in a task-dependent manner based on different eye movement conditions, specifically fixation and pursuit. We show that the image complexity at eye fixation points during fixation, and the pursuit direction during pursuit are significant factors in attention allocation. Results of the study are applied to the design of an interactive computer game. Real-time eye movement information is taken as one of inputs for the game. The utility of such eye information for controlling game difficulty is shown. © 2008 ACM.",Entertainment; Eye movements; Eye tracking; HCI; Video games; Visual attention,Chlorine compounds; Game theory; Entertainment; Eye tracking; Game designs; HCI; Image complexity; Interactive computer game; Video games; Visual attention; Eye movements
A neural-network-based context-aware handoff algorithm for multimedia computing,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-52949112584&doi=10.1145%2f1386109.1386110&partnerID=40&md5=f8791273d89258d9508470a0f2d096f4,"The access of multimedia computing in wireless networks is concerned with the performance of handoff because of the irretrievable property of real-time data delivery. To lessen throughput degradation incurred by unnecessary handoffs or handoff latencies leading to media disruption perceived by users, this paper presents a link quality based handoff algorithm. Neural networks are used to learn the cross-layer correlation between the link quality estimator such as packet success rate and the corresponding context metric indictors, for example, the transmitting packet length, received signal strength, and signal to noise ratio. Based on a pre-processed learning of link quality profile, neural networks make essential handoff decisions efficiently with the evaluations of link quality instead of the comparisons between relative signal strength. The experiment and simulation results show that the proposed algorithm improves the user perceived qualities in a transmission scenario of VoIP applications by minimizing both the number of lost packets and unnecessary handoffs. © 2008 ACM.",Context-aware; Handoff; Multimedia computing; Neural networks,Neural networks; Signal to noise ratio; Context-Aware; Handoff; Handoff algorithms; Multimedia computing; Received signal strength; Relative signal strengths; Throughput degradation; VoIP applications; Multilayer neural networks
Data placement and prefetching with accurate bit rate control for interactive media server,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-52949144178&doi=10.1145%2f1386109.1386114&partnerID=40&md5=cae132808e998396e6146daa6edb3b07,"An interactive Media Server should support unrestricted control to viewers with their service level agreements. It is important to manage video data effectively to facilitate efficient retrieval. In this paper, we propose an efficient placement algorithm as part of an effective retrieval scheme to increase the number of clients who can be provided with interactive service. The proposed management schemes are incorporated with a bit count control method that is based on repeated tuning of quantization parameters to adjust the actual bit count to the target bit count. The encoder using this method can generate coded frames whose sizes are synchronized with the RAID stripe size, so that when various fast-forward levels are accessed we can reduce the seek and rotational latency and enhance the disk throughput of each disk in the RAID system. Experimental results demonstrate that the proposed schemes can significantly improve the average service time and guarantee more users service of quality, and the interactive media server can thereby efficiently service a large number of clients. © 2008 ACM.",Bit count control; Disk array; Interactive media server; Stripe size; Video rate,Disks (structural components); Image coding; Video recording; Average service time; Bit count control; Bit-rate control; Control methods; Data placement; Disk array; Fast forwarding; Interactive media; Interactive media server; Interactive services; Placement algorithms; Pre-fetching; Quantization parameters; RAID systems; Service-level agreements; Stripe size; Video data; Video rate; Quality of service
A scalable and extensible segment-event-object-based sports video retrieval system,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-44249101396&doi=10.1145%2f1352012.1352017&partnerID=40&md5=899a5f0e5bc391a5eee61864c854a21f,"Sport video data is growing rapidly as a result of the maturing digital technologies that support digital video capture, faster data processing, and large storage. However, (1) semi-automatic content extraction and annotation, (2) scalable indexing model, and (3) effective retrieval and browsing, still pose the most challenging problems for maximizing the usage of large video databases. This article will present the findings from a comprehensive work that proposes a scalable and extensible sports video retrieval system with two major contributions in the area of sports video indexing and retrieval. The first contribution is a new sports video indexing model that utilizes semi-schema-based indexing scheme on top of an Object-Relationship approach. This indexing model is scalable and extensible as it enables gradual index construction which is supported by ongoing development of future content extraction algorithms. The second contribution is a set of novel queries which are based on XQuery to generate dynamic and user-oriented summaries and event structures. The proposed sports video retrieval system has been fully implemented and populated with soccer, tennis, swimming, and diving video. The system has been evaluated against 20 users to demonstrate and confirm its feasibility and benefits. The experimental sports genres were specifically selected to represent the four main categories of sports domain: period-, set-point-, time (race)-, and performance-based sports. Thus, the proposed system should be generic and robust for all types of sports. © 2008 ACM.",Automatic content extraction; Indexing; Mobile video interaction; MPEG-7; Sports video retrieval; Video database system; XML; XQuery,Database systems; Image acquisition; Object recognition; Optimization; Problem solving; Query processing; Video streaming; XML; Automatic content extraction; Mobile video interaction; Sports video retrieval; Video database systems; Content based retrieval
Achieving simultaneous distribution control and privacy protection for Internet media delivery,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-44249122379&doi=10.1145%2f1352012.1352013&partnerID=40&md5=5dcdaaa9998b061a52769918ea8bb351,"Massive Internet media distribution demands prolonged continuous consumption of networking and disk bandwidths in large capacity. Many proxy-based Internet media distribution algorithms and systems have been proposed, implemented, and evaluated to address the scalability and performance issue. However, few of them have been used in practice, since two important issues are not satisfactorily addressed. First, existing proxy-based media distribution architectures lack an efficient media distribution control mechanism. Without copyright protection, content providers are hesitant to use proxy-based fast distribution techniques. Second, little has been done to protect client privacy during content accesses on the Internet. Straightforward solutions to address these two issues independently lead to conflicts. For example, to enforce distribution control, only legitimate users should be granted access rights. However, this normally discloses more information (such as which object the client is accessing) other than the client identity, which conflicts with the client's desire for privacy protection. In this article, we propose a unified proxy-based media distribution protocol to effectively address these two problems simultaneously. We further design a set of new algorithms in a cooperative proxy environment where our proposed scheme works efficiently and practically. Simulation-based experiments are conducted to extensively evaluate the proposed system. Preliminary results demonstrate the effectiveness of our proposed strategy. © 2008 ACM.",Cooperative proxy; Distribution control; Media delivery; Privacy; Proxy caching,Bandwidth; Information services; Internet; User interfaces; Web services; Cooperative proxy; Distribution control; Media delivery; Proxy caching; Data privacy
Multimedia streaming using multiple TCP connections,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-44249126466&doi=10.1145%2f1352012.1352016&partnerID=40&md5=9fb0024df705450ed699ed2f5058893c,"In recent years, multimedia applications over the Internet become increasingly popular. However, packet loss, delay, and time-varying bandwidth of the Internet have remained the major problems for multimedia streaming applications. As such, a number of approaches, including network infrastructure and protocol, source and channel coding, have been proposed to either overcome or alleviate these drawbacks of the Internet. In this article, we propose the MultiTCP system, a receiver-driven, TCP-based system for multimedia streaming over the Internet. Our proposed algorithm aims at providing resilience against short term insufficient bandwidth by using multiple TCP connections for the same application. Our proposed system enables the application to achieve and control the desired sending rate during congested periods, which cannot be achieved using traditional TCP. Finally, our proposed system is implemented at the application layer, and hence, no kernel modification to TCP is necessary. We analyze the proposed system, and present simulation and experimental results to demonstrate its advantages over the traditional single-TCP-based approach. © 2008 ACM.",Multimedia streaming,Bandwidth; Data transfer; Multimedia services; Packet loss; Transmission control protocol; Multimedia streaming; Network infrastructure; Multimedia systems
Feature synthesized em algorithm for image retrieval,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-44249127953&doi=10.1145%2f1352012.1352014&partnerID=40&md5=7ea9a1ed5c486868a3722ecc8ec2f0b7,"As a commonly used unsupervised learning algorithm in Content-Based Image Retrieval (CBIR), Expectation-Maximization (EM) algorithm has several limitations, including the curse of dimensionality and the convergence at a local maximum. In this article, we propose a novel learning approach, namely Coevolutionary Feature Synthesized Expectation-Maximization (CFS-EM), to address the above problems. The CFS-EM is a hybrid of coevolutionary genetic programming (CGP) and EM algorithm applied on partially labeled data. CFS-EM is especially suitable for image retrieval because the images can be searched in the synthesized low-dimensional feature space, while a kernel-based method has to make classification computation in the original high-dimensional space. Experiments on real image databases show that CFS-EM outperforms Radial Basis Function Support Vector Machine (RBF-SVM), CGP, Discriminant-EM (D-EM) and Transductive-SVM (TSVM) in the sense of classification performance and it is computationally more efficient than RBF-SVM in the query phase. © 2008 ACM.",Coevolutionary feature synthesis; Content-based image retrieval; Expectation maximization; Semi-supervised learning,Content based retrieval; Genetic algorithms; Genetic programming; Optimization; Supervised learning; Coevolutionary feature synthesis; Expectation maximization; Semisupervised learning; Feature extraction
On the accuracy and complexity of rate-distortion models for fine-grained scalable video sequences,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-44249090915&doi=10.1145%2f1352012.1352019&partnerID=40&md5=39ff7cde465f2d5db73534d439ff9bb2,"Rate-distortion (R-D) models are functions that describe the relationship between the bitrate and expected level of distortion in the reconstructed video stream. R-D models enable optimization of the received video quality in different network conditions. Several R-D models have been proposed for the increasingly popular fine-grained scalable video sequences. However, the models' relative performance has not been thoroughly analyzed. Moreover, the time complexity of each model is not known, nor is the range of bitrates in which the model produces valid results. This lack of quantitative performance analysis makes it difficult to select the model that best suits a target streaming system. In this article, we classify, analyze, and rigorously evaluate all R-D models proposed for FGS coders in the literature. We classify R-D models into three categories: analytic, empirical, and semi-analytic. We describe the characteristics of each category. We analyze the R-D models by following their mathematical derivations, scrutinizing the assumptions made, and explaining when the assumptions fail and why. In addition, we implement all R-D models, a total of eight, and evaluate them using a diverse set of video sequences. In our evaluation, we consider various source characteristics, diverse channel conditions, different encoding/decoding parameters, different frame types, and several performance metrics including accuracy, range of applicability, and time complexity of each model. We also present clear systematic ways (pseudo codes) for constructing various R-D models from a given video sequence. Based on our experimental results, we present a justified list of recommendations on selecting the best R-D models for video-on-demand, video conferencing, real-time, and peer-to-peer streaming systems. © 2008 ACM.",Fine-grained scalable coding; Multimedia streaming; Rate-distortion models,Image coding; Image quality; Mathematical models; Multimedia systems; Optimization; Parameter estimation; Fine-grained scalable coding; Multimedia streaming; Rate-distortion models; Video signal processing
Multimedia streaming via TCP: An analytic performance study,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-44249098296&doi=10.1145%2f1352012.1352020&partnerID=40&md5=035332c6df7fbb3a63c44832ddfc506a,"TCP is widely used in commercial multimedia streaming systems, with recent measurement studies indicating that a significant fraction of Internet streaming media is currently delivered over HTTP/TCP. These observations motivate us to develop analytic performance models to systematically investigate the performance of TCP for both live and stored-media streaming. We validate our models via ns simulations and experiments conducted over the Internet. Our models provide guidelines indicating the circumstances under which TCP streaming leads to satisfactory performance, showing, for example, that TCP generally provides good streaming performance when the achievable TCP throughput is roughly twice the media bitrate, with only a few seconds of startup delay. © 2008 ACM.",Multimedia streaming; Performance modeling,Computer simulation; HTTP; Mathematical models; Multimedia services; Transmission control protocol; Multimedia streaming; Performance modeling; Multimedia systems
Distributed musical performances: Architecture and stream management,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-44249094186&doi=10.1145%2f1352012.1352018&partnerID=40&md5=cc2d15b9adbbea20b036845562975af4,"An increasing number of novel applications produce a rich set of different data types that need to be managed efficiently and coherently. In this article we present our experience with designing and implementing a data management infrastructure for a distributed immersive performance (DIP) application. The DIP project investigates a versatile framework for the capture, recording, and replay of video, audio, and MIDI (Musical Instrument Digital Interface) streams in an interactive environment for collaborative music performance. We are focusing on two classes of data streams that are generated within this environment. The first category consists of high-resolution isochronous media streams, namely audio and video. The second class comprises MIDI data produced by electronic instruments. MIDI event sequences are alphanumeric in nature and fall into the category of the data streams that have been of interest to data management researchers in recent years. We present our data management architecture, which provides a repository for all DIP data. Streams of both categories need to be acquired, transmitted, stored, and replayed in real time. Data items are correlated across different streams with temporal indices. The audio and video streams are managed in our own High-performance Data Recording Architecture (HYDRA), which integrates multistream recording and retrieval in a consistent manner. This paper reports on the practical issues and challenges that we encountered during the design, implementation and experimental phases of our prototype. We also present some analysis results and discuss future extensions for the architecture. © 2008 ACM.",Distributed immersive performance; Multimedia storage; Multimodal data recorder; Networked musical performance,Data recording; Data structures; Information management; Multimedia systems; Musical instruments; Distributed immersive performance; Multimedia storage; Multimodal data recorders; Networked musical performance; Computer music
Audio keywords generation for sports video analysis,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-44249121489&doi=10.1145%2f1352012.1352015&partnerID=40&md5=a4c6c47e8f9a54cfe6fc868ff9c99dc6,"Sports video has attracted a global viewership. Research effort in this area has been focused on semantic event detection in sports video to facilitate accessing and browsing. Most of the event detection methods in sports video are based on visual features. However, being a significant component of sports video, audio may also play an important role in semantic event detection. In this paper, we have borrowed the concept of the keyword from the text mining domain to define a set of specific audio sounds. These specific audio sounds refer to a set of game-specific sounds with strong relationships to the actions of players, referees, commentators, and audience, which are the reference points for interesting sports events. Unlike low-level features, audio keywords can be considered as a mid-level representation, able to facilitate high-level analysis from the semantic concept point of view. Audio keywords are created from low-level audio features with learning by support vector machines. With the help of video shots, the created audio keywords can be used to detect semantic events in sports video by Hidden Markov Model (HMM) learning. Experiments on creating audio keywords and, subsequently, event detection based on audio keywords have been very encouraging. Based on the experimental results, we believe that the audio keyword is an effective representation that is able to achieve satisfying results for event detection in sports video. Application in three sports types demonstrates the practicality of the proposed method. © 2008 ACM.",Audio keywords; Event detection; Semantics analysis; Sports video analysis; Support vector machines,Audio acoustics; Data mining; Object recognition; Support vector machines; Text processing; Audio keywords; Event detection; Semantics analysis; Sports video analysis; Image analysis
Partial query resolution for animation authoring,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049109424&doi=10.1145%2f1324287.1324291&partnerID=40&md5=cdce55dac32e798197b9244d36b61b2f,"Animations are a part of multimedia and techniques such as motion mapping and inverse kinematics aid in reusing models and motion sequences to create new animations. This reuse approach is facilitated by the use of content-based retrieval techniques that often require fuzzy query resolution. Most fuzzy query resolution approaches work on all the attributes of the query to minimize the database access cost thus resulting in an unsatisfactory result set. It turns out that the query resolution can be carried out in a partial manner to achieve user satisfactory results and aid in easy authoring. In this article, we present two partial fuzzy query resolution approaches, one that results in high-quality animations and the other that produces results with decreasing number of satisfied conditions in the query. © 2008 ACM.",Aggregation function; Animation toolkit; Fuzzy query; Multimedia authoring; Partial ordering; Top-k query,Animation; Computer aided software engineering; Computer software reusability; Fuzzy sets; Multimedia systems; Query processing; Aggregation functions; Animation toolkits; Fuzzy query; Multimedia authoring; Partial ordering; Top-k query; Image resolution
A revenue-rewarding scheme of providing incentive for cooperative proxy caching for media streaming systems,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049093444&doi=10.1145%2f1324287.1324292&partnerID=40&md5=cae380ecd4660b01dbc7a8162148356e,"Network entities cooperating together can improve system performance of media streaming. In this paper, we address the ""incentive issue"" of a cooperative proxy caching system and how to motivate each proxy to provide cache space to the system. To encourage proxies to participate, we propose a ""revenue-rewarding scheme"" to credit the cooperative proxies according to the resources they contribute. A game-theoretic model is used to analyze the interactions among proxies under the revenue-rewarding scheme. We propose two cooperative game settings that lead to optimal situations. In particular, (1) We propose a distributed incentive framework for peers to participate in resource contribution for media streaming; (2) Proxies are encouraged to cooperate under the revenue-rewarding scheme; (3) Profit and social welfare are maximized in these cooperative games; and (4) Cost-effective resource allocation is achieved in these cooperative games. Large scale simulation is carried out to validate and verify the merits of our proposed incentive schemes. © 2008 ACM.",Game-theoretic analysis; Incentive mechanism; Nash equilibrium; Pricing; Resource allocation,Cache memory; Cost effectiveness; Game theory; Large scale systems; Mathematical models; Resource allocation; Game-theoretic analysis; Incentive mechanisms; Nash equilibrium; Network entities; Video streaming
Optimization of interactive visual-similarity-based search,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049088833&doi=10.1145%2f1324287.1324294&partnerID=40&md5=636398f3b40a67a5e708789a0700c823,"At one end of the spectrum, research in interactive content-based retrieval concentrates on machine learning methods for effective use of relevance feedback. On the other end, the information visualization community focuses on effective methods for conveying information to the user. What is lacking is research considering the information visualization and interactive retrieval as truly integrated parts of one content-based search system. In such an integrated system, there are many degrees of freedom like the similarity function, the number of images to display, the image size, different visualization modes, and possible feedback modes. To base the optimal values for all of those on user studies is unfeasible. We therefore develop search scenarios in which tasks and user actions are simulated. From there, the proposed scheme is optimized based on objective constraints and evaluation criteria. In such a manner, the degrees of freedom are reduced and the remaining degrees can be evaluated in user studies. In this article, we present a system that integrates advanced similarity based visualization with active learning. We have performed extensive experimentation on interactive category search with different image collections. The results using the proposed simulation scheme show that indeed the use of advanced visualization and active learning pays off in all of these datasets. © 2008 ACM.",Active learning; Interactive search; Similarity based visualization,Content based retrieval; Data visualization; Image analysis; Interactive computer graphics; Learning systems; Visual servoing; Active learning; Content-based search; Interactive search; Similarity based visualization; Tabu search
Video quality estimation in wireless IP networks: Algorithms and applications,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049083439&doi=10.1145%2f1324287.1324290&partnerID=40&md5=740e4e931aa26b896a088582f58e3cec,"This article proposes three methods to estimate the distortion deriving from packet losses in wireless video communication. The proposed methods take into account the short-term properties of the encoded video sequences. A suitable set of functions is adopted to model the distortion envelope resulting from multiple losses. The estimated performance is compared with the actual distortion, evaluated by decoding the received sequence with a properly designed decoder. Numerical results confirm the accuracy of the proposed models in approximating the actual Mean Square Error (MSE) for a wide range of loss rates. Some applications of the proposed algorithms are presented. © 2008 ACM.",Distortion estimation; Error-concealment; Error-resilience; H.264; Packet loss rate; Real time video; Wireless networks,Algorithms; Internet protocols; Packet loss; Real time systems; Signal distortion; Wireless networks; Distortion estimation; Error resilience; Error-concealment; Packet loss rates; Real time video; Image quality
Rate-distortion optimized streaming of fine-grained scalable video sequences,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049099170&doi=10.1145%2f1324287.1324289&partnerID=40&md5=965a71f317b3ef5372125e1c23aa2cbf,"We present optimal schemes for allocating bits of fine-grained scalable video sequences among multiple senders streaming to a single receiver. This allocation problem is critical in optimizing the perceived quality in peer-to-peer and distributed multi-server streaming environments. Senders in such environments are heterogeneous in their outgoing bandwidth and they hold different portions of the video stream. We first formulate and optimally solve the problem for individual frames, then we generalize to the multiple frame case. Specifically, we formulate the allocation problem as an optimization problem, which is nonlinear in general. We use rate-distortion models in the formulation to achieve the minimum distortion in the rendered video, constrained by the outgoing bandwidth of senders, availability of video data at senders, and incoming bandwidth of receiver. We show how the adopted rate-distortion models transform the nonlinear problem to an integer linear programming (ILP) problem. We then design a simple rounding scheme that transforms the ILP problem to a linear programming (LP) one, which can be solved efficiently using common optimization techniques such as the Simplex method. We prove that our rounding scheme always produces a feasible solution, and the solution is within a negligible margin from the optimal solution. We also propose a new algorithm (FGSAssign) for the single-frame allocation problem that runs in O(nlog n) steps, where n is the number of senders. We prove that FGSAssign is optimal. Furthermore, we propose a heuristic algorithm (mFGSAssign) that produces near-optimal solutions for the multiple-frame case, and runs an order of magnitude faster than the optimal one. Because of its short running time, mFGSAssign can be used in real time. Our experimental study validates our analytical analysis and shows the effectiveness of our allocation algorithms in improving the video quality. © 2008 ACM.",Distributed streaming; FGS; Fine-grained scalable streaming; Peer-to-peer streaming; Rate-distortion models; Rate-distortion optimized streaming; Video streaming,Bandwidth; Distributed computer systems; Mathematical models; Optimization; Signal distortion; Signal receivers; Distributed streaming; Fine-grained scalable streaming (FGS); Peer-to-peer streaming; Rate-distortion models; Rate-distortion optimized streaming; Video streaming
Incorporating feature hierarchy and boosting to achieve more effective classifier training and concept-oriented video summarization and skimming,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049084529&doi=10.1145%2f1324287.1324288&partnerID=40&md5=a6d0027726833c75925fdaf61a9f3bde,"For online medical education purposes, we have developed a novel scheme to incorporate the results of semantic video classification to select the most representative video shots for generating concept-oriented summarization and skimming of surgery education videos. First, salient objects are used as the video patterns for feature extraction to achieve a good representation of the intermediate video semantics. The salient objects are defined as the salient video compounds that can be used to characterize the most significant perceptual properties of the corresponding real world physical objects in a video, and thus the appearances of such salient objects can be used to predict the appearances of the relevant semantic video concepts in a specific video domain. Second, a novel multi-modal boosting algorithm is developed to achieve more reliable video classifier training by incorporating feature hierarchy and boosting to dramatically reduce both the training cost and the size of training samples, thus it can significantly speed up SVM (support vector machine) classifier training. In addition, the unlabeled samples are integrated to reduce the human efforts on labeling large amount of training samples. Finally, the results of semantic video classification are incorporated to enable concept-oriented video summarization and skimming. Experimental results in a specific domain of surgery education videos are provided. © 2008 ACM.",Concept-oriented video skimming; Feature hierarchy; Multi-modal boosting; Salient objects; Semantic video classification; Unlabeled samples,Hierarchical systems; Modal analysis; Sampling; Semantic Web; Support vector machines; Video signal processing; Concept-oriented video skimming; Feature hierarchy; Multi-modal boosting; Salient objects; Semantic video classification; Unlabeled samples; Image classification
An automated end-to-end lecture capture and broadcasting system,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049098024&doi=10.1145%2f1324287.1324293&partnerID=40&md5=fe5607173b258bdbf52bcc489cc4bd9a,"Remote viewing of lectures presented to a live audience is becoming increasingly popular. At the same time, the lectures can be recorded for subsequent on-demand viewing over the Internet. Providing such services, however, is often prohibitive due to the labor-intensive cost of capturing and pre/post-processing. This article presents a complete automated end-to-end system that supports capturing, broadcasting, viewing, archiving and searching of presentations. Specifically, we describe a system architecture that minimizes the pre- and post-production time, and a fully automated lecture capture system called iCam2 that synchronously captures all contents of the lecture, including audio, video, and presentation material. No staff is needed during lecture capture and broadcasting, so the operational cost of the system is negligible. The system has been used on a daily basis for more than 4 years, during which 522 lectures have been captured. These lectures have been viewed over 20,000 times. © 2008 ACM.",Automated lecture capture; Lecture broadcasting; Live/on-demand broadcasting,Data acquisition; Internet; Network architecture; Optimization; Production engineering; Automated lecture capture; Lecture broadcasting; Live/on-demand broadcasting; Radio broadcasting
Hierarchical video patching with optimal server bandwidth,2008,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049085645&doi=10.1145%2f1324287.1324295&partnerID=40&md5=a951b81ce261e18359b28e6ddcc3a4a6,"Video patching is a way for transporting true video-on-demand, that is, instantaneous without any delay, from a video server to several clients. Instead of sending a unique stream to each newly arriving client, clients share as many multicast transmissions as possible, and are serviced only those parts of the video that they have missed. We present a novel video patching scheme using hierarchies of patches. Our scheme minimizes the bandwidth needed by the video server, and may result in the fact that clients receive several streams in parallel. We show analytically that for Poisson arrival our algorithm achieves the optimal possible server bandwidth for all schemes where clients share multicast transmissions. We also show, how our approach can be combined with batching. This combination requires less server bandwidth than all fixed start point periodic broadcast algorithms. © 2008 ACM.",Batching; Server bandwidth; True video-on-demand; Video patching,Bandwidth; Delay circuits; Hierarchical systems; Multicasting; Optimization; Video streaming; Multicast transmissions; Server bandwidth; Video patching; Video servers; Video on demand
Towards efficient context-specific video coding based on gaze-tracking analysis,2007,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-38349162308&doi=10.1145%2f1314303.1314307&partnerID=40&md5=d8303204c58b9c240cb1ef7f56ba8611,"This article discusses a framework for model-based, context-dependent video coding based on exploitation of characteristics of the human visual system. The system utilizes variable-quality coding based on priority maps which are created using mostly context-dependent rules. The technique is demonstrated through two case studies of specific video context, namely open signed content and football sequences. Eye-tracking analysis is employed for identifying the characteristics of each context, which are subsequently exploited for coding purposes, either directly or through a gaze prediction model. The framework is shown to achieve a considerable improvement in coding efficiency. © 2007 ACM.",Applications; Context-based video coding; Eye tracking; multimedia perceptual quality; Subjective video quality; Transformation of eye movements into useful knowledge,Electronic document identification systems; Image quality; Multimedia services; Signal filtering and prediction; Eye tracking; Gaze prediction model; Multimedia perceptual quality; Image coding
Introduction to special issue on eye-tracking applications in multimedia systems,2007,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-38349167558&doi=10.1145%2f1314303.1314304&partnerID=40&md5=349a2b8bea677bdd513f2fcd1e087499,[No abstract available],,
Robust tracking and remapping of eye appearance with passive computer vision,2007,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-38349156115&doi=10.1145%2f1314303.1314305&partnerID=40&md5=997c221c1dcbef08c825e353764ec088,"A single-camera iris-tracking and remapping approach based on passive computer vision is presented. Tracking is aimed at obtaining accurate and robust measurements of the iris/pupil position. To this purpose, a robust method for ellipse fitting is used, employing search constraints so as to achieve better performance with respect to the standard RANSAC algorithm. Tracking also embeds an iris localization algorithm (working as a bootstrap multiple-hypotheses generation step), and a blink detector that can detect voluntary eye blinks in human-computer interaction applications. On-screen remapping incorporates a head-tracking method capable of compensating for small user-head movements. The approach operates in real time under different light conditions and in the presence of distractors. An extensive set of experiments is presented and discussed. In particular, an evaluation method for the choice of layout of both hardware components and calibration points is described. Experiments also investigate the importance of providing a visual feedback to the user, and the benefits gained from performing head compensation, especially during image-to-screen map calibration. © 2007 ACM.",Eye blink detection; Eye tracking and remapping; Eye-driven human-computer interaction; Robust fitting,Algorithms; Cameras; Computer vision; Feedback; Human computer interaction; User interfaces; Eye blink detection; Robust fitting; Eye movements
The conductor interaction method,2007,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-38349133033&doi=10.1145%2f1314303.1314312&partnerID=40&md5=ac208fec809d90f53d7faa1031979e9a,"Computers have increasingly become part of our everyday lives, with many activities either involving their direct use or being supported by one. This has prompted research into developing methods and mechanisms to assist humans in interacting with computers (human-computer interaction, or HCI). A number of HCI techniques have been developed over the years, some of which are quite old but continue to be used, and some more recent and still evolving. Many of these interaction techniques, however, are not natural in their use and typically require the user to learn a new means of interaction. Inconsistencies within these techniques and the restrictions they impose on user creativity can also make such interaction techniques difficult to use, especially for novice users. This article proposes an alternative interaction method, the conductor interaction method (CIM), which aims to provide a more natural and easier-to-learn interaction technique. This novel interaction method extends existing HCI methods by drawing upon techniques found in human-human interaction. It is argued that the use of a two-phased multimodal interaction mechanism, using gaze for selection and gesture for manipulation, incorporated within a metaphor-based environment, can provide a viable alternative for interacting with a computer (especially for novice users). Both the model and an implementation of the CIM within a system are presented in this article. This system formed the basis of a number of user studies that have been performed to assess the effectiveness of the CIM, the findings of which are discussed in this work. © 2007 ACM.",Gaze- and gesture-based interfaces; Human-computer interaction,Gesture recognition; Learning systems; Mathematical models; User interfaces; Conductor interaction method (CIM); Novice users; Human computer interaction
Using geometric properties of topographic manifold to detect and track eyes for human-computer interaction,2007,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-38349176504&doi=10.1145%2f1314303.1314306&partnerID=40&md5=b08b6954d3f0a255120b8bd3031d2d54,"Automatic eye detection and tracking is an important component for advanced human-computer interface design. Accurate eye localization can help develop a successful system for face recognition and emotion identification. In this article, we propose a novel approach to detect and track eyes using geometric surface features on topographic manifold of eye images. First, in the joint spatial-intensity domain, a facial image is treated as a 3D terrain surface or image topographic manifold. In particular, eye regions exhibit certain intrinsic geometric traits on this topographic manifold, namely, the pit-labeled center and hillside-like surround regions. Applying a terrain classification procedure on the topographic manifold of facial images, each location of the manifold can be labeled to generate a terrain map. We use the distribution of terrain labels to represent the eye terrain pattern. The Bhattacharyya affinity is employed to measure the distribution similarity between two topographic manifolds. Based on the Bhattacharyya kernel, a support vector machine is applied for selecting proper eye pairs from the pit-labeled candidates. Second, given detected eyes on the first frame of a video sequence, a mutual-information-based fitting function is defined to describe the similarity between two terrain surfaces of neighboring frames. By optimizing the fitting function, eye locations are updated for subsequent frames. The distinction of the proposed approach lies in that both eye detection and eye tracking are performed on the derived topographic manifold, rather than on an original-intensity image domain. The robustness of the approach is demonstrated under various imaging conditions and with different facial appearances, using both static images and video sequences without background constraints. © 2007 ACM.",Bhattacharyya affinity; Eye detection; Eye tracking; Mutual information; Topographic manifold,Classification (of information); Computational geometry; Face recognition; Human computer interaction; Topography; User interfaces; Bhattacharyya affinity; Mutual information; Topographic manifold; Eye movements
An assessment of eye-gaze potential within immersive virtual environments,2007,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-38349183302&doi=10.1145%2f1314303.1314311&partnerID=40&md5=5c9f2ce1bc5cc184b668a0e8bf734d04,"In collaborative situations, eye gaze is a critical element of behavior which supports and fulfills many activities and roles. In current computer-supported collaboration systems, eye gaze is poorly supported. Even in a state-of-the-art video conferencing system such as the access grid, although one can see the face of the user, much of the communicative power of eye gaze is lost. This article gives an overview of some preliminary work that looks towards integrating eye gaze into an immersive collaborative virtual environment and assessing the impact that this would have on interaction between the users of such a system. Three experiments were conducted to assess the efficacy of eye gaze within immersive virtual environments. In each experiment, subjects observed on a large screen the eye-gaze behavior of an avatar. The eye-gaze behavior of that avatar had previously been recorded from a user with the use of a head-mounted eye tracker. The first experiment was conducted to assess the difference between users' abilities to judge what objects an avatar is looking at with only head gaze being viewed and also with eye- and head-gaze data being displayed. The results from the experiment show that eye gaze is of vital importance to the subjects, correctly identifying what a person is looking at in an immersive virtual environment. The second experiment examined whether a monocular or binocular eye-tracker would be required. This was examined by testing subjects' ability to identify where an avatar was looking from their eye direction alone, or by eye direction combined with convergence. This experiment showed that convergence had a significant impact on the subjects' ability to identify where the avatar was looking. The final experiment looked at the effects of stereo and mono-viewing of the scene, with the subjects being asked to identify where the avatar was looking. This experiment showed that there was no difference in the subjects' ability to detect where the avatar was gazing. This is followed by a description of how the eye-tracking system has been integrated into an immersive collaborative virtual environment and some preliminary results from the use of such a system. © 2007 ACM.",Eye gaze; Immersive virtual environments,Convergence of numerical methods; Electronic document identification systems; Grid computing; User interfaces; Video conferencing; Eye-tracking system; Immersive virtual environments; Virtual reality
Detecting eye fixations by projection clustering,2007,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-38349116843&doi=10.1145%2f1314303.1314308&partnerID=40&md5=5e53c5041a4fe5fde87d91b1450e04b8,"Eye movements are certainly the most natural and repetitive movement of a human being. The most mundane activity, such as watching television or reading a newspaper, involves this automatic activity which consists of shifting our gaze from one point to another. Identification of the components of eye movements (fixations and saccades) is an essential part in the analysis of visual behavior because these types of movements provide the basic elements used by further investigations of human vision. However, many of the algorithms that detect fixations present a number of problems. In this article, we present a new fixation identification technique that is based on clustering of eye positions, using projections and projection aggregation applied to static pictures. We also present a new method that computes dispersion of eye fixations in videos considering a multiuser environment. To demonstrate the performance and usefulness of our approach we discuss our experimental work with two different applications: on fixed image and video. © 2007 ACM.",Eye fixations; Interaction modeling; Projected clustering; Static pictures; Videos,Algorithms; Cluster analysis; Human computer interaction; Image processing; Television; Interaction modeling; Projected clustering; Static pictures; Eye movements
How late can you update gaze-contingent multiresolutional displays without detection,2007,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-38349169893&doi=10.1145%2f1314303.1314310&partnerID=40&md5=781111e61aa3287945f96642cb2eef8e,"This study investigated perceptual disruptions in gaze-contingent multiresolutional displays (GCMRDs) due to delays in updating the center of highest resolution after an eye movement. GCMRDs can be used to save processing resources and transmission bandwidth in many types of single-user display applications, such as virtual reality, video-telephony, simulators, and remote piloting. The current study found that image update delays as late as 60 ms after an eye movement did not significantly increase the detectability of image blur and/or motion transients due to the update. This is good news for designers of GCMRDs, since 60 ms is ample time to update many GCMRDs after an eye movement without disrupting perception. The study also found that longer eye movements led to greater blur and/or transient detection due to moving the eyes further into the low-resolution periphery, effectively reducing the image resolution at fixation prior to the update. In GCMRD applications where longer saccades are more likely (e.g., displays with relatively large distances between objects), this problem could be overcome by increasing the size of the region of highest resolution. © 2007 ACM.",Area of interest; Blur detection; Contrast thresholds; Display updates; Eye movements; eye tracking; Foveated; Foveation; Gaze-contingent; Level-of-detail; Multiresolution; Perceptual compression; Saccades; Saccadic suppression; Visual perception,Bandwidth; Multiresolution analysis; Problem solving; Virtual reality; Gaze-contingent multiresolutional displays (GCMRD); Perceptual compression; Visual perception; Image resolution
"Foveated gaze-contingent displays for peripheral LOD management, 3D visualization, and stereo imaging",2007,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-38349100181&doi=10.1145%2f1314303.1314309&partnerID=40&md5=93009906738e7a45b1f64abe0bfe8fd6,"Advancements in graphics hardware have allowed development of hardware-accelerated imaging displays. This article reviews techniques for real-time simulation of arbitrary visual fields over still images and video. The goal is to provide the vision sciences and perceptual graphics communities techniques for the investigation of fundamental processes of visual perception. Classic gaze-contingent displays used for these purposes are reviewed and for the first time a pixel shader is introduced for display of a high-resolution window over peripherally degraded stimulus. The pixel shader advances current state-of-the-art by allowing real-time processing of still or streamed images, obviating the need for preprocessing or storage. © 2007 ACM.",Eye tracking; Foveation; Gaze-contingent displays; Level-of-detail,Computer hardware; Graphic methods; Pixels; Real time systems; Three dimensional; Gaze-contingent displays; Level-of-detail; Vision sciences; Stereo vision
Robust content-based MPEG-4 XMT scene structure authentication and multimedia content location,2007,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548706526&doi=10.1145%2f1236471.1236477&partnerID=40&md5=f116af8e230285c2ea51d67e221ec0f7,"For the past decade, there have been numerous research works focusing on the protection of digital images, audio, video, 3D virtual scenes, and software data from unauthorized use and distribution. With the emerging technology of the MPEG-4 standard, MPEG-4 scenes that may include images, video, audio, and 3D objects can easily be built using the text-based MPEG-4 XMT standard. XMT allows content authors to exchange their content with other authors, tools, or service providers and facilitates interoperability with MPEG-4, X3D, and SMIL. In order for owners and designers to protect and/or authenticate their work, some form of security needs to be applied into the MPEG-4 XMT structure and its media content. Unlike images or videos, watermarking an XMT structure is not an easy task, since the structure contains no noise components to embed the watermark. This article is the first one proposing a novel robust algorithm for the authentication of a given MPEG-4 XMT structured scene and the location of its multimedia content. © 2007 ACM.",MPEG-4; Multimedia; Polynomial; Pseudorandom sequences; Steganography; VRML; Watermarking; XML; XMT,Digital watermarking; Multimedia systems; Polynomials; XML; Pseudorandom sequences; Steganography; XMT; Image compression
Segmentation and recognition of motion streams by similarity search,2007,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548729146&doi=10.1145%2f1236471.1236475&partnerID=40&md5=a6697decaff81f82983e7f519dcc3374,"Fast and accurate recognition of motion data streams from gesture sensing and motion capture devices has many applications and is the focus of this article. Based on the analysis of the geometric structures revealed by singular value decompositions (SVD) of motion data, a similarity measure is proposed for simultaneously segmenting and recognizing motion streams. A direction identification approach is explored to further differentiate motions with similar data geometric structures. Experiments show that the proposed similarity measure can segment and recognize motion streams of variable lengths with high accuracy, without knowing beforehand the number of motions in a stream. © 2007 ACM.",Gesture recognition; Motion capture; Pattern analysis; Principal component analysis; Segmentation; Similarity measures; Singular value decomposition,Image segmentation; Principal component analysis; Search engines; Singular value decomposition; Motion capture; Pattern analysis; Similarity measures; Gesture recognition
Efficient sampling of training set in large and noisy multimedia data,2007,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548749801&doi=10.1145%2f1236471.1236473&partnerID=40&md5=76655d9912b87e4441f2d0f4f2456979,"As the amount of multimedia data is increasing day-by-day thanks to less expensive storage devices and increasing numbers of information sources, machine learning algorithms are faced with large-sized and noisy datasets. Fortunately, the use of a good sampling set for training influences the final results significantly. But using a simple random sample (SRS) may not obtain satisfactory results because such a sample may not adequately represent the large and noisy dataset due to its blind approach in selecting samples. The difficulty is particularly apparent for huge datasets where, due to memory constraints, only very small sample sizes are used. This is typically the case for multimedia applications, where data size is usually very large. In this article we propose a new and efficient method to sample of large and noisy multimedia data. The proposed method is based on a simple distance measure that compares the histograms of the sample set and the whole set in order to estimate the representativeness of the sample. The proposed method deals with noise in an elegant manner which SRS and other methods are not able to deal with. We experiment on image and audio datasets. Comparison with SRS and other methods shows that the proposed method is vastly superior in terms of sample representativeness, particularly for small sample sizes although time-wise it is comparable to SRS, the least expensive method in terms of time. © 2007 ACM.",Audio event identification; Histogram; Image classification; Noise; Sampling,Data storage equipment; Data structures; Image classification; Multimedia services; Sampling; Signal to noise ratio; Audio event identification; Histograms; Simple random sample (SRS); Learning systems
An open architecture for transport-level protocol coordination in distributed multimedia applications,2007,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548723895&doi=10.1145%2f1236471.1236476&partnerID=40&md5=58ad9589a7baece92104628b1f654a4e,"We consider the problem of flow coordination in distributed multimedia applications. Most transport-level protocols are designed to operate independently and lack mechanisms for sharing information with other flows and coordinating data transport in various ways. This limitation becomes problematic in distributed applications that employ numerous flows between two computing clusters sharing the same intermediary forwarding path across the Internet. In this article, we propose an open architecture that supports the sharing of network state information, peer flow information, and application-specific information. Called simply the coordination protocol (CP), the scheme facilitates coordination of network resource usage across flows belonging to the same application, as well as aiding other types of coordination. The effectiveness of our approach is illustrated in the context of multistreaming in 3D tele-immersion where consistency of network information across flows both greatly improves frame transport synchrony and minimizes buffering delay. © 2007 ACM.",Distributed applications; Flow coordination; Network protocols,Data transfer; Multimedia systems; Network architecture; Problem solving; Distributed applications; Flow coordination; Network protocols
Critical causal order of events in distributed virtual environments,2007,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548742280&doi=10.1145%2f1236471.1236474&partnerID=40&md5=1e640eb02d32fd33f650cd0ddc6585b4,"We investigate the causal order of events in distributed virtual environments (DVEs). We first define the critical causal order relation among the events. Then, we propose some mechanisms to enhance the prevalent RO (receive order delivery) mechanism in DVEs so that the real-time property of DVEs is preserved while the critical causal order violations are reduced. These mechanisms are implemented as a middleware. Experimental results show that the middleware performs well in reducing the critical causality violations in simulation and incurs little processing overhead. © 2007 ACM.",Causal order; Distributed simulation; Virtual environments,Distributed computer systems; Middleware; Real time systems; Causal order; Distributed simulation; Order delivery; Virtual reality
Exploring many-to-one speech-to-text correlation for web-based language learning,2007,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548771776&doi=10.1145%2f1236471.1236472&partnerID=40&md5=5daba516d2fe9bf83510e74606b37ddd,"This article investigates the correlations between multimedia objects (particularly speech and text) involved in language lectures in order to design an effective presentation mechanism for web-based learning. The cross-media correlations are classified into implicit relations (retrieved by computing) and explicit relations (recorded during the preprocessing stage). The implicit temporal correlation between speech and text is primarily to help to negotiate supplementary lecture navigations like tele-pointer movement, lips-sync movement, and content scrolling. We propose a speech-text alignment framework, using an iterative algorithm based on local alignment, to probe many-to-one temporal correlations, and not the one-to-one only. The proposed framework is a more practical method for analyzing general language lectures, and the algorithm's time complexity conforms to the best-possible computation cost, O(nm), without introducing additional computation. In addition, we have shown the feasibility of creating vivid presentations by exploiting implicit relations and artificially simulating some explicit media. To facilitate the navigation of integrated multimedia documents, we develop several visualization techniques for describing media correlations, including guidelines for speech-text correlations, visible-automatic scrolling, and levels of detail of timeline, to provide intuitive and easy-to-use random access mechanisms. We evaluated the performance of the analysis method and human perceptions of the synchronized presentation. The overall performance of the analysis method is that about 99.5% of the words analyzed are of a temporal error within 0.5 sec and the subjective evaluation result shows that the synchronized presentation is highly acceptable to human beings. © 2007 ACM.",Analysis and presentation; Computed synchronization; Cross-media correlation; Lips sync; Speech-to-text alignment,Computational methods; Correlation methods; E-learning; Multimedia services; Speech processing; Synchronization; Text processing; Computed synchronization; Cross media correlation; Lips sync; Speech to text alignment; Learning systems
Computational approaches to temporal sampling of video sequences,2007,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250218568&doi=10.1145%2f1230812.1230813&partnerID=40&md5=f44583d8b693c6928cad17dca4912961,"Video key frame extraction is one of the most important research problems for video summarization, indexing, and retrieval. For a variety of applications such as ubiquitous media access and video streaming, the temporal boundaries between video key frames are required for synchronizing visual content with audio. In this article, we define temporal video sampling as a unified process of extracting video key frames and computing their temporal boundaries, and formulate it as an optimization problem. We first provide an optimal approach that minimizes temporal video sampling error using a dynamic programming process. The optimal approach retrieves a key frame hierarchy and all temporal boundaries in O(n4) time and O(n2) space. To further reduce computational complexity, we also provide a suboptimal greedy algorithm that exploits the data structure of a binary heap and uses a novel look-ahead computational technique, enabling all levels of key frames to be extracted with an average-case computational time of O(n log n) and memory usage of O(n). Both the optimal and the greedy methods are free of parameters, thus avoiding the threshold-selection problem that exists in other approaches. We empirically compare the proposed optimal and greedy methods with several existing methods in terms of video sampling error, computational cost, and subjective quality. An evaluation of eight videos of different genres shows that the greedy approach achieves performance very close to that of the optimal approach while drastically reducing computational cost, making it suitable for processing long video sequences in large video databases. © 2007 ACM.",Key frame selection; Temporal video sampling; Ubiquitous media access; Video content analysis; Video summarization,Computational complexity; Computational methods; Image analysis; Multimedia services; Optimization; Sampling; Optimization problems; Temporal video sampling; Ubiquitous media access; Video content analysis; Video summarization; Video streaming
Clustering and searching WWW images using link and page layout analysis,2007,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250189184&doi=10.1145%2f1230812.1230816&partnerID=40&md5=5bad09abc356bf1b8c7aef60f7056244,"Due to the rapid growth of the number of digital images on the Web, there is an increasing demand for an effective and efficient method for organizing and retrieving the available images. This article describes iFind, a system for clustering and searching WWW images. By using a vision-based page segmentation algorithm, a Web page is partitioned into blocks, and the textual and link information of an image can be accurately extracted from the block containing that image. The textual information is used for image indexing. By extracting the page-to-block, block-to-image, block-to-page relationships through link structure and page layout analysis, we construct an image graph. Our method is less sensitive to noisy links than previous methods like PageRank, HITS, and PicASHOW, and hence the image graph can better reflect the semantic relationship between images. Using the notion of Markov Chain, we can compute the limiting probability distributions of the images, ImageRanks, which characterize the importance of the images. The ImageRanks are combined with the relevance scores to produce the final ranking for image search. With the graph models, we can also use techniques from spectral graph theory for image clustering and embedding, or 2-D visualization. Some experimental results on 11.6 million images downloaded from the Web are provided in the article. © 2007 ACM.",Image clustering; Image search; Link analysis; Web mining,Cluster analysis; Data mining; Graphic methods; Image retrieval; Indexing (of information); Probability distributions; Websites; Image clustering; Image graphs; Image indexing; Image search; Web mining; Online searching
A narrative-based abstraction framework for story-oriented video,2007,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250157033&doi=10.1145%2f1230812.1230817&partnerID=40&md5=8444fc8eebc0dfdae1401f430ea0ff60,"This article proposes a novel video abstraction framework for online review services of story-oriented videos such as dramas. Among the many genres of TV programs, a drama is one of the most popularly watched on the Web. The abstracts generated by the proposed framework not only give a summary of a video but also effectively help viewers understand the overall story. In addition, our method is duration-flexible. We get clues about human understanding of a story from scenario writing rules and editorial techniques that are popularly used in the process of video production to explicitly express a narrative, and propose a new video abstraction model, called a Narrative Abstraction Model. The model effectively captures the narrative structure embedded in a story-oriented video and articulates the progress of the story in a weighted directed graph, called a Narrative Structure Graph (NSG). The model provides a basis for a flexible framework for abstract generation using the NSG as the intermediary representation of a video. Different abstracts can be appropriately generated based upon different user requirements. To show the effectiveness of the proposed model and method, we developed a video abstraction system realizing the framework, and successfully applied it to large volumes of TV dramas. The evaluation results show that the proposed framework is a feasible solution for online review services. © 2007 ACM.",Film; Narrative structure; Online review services; Story understanding; Story-oriented; Video abstraction; Video abstraction system,Computational methods; Graphic methods; Multimedia services; Online systems; Television; Narrative Structure Graph (NSG); Online review services; Video abstraction; Video production; Video streaming
Online audio background determination for complex audio environments,2007,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250198271&doi=10.1145%2f1230812.1230814&partnerID=40&md5=5cb1214fd377fde53c95cb2eec232092,"We present a method for foreground/background separation of audio using a background modelling technique. The technique models the background in an online, unsupervised, and adaptive fashion, and is designed for application to long term surveillance and monitoring problems. The background is determined using a statistical method to model the states of the audio over time. In addition, three methods are used to increase the accuracy of background modelling in complex audio environments. Such environments can cause the failure of the statistical model to accurately capture the background states. An entropy-based approach is used to unify background representations fragmented over multiple states of the statistical model. The approach successfully unifies such background states, resulting in a more robust background model. We adaptively adjust the number of states considered background according to background complexity, resulting in the more accurate classification of background models. Finally, we use an auxiliary model cache to retain potential background states in the system. This prevents the deletion of such states due to a rapid influx of observed states that can occur for highly dynamic sections of the audio signal. The separation algorithm was successfully applied to a number of audio environments representing monitoring applications. © 2007 ACM.",Audio analysis; Online background modelling; Surveillance and monitoring,Acoustic signal processing; Algorithms; Computer simulation; Mathematical models; Online systems; Pattern recognition; Statistical methods; Audio analysis; Online background modelling; Statistical models; Audio systems
A piano duo support system for parents to lead children to practice musical performances,2007,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250196126&doi=10.1145%2f1230812.1230815&partnerID=40&md5=4c644cc0fe15f454815fd8da970c5bfc,"In this article, we propose Family Ensemble, a piano duo support system for a musically inept parent and his/her child who is a beginner at playing the piano. The system makes it easier for parents to correctly reproduce a given sequence of pitches along with the child's performance by using score tracking and note-replacement functions. The experiments with this support system showed that the parents can immediately participate in the piano duo. Furthermore, we found that during joint practices using Family Ensemble some subjects discussed musical ideas that they would not have talked about without using the system. © 2007 ACM.",Entertainment; Musical expression; Piano duo; Score tracking; Support system,Acoustic signal processing; Multimedia services; Entertainment; Pitches; Support systems; Musical instruments
Ubiquitous device personalization and use: The next generation of IP multimedia communications,2007,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250200103&doi=10.1145%2f1230812.1230818&partnerID=40&md5=ba280313f07d7d46addebe352343fe77,"Service usage in emerging ubiquitous environments includes seamless and personalized usage of public and private devices discovered in the vicinity of a user. In our work, we describe an architecture for device discovery, device configuration, and the transfer of active sessions between devices. The presented architecture uses the Session Initiation Protocol (SIP) as a standardized, widely used signaling protocol for IP-based multimedia services. Our solution includes support of simple existing devices, split of sessions between devices, user-control of location-based behavior, and handling of security and privacy concerns. We present the implementation and show the feasibility of our work with analytical evaluation and measurements. © 2007 ACM.",Internet multimedia; Location-based services; Mobile communications; Ubiquitous computing,Data privacy; Data transfer; Internet protocols; Mobile telecommunication systems; Personal digital assistants; Ubiquitous computing; User interfaces; Internet multimedia; Session Initiation Protocol (SIP); Ubiquitous device; Multimedia services
Goal-oriented optimal subset selection of correlated multimedia streams,2007,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847757153&doi=10.1145%2f1198302.1198304&partnerID=40&md5=05ac6783c8588ced92adb15740d21eea,"A multimedia analysis system utilizes a set of correlated media streams, each of which, we assume, has a confidence level and a cost associated with it, and each of which partially helps in achieving the system goal. However, the fact that at any instant, not all of the media streams contribute towards a system goal brings up the issue of finding the best subset from the available set of media streams. For example, a subset of two video cameras and two microphones could be better than any other subset of sensors at some time instance to achieve a surveillance goal (e.g. event detection). This article presents a novel framework that finds the optimal subset of media streams so as to achieve the system goal under specified constraints. The proposed framework uses a dynamic programming approach to find the optimal subset of media streams based on three different criteria: first, by maximizing the probability of achieving the goal under the specified cost and confidence; second, by maximizing the confidence in the achieved goal under the specified cost and probability with which the goal is achieved; and third, by minimizing the cost to achieve the goal with a specified probability and confidence. Each of these problems is proven to be NP-Complete. From an AI point of view, the solution we propose is heuristic-based, and for each criterion, utilizes a heuristic function which for a given problem, combines optimal solutions of small-sized subproblems to yield a potential near-optimal solution to the original problem. The proposed framework allows for a tradeoff among the aforementioned three criteria, and offers the flexibility to compare whether any one set of media streams of low cost would be better than any other set of higher cost, or whether any one set of media streams of high confidence would be better than any other set of low confidence. To show the utility of our framework, we provide the experimental results for event detection in a surveillance scenario. © 2007 ACM.",Agreement coefficient; Confidence fusion; Media fusion; Optimal media selection,Artificial intelligence; Computational complexity; Dynamic programming; Heuristic methods; Microphones; Sensors; Agreement coefficient; Confidence fusion; Media fusion; Optimal media selection; Multimedia systems
Virtual videography,2007,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847693166&doi=10.1145%2f1198302.1198306&partnerID=40&md5=227da348562b5cbba4465a87208bf49d,"Well-produced videos provide a convenient and effective way to archive lectures. In this article, we offer a new way to create lecture videos that retains many of the advantages of well-composed recordings, without the cost and intrusion of a video production crew. We present an automated system called Virtual Videography that employs the art of videography to mimic videographer-produced videos, while unobtrusively recording lectures. The system uses the data recorded by unattended video cameras and microphones to produce a new edited video as an offline postprocess. By producing videos offline, our system can use future information when planning shot sequences and synthesizing new shots. Using simple syntactic cues gathered from the original video and a novel shot planning algorithm, the system makes cinematic decisions without any semantic understanding of the lecture. © 2007 ACM.",Attention modeling; Automated camera management; Computational cinematography; Video production,Algorithms; Microphones; Semantics; Video cameras; Attention modeling; Automated camera management; Computational cinematography; Video production; Video signal processing
Content-adaptive digital music watermarking based on music structure analysis,2007,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847713373&doi=10.1145%2f1198302.1198303&partnerID=40&md5=4e12936be6b5f22ed818ebea2810334c,"A novel content-adaptive music watermarking technique is proposed in this article. To optimally balance inaudibility and robustness when embedding and extracting watermarks, the embedding scheme is highly related to the music structure and human auditory system (HAS). A note-based segmentation method is proposed and used for music vocal/instrumental boundary detection. A multiple bit hopping and hiding scheme with different embedding parameters is applied to vocal and instrumental frames of the music. The experimental results in inaudibility and robustness are provided to support all novel features in the proposed watermarking scheme. © 2007 ACM.",Content-adaptive; Digital watermarking; Inaudibility; Music structure; Note-based segmentation; Robustness,Embedded systems; Multimedia services; Musical instruments; Robustness (control systems); Bit hopping; Content-adaptive; Human auditory system (HAS); Inaudibility; Music structure analysis; Note-based segmentation; Digital watermarking
Detecting social interactions of the elderly in a nursing home environment,2007,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847697367&doi=10.1145%2f1198302.1198308&partnerID=40&md5=b86c42594491eb11ac562ce89ad81a72,"Social interaction plays an important role in our daily lives. It is one of the most important indicators of physical or mental changes in aging patients. In this article, we investigate the problem of detecting social interaction patterns of patients in a skilled nursing facility using audio/visual records. Our studies consist of both a Wizard of Oz style study and an experimental study of various sensors and detection models for detecting and summarizing social interactions among aging patients and caregivers. We first simulate plausible sensors using human labeling on top of audio and visual data collected from a skilled nursing facility. The most useful sensors and robust detection models are determined using the simulated sensors. We then present the implementation of some real sensors based on video and audio analysis techniques and evaluate the performance of these implementations in detecting interactions. We conclude the article with discussions and future work. © 2007 ACM.",Health care; Human activity; Multimedia processing; Social interaction; Stochastic modeling,Computer simulation; Data acquisition; Data processing; Health care; Multimedia systems; Nursing; Patient monitoring; Random processes; Sensors; Human activity; Multimedia processing; Social interaction; Stochastic modeling; Social aspects
Video abstraction: A systematic review and classification,2007,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847733529&doi=10.1145%2f1198302.1198305&partnerID=40&md5=93adc1a2c4f90b64a2231c613f0c7b8b,"The demand for various multimedia applications is rapidly increasing due to the recent advance in the computing and network infrastructure, together with the widespread use of digital video technology. Among the key elements for the success of these applications is how to effectively and efficiently manage and store a huge amount of audio visual information, while at the same time providing user-friendly access to the stored data. This has fueled a quickly evolving research area known as video abstraction. As the name implies, video abstraction is a mechanism for generating a short summary of a video, which can either be a sequence of stationary images (keyframes) or moving images (video skims). In terms of browsing and navigation, a good video abstract will enable the user to gain maximum information about the target video sequence in a specified time constraint or sufficient information in the minimum time. Over past years, various ideas and techniques have been proposed towards the effective abstraction of video contents. The purpose of this article is to provide a systematic classification of these works. We identify and detail, for each approach, the underlying components and how they are addressed in specific works. © 2007 ACM.",Keyframe; Survey; Video abstraction; Video skimming; Video summarization,Abstracting; Information analysis; Surveys; Video signal processing; Keyframe; Video abstraction; Video skimming; Video summarization; Multimedia systems
Multimedia simplification for optimized MMS synthesis,2007,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847696174&doi=10.1145%2f1198302.1198307&partnerID=40&md5=57b10ad7032b07ac60bf80f51d364823,"We propose a novel transcoding technique called multimedia simplification which is based on experiential sampling. Multimedia simplification helps optimize the synthesis of MMS (multimedia messaging service) messages for mobile phones. Transcoding is useful in overcoming the limitations of these compact devices. The proposed approach aims at reducing the redundancy in the multimedia data captured by multiple types of media sensors. The simplified data is first stored into a gallery for further usage. Once a request for MMS is received, the MMS server makes use of the simplified media from the gallery. The multimedia data is aligned with respect to the timeline for MMS message synthesis. We demonstrate the use of the proposed techniques for two applications, namely, soccer video and home care monitoring video. The MMS sent to the receiver can basically reflect the gist of important events of interest to the user. Our technique is targeted towards users who are interested in obtaining salient multimedia information via mobile devices. © 2007 ACM.",Experiential sampling; Home care monitoring; Hypermedia coherence; MMS synthesis; Mobile phone; Multimedia simplification; Soccer video,Data acquisition; Information science; Mobile phones; Multimedia services; Sensors; Experiential sampling; Home care monitoring; Hypermedia coherence; MMS synthesis; Mobile phone; Soccer video; Multimedia systems
Smooth and efficient real-time video transport in the presence of wireless errors,2006,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749027786&doi=10.1145%2f1142020.1142022&partnerID=40&md5=9ad5ab9df0a2d3bba4eccad7f9e9f82a,"In this article we study a smooth and efficient transport protocol for real-time video over wireless networks. The proposed scheme, named the video transport protocol (VTP), has a new and unique end-to-end rate control mechanism that aims to avoid drastic rate fluctuations while maintaining friendliness to legacy protocols. VTP is also equipped with an achieved rate estimation scheme and a loss discrimination algorithm, both end-to-end, to cope with random errors in wireless networks efficiently. We show by analysis that VTP preserves most of the convergence properties of AIMD and converges to its fair share fast. VTP is compared to two recent TCP friendly rate control (TFRC) extensions, namely TFRC Wireless and MULTFRC, in wired-cum-wireless scenarios in Ns-2. Results show that VTP excels in all tested scenarios in terms of smoothness, fairness, and opportunistic friendliness. VTP is also implemented to work with a video camera and an H.263 video codec as part of our hybrid testbed, where its good performance as a transport layer protocol is confirmed by measurement results. © 2006 ACM.",Real-time video; TCP friendly rate control; Video transport protocol; Wireless networks,Algorithms; Error analysis; Network protocols; Real time systems; Transport properties; Video cameras; Real time video; TCP friendly rate control; TCP friendly rate control (TFRC); Video transport protocols (VTP); Wireless telecommunication systems
Real-time video content analysis: QoS-aware application composition and parallel processing,2006,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749023060&doi=10.1145%2f1142020.1142024&partnerID=40&md5=484aaa524c5ec51e03b345171cce43a2,"Real-Time content-based access to live video data requires content analysis applications that are able to process video streams in real-time and with an acceptable error rate. Statements such as this express quality of service (QoS) requirements. In general, control of the QoS provided can be achieved by sacrificing application quality in one QoS dimension for better quality in another, or by controlling the allocation of processing resources to the application. However, controlling QoS in video content analysis is particularly difficult, not only because main QoS dimensions like accuracy are nonadditive, but also because both the communication- and the processing-resource requirements are challenging. This article presents techniques for QoS-aware composition of applications for real-time video content analysis, based on dynamic Bayesian networks. The aim of QoS-aware composition is to determine application deployment configurations which satisfy a given set of QoS requirements. Our approach consists of: (1) an algorithm for QoS-aware selection of configurations of feature extractor and classification algorithms which balances requirements for timeliness and accuracy against available processing resources, (2) a distributed content-based publish/subscribe system which provides application scalability at multiple logical levels of distribution, and (3) scalable solutions for video streaming, filtering/transformation, feature extraction, and classification. We evaluate our approach based on experiments with an implementation of a real-time motion vector based object-tracking application. The evaluation shows that the application largely behaves as expected when resource availability and selections of configurations of feature extractor and classification algorithms vary. The evaluation also shows that increasing QoS requirements can be met by allocating additional CPUs for parallel processing, with only minor overhead. © 2006 ACM.",Event-based communication; Parallel processing; Publish/subscribe; QoS and resource management; Real-Time video content analysis; Task graph scheduling,Algorithms; Distributed computer systems; Error correction; Formal logic; Parallel processing systems; Real time systems; Event based communication; Publish/subscribe; Real time video content analysis; Resource management; Task graph scheduling; Quality of service
Statistical admission control using delay distribution measurements,2006,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846587810&doi=10.1145%2f1201730.1201732&partnerID=40&md5=8d8b8b8ad93bdff64716ec982acf2230,"Growth of performance sensitive applications, such as voice and multimedia, has led to widespread adoption of resource virtualization by a variety of service providers (xSPs). For instance, Internet Service Providers (ISPs) increasingly differentiate their offerings by means of customized services, such as virtual private networks (VPN) with Quality of Service (QoS) guarantees or QVPNs. Similarly Storage Service Providers (SSPs) use storage area networks (SAN)/network attached storage (NAS) technology to provision virtual disks with QoS guarantees or QVDs. The key challenge faced by these xSPs is to maximize the number of virtual resource units they can support by exploiting the statistical multiplexing nature of the customers' input request load.While a number of measurement-based admission control algorithms utilize statistical multiplexing along the bandwidth dimension, they do not satisfactorily exploit statistical multiplexing along the delay dimension to guarantee distinct per-virtual-unit delay bounds. This article presents Delay Distribution Measurement (DDM) based admission control algorithm, the first measurement-based approach that effectively exploits statistical multiplexing along the delay dimension. In other words, DDM exploits the well-known fact that the actual delay experienced by most service requests (packets or disk I/O requests) for a virtual unit is usually far smaller than its worst-case delay bound requirement because multiple virtual units rarely send request bursts at the same time. Additionally, DDM supports virtual units with distinct probabilistic delay guarantees - -virtual units that can tolerate more delay violations can reserve fewer resources than those that tolerate less, even though they require the same delay bound. Comprehensive trace-driven performance evaluation of QVPNs (using Voice over IP traces) and QVDs (using video stream, TPC-C, and Web search I/O traces) shows that, when compared to deterministic admission control, DDM can potentially increase the number of admitted virtual units (and resource utilization) by up to a factor of 3. © 2007 ACM.",Algorithms; Measurement; Performance,Algorithms; Internet; Multimedia systems; Multiplexing; Quality of service; Resource allocation; Statistical methods; Storage allocation (computer); Virtual reality; Delay Distribution Measurement (DDM); Internet Service providers (ISP); Statistical admission control; Storage area networks (SAN); Distributed parameter control systems
Metadata handling: A video perspective,2006,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846564815&doi=10.1145%2f1201730.1201736&partnerID=40&md5=21c1bf1409f8d6306c36cc22bc220d4d,This article addresses the problem of processing the annotations of preexisting video productions to enable reuse and repurposing of metadata. We introduce the concept of automatic content-based editing of preexisting semantic home video metadata. We propose a formal representation and implementation techniques for reusing and repurposing semantic video metadata in concordance with the actual video editing operations. A novel representation for metadata editing is proposed and an implementation framework for editing the metadata in accordance with the video editing operations is demonstrated. Conflict resolution and regularization operations are defined and implemented in the context of the video metadata editing operations. © 2007 ACM.,Content; Editing; Metadata; Preexisting video; Representation; Reuse; Semantic,File editors; Problem solving; Semantics; Video signal processing; Metadata editing; Preexisting video; Regularization operations; Video productions; Metadata
Middleware for streaming 3D progressive meshes over lossy networks,2006,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846596057&doi=10.1145%2f1201730.1201733&partnerID=40&md5=0ba168b8f4b159ccb3cb2ddde90ddf08,"Streaming 3D graphics have been widely used in multimedia applications such as online gaming and virtual reality. However, a gap exists between the zero-loss-tolerance of the existing compression schemes and the lossy network transmissions. In this article, we propose a generic 3D middleware between the 3D application layer and the transport layer for the transmission of triangle-based progressively compressed 3D models. Significant features of the proposed middleware include. 1) handling 3D compressed data streams from multiple progressive compression techniques. 2) considering end user hardware capabilities for effectively saving the data size for network delivery. 3) a minimum cost dynamic reliable set selector to choose the transport protocol for each sublayer based on the real-time network traffic. Extensive simulations with TCP/UDP and SCTP show that the proposed 3D middleware can achieve the dual objectives of maintaining low transmission delay and small distortion, and thus supporting high quality 3D streaming with high flexibility. © 2007 ACM.",3D streaming; Progressive compression,Computer hardware; Data compression; Multimedia systems; Network protocols; Online systems; Real time systems; Three dimensional computer graphics; Virtual reality; Lossy network transmissions; Online gaming; Progressive compression; Real-time network traffic; Middleware
Learning rich semantics from news video archives by style analysis,2006,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748994587&doi=10.1145%2f1142020.1142021&partnerID=40&md5=9e56863463e434d63af0a12596723793,"We propose a generic and robust framework for news video indexing which we founded on a broadcast news production model. We identify within this model four production phases, each providing useful metadata for annotation. In contrast to semiautomatic indexing approaches which exploit this information at production time, we adhere to an automatic data-driven approach. To that end, we analyze a digital news video using a separate set of multimodal detectors for each production phase. By combining the resulting production-derived features into a statistical classifier ensemble, the framework facilitates robust classification of several rich semantic concepts in news video; rich meaning that concepts share many similarities in their production process. Experiments on an archive of 120 hours of news video from the 2003 TRECVID benchmark show that a combined analysis of production phases yields the best results. In addition, we demonstrate that the accuracy of the proposed style analysis framework for classification of several rich semantic concepts is state-of-the-art. © 2006 ACM.",Benchmark evaluation; Multimedia understanding; Multimodal detectors; News video indexing; Semantic classification; Statistical pattern recognition; Style analysis,Automation; Broadcasting; Data reduction; Indexing (of information); Mathematical models; Semantics; Multimodal detectors; Statistical classifiers; TRECVID benchmarks; Video archives; Learning systems
A graphics architecture for high-end interactive television terminals,2006,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846608117&doi=10.1145%2f1201730.1201735&partnerID=40&md5=df45c7d87df0be570a014fa6f7653341,"This article presents a graphics software architecture for next-generation digital television receivers. We propose that such receivers should include a standardised Java-based procedural environment capable of rendering 2D/3D graphics and video, and a declarative environment supporting W3C recommendations such as SMIL and XForms. We also introduce a graphics architecture model that meets such requirements. As a proof-of-concept, a prototype implementation of the model is presented. This implementation enhances television content by allowing the user to play 3D graphics games, to run Java applications, and to browse XML-based documents while meeting current hardware restrictions. © 2007 ACM.",3D graphics; Digital TV; MHP; SMIL; XForms; XML,Computer software; Digital television; Interactive computer graphics; Java programming language; Mathematical models; Program documentation; XML; Graphics architecture; Graphics games; High-end interactive television terminals; Video on demand
Process prioritization using output production: Scheduling for multimedia,2006,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846608118&doi=10.1145%2f1201730.1201734&partnerID=40&md5=90d3b1fd8c8f9a95132b99a8b429faac,"Desktop operating systems such as Windows and Linux base scheduling decisions on CPU consumption; processes that consume fewer CPU cycles are prioritized, assuming that interactive processes gain from this since they spend most of their time waiting for user input. However, this doesn't work for modern multimedia applications which require significant CPU resources. We therefore suggest a new metric to identify interactive processes by explicitly measuring interactions with the user, and we use it to design and implement a process scheduler. Measurements using a variety of applications indicate that this scheduler is very effective in distinguishing between competing interactive and noninteractive processes. © 2007 ACM.",Multimedia; Resource management,Decision making; Process control; Resource allocation; Scheduling; CPU resources; Desktop operating systems; Process prioritization; Resource management; Multimedia systems
Defining user perception of distributed multimedia quality,2006,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846628881&doi=10.1145%2f1201730.1201731&partnerID=40&md5=cc1329520f450e7f5bab6e6afa2c8c8b,"This article presents the results of a study that explored the human side of the multimedia experience. We propose a model that assesses quality variation from three distinct levels: the network, the media and the content levels; and from two views: the technical and the user perspective. By facilitating parameter variation at each of the quality levels and from each of the perspectives, we were able to examine their impact on user quality perception. Results show that a significant reduction in frame rate does not proportionally reduce the user's understanding of the presentation independent of technical parameters, that multimedia content type significantly impacts user information assimilation, user level of enjoyment, and user perception of quality, and that the device display type impacts user information assimilation and user perception of quality. Finally, to ensure the transfer of information, low-level abstraction (network-level) parameters, such as delay and jitter, should be adapted; to maintain the user's level of enjoyment, high-level abstraction quality parameters (content-level), such as the appropriate use of display screens, should be adapted. © 2007 ACM.",Human-computer interaction; Multimedia quality; Quality of perception,Display devices; Human computer interaction; Information retrieval; Multimedia systems; Quality of service; User interfaces; Information assimilation; Multimedia quality; Quality of perception; User perception; Distributed computer systems
Automatic summarization of music videos,2006,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748997969&doi=10.1145%2f1142020.1142023&partnerID=40&md5=02f3a89de12bde88a798cb47dd130f2b,"In this article, we propose a novel approach for automatic music video summarization. The proposed summarization scheme is different from the current methods used for video summarization. The music video is separated into the music track and video track. For the music track, a music summary is created by analyzing the music content using music features, an adaptive clustering algorithm, and music domain knowledge. Then, shots in the video track are detected and clustered. Finally, the music video summary is created by aligning the music summary and clustered video shots. Subjective studies by experienced users have been conducted to evaluate the quality of music summaries and effectiveness of the proposed summarization approach. Experiments are performed on different genres of music videos and comparisons are made with the summaries generated based on music track, video track, and manually. The evaluation results indicate that summaries generated using the proposed method are effective in helping realize users' expectations. © 2006 ACM.",Music summarization; Music video; Video summarization,Adaptive systems; Automation; Knowledge based systems; Quality assurance; User interfaces; Adaptive clustering algorithms; Music summary; Quality of music; Video summarization; Multimedia systems
Introduction to special issue on the use of context in multimedia information systems,2006,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748563967&doi=10.1145%2f1152149.1152150&partnerID=40&md5=ea596e0adde9f54437c596741e6d5416,[No abstract available],,
"Modeling context in haptic perception, rendering, and visualization",2006,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748559451&doi=10.1145%2f1152149.1152153&partnerID=40&md5=a72d068f4b1a328b4313f5fe777d046c,"Haptic perception refers to the ability of human beings to perceive spatial properties through touch-based sensations. In haptics, contextual clues about material,shape, size, texture, and weight configurations of an object are perceived by individuals leading to recognition of the object and its spatial features. In this paper, we present strategies and algorithms to model context in haptic applications that allow users to haptically explore objects in virtual reality/augmented reality environments. Initial results show significant improvement in accuracy and efficiency of haptic perception in augmented reality environments when compared to conventional approaches that do not model context in haptic rendering. © 2006 ACM.",Context modeling; Haptic cueing systems; Haptic user interfaces; Haptics,Biodiversity; Mathematical models; Object recognition; Sensory perception; Textures; Virtual reality; Context modeling; Haptic cueing systems; Haptic user interfaces; Haptics; Haptic interfaces
Handling multiple points of view in a multimedia data warehouse,2006,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748573983&doi=10.1145%2f1152149.1152152&partnerID=40&md5=b267854c754cf6593e9b5f1f7f6054e3,"Data warehouses are dedicated to collecting heterogeneous and distributed data in order to perform decision analysis. Based on multidimensional model, OLAP commercial environments such as they are currently designed in traditional applications are used to provide means for the analysis of facts that are depicted by numeric data (e.g., sales depicted by amount or quantity sold). However, in numerous fields, like in medical or bioinformatics, multimedia data are used as valuable information in the decisional process. One of the problems when integrating multimedia data as facts in a multidimensional model is to deal with dimensions built on descriptors that can be obtained by various computation modes on raw multimedia data. Taking into account these computation modes makes possible the characterization of the data by various points of view depending on the user's profile, his best-practices, his level of expertise, and so on. We propose a new multidimensional model that integrates functional dimension versions allowing the descriptors of the multidimensional data to be computed by different functions. With this approach, the user is able to obtain and choose multiple points of view on the data he analyses. This model is used to develop an OLAP application for navigation into a hypercube integrating various functional dimension versions for the calculus of descriptors in a medical use case. © 2006 ACM.",Data warehouse; Descriptor; Functional version; Multimedia; OLAP,Data handling; Decision theory; Integration; Mathematical models; Multimedia systems; Problem solving; Virtual reality; Decisional process; Descriptors; Functional version; Numeric data; Data warehouses
A semantic web ontology for context-based classification and retrieval of music resources,2006,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748569108&doi=10.1145%2f1152149.1152151&partnerID=40&md5=a9d700b0d2ee9d5068161467c2fc3a81,"In this article, we describe the MX-Onto ontology for providing a Semantic Web compatible representation of music resources based on their context. The context representation is realized by means of an OWL ontology that describes music information and that defines rules and classes for a flexible genre classification. By flexible classification we mean that the proposed approach enables capturing the subjective interpretation of music genres by denning multiple membership relations between a music resource and the corresponding music genres, thus supporting context-based and proximity-based search of music resources. © 2006 ACM.",Music classification; Music retrieval; Ontology-based music representation,Information analysis; Information retrieval; Resource allocation; Semantics; Music classifications; Music retrieval; Ontology-based music representations; World Wide Web
The story picturing engine - A system for automatic text illustration,2006,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745125045&doi=10.1145%2f1126004.1126008&partnerID=40&md5=fb622db1d998d745c55a10c098b08c2b,"We present an unsupervised approach to automated story picturing. Semantic keywords are extracted from the story, an annotated image database is searched. Thereafter, a novel image ranking scheme automatically determines the importance of each image. Both lexical annotations and visual content play a role in determining the ranks. Annotations are processed using the Wordnet. A mutual reinforcement-based rank is calculated for each image. We have implemented the methods in our Story Picturing Engine (SPE) system. Experiments on large-scale image databases are reported. A user study has been performed and statistical analysis of the results has been presented. © 2006 ACM.",Image retrieval; Lexical referencing; Markov chain; Mutual reinforcement; Story picturing,Automation; Database systems; Image analysis; Image retrieval; Semantics; Statistical methods; Text processing; Lexical referencing; Markov chain; Mutual reinforcement; Story picturing; Search engines
Fusion of AV features and external information sources for event detection in team sports video,2006,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745151973&doi=10.1145%2f1126004.1126007&partnerID=40&md5=94dadb2f9db4d0a13e25fd6a67210347,"The use of AV features alone is insufficient to induce high-level semantics. This article proposes a framework that utilizes both internal AV features and various types of external information sources for event detection in team sports video. Three schemes are also proposed to tackle the asynchronism between the fusion of AV and external information. The framework is extensible as it can provide increasing functionalities given more detailed external information and domain knowledge. By demonstrating its effectiveness on soccer and American football, we believe that with the availability of appropriate domain knowledge, the framework is applicable to other team sports. © 2006 ACM.",Event detection; Event modeling; Semantic; Sports video,Communication systems; Economic and social effects; Information analysis; Semantics; Video signal processing; Event detection; Event modeling; Semantic; Sports video; Multimedia systems
Content-based retrieval of 3D models,2006,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745158320&doi=10.1145%2f1126004.1126006&partnerID=40&md5=94743fcd8c634f3dc2285427dca6f6d4,"In the past few years, there has been an increasing availability of technologies for the acquisition of digital 3D models of real objects and the consequent use of these models in a variety of applications, in medicine, engineering, and cultural heritage. In this framework, content-based retrieval of 3D objects is becoming an important subject of research, and finding adequate descriptors to capture global or local characteristics of the shape has become one of the main investigation goals. In this article, we present a comparative analysis of a few different solutions for description and retrieval by similarity of 3D models that are representative of the principal classes of approaches proposed. We have developed an experimental analysis by comparing these methods according to their robustness to deformations, the ability to capture an object's structural complexity, and the resolution at which models are considered. © 2006 ACM.",3D shape description; Comparative analysis; Retrieval by content of 3D models,Biomedical engineering; Information retrieval; Large scale systems; Optical resolving power; Robustness (control systems); 3D shape description; Comparative analysis; Retrieval by content of 3D models; Three dimensional computer graphics
Content-based multimedia information retrieval: State of the art and challenges,2006,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745130042&doi=10.1145%2f1126004.1126005&partnerID=40&md5=e6700aaa0224f594b4b4d2efeae952b7,"Extending beyond the boundaries of science, art, and culture, content-based multimedia information retrieval provides new paradigms and methods for searching through the myriad variety of media all over the world. This survey reviews 100+ recent browsing and search paradigms, user studies, affective computing, learning, semantic queries, new features and media types, high performance indexing, and evaluation techniques. Based on the current state of the art, we discuss the major challenges for the future. © 2006 ACM.",Audio retrieval; Human-computer interaction; Image databases; Image search; Multimedia indexing; Multimedia information retrieval; Video retrieval,Computation theory; Electronic publishing; Evaluation; Human computer interaction; Information retrieval; Search engines; Semantics; Audio retrieval; Image databases; Image search; Multimedia indexing; Multimedia information retrieval; Video retrieval; Multimedia systems
Adjusting Forward Error Correction with Temporal Scaling for TCP-Friendly Streaming MPEG,2005,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014821362&doi=10.1145%2f1111604.1111605&partnerID=40&md5=c21441b43cf50645ff610b1b44a29984,[No abstract available],Design; forward error correction; MPEG; Multimedia networking; Performance; TCP-friendly,
Layered Unequal Loss Protection with Pre-Interleaving for Fast Progressive Image Transmission Over Packet-Loss Channels,2005,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-34249326371&doi=10.1145%2f1111604.1111606&partnerID=40&md5=6d1b5a380b4f1fc88bf99da77a6ee68b,"Most existing unequal loss protection (ULP) schemes do not consider the minimum quality requirement and usually have high computation complexity. In this research, we propose a layered ULP (L-ULP) scheme to solve these problems. In particular, we use the rate-based optimal solution with a local search to find the average forward error correction (FEC) allocation and use the gradient search to find the FEC solution for each layer. Experimental results show that the executing time of L-ULP is much faster than the traditional ULP scheme but the average distortion is worse. Therefore, we further propose to combine the L-ULP with the pre-interleaving to have an improved L-ULP (IL-ULP) system. By using the pre-interleaving, we are able to delay the occurrence of the first unrecoverable loss in the source bitstream and thus improve the loss resilience performance. With the better loss resilience performance in the source bitstream, our proposed IL-ULP scheme is allowed to have a weaker FEC protection and allocate more bits to the source coding which leads to the improvement of overall performance. Experimental results show that our proposed IL-ULP scheme even outperforms the global optimal result obtained by any traditional ULP scheme while the complexity of IL-ULP is almost the same as L-ULP. Copyright © 2005, ACM. All rights reserved.",Algorithms; forward error correction; joint source-channel coding; packet loss; Progressive image transmission; unequal loss protection,
An Analytical Study of Peer-to-Peer Media Streaming Systems,2005,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-56849117421&doi=10.1145%2f1111604.1111607&partnerID=40&md5=a861980e67408bc16cc45ea073f3ec19,[No abstract available],,
Coordinated Enroute Multimedia Object Caching in Transcoding Proxies for Tree Networks,2005,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974610267&doi=10.1145%2f1083314.1083318&partnerID=40&md5=e4fb9f0b2a61eed5f33b34c03ba497a9,"Transcoding is a promising technology that allows systems to effect a quality-versus-size tradeoff on multimedia objects. As audio and video applications have proliferated on the Internet, caching in transcoding proxies has become an important technique for improving network performance, especially in mobile networks. This article addresses the problem of coordinated enroute multimedia object caching in transcoding proxies for tree networks.We formulate this problem as an optimization problem based on our proposed model, in which multimedia object caching decisions are made on all enroute caches along the routing path by integrating both object placement and replacement policies and cache status information along the routing path of a request is used to determine the optimal locations for caching multiple versions of the same multimedia object. We propose an optimal solution using dynamic programming to compute the optimal locations.We also extend this solution to solve the same problem for several constrained cases, including constraints on the cost gain per node and on the number of versions to be placed. Our model is evaluated on different performance metrics through extensive simulation experiments. The implementation results show that our model significantly outperforms existing models that consider Web caching in transcoding proxies either on a single path or at individual nodes. Copyright © 2005, ACM. All rights reserved.",Algorithms; Design; Internet; Management; multimedia object; optimization problem; Performance; transcoding proxy; tree network; Web caching,
Flexible Cross-Domain Event Delivery for Quality-Managed Multimedia Applications,2005,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024282333&doi=10.1145%2f1083314.1083316&partnerID=40&md5=79d3f44bce3ed0b1d49075381f8210b7,"To meet end users quality-of-service (QoS) requirements, online quality management for multimedia applications must include appropriate allocation of the underlying computing platform‗s resources. Previous work has developed novel operating system (OS) functionality for dynamic QoS management, including multimedia or real-time CPU schedulers and OS extensions for online performance monitoring and for adaptations, as well as QoS-aware applications that adapt their behavior to gain additional benefits from such functionality. This article describes a general OS mechanism that may be used to implement a wide variety of online quality management functions. ECalls is a communication mechanism that implements multiple cross-domain calling conventions that can be customized to the quality management needs of applications. The ECalls mechanism is based on the notions of events, event channels, and event handlers. Using events, applications can share relevant QoS attributes with OS services, and OS-level resource management services can efficiently provide monitoring data to target applications or application managers. Dynamically generated event handlers can be used to customize event delivery to meet diverse application needs, for example, to achieve high scalability for Web servers or small jitter for real-time data delivery. © 2005, ACM. All rights reserved.",Design; dynamic code generation; Event delivery; operating system; Performance; quality management; quality-of-service; real-time events,
IMCE: Integrated Media Creation Environment,2005,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024250120&doi=10.1145%2f1083314.1083315&partnerID=40&md5=7ba99c7e7a27e0d620345ba67df2c0b9,"We discuss the design goals for an integrated media creation environment (IMCE) aimed at enabling the average user to create media artifacts with professional qualities. The resulting requirements are implemented and we demonstrate the efficacy of the resulting system with the generation of two simple home movies. The significance for the average user seeking to create home movies lies in the flexible and automatic application of film principles to the task, removal of tedious low-level editing by means of well-formed media transformations in terms of high-level film constructs (e.g., tempo), and content repurposing powered by those same transformations added to the rich semantic information maintained at each phase of the process. Copyright © 2005, ACM. All rights reserved.",Algorithms; computational media aesthetics; Home movie; Human Factors; Measurement; semantics; video collection,
Temporal Event Clustering for Digital Photo Collections,2005,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016655082&doi=10.1145%2f1083314.1083317&partnerID=40&md5=8de26abac222028fe1e984a8aab9fd8d,"Organizing digital photograph collections according to events such as holiday gatherings or vacations is a common practice among photographers. To support photographers in this task, we present similarity-based methods to cluster digital photos by time and image content. The approach is general and unsupervised, and makes minimal assumptions regarding the structure or statistics of the photo collection. We present several variants of an automatic unsupervised algorithm to partition a collection of digital photographs based either on temporal similarity alone, or on temporal and content-based similarity. First, interphoto similarity is quantified at multiple temporal scales to identify likely event clusters. Second, the final clusters are determined according tone of three clustering goodness criteria. The clustering criteria trade off computational complexity and performance. We also describe a supervised clustering method based on learning vector quantization. Finally, we review the results of an experimental evaluation of the proposed algorithms and existing approaches on two test collections. Copyright © 2005, ACM. All rights reserved.",Algorithms; digital libraries; Digital photo organization; Management; temporal media indexing,
Selected Papers from the ACM Multimedia Conference 2003,2005,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024255524&doi=10.1145%2f1062253.1062254&partnerID=40&md5=50be61966045b3a6b500dab851eb2720,[No abstract available],,
Semantics and Feature Discovery via Confidence-Based Ensemble,2005,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962204456&doi=10.1145%2f1062253.1062257&partnerID=40&md5=538b97374e84bf06158cc189cb36dfe9,"Providing accurate and scalable solutions to map low-level perceptual features to high-level semantics is essential for multimedia information organization and retrieval. In this paper, we propose a confidence-based dynamic ensemble (CDE) to overcome the shortcomings of the traditional static classifiers. In contrast to the traditional models, CDE can make dynamic adjustments to accommodate new semantics, to assist the discovery of useful low-level features, and to improve class-prediction accuracy. We depict two key components of CDE: a multi-level function that asserts class-prediction confidence, and the dynamic ensemble method based upon the confidence function. Through theoretical analysis and empirical study, we demonstrate that CDE is effective in annotating large-scale, real-world image datasets. Copyright © 2005, ACM. All rights reserved.",Algorithms; Classification confidence; image annotation; Performance; semantics discovery,
Real-Time Multidepth Stream Compression,2005,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969697650&doi=10.1145%2f1062253.1062255&partnerID=40&md5=067fcb13d900c85ddbb4cccba05bf083,"The goal of tele-immersion has long been to enable people at remote locations to share a sense of presence. A tele-immersion system acquires the 3D representation of a collaborator‗s environment remotely and sends it over the network where it is rendered in the user‗s environment. Acquisition, reconstruction, transmission, and rendering all have to be done in real-time to create a sense of presence.With added commodity hardware resources, parallelism can increase the acquisition volume and reconstruction data quality while maintaining real-time performance. However, this is not as easy for rendering since all of the data need to be combined into a single display. In this article, we present an algorithm to compress data from such 3D environments in real-time to solve this imbalance. We present a compression algorithm which scales comparably to the acquisition and reconstruction, reduces network transmission bandwidth, and reduces the rendering requirement for real-time performance. This is achieved by exploiting the coherence in the 3D environment data and removing them in real-time. We have tested the algorithm using a static office data set as well as a dynamic scene, the results of which are presented in the article. Copyright © 2005, ACM. All rights reserved.",Algorithms; augmented reality; k-means algorithm; k-means initialization; Real-time compression; tele-immersion; tele-presence; virtual reality,
Panoptes: Scalable Low-Power Video Sensor Networking Technologies,2005,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017303035&doi=10.1145%2f1062253.1062256&partnerID=40&md5=93f5de204765ca6d630a31d04797bfeb,"Video-based sensor networks can provide important visual information in a number of applications including: environmental monitoring, health care, emergency response, and video security. This article describes the Panoptes video-based sensor networking architecture, including its design, implementation, and performance.We describe two video sensor platforms that can deliver high-quality video over 802.11 networks with a power requirement less than 5 watts. In addition, we describe the streaming and prioritization mechanisms that we have designed to allow it to survive long-periods of disconnected operation. Finally, we describe a sample application and bitmapping algorithm that we have implemented to show the usefulness of our platform. Our experiments include an in-depth analysis of the bottlenecks within the system as well as power measurements for the various components of the system. Copyright © 2005, ACM. All rights reserved.",adaptive video; Design; Measurement; Performance; video collection; Video sensor networking,
"Understanding Performance in Coliseum, An Immersive Videoconferencing System",2005,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877947497&doi=10.1145%2f1062253.1062258&partnerID=40&md5=05bb0c7bd9adf352b1cdd53bd42011a2,"Coliseum is a multiuser immersive remote teleconferencing system designed to provide collaborative workers the experience of face-to-face meetings from their desktops. Five cameras are attached to each PC display and directed at the participant. From these video streams, view synthesis methods produce arbitrary-perspective renderings of the participant and transmit them to others at interactive rates, currently about 15 frames per second. Combining these renderings in a shared synthetic environment gives the appearance of having all participants interacting in a common space. In this way, Coliseum enables users to share a virtual world, with acquired-image renderings of their appearance replacing the synthetic representations provided by more conventional avatar-populated virtual worlds. The system supports virtual mobility-participants may move around the shared space—and reciprocal gaze, and has been demonstrated in collaborative sessions of up to ten Coliseum workstations, and sessions spanning two continents. Coliseum is a complex software system which pushes commodity computing resources to the limit. We set out to measure the different aspects of resource, network, CPU, memory, and disk usage to uncover the bottlenecks and guide enhancement and control of system performance. Latency is a key component of Quality of Experience for video conferencing. We present how each aspect of the system-cameras, image processing, networking, and display—contributes to total latency. Performance measurement is as complex as the system to which it is applied.We describe several techniques to estimate performance through direct light-weight instrumentation as well as use of realistic end-to-end measures that mimic actual user experience. We describe the various techniques and how they can be used to improve system performance for Coliseum and other network applications. This article summarizes the Coliseum technology and reports on issues related to its performance its measurement, enhancement, and control. Copyright © 2005, ACM. All rights reserved.",3D virtual environments; Algorithms; Design; Experimentation; Measurement; network applications; Performance; performance measurement; streaming media; Telepresence; videoconferencing; view synthesis,
Guest Editorial,2005,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024290928&doi=10.1145%2f1047936.1047939&partnerID=40&md5=03f01c879b867c99bd9f34cf083c4be1,[No abstract available],,
"EDITORIAL The Birth of the ACM Transactions on Multimedia Computing, Communications and Applications (TOMCCAP)",2005,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024251970&doi=10.1145%2f1047936.1047937&partnerID=40&md5=ffb82c0489478e09746e88b069cd2777,[No abstract available],,
ACM SIGMM Retreat Report on Future Directions in Multimedia Research,2005,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018199322&doi=10.1145%2f1047936.1047938&partnerID=40&md5=93ccc5de3a11a19752406f69d8caf7eb,"The ACM Multimedia Special Interest Group was created ten years ago. Since that time, researchers have solved a number of important problems related to media processing, multimedia databases, and distributed multimedia applications. A strategic retreat was organized as part of ACM Multimedia 2003 to assess the current state of multimedia research and suggest directions for future research. This report presents the recommendations developed during the retreat. The major observation is that research in the past decade has significantly advanced hardware and software support for distributed multimedia applications and that future research should focus on identifying and delivering applications that impact users in the real-world. The retreat suggested that the community focus on solving three grand challenges: (1) make authoring complex multimedia titles as easy as using a word processor or drawing program, (2) make interactions with remote people and environments nearly the same as interactions with local people and environments, and (3) make capturing, storing, finding, and using digital media an everyday occurrence in our computing environment. The focus of multimedia researchers should be on applications that incorporate correlated media, fuse data from different sources, and use context to improve application performance. Copyright © 2005, ACM. All rights reserved.",distributed collaboration; Multimedia; Multimedia authoring; multimedia query; multimedia storage and indexing; Research; tele-presence,
Disk Scheduling in a Multimedia I/O System,2005,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014137162&doi=10.1145%2f1047936.1047941&partnerID=40&md5=efa3665a4e5eba97b4ddd49a8ebc3cb1,"This article provides a retrospective of our original paper by the same title in the Proceedings of the First ACM Conference on Multimedia, published in 1993. This article examines the problem of disk scheduling in a multimedia I/O system. In a multimedia server, the disk requests may have constant data rate requirements and need guaranteed service. We propose a new scheduling algorithm, SCAN-EDF, that combines the features of SCAN type of seek optimizing algorithm with an Earliest Deadline First (EDF) type of real-time scheduling algorithm. We compare SCAN-EDF with other scheduling strategies and show that SCANEDF combines the best features of both SCAN and EDF.We also investigate the impact of buffer space on the maximum number of video streams that can be supported. We show that by making the deadlines larger than the request periods, a larger number of streams can be supported. We also describe how we extended the SCAN-EDF algorithm in the PRISM multimedia architecture. PRISM is an integrated multimedia server, designed to satisfy the QOS requirements of multiple classes of requests. Our experience in implementing the extended SCAN-EDF algorithm in a generic operating system is discussed and performance metrics and results are presented to illustrate how the SCAN-EDF extensions and implementation strategies have succeeded in meeting the QOS requirements of different classes of requests. Copyright © 2005, ACM. All rights reserved.",Algorithms; disk scheduling; I/O systems; multimedia applications; performance evaluation; Performancem; real-time,
Automatic Temporal Layout Mechanisms Revisited,2005,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751030288&doi=10.1145%2f1047936.1047942&partnerID=40&md5=2bb67194d42e7f620230a90453036994,"A traditional static document has a spatial layout that specifies where objects in the document appear. Because multimedia documents incorporate time, they also require a temporal layout, or schedule, that specifies when events in the document occur. This article argues that multimedia document systems should provide mechanisms for automatically producing temporal layouts for documents. The major advantage of this approach is that it makes it easier for authors to create and modify multimedia documents. This article revisits our 1993 framework for understanding automatic temporal formatters and explores the basic issues surrounding them. It also describes the Firefly multimedia document system, which was developed in 1992 to test the potential of automatic temporal formatting. Using our original framework, the paper reviews a representative sample of recent automatic document formatters. This analysis validates the basic framework and demonstrates the progress of the field in the intervening decade. A discussion of potential extensions to the framework is included. Copyright © 2005, ACM. All rights reserved.",Algorithms; Languages; multimedia authoring; Multimedia documents; temporal formatting; temporal specification,
The Berkeley Software MPEG-1 Video Decoder,2005,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876209922&doi=10.1145%2f1047936.1047944&partnerID=40&md5=034c191973fbfcee002dce3632e19294,"This article reprises the description of the Berkeley software-only MPEG-1 video decoder originally published in the proceedings of the 1st International ACM Conference on Multimedia in 1993. The software subsequently became widely used in a variety of research systems and commercial products. Its main impact was to provide a platform for experimenting with streaming compressed video and to expose the strengths and weaknesses of software-only video decoding using general purpose computing architectures. This article compares the original performance results with experiments run on a modern processor to demonstrate the gains of processing power in the past ten years relative to this specific application and discusses the history of MPEG-1 video software decoding and the Berkeley MPEG research group. Copyright © 2005, ACM. All rights reserved.",MPEG; Performance; Video compression,
Salient Stills,2005,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250181255&doi=10.1145%2f1047936.1047940&partnerID=40&md5=b32987cdb4d50a6af1850fb4750b99be,"Salient Stills are a class of images that reflect the aggregation of the temporal changes that occur in a moving-image sequence with the salient features of individual frames preserved. They convey the intended expression of an entire series of moving frames-a visual summary of camera and object movements. The original frames, which may include variations in focal length or field of view, or moving objects, are combined to create a single still image. The still image may have multiresolution patches, a larger field of view, or higher overall resolution than any individual frame in the original image sequence. Salient Stills may also contain selected significant objects from individual or multiple video frames. Salient Stills attempt to retain much of the original content (detail) and spatial and temporal extent (context) of the original video or film sequence. Copyright © 2005, ACM. All rights reserved.",Algorithms; media transcoding; Salient stills; semantic image processing; shape-time photography; synopsis mosaic; timeprints; video database; video mosaic; video summary,
Structured Multimedia Authoring,2005,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002299188&doi=10.1145%2f1047936.1047943&partnerID=40&md5=f320a55b6b7ce557e4a10401e0f6189a,"Authoring context sensitive, interactive multimedia presentations is much more complex than authoring either purely audiovisual applications or text. Interactions among media objects need to be described as a set of spatio-temporal relationships that account for synchronous and asynchronous interactions, as well as on-demand linking behavior. This article considers the issues that need to be addressed by an authoring environment. We begin with a partitioning of concerns based on seven classes of authoring problems. We then describe a selection of multimedia authoring environments within four different authoring paradigms: structured, timeline, graph and scripting. We next provide observations and insights into the authoring process and argue that the structured paradigm provides the most useful framework for presentation authoring. We close with an example application of the structured multimedia authoring paradigm in the context of our own structure-based system GRiNS. Copyright © 2005, ACM. All rights reserved.",Design; hypermedia; Languages; Multimedia authoring; SMIL; synchronization; Verification,
A Normalized Slicing-assigned Virtualization Method for 6G-based Wireless Communication Systems,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151863495&doi=10.1145%2f3546077&partnerID=40&md5=b1dcaf08818ee127a6f8b48d1bf7f4f4,"The next generation of wireless communication systems will rely on advantageous sixth-generation wireless network (6G) features and sophisticated edge Internet-of-Things technology to provide continuous service delegation and resource allocation. Network slicing and virtualization are common in these scenarios to meet user demands and application services. This article introduces a Normalized Slicing-assigned Virtualization Method for satisfying the 6G features in future generation systems. The proposed method relies on available resource roots and time intervals for replications. Based on the availability and Accessibility, the resource virtualization and network slicing processes are forwarded. The proposed method exploits federated learning for determining availability and accessibility models in detecting slicing, virtualization, or both the requirements throughout the resource sharing process. This improves the resource sharing rate, with less latency and high processing despite the user and application demands. The learning models are trained to balance replication and network slicing for confining complexity across different resources. The proposed method's performance is validated using the above metrics for varying users and intervals.  © 2022 Association for Computing Machinery.",6G; federated learning; network slicing; virtualization,Learning systems; Virtual reality; 6g; Continuous services; Federated learning; Internet of things technologies; Network slicing; Network virtualization; Resources allocation; Resources sharing; Virtualizations; Wireless communication system; Virtualization
Hiding Message Using a Cycle Generative Adversarial Network,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151788909&doi=10.1145%2f3495566&partnerID=40&md5=e85361a84c5e08517ff0c43d1c5073c2,"Training an image steganography is an unsupervised problem, because it is impossible to obtain an ideal supervised steganographic image corresponding to the cover image and secret message. Inspired by the success of cycle generative adversarial networks in unsupervised tasks such as style transfer, this article proposes to use a cycle generative adversarial network to solve the problem of unsupervised image steganography. Specifically, this article jointly trains five networks, i.e., a steganographic network, an inverse steganographic network, a hidden message reconstruction network, and two discriminative networks, which together constitute a hidden message cycle generative adversarial network (HCGAN). Compared with the recent image steganography based on generative adversative network, HCGAN provides more accurate supervised information, which makes the training process of HCGAN converge faster and the performance of the trained image steganography network is better. In addition, this article introduces an image steganographic network based on residual learning and shows that residual learning can effectively improve the performance of steganography. Furthermore, to the best of our knowledge, we are the first to propose an inverse steganographic network for eliminating steganographic message from steganographic images, which can be used to avoid steganographic message being discovered or acquired by a third party. The experimental results show that compared with the steganography based on generative adversarial network, the proposed HCGAN has a higher correct decoding rate, better visual quality of steganographic image, and higher secrecy.  © 2022 Association for Computing Machinery.",cycle generative adversarial network; generative adversarial network; inverse steganographic network; residual learning; Steganography,Image enhancement; Steganography; Cover-image; Cycle generative adversarial network; Discriminative networks; Hidden messages; Image steganography; Inverse steganographic network; Performance; Reconstruction networks; Residual learning; Secret messages; Generative adversarial networks
When Did It Happen? Duration-informed Temporal Localization of Narrated Actions in Vlogs,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151859991&doi=10.1145%2f3495211&partnerID=40&md5=123b7bafb2b7ad8dbc057d191f562e45,"We consider the task of temporal human action localization in lifestyle vlogs. We introduce a novel dataset consisting of manual annotations of temporal localization for 13,000 narrated actions in 1,200 video clips. We present an extensive analysis of this data, which allows us to better understand how the language and visual modalities interact throughout the videos. We propose a simple yet effective method to localize the narrated actions based on their expected duration. Through several experiments and analyses, we show that our method brings complementary information with respect to previous methods, and leads to improvements over previous work for the task of temporal action localization.  © 2022 Association for Computing Machinery.",action duration; Action temporal localization; multimodal processing; natural language processing; video processing; vlogs,Natural language processing systems; Video signal processing; Action duration; Action temporal localization; Language processing; Localisation; Multimodal processing; Natural language processing; Natural languages; Temporal localization; Video processing; Vlogs; Visual languages
Toward a Holistic Approach to the Socio-historical Analysis of Vernacular Photos,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138214947&doi=10.1145%2f3507918&partnerID=40&md5=682b48eee1af79348f89c2d8b2bb38e8,"Although one of the most popular practices in photography since the end of the 19th century, an increase in scholarly interest in family photo albums dates back to the early 1980s. Such collections of photos may reveal sociological and historical insights regarding specific cultures and times. They are, however, in most cases scattered among private homes and only available on paper or photographic film, thus making their collection and analysis by historians, socio-cultural anthropologists, and cultural theorists very cumbersome. Computer-based methodologies could aid such a process in various ways, speeding up the cataloging step, for example, with the use of modern computer vision techniques. We here investigate such an approach, introducing the design and development of a multimedia application that may automatically catalog vernacular pictures drawn from family photo albums. To this aim, we introduce the IMAGO dataset, which is composed of photos belonging to family albums assembled at the University of Bologna's Rimini campus since 2004. Exploiting the proposed application, IMAGO has offered the opportunity of experimenting with photos taken between the years 1845 and 2009. In particular, it has been possible to estimate their socio-historical content, i.e., the dates and contexts of the images, without resorting to any other sources of information. Exceeding our initial expectations, such an approach has revealed its merit not only in terms of performance but also in terms of the foreseeable implications for the benefit of socio-historical research. To the best of our knowledge, this contribution is among the few that move along this path at the intersection of socio-historical studies, multimedia computing, and artificial intelligence.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Family photo albums; historical images; image dating; multimedia application; socio-historical context classification,Image analysis; Image classification; 19th century; Context classification; Family photo album; Historical analysis; Historical image; Holistic approach; Image dating; Multimedia applications; Photo album; Socio-historical context classification; Photographic films
Context-aware Pseudo-true Video Interpolation at 6G Edge,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151819047&doi=10.1145%2f3555313&partnerID=40&md5=fa41c9eb613b7ffcb33809fc8a983c6a,"In the 6G network, lots of edge devices facilitate the low-latency transmission of video. However, with limited processing and storage capabilities, the edge devices cannot afford to reconstruct the vast amount of video data. On the condition of edge computing in the 6G network, this article fuses a self-similarity-based context feature into Frame Rate Up-Conversion (FRUC) to generate the pseudo-true video sequences at high frame rate, and its core is the extraction of the context layer for each video frame. First, we extract the patch centered at each pixel and use the self-similarity descriptor to generate the correlation surface. Then, the expectation or skewness of the correlation surface in statistics is computed to represent its context feature. By attaching an expectation or a skewness to each pixel, the context layer is constructed and added to the video frame as a new channel. According to the context layer, we predict the motion vector field of the absent frame by using the bidirectional context match and finally produce the interpolated frame. From the experimental results, it can be seen that by deploying the proposed FRUC algorithm on edge devices, the output pseudo-true video sequences have satisfying objective and subjective qualities.  © 2022 Association for Computing Machinery.",6G network; bidirectional motion estimation; context layer; edge computing; motion-compensated frame interpolation,Digital storage; Edge computing; Higher order statistics; Image coding; Interpolation; Motion Picture Experts Group standards; Pixels; Video recording; Video signal processing; 6g network; Bidirectional motion; Bidirectional motion estimation; Context features; Context layer; Edge computing; Frame rate up-conversion; Motion-compensated frame interpolations; Self-similarities; Video sequences; Motion estimation
Deep Self-Supervised Hyperspectral Image Reconstruction,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147686400&doi=10.1145%2f3510373&partnerID=40&md5=a5a1ca1d728936971e30387d1c108d4e,"Reconstructing a high-resolution hyperspectral (HR-HS) image via merging a low-resolution hyperspectral (LR-HS) image and a high-resolution RGB (HR-RGB) image has become a hot research topic, and can greatly benefit for different subsequent high-level vision tasks. Recently, deep learning-based approaches have evolved for HS image reconstruction and validated impressive performance. However, to learn a good reconstruction model in the deep learning-based methods, it is mandatory to previously collect large-scale training triplets consisting of the LR-HS, HR-RGB, and HR-HS images, which is difficult to be collected in real applications. This study proposes a deep self-supervised HS image reconstruction framework (DSSH), which does not have to depend on any handcrafted prior and previously collected training triplets at all. The proposed DSSH method leverages the designed network architecture itself for capturing the prior of the underlying structure in the latent HR-HS image and employs the observed LR-HS and HR-RGB images only for network parameter learning. Experiments on two benchmark HS image datasets validated that the proposed DSSH method manifests very impressive reconstruction performance, and is even better than some state-of-the-art supervised learning approaches.  © 2022 Association for Computing Machinery.",Deep learning; hyperspectral and RGB image fusion; hyperspectral image reconstruction; self-supervised learning,Benchmarking; Color; Deep learning; Image fusion; Learning systems; Network architecture; Deep learning; High resolution; HyperSpectral; Hyperspectral and RGB image fusion; Hyperspectral image reconstruction; Images reconstruction; Lower resolution; RGB images; Self-supervised learning; Image reconstruction
Deeply Activated Salient Region for Instance Search,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151847171&doi=10.1145%2f3510004&partnerID=40&md5=410d86580bb6fb131bfb43c4394b934c,"The performance of instance search relies heavily on the ability to locate and describe a wide variety of object instances in a video/image collection. Due to the lack of a proper mechanism for locating instances and deriving feature representation, instance search is generally only effective when the instances are from known object categories. In this article, a simple but effective instance-level feature representation approach is presented. Different from the existing approaches, the issues of class-agnostic instance localization and distinctive feature representation are considered. The former is achieved by detecting salient instance regions from an image by a layer-wise back-propagation process. The back-propagation starts from the last convolution layer of a pre-trained CNNs that is originally used for classification. The back-propagation proceeds layer by layer until it reaches the input layer. This allows the salient instance regions in the input image from both known and unknown categories to be activated. Each activated salient region covers the full or, more usually, a major range of an instance. The distinctive feature representation is produced by average-pooling on the feature map of a certain layer with the detected instance region. Experiments show that this kind of feature representation demonstrates considerably better performance than most of the existing approaches.  © 2022 Association for Computing Machinery.",back-propagation; Instance search; instance-level; response peak,Back Propagation; Feature representation; Image collections; Instance search; Instance-level; Object categories; Performance; Response peak; Salient regions; Video image; Backpropagation
Spatio-Temporal Context Based Adaptive Camcorder Recording Watermarking,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151827325&doi=10.1145%2f3503160&partnerID=40&md5=bb17ab59b03fdd56669f28c3890ba3ba,"Video watermarking technology has attracted increasing attention in the past few years, and a great deal of traditional and deep learning-based methods have been proposed. However, these existing methods usually suffer from the following two challenges: First, most algorithms cannot resist camcorder recording attack, which limits their practical application. Second, watermark embedding may cause substantial degradation of video quality. Through analyzing the unique distortions presented in the camcorder recording process, including geometric distortion, temporal sampling distortion, sensor distortion and processing distortion, this paper proposes a novel spatio-temporal context based adaptive camcorder recording watermarking scheme STACR. In STACR, considering the geometric distortion and video visual quality, we embed the watermark by constructing a spatio-temporal histogram and incorporate a content features based adaptive locating algorithm to select embedding blocks and embedding strengths. As for the temporal sampling attack, we put forward a watermark correlation-based synchronization algorithm and combine it with cross-validation. Moreover, to resist the sensor distortion, we design a local matching-based algorithm to improve the extraction accuracy. In addition, grouped and repeated embedding strategies are combined to cope with the processing distortion. Experimental results compared with the state-of-the-art show that the proposed scheme achieves high video quality and is robust to geometric attacks, compression, scaling, transcoding, recoding, frame rate changes and especially for camcorder recording.  © 2022 Association for Computing Machinery.",camcorder recording; geometric distortion; spatio-temporal context; Video watermarking,Deep learning; Embeddings; Video cameras; Video recording; Watermarking; Camcord recording; Context-based; Geometric distortion; Learning-based methods; Spatio-temporal; Spatio-temporal context; Temporal sampling; Video quality; Video watermarking; Video watermarking technology; Geometry
Affective Interaction: Attentive Representation Learning for Multi-Modal Sentiment Classification,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151793852&doi=10.1145%2f3527175&partnerID=40&md5=9d35ceafae3b4e5e5831bb9f355e401f,"The recent booming of artificial intelligence (AI) applications, e.g., affective robots, human-machine interfaces, autonomous vehicles, and so on, has produced a great number of multi-modal records of human communication. Such data often carry latent subjective users' attitudes and opinions, which provides a practical and feasible path to realize the connection between human emotion and intelligence services. Sentiment and emotion analysis of multi-modal records is of great value to improve the intelligence level of affective services. However, how to find an optimal manner to learn people's sentiments and emotional representations has been a difficult problem, since both of them involve subtle mind activity. To solve this problem, a lot of approaches have been published, but most of them are insufficient to mine sentiment and emotion, since they have treated sentiment analysis and emotion recognition as two separate tasks. The interaction between them has been neglected, which limits the efficiency of sentiment and emotion representation learning. In this work, emotion is seen as the external expression of sentiment, while sentiment is the essential nature of emotion. We thus argue that they are strongly related to each other where one's judgment helps the decision of the other. The key challenges are multi-modal fused representation and the interaction between sentiment and emotion. To solve such issues, we design an external knowledge enhanced multi-task representation learning network, termed KAMT. The major elements contain two attention mechanisms, which are inter-modal and inter-task attentions and an external knowledge augmentation layer. The external knowledge augmentation layer is used to extract the vector of the participant's gender, age, occupation, and of overall color or shape. The main use of inter-modal attention is to capture effective multi-modal fused features. Inter-task attention is designed to model the correlation between sentiment analysis and emotion classification. We perform experiments on three widely used datasets, and the experimental performance proves the effectiveness of the KAMT model.  © 2022 Association for Computing Machinery.",artificial intelligence; deep learning; emotion recognition; Multi-modal sentiment analysis; representation learning,Deep learning; Learning systems; Modal analysis; Speech recognition; Affective interaction; Deep learning; Emotion recognition; External knowledge; Human Machine Interface; Multi-modal; Multi-modal sentiment analyse; Representation learning; Sentiment analysis; Sentiment classification; Emotion Recognition
"Contrast-Enhanced Color Visual Cryptography for (k, n) Threshold Schemes",2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151801405&doi=10.1145%2f3508394&partnerID=40&md5=2cbfd1651987432155beb4c79c63c736,"In traditional visual cryptography schemes (VCSs), pixel expansion remains to be an unsolved challenge. To alleviate the impact of pixel expansion, several colored-black-and-white VCSs, called CBW-VCSs, were proposed in recent years. Although these methods could ease the effect of pixel expansion, the reconstructed image obtained by these methods may also suffer from low contrasts. To address this issue, we propose a contrast-enhanced (k, n) CBW-VCS based on random grids, named (k,n) RG-CBW-VCS, in this article. By applying color random grids, a binary secret image is encrypted into n color shares that have no pixel expansion. When any k1 (k1> k) color shares are collected together, the stacked results of them can be identified as the secret image; whereas the superposition of any k2 (k2< k) color shares shows nothing. Through theoretical analysis and experimental results, we justify the effectiveness of the proposed (k, n) RG-CBW-VCS. Compared with related methods in feature, contrast, and pixel expansion, the results indicate that the proposed method generally achieves better performance.  © 2022 Association for Computing Machinery.",contrast enhancement; pixel expansion; random grid; threshold scheme; Visual cryptography,Binary images; Cryptography; Pixels; Contrast Enhancement; Contrast-enhanced; Low contrast; Pixel expansion; Random grids; Reconstructed image; Secret images; Threshold schemes; Visual cryptography; Visual cryptography schemes; Color
Improving Feature Discrimination for Object Tracking by Structural-similarity-based Metric Learning,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127588075&doi=10.1145%2f3497746&partnerID=40&md5=8305d432a0c1409e7cea912f91fc91c2,"Existing approaches usually form the tracking task as an appearance matching procedure. However, the discrimination ability of appearance features is insufficient in these trackers, which is caused by their weak feature supervision constraints and inadequate exploitation of spatial contexts. To tackle this issue, this article proposes a novel appearance matching tracking (AMT) method to strengthen the feature restraints and capture discriminative spatial representations. Specifically, we first utilize a triplet structural loss function, which improves the learning capability of features by applying a structural similarity constraint with a triplet metric format on the features. It leverages feature statistics to capture the complex interactions of visual parts. Second, we put forward an adaptive matching module that exploits the dual spatial enhancement module to reinforce target feature discrimination. This not only boosts the representation ability of spatial context but also realizes spatially dynamic feature selection by attending to target deformation information. Moreover, this model introduces a simple but effective matching unit to intuitively evaluate the relative appearance differences between the target and the proposals. In addition, with the obtained discriminative features, AMT is capable of providing precise localization for the target. Therefore, the impact of spatial suppression imposed by window functions can be alleviated, allowing for effective tracking of high-speed moving objects. Extensive experiments prove that AMT outperforms state-of-the-art methods on six public datasets and demonstrate the effectiveness of each component in AMT.  © 2022 Association for Computing Machinery.",adaptive matching module; appearance matching; feature discrimination; reduction of spatial suppression; triplet structural loss; Visual object tracking,% reductions; Adaptive matching; Adaptive matching module; Appearance matching; Feature discrimination; Reduction of spatial suppression; Spatial suppression; Structural loss; Triplet structural loss; Visual object tracking; Tracking (position)
Cross-modal Graph Matching Network for Image-text Retrieval,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127552436&doi=10.1145%2f3499027&partnerID=40&md5=d4c8255a5edfef30fb9ea650399d8a1b,"Image-text retrieval is a fundamental cross-modal task whose main idea is to learn image-text matching. Generally, according to whether there exist interactions during the retrieval process, existing image-text retrieval methods can be classified into independent representation matching methods and cross-interaction matching methods. The independent representation matching methods generate the embeddings of images and sentences independently and thus are convenient for retrieval with hand-crafted matching measures (e.g., cosine or Euclidean distance). As to the cross-interaction matching methods, they achieve improvement by introducing the interaction-based networks for inter-relation reasoning, yet suffer the low retrieval efficiency. This article aims to develop a method that takes the advantages of cross-modal inter-relation reasoning of cross-interaction methods while being as efficient as the independent methods. To this end, we propose a graph-based Cross-modal Graph Matching Network (CGMN), which explores both intra- and inter-relations without introducing network interaction. In CGMN, graphs are used for both visual and textual representation to achieve intra-relation reasoning across regions and words, respectively. Furthermore, we propose a novel graph node matching loss to learn fine-grained cross-modal correspondence and to achieve inter-relation reasoning. Experiments on benchmark datasets MS-COCO, Flickr8K, and Flickr30K show that CGMN outperforms state-of-the-art methods in image retrieval. Moreover, CGMM is much more efficient than state-of-the-art methods using interactive matching. The code is available at https://github.com/cyh-sj/CGMN.  © 2022 Association for Computing Machinery.",cross-modal matching; graph matching; Image-text retrieval; relation reasoning,Image retrieval; Pattern matching; Cross-modal; Cross-modal matching; Graph matchings; Image texts; Image-text retrieval; Matching methods; Matching networks; Modal matching; Relation reasoning; Text retrieval; Graphic methods
Multi-granularity Brushstrokes Network for Universal Style Transfer,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127591335&doi=10.1145%2f3506710&partnerID=40&md5=4bba7c0f3662d569382212c5c26ae433,"Neural style transfer has been developed in recent years, where both performance and efficiency have been greatly improved. However, most existing methods do not transfer the brushstrokes information of style images well. In this article, we address this issue by training a multi-granularity brushstrokes network based on a parallel coding structure. Specifically, we first adopt the content parsing module to obtain the spatial distribution of content image and the smoothness of different regions. Then, different brushstrokes features are transformed by a multi-granularity style-swap module guided by the region content map. Finally, the stylized features of the two branches are fused to enhance the stylized results. The multi-granularity brushstrokes network is jointly supervised by a new multi-layer brushstroke loss and pre-existing loss. The proposed method is close to the artistic drawing process. In addition, we can control whether the color of the stylized results tend to be the style image or the content image. Experimental results demonstrate the advantage of our proposed method compare with the existing schemes.  © 2022 Association for Computing Machinery.",brushstrokes; deep feature; Neural style transfer,Brushstroke; Coding structures; Deep feature; Image wells; Multi-granularity; Multi-layers; Network-based; Neural style transfer; Parallel coding; Performance
Accelerating Transform Algorithm Implementation for Efficient Intra Coding of 8K UHD Videos,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127566052&doi=10.1145%2f3507970&partnerID=40&md5=d44ab7b73be7afba2b055eec60e49db7,"Real-time ultra-high-definition (UHD) video applications have attracted much attention, where the encoder side urgently demands the high-throughput two-dimensional (2D) transform hardware implementation for the latest video coding standards. This article proposes an effective acceleration method for transform algorithm in UHD intra coding based on the third generation of audio video coding standard (AVS3). First, by conducting detailed statistical analysis, we devise an efficient hardware-friendly transform algorithm that can reduce running cycles and resource consumption remarkably. Second, to implement multiplierless computation for saving resources and power, a series of shift-and-add unit (SAU) hardwares are investigated to have much less adoptions of shifters and adders than the existing methods. Third, different types of hardware acceleration methods, including calculation pipelining, logical-loop unrolling, and module-level parallelism, are designed to efficaciously support the data-intensive high frame-rate 8K UHD video coding. Finally, due to the scarcity of 8K video sources, we also provide a new dataset for the performance verification. Experimental results demonstrate that our proposed method can effectively fulfill the real-time 8K intra encoding at beyond 60 fps, with very negligible loss on rate-distortion (R-D) performance, which is averagely 0.98% Bjontegaard-Delta Bit-Rate (BD-BR).  © 2022 Association for Computing Machinery.",8K dataset; FPGA implementation; Image and video coding; transform hardware architecture; ultra-high-definition (UHD) video,Acceleration; Electric distortion; Field programmable gate arrays (FPGA); Image coding; Signal distortion; Video signal processing; 8k dataset; FPGA implementations; FPGAs implementation; Hardware architecture; High-definition videos; Image and video coding; Transform algorithm; Transform hardware architecture; Ultra-high; Ultra-high-definition  video; Signal encoding
Pansharpening Scheme Using Bi-dimensional Empirical Mode Decomposition and Neural Network,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127570271&doi=10.1145%2f3506709&partnerID=40&md5=0b21f768646d2e1fb3eaa72ace41baf5,"The pansharpening is a combination of multispectral (MS) and panchromatic (PAN) images that produce a high-spatial-spectral-resolution MS images. In multiresolution analysis-based pansharpening schemes, some spatial and spectral distortions are found. It can be reduced by adding spatial detail images of the PAN image into MS images. In the convolution neural network- (CNN) based method, the lowpass filter image extracted by the CNN model when MS and PAN images are directly applied into the input. The feature values are very high and reduce the conversion efficiency. In the proposed scheme, bi-dimensional empirical mode decomposition is used to extract the spatial detail information of the PAN image to reduce the feature values of the input. This extracted PAN image information is applied to the CNN to produce the non-linear changes in the image pixels and transformed into the perfect spatial detail image. It identifies the spatial and spectral detail quantity for the proposed scheme and it also varies with the different datasets automatically of the same satellite images. Simulation results in the context of qualitative and quantitative analysis demonstrate the effectiveness of proposed scheme applied on datasets collected by different satellites.  © 2022 Association for Computing Machinery.",Bi-dimensional empirical mode decomposition; convolutional neural network; multi-spectral images; pansharpening scheme,Convolution; Convolutional neural networks; Image processing; Spectral resolution; Spectroscopy; Bi-dimensional empirical mode decomposition; Convolution neural network; Convolutional neural network; Decomposition networks; Empirical Mode Decomposition; Feature values; Multi-spectral; Multispectral images; Pan-sharpening; Pansharpening scheme; Low pass filters
Harmonious Multi-branch Network for Person Re-identification with Harder Triplet Loss,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127556645&doi=10.1145%2f3501405&partnerID=40&md5=70913f47fbb2d2c955602bb9ca60c6a1,"Recently, advances in person re-identification (Re-ID) has benefitted from use of the popular multi-branch network. However, performing feature learning in a single branch with uniform partitioning is likely to separate meaningful local regions, and correlation among different branches is not well established. In this article, we propose a novel harmonious multi-branch network (HMBN) to relieve these intra-branch and inter-branch problems harmoniously. HMBN is a multi-branch network with various stripes on different branches to learn coarse-to-fine pedestrian information. We first replace the uniform partition with a horizontal overlapped partition to cover meaningful local regions between adjacent stripes in a single branch. We then incorporate a novel attention module to make all branches interact by modeling spatial contextual dependencies across branches. Finally, in order to train the HMBN more effectively, a harder triplet loss is introduced to optimize triplets in a harder manner. Extensive experiments are conducted on three benchmark datasets - DukeMTMC-reID, CUHK03, and Market-1501 - demonstrating the superiority of our proposed HMBN over state-of-the-art methods.  © 2022 Association for Computing Machinery.",attention mechanism; Person re-identification; pooling strategy; triplet loss,Attention mechanisms; Benchmark datasets; Coarse to fine; Feature learning; Learn+; Local correlations; Local region; Person re identifications; Pooling strategy; Triplet loss
3D Tooth Instance Segmentation Learning Objectness and Affinity in Point Cloud,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127564701&doi=10.1145%2f3504033&partnerID=40&md5=854adb5fc61193a36c8ccd50dad51db9,"Digital dentistry has received more attention in the past decade. However, current deep learning-based methods still encounter difficult challenges. The proposal-based methods are sensitive to the localization results due to the lack of local cues, while the proposal-free methods have poor clustering outputs because of the affinity measured by the low-level characteristics, especially in situations of tightly arranged teeth. In this article, we present a novel proposal-based approach to combine objectness and pointwise knowledge in an attention mechanism for point cloud-based tooth instance segmentation, using local information to improve 3D proposal generation and measuring the importance of local points by calculating the center distance. We evaluate the performance of our approach by constructing a Shining3D tooth instance segmentation dataset. The experimental results verify that our approach gives competitive results when compared with the other available approaches.  © 2022 Association for Computing Machinery.",Computer vision; deep learning; instance segmentation; objectness,Deep learning; 'current; Attention mechanisms; Clusterings; Deep learning; Learning-based methods; Level characteristic; Localisation; Objectness; Point wise; Point-clouds; Computer vision
A Spatial Relationship Preserving Adversarial Network for 3D Reconstruction from a Single Depth View,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127577852&doi=10.1145%2f3506733&partnerID=40&md5=9dda5a02dd142d390b7555f6fa1ec5c9,"Recovering the geometry of an object from a single depth image is an interesting yet challenging problem. While previous learning based approaches have demonstrated promising performance, they don't fully explore spatial relationships of objects, which leads to unfaithful and incomplete 3D reconstruction. To address these issues, we propose a Spatial Relationship Preserving Adversarial Network (SRPAN) consisting of 3D Capsule Attention Generative Adversarial Network (3DCAGAN) and 2D Generative Adversarial Network (2DGAN) for coarse-to-fine 3D reconstruction from a single depth view of an object. Firstly, 3DCAGAN predicts the coarse geometry using an encoder-decoder based generator and a discriminator. The generator encodes the input as latent capsules represented as stacked activity vectors with local-to-global relationships (i.e., the contribution of components to the whole shape), and then decodes the capsules by modeling local-to-local relationships (i.e., the relationships among components) in an attention mechanism. Afterwards, 2DGAN refines the local geometry slice-by-slice, by using a generator learning a global structure prior as guidance, and stacked discriminators enforcing local geometric constraints. Experimental results show that SRPAN not only outperforms several state-of-the-art methods by a large margin on both synthetic datasets and real-world datasets, but also reconstructs unseen object categories with a higher accuracy.  © 2022 Association for Computing Machinery.",3D reconstruction; a single depth view; latent capsule; self-attention,Computer vision; Decoding; Geometry; Image reconstruction; Large dataset; 3D reconstruction; A single depth view; Adversarial networks; Coarse to fine; Depth image; Latent capsule; Learning-based approach; Performance; Self-attention; Spatial relationships; Generative adversarial networks
Answer Questions with Right Image Regions: A Visual Attention Regularization Approach,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127553267&doi=10.1145%2f3498340&partnerID=40&md5=deda41e14dfc3cf6b122badd1834b055,"Visual attention in Visual Question Answering (VQA) targets at locating the right image regions regarding the answer prediction, offering a powerful technique to promote multi-modal understanding. However, recent studies have pointed out that the highlighted image regions from the visual attention are often irrelevant to the given question and answer, leading to model confusion for correct visual reasoning. To tackle this problem, existing methods mostly resort to aligning the visual attention weights with human attentions. Nevertheless, gathering such human data is laborious and expensive, making it burdensome to adapt well-developed models across datasets. To address this issue, in this article, we devise a novel visual attention regularization approach, namely, AttReg, for better visual grounding in VQA. Specifically, AttReg first identifies the image regions that are essential for question answering yet unexpectedly ignored (i.e., assigned with low attention weights) by the backbone model. And then a mask-guided learning scheme is leveraged to regularize the visual attention to focus more on these ignored key regions. The proposed method is very flexible and model-agnostic, which can be integrated into most visual attention-based VQA models and require no human attention supervision. Extensive experiments over three benchmark datasets, i.e., VQA-CP v2, VQA-CP v1, and VQA v2, have been conducted to evaluate the effectiveness of AttReg. As a by-product, when incorporating AttReg into the strong baseline LMH, our approach can achieve a new state-of-the-art accuracy of 60.00% with an absolute performance gain of 7.01% on the VQA-CP v2 benchmark dataset. In addition to the effectiveness validation, we recognize that the faithfulness of the visual attention in VQA has not been well explored in literature. In the light of this, we propose to empirically validate such property of visual attention and compare it with the prevalent gradient-based approaches.  © 2022 Association for Computing Machinery.",mask-guided learning; visual attention regularization; Visual question answering,Behavioral research; Benchmark datasets; Human attention; Image regions; Mask-guided learning; Question Answering; Regularisation; Regularization approach; Visual Attention; Visual attention regularization; Visual question answering; Benchmarking
Balanced and Accurate Pseudo-Labels for Semi-Supervised Image Classification,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151844490&doi=10.1145%2f3506711&partnerID=40&md5=a45d2e90374ca70d56f56699201ce3ef,"Image classification by semi-supervised learning has recently become a hot spot, and the Co-Training framework is an important method of semi-supervised image classification. In the traditional Co-Training structure, the sub-networks will generate pseudo-labels for each other, and these pseudo-labels will further be used as a supervisory signal for model training. However, the pseudo-labels will hurt classification performance because of their low accuracy and unbalanced distribution. In this article, we are trying to solve the preceding two problems by designing the Balanced Module (BM) and Gaussian Mixture Module (GMM), and propose BAPS (the Balanced and Accurate Pseudo-labels for Semi-supervised image classification). In BM, the two sub-networks jointly predict the unlabeled images, then select the pseudo-labels with a high-confidence threshold to perform the balancing operation to obtain the initial samples with balanced distribution of each category. In GMM, referring to the common practice of the Learning from Noise Labels task, we use GMM to fit the loss distribution of images with pseudo-labels output by BM, then clean samples and noise samples are divided based on the observation that the loss of correctly labeled images is generally smaller than that of wrongly labeled ones. Through BM and GMM, pseudo-labels with balanced distribution and high accuracy are obtained for the subsequent model training process. Our model has achieved better classification accuracy than most state-of-the-art semi-supervised image classification algorithms on the CIFAR-10/100 and SVHN datasets, and further ablation experiments demonstrate the effectiveness of our BAPS. The source code of BAPS will be available at https://github.com/zhaojianaaa.  © 2022 Association for Computing Machinery.",Balanced Module; Deep Co-Training; Gaussian Mixture Module; semi-supervised classification,Classification (of information); Deep learning; Learning algorithms; Supervised learning; Balanced module; Co-training; Deep co-training; Gaussian mixture module; Gaussian-mixtures; Model training; Semi-supervised; Semisupervised classification (SSC); Subnetworks; Supervised image classifications; Image classification
DRL based Joint Affective Services Computing and Resource Allocation in ISTN,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151910593&doi=10.1145%2f3561821&partnerID=40&md5=51fd2098d3d7ff7d931d2334607a52f3,"Affective services will become a research hotspot in artificial intelligence (AI) in the next decade. In this paper, a novel service paradigm combined with wireless communication in integrated satellite-terrestrial network (ISTN) is proposed. On this basis, an affective services computing offloading and transmission network (ASCTN) with a three-tier computation architecture is proposed, which is able to assist users to obtain affective computing services and regulate emotions. The optimization problem is investigated in the ASCTN, which is a discrete, non-linear, and non-convex problem with the limitation of computation ability of satellite and transmit power. Specifically, with the objective to minimize the cost utility related to latency and energy consumption, a joint affective services tasks computing offloading strategy, sub-channel, and power allocation algorithm based on dueling deep Q-network (Dueling-DQN) is proposed, which is in possession of better stability. The simulation results reveal the effectiveness of the optimization algorithm in terms of the cost utility in the ASCTN system.  © 2022 Association for Computing Machinery.",Affective services; computing offloading; deep reinforcement learning; resource allocation,Computation offloading; Computing power; Deep learning; Electric power transmission; Energy utilization; Green computing; Optimization; Resource allocation; Affective service; Computing offloading; Cost utility; Deep reinforcement learning; Hotspots; Reinforcement learnings; Resources allocation; Satellite-terrestrial network; Service computing; Service resources; Reinforcement learning
Binary Representation via Jointly Personalized Sparse Hashing,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151797043&doi=10.1145%2f3558769&partnerID=40&md5=58a2577fbdeca370a2a57d5707f71f9e,"Unsupervised hashing has attracted much attention for binary representation learning due to the requirement of economical storage and efficiency of binary codes. It aims to encode high-dimensional features in the Hamming space with similarity preservation between instances. However, most existing methods learn hash functions in manifold-based approaches. Those methods capture the local geometric structures (i.e., pairwise relationships) of data, and lack satisfactory performance in dealing with real-world scenarios that produce similar features (e.g., color and shape) with different semantic information. To address this challenge, in this work, we propose an effective unsupervised method, namely, Jointly Personalized Sparse Hashing (JPSH), for binary representation learning. To be specific, first, we propose a novel personalized hashing module, i.e., Personalized Sparse Hashing (PSH). Different personalized subspaces are constructed to reflect category-specific attributes for different clusters, adaptively mapping instances within the same cluster to the same Hamming space. In addition, we deploy sparse constraints for different personalized subspaces to select important features. We also collect the strengths of the other clusters to build the PSH module with avoiding over-fitting. Then, to simultaneously preserve semantic and pairwise similarities in our proposed JPSH, we incorporate the proposed PSH and manifold-based hash learning into the seamless formulation. As such, JPSH not only distinguishes the instances from different clusters but also preserves local neighborhood structures within the cluster. Finally, an alternating optimization algorithm is adopted to iteratively capture analytical solutions of the JPSH model. We apply the proposed representation learning algorithm JPSH to the similarity search task. Extensive experiments on four benchmark datasets verify that the proposed JPSH outperforms several state-of-the-art unsupervised hashing algorithms.  © 2022 Association for Computing Machinery.",Binary representation; manifold hashing; personalized hashing; similarity search,Digital storage; Iterative methods; Learning algorithms; Learning systems; Semantics; Binary representations; Hamming space; Higher dimensional features; Learn+; Local geometric structures; Manifold hashing; Performance; Personalized hashing; Real-world scenario; Similarity search; Hash functions
Scenario-Aware Recurrent Transformer for Goal-Directed Video Captioning,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127581357&doi=10.1145%2f3503927&partnerID=40&md5=b5cc7b68eea4ae1119ce962e65ca8ad9,"Fully mining visual cues to aid in content understanding is crucial for video captioning. However, most state-of-the-art video captioning methods are limited to generating captions purely based on straightforward information while ignoring the scenario and context information. To fill the gap, we propose a novel, simple but effective scenario-aware recurrent transformer (SART) model to execute video captioning. Our model contains a ""scenario understanding""module to obtain a global perspective across multiple frames, providing a specific scenario to guarantee a goal-directed description. Moreover, for the sake of achieving narrative continuity in the generated paragraph, a unified recurrent transformer is adopted. To demonstrate the effectiveness of our proposed SART, we have conducted comprehensive experiments on various large-scale video description datasets, including ActivityNet, YouCookII, and VideoStory. Additionally, we extend a story-oriented evaluation framework for assessing the quality of the generated caption more precisely. The superior performance has shown that SART has a strong ability to generate correct, deliberative, and narrative coherent video descriptions.  © 2022 Association for Computing Machinery.",long-time dependency; scenario-aware; Transformer; video captioning,Context information; Goal-directed; Long-time dependency; Scenario-aware; Simple++; State of the art; Time dependency; Transformer; Video captioning; Visual cues; Large dataset
A Novel Multi-Sample Generation Method for Adversarial Attacks,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127580702&doi=10.1145%2f3506852&partnerID=40&md5=4a886f2ae4ea17c184d30e5adadfb78d,"Deep learning models are widely used in daily life, which bring great convenience to our lives, but they are vulnerable to attacks. How to build an attack system with strong generalization ability to test the robustness of deep learning systems is a hot issue in current research, among which the research on black-box attacks is extremely challenging. Most current research on black-box attacks assumes that the input dataset is known. However, in fact, it is difficult for us to obtain detailed information for those datasets. In order to solve the above challenges, we propose a multi-sample generation model for black-box model attacks, called MsGM. MsGM is mainly composed of three parts: multi-sample generation, substitute model training, and adversarial sample generation and attack. Firstly, we design a multi-task generation model to learn the distribution of the original dataset. The model first converts an arbitrary signal of a certain distribution into the shared features of the original dataset through deconvolution operations, and then according to different input conditions, multiple identical sub-networks generate the corresponding targeted samples. Secondly, the generated sample features achieve different outputs through querying the black-box model and training the substitute model, which are used to construct different loss functions to optimize and update the generator and substitute model. Finally, some common white-box attack methods are used to attack the substitute model to generate corresponding adversarial samples, which are utilized to attack the black-box model. We conducted a large number of experiments on the MNIST and CIFAR-10 datasets. The experimental results show that under the same settings and attack algorithms, MsGM achieves better performance than the based models.  © 2022 Association for Computing Machinery.",Black-box attacks; GAN; multi-task; substitute model,Deconvolution; Large dataset; 'current; Black box modelling; Black boxes; Black-box attack; GAN; Generation method; Multi tasks; Multi-samples; Sample generations; Substitute model; Deep learning
Semantic Guided Single Image Reflection Removal,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134729073&doi=10.1145%2f3510821&partnerID=40&md5=296f30df8f11f44b31526d9fec24c960,"Reflection is common when we see through a glass window, which not only is a visual disturbance but also influences the performance of computer vision algorithms. Removing the reflection from a single image, however, is highly ill-posed since the color at each pixel needs to be separated into two values belonging to the clear background and the reflection, respectively. To solve this, existing methods use additional priors such as reflection layer smoothness, double reflection effect, and color consistency to distinguish the two layers. However, these low-level priors may not be consistently valid in real cases. In this paper, inspired by the fact that human beings can separate the two layers easily by recognizing the objects and understanding the scene, we propose to use the object semantic cue, which is high-level information, as the guidance to help reflection removal. Based on the data analysis, we develop a multi-task end-to-end deep learning method with a semantic guidance component, to solve reflection removal and semantic segmentation jointly. Extensive experiments on different datasets show significant performance gain when using high-level object-oriented information. We also demonstrate the application of our method to other computer vision tasks.  © 2022 Association for Computing Machinery.",deep learning; high-level guidance; multi-task learning; Reflection removal; semantic segmentation,Computer vision; Deep learning; Learning systems; Semantics; Computer vision algorithms; Deep learning; Glass windows; High-level guidance; Multitask learning; Performance; Reflection removals; Semantic segmentation; Single images; Two-layer; Semantic Segmentation
ESRNet: Efficient Search and Recognition Network for Image Manipulation Detection,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127553030&doi=10.1145%2f3506853&partnerID=40&md5=3403763ad0bd2a612e8515cff1f6360a,"With the widespread use of smartphones and the rise of intelligent software, we can manipulate captured photos anytime and anywhere, so the fake photos finally obtained look ""Real.""If these intelligent operation methods are maliciously applied to our daily life, then fake news, fake photos, rumors, slander, fraud, threats, and other information security issues around us can happen all the time. Today's intelligent retouching software can make various modifications to photos, some of which do not change the content that the photos themselves want to express, such as retouching, contrast improvement, and so on. In this article, we mainly study the three operation modes of changing the authenticity of photo contents, which are Copy-move, Splicing, and Removal. Few scholars have done relevant research due to the lack of a corresponding dataset. To address this issue, we elaborately collect a novel dataset, called the multi-realistic scene manipulation dataset (MSM30K), which consists of 30,000 images, including three types of tampering methods, and covering 32 different tampering scenes in life. In addition, we propose a unified detection network: the efficient search and recognition network (ESRNet) for three tampering methods. It mainly includes four main modules: Efficient feature pyramid network (EFPN), Residual receptive field block with attention (RFBA), Hierarchical decoding identification (HDI), and Cascaded group-reversal attention (GRA) blocks. On these three datasets, ESRNet can reach 0.81 on the S-measure, 0.72 on the F-measure, and 0.85 on the E-measure. The inference speed is ∼53 fps on a single GPU without I/O time. ESRNet outperforms various state-of-the-art manipulation detection baselines on three image manipulation datasets.  © 2022 Association for Computing Machinery.",image manipulation; Information security; novel dataset; unified detection network,Fake detection; Network security; Daily lives; Detection networks; Image manipulation; Intelligent operations; Intelligent software; Network efficient; Novel dataset; Operation methods; Smart phones; Unified detection network; Image enhancement
Generation of Realistic Synthetic Financial Time-series,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127599835&doi=10.1145%2f3501305&partnerID=40&md5=d51498ba9625b2ee3c6e38a6280e65dd,"Financial markets have always been a point of interest for automated systems. Due to their complex nature, financial algorithms and fintech frameworks require vast amounts of data to accurately respond to market fluctuations. This data availability is tied to the daily market evolution, so it is impossible to accelerate its acquisition. In this article, we discuss several solutions for augmenting financial datasets via synthesizing realistic time-series with the help of generative models. This problem is complex, since financial time series present very specific properties, e.g., fat-tail distribution, cross-correlation between different stocks, specific autocorrelation, cluster volatility and so on. In particular, we propose solutions for capturing cross-correlations between different stocks and for transitioning from fixed to variable length time-series without resorting to sequence modeling networks, and adapt various network architectures, e.g., fully connected and convolutional GANs, variational autoencoders, and generative moment matching networks. Finally, we tackle the problem of evaluating the quality of synthetic financial time-series. We introduce qualitative and quantitative metrics, along with a portfolio trend prediction framework that validates our generative models' performance. We carry out experiments on real-world financial data extracted from the US stock market, proving the benefits of these techniques.  © 2022 Association for Computing Machinery.",financial data prediction; fintech; generative models; synthetic financial data generation; Time-series,Automation; Commerce; Complex networks; Computer vision; Network architecture; Cross-correlations; Data generation; Data prediction; Financial data; Financial data prediction; Financial time series; Generative model; Synthetic financial data generation; Times series; Time series
Joint Source-Channel Decoding of Polar Codes for HEVC-Based Video Streaming,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127604967&doi=10.1145%2f3502208&partnerID=40&md5=f21c8c538f4fb633cce015b7ef00d1e7,"Ultra High-Definition (UHD) and Virtual Reality (VR) video streaming over 5G networks are emerging, in which High-Efficiency Video Coding (HEVC) is used as source coding to compress videos more efficiently and polar code is used as channel coding to transmit bitstream reliably over an error-prone channel. In this article, a novel Joint Source-Channel Decoding (JSCD) of polar codes for HEVC-based video streaming is presented to improve the streaming reliability and visual quality. Firstly, a Kernel Density Estimation (KDE) fitting approach is proposed to estimate the positions of error channel decoded bits. Secondly, a modified polar decoder called R-SCFlip is designed to improve the channel decoding accuracy. Finally, to combine the KDE estimator and the R-SCFlip decoder together, the JSCD scheme is implemented in an iterative process. Extensive experimental results reveal that, compared to the conventional methods without JSCD, the error data-frame correction ratios are increased. Averagely, 1.07% and 1.11% Frame Error Ratio (FER) improvements have been achieved for Additive White Gaussian Noise (AWGN) and Rayleigh fading channels, respectively. Meanwhile, the qualities of the recovered videos are significantly improved. For the 2D videos, the average Peak Signal-to-Noise Ratio (PSNR) and Structural SIMilarity (SSIM) gains reach 14% and 34%, respectively. For the 360 videos, the average improvements in terms of Weighted-to-Spherically-uniform PSNR (WS-PSNR) and Voronoi-based Video Multimethod Assessment Fusion (VI-VMAF) reach 21% and 7%, respectively.  © 2022 Association for Computing Machinery.",HEVC; Joint source-channel decoding; polar code; video streaming,5G mobile communication systems; Channel coding; Decoding; Errors; Fading channels; Gaussian noise (electronic); Iterative methods; Rayleigh fading; Signal to noise ratio; Virtual reality; White noise; Bitstreams; High definition; High-efficiency video coding; Joint source channel decoding; Kernel Density Estimation; Peak signal to noise ratio; Polar codes; Source-coding; Ultra-high; Video-streaming; Video streaming
Clustering Matters: Sphere Feature for Fully Unsupervised Person Re-identification,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127594279&doi=10.1145%2f3501404&partnerID=40&md5=ed7ec988d46df95eb7bd66baef492fb6,"In person re-identification (Re-ID), the data annotation cost of supervised learning, is huge and it cannot adapt well to complex situations. Therefore, compared with supervised deep learning methods, unsupervised methods are more in line with actual needs. In unsupervised learning, a key to solving Re-ID is to find a standard that can effectively distinguish the difference (distance) between the features of images belonging to different pedestrian identities. However, there are some differences in the images captured by different cameras (such as brightness, angle, etc.). It is well known that the training of neural networks is mainly based on the distance between features, while in unsupervised learning, especially in unsupervised learning methods based on hierarchical clustering, the distance between features plays a more important role in the clustering phase. We improve the accuracy of a deep learning method based on hierarchical clustering under fully unsupervised conditions, starting from both feature and distance metrics. First, we propose to use spherical features, by normalizing the images in the feature space, to weaken the structural differences (length) between features, while saving the feature differences (direction) between different identities. Then, we use the sum of squared errors (SSE) as a regularization term to balance different cluster states. We evaluate our method on four large-scale Re-ID datasets, and experiments show that our method achieves better results than the state-of-the-art unsupervised methods.  © 2022 Association for Computing Machinery.",deep learning; feature mapping; hierarchical clustering; Person re-identification; sphere feature; unsupervised learning,Deep learning; Large dataset; Spheres; Clusterings; Data annotation; Deep learning; Feature mapping; Hier-archical clustering; Hierarchical Clustering; Person re identifications; Re identifications; Sphere feature; Unsupervised method; Unsupervised learning
Densely Enhanced Semantic Network for Conversation System in Social Media,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127547697&doi=10.1145%2f3501799&partnerID=40&md5=c1cdee80f3bb915f40f87db2ca4d7035,"The human-computer conversation system is a significant application in the field of multimedia. To select an appropriate response, retrieval-based systems model the matching between the dialogue history and response candidates. However, most of the existing methods cannot fully capture and utilize varied matching patterns, which may degrade the performance of the systems. To address the issue, a densely enhanced semantic network (DESN) is proposed in our work. Given a multi-turn dialogue history and a response candidate, DESN first constructs the semantic representations of sentences from the word perspective, the sentence perspective, and the dialogue perspective. In particular, the dialogue perspective is a novel one introduced in our work. The dependencies between a single sentence and the whole dialogue are modeled from the dialogue perspective. Then, the response candidate and each utterance in the dialogue history are made to interact with each other. The varied matching patterns are captured for each utterance-response pair by using a dense matching module. The matching patterns of all the utterance-response pairs are accumulated in chronological order to calculate the matching degree between the dialogue history and the response. The responses in the candidate pool are ranked with the matching degree, thereby returning the most appropriate candidate. Our model is evaluated on the benchmark datasets. The experimental results prove that our model achieves significant and consistent improvement when compared with other baselines.  © 2022 Association for Computing Machinery.",deep learning; multimedia application; question answering; Social media,Deep learning; Semantic Web; Semantics; Conversation systems; Deep learning; Enhanced semantics; Human-computer conversations; Matching degree; Matching patterns; Multimedia applications; Question Answering; Semantics networks; Social media; Social networking (online)
FasterPose: A Faster Simple Baseline for Human Pose Estimation,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127579722&doi=10.1145%2f3503464&partnerID=40&md5=8a7d892ebd87a86ea4e0f8b16d9e3e54,"The performance of human pose estimation depends on the spatial accuracy of keypoint localization. Most existing methods pursue the spatial accuracy through learning the high-resolution (HR) representation from input images. By the experimental analysis, we find that the HR representation leads to a sharp increase of computational cost, while the accuracy improvement remains marginal compared with the low-resolution (LR) representation. In this article, we propose a design paradigm for cost-effective network with LR representation for efficient pose estimation, named FasterPose. Whereas the LR design largely shrinks the model complexity, how to effectively train the network with respect to the spatial accuracy is a concomitant challenge. We study the training behavior of FasterPose and formulate a novel regressive cross-entropy (RCE) loss function for accelerating the convergence and promoting the accuracy. The RCE loss generalizes the ordinary cross-entropy loss from the binary supervision to a continuous range, thus the training of pose estimation network is able to benefit from the sigmoid function. By doing so, the output heatmap can be inferred from the LR features without loss of spatial accuracy, while the computational cost and model size has been significantly reduced. Compared with the previously dominant network of pose estimation, our method reduces 58% of the FLOPs and simultaneously gains 1.3% improvement of accuracy. Extensive experiments show that FasterPose yields promising results on the common benchmarks, i.e., COCO and MPII, consistently validating the effectiveness and efficiency for practical utilization, especially the low-latency and low-energy-budget applications in the non-GPU scenarios.  © 2022 Association for Computing Machinery.",keypoint detection; Pose estimation,Benchmarking; Cost benefit analysis; Cost effectiveness; Entropy; Gesture recognition; Low power electronics; Computational costs; Cross entropy; Entropy loss; High resolution; Human pose estimations; Keypoint detection; Lower resolution; Pose-estimation; Simple++; Spatial accuracy; Budget control
An End-to-end Heterogeneous Restraint Network for RGB-D Cross-modal Person Re-identification,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127587440&doi=10.1145%2f3506708&partnerID=40&md5=cbedecd70ec7c54e95cdafe0eea0b7a7,"The RGB-D cross-modal person re-identification (re-id) task aims to identify the person of interest across the RGB and depth image modes. The tremendous discrepancy between these two modalities makes this task difficult to tackle. Few researchers pay attention to this task, and the deep networks of existing methods still cannot be trained in an end-to-end manner. Therefore, this article proposes an end-to-end module for RGB-D cross-modal person re-id. This network introduces a cross-modal relational branch to narrow the gaps between two heterogeneous images. It models the abundant correlations between any cross-modal sample pairs, which are constrained by heterogeneous interactive learning. The proposed network also exploits a dual-modal local branch, which aims to capture the common spatial contexts in two modalities. This branch adopts shared attentive pooling and mutual contextual graph networks to extract the spatial attention within each local region and the spatial relations between distinct local parts, respectively. Experimental results on two public benchmark datasets, that is, the BIWI and RobotPKU datasets, demonstrate that our method is superior to the state-of-the-art. In addition, we perform thorough experiments to prove the effectiveness of each component in the proposed method.  © 2022 Association for Computing Machinery.",cross-modal relational branch; end-to-end deep network; heterogeneous interactive learning; mutual contextual graph networks; RGB-D cross-modal person re-identification,Deep learning; Educational technology; Contextual graph; Cross-modal; Cross-modal relational branch; End to end; End-to-end deep network; Graph networks; Heterogeneous interactive learning; Interactive learning; Mutual contextual graph network; Person re identifications; RGB-D cross-modal person re-identification; Graph neural networks
Online Correction of Camera Poses for the Surround-view System: A Sparse Direct Approach,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127541004&doi=10.1145%2f3505252&partnerID=40&md5=2e689aa0b980185e3504489be5977be1,"The surround-view module is an indispensable component of a modern advanced driving assistance system. By calibrating the intrinsics and extrinsics of the surround-view cameras accurately, a top-down surround-view can be generated from raw fisheye images. However, poses of these cameras sometimes may change. At present, how to correct poses of cameras in a surround-view system online without re-calibration is still an open issue. To settle this problem, we introduce the sparse direct framework and propose a novel optimization scheme of a cascade structure. This scheme is actually composed of two levels of optimization and two corresponding photometric error based models are proposed. The model for the first-level optimization is called the ground model, as its photometric errors are measured on the ground plane. For the second level of the optimization, it's based on the so-called ground-camera model, in which photometric errors are computed on the imaging planes. With these models, the pose correction task is formulated as a nonlinear least-squares problem to minimize photometric errors in overlapping regions of adjacent bird's-eye-view images. With a cascade structure of these two levels of optimization, an appropriate balance between the speed and the accuracy can be achieved. Experiments show that our method can effectively eliminate the misalignment caused by cameras' moderate pose changes in the surround-view system. Source code and test cases are available online at https://cslinzhang.github.io/CamPoseCorrection/.  © 2022 Association for Computing Machinery.",cascade structure; direct method; photometric error minimization; Surround-view system,Errors; Photometry; Cascade structures; Direct approach; Direct method; Driving assistance systems; Error minimization; On-line corrections; Optimisations; Photometric error minimization; Photometrics; Surround-view system; Cameras
Sensor-based Human Activity Recognition Using Graph LSTM and Multi-task Classification Model,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150643116&doi=10.1145%2f3561387&partnerID=40&md5=9f529b618e75f4685d87d9977d7135b8,"This paper explores human activities recognition from sensor-based multi-dimensional streams. Recently, deep learning-based methods such as LSTM and CNN have achieved important progress in practical application scenarios. However, in most previous deep learning-based methods exist potential challenges such as class imbalance and multi-modal heterogeneity with time and sensor signals. To handle those problems, we propose a graph LSTM and Metric Learning model (GLML) with multiple construction graph fusion by modeling the sensor-aspect signals and the graph-aspect activities. GLML is a semi-supervised co-training architecture, which can be seen as several iteratively pseudo-labels sampling processing in the unlabeled data. Specifically, we construct three graphs to capture the different relations in each timestamp. Meanwhile, the graph attention model and attention mechanism are proposed to integrate multiple graph interactions for different sensor signals. Furthermore, to obtain a fixed representation of hidden state units and their neighboring nodes, we introduce the Graph LSTM to learn the graph-aspect relations from graph-structured constructed graphs. Notably, we propose a multi-task classification model combining loss function for classification distribution with deep metric learning to enhance the representation ability of the multi-modal sensor data. Experimental results on three public datasets demonstrate that our proposed GLML model has at least 2.44% improved in average against the state-of-the-art methods.  © 2022 Association for Computing Machinery.",graph LSTM; graph structure learning; Human activity recognition; multiple graph fusion; pseudo-labeling,Classification (of information); Graph structures; Graphic methods; Learning systems; Long short-term memory; Pattern recognition; Graph LSTM; Graph structure learning; Graph structures; Human activity recognition; Labelings; Learning models; Metric learning; Multiple graph fusion; Pseudo-labeling; Structure-learning; Iterative methods
Towards Corruption-Agnostic Robust Domain Adaptation,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127570107&doi=10.1145%2f3501800&partnerID=40&md5=a4db8122c29ea81b22a807d4d94def77,"Great progress has been achieved in domain adaptation in decades. Existing works are always based on an ideal assumption that testing target domains are independent and identically distributed with training target domains. However, due to unpredictable corruptions (e.g., noise and blur) in real data, such as web images and real-world object detection, domain adaptation methods are increasingly required to be corruption robust on target domains. We investigate a new task, corruption-agnostic robust domain adaptation (CRDA), to be accurate on original data and robust against unavailable-for-training corruptions on target domains. This task is non-trivial due to the large domain discrepancy and unsupervised target domains. We observe that simple combinations of popular methods of domain adaptation and corruption robustness have suboptimal CRDA results. We propose a new approach based on two technical insights into CRDA, as follows: (1) an easy-to-plug module called domain discrepancy generator (DDG) that generates samples that enlarge domain discrepancy to mimic unpredictable corruptions; (2) a simple but effective teacher-student scheme with contrastive loss to enhance the constraints on target domains. Experiments verify that DDG maintains or even improves its performance on original data and achieves better corruption robustness than baselines. Our code is available at: https://github.com/YifanXu74/CRDA.  © 2022 Association for Computing Machinery.",corruption robustness; Domain adaptation; transfer learning,HTTP; Object detection; Adaptation methods; Corruption robustness; Domain adaptation; Non-trivial; Real-world; Robust domain; Simple++; Target domain; Transfer learning; Web images; Crime
Image-Based Personality Questionnaire Design,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127544117&doi=10.1145%2f3503489&partnerID=40&md5=d5ca1e58b75c65bb43ea4072bf47e3d3,"This article explores the problem of image-based personality questionnaire design. Compared with the traditional text-based personality questionnaire, the image-based personality questionnaire is more natural, truthful, and language insensitive. Instead of responding to textual questions, the subjects are provided a set of ""choose-your-favorite-image""visual questions. With each question, consisting of image options describing the same semantic concept, the subjects are requested to choose their favorite image. Based on responses to typically 15 to 25 questions, we can accurately estimate the subjects' personality traits in five dimensions. The solution to design such an image-based personality questionnaire consists of concept-question identification and image-option selection. We have presented a preliminary framework to regularize these two steps in this exploratory study. A demo automatically adapting between desktop and mobile devices is available at http://120.27.209.14/vbfi. Subjective and objective evaluations have demonstrated the feasibility of accurately estimating a subject's personality in a limited round of questions.  © 2022 Association for Computing Machinery.",image favorite behavior analysis; personality estimation; Personality questionnaire design,Behavioral research; Semantics; Behavior analysis; Concept questions; Image favorite behavior analyse; Image-based; Personality estimation; Personality questionnaire design; Personality traits; Question identifications; Questionnaire design; Semantic concept; Surveys
Decoupled Low-Light Image Enhancement,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127588776&doi=10.1145%2f3498341&partnerID=40&md5=49ed96ffb52b7f2fe119e9fb0e5fbe44,"The visual quality of photographs taken under imperfect lightness conditions can be degenerated by multiple factors, e.g., low lightness, imaging noise, color distortion, and so on. Current low-light image enhancement models focus on the improvement of low lightness only, or simply deal with all the degeneration factors as a whole, therefore leading to sub-optimal results. In this article, we propose to decouple the enhancement model into two sequential stages. The first stage focuses on improving the scene visibility based on a pixel-wise non-linear mapping. The second stage focuses on improving the appearance fidelity by suppressing the rest degeneration factors. The decoupled model facilitates the enhancement in two aspects. On the one hand, the whole low-light enhancement can be divided into two easier subtasks. The first one only aims to enhance the visibility. It also helps to bridge the large intensity gap between the low-light and normal-light images. In this way, the second subtask can be described as the local appearance adjustment. On the other hand, since the parameter matrix learned from the first stage is aware of the lightness distribution and the scene structure, it can be incorporated into the second stage as the complementary information. In the experiments, our model demonstrates the state-of-the-art performance in both qualitative and quantitative comparisons, compared with other low-light image enhancement models. In addition, the ablation studies also validate the effectiveness of our model in multiple aspects, such as model structure and loss function.  © 2022 Association for Computing Machinery.",decoupling degeneration factors; deep neural networks; Image enhancement; low-light images,Computer vision; Deep neural networks; Visibility; Color distortions; Condition; Decoupling degeneration factor; Decouplings; Imaging noise; Low light; Low-light images; Multiple factors; Subtask; Visual qualities; Image enhancement
NR-CNN: Nested-Residual Guided CNN In-loop Filtering for Video Coding,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127554955&doi=10.1145%2f3502723&partnerID=40&md5=36d6c76ee5739435d5170461c61d5906,"Recently, deep learning for video coding, such as deep predictive coding, deep transform coding, and deep in-loop filtering, has been an emerging research area. The coding gain of hybrid coding framework could be extensively promoted by the data-driven models. However, previous deep coding tools especially deep in-loop filtering mainly consider the performance improvement while pay less attention to the reliability, usability, and adaptivity of the networks. In this article, a nested-residual guided convolutional neural network (NR-CNN) structure with cascaded global shortcut and configurable residual blocks is proposed for in-loop filtering. By taking advantage of the correlation between different color components, we further extend the NR-CNN by utilizing luminance as textural and structural guidance for chrominance filtering, which significantly improves the filtering performance. To fully exploit the proposed network into codec integration, we subsequently introduce an efficient and adaptive framework consisting of an adaptive granularity optimization and a parallel inference pipeline for deep learning based filtering. The former contributes to the coding performance improvement through an adaptive decision-making based on rate-distortion analysis at various granularities. The latter reduces the running time of network inference. The extensive experimental results show the superiority of the proposed method, achieving 8.2%, 14.9%, and 13.2% BD-rate savings on average under random access (RA) configuration. Meanwhile, the proposed method also obtains better subjective quality.  © 2022 Association for Computing Machinery.",deep learning based video coding; In-loop filter; rate and distortion optimization; video coding,Convolutional neural networks; Decision making; Deep learning; Electric distortion; Image coding; Signal distortion; Coding gains; Convolutional neural network; Deep learning based video coding; Distortion optimization; In-loop filters; Loop filtering; Predictive coding; Rate optimizations; Research areas; Transform coding; Video signal processing
Detection of AI-Manipulated Fake Faces via Mining Generalized Features,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127602691&doi=10.1145%2f3499026&partnerID=40&md5=fa2a40768617678db0a2750f70613bea,"Recently, AI-manipulated face techniques have developed rapidly and constantly, which has raised new security issues in society. Although existing detection methods consider different categories of fake faces, the performance on detecting the fake faces with ""unseen""manipulation techniques is still poor due to the distribution bias among cross-manipulation techniques. To solve this problem, we propose a novel framework that focuses on mining intrinsic features and further eliminating the distribution bias to improve the generalization ability. First, we focus on mining the intrinsic clues in the channel difference image (CDI) and spectrum image (SI) view of two different aspects, including the camera imaging process and the indispensable step in AI manipulation process. Then, we introduce the Octave Convolution and an attention-based fusion module to effectively and adaptively mine intrinsic features from CDI and SI view of these two different but intrinsic aspects. Finally, we design an alignment module to eliminate the bias of manipulation techniques to obtain a more generalized detection framework. We evaluate the proposed framework on four categories of fake faces datasets with the most popular and state-of-the-art manipulation techniques and achieve very competitive performances. We further conduct experiments on cross-manipulation techniques, and the results of our method show the superior advantages on improving generalization ability.  © 2022 Association for Computing Machinery.",AI-manipulated face detection; attention fusion; generalization ability; intrinsic features mining,Fake detection; AI-manipulated face detection; Attention fusion; Difference images; Faces detection; Feature mining; Generalization ability; Intrinsic feature mining; Intrinsic features; Manipulation techniques; Spectrum images; Face recognition
SDCN2: A Shallow Densely Connected CNN for Multi-Purpose Image Manipulation Detection,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147509905&doi=10.1145%2f3510462&partnerID=40&md5=5688d77c9685c53b92e74065711ca28e,"Digital image information can be easily tampered with to harm the integrity of someone. Thus, recognizing the truthfulness and processing history of an image is one of the essential concerns in multimedia forensics. Numerous forensic methods have been developed by researchers with the ability to detect targeted editing operations. However, creating a unified forensic approach capable of detecting multiple image manipulations remains a challenging problem. In this article, a new general-purpose forensic approach is designed based on a shallow densely connected convolutional neural network (SDCN2) that exploits local dense connections and global residual learning. The residual domain is considered in the proposed network rather than the spatial domain to analyze the image manipulation artifacts because the residual domain is less dependent on image content information. To attain this purpose, a residual convolutional layer is employed at the beginning of the proposed model to adaptively learn the image manipulation features by suppressing the image content information. Then, the obtained image residuals or prediction error features are further processed by the shallow densely connected convolutional neural network for high-level feature extraction. In addition, the hierarchical features produced by the densely connected blocks and prediction error features are fused globally for better information flow across the network. The extensive experiment results show that the proposed scheme outperforms the existing state-of-the-art general-purpose forensic schemes even under anti-forensic attacks, when tested on large-scale datasets. The proposed model offers overall detection accuracies of 98.34% and 99.22% for BOSSBase and Dresden datasets, respectively, for multiple image manipulation detection. Moreover, the proposed network is highly efficient in terms of computational complexity as compared to the existing approaches.  © 2022 Association for Computing Machinery.",anti-forensic attacks; convolutional neural networks; Digital image forensics; residual learning,Convolutional neural networks; Digital forensics; E-learning; Electronic crime countermeasures; Image processing; Large dataset; Anti-forensic attack; Anti-Forensics; Content information; Convolutional neural network; Digital image forensics; Image content; Image manipulation; Multiple image; Prediction errors; Residual learning; Convolution
A Comprehensive Study of Deep Learning-based Covert Communication,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146434985&doi=10.1145%2f3508365&partnerID=40&md5=c6771a9ea0eddbf525b3469782773532,"Deep learning-based methods have been popular in multimedia analysis tasks, including classification, detection, segmentation, and so on. In addition to conventional applications, this model can be widely used for cover communication, i.e., information hiding. This article presents a review of deep learning-based covert communication scheme for protecting digital contents, devices, and models. In particular, we discuss the background knowledge, current applications, and constraints of existing deep learning-based information hiding schemes, identify recent challenges, and highlight possible research directions. Further, major role of deep learning in the area of information hiding are highlighted. Then, the contribution of surveyed scheme is also summarized and compared in the context of estimation of design objectives, approaches, evaluation metric, and weaknesses. We believe that this survey can pave the way to new research in this crucial field of information hiding in deep-learning environment.  © 2022 Association for Computing Machinery.",Deep learning; IoT; ownership; watermarking,Computer aided instruction; Digital devices; Internet of things; Learning systems; Communication schemes; Covert communications; Deep learning; Digital contents; Digital modeling; Information hiding; IoT; Learning-based methods; Multi-media analysis; Ownership; Deep learning
Deepfake Video Detection via Predictive Representation Learning,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146434617&doi=10.1145%2f3536426&partnerID=40&md5=914ce8acc1f34506acc0fea4cedaf874,"Increasingly advanced deepfake approaches have made the detection of deepfake videos very challenging. We observe that the general deepfake videos often exhibit appearance-level temporal inconsistencies in some facial components between frames, resulting in discriminative spatiotemporal latent patterns among semantic-level feature maps. Inspired by this finding, we propose a predictive representative learning approach termed Latent Pattern Sensing to capture these semantic change characteristics for deepfake video detection. The approach cascades a Convolution Neural Network-based encoder, a ConvGRU-based aggregator, and a single-layer binary classifier. The encoder and aggregator are pretrained in a self-supervised manner to form the representative spatiotemporal context features. Then, the classifier is trained to classify the context features, distinguishing fake videos from real ones. Finally, we propose a selective self-distillation fine-tuning method to further improve the robustness and performance of the detector. In this manner, the extracted features can simultaneously describe the latent patterns of videos across frames spatially and temporally in a unified way, leading to an effective and robust deepfake video detector. Extensive experiments and comprehensive analysis prove the effectiveness of our approach, e.g., achieving a very highest Area Under Curve (AUC) score of 99.94% on FaceForensics++ benchmark and surpassing 12 states of the art at least 7.90%@AUC and 8.69%@AUC on challenging DFDC and Celeb-DF(v2) benchmarks, respectively.  © 2022 Copyright held by the owner/author(s).",deep learning; Deepfake video detection; representation learning; video understanding,Classification (of information); Computer vision; Deep learning; Distillation; Fake detection; Multilayer neural networks; Signal encoding; Context features; Deep learning; Deepfake video detection; Facial components; Feature map; Representation learning; Semantic levels; Temporal inconsistencies; Video detection; Video understanding; Semantics
GLPose: Global-Local Representation Learning for Human Pose Estimation,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146431343&doi=10.1145%2f3519305&partnerID=40&md5=c6b1861e6ef5f23b072f896bf0f6e3ec,"Multi-frame human pose estimation is at the core of many computer vision tasks. Although state-of-the-art approaches have demonstrated remarkable results for human pose estimation on static images, their performances inevitably come short when being applied to videos. A central issue lies in the visual degeneration of video frames induced by rapid motion and pose occlusion in dynamic environments. This problem, by nature, is insurmountable for a single frame. Therefore, incorporating complementary visual cues from other video frames becomes an intuitive paradigm. Current state-of-the-art methods usually leverage information from adjacent frames, which unfortunately place excessive focus on only the temporally nearby frames. In this paper, we argue that combining global semantically similar information and local temporal visual context will deliver more comprehensive and more robust representations for human pose estimation. Towards this end, we present an effective framework, namely global-local enhanced pose estimation (GLPose) network. Our framework consists of a feature processing module that conditionally incorporates global semantic information and local visual context to generate a robust human representation and a feature enhancement module that excavates complementary information from this aggregated representation to enhance keyframe features for precise estimation. We empirically find that the proposed GLpose outperforms existing methods by a large margin and achieves new state-of-the-art results on large benchmark datasets.  © 2022 Association for Computing Machinery.",feature aggregation; global-local representation; Human pose estimation; pose estimation,Computer vision; Large dataset; Feature aggregation; Global-local; Global-local representation; Human pose estimations; Multi-frame; Pose-estimation; State-of-the-art approach; Static images; Video frame; Visual context; Semantics
Optimized Deep-Neural Network for Content-based Medical Image Retrieval in a Brownfield IoMT Network,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146416417&doi=10.1145%2f3546194&partnerID=40&md5=5c9ab29bd199e36683847cfd5c277393,"In this paper, a brownfield Internet of Medical Things network is introduced for imaging data that can be easily scaled out depending on the objectives, functional requirements, and the number of facilities and devices connected to it. This is further used to develop a novel Content-based Medical Image Retrieval framework. The developed framework uses DenseNet-201 architecture for generating the image descriptors. Then for classification, the optimized Deep Neural Network model has been configured through a population-based metaheuristic Differential Evolution. Differential Evolution iteratively performs the joint optimization of hyperparameters and architecture of Deep Neural Networks. The competence of the proposed model is validated on three publicly available datasets: Brain Tumor MRI dataset, Covid-19 Radiography database, and Breast Cancer MRI dataset, and by comparing it with selected models over different aspects of performance evaluation. Results show that the convergence rate of the proposed framework is very fast, and it achieves at least 97.28% accuracy across all the models. © 2022 Association for Computing Machinery.",Content-based Image Retrieval; Differential Evolution; IoMT; Optimized Neural Network,Content based retrieval; Magnetic resonance imaging; Medical imaging; Network architecture; Optimization; Brownfields; Content based medical image retrieval; Content-Based Image Retrieval; Contents-based image retrievals; Differential Evolution; Functional requirement; Imaging data; IoMT; Neural-networks; Optimized neural network; Deep neural networks
Deep Q Network-Driven Task Offloading for Efficient Multimedia Data Analysis in Edge Computing-Assisted IoV,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146423864&doi=10.1145%2f3548687&partnerID=40&md5=a572b7c3bd0edd34afd6c3810110ac6b,"With the prosperity of Industry 4.0, numerous emerging industries continue to gain popularity and their market scales are expanding ceaselessly. The Internet of Vehicles (IoV), one of the thriving intelligent industries, enjoys bright development prospects. However, at the same time, the reliability and availability of IoV applications are confronted with two major bottlenecks of time delay and energy consumption. To make matters worse, massive heterogeneous and multi-dimensional multimedia data generated on the IoV present a huge obstacle to effective data analysis. Fortunately, the advent of edge computing technology enables tasks to be offloaded to edge servers, which significantly reduces total overhead of IoV systems. Deep reinforcement learning (DRL), equipped with its excellent perception and decision-making capability, is undoubtedly a dominant technology to solve task offloading problems. In this article, we first employ an optimized Fuzzy C-means algorithm to cluster vehicles and other edge devices according to their respective service quality requirements. Then, we employ an election algorithm to assist in maintaining the stability of the IoV. Last, we propose a task-offloading algorithm based on the Deep Q Network (DQN) to acquire an optimal task offloading scheme. Massive simulation experiments demonstrate the superiority of our method in minimizing time delay and energy consumption. © 2022 Association for Computing Machinery.",Deep Q Network; Deep reinforcement learning; edge computing; Fuzzy C-means; Industry 4.0; Internet of Vehicles; multimedia data,Clustering algorithms; Decision making; Deep learning; Edge computing; Energy utilization; Fuzzy clustering; Industry 4.0; Information analysis; Learning systems; Time delay; Vehicles; C-means; Deep Q network; Deep reinforcement learning; Edge computing; Fuzzy C-mean; Internet of vehicle; Multimedia data; Reinforcement learnings; Task offloading; Time-delays; Reinforcement learning
3D Skeleton and Two Streams Approach to Person Re-identification Using Optimized Region Matching,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146427676&doi=10.1145%2f3538490&partnerID=40&md5=a54d7cc1524b0c3308c20bbe03f1d537,"Person re-identification (Re-ID) is a challenging and arduous task due to non-overlapping views, complex background, and uncontrollable occlusion in video surveillance. An existing method for capturing pedestrian local region information is to divide person regions into horizontal stripes, which may lead to invalid features and erroneous learning. To solve this problem, this paper proposes a 3D skeleton and a two-stream approach to person Re-ID. The first stream of the method uses the 3D skeleton for background filtering and region segmentation. The second stream uses Siamese net to extract the global descriptor. The features of the two streams are fused to preserve the integrity of the person. An optimized region matching method for metric learning is designed. Extensive comparing experiments were conducted with state-of-the-art Re-ID methods on the Market-1501, CUHK03, and DukeMTMC-reID datasets. Experimental results show that the proposed method outperforms the existing methods in recognition accuracy.  © 2022 Association for Computing Machinery.",3D skeleton; optimized region matching; Person re-identification; two streams,Learning systems; Musculoskeletal system; 3D skeleton; Background region; Complex background; Local region; Optimized region matching; Person re identifications; Region information; Region matching; Two-stream; Video surveillance; Security systems
Dynamic Transfer Exemplar based Facial Emotion Recognition Model Toward Online Video,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146423594&doi=10.1145%2f3538385&partnerID=40&md5=c4ed6360a3e6d063deabb42f9cc11a33,"In this article, we focus on the dynamic facial emotion recognition from online video. We combine deep neural networks with transfer learning theory and propose a novel model named DT-EFER. In detail, DT-EFER uses GoogLeNet to extract the deep features of key images from video clips. Then to solve the dynamic facial emotion recognition scenario, the framework introduces transfer learning theory. Thus, to improve the recognition performance, model DT-EFER focuses on the differences between key images instead of those images themselves. Moreover, the time complexity of this model is not high, even if previous exemplars are introduced here. In contrast to other exemplar-based models, experiments based on two datasets, namely, BAUM-1s and Extended Cohn-Kanade, have shown the efficiency of the proposed DT-EFER model.  © 2022 Association for Computing Machinery.",dynamic facial emotion recognition; exemplar-based learning model; GoogLeNet; Transfer learning,Deep neural networks; E-learning; Face recognition; Image enhancement; Learning systems; Speech recognition; Transfer learning; Dynamic facial emotion recognition; Emotion recognition; Exemplar based learning; Exemplar-based; Exemplar-based learning model; Facial emotions; Googlenet; Learning models; Online video; Transfer learning; Emotion Recognition
PMAL: A Proxy Model Active Learning Approach for Vision Based Industrial Applications,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145198540&doi=10.1145%2f3534932&partnerID=40&md5=4f8c6f84311215612fe28a688cca9220,"Deep Learning models' performance strongly correlate with availability of annotated data; however, massive data labeling is laborious, expensive, and error-prone when performed by human experts. Active Learning (AL) effectively handles this challenge by selecting the uncertain samples from unlabeled data collection, but the existing AL approaches involve repetitive human feedback for labeling uncertain samples, thus rendering these techniques infeasible to be deployed in industry related real-world applications. In the proposed Proxy Model based Active Learning technique (PMAL), this issue is addressed by replacing human oracle with a deep learning model, where human expertise is reduced to label only two small subsets of data for training proxy model and initializing the AL loop. In the PMAL technique, firstly, proxy model is trained with a small subset of labeled data, which subsequently acts as an oracle for annotating uncertain samples. Secondly, active model's training, uncertain samples extraction via uncertainty sampling, and annotation through proxy model is carried out until predefined iterations to achieve higher accuracy and labeled data. Finally, the active model is evaluated using testing data to verify the effectiveness of our technique for practical applications. The correct annotations by the proxy model are ensured by employing the potentials of explainable artificial intelligence. Similarly, emerging vision transformer is used as an active model to achieve maximum accuracy. Experimental results reveal that the proposed method outperforms the state-of-the-art in terms of minimum labeled data usage and improves the accuracy with 2.2%, 2.6%, and 1.35% on Caltech-101, Caltech-256, and CIFAR-10 datasets, respectively. Since the proposed technique offers a highly reasonable solution to exploit huge multimedia data, it can be widely used in different evolutionary industrial domains.  © 2022 Association for Computing Machinery.",Active learning; convolution neural networks; data analytics; image classification; patterns matching; proxy model; uncertainty sampling; vision transformer,Computer vision; Data Analytics; Deep learning; Learning systems; Uncertainty analysis; Active Learning; Convolution neural network; Data analytics; Images classification; Learning techniques; Model-based OPC; Pattern-matching; Proxy model; Uncertainty samplings; Vision transformer; Image classification
Perturbation-enabled Deep Federated Learning for Preserving Internet of Things-based Social Networks,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146428932&doi=10.1145%2f3537899&partnerID=40&md5=2d0324ba27fd5713a653ed0131c2b046,"Federated Learning (FL), as an emerging form of distributed machine learning (ML), can protect participants' private data from being substantially disclosed to cyber adversaries. It has potential uses in many large-scale, data-rich environments, such as the Internet of Things (IoT), Industrial IoT, Social Media (SM), and the emerging SM 3.0. However, federated learning is susceptible to some forms of data leakage through model inversion attacks. Such attacks occur through the analysis of participants' uploaded model updates. Model inversion attacks can reveal private data and potentially undermine some critical reasons for employing federated learning paradigms. This article proposes novel differential privacy (DP)-based deep federated learning framework. We theoretically prove that our framework can fulfill DP's requirements under distinct privacy levels by appropriately adjusting scaled variances of Gaussian noise. We then develop a Differentially Private Data-Level Perturbation (DP-DLP) mechanism to conceal any single data point's impact on the training phase. Experiments on real-world datasets, specifically the social media 3.0, Iris, and Human Activity Recognition (HAR) datasets, demonstrate that the proposed mechanism can offer high privacy, enhanced utility, and elevated efficiency. Consequently, it simplifies the development of various DP-based FL models with different tradeoff preferences on data utility and privacy levels.  © 2022 Association for Computing Machinery.",data perturbation; deep federated learning; Differential privacy; model inversion attacks; privacy preservation,Deep learning; Gaussian noise (electronic); Learning systems; Privacy-preserving techniques; Social networking (online); Data perturbation; Deep federated learning; Differential privacies; Distributed machine learning; Model inversion; Model inversion attack; Privacy Levels; Privacy preservation; Private data; Social media; Internet of things
Rank-in-Rank Loss for Person Re-identification,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135828522&doi=10.1145%2f3532866&partnerID=40&md5=708b3861c0baee34205c8a3ca92dd9be,"Person re-identification (re-ID) is commonly investigated as a ranking problem. However, the performance of existing re-ID models drops dramatically, when they encounter extreme positive-negative class imbalance (e.g., very small ratio of positive and negative samples) during training. To alleviate this problem, this article designs a rank-in-rank loss to optimize the distribution of feature embeddings. Specifically, we propose a Differentiable Retrieval-Sort Loss (DRSL) to optimize the re-ID model by ranking each positive sample ahead of the negative samples according to the distance and sorting the positive samples according to the angle (e.g., similarity score). The key idea of the proposed DRSL lies in minimizing the distance between samples of the same category along with the angle between them. Considering that the ranking and sorting operations are non-differentiable and non-convex, the DRSL also performs the optimization of automatic derivation and backpropagation. In addition, the analysis of the proposed DRSL is provided to illustrate that the DRSL not only maintains the inter-class distance distribution but also preserves the intra-class similarity structure in terms of angle constraints. Extensive experimental results indicate that the proposed DRSL can improve the performance of the state-of-the-art re-ID models, thus demonstrating its effectiveness and superiority in the re-ID task.  © 2022 Association for Computing Machinery.",loss function; metric learning; Person re-identification,Class imbalance; Identification modeling; Loss functions; Metric learning; Negative samples; Performance; Person re identifications; Positive/negative; Ranking problems; Re identifications
Smart City Construction and Management by Digital Twins and BIM Big Data in COVID-19 Scenario,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131696198&doi=10.1145%2f3529395&partnerID=40&md5=2806b522b52032b6005efb6ea01a6176,"With the rapid development of information technology and the spread of Corona Virus Disease 2019 (COVID-19), the government and urban managers are looking for ways to use technology to make the city smarter and safer. Intelligent transportation can play a very important role in the joint prevention. This work expects to explore the building information modeling (BIM) big data (BD) processing method of digital twins (DTs) of Smart City, thus speeding up the construction of Smart City and improve the accuracy of data processing. During construction, DTs build the same digital copy of the smart city. On this basis, BIM designs the building's keel and structure, optimizing various resources and configurations of the building. Regarding the fast data growth in smart cities, a complex data fusion and efficient learning algorithm, namely Multi-Graphics Processing Unit (GPU), is proposed to process the multi-dimensional and complex BD based on the compositive rough set model. The Bayesian network solves the multi-label classification. Each label is regarded as a Bayesian network node. Then, the structural learning approach is adopted to learn the label Bayesian network's structure from data. On the P53-old and the P53-new datasets, the running time of Multi-GPU decreases as the number of GPUs increases, approaching the ideal linear speedup ratio. With the continuous increase of K value, the deterministic information input into the tag BN will be reduced, thus reducing the classification accuracy. When K = 3, MLBN can provide the best data analysis performance. On genbase dataset, the accuracy of MLBN is 0.982 ± 0.013. Through experiments, the BIM BD processing algorithm based on Bayesian Network Structural Learning (BNSL) helps decision-makers use complex data in smart cities efficiently.  © 2022 Association for Computing Machinery.",Bayesian Network Structural Learning Algorithm; BIM; digital twins; multimedia big data; Smart City,Bayesian networks; Classification (of information); Complex networks; Computer graphics; Computer graphics equipment; Data fusion; Data handling; Decision making; E-learning; Graphics processing unit; Information management; Learning algorithms; Program processors; Smart city; Bayesia n networks; Bayesian network structural learning algorithm; Building Information Modelling; City construction; City management; Complex data; Intelligent transportation; Multimedium big data; Structural learning; Virus disease; Big data
Revisiting Local Descriptor for Improved Few-Shot Classification,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130840336&doi=10.1145%2f3511917&partnerID=40&md5=fd22238ae466079c03549958b6c1b056,"Few-shot classification studies the problem of quickly adapting a deep learner to understanding novel classes based on few support images. In this context, recent research efforts have been aimed at designing more and more complex classifiers that measure similarities between query and support images but left the importance of feature embeddings seldom explored. We show that the reliance on sophisticated classifiers is not necessary, and a simple classifier applied directly to improved feature embeddings can instead outperform most of the leading methods in the literature. To this end, we present a new method, named DCAP, for few-shot classification, in which we investigate how one can improve the quality of embeddings by leveraging Dense Classification and Attentive Pooling (DCAP). Specifically, we propose to train a learner on base classes with abundant samples to solve dense classification problem first and then meta-train the learner on plenty of randomly sampled few-shot tasks to adapt it to few-shot scenario or the test time scenario. During meta-training, we suggest to pool feature maps by applying attentive pooling instead of the widely used global average pooling to prepare embeddings for few-shot classification. Attentive pooling learns to reweight local descriptors, explaining what the learner is looking for as evidence for decision making. Experiments on two benchmark datasets show the proposed method to be superior in multiple few-shot settings while being simpler and more explainable.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",attention networks; Few-shot learning; image classification; meta-learning; visual recognition,Classification (of information); Decision making; Embeddings; Image enhancement; Attention network; Embeddings; Feature embedding; Few-shot learning; Images classification; Local descriptors; Metalearning; Shot classification; Simple++; Visual recognition; Image classification
SETTI: A Self-supervised AdvErsarial Malware DeTection ArchiTecture in an IoT Environment,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142474764&doi=10.1145%2f3536425&partnerID=40&md5=859c11415609a37610343c4ef33a2b43,"In recent years, malware detection has become an active research topic in the area of Internet of Things (IoT) security. The principle is to exploit knowledge from large quantities of continuously generated malware. Existing algorithms practise available malware features for IoT devices and lack real-time prediction behaviours. More research is thus required on malware detection to cope with real-time misclassification of the input IoT data. Motivated by this, in this article, we propose an adversarial self-supervised architecture for detecting malware in IoT networks, SETTI, considering samples of IoT network traffic that may not be labeled. In the SETTI architecture, we design three self-supervised attack techniques, namely, Self-MDS, GSelf-MDS, and ASelf-MDS. The Self-MDS method considers the IoT input data and the adversarial sample generation in real-time. The GSelf-MDS builds a generative adversarial network model to generate adversarial samples in the self-supervised structure. Finally, ASelf-MDS utilises three well-known perturbation sample techniques to develop adversarial malware and inject it over the self-supervised architecture. Also, we apply a defence method to mitigate these attacks, namely, adversarial self-supervised training, to protect the malware detection architecture against injecting the malicious samples. To validate the attack and defence algorithms, we conduct experiments on two recent IoT datasets: IoT23 and NBIoT. Comparison of the results shows that in the IoT23 dataset, the Self-MDS method has the most damaging consequences from the attacker's point of view by reducing the accuracy rate from 98% to 74%. In the NBIoT dataset, the ASelf-MDS method is the most devastating algorithm that can plunge the accuracy rate from 98% to 77%.  © 2022 Association for Computing Machinery.",adversarial examples; Internet of Things (IoT); Machine Learning (ML); malware detection; robustness; Self-Supervised Training (SSL),Generative adversarial networks; Malware; Network architecture; Perturbation techniques; Adversarial example; Internet of thing; Machine learning; Machine-learning; Malware detection; Malwares; Real- time; Robustness; Self-supervised training (SSL); Supervised trainings; Internet of things
Exploiting Attention-Consistency Loss For Spatial-Temporal Stream Action Recognition,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146425056&doi=10.1145%2f3538749&partnerID=40&md5=e21570635f26c8882a5e7841d6d9fb36,"Currently, many action recognition methods mostly consider the information from spatial streams. We propose a new perspective inspired by the human visual system to combine both spatial and temporal streams to measure their attention consistency. Specifically, a branch-independent convolutional neural network (CNN) based algorithm is developed with a novel attention-consistency loss metric, enabling the temporal stream to concentrate on consistent discriminative regions with the spatial stream in the same period. The consistency loss is further combined with the cross-entropy loss to enhance the visual attention consistency. We evaluate the proposed method for action recognition on two benchmark datasets: Kinetics400 and UCF101. Despite its apparent simplicity, our proposed framework with the attention consistency achieves better performance than most of the two-stream networks, i.e., 75.7% top-1 accuracy on Kinetics400 and 95.7% on UCF101, while reducing 7.1% computational cost compared with our baseline. Particularly, our proposed method can attain remarkable improvements on complex action classes, showing that our proposed network can act as a potential benchmark to handle complicated scenarios in industry 4.0 applications. © 2022 Association for Computing Machinery.",Action recognition; attention consistency; multi-level attention; two-stream structure,Behavioral research; Convolutional neural networks; Action recognition; Attention consistency; Human Visual System; Multi-level attention; Multilevels; Recognition methods; Spatial temporals; Stream structure; Two-stream; Two-stream structure; Benchmarking
Discriminative Visual Similarity Search with Semantically Cycle-consistent Hashing Networks,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146423824&doi=10.1145%2f3532519&partnerID=40&md5=d89905bed5927dd29f4253260ce91131,"Deep hashing has great potential in large-scale visual similarity search due to its preferable efficiency in storage and computation. Technically, deep hashing for visual similarity search inherits the powerful representation capability of deep neural networks, and it encodes visual features into compact binary codes by preserving representative semantic visual features. Works in this field mainly focus on building the relationship between the visual and objective hash spaces, while they seldom study the triadic cross-domain semantic knowledge transfer among visual, semantic, and hashing spaces, leading to a serious semantic ignorance problem during space transformation. In this article, we propose a novel deep tripartite semantically interactive hashing framework, dubbed Semantically Cycle-consistent Hashing Networks (SCHNs), for discriminative hash code learning. Particularly, we construct a flexible semantic space and a transitive latent space, in conjunction with the visual space, to jointly deduce the privileged discriminative hash space. Specifically, a new semantic space is conceived to strengthen the flexibility and completeness of categories in the semantic feature inference phase. At the same time, a transitive latent space is formulated to explore and uncover the shared semantic interactivity embedded in visual and semantic features. Moreover, to further ensure semantic consistency across multiple spaces, we propose to build a cyclic adversarial learning module to preserve and keep their semantic concurrence during space transformation. Notably, our SCHN, for the first time, establishes the cyclic principle of deep semantic-preserving hashing by adaptive semantic parsing across different spaces in a single-modal visual similarity search. In addition, the entire learning framework is jointly optimized in an end-to-end manner. Extensive experiments performed on diverse large-scale datasets evidence the superiority of our method against other state-of-the-art deep hashing algorithms.  © 2022 Association for Computing Machinery.",cycle-consistent hashing; deep hashing networks; graph hashing; image retrieval; Semantic-preserving hashing,Deep neural networks; Hash functions; Image retrieval; Knowledge management; Large dataset; Semantic Web; Consistent hashing; Cycle-consistent hashing; Deep hashing network; Graph hashing; Semantic Space; Semantic-preserving hashing; Similarity search; Space transformations; Visual feature; Visual similarity; Semantics
LANBIQUE: LANguage-based Blind Image QUality Evaluation,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146415891&doi=10.1145%2f3538649&partnerID=40&md5=402f411b9ec69b497dd98dca8685e50d,"Image quality assessment is often performed with deep networks that are fine-tuned to regress a human provided quality score of a given image. Usually, this approach may lack generalization capabilities and, while being highly precise on similar image distribution, it may yield lower correlation on unseen distortions. In particular, they show poor performances, whereas images corrupted by noise, blur, or compression have been restored by generative models. As a matter of fact, evaluation of these generative models is often performed providing anecdotal results to the reader. In the case of image enhancement and restoration, reference images are usually available. Nevertheless, using signal based metrics often leads to counterintuitive results: Highly natural crisp images may obtain worse scores than blurry ones. However, blind reference image assessment may rank images reconstructed with GANs higher than the original undistorted images. To avoid time-consuming human-based image assessment, semantic computer vision tasks may be exploited instead. In this article, we advocate the use of language generation tasks to evaluate the quality of restored images. We refer to our assessment approach as LANguage-based Blind Image QUality Evaluation (LANBIQUE). We show experimentally that image captioning, used as a downstream task, may serve as a method to score image quality, independently of the distortion process that affects the data. Captioning scores are better aligned with human rankings with respect to classic signal based or No-reference image quality metrics. We show insights on how the corruption, by artefacts, of local image structure may steer image captions in the wrong direction.  © 2022 Association for Computing Machinery.",GAN; generative models evaluation; image captioning; Image quality enhancement; image quality evaluation,Image quality; Image reconstruction; Quality control; Restoration; Semantics; GAN; Generative model; Generative model evaluation; Image assessment; Image captioning; Image quality assessment; Image quality enhancements; Image quality evaluation; Model evaluation; Reference image; Image enhancement
A Sorting Fuzzy Min-Max Model in an Embedded System for Atrial Fibrillation Detection,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144055964&doi=10.1145%2f3554737&partnerID=40&md5=b973f7ad6d5d31a02db0e546ad845e7e,"Atrial fibrillation detection (AFD) has attracted much attention in the field of embedded systems. In this study, we propose a sorting fuzzy min-max (SFMM) model, and then develop an SFMM-based embedded system for AF detection. The proposed SFMM model is essentially enhanced the fuzzy min-max (FMM) model that have been successfully applied in many classification fields. In comparison with the typical FMM model, the proposed SFMM model can overcome the limitation of the input order problem encountered in the typical FMM model. The embedded system consists of a control chip and an analog-digital conversion (ADC) chip. The STM32F407 chip is used as the control chip and the ADS1292 chip, which has a high common-mode rejection ratio (CMRR), is used as the ADC chip. A series of machine learning benchmarks are included to evaluate the performance of the SFMM model. Experimental results on AF data further demonstrate the effectiveness of the SFMM-based embedded system. © 2022 Association for Computing Machinery.",Atrial fibrillation (AF); electrocardiogram (ECG); embedded system; fuzzy min-max (FMM) model; Sorting fuzzy min-max (SFMM) model,Analog to digital conversion; Benchmarking; Diseases; Embedded systems; Sorting; Analog-digital conversion; Atrial fibrillation; Control chips; Electrocardiogram; Embedded-system; Fuzzy min-max  model; Min-max; Sorting fuzzy min-max  model; Electrocardiograms
Domain-invariant Graph for Adaptive Semi-supervised Domain Adaptation,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127436577&doi=10.1145%2f3487194&partnerID=40&md5=414cf22e7d7cc5b2d8db2951db53924e,"Domain adaptation aims to generalize a model from a source domain to tackle tasks in a related but different target domain. Traditional domain adaptation algorithms assume that enough labeled data, which are treated as the prior knowledge are available in the source domain. However, these algorithms will be infeasible when only a few labeled data exist in the source domain, thus the performance decreases significantly. To address this challenge, we propose a Domain-invariant Graph Learning (DGL) approach for domain adaptation with only a few labeled source samples. Firstly, DGL introduces the Nyström method to construct a plastic graph that shares similar geometric property with the target domain. Then, DGL flexibly employs the Nyström approximation error to measure the divergence between the plastic graph and source graph to formalize the distribution mismatch from the geometric perspective. Through minimizing the approximation error, DGL learns a domain-invariant geometric graph to bridge the source and target domains. Finally, we integrate the learned domain-invariant graph with the semi-supervised learning and further propose an adaptive semi-supervised model to handle the cross-domain problems. The results of extensive experiments on popular datasets verify the superiority of DGL, especially when only a few labeled source samples are available.  © 2022 Association for Computing Machinery.",Domain adaptation; Domain-invariant graph; Few labeled source samples; The Nyström method,Adaptation algorithms; Approximation errors; Domain adaptation; Domain-invariant graph; Few labeled source sample; Labeled data; Semi-supervised; Target domain; The nystrom method; Geometry
Learning Adaptive Spatial-Temporal Context-Aware Correlation Filters for UAV Tracking,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127202704&doi=10.1145%2f3486678&partnerID=40&md5=8fa839c57cccd5d6803bb2c0f6e41837,"Tracking in the unmanned aerial vehicle (UAV) scenarios is one of the main components of target-tracking tasks. Different from the target-tracking task in the general scenarios, the target-tracking task in the UAV scenarios is very challenging because of factors such as small scale and aerial view. Although the discriminative correlation filter (DCF)-based tracker has achieved good results in tracking tasks in general scenarios, the boundary effect caused by the dense sampling method will reduce the tracking accuracy, especially in UAV-tracking scenarios. In this work, we propose learning an adaptive spatial-temporal context-aware (ASTCA) model in the DCF-based tracking framework to improve the tracking accuracy and reduce the influence of boundary effect, thereby enabling our tracker to more appropriately handle UAV-tracking tasks. Specifically, our ASTCA model can learn a spatial-temporal context weight, which can precisely distinguish the target and background in the UAV-tracking scenarios. Besides, considering the small target scale and the aerial view in UAV-tracking scenarios, our ASTCA model incorporates spatial context information within the DCF-based tracker, which could effectively alleviate background interference. Extensive experiments demonstrate that our ASTCA method performs favorably against state-of-the-art tracking methods on some standard UAV datasets.  © 2022 Association for Computing Machinery.",Adaptive spatial-temporal context-aware; Discriminative correlation filters; UAV tracking,Adaptive filtering; Antennas; Clutter (information theory); Learning systems; Unmanned aerial vehicles (UAV); Adaptive spatial-temporal context-aware; Boundary effects; Context-Aware; Context-aware models; Correlation filters; Discriminative correlation filter; Filter-based; Spatial temporals; Tracking accuracy; Unmanned aerial vehicle tracking; Aircraft detection
Distributed Gateway Selection for Video Streaming in VANET Using IP Multicast,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127404852&doi=10.1145%2f3491388&partnerID=40&md5=6b7bb486b89f2e638bd532b36bfab6d6,"The volume of video traffic as infotainment service over vehicular ad hoc network (VANET) has rapidly increased for past few years. Providing video streaming as VANET infotainment service is very challenging because of high mobility and heterogeneity of vehicular networks. While the number of mobile gateways (vehicles connected to the Internet) needs to be minimized to reduce service cost, the streaming quality also needs to be satisfactory for end-users. Existing works either focus on gateway minimization or focus on enhancing user satisfaction. We propose a video streaming solution, namely, DGSVS, which does gateway minimization with the constrained time data delivery to end-users. We formulate our constrained gateway minimization problem as minimum set covering (MSC) problem and solve with a distributed approximation method for MSC. We assume that only a subset of vehicles in VANET run DGSVS application. Therefore, instead of application layer cooperation for gateway-client association, network layer cooperation is proposed. We propose a novel multicast protocol DSS-CAST for this purpose, which is specialized in streaming data distribution for dynamic scenarios. We compare the performance of DGSVS with other existing protocols and found that DGSVS is most effective in service cost minimization while it is able to achieve competitive QoE performance.  © 2022 Association for Computing Machinery.",Gateway selection; Heterogenous vehicular network; IP multicast; VANET; Video streaming,Gateways (computer networks); Internet protocols; Multicasting; Network layers; Quality of service; Vehicular ad hoc networks; End-users; Gateway selection; Heterogenous vehicular network; Infotainment; IP Multicast; Minimisation; Service costs; Vehicular Adhoc Networks (VANETs); Vehicular networks; Video-streaming; Video streaming
Scribble-Supervised Meibomian Glands Segmentation in Infrared Images,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127375540&doi=10.1145%2f3497747&partnerID=40&md5=c1f146b3fdf9880e944505c68b53e5ca,"Infrared imaging is currently the most effective clinical method to evaluate the morphology of the meibomian glands (MGs) in patients. As an important indicator for monitoring the development of MG dysfunction, it is necessary to accurately measure gland-drop and gland morphology. Although there are existing methods for automatic segmentation of MGs using deep learning frameworks, they require fully annotated ground-truth labels for training, which is time-consuming and laborious. In this article, we proposed a new scribble-supervised deep learning framework for segmenting the MGs, which only requires easily attainable scribble annotations for training. To cope with the shortage of supervision and regularize the network, a transformation consistent strategy is incorporated, which requires the prediction to follow the same transformation if a transform is performed on an input image of the network. The proposed segmentation method consists of two stages. In the first stage, a U-Net network is used to obtain the meibomian region segmentation map. In the second stage, we concentrate on segmenting glands in the meibomian region. We utilize the gradient prior information of the original image at the decoder part of the segmentation network, which can coarsely locate the target contour. We automatically generate reliable labels using the exponential moving average of the predictions during training and filter out the unreliable pseudo-label by uncertainty threshold. Experimental results on a local MG dataset and two other public medical image datasets demonstrate the effectiveness of the proposed segmentation framework.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Infrared imaging; Meibomian gland segmentation; Scribble-supervised; Transformation consistency,Deep learning; Image segmentation; Medical imaging; Thermography (imaging); A-transform; Automatic segmentations; Clinical methods; Gland segmentations; Ground truth; Learning frameworks; Meibomian gland segmentation; Meibomian glands; Scribble-supervised; Transformation consistency; Morphology
Objective Object Segmentation Visual Quality Evaluation: Quality Measure and Pooling Method,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127404736&doi=10.1145%2f3491229&partnerID=40&md5=61ee1b2977ad0dad57609e9a1752df0e,"Objective object segmentation visual quality evaluation is an emergent member of the visual quality assessment family. It aims to develop an objective measure instead of a subjective survey to evaluate the object segmentation quality in agreement with human visual perception. It is an important benchmark for assessing and comparing the performances of object segmentation methods in terms of visual quality. Despite its essential role, sufficient study compared with other visual quality evaluation studies is still lacking. In this article, we propose a novel full-reference objective measure that includes a two-level single object segmentation visual quality measure and a pooling method for multiple object segmentation overall visual quality. The single object segmentation visual quality measure combines a pixel-level sub-measure and a region-level sub-measure for evaluating the similarity of area, shape, and object completeness between the segmentation result and the ground truth in terms of human visual perception. For the proposed multiple object segmentation overall visual quality pooling method, the rank of each object's segmentation quality as a novel factor is integrated into the weighted harmonic mean to evaluate the overall quality. To evaluate the performance of our proposed measure, we tested it on an object segmentation subjective visual quality assessment database. The experimental results demonstrate that our proposed two-level measure and pooling method with good robustness perform better in matching subjective assessments compared with other state-of-the-art objective measures.  © 2022 Association for Computing Machinery.",Object segmentation; Objective measure; Pooling method; Visual quality evaluation,Benchmarking; Image segmentation; Vision; Human visual perception; Objective measure; Objects segmentation; Performance; Pooling method; Quality measures; Segmentation quality; Visual qualities; Visual quality assessment; Visual quality evaluation; Quality control
Deep Illumination-Enhanced Face Super-Resolution Network for Low-Light Images,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127423369&doi=10.1145%2f3495258&partnerID=40&md5=46f0c6da99668cd37a29697f9deacd86,"Face images are typically a key component in the fields of security and criminal investigation. However, due to lighting and shooting angles, faces taken under low-light conditions are often difficult to recognize. Face super-resolution (FSR) technology can restore high-resolution faces based on low-resolution inputs. However, existing face super-resolution methods typically rely on prior knowledge of inaccurate faces estimated from low-resolution images. Faces restored by low-light inputs may suffer from problems such as low brightness and many missing details. In this article, we proposed an Illumination-Enhanced Face Super-Resolution (IEFSR) model that can progressively super-resolve low-light faces of 32 × 32 pixels by an upscaling factor of 8. While reconstructing the low-light low-resolution face into a clear and high-quality face, we introduce a coarse low-resolution (LR) restoration network to recover the LR face details hidden in the dark. In the generator, we use a series of style blocks with noise to make the generated faces appear to have a more realistic visual aesthetic. Additionally, we introduce spectrum normalization in the discriminator to improve training stability. Extensive experimental evaluations show that the proposed IEFSR yields visually and metrically more attractive results than existing state-of-the-art FSR methods.  © 2022 Association for Computing Machinery.",Face super-resolution; Generative adversarial network; Low-light face images; Spectrum normalization,Image enhancement; Image reconstruction; Optical resolving power; Restoration; Face images; Face super-resolution; Low light; Low-light face image; Low-light images; Lower resolution; Normalisation; Spectra's; Spectrum normalization; Superresolution methods; Generative adversarial networks
Improving Crowd Density Estimation by Fusing Aerial Images and Radio Signals,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127420255&doi=10.1145%2f3492346&partnerID=40&md5=c20d35bfa204ae74a162b36d2ea82d8a,"A recent line of research focuses on crowd density estimation from RGB images for a variety of applications, for example, surveillance and traffic flow control. The performance drops dramatically for low-quality images, such as occlusion, or poor light conditions. However, people are equipped with various wireless devices, allowing the received signals to be easily collected at the base station. As such, another line of research utilizes received signals for crowd counting. Nevertheless, received signals offer only information regarding the number of people, while an accurate density map cannot be derived. As unmanned aerial vehicles (UAVs) are now treated as flying base stations and equipped with cameras, we make the first attempt to leverage both RGB images and received signals for crowd density estimation on UAVs. Specifically, we propose a novel network to effectively fuse the RGB images and received signal strength (RSS) information. Moreover, we design a new loss function that considers the uncertainty from RSS and makes the prediction consistent with the received signals. Experimental results show that the proposed method successfully helps break the limit of traditional crowd density estimation methods and achieves state-of-the-art performance. The proposed dataset is released as a public download for future research.  © 2022 Association for Computing Machinery.",Crowd density estimation; Data fusion; Datasets; Unmanned aerial vehicles,Antennas; Base stations; Data fusion; Image enhancement; Aerial images; Crowd density; Crowd density estimation; Dataset; Density estimation; Image signal; Radio signals; Received signal strength; Received signals; RGB images; Unmanned aerial vehicles (UAV)
GraSP: Local Grassmannian Spatio-Temporal Patterns for Unsupervised Pose Sequence Recognition,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127426478&doi=10.1145%2f3491227&partnerID=40&md5=ec5b4dc52decd7e996d6888cf746057d,"Many applications of action recognition, especially broad domains like surveillance or anomaly-detection, favor unsupervised methods considering that exhaustive labeling of actions is not possible. However, very limited work has happened in this domain. Moreover, the existing self-supervised approaches suffer from their dependency upon labeled data for finetuning. To this end, this paper puts forward a manifold based unsupervised pose-sequence recognition approach that leverages only the natural biases present in the data. It works by clustering the projections of temporal derivatives of the fragmented data on the Grassmann manifold. Temporal derivatives are formed by the inter-frame gradients with local and global metrics. To commensurate with this, a dynamic view-invariant pose representation is proposed. Additionally, a variable aggregation step is introduced for better feature vector quantization. Extensive empirical evaluation and ablations on several challenging datasets under three categories confirm the superiority of the proposed approach in contrast to current methods.  © 2022 Association for Computing Machinery.",Action recognition; Grassmann manifolds; Spatio-temporal patterns; Unsupervised pose sequence recognition,Gesture recognition; Action recognition; Anomaly detection; Grassmann manifold; Grassmannian; Labelings; Sequence recognition; Spatiotemporal patterns; Temporal derivatives; Unsupervised method; Unsupervised pose sequence recognition; Anomaly detection
Recognizing Gaits Across Walking and Running Speeds,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127448325&doi=10.1145%2f3488715&partnerID=40&md5=8d52db0d3f726a0e02af7567ed5d5cf7,"For decades, very few methods were proposed for cross-mode (i.e., walking vs. running) gait recognition. Thus, it remains largely unexplored regarding how to recognize persons by the way they walk and run. Existing cross-mode methods handle the walking-versus-running problem in two ways, either by exploring the generic mapping relation between walking and running modes or by extracting gait features which are non-/less vulnerable to the changes across these two modes. However, for the first approach, a mapping relation fit for one person may not be applicable to another person. There is no generic mapping relation given that walking and running are two highly self-related motions. The second approach does not give more attention to the disparity between walking and running modes, since mode labels are not involved in their feature learning processes. Distinct from these existing cross-mode methods, in our method, mode labels are used in the feature learning process, and a mode-invariant gait descriptor is hybridized for cross-mode gait recognition to handle this walking-versus-running problem. Further research is organized in this article to investigate the disparity between walking and running. Running is different from walking not only in the speed variances but also, more significantly, in prominent gesture/motion changes. According to these rationales, in our proposed method, we give more attention to the differences between walking and running modes, and a robust gait descriptor is developed to hybridize the mode-invariant spatial and temporal features. Two multi-task learning-based networks are proposed in this method to explore these mode-invariant features. Spatial features describe the body parts non-/less affected by mode changes, and temporal features depict the instinct motion relation of each person. Mode labels are also adopted in the training phase to guide the network to give more attention to the disparity across walking and running modes. In addition, relevant experiments on OU-ISIR Treadmill Dataset A have affirmed the effectiveness and feasibility of the proposed method. A state-of-the-art result can be achieved by our proposed method on this dataset.  © 2022 Association for Computing Machinery.",Cross-mode gait recognition; Running mode; Walking mode,Machine learning; Mapping; Pattern recognition; Cross-mode gait recognition; Descriptors; Feature learning; Gait recognition; Generic mapping; Learning process; Mapping relation; Mode method; Running mode; Walking mode; Gait analysis
When Pairs Meet Triplets: Improving Low-Resource Captioning via Multi-Objective Optimization,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127386007&doi=10.1145%2f3492325&partnerID=40&md5=ce314edc088aebe6af3a379788efc12f,"Image captioning for low-resource languages has attracted much attention recently. Researchers propose to augment the low-resource caption dataset into (image, rich-resource language, and low-resource language) triplets and develop the dual attention mechanism to exploit the existence of triplets in training to improve the performance. However, datasets in triplet form are usually small due to their high collecting cost. On the other hand, there are already many large-scale datasets, which contain one pair from the triplet, such as caption datasets in the rich-resource language and translation datasets from the rich-resource language to the low-resource language. In this article, we revisit the caption-translation pipeline of the translation-based approach to utilize not only the triplet dataset but also large-scale paired datasets in training. The caption-translation pipeline is composed of two models, one caption model of the rich-resource language and one translation model from the rich-resource language to the low-resource language. Unfortunately, it is not trivial to fully benefit from incorporating both the triplet dataset and paired datasets into the pipeline, due to the gap between the training and testing phases and the instability in the training process. We propose to jointly optimize the two models of the pipeline in an end-to-end manner to bridge the training and testing gap, and introduce two auxiliary training objectives to stabilize the training process. Experimental results show that the proposed method improves significantly over the state-of-the-art methods.  © 2022 Association for Computing Machinery.",Auxiliary training objective; Bridge the gap; Image captioning; Low-resource; Paired dataset; Triplet dataset,Deep learning; Image enhancement; Large dataset; Multiobjective optimization; Statistical tests; Translation (languages); Auxiliary training objective; Bridge the gap; Image captioning; Low resource languages; Low-resource; Multi-objectives optimization; Paired dataset; Training and testing; Training process; Triplet dataset; Pipelines
Towards Integrating Image Encryption with Compression: A Survey,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127429288&doi=10.1145%2f3498342&partnerID=40&md5=fb868b0f1b64ad2c01b2b1bebc684821,"As digital images are consistently generated and transmitted online, the unauthorized utilization of these images is an increasing concern that has a significant impact on both security and privacy issues; additionally, the representation of digital images requires a large amount of data. In recent years, an image compression scheme has been widely considered; such a scheme saves on hardware storage space and lowers both the transmission time and bandwidth demand for various potential applications. In this article, we review the various approaches taken to consider joint encryption and compression, assessing both their merits and their limitations. In addition to the survey, we also briefly introduce the most interesting and most often utilized applications of image encryption and evaluation metrics, providing an overview of the various kinds of image encryption schemes available. The contribution made by these approaches is then summarized and compared, offering a consideration of the different technical perspectives. Lastly, we highlight the recent challenges and some potential research directions that could fill the gaps in these domains for both researchers and developers.  © 2022 Association for Computing Machinery.",Compression; Digital images; Encryption; Joint encryption compression,Digital storage; Image compression; Surveys; Compression; Compression scheme; Digital image; Images compression; Images encryptions; Joint encryption compression; Large amounts of data; Security and privacy issues; Storage spaces; Transmission time; Cryptography
Shuffle-invariant Network for Action Recognition in Videos,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127410945&doi=10.1145%2f3485665&partnerID=40&md5=d27caa7e50b44f85adc1c1ab859ca607,"The local key features in video are important for improving the accuracy of human action recognition. However, most end-to-end methods focus on global feature learning from videos, while few works consider the enhancement of the local information in a feature. In this article, we discuss how to automatically enhance the ability to discriminate the local information in an action feature and improve the accuracy of action recognition. To address these problems, we assume that the critical level of each region for the action recognition task is different and will not change with the region location shuffle. We therefore propose a novel action recognition method called the shuffle-invariant network. In the proposed method, the shuffled video is generated by regular region cutting and random confusion to enhance the input data. The proposed network adopts the multitask framework, which includes one feature backbone network and three task branches: local critical feature shuffle-invariant learning, adversarial learning, and an action classification network. To enhance the local features, the feature response of each region is predicted by a local critical feature learning network. To train this network, an L1-based critical feature shuffle-invariant loss is defined to ensure that the ordered feature response list of these regions remains unchanged after region location shuffle. Then, the adversarial learning is applied to eliminate the noise caused by the region shuffle. Finally, the action classification network combines these two tasks to jointly guide the training of the feature backbone network and obtain more effective action features. In the testing phase, only the action classification network is applied to identify the action category of the input video. We verify the proposed method on the HMDB51 and UCF101 action datasets. Several ablation experiments are constructed to verify the effectiveness of each module. The experimental results show that our approach achieves the state-of-the-art performance.  © 2022 Association for Computing Machinery.",Action recognition; Adversarial learning; Critical feature sort loss; Key region detection; Shuffle-invariant network,Classification (of information); Action classifications; Action recognition; Adversarial learning; Classification networks; Critical feature sort loss; Critical features; Feature learning; Key region detection; Region detection; Shuffle-invariant network; Machine learning
Inner Knowledge-based Img2Doc Scheme for Visual Question Answering,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127410784&doi=10.1145%2f3489142&partnerID=40&md5=5584157cbe713524f09241a1e19306bd,"Visual Question Answering (VQA) is a research topic of significant interest at the intersection of computer vision and natural language understanding. Recent research indicates that attributes and knowledge can effectively improve performance for both image captioning and VQA. In this article, an inner knowledge-based Img2Doc algorithm for VQA is presented. The inner knowledge is characterized as the inner attribute relationship in visual images. In addition to using an attribute network for inner knowledge-based image representation, VQA scheme is associated with a question-guided Doc2Vec method for question-answering. The attribute network generates inner knowledge-based features for visual images, while a novel question-guided Doc2Vec method aims at converting natural language text to vector features. After the vector features are extracted, they are combined with visual image features into a classifier to provide an answer. Based on our model, the VQA problem is resolved by textual question answering. The experimental results demonstrate that the proposed method achieves superior performance on multiple benchmark datasets.  © 2022 Association for Computing Machinery.",Attribute network; Dense image captioning; Doc2Vec; Inner knowledge-based; VQA,Image enhancement; Image representation; Natural language processing systems; Visual languages; Attribute network; Dense image captioning; Doc2vec; Image captioning; Inner knowledge-based; Knowledge based; Question Answering; Research topics; Visual image; Visual question answering; Benchmarking
Blockchain-Based Audio Watermarking Technique for Multimedia Copyright Protection in Distribution Networks,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127399262&doi=10.1145%2f3492803&partnerID=40&md5=4e3d5852c19b710a2acab118a6e97cf3,"Copyright protection in multimedia protection distribution is a challenging problem. To protect multimedia data, many watermarking methods have been proposed in the literature. However, most of them cannot be used effectively in a multimedia distribution network (MDN) as they are not designed to support multi-layer watermark embedding. Multi-layer watermarking mechanisms were developed to protect multimedia data across different layers in an MDN. However, in those mechanisms, we need to trust the entities in the MDN, such as regional and country distributors. To overcome this potential drawback, in this article, we propose a novel privacy protection mechanism for MDNs by combining the advantages of both blockchain and watermarking technologies. A specifically designed watermarking algorithm is used to link the copyright information with the audio file, while a novel blockchain-based smart contract mechanism is developed to enforce the proper functioning of each entity in the distribution network. Moreover, the new audio mechanism is computationally efficient. Although audio signals are used to show the effectiveness of the proposed mechanism, the proposed approach can easily be extended to other multimedia objects, such as an image. The validity of the proposed mechanism is demonstrated by our simulation results. The proposed mechanism can benefit multimedia production companies and other entities in the MDN.  © 2022 Association for Computing Machinery.",Blockchain; Copyright protection; Smart contract; Watermarking,Audio watermarking; Copyrights; Watermarking; Block-chain; Copyright protections; Multi-layers; Multimedia copyright protection; Multimedia data; Multimedia distribution networks; Multimedia protection; Watermark embedding; Watermarking algorithms; Watermarking methods; Smart contract
Causal Inference with Knowledge Distilling and Curriculum Learning for Unbiased VQA,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127390611&doi=10.1145%2f3487042&partnerID=40&md5=154280d2dc68440a809bd71389cb6973,"Recently, many Visual Question Answering (VQA) models rely on the correlations between questions and answers yet neglect those between the visual information and the textual information. They would perform badly if the handled data distribute differently from the training data (i.e., out-of-distribution (OOD) data). Towards this end, we propose a two-stage unbiased VQA approach that addresses the unbiased issue from a causal perspective. In the causal inference stage, we mark the spurious correlation on the causal graph, explore the counterfactual causality, and devise a causal target based on the inherent correlations between the conventional and counterfactual VQA models. In the distillation stage, we introduce the causal target into the training process and leverages distilling as well as curriculum learning to capture the unbiased model. Since Causal Inference with Knowledge Distilling and Curriculum Learning (CKCL) reinforces the contribution of the visual information and eliminates the impact of the spurious correlation by distilling the knowledge in causal inference to the VQA model, it contributes to the good performance on both the standard data and out-of-distribution data. The extensive experimental results on VQA-CP v2 dataset demonstrate the superior performance of the proposed method compared to the state-of-the-art (SotA) methods.  © 2022 Copyright held by the owner/author(s).",Causal inference; Knowledge distillation; Neural networks; Visual question answering,Distillation; Learning systems; Causal inferences; Counterfactuals; Knowledge distillation; Neural-networks; Performance; Question Answering; Textual information; Training data; Visual information; Visual question answering; Curricula
Matching Faces and Attributes between the Artistic and the Real Domain: The PersonArt Approach,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127378607&doi=10.1145%2f3490033&partnerID=40&md5=5ae679dca73d75b4225613e66fe702ab,"In this article, we present an approach for retrieving similar faces between the artistic and the real domain. The application we refer to is an interactive exhibition inside a museum, in which a visitor can take a photo of himself and search for a lookalike in the collection of paintings. The task requires not only to identify faces but also to extract discriminative features from artistic and photo-realistic images, tackling a significant domain shift. Our method integrates feature extraction networks which account for the aesthetic similarity of two faces and their correspondences in terms of semantic attributes. Also, it addresses the domain shift between realistic images and paintings by translating photo-realistic images into the artistic domain. Noticeably, by exploiting the same technique, our model does not need to rely on annotated data in the artistic domain. Experimental results are conducted on different paired datasets to show the effectiveness of the proposed solution in terms of identity and attribute preservation. The approach is also evaluated on unpaired settings and in combination with an interactive relevance feedback strategy. Finally, we show how the proposed algorithm has been implemented in a real showcase at the Gallerie Estensi museum in Italy, with the participation of more than 1,100 visitors in just three days.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Cultural heritage; Face recognition; Face retrieval; Face similarity,Exhibitions; Museums; Semantics; Cultural heritages; Discriminative features; Face retrieval; Face similarity; Features extraction; Interactive exhibitions; Matchings; Photorealistic images; Realistic images; Semantic attribute; Face recognition
Skeleton Sequence and RGB Frame Based Multi-Modality Feature Fusion Network for Action Recognition,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127422474&doi=10.1145%2f3491228&partnerID=40&md5=ef402bed566e1bb96342ed3a9c5c85a7,"Action recognition has been a heated topic in computer vision for its wide application in vision systems. Previous approaches achieve improvement by fusing the modalities of the skeleton sequence and RGB video. However, such methods pose a dilemma between the accuracy and efficiency for the high complexity of the RGB video network. To solve the problem, we propose a multi-modality feature fusion network to combine the modalities of the skeleton sequence and RGB frame instead of the RGB video, as the key information contained by the combination of the skeleton sequence and RGB frame is close to that of the skeleton sequence and RGB video. In this way, complementary information is retained while the complexity is reduced by a large margin. To better explore the correspondence of the two modalities, a two-stage fusion framework is introduced in the network. In the early fusion stage, we introduce a skeleton attention module that projects the skeleton sequence on the single RGB frame to help the RGB frame focus on the limb movement regions. In the late fusion stage, we propose a cross-attention module to fuse the skeleton feature and the RGB feature by exploiting the correlation. Experiments on two benchmarks, NTU RGB+D and SYSU, show that the proposed model achieves competitive performance compared with the state-of-the-art methods while reducing the complexity of the network.  © 2022 Association for Computing Machinery.",Action recognition; Attention; Feature fusion; Multi-modality; Neural networks,Complex networks; Computer vision; Musculoskeletal system; Action recognition; Attention; Features fusions; Frame-based; High complexity; Large margins; Multi-modality; Neural-networks; Video networks; Vision systems; Benchmarking
Interactive Re-ranking via Object Entropy-Guided Question Answering for Cross-Modal Image Retrieval,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127396893&doi=10.1145%2f3485042&partnerID=40&md5=9416d7da7f118362b162882ca074d4bb,"Cross-modal image-retrieval methods retrieve desired images from a query text by learning relationships between texts and images. Such a retrieval approach is one of the most effective ways of achieving the easiness of query preparation. Recent cross-modal image-retrieval methods are convenient and accurate when users input a query text that can be used to uniquely identify the desired image. However, in reality, users frequently input ambiguous query texts, and these ambiguous queries make it difficult to obtain desired images. To overcome these difficulties, in this study, we propose a novel interactive cross-modal image-retrieval method based on question answering. The proposed method analyzes candidate images and asks users questions to obtain information that can narrow down retrieval candidates. By only answering questions generated by the proposed method, users can reach their desired images, even when using an ambiguous query text. Experimental results show the proposed method's effectiveness.  © 2022 Association for Computing Machinery.",Cross-modal image retrieval; Question answering; Re-ranking,Cross-modal; Cross-modal image retrieval; Method analysis; Question Answering; Re-ranking; Retrieval methods; User input; Image retrieval
Multilayer Video Encoding for QoS Managing of Video Streaming in VANET Environment,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127433527&doi=10.1145%2f3491433&partnerID=40&md5=74ee870f05ac4de863990acab797c8ae,"Efficient delivery and maintenance of the quality of service (QoS) of audio/video streams transmitted over VANETs for mobile and heterogeneous nodes are one of the major challenges in the convergence of this network type and these services. In this context, we propose an inter-layer approach for multimedia stream transmission in a VANET environment (VSMENET). The main idea of our work is based on the dynamic adaptation of the transmission rate according to the physical rate available in the VANET. VSMENET is all about eliminating downtime during video playback by vehicle users. This involves adapting the quality of the video to the actual performance of the VANETs, intelligent encoding of video on the Road Side Units (RSU) side, and finally continuous maintenance of the calculation tasks on the RSU side and sufficient video data on the vehicle node side. Thus, we are interested in the process of evaluating the strict parameters of the VANETs, influencing the video transmission. For example, we propose, on the one hand, an architecture for intelligent data selection and good clock synchronization, and, on the other hand, efficient management of the availability and consumption of video data. We used the NetSim simulator to test the proposed approach performance. To this end, several algorithms such as OCLFEC, MAC, ShieldHEVC, and AntArmour have been implemented for such a performance comparison. Our work suggests that VSMENET is well concerning the average lifetime of the video packets and their delivery rate (more than 9% gain compared with other approaches).  © 2022 Association for Computing Machinery.",Adaptive broadcasting; Double rotating buffer; Limited resources; QoS; Superimposable images; VANET; Video Streaming,Encoding (symbols); Image coding; Image communication systems; Maintenance; Quality of service; Signal encoding; Vehicular ad hoc networks; Video recording; Adaptive broadcasting; Double rotating; Double rotating buffer; Limited resource; Performance; Quality-of-service; Road sides; Superimposable image; VANET; Video-streaming; Video streaming
CRAR: Accelerating Stereo Matching with Cascaded Residual Regression and Adaptive Refinement,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127374189&doi=10.1145%2f3488719&partnerID=40&md5=4548faa68ba252f2dbbe3e976ccdb636,"Dense stereo matching estimates the depth for each pixel of the referenced images. Recently, deep learning algorithms have dramatically promoted the development of stereo matching. The state-of-the-art result is achieved by models adopting deep convolutional neural networks. However, a considerable computational burden is also introduced, which slows the inference. To solve this problem, previous works down-sampled the input images to decrease the spatial size. However, down-sampling increases the error rate and its lower bound. In this article, we accelerate stereo matching algorithms through the improvement of network structure. Inspired by network compression, we conduct decomposition and sparsification to squeeze the computationally expensive cost optimization network. It is sparsified and then decomposed into smaller networks, which are designed and trained in a cascaded manner to reach the nearest possible performance of the larger network. Previous methods have utilized numerous refinement methods to adjust the coarse disparity. We integrate refinement methods to create an unified algorithm to utilize parallelism for running devices to further accelerate the inference. The extensive experiments on Kitti2015, Kitti2012, and Middlebury datasets demonstrate the efficiency of our method.  © 2022 Association for Computing Machinery.",Depth estimation; Real-time algorithm; Stereo matching,Inference engines; Stereo image processing; Adaptive refinement; Computational burden; Dense stereo matching; Depth Estimation; Input image; Real time algorithms; Refinement methods; Spatial size; State of the art; Stereo-matching; Deep neural networks
A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127410833&doi=10.1145%2f3490686&partnerID=40&md5=ee9601695bdb2706e12a19f73adfbcb3,"Considerable attention has been paid to physiological signal-based emotion recognition in the field of affective computing. For reliability and user-friendly acquisition, electrodermal activity (EDA) has a great advantage in practical applications. However, EDA-based emotion recognition with large-scale subjects is still a tough problem. The traditional well-designed classifiers with hand-crafted features produce poorer results because of their limited representation abilities. And the deep learning models with auto feature extraction suffer the overfitting drop-off because of large-scale individual differences. Since music has a strong correlation with human emotion, static music can be involved as the external benchmark to constrain various dynamic EDA signals. In this article, we make an attempt by fusing the subject's individual EDA features and the external evoked music features. And we propose an end-to-end multimodal framework, the one-dimensional residual temporal and channel attention network (RTCAN-1D). For EDA features, the channel-temporal attention mechanism for EDA-based emotion recognition is first involved in mine the temporal and channel-wise dynamic and steady features. The comparisons with single EDA-based SOTA models on DEAP and AMIGOS datasets prove the effectiveness of RTCAN-1D to mine EDA features. For music features, we simply process the music signal with the open-source toolkit openSMILE to obtain external feature vectors. We conducted systematic and extensive evaluations. The experiments on the current largest music emotion dataset PMEmo validate that the fusion of EDA and music is a reliable and efficient solution for large-scale emotion recognition.  © 2022 Association for Computing Machinery.",Attention mechanism; Large-scale emotion recognition; Multimodal fusion,Deep learning; Dynamics; Electrodes; Activity-based; Affective Computing; Attention mechanisms; Electrodermal activity; Emotion recognition; Large-scale emotion recognition; Large-scales; Multi-modal fusion; Multimodal frameworks; Physiological signals; Speech recognition
Enhanced 3D Shape Reconstruction with Knowledge Graph of Category Concept,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127415570&doi=10.1145%2f3491224&partnerID=40&md5=086512c03fc36835e572ea645d69c566,"Reconstructing three-dimensional (3D) objects from images has attracted increasing attention due to its wide applications in computer vision and robotic tasks. Despite the promising progress of recent deep learning-based approaches, which directly reconstruct the full 3D shape without considering the conceptual knowledge of the object categories, existing models have limited usage and usually create unrealistic shapes. 3D objects have multiple forms of representation, such as 3D volume, conceptual knowledge, and so on. In this work, we show that the conceptual knowledge for a category of objects, which represents objects as prototype volumes and is structured by graph, can enhance the 3D reconstruction pipeline. We propose a novel multimodal framework that explicitly combines graph-based conceptual knowledge with deep neural networks for 3D shape reconstruction from a single RGB image. Our approach represents conceptual knowledge of a specific category as a structure-based knowledge graph. Specifically, conceptual knowledge acts as visual priors and spatial relationships to assist the 3D reconstruction framework to create realistic 3D shapes with enhanced details. Our 3D reconstruction framework takes an image as input. It first predicts the conceptual knowledge of the object in the image, then generates a 3D object based on the input image and the predicted conceptual knowledge. The generated 3D object satisfies the following requirements: (1) it is consistent with the predicted graph in concept, and (2) consistent with the input image in geometry. Extensive experiments on public datasets (i.e., ShapeNet, Pix3D, and Pascal3D+) with 13 object categories show that (1) our method outperforms the state-of-the-art methods, (2) our prototype volume-based conceptual knowledge representation is more effective, and (3) our pipeline-agnostic approach can enhance the reconstruction quality of various 3D shape reconstruction pipelines.  © 2022 Association for Computing Machinery.",3D reconstruction; Conceptual knowledge; Deep learning,Deep neural networks; Graphic methods; Image reconstruction; Knowledge graph; Three dimensional computer graphics; 3-D shape; 3D object; 3D reconstruction; 3D shape reconstruction; Conceptual knowledge; Deep learning; Input image; Knowledge graphs; Object categories; Reconstruction frameworks; Pipelines
A Format-compatible Searchable Encryption Scheme for JPEG Images Using Bag-of-words,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127423464&doi=10.1145%2f3492705&partnerID=40&md5=c76711f5d4af55abba7808699e23ace2,"The development of cloud computing attracts enterprises and individuals to outsource their data, such as images, to the cloud server. However, direct outsourcing causes the extensive concern of privacy leakage, as images often contain rich sensitive information. A straightforward way to protect privacy is to encrypt the images using the standard cryptographic tools before outsourcing. However, in such a way the possible usage of the outsourced images would be strongly limited together with the services provided to users, like the Content-Based Image Retrieval (CBIR). In this article, we propose a secure outsourced CBIR scheme, in which an encryption scheme is designed for the widely used JPEG-format images, and the secure features can be directly extracted from such encrypted images. Specifically, the JPEG images are encrypted by the block permutation, intra-block permutation, polyalphabetic cipher, and stream cipher. Then secure local histograms are extracted from the encrypted DCT blocks and the Bag-Of-Words (BOW) model is further used to organize the encrypted local features to represent the image. The proposed image encryption gets all of the image data protected and the experimental results show that the proposed scheme achieves improved accuracy with a small file size expansion.  © 2022 Association for Computing Machinery.",Outsourced computing; Privacy-preserving image retrieval; Searchable encryption,Cloud computing; Content based retrieval; Image enhancement; Privacy-preserving techniques; Bag of words; Cloud-computing; Content-Based Image Retrieval; Contents-based image retrievals; Encryption schemes; JPEG image; Outsourced computing; Privacy preserving; Privacy-preserving image retrieval; Searchable encryptions; Outsourcing
Machine Learning Based Content-Agnostic Viewport Prediction for 360-Degree Video,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123016339&doi=10.1145%2f3474833&partnerID=40&md5=e924c4eced36338f8181389d77c2dc29,"Accurate and fast estimations or predictions of the (near) future location of the users of head-mounted devices within the virtual omnidirectional environment open a plethora of opportunities in application domains such as interactive immersive gaming and tele-surgery. Therefore, the past years have seen growing attention to models for viewport prediction in 360 ¯ environments. Among the approaches, content-agnostic, trajectory-based methods have the potential to provide very fast solutions, as they do not require complex analysis of the videos to provide a prediction. However, accurate trajectory-based viewport prediction is rather difficult due to the intrinsic variability in user behaviour. Furthermore, even when making use of machine learning, current approaches tend to be brute-force and heavily tailored to specific datasets with little comparison to existing benchmarks or publicly available studies. This article presents a generic, content-agnostic viewport prediction method consisting of a window-based approach combined with a preprocessing system to classify behavioural patterns in terms of user clustering and trajectory correlation. Moreover, as the state of the art does not provide a comparative analysis of different approaches, this work contributes to this. Based on the obtained results, a combined prediction model is proposed and evaluated. Our method shows a 36.8% to 53.9% improvement when compared to the static prediction baseline for a prediction horizon of 8 seconds. In addition, a 11.5% to 24.0% improvement to a brute-force machine learning prediction approach is obtained. As such, this work contributes towards the creation of more generic and structured solutions for content-agnostic viewport prediction in terms of data representation, preprocessing and modelling.  © 2022 Association for Computing Machinery.",360-degree video; Content-agnostic; Machine learning; Viewport prediction; Virtual Reality (VR),Behavioral research; E-learning; Machine learning; Trajectories; Virtual reality; 360-degree video; Accurate estimation; Applications domains; Brute force; Content-agnostic; Fast estimation; Machine-learning; Trajectory-based; Viewport prediction; Virtual reality; Forecasting
Structure-aware Meta-fusion for Image Super-resolution,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127377395&doi=10.1145%2f3477553&partnerID=40&md5=089dfa7d1b0f68a8c0e046060702b325,"There are two main categories of image super-resolution algorithms: distortion oriented and perception oriented. Recent evidence shows that reconstruction accuracy and perceptual quality are typically in disagreement with each other. In this article, we present a new image super-resolution framework that is capable of striking a balance between distortion and perception. The core of our framework is a deep fusion network capable of generating a final high-resolution image by fusing a pair of deterministic and stochastic images using spatially varying weights. To make a single fusion model produce images with varying degrees of stochasticity, we further incorporate meta-learning into our fusion network. Once equipped with the kernel produced by a kernel prediction module, our meta fusion network is able to produce final images at any desired level of stochasticity. Experimental results indicate that our meta fusion network outperforms existing state-of-the-art SISR algorithms on widely used datasets, including PIRM-val, DIV2K-val, Set5, Set14, Urban100, Manga109, and B100. In addition, it is capable of producing high-resolution images that achieve low distortion and high perceptual quality simultaneously.  © 2022 Association for Computing Machinery.",Image fusion; Meta-learning; Super-resolution,Image fusion; Stochastic systems; Deterministics; High-resolution images; Image super resolutions; Metalearning; Perceptual quality; Reconstruction accuracy; Stochasticity; Structure-aware; Super resolution algorithms; Superresolution; Optical resolving power
Learning Transferable Perturbations for Image Captioning,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127436621&doi=10.1145%2f3478024&partnerID=40&md5=74601db590a8e474f3d2ce660106e15f,"Present studies have discovered that state-of-the-art deep learning models can be attacked by small but well-designed perturbations. Existing attack algorithms for the image captioning task is time-consuming, and their generated adversarial examples cannot transfer well to other models. To generate adversarial examples faster and stronger, we propose to learn the perturbations by a generative model that is governed by three novel loss functions. Image feature distortion loss is designed to maximize the encoded image feature distance between original images and the corresponding adversarial examples at the image domain, and local-global mismatching loss is introduced to separate the mapping encoding representation of the adversarial images and the ground true captions from a local and global perspective in the common semantic space as far as possible cross image and caption domain. Language diversity loss is to make the image captions generated by the adversarial examples as different as possible from the correct image caption at the language domain. Extensive experiments show that our proposed generative model can efficiently generate adversarial examples that successfully generalize to attack image captioning models trained on unseen large-scale datasets or with different architectures, or even the image captioning commercial service.  © 2022 Association for Computing Machinery.",Adversarial examples; Image captioning; Robustness of neural network,Large dataset; Semantics; Adversarial example; Generative model; Image caption; Image captioning; Image features; Learn+; Learning models; Neural-networks; Robustness of neural network; State of the art; Deep learning
Learning from Temporal Spatial Cubism for Cross-Dataset Skeleton-based Action Recognition,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127424095&doi=10.1145%2f3472722&partnerID=40&md5=b8cab519535cf151183514561dd51eed,"Rapid progress and superior performance have been achieved for skeleton-based action recognition recently. In this article, we investigate this problem under a cross-dataset setting, which is a new, pragmatic, and challenging task in real-world scenarios. Following the unsupervised domain adaptation (UDA) paradigm, the action labels are only available on a source dataset, but unavailable on a target dataset in the training stage. Different from the conventional adversarial learning-based approaches for UDA, we utilize a self-supervision scheme to reduce the domain shift between two skeleton-based action datasets. Our inspiration is drawn from Cubism, an art genre from the early 20th century, which breaks and reassembles the objects to convey a greater context. By segmenting and permuting temporal segments or human body parts, we design two self-supervised learning classification tasks to explore the temporal and spatial dependency of a skeleton-based action and improve the generalization ability of the model. We conduct experiments on six datasets for skeleton-based action recognition, including three large-scale datasets (NTU RGB+D, PKU-MMD, and Kinetics) where new cross-dataset settings and benchmarks are established. Extensive results demonstrate that our method outperforms state-of-the-art approaches. The source codes of our model and all the compared methods are available at https://github.com/shanice-l/st-cubism.  © 2022 Association for Computing Machinery.",Cross-dataset; Cubism; Self-supervised learning; Skeleton-based action recognition; Unsupervised domain adaptation,Musculoskeletal system; Supervised learning; Action recognition; Adversarial learning; Cross-dataset; Cubism; Domain adaptation; Performance; Real-world scenario; Self-supervised learning; Skeleton-based action recognition; Unsupervised domain adaptation; Large dataset
Music2Dance: DanceNet for Music-Driven Dance Generation,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127381638&doi=10.1145%2f3485664&partnerID=40&md5=db5c25ee3947322caf94495778b1fa70,"Synthesize human motions from music (i.e., music to dance) is appealing and has attracted lots of research interests in recent years. It is challenging because of the requirement for realistic and complex human motions for dance, but more importantly, the synthesized motions should be consistent with the style, rhythm, and melody of the music. In this article, we propose a novel autoregressive generative model, DanceNet, to take the style, rhythm, and melody of music as the control signals to generate 3D dance motions with high realism and diversity. Due to the high long-term spatio-temporal complexity of dance, we propose the dilated convolution to improve the receptive field, and adopt the gated activation unit as well as separable convolution to enhance the fusion of motion features and control signals. To boost the performance of our proposed model, we capture several synchronized music-dance pairs by professional dancers and build a high-quality music-dance pair dataset. Experiments have demonstrated that the proposed method can achieve state-of-the-art results.  © 2022 Association for Computing Machinery.",3D human dance; Generative model; Temporal convolution,3d human dance; Auto-regressive; Control signal; Generative model; Human motions; Receptive fields; Research interests; Spatio-temporal complexity; Synthesised; Temporal convolution; Convolution
Uni-EDEN: Universal Encoder-Decoder Network by Multi-Granular Vision-Language Pre-training,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127410482&doi=10.1145%2f3473140&partnerID=40&md5=4773d8116685f92e7464bfcb9b3ab145,"Vision-language pre-training has been an emerging and fast-developing research topic, which transfers multi-modal knowledge from rich-resource pre-training task to limited-resource downstream tasks. Unlike existing works that predominantly learn a single generic encoder, we present a pre-trainable Universal Encoder-DEcoder Network (Uni-EDEN) to facilitate both vision-language perception (e.g., visual question answering) and generation (e.g., image captioning). Uni-EDEN is a two-stream Transformer-based structure, consisting of three modules: object and sentence encoders that separately learns the representations of each modality and sentence decoder that enables both multi-modal reasoning and sentence generation via inter-modal interaction. Considering that the linguistic representations of each image can span different granularities in this hierarchy including, from simple to comprehensive, individual label, a phrase, and a natural sentence, we pre-train Uni-EDEN through multi-granular vision-language proxy tasks: Masked Object Classification, Masked Region Phrase Generation, Image-Sentence Matching, and Masked Sentence Generation. In this way, Uni-EDEN is endowed with the power of both multi-modal representation extraction and language modeling. Extensive experiments demonstrate the compelling generalizability of Uni-EDEN by fine-tuning it to four vision-language perception and generation downstream tasks.  © 2022 Association for Computing Machinery.",Encoder-decoder networks; Vision-language pre-training,Computational linguistics; Computer hardware description languages; Decoding; Modeling languages; Natural language processing systems; Network coding; Visual languages; Down-stream; Encoder-decoder; Encoder-decoder network; Language perception; Learn+; Multi-modal; Pre-training; Question Answering; Research topics; Vision-language pre-training; Classification (of information)
An Effective Forest Fire Detection Framework Using Heterogeneous Wireless Multimedia Sensor Networks,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127419734&doi=10.1145%2f3473037&partnerID=40&md5=2dbfb78ab88d655a872f1f04bccd5599,"With improvements in the area of Internet of Things (IoT), surveillance systems have recently become more accessible. At the same time, optimizing the energy requirements of smart sensors, especially for data transmission, has always been very important and the energy efficiency of IoT systems has been the subject of numerous studies. For environmental monitoring scenarios, it is possible to extract more accurate information using smart multimedia sensors. However, multimedia data transmission is an expensive operation. In this study, a novel hierarchical approach is presented for the detection of forest fires. The proposed framework introduces a new approach in which multimedia and scalar sensors are used hierarchically to minimize the transmission of visual data. A lightweight deep learning model is also developed for devices at the edge of the network to improve detection accuracy and reduce the traffic between the edge devices and the sink. The framework is evaluated using a real testbed, network simulations, and 10-fold cross-validation in terms of energy efficiency and detection accuracy. Based on the results of our experiments, the validation accuracy of the proposed system is 98.28%, and the energy saving is 29.94%. The proposed deep learning model's validation accuracy is very close to the accuracy of the best performing architectures when the existing studies and lightweight architectures are considered. In terms of suitability for edge computing, the proposed approach is superior to the existing ones with reduced computational requirements and model size.  © 2022 Association for Computing Machinery.",Deep learning; Edge computing; Energy efficiency; Heterogeneous WMSN architecture; IoT; WMSNs,Data transfer; Deep learning; Deforestation; Edge computing; Fires; Internet of things; Network architecture; Network security; Wireless sensor networks; Deep learning; Detection accuracy; Detection framework; Edge computing; Forest fire detection; Heterogeneous WMSN architecture; Learning models; Multimedia sensor networks; Wireless multimedia; WMSN; Energy efficiency
Generating Virtual Wire Sculptural Art from 3D Models,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127403142&doi=10.1145%2f3475798&partnerID=40&md5=4570fbe163c543583891e9a1194905a8,"Wire sculptures are objects sculpted by the use of wires. In this article, we propose practical methods to create 3D virtual wire sculptural art from a given 3D model. In contrast, most of the previous 3D wire art results are reconstructed from input 2D wire art images. Artists usually tend to design their wire art with a single wire if possible. If not possible, they try to create it with the least number of wires. To follow this general design trend, our proposed method generates 3D virtual wire art with the minimum number of continuous wire lines. To achieve this goal, we first adopt a greedy approach to extract important edges of a given 3D model. These extracted important edges become the basis for the subsequent lines to roughly represent the shape of the input model. Then, we connect them with the minimum number of continuous wire lines by the order obtained by optimally solving a traveling salesman problem with some constraints. Finally, we smooth the obtained 3D wires to simulate the real 3D wire results by artists. In addition, we also provide a user interface to control the winding of wires by their design preference. Finally, we experimentally show our 3D virtual wire results and evaluate these created results. As a result, the proposed method is computed effectively and interactively, and results are appealing and comparable to real 3D wire art work.  © 2022 Association for Computing Machinery.",3D wire art; Edge segments; Edges; Smoothing; Wire composition; Wire sculpture,3D modeling; Computer aided design; Traveling salesman problem; User interfaces; 3D models; 3d wire art; 3d-modeling; Edge; Edge segments; Practical method; Smoothing; Wire composition; Wire lines; Wire sculpture; Wire
Non-Acted Text and Keystrokes Database and Learning Methods to Recognize Emotions,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127381404&doi=10.1145%2f3480968&partnerID=40&md5=ee02c1373bba5a60ea26a60d0c7d4c93,"The modern computing applications are presently adapting to the convenient availability of huge and diverse data for making their pattern recognition methods smarter. Identification of dominant emotion solely based on the text data generated by humans is essential for the modern human-computer interaction. This work presents a multimodal text-keystrokes dataset and associated learning methods for the identification of human emotions hidden in small text. For this, a text-keystrokes data of 69 participants is collected in multiple scenarios. Stimuli are induced through videos in a controlled environment. After the stimuli induction, participants write their reviews about the given scenario in an unguided manner. Afterward, keystroke and in-text features are extracted from the dataset. These are used with an assortment of learning methods to identify emotion hidden in the short text. An accuracy of 86.95% is achieved by fusing text and keystroke features. Whereas, 100% accuracy is obtained for pleasure-displeasure classes of emotions using the fusion of keystroke/text features, tree-based feature selection method, and support vector machine classifier. The present work is also compared with four state-of-the-art techniques for the same task, where the results suggest that the present proposal performs better in terms of accuracy.  © 2022 Association for Computing Machinery.",Affective computing; Affective states; Data-driven decision-making; Machine learning; Pattern recognition,Character recognition; Human computer interaction; Support vector machines; Affective Computing; Affective state; Computing applications; Data driven decision; Data-driven decision-making; Decisions makings; Learning methods; Pattern recognition method; Text data; Text feature; Decision making
Moment is Important: Language-Based Video Moment Retrieval via Adversarial Learning,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127405570&doi=10.1145%2f3478025&partnerID=40&md5=8c6b6684a8a8fc66d63ca37d777b861d,"The newly emerging language-based video moment retrieval task aims at retrieving a target video moment from an untrimmed video given a natural language as the query. It is more applicable in reality since it is able to accurately localize a specific video moment, as compared to traditional whole video retrieval. In this work, we propose a novel solution to thoroughly investigate the language-based video moment retrieval issue under the adversarial learning. The key of our solution is to formulate the language-based video moment retrieval task as an adversarial learning problem with two tightly connected components. Specifically, a reinforcement learning is employed as a generator to produce a set of possible video moments. Meanwhile, a multi-task learning is utilized as a discriminator, which integrates inter-modal and intra-modal in a unified framework by employing a sequential update strategy. Finally, the generator and the discriminator are mutually reinforced in the adversarial learning, which is able to jointly optimize the performance of both video moment ranking and video moment localization. Extensive experimental results on two challenging benchmarks, i.e., Charades-STA and TACoS datasets, have well demonstrated the effectiveness and rationality of our proposed solution. Meanwhile, on the larger and unbiased datasets, i.e., ActivityNet Captions and ActivityNet-CD, our proposed framework exhibits excellent robustness.  © 2022 Association for Computing Machinery.",Adversarial learning; Continual multi-task learning; Cross-modal retrieval; Reinforcement learning; Video moment retrieval,Adversarial learning; Connected component; Continual multi-task learning; Cross-modal; Cross-modal retrieval; Learning problem; Natural languages; Novel solutions; Video moment retrieval; Video retrieval; Reinforcement learning
Adversarial Multi-Grained Embedding Network for Cross-Modal Text-Video Retrieval,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127406781&doi=10.1145%2f3483381&partnerID=40&md5=3465538604b2198b8b9c8e2f710e993b,"Cross-modal retrieval between texts and videos has received consistent research interest in the multimedia community. Existing studies follow a trend of learning a joint embedding space to measure the distance between text and video representations. In common practice, video representation is constructed by feeding clips into 3D convolutional neural networks for a coarse-grained global visual feature extraction. In addition, several studies have attempted to align the local objects of video with the text. However, these representations share a drawback of neglecting rich fine-grained relation features capturing spatial-temporal object interactions that benefits mapping textual entities in the real-world retrieval system. To tackle this problem, we propose an adversarial multi-grained embedding network (AME-Net), a novel cross-modal retrieval framework that adopts both fine-grained local relation and coarse-grained global features in bridging text-video modalities. Additionally, with the newly proposed visual representation, we also integrate an adversarial learning strategy into AME-Net, to further narrow the domain gap between text and video representations. In summary, we contribute AME-Net with an adversarial learning strategy for learning a better joint embedding space, and experimental results on MSR-VTT and YouCook2 datasets demonstrate that our proposed framework consistently outperforms the state-of-the-art method.  © 2022 Association for Computing Machinery.",Multi-grained fusion; Spatial-temporal object relationships; Text-video retrieval,Convolutional neural networks; Network embeddings; Cross-modal; Embedding network; Embeddings; Multi-grained fusion; Spatial temporals; Spatial-temporal object relationship; Temporal objects; Text-video retrieval; Video representations; Video retrieval; Video recording
"Transform, Warp, and Dress: A New Transformation-guided Model for Virtual Try-on",2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127400041&doi=10.1145%2f3491226&partnerID=40&md5=3a11a5e82e4826902e930b3ce4f4997c,"Virtual try-on has recently emerged in computer vision and multimedia communities with the development of architectures that can generate realistic images of a target person wearing a custom garment. This research interest is motivated by the large role played by e-commerce and online shopping in our society. Indeed, the virtual try-on task can offer many opportunities to improve the efficiency of preparing fashion catalogs and to enhance the online user experience. The problem is far to be solved: current architectures do not reach sufficient accuracy with respect to manually generated images and can only be trained on image pairs with a limited variety. Existing virtual try-on datasets have two main limits: they contain only female models, and all the images are available only in low resolution. This not only affects the generalization capabilities of the trained architectures but makes the deployment to real applications impractical. To overcome these issues, we present Dress Code, a new dataset for virtual try-on that contains high-resolution images of a large variety of upper-body clothes and both male and female models. Leveraging this enriched dataset, we propose a new model for virtual try-on capable of generating high-quality and photo-realistic images using a three-stage pipeline. The first two stages perform two different geometric transformations to warp the desired garment and make it fit into the target person's body pose and shape. Then, we generate the new image of that same person wearing the try-on garment using a generative network. We test the proposed solution on the most widely used dataset for this task as well as on our newly collected dataset and demonstrate its effectiveness when compared to current state-of-the-art methods. Through extensive analyses on our Dress Code dataset, we show the adaptability of our model, which can generate try-on images even with a higher resolution.  © 2022 Association for Computing Machinery.",Generative adversarial networks; Geometric transformations; Virtual try-on,Electronic commerce; Generative adversarial networks; Network architecture; Statistical tests; Wear of materials; 'current; E- commerces; Geometric transformations; Multimedia community; Online shopping; Online users; Realistic images; Research interests; Virtual try-on; Vision communities; Large dataset
Will You Ever Become Popular? Learning to Predict Virality of Dance Clips,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127428075&doi=10.1145%2f3477533&partnerID=40&md5=a46930bac941398ab0167d2371ff5b70,"Dance challenges are going viral in video communities like TikTok nowadays. Once a challenge becomes popular, thousands of short-form videos will be uploaded within a couple of days. Therefore, virality prediction from dance challenges is of great commercial value and has a wide range of applications, such as smart recommendation and popularity promotion. In this article, a novel multi-modal framework that integrates skeletal, holistic appearance, facial and scenic cues is proposed for comprehensive dance virality prediction. To model body movements, we propose a pyramidal skeleton graph convolutional network (PSGCN) that hierarchically refines spatio-temporal skeleton graphs. Meanwhile, we introduce a relational temporal convolutional network (RTCN) to exploit appearance dynamics with non-local temporal relations. An attentive fusion approach is finally proposed to adaptively aggregate predictions from different modalities. To validate our method, we introduce a large-scale viral dance video (VDV) dataset, which contains over 4,000 dance clips of eight viral dance challenges. Extensive experiments on the VDV dataset well demonstrate the effectiveness of our approach. Furthermore, we show that short video applications such as multi-dimensional recommendation and action feedback can be derived from our model.  © 2022 Association for Computing Machinery.",Dance challenge; Multi-modal approach; Virality prediction,Convolution; Large dataset; Musculoskeletal system; Body movements; Convolutional networks; Dance challenge; Multi-modal; Multi-modal approach; Nonlocal; Spatio-temporal; Temporal relation; Video dataset; Virality prediction; Forecasting
Deep Semantic and Attentive Network for Unsupervised Video Summarization,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127404784&doi=10.1145%2f3477538&partnerID=40&md5=a0be00b1b260cd1fd3f3f9bbc376af09,"With the rapid growth of video data, video summarization is a promising approach to shorten a lengthy video into a compact version. Although supervised summarization approaches have achieved state-of-the-art performance, they require frame-level annotated labels. Such an annotation process is time-consuming and tedious. In this article, we propose a novel deep summarization framework named Deep Semantic and Attentive Network for Video Summarization (DSAVS) that can select the most semantically representative summary by minimizing the distance between video representation and text representation without any frame-level labels. Another challenge associated with video summarization tasks mainly originates from the difficulty of considering temporal information over a long time. Long Short-Term Memory (LSTM) performs well for temporal dependencies modeling but does not work well with long video clips. Therefore, we introduce a self-attention mechanism into our summarization framework to capture the long-range temporal dependencies among the frames. Extensive experiments on two popular benchmark datasets, i.e., SumMe and TVSum, show that our proposed framework outperforms other state-of-the-art unsupervised approaches and even most supervised methods.  © 2022 Association for Computing Machinery.",Self-attention; Video summarization; Visual-semantic embedding,Long short-term memory; Semantic Web; Semantics; Data Video; Rapid growth; Self-attention; Semantic embedding; State-of-the-art performance; Video data; Video representations; Video summarization; Visual semantics; Visual-semantic embedding; Video recording
An l1/2 and Graph Regularized Subspace Clustering Method for Robust Image Segmentation,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127449570&doi=10.1145%2f3476514&partnerID=40&md5=ef256eac8d148c0405ac0ae1c182aad3,"Segmenting meaningful visual structures from an image is a fundamental and most-addressed problem in image analysis algorithms. However, among factors such as diverse visual patterns, noise, complex backgrounds, and similar textures present in foreground and background, image segmentation still stands as a challenging research problem. In this article, the proposed method employs an unsupervised method that addresses image segmentation as subspace clustering of image feature vectors. Initially, an image is partitioned into a set of homogeneous regions called superpixels, from which Local Spectral Histogram features are computed. Subsequently, a feature data matrix is created whereupon subspace clustering methodology is applied. A single-stage optimization model is formulated with enhanced segmentation capabilities by the combined action of l½ and l2 norm minimization. Robustness of l½ regularization toward both the noise and overestimation of sparsity provides simultaneous noise robustness and better subspace selection, respectively. While l2 norm facilitates grouping effect. Hence, the designed optimization model ensures an improved sparse solution and a sparse representation matrix with an accurate block diagonal structure, which thereby favours getting properly segmented images. Then, experimental results of the proposed method are compared with the state-of-art algorithms. Results demonstrate the improved performance of our method over the state-of-art algorithms.  © 2022 Association for Computing Machinery.",Constrained optimization; Image segmentation; Sparsity; Subspace clustering,Constrained optimization; Image enhancement; Image segmentation; Textures; Vectors; Clustering methods; Image analysis algorithms; Images segmentations; L2-norm; Optimization models; Robust image segmentation; Sparsity; Subspace clustering; Visual pattern; Visual structure; Clustering algorithms
Response Generation by Jointly Modeling Personalized Linguistic Styles and Emotions,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127421794&doi=10.1145%2f3475872&partnerID=40&md5=da5dfbb1286fc102a25bb161b8fe000e,"Natural language generation (NLG) has been an essential technique for various applications, like XiaoIce and Siri, and engaged increasing attention recently. To improve the user experience, several emotion-aware NLG methods have been developed to generate responses coherent with a pre-designated emotion (e.g., the positive or negative). Nevertheless, existing methods cannot generate personalized responses as they frequently overlook the personalized linguistic style. Apparently, different human responsers tend to have different linguistic styles. Inspired by this, in this work, we focus on a novel research theme of personalized emotion-aware NLG (PENLG), whereby the generated responses should be coherent with the linguistic style of a pre-designated responser and emotion. In particular, we study PENLG under a scenario of generating personalized emotion-aware response for social media post. Yet it faces certain research challenges: (1) the user linguistic styles are implicit and complex by nature, and hence it is hard to learn their representations; and (2) linguistic styles and emotions are usually expressed in different manners in a response, and thus how to convey them properly in the generated responses is not easy. Toward this end, we present a novel scheme of PENLG, named CRobot, which consists of a personalized emotion-aware response generator and two discriminators, i.e., general discriminator and personalized emotion-aware discriminator. To be more specific, the post-based and avatar-based user linguistic style modeling methods are incorporated into the encoder-decoder-based generator, while the discriminators are devised to ensure that the generated response is fluent and consistent with both the emotion and the linguistic style of the user. Different from the traditional adversarial networks, we embed adversarial learning under the umbrella of reinforcement learning. In this way, the response generation problem can be tackled by the generator taking a sequence of actions on selecting the proper word of each timestep for output. To justify our model, we construct a large-scale response generation dataset based on Twitter, consisting of 6,763 tweets with a corresponding 1,461,713 response created by 153,664 users. Extensive experiments demonstrate that CRobot surpasses the state-of-the-art baselines regarding both subjective and objective evaluation.  © 2022 Association for Computing Machinery.",Personalized and emotionalized response; Response generation; Social media dataset construction,Large dataset; Linguistics; Modeling languages; Natural language processing systems; Reinforcement learning; User profile; Generation method; Learn+; Linguistic styles; Natural language generation; Personalized and emotionalized response; Research challenges; Response generation; Social media; Social medium dataset construction; Users' experiences; Social networking (online)
"Tell, Imagine, and Search: End-to-end Learning for Composing Text and Image to Image Retrieval",2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127403064&doi=10.1145%2f3478642&partnerID=40&md5=66c50f5107e13fcd8ca98bb90421cb6c,"Composing Text and Image to Image Retrieval (CTI-IR) is an emerging task in computer vision, which allows retrieving images relevant to a query image with text describing desired modifications to the query image. Most conventional cross-modal retrieval approaches usually take one modality data as the query to retrieve relevant data of another modality. Different from the existing methods, in this article, we propose an end-to-end trainable network for simultaneous image generation and CTI-IR. The proposed model is based on Generative Adversarial Network (GAN) and enjoys several merits. First, it can learn a generative and discriminative feature for the query (a query image with text description) by jointly training a generative model and a retrieval model. Second, our model can automatically manipulate the visual features of the reference image in terms of the text description by the adversarial learning between the synthesized image and target image. Third, global-local collaborative discriminators and attention-based generators are exploited, allowing our approach to focus on both the global and local differences between the query image and the target image. As a result, the semantic consistency and fine-grained details of the generated images can be better enhanced in our model. The generated image can also be used to interpret and empower our retrieval model. Quantitative and qualitative evaluations on three benchmark datasets demonstrate that the proposed algorithm performs favorably against state-of-the-art methods.  © 2022 Association for Computing Machinery.",Composing text and image to image retrieval; End-to-end; Generative adversarial network; Global-local; Image generation,Image enhancement; Image retrieval; Semantics; Composing text and image to image retrieval; Cross-modal; Discriminative features; End to end; Global-local; Image generations; Learn+; Query images; Retrieval models; Target images; Generative adversarial networks
Understanding and Creating Art with AI: Review and Outlook,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127431376&doi=10.1145%2f3475799&partnerID=40&md5=0935f8744e4ccaf7203b39b362efb6e1,"Technologies related to artificial intelligence (AI) have a strong impact on the changes of research and creative practices in visual arts. The growing number of research initiatives and creative applications that emerge in the intersection of AI and art motivates us to examine and discuss the creative and explorative potentials of AI technologies in the context of art. This article provides an integrated review of two facets of AI and art: (1) AI is used for art analysis and employed on digitized artwork collections, or (2) AI is used for creative purposes and generating novel artworks. In the context of AI-related research for art understanding, we present a comprehensive overview of artwork datasets and recent works that address a variety of tasks such as classification, object detection, similarity retrieval, multimodal representations, and computational aesthetics, among others. In relation to the role of AI in creating art, we address various practical and theoretical aspects of AI Art and consolidate related works that deal with those topics in detail. Finally, we provide a concise outlook on the future progression and potential impact of AI technologies on our understanding and creation of art.  © 2022 Association for Computing Machinery.",AI Art; Computational creativity; Convolutional neural networks; Deep learning; Generative adversarial networks; Generative art; Image understanding; Visual arts,Arts computing; Classification (of information); Convolution; Convolutional neural networks; Deep neural networks; Object detection; Art analysis; Artificial intelligence art; Artificial intelligence technologies; Computational creativities; Convolutional neural network; Creatives; Deep learning; Generative art; Research initiatives; Visual arts; Generative adversarial networks
Fully Unsupervised Person Re-Identification via Selective Contrastive Learning,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127431379&doi=10.1145%2f3485061&partnerID=40&md5=f18f000f1f08e500f124ea8b872810da,"Person re-identification (ReID) aims at searching the same identity person among images captured by various cameras. Existing fully supervised person ReID methods usually suffer from poor generalization capability caused by domain gaps. Unsupervised person ReID has attracted a lot of attention recently, because it works without intensive manual annotation and thus shows great potential in adapting to new conditions. Representation learning plays a critical role in unsupervised person ReID. In this work, we propose a novel selective contrastive learning framework for fully unsupervised feature learning. Specifically, different from traditional contrastive learning strategies, we propose to use multiple positives and adaptively selected negatives for defining the contrastive loss, enabling to learn a feature embedding model with stronger identity discriminative representation. Moreover, we propose to jointly leverage global and local features to construct three dynamic memory banks, among which the global and local ones are used for pairwise similarity computation and the mixture memory bank are used for contrastive loss definition. Experimental results demonstrate the superiority of our method in unsupervised person ReID compared with the state of the art. Our code is available at https://github.com/pangbo1997/Unsup_ReID.git.  © 2022 Association for Computing Machinery.",Contrastive learning; Person re-identification; Unsupervised learning,Condition; Contrastive learning; Generalization capability; Learn+; Learning frameworks; Learning strategy; Manual annotation; Memory banks; Person re identifications; Unsupervised feature learning; Unsupervised learning
SADnet: Semi-supervised Single Image Dehazing Method Based on an Attention Mechanism,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127373448&doi=10.1145%2f3478457&partnerID=40&md5=29fc3b62d1c6d8dba06300101c79971d,"Many real-life tasks such as military reconnaissance and traffic monitoring require high-quality images. However, images acquired in foggy or hazy weather pose obstacles to the implementation of these real-life tasks; consequently, image dehazing is an important research problem. To meet the requirements of practical applications, a single image dehazing algorithm has to be able to effectively process real-world hazy images with high computational efficiency. In this article, we present a fast and robust semi-supervised dehazing algorithm named SADnet for practical applications. SADnet utilizes both synthetic datasets and natural hazy images for training, so it has good generalizability for real-world hazy images. Furthermore, considering the uneven distribution of haze in the atmospheric environment, a Channel-Spatial Self-Attention (CSSA) mechanism is presented to enhance the representational power of the proposed SADnet. Extensive experimental results demonstrate that the presented approach achieves good dehazing performances and competitive running times compared with other state-of-the-art image dehazing algorithms.  © 2022 Association for Computing Machinery.",Attention; Deep learning; Practical applications; Semi-supervised; Single image dehazing,Computational efficiency; Deep learning; Military photography; Attention; Attention mechanisms; Deep learning; Dehazing; Military reconnaissance; Military traffic; Practical application; Real-world; Semi-supervised; Single image dehazing; Demulsification
Evaluation of an Intervention Program Based on Mobile Apps to Learn Sexism Prevention in Teenagers,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127447434&doi=10.1145%2f3471139&partnerID=40&md5=faa54adc8aaac3c883e40879cebb3a9d,"The fight against sexism is nowadays one of the flagship social movements in western countries. Adolescence is a crucial period, and some empirical studies have focused on the socialization of teenagers, proving that the socialization with the surrounding environment prevent sexist practices. In a previous work, we developed and tested the effectiveness of a mobile app, called Liad@s, with the goals of helping teenagers to prevent sexism and build healthy couple relationships. In this article, we carry out a study where (using a real situation) we compare the effectiveness of the Liad@s app in front of traditional interventions like a workshop about sexism for teenagers. Also, we evaluate the usability of the app and the user satisfaction with this application. In this study, our primary hypothesis is that the effectiveness of using our mobile application, in terms of knowledge acquired about sexism, would be at least as good as attending the workshop. Our secondary hypothesis is that the user satisfaction with the mobile application would be higher than the one with the workshop, causing a preference for the app. The results of this study show significant differences in learning appeared between gender and between the two different procedures when separately evaluating the data collected from both hostile sexism (HS) and benevolent sexism (BS) questionnaires. These results validate our primary hypothesis. Also, most of the population under study preferred the mobile app in front of the traditional workshop, validating also our secondary hypothesis.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Ambivalent sexism; Mobile apps; Real user evaluation; Usability outcomes,Surveys; Ambivalent sexism; Intervention programs; Learn+; Mobile app; Mobile applications; Real user evaluation; Social movements; Usability outcome; User evaluations; Users' satisfactions; Mobile computing
Cascaded Structure-Learning Network with Using Adversarial Training for Robust Facial Landmark Detection,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127405377&doi=10.1145%2f3474595&partnerID=40&md5=486fcfc790ebd874df2d2eddb1a48d54,"Recently, great progress has been achieved on facial landmark detection based on convolutional neural network, while it is still challenging due to partial occlusion and extreme head pose. In this paper, we propose a Cascaded Structure-Learning Network (CSLN) with using adversarial training to improve the performance of 2D facial landmark detection by taking the structure of facial landmarks into account. In the first stage, we improve the original stacked hourglass network, which applies a multi-branch module to capture different scales of features, a progressive convolution structure to compensate for the missing structural features in hourglass networks, and a pyramid inception structure to expand the receptive field. Specially, by introducing a discriminator, we use the adversarial training strategy to urge the improved hourglass network for generating more accurate heatmaps. The second stage, which is based on attention mechanism, optimizes the spatial correlations between different facial landmarks by reusing the structural features. Moreover, we propose a novel region loss, which can adaptively allocate proper weights to different regions. In this way, the network can focus more on those occluded landmarks. The experimental results on several datasets, i.e. 300W, COFW, and AFLW, show that our proposed method achieves superior performance compared with the state-of-the-art methods.  © 2022 Association for Computing Machinery.",2D Facial landmark detection; Adversarial training; Convolutional neural networks; Keypoints structure learning,Convolution; Convolutional neural networks; Face recognition; 2d facial landmark detection; Adversarial training; Cascaded structure; Convolutional neural network; Facial landmark detection; Keypoint structure learning; Keypoints; Learning network; Performance; Structure-learning; Learning systems
Efficient Light Field Image Compression with Enhanced Random Access,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127455785&doi=10.1145%2f3471905&partnerID=40&md5=cf920111ed7b7e4b662235b2bbedd7da,"In light field image compression, facilitating random access to individual views plays a significant role in decoding views quickly, reducing memory footprint, and decreasing the bandwidth requirement for transmission. Highly efficient light field image compression methods mainly use inter view prediction. Therefore, they typically do not provide random access to individual views. On the other hand, methods that provide full random access usually reduce compression efficiency. To address this trade-off, a light field image encoding method that favors random access is proposed in this paper. Light field image views are grouped into independent (3× 3) views, which are called Macro View Images (MVIs). To encode MVIs, the central view is used as a reference to compress its adjacent neighboring views using a hierarchical reference structure. To encode the central view of each MVI, the most central view along with the center of a maximum of three MVIs, are used as reference images for the disparity estimation. In addition, the proposed method allows the use of parallel processing to reduce the maximum encoding/decoding time-complexity in multi-core processors. Tile partitioning can also be used to randomly access different regions of the light field images. The simulation results show that the proposed method outperforms other state-of-the-art methods in terms of compression efficiency while providing random access to both views and regions of interest.  © 2022 Association for Computing Machinery.",HEVC; Image compression; Light field; Random access; VVC,Economic and social effects; Efficiency; Encoding (symbols); Image enhancement; Signal encoding; Bandwidth requirement; Compression efficiency; Field images; HEVC; Image compression methods; Images compression; Light fields; Memory footprint; Random access; VVC; Image compression
Fine-grained Human Analysis under Occlusions and Perspective Constraints in Multimedia Surveillance,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127418866&doi=10.1145%2f3476839&partnerID=40&md5=c75998c306be9b829b0fd9c1215415ea,"Human detection in the wild is a research topic of paramount importance in computer vision, and it is the starting step for designing intelligent systems oriented to human interaction that work in complete autonomy. To achieve this goal, computer vision and machine learning should aim at superhuman capabilities. In this work, we address the problem of fine-grained human analysis under occlusions and perspective constraints. More specifically, we discuss some issues and some possible solutions to effectively detect people using pose estimation methods and to detect humans under occlusions both in the two-dimensional (2D) image plane and in the 3D space exploiting single monocular cameras. Dealing with occlusion can be done at the joint level or pixel level: We discuss two different solutions, the former based on a supervised neural network architecture for detecting occluded joints and the latter based on a semi-supervised specialized GAN that exploits both appearance and human shape attributes to determine the missing parts of the visible shape. To deal with perspective constraints, we further discuss a neural approach based on a double architecture that learns to create an optimal neural representation, which is useful to reconstruct the 3D position of human keypoints starting with simple RGB images. All these approaches have a critical point in common: the need for large annotated datasets. To have large, fair, consistent, transparent, and ethical datasets, we propose the adoption of synthetic datasets as, for example, JTA and MOTSynth. In this article, we discuss the pros and cons of using synthetic datasets while tackling several human-centered AI issues with respect to European GDPR rules for privacy. We further explore and discuss an application in the field of risk assessment by space occupancy estimation during the COVID-19 pandemic called Inter-Homines.  © 2022 Copyright held by the owner/author(s).",3D localization; Human pose estimation; People detection; Synthetic dataset; Tracking,Gesture recognition; Human computer interaction; Intelligent systems; Large dataset; Network architecture; Neural networks; Risk assessment; Risk perception; 3D localization; Fine grained; Human analysis; Human detection; Human pose estimations; Humaninteraction; People detection; Research topics; Synthetic datasets; Tracking; Computer vision
Deep Learning-based Smart Predictive Evaluation for Interactive Multimedia-enabled Smart Healthcare,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127412359&doi=10.1145%2f3468506&partnerID=40&md5=05990537bf4708fceed95a5b1b90a203,"Two-dimensional1 arrays of bi-component structures made of cobalt and permalloy elliptical dots with thickness of 25 nm, length 1 mm and width of 225 nm, have been prepared by a self-aligned shadow deposition technique. Brillouin light scattering has been exploited to study the frequency dependence of thermally excited magnetic eigenmodes on the intensity of the external magnetic field, applied along the easy axis of the elements.This study aims to enhance the security for people's health, improve the medical level further, and increase the confidentiality of people's privacy information. Under the trend of wide application of deep learning algorithms, the convolutional neural network (CNN) is modified to build an interactive smart healthcare prediction and evaluation model (SHPE model) based on the deep learning model. The model is optimized and standardized for data processing. Then, the constructed model is simulated to analyze its performance. The results show that accuracy of the constructed system reaches 82.4%, which is at least 2.4% higher than other advanced CNN algorithms and 3.3% higher than other classical machine algorithms. It is proved based on comparison that the accuracy, precision, recall, and F1 of the constructed model are the highest. Further analysis on error shows that the constructed model shows the smallest error of 23.34 pixels. Therefore, it is proved that the built SHPE model shows higher prediction accuracy and smaller error while ensuring the safety performance, which provides an experimental reference for the prediction and evaluation of smart healthcare treatment in the later stage.  © 2022 Association for Computing Machinery.",Convolutional neural network; Deep learning; Healthcare prediction and evaluation model; Precision; Smart healthcare,Convolutional neural networks; Data handling; Deep neural networks; Errors; Forecasting; Health care; Iron alloys; Multimedia systems; Component structure; Convolutional neural network; Deep learning; Evaluation models; Healthcare prediction and evaluation model; Interactive multimedia; Permalloys; Precision; Prediction modelling; Smart healthcare; Convolution
Instance Correlation Graph for Unsupervised Domain Adaptation,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127447477&doi=10.1145%2f3486251&partnerID=40&md5=34fdb0b95416d5782b0ce9ef3aa4e362,"In recent years, deep neural networks have emerged as a dominant machine learning tool for a wide variety of application fields. Due to the expensive cost of manual labeling efforts, it is important to transfer knowledge from a label-rich source domain to an unlabeled target domain. The core problem is how to learn a domain-invariant representation to address the domain shift challenge, in which the training and test samples come from different distributions. First, considering the geometry of space probability distributions, we introduce an effective Hellinger Distance to match the source and target distributions on statistical manifold. Second, the data samples are not isolated individuals, and they are interrelated. The correlation information of data samples should not be neglected for domain adaptation. Distinguished from previous works, we pay attention to the correlation distributions over data samples. We design elaborately a Residual Graph Convolutional Network to construct the Instance Correlation Graph (ICG). The correlation information of data samples is exploited to reduce the domain shift. Therefore, a novel Instance Correlation Graph for Unsupervised Domain Adaptation is proposed, which is trained end-to-end by jointly optimizing three types of losses, i.e., Supervised Classification loss for source domain, Centroid Alignment loss to measure the centroid difference between source and target domain, ICG Alignment loss to match Instance Correlation Graph over two related domains. Extensive experiments are conducted on several hard transfer tasks to learn domain-invariant representations on three benchmarks: Office-31, Office-Home, and VisDA2017. Compared with other state-of-the-art techniques, our method achieves superior performance.  © 2022 Association for Computing Machinery.",Instance correlation graph; Residual graph convolutional network; Unsupervised domain adaptation,Deep neural networks; Probability distributions; Supervised learning; Convolutional networks; Correlation graphs; Data sample; Domain adaptation; Instance correlation graph; Invariant representation; Learn+; Residual graph convolutional network; Target domain; Unsupervised domain adaptation; Convolution
Exploring Relations in Untrimmed Videos for Self-Supervised Learning,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127432991&doi=10.1145%2f3473342&partnerID=40&md5=f8d748b0f9501de4baeaf63d985d71bc,"Existing video self-supervised learning methods mainly rely on trimmed videos for model training. They apply their methods and verify the effectiveness on trimmed video datasets including UCF101 and Kinetics-400, among others. However, trimmed datasets are manually annotated from untrimmed videos. In this sense, these methods are not truly unsupervised. In this article, we propose a novel self-supervised method, referred to as Exploring Relations in Untrimmed Videos (ERUV), which can be straightforwardly applied to untrimmed videos (real unlabeled) to learn spatio-temporal features. ERUV first generates single-shot videos by shot change detection. After that, some designed sampling strategies are used to model relations for video clips. The strategies are saved as our self-supervision signals. Finally, the network learns representations by predicting the category of relations between the video clips. ERUV is able to compare the differences and similarities of video clips, which is also an essential procedure for video-related tasks. We validate our learned models with action recognition, video retrieval, and action similarity labeling tasks with four kinds of 3D convolutional neural networks. Experimental results show that ERUV is able to learn richer representations with untrimmed videos, and it outperforms state-of-the-art self-supervised methods with significant margins.  © 2022 Association for Computing Machinery.",Action detection; Action recognition; Self-supervised learning,Convolutional neural networks; Supervised learning; Action detection; Action recognition; Learn+; Model training; Self-supervised learning; Spatiotemporal feature; Supervised learning methods; Supervised methods; Video dataset; Video-clips; Video cameras
JoT-GAN: A Framework for Jointly Training GAN and Person Re-Identification Model,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127428967&doi=10.1145%2f3491225&partnerID=40&md5=b9892d48f34f03c571849912caf3370c,"To cope with the problem caused by inadequate training data, many person re-identification (re-id) methods exploit generative adversarial networks (GAN) for data augmentation, where the training of GAN is typically independent of that of the re-id model. The coupling relation between them that probably brings in a performance gain of re-id is thus ignored. In this work, we propose a general framework, namely JoT-GAN, to jointly train GAN and the re-id model. It can simultaneously achieve the optima of both the generator and the re-id model, where the training is guided by each other through a discriminator. The re-id model is boosted for two reasons: (1) the adversarial training encourages it to fool the discriminator, and (2) the generated samples augment the training data. Extensive results on benchmark datasets show that for the re-id model trained with the identification loss as well as the triplet loss, the proposed joint training framework outperforms existing methods with separate training and achieves state-of-the-art re-id performance.  © 2022 Association for Computing Machinery.",Generative adversarial networks; Joint training; Person re-identification,Benchmarking; Benchmark datasets; Coupling relation; Data augmentation; Identification modeling; Joint training; Performance Gain; Person re identifications; Re identifications; Training data; Training framework; Generative adversarial networks
Rectified Meta-learning from Noisy Labels for Robust Image-based Plant Disease Classification,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127373979&doi=10.1145%2f3472809&partnerID=40&md5=c1e75d451f0b86acb6f562245e367d03,"Plant diseases serve as one of main threats to food security and crop production. It is thus valuable to exploit recent advances of artificial intelligence to assist plant disease diagnosis. One popular approach is to transform this problem as a leaf image classification task, which can be then addressed by the powerful convolutional neural networks (CNNs). However, the performance of CNN-based classification approach depends on a large amount of high-quality manually labeled training data, which inevitably introduce noise on labels in practice, leading to model overfitting and performance degradation. To overcome this problem, we propose a novel framework that incorporates rectified meta-learning module into common CNN paradigm to train a noise-robust deep network without using extra supervision information. The proposed method enjoys the following merits: (i) A rectified meta-learning is designed to pay more attention to unbiased samples, leading to accelerated convergence and improved classification accuracy. (ii) Our method is free on assumption of label noise distribution, which works well on various kinds of noise. (iii) Our method serves as a plug-and-play module, which can be embedded into any deep models optimized by gradient descent-based method. Extensive experiments are conducted to demonstrate the superior performance of our algorithm over the state-of-the-arts.  © 2022 Association for Computing Machinery.",Meta learning; Noisy labels; Plant disease classification,Classification (of information); Cultivation; Food supply; Gradient methods; Image classification; Learning systems; Convolutional neural network; Disease classification; Food crops; Food security; Image-based; Metalearning; Noisy labels; Performance; Plant disease; Plant disease classification; Convolutional neural networks
TT-TSVD: A Multi-modal Tensor Train Decomposition with Its Application in Convolutional Neural Networks for Smart Healthcare,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127432919&doi=10.1145%2f3491223&partnerID=40&md5=1fe2d5b8ba719ae71ca64cbc255909de,"Smart healthcare systems are generating a large scale of heterogenous high-dimensional data with complex relationships. It is hard for current methods to analyze such high-dimensional healthcare data. Specifically, the traditional data reduction methods can not keep the correlation among different modalities of data objects, while the latest methods based on tensor singular value decomposition are not effective for data reduction, although they can keep the correlation. This article presents a tensor train-tensor singular value decomposition (TT-TSVD) algorithm for data reduction. Particularly, the presented algorithm balances the correlation-preservation ability of modalities and data reduction ability by combining the advantages of the train structure of the tensor train decomposition and the association relationship between the tensor singular value decomposition retention mode. Extensive experiments are conducted on the convolutional neural network and the results clearly show that the presented algorithm performs effectively for data reduction with a low-loss classification accuracy; what is more, classification accuracy on medical image dataset has been improved a little.  © 2022 Association for Computing Machinery.",Convolutional neural network compression; Medical auxiliary diagnosis; Tensor decomposition; Tensor train-tensor singular value decomposition,Classification (of information); Convolutional neural networks; Data reduction; Diagnosis; Health care; Image enhancement; Medical imaging; Singular value decomposition; Tensors; Classification accuracy; Convolutional neural network; Convolutional neural network compression; Medical auxiliary diagnose; Multi-modal; Network compression; Tensor decomposition; Tensor singular value decompositions; Tensor train-tensor singular value decomposition; Tensor trains; Convolution
ECCNAS: Efficient Crowd Counting Neural Architecture Search,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127376605&doi=10.1145%2f3465455&partnerID=40&md5=2be9037e6c95c61e9360a2676a941582,"Recent solutions to crowd counting problems have already achieved promising performance across various benchmarks. However, applying these approaches to real-world applications is still challenging, because they are computation intensive and lack the flexibility to meet various resource budgets. In this article, we propose an efficient crowd counting neural architecture search (ECCNAS) framework to search efficient crowd counting network structures, which can fill this research gap. A novel search from pre-trained strategy enables our cross-task NAS to explore the significantly large and flexible search space with less search time and get more proper network structures. Moreover, our well-designed search space can intrinsically provide candidate neural network structures with high performance and efficiency. In order to search network structures according to hardwares with different computational performance, we develop a novel latency cost estimation algorithm in our ECCNAS. Experiments show our searched models get an excellent trade-off between computational complexity and accuracy and have the potential to deploy in practical scenarios with various resource budgets. We reduce the computational cost, in terms of multiply-and-accumulate (MACs), by up to 96% with comparable accuracy. And we further designed experiments to validate the efficiency and the stability improvement of our proposed search from pre-trained strategy.  © 2022 Association for Computing Machinery.",AutoDL; Continuous search space; Crowd counting; Neural architecture search; Object counting,Benchmarking; Budget control; Economic and social effects; Efficiency; Network architecture; Autodl; Continuous search space; Crowd counting; Network structures; Neural architecture search; Neural architectures; Object counting; Performance; Resource budget; Search spaces; Cost estimating
Mask or Non-Mask? Robust Face Mask Detector via Triplet-Consistency Representation Learning,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127452998&doi=10.1145%2f3472623&partnerID=40&md5=9cb38d8c2712d3ad39b17bf689c6680a,"In the absence of vaccines or medicines to stop COVID-19, one of the effective methods to slow the spread of the coronavirus and reduce the overloading of healthcare is to wear a face mask. Nevertheless, to mandate the use of face masks or coverings in public areas, additional human resources are required, which is tedious and attention-intensive. To automate the monitoring process, one of the promising solutions is to leverage existing object detection models to detect the faces with or without masks. As such, security officers do not have to stare at the monitoring devices or crowds, and only have to deal with the alerts triggered by the detection of faces without masks. Existing object detection models usually focus on designing the CNN-based network architectures for extracting discriminative features. However, the size of training datasets of face mask detection is small, while the difference between faces with and without masks is subtle. Therefore, in this article, we propose a face mask detection framework that uses the context attention module to enable the effective attention of the feed-forward convolution neural network by adapting their attention maps' feature refinement. Moreover, we further propose an anchor-free detector with Triplet-Consistency Representation Learning by integrating the consistency loss and the triplet loss to deal with the small-scale training data and the similarity between masks and occlusions. Extensive experimental results show that our method outperforms the other state-of-the-art methods. The source code is released as a public download to improve public health at https://github.com/wei-1006/MaskFaceDetection.  © 2022 Association for Computing Machinery.",Deep learning; Face detection; Face occlusion; Object detection,Deep learning; Face recognition; Feature extraction; Network architecture; Object recognition; Coronaviruses; Deep learning; Face masks; Face occlusion; Faces detection; Human resource is; Monitoring process; OR coverings; Public areas; Slow the spread; Object detection
BiRe-ID: Binary Neural Network for Efficient Person Re-ID,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127375170&doi=10.1145%2f3473340&partnerID=40&md5=6afd25c47ada65990cf34f01cbd0cecb,"Person re-identification (Re-ID) has been promoted by the significant success of convolutional neural networks (CNNs). However, the application of such CNN-based Re-ID methods depends on the tremendous consumption of computation and memory resources, which affects its development on resource-limited devices such as next generation AI chips. As a result, CNN binarization has attracted increasing attention, which leads to binary neural networks (BNNs). In this article, we propose a new BNN-based framework for efficient person Re-ID (BiRe-ID). In this work, we discover that the significant performance drop of binarized models for Re-ID task is caused by the degraded representation capacity of kernels and features. To address the issues, we propose the kernel and feature refinement based on generative adversarial learning (KR-GAL and FR-GAL) to enhance the representation capacity of BNNs. We first introduce an adversarial attention mechanism to refine the binarized kernels based on their real-valued counterparts. Specifically, we introduce a scale factor to restore the scale of 1-bit convolution. And we employ an effective generative adversarial learning method to train the attention-aware scale factor. Furthermore, we introduce a self-supervised generative adversarial network to refine the low-level features using the corresponding high-level semantic information. Extensive experiments demonstrate that our BiRe-ID can be effectively implemented on various mainstream backbones for the Re-ID task. In terms of the performance, our BiRe-ID surpasses existing binarization methods by significant margins, at the level even comparable with the real-valued counterparts. For example, on Market-1501, BiRe-ID achieves 64.0% mAP on ResNet-18 backbone, with an impressive 12.51× speedup in theory and 11.75× storage saving. In particular, the KR-GAL and FR-GAL methods show strong generalization on multiple tasks such as Re-ID, image classification, object detection, and 3D point cloud processing.  © 2022 Association for Computing Machinery.",Network binarization; Network compression; Person re-identification,Computation theory; Convolutional neural networks; Generative adversarial networks; Object detection; Semantics; Adversarial learning; Binarizations; Binary neural networks; Convolutional neural network; Network binarization; Network compression; Performance; Person re identifications; Re identifications; Scale Factor; Convolution
A Convolutional Neural Network Model Using Weighted Loss Function to Detect Diabetic Retinopathy,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127424820&doi=10.1145%2f3470976&partnerID=40&md5=199f1ca9d54c766ad7ae40529fb18d38,"Nowadays, artificial intelligence (AI) provides tremendous prospects for driving future healthcare while empowering patients and service providers. The extensive use of digital healthcare produces a massive amount of multimedia healthcare data continuously (e.g., MRI, X-Ray, ultrasound images, etc.). Hence, it needs special data analytics techniques to provide a smart diagnosis to the patients. Recent advancements in artificial intelligence and machine learning techniques, particularly Deep learning (DL) methods, have demonstrated tremendous medical diagnosis progress and achievements. Diabetic Retinopathy (DR), cataract, macular degeneration, and glaucoma are the most common eye problems due to diabetes. Numerous models have been proposed using deep learning models to diagnose diabetic retinopathy, but no model is perfect for detecting DR diseases. This article presents a deep learning model to analyze diabetic retinopathy images to classify DR patients' severity levels. The model applies a custom-weighted loss function in the model's training and achieves 92.49% accuracy and a 0.945 Cohen Kappa score on test data. The model's weighted average precision was 93%, recall 92%, and f1 score 93%. The model is compared with several state-of-the-art pre-trained models. We observe that the proposed model performs better in accuracy results and Cohen Kappa score.  © 2022 Association for Computing Machinery.",Computer vision; Convolutional neural network; Deep learning; Diabetic retinopathy,Computer vision; Convolution; Convolutional neural networks; Data Analytics; Deep neural networks; Diagnosis; Health care; Magnetic resonance imaging; Ophthalmology; Ultrasonic applications; Cohen's kappas; Convolutional neural network; Data analytics; Deep learning; Diabetic retinopathy; Learning models; Neural network model; Service provider; Ultrasound images; Weighted loss function; Eye protection
A Multi-feature and Time-aware-based Stress Evaluation Mechanism for Mental Status Adjustment,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127440408&doi=10.1145%2f3462763&partnerID=40&md5=6d06d5941d6e18e5a3fb123eccda652d,"With the rapid economic development, the prominent social competition has led to increasing psychological pressure of people felt from each aspect of life. Driven by the Internet of Things and artificial intelligence, intelligent psychological pressure detection systems based on deep learning and wearable devices have acquired some good results in practical application. However, existing studies argue that the psychological stress state is influenced by the current environment. They put much attention on the momentary features but ignore the dynamic change process of mental status in the time dimension. Besides, the lack of research in the general laws of psychological stress makes it difficult to quantitatively evaluate the stress status, resulting in the inability to perceive the stress state of users effectively. Thus, this article proposes an evaluation mechanism of psychological stress for adjusting the mental status of users. Specifically, we design a multi-dimensional feature space and a time-aware feature encoder, which integrate various stress features and capture time characteristics of stress state change. Moreover, a novel mental state model is proposed, which uses the pressure features with time characteristics to evaluate the pressure stress level. This model also quantifies the internal relationship between pressure features. Last, we establish a practicable testbed to demonstrate how to evaluate and adjust mental state of users by the proposed evaluation mechanism of psychological stress.  © 2022 Association for Computing Machinery.",Evaluation mechanism of stress; Mental status adjustment; Multi-dimension feature; Psychological pressure,Evaluation mechanism of stress; Mental state; Mental status adjustment; Multi dimensions; Multi-dimension feature; Multifeatures; Psychological pressure; Psychological stress; Stress state; Time characteristics; Deep learning
Age-Invariant Face Recognition by Multi-Feature Fusionand Decomposition with Self-attention,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126207398&doi=10.1145%2f3472810&partnerID=40&md5=9d664d8bc3bf9ab2ed6328747897d5a5,"Different from general face recognition, age-invariant face recognition (AIFR) aims at matching faces with a big age gap. Previous discriminative methods usually focus on decomposing facial feature into age-related and age-invariant components, which suffer from the loss of facial identity information. In this article, we propose a novel Multi-feature Fusion and Decomposition (MFD) framework for age-invariant face recognition, which learns more discriminative and robust features and reduces the intra-class variants. Specifically, we first sample multiple face images of different ages with the same identity as a face time sequence. Then, the multi-head attention is employed to capture contextual information from facial feature series, extracted by the backbone network. Next, we combine feature decomposition with fusion based on the face time sequence to ensure that the final age-independent features effectively represent the identity information of the face and have stronger robustness against the aging process. Besides, we also mitigate imbalanced age distribution in the training data by a re-weighted age loss. We experimented with the proposed MFD over the popular CACD and CACD-VS datasets, where we show that our approach improves the AIFR performance than previous state-of-the-art methods. We simultaneously show the performance of MFD on LFW dataset.  © 2022 Association for Computing Machinery.",Age-invariant face recognition; Feature decomposition; Feature fusion; Self-attention,Age-invariant face recognition; Facial feature; Feature decomposition; Features fusions; Identity information; Matchings; Multi-feature fusion; Multifeatures; Self-attention; Time sequences; Face recognition
Seeing Crucial Parts: Vehicle Model Verification via a Discriminative Representation Model,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127444448&doi=10.1145%2f3474596&partnerID=40&md5=70000381d192cdf265a0cd8b6be02fb8,"Widely used surveillance cameras have promoted large amounts of street scene data, which contains one important but long-neglected object: the vehicle. Here we focus on the challenging problem of vehicle model verification. Most previous works usually employ global features (e.g., fully connected features) to further perform vehicle-level deep metric learning (e.g., triplet-based network). However, we argue that it is noteworthy to investigate the distinctiveness of local features and consider vehicle-part-level metric learning by reducing the intra-class variance as much as possible. In this article, we introduce a simple yet powerful deep model - the enforced intra-class alignment network (EIA-Net) - which can learn a more discriminative image representation by localizing key vehicle parts and jointly incorporating two distance metrics: vehicle-level embedding and vehicle-part-sensitive embedding. For learning features, we propose an effective feature extraction module that is composed of two components: the regional proposal network (RPN)-based network and part-based CNN. The RPN is used to define key vehicle regions and aggregate local features on these regions, whereas part-based CNN offers supplementary global features for the RPN-based network. The fusion features learned by feature extraction module are cast into the deep metric learning module. Especially, we derived an enforced intra-class alignment loss by re-utilizing key vehicle part information to enhance reducing intra-class variance. Furthermore, we modify the coupled cluster loss to model the vehicle-level embedding by enlarging the inter-class variance while shortening intra-class variance. Extensive experiments over benchmark datasets VehicleID and CompCars have shown that the proposed EIA-Net significantly outperforms the state-of-the-art approaches for vehicle model verification. Furthermore, we also conduct comprehensive experiments on vehicle re-identification datasets (i.e., VehicleID and VeRi776) to validate the generalization ability effectiveness of our proposed method.  © 2022 Association for Computing Machinery.",Deep metric learning; Image representation learning; Vehicle model verification; Vehicle re-identification,Deep learning; Embeddings; Extraction; Feature extraction; Image representation; Numerical methods; Security systems; Deep metric learning; Image representation learning; Image representations; Intra class; Metric learning; Model verification; Re identifications; Vehicle model verification; Vehicle modelling; Vehicle re-identification; Vehicles
Introduction to the Special Issue on Fine-Grained Visual Recognition and Re-Identification,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127401796&doi=10.1145%2f3505280&partnerID=40&md5=bd0407d974cf4cef04abe9f7d0b56a86,[No abstract available],,
From Coarse to Fine: Hierarchical Structure-aware Video Summarization,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127422827&doi=10.1145%2f3485472&partnerID=40&md5=43a44344a86be3d47774c2a99df71475,"Hierarchical structure is a common characteristic for some kinds of videos (e.g., sports videos, game videos): The videos are composed of several actions hierarchically and there exist temporal dependencies among segments with different scales, where action labels can be enumerated. Our ideas are based on two observations: First, the actions are the fundamental units for people to understand these videos. Second, the humans summarize a video by iteratively observing and refining, i.e., observing segments in video and hierarchically refining the boundaries of important actions. Based on the above insights, we generate action proposals to construct the structure of the video and formulate the summarization process as a hierarchical refining process. We also train a hierarchical summarization network with deep Q-learning (HQSN) to achieve the refining process and explore temporal dependency. Besides, we collect a new dataset that consists of structured game videos with fine-grain actions and importance annotations. The experimental results demonstrate the effectiveness of the proposed method.  © 2022 Association for Computing Machinery.",Reinforcement learning; Video understanding,Deep learning; Iterative methods; Refining; Coarse to fine; Fundamental units; Game videos; Hierarchical structures; Refining process; Sport video; Structure-aware; Video summarization; Video understanding; Video-games; Reinforcement learning
Fine-grained Image Classification via Multi-scale Selective Hierarchical Biquadratic Pooling,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127437964&doi=10.1145%2f3492221&partnerID=40&md5=25534dce91506680c3272c545762297a,"How to extract distinctive features greatly challenges the fine-grained image classification tasks. In previous models, bilinear pooling has been frequently adopted to address this problem. However, most bilinear pooling models neglect either intra or inter layer feature interaction. This insufficient interaction brings in the loss of discriminative information. In this article, we devise a novel fine-grained image classification approach named Multi-scale Selective Hierarchical biQuadratic Pooling (MSHQP). The proposed biquadratic pooling simultaneously models intra and inter layer feature interactions and enhances part response by integrating multi-layer features. The subsequent coarse-to-fine multi-scale interaction structure captures the complementary information within features. Finally, the active interaction selection module adaptively learns the optimal interaction subset for a specific dataset. Consequently, we obtain a robust image representation with coarse-to-fine semantics. We conduct experiments on five benchmark datasets. The experimental results demonstrate that MSHQP achieves competitive or even match the state-of-the-art methods in terms of both accuracy and computational efficiency, with 89.0%, 94.9%, 93.4%, 90.4%, and 91.5% top-1 classification accuracy on CUB-200-2011, Stanford-Cars, FGVC-Aircraft, Stanford-Dog, and VegFru, respectively.  © 2022 Association for Computing Machinery.",Biquadratic pooling; Feature interaction; Fine-grained classification; Interaction selection; Multi-scale feature,Classification (of information); Computational efficiency; Image representation; Semantics; Biquadratic pooling; Feature interactions; Fine grained; Fine-grained classification; Images classification; Inter-layers; Interaction selection; Intra-layer; Multi-scale features; Multi-scales; Image classification
Special Section on AI-empowered Multimedia Data Analytics for Smart Healthcare,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127421210&doi=10.1145%2f3505281&partnerID=40&md5=79de6aa31c3cf1c93bf00bf0d24b8e72,[No abstract available],,
Fine-Grained Adversarial Semi-Supervised Learning,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127370957&doi=10.1145%2f3485473&partnerID=40&md5=f50a88fa3b1b4d9cae7b80de088a1925,"In this article, we exploit Semi-Supervised Learning (SSL) to increase the amount of training data to improve the performance of Fine-Grained Visual Categorization (FGVC). This problem has not been investigated in the past in spite of prohibitive annotation costs that FGVC requires. Our approach leverages unlabeled data with an adversarial optimization strategy in which the internal features representation is obtained with a second-order pooling model. This combination allows one to back-propagate the information of the parts, represented by second-order pooling, onto unlabeled data in an adversarial training setting. We demonstrate the effectiveness of the combined use by conducting experiments on six state-of-the-art fine-grained datasets, which include Aircrafts, Stanford Cars, CUB-200-2011, Oxford Flowers, Stanford Dogs, and the recent Semi-Supervised iNaturalist-Aves. Experimental results clearly show that our proposed method has better performance than the only previous approach that examined this problem; it also obtained higher classification accuracy with respect to the supervised learning methods with which we compared.  © 2022 Association for Computing Machinery.",Adversarial learning; Deep neural networks; Fine-grained visual categorization; Semi-supervised learning,Computer vision; Adversarial learning; Fine grained; Fine-grained visual categorization; Performance; Second orders; Semi-supervised learning; Stanford; Training data; Unlabeled data; Visual categorization; Deep neural networks
Hybrid Modality Metric Learning for Visible-Infrared Person Re-Identification,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127445356&doi=10.1145%2f3473341&partnerID=40&md5=bde1760501a56e835038d0bcd084a8cc,"Visible-infrared person re-identification (Re-ID) has received increasing research attention for its great practical value in night-time surveillance scenarios. Due to the large variations in person pose, viewpoint, and occlusion in the same modality, as well as the domain gap brought by heterogeneous modality, this hybrid modality person matching task is quite challenging. Different from the metric learning methods for visible person re-ID, which only pose similarity constraints on class level, an efficient metric learning approach for visible-infrared person Re-ID should take both the class-level and modality-level similarity constraints into full consideration to learn sufficiently discriminative and robust features. In this article, the hybrid modality is divided into two types, within modality and cross modality. We first fully explore the variations that hinder the ranking results of visible-infrared person re-ID and roughly summarize them into three types: within-modality variation, cross-modality modality-related variation, and cross-modality modality-unrelated variation. Then, we propose a comprehensive metric learning framework based on four kinds of paired-based similarity constraints to address all the variations within and cross modality. This framework focuses on both class-level and modality-level similarity relationships between person images. Furthermore, we demonstrate the compatibility of our framework with any paired-based loss functions by giving detailed implementation of combing it with triplet loss and contrastive loss separately. Finally, extensive experiments of our approach on SYSU-MM01 and RegDB demonstrate the effectiveness and superiority of our proposed metric learning framework for visible-infrared person Re-ID.  © 2022 Association for Computing Machinery.",Cross-modality; Metric learning; Visible-infrared person re-identification,Class level; Cross modality; Learning approach; Learning frameworks; Learning methods; Matchings; Metric learning; Night time; Person re identifications; Visible-infrared person re-identification; Learning systems
Facial-expression-Aware Emotional Color Transfer Based on Convolutional Neural Network,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127877500&doi=10.1145%2f3464382&partnerID=40&md5=7024b33a91bc6e29913859215f3293b0,"Emotional color transfer aims to change the evoked emotion of a source image to that of a target image by adjusting color distribution. Most of existing emotional color transfer methods only consider the low-level visual features of an image and ignore the facial expression features when the image contains a human face, which would cause incorrect emotion evaluation for the given image. In addition, previous emotional color transfer methods may easily result in ambiguity between the emotion of resulting image and target image. For example, if the background of the target image is dark while the facial expression is happiness, then previous methods would directly transfer dark color to the source image, neglecting the facial emotion in the image. To solve this problem, we propose a new facial-expression-Aware emotional color transfer framework. Given a target image with facial expression features, we first predict the facial emotion label of the image through the emotion classification network. Then, facial emotion labels are matched with pre-Trained emotional color transfer models. Finally, we use the matched emotion model to transfer the color of the target image to the source image. Considering none of the existing emotion image databases, which focus on images that contain face and background, we built an emotion database for our new emotional color transfer framework that is called ""Face-Emotion database.""Experiments demonstrate that our method can successfully capture and transfer facial emotions, outperforming state-of-The-Art methods.  © 2022 Association for Computing Machinery.",emotion classification network; emotional color transfer; Face-Emotion database; Facial-expression,Color; Convolutional neural networks; Database systems; Classification networks; Color transfers; Emotion classification; Emotion classification network; Emotional color transfer; Face-emotion database; Facial emotions; Facial Expressions; Source images; Target images; Classification (of information)
Multi-feature Fusion VoteNet for 3D Object Detection,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127906426&doi=10.1145%2f3462219&partnerID=40&md5=a80ba267103d7d3f0906c96f7924d29c,"In this article, we propose a Multi-feature Fusion VoteNet (MFFVoteNet) framework for improving the 3D object detection performance in cluttered and heavily occluded scenes. Our method takes the point cloud and the synchronized RGB image as inputs to provide object detection results in 3D space. Our detection architecture is built on VoteNet with three key designs. First, we augment the VoteNet input with point color information to enhance the difference of various instances in a scene. Next, we integrate an image feature module into the VoteNet to provide a strong object class signal that can facilitate deterministic detections in occlusion. Moreover, we propose a Projection Non-Maximum Suppression (PNMS) method in 3D object detection to eliminate redundant proposals and hence provide more accurate positioning of 3D objects. We evaluate the proposed MFFVoteNet on two challenging 3D object detection datasets, i.e., ScanNetv2 and SUN RGB-D. Extensive experiments show that our framework can effectively improve the performance of 3D object detection.  © 2022 Association for Computing Machinery.",3D object detection; Images; multi-feature fusion; occlusion; point cloud,Feature extraction; Image fusion; Object detection; 3D object; 3d object detection; 3D spaces; Image; Multi-feature fusion; Objects detection; Occlusion; Performance; Point-clouds; RGB images; Object recognition
The Impact of Artificial Intelligence on the Creativity of Videos,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127901938&doi=10.1145%2f3462634&partnerID=40&md5=900ddbae93146cb8e07c5436416b1fa5,"This study explored the impact Artificial Intelligence (AI) has on the evaluation of creative elements in artistic videos. The aim was to verify to what extent the use of an AI algorithm (Style Transfer) contributes to changes in the perceived creativity of the videos. Creativity was evaluated in six quantitative items (Likert-Type scale) and one qualitative question (qualitative description of the creativity expressed in the video by two words or expressions). Six videos were shown to both control (N = 49) and experimental group (N = 52) aiming at determining possible differences in creativity assessment criteria. Furthermore, both groups contained experts (Experimental, N = 27; Control, N = 25) and non-experts (Experimental, N = 25; Control, N = 24). The first round of videos composed of six videos that were the same for both the experimental and control condition (used to check for bias). No significant differences were found. In a second round, six videos were shown with AI transformation (experimental condition) and without that transformation (control group). Results showed that in two cases the perceived creativity increased in experimental condition, in one case a decrease occurred. In most evaluations no differences were observed. Qualitative evaluations reinforce the absence of a general pattern of improvements in AI transformations. Altogether, the results emphasize the importance of human mediation in the application of AI in creative production: A hybrid approach, or rather, Hybrid Intelligence.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Creativity; real-Time style transfer,Artificial intelligence algorithms; Assessment criteria; Condition; Creatives; Creativity; Experimental conditions; Experimental groups; Perceived creativities; Real- time; Real-time style transfer; Artificial intelligence
Robust Unsupervised Gaze Calibration Using Conversation and Manipulation Attention Priors,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127891881&doi=10.1145%2f3472622&partnerID=40&md5=61225e4a1aace1400c13be7273b75807,"Gaze estimation is a difficult task, even for humans. However, as humans, we are good at understanding a situation and exploiting it to guess the expected visual focus of attention of people, and we usually use this information to retrieve people's gaze. In this article, we propose to leverage such situation-based expectation about people's visual focus of attention to collect weakly labeled gaze samples and perform person-specific calibration of gaze estimators in an unsupervised and online way. In this context, our contributions are the following: (i) we show how task contextual attention priors can be used to gather reference gaze samples, which is a cumbersome process otherwise; (ii) we propose a robust estimation framework to exploit these weak labels for the estimation of the calibration model parameters; and (iii) we demonstrate the applicability of this approach on two human-human and human-robot interaction settings, namely conversation and manipulation. Experiments on three datasets validate our approach, providing insights on the priors effectiveness and on the impact of different calibration models, particularly the usefulness of taking head pose into account.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",conversation; Gaze estimation; manipulation; online calibration; remote sensor; RGB-D camera; unsupervised calibration; visual focus of attention,Calibration model; Conversation; Gaze estimation; Manipulation; On-line calibration; Remote sensors; RGB-D camera; Robust estimation; Unsupervised calibration; Visual focus of attentions; Human robot interaction
Modeling the User Experience of Watching 360° Videos with Head-Mounted Displays,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127879866&doi=10.1145%2f3463825&partnerID=40&md5=38ed073d8ee917fbf6afd746d4774a4b,"Conducting user studies to quantify the Quality of Experience (QoE) of watching the increasingly more popular 360° videos in Head-Mounted Displays (HMDs) is time-consuming, tedious, and expensive. Deriving QoE models, however, is very challenging because of the diverse viewing behaviors and complex QoE features and factors. In this article, we compile a wide spectrum of QoE features and factors that may contribute to the overall QoE. We design and conduct a user study to build a dataset of the overall QoE, QoE features, and QoE factors. Using the dataset, we derive the QoE models for both the Mean Opinion Score (MOS) and Individual Score (IS), where MOS captures the aggregated QoE across all subjects, while IS captures the QoE of individual subjects. Our derived overall QoE models achieve 0.98 and 0.91 in Pearson's Linear Correlation Coefficient (PLCC) for MOS and IS, respectively. Besides, we make several new observations on our user study results, such as (1) content factors dominate the overall QoE across all factor categories, (2) Video Multi-Method Assessment Fusion (VMAF) is the dominating factor among content factors, and (3) the perceived cybersickness is affected by human factors more among others. Our proposed user study design is useful for QoE modeling (specifically) and subjective evaluations (in general) of emerging 360° tiled video streaming to HMDs.  © 2022 Association for Computing Machinery.",individual score; mean opinion score; user study; Virtual reality,Helmet mounted displays; Virtual reality; Head-mounted-displays; Individual scores; Linear correlation coefficient; Mean opinion scores; Method assessment; Multi methods; Overall quality; User study; Users' experiences; Wide spectrum; Quality of service
TTV Regularized LRTA Technique for the Estimation of Haze Model Parameters in Video Dehazing,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127844158&doi=10.1145%2f3465454&partnerID=40&md5=4d92db09bc23c14b5c9d84838b1224c0,"Nowadays, intelligent transport systems have a major role in providing a safe and secure traffic society for passengers, pedestrians, and vehicles. However, some bad weather conditions such as haze or fog may affect the visual clarity of video footage captured by the camera. This will cause a malfunction in further video processing algorithms performed by such automated systems. This article proposes an efficient technique for estimating the atmospheric light and the transmission map in the haze model entirely in tensor domain for video dehazing. In this work, the atmospheric light is appraised using the Mie scattering principle of visible light and the temporal coherency among the frames is achieved by means of tensor algebra. Furthermore, the transmission map is computed using Low Rank Tensor Approximation (LRTA) based on Weighted Tensor Nuclear Norm (WTNN) minimization and Tensor Total Variation (TTV) regularization. WTNN minimization is used to smooth the coarse transmission map, and TTV regularization is employed to maintain spatio-Temporal continuity by preserving the details of salient structures and edges. The novelty of the proposed model is confined in the efficient formulation of a unified optimization model for the estimation of transmission map and atmospheric light in the tensor domain with fine-Tuned regularization terms, which is not reported till now in the direction of video dehazing. Extensive experiments show that the proposed method outperforms state-of-The-Art methods in video dehazing.  © 2022 Association for Computing Machinery.",Low Rank Tensor Approximation (LRTA); tensor total variation and video deweathering; Video dehazing,Automation; Intelligent systems; Light; Light transmission; Tensors; Video signal processing; Approximation techniques; Dehazing; Low rank tensor approximation; Low-rank tensor approximations; Modeling parameters; Nuclear norm minimizations; Tensor total variation and video deweathering; Total variation regularization; Total-variation; Video dehazing; Demulsification
CAPTAIN: Comprehensive Composition Assistance for Photo Taking,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127904398&doi=10.1145%2f3462762&partnerID=40&md5=790a4514ecc85c8506ed95ac3d1296ae,"Many people are interested in taking astonishing photos and sharing them with others. Emerging high-Tech hardware and software facilitate the ubiquitousness and functionality of digital photography. Because composition matters in photography, researchers have leveraged some common composition techniques, such as the rule of thirds and the perspective-related techniques, in providing photo-Taking assistance. However, composition techniques developed by professionals are far more diverse than well-documented techniques can cover. We present a new approach to leverage the underexplored photography ideas, which are virtually unlimited, diverse, and correlated. We propose a comprehensive fork-join framework, named CAPTAIN (Composition Assistance for Photo Taking), to guide a photographer with a variety of photography ideas. The framework consists of a few components: integrated object detection, photo genre classification, artistic pose clustering, and personalized aesthetics-Aware image retrieval. CAPTAIN is backed by a large managed dataset crawled from a Website with ideas from photography enthusiasts and professionals. The work proposes steps to decompose a given amateurish shot into composition ingredients and compose them to bring the photographer a list of useful and related ideas. The work addresses personal preferences for composition by presenting a user-specified preference list of photography ideas. We have conducted many experiments on the newly proposed components and reported findings. A user study demonstrates that the work is useful to those taking photos.  © 2022 Association for Computing Machinery.",deep learning; Image aesthetics; image retrieval; recommender system,Deep learning; Image retrieval; Object detection; Search engines; Clusterings; Composition technique; Deep learning; Digital photography; Genre classification; Hardware and software; High tech; Image Aesthetics; New approaches; Personal preferences; Large dataset
Online Learning for Adaptive Video Streaming in Mobile Networks,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127911631&doi=10.1145%2f3460819&partnerID=40&md5=aa0678c80eb236ca212278b5f933643d,"In this paper, we propose a novel algorithm for video bitrate adaptation in HTTP Adaptive Streaming (HAS), based on online learning. The proposed algorithm, named Learn2Adapt (L2A), is shown to provide a robust bitrate adaptation strategy which, unlike most of the state-of-The-Art techniques, does not require parameter tuning, channel model assumptions, or application-specific adjustments. These properties make it very suitable for mobile users, who typically experience fast variations in channel characteristics. Experimental results, over real 4G traffic traces, show that L2A improves on the overall Quality of Experience (QoE) and in particular the average streaming bitrate, a result obtained independently of the channel and application scenarios.  © 2022 Association for Computing Machinery.",Adaptive video streaming; online optimization,E-learning; Mobile telecommunication systems; Video streaming; Adaptation strategies; Adaptive streaming; Adaptive video streaming; Bit rates; Novel algorithm; Online learning; Online optimization; Parameters tuning; State-of-the-art techniques; Video bitrate; Quality of service
Defining Scents: A Systematic Literature Review of Olfactory-based Computing Systems,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127906853&doi=10.1145%2f3470975&partnerID=40&md5=0162e3562ad7a5a081a620ee67642234,"The human sense of smell is a primal ability that has the potential to reveal unexplored relationships between user behaviors and technology. Humans use millions of olfactory receptor cells to observe the environment around them. Olfaction studies are gaining popularity with the progression of scent delivering (commercial and prototype) devices. This influx of research features various software and hardware designs. Additionally, previous studies have explored numerous target audiences and evaluation methodologies. This article presents a systematic review of pertinent literature that investigates olfactory-based computing (OBC) systems in the field of Human-Computer Interaction. Last, this article highlights state-of-The-Art study/system designs, evaluation methods, and offers insights on ways to address current challenges/contributions relevant to OBC technologies.  © 2022 Association for Computing Machinery.",human-computer interaction; multisensory multimedia; Olfaction; olfactory displays; smell,Behavioral research; Odors; Computing system; Human sense; Human use; Multisensory; Multisensory multimedium; Olfaction; Olfactory display; Olfactory receptors; Systematic literature review; User behaviors; Human computer interaction
Sparse LIDAR Measurement Fusion with Joint Updating Cost for Fast Stereo Matching,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127900351&doi=10.1145%2f3471870&partnerID=40&md5=c3d91bdc0e67a952595011d3603af291,"The complementary virtues of active and passive depth sensors inspire the LIDAR-Stereo fusion for enhancing the accuracy of stereo matching. However, most of the fusion based stereo matching algorithms have exploited dense LIDAR priors with single fusion methodology. In this paper, we intend to break these fetters, utilizing sparse LIDAR priors with multi-step fusion strategy for obtaining accurate disparity estimation more efficiently. At first, random sparse sampling LIDAR depth measurements are provided in Naive Fusion for updating the matching cost of Semi-Global Matching (SGM). Then Neighborhood Based Fusion is performed based on the former step for further updating the cost. Subsequently, Diffusion Based Fusion is utilized to update both the cost and disparities. At last, Tree Filtering is applied for removing speckle outliers and smoothing disparities. Performance evaluations on various stereo data sets demonstrate that the proposed algorithm outperforms other most challenging stereo matching algorithms significantly with approximately real-Time implementation efficiency. Furthermore, it is worth pointing out that our proposal surprisingly possesses one of the top ten performances on Middlebury v.3 online evaluation system even if it has not been adopted any learning-based techniques.  © 2022 Association for Computing Machinery.",LIDAR-Stereo fusion; Stereo matching; Tree Filtering,Forestry; Optical radar; Depth sensors; Disparity estimations; Fusion methodology; Fusion strategies; LIDAR-stereo fusion; Measurement fusion; Multi-step fusion; Stereo matching algorithm; Stereo-matching; Tree filtering; Real time control
Diversely-Supervised Visual Product Search,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127913937&doi=10.1145%2f3461646&partnerID=40&md5=0410bb52dd1efddef2c3f5c5ea381823,"This article strives for a diversely supervised visual product search, where queries specify a diverse set of labels to search for. Where previous works have focused on representing attribute, instance, or category labels individually, we consider them together to create a diverse set of labels for visually describing products. We learn an embedding from the supervisory signal provided by every label to encode their interrelationships. Once trained, every label has a corresponding visual representation in the embedding space, which is an aggregation of selected items from the training set. At search time, composite query representations retrieve images that match a specific set of diverse labels. We form composite query representations by averaging over the aggregated representations of each diverse label in the specific set. For evaluation, we extend existing product datasets of cars and clothes with a diverse set of labels. Experiments show the benefits of our embedding for diversely supervised visual product search in seen and unseen product combinations and for discovering product design styles.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",diverse representation; diverse supervision; Product retrieval,Product design; Diverse representation; Diverse supervision; Embeddings; Learn+; Product combinations; Product retrieval; Query representations; Search time; Training sets; Visual representations; Embeddings
RD-IOD: Two-Level Residual-Distillation-Based Triple-Network for Incremental Object Detection,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127866892&doi=10.1145%2f3472393&partnerID=40&md5=da0e6ef58b1acbadcc062ddec1b40e1f,"As a basic component in multimedia applications, object detectors are generally trained on a fixed set of classes that are pre-defined. However, new object classes often emerge after the models are trained in practice. Modern object detectors based on Convolutional Neural Networks (CNN) suffer from catastrophic forgetting when fine-Tuning on new classes without the original training data. Therefore, it is critical to improve the incremental learning capability on object detection. In this article, we propose a novel Residual-Distillation-based Incremental learning method on Object Detection (RD-IOD). Our approach rests on the creation of a triple-network based on Faster R-CNN. To enable continuous learning from new classes, we use the original model as well as a residual model to guide the learning of the incremental model on new classes while maintaining the previous learned knowledge. To better maintain the discrimination between the features of old and new classes, the residual model is jointly trained with the incremental model on new classes in the incremental learning procedure. In addition, a two-level distillation scheme is designed to guide the training process, which consists of (1) a general distillation for imitating the original model in feature space along with a residual distillation on the features in both image level and instance level, and (2) a joint classification distillation on the output layers. To well preserve the learned knowledge, we design a 2-Threshold training strategy to guide the learning of a Region Proposal Network and a detection head. Extensive experiments conducted on VOC2007 and COCO demonstrate that the proposed method can effectively learn to incrementally detect objects of new classes, and the problem of catastrophic forgetting is mitigated. Our code is available at https://github.com/yangdb/RD-IOD.  © 2022 Association for Computing Machinery.",incremental learning; Object detection; residual distillation,Convolutional neural networks; Distillation; Learning systems; Object recognition; Catastrophic forgetting; Convolutional neural network; Incremental learning; Incremental models; Multimedia applications; Object detectors; Objects detection; Original model; Residual distillation; Residual model; Object detection
Privacy-preserving Motion Detection for HEVC-compressed Surveillance Video,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127861801&doi=10.1145%2f3472669&partnerID=40&md5=6bd91b58e4207f664c70ebd7711c2fb9,"In the cloud era, a large amount of data is uploaded to and processed by public clouds. The risk of privacy leakage has become a major concern for cloud users. Cloud-based video surveillance requires motion detection, which may reveal the privacy of people in a surveillance video. Privacy-preserving video surveillance allows motion detection while protecting privacy. The existing scheme [25], designed to detect motion on encrypted and H.264-compressed surveillance videos, does not work well on more advanced video compression schemes such as HEVC.In this article, we propose the first motion detection method on encrypted and HEVC-compressed videos. It adopts a novel approach that exploits inter-prediction reference relationships among coding blocks to detect motion regions. The partition pattern and the number of coding bits of each detection block used in prior art are also used to help detect motion regions. Spatial and temporal consistency of a moving object and Kalman filtering are applied to segment connected/merged motion regions, remove noise and background motions, and refine trajectories and shapes of detected moving objects. Experimental results indicate that our detection method achieves high detection recall, precision, and F1-score for surveillance videos of both high and low resolutions with various scenes. It has a similarly high detection accuracy on encrypted and HEVC-compressed videos as that of the existing motion detection method [25] on encrypted and H.264-compressed videos. Our proposed method incurs no bit-rate overhead and has a very low computational complexity for both motion detection and encryption of HEVC videos.  © 2022 Association for Computing Machinery.",HEVC; motion detection; privacy protection; Privacy-preserving motion detection; surveillance videos; video encryption,Image compression; Monitoring; Object detection; Privacy-preserving techniques; Security systems; Compressed video; Detection methods; HEVC; Motion detection; Privacy preserving; Privacy protection; Privacy-preserving motion detection; Surveillance video; Video encryption; Motion analysis
LogoDet-3K: A Large-scale Image Dataset for Logo Detection,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127901903&doi=10.1145%2f3466780&partnerID=40&md5=be81271fd7516dc0fb85dacc038fe931,"Logo detection has been gaining considerable attention because of its wide range of applications in the multimedia field, such as copyright infringement detection, brand visibility monitoring, and product brand management on social media. In this article, we introduce LogoDet-3K, the largest logo detection dataset with full annotation, which has 3,000 logo categories, about 200,000 manually annotated logo objects, and 158,652 images. LogoDet-3K creates a more challenging benchmark for logo detection, for its higher comprehensive coverage and wider variety in both logo categories and annotated objects compared with existing datasets. We describe the collection and annotation process of our dataset and analyze its scale and diversity in comparison to other datasets for logo detection. We further propose a strong baseline method Logo-Yolo, which incorporates Focal loss and CIoU loss into the basic YOLOv3 framework for large-scale logo detection. It obtains about 4% improvement on the average performance compared with YOLOv3, and greater improvements compared with reported several deep detection models on LogoDet-3K. We perform extensive evaluation on three other existing datasets to further verify on both logo detection and retrieval tasks, and we demonstrate better generalization ability of LogoDet-3K on logo detection and retrieval tasks. The LogoDet-3K dataset is used to promote large-scale logo-related research. The code and LogoDet-3K can be found at https://github.com/Wangjing1551/LogoDet-3K-Dataset.  © 2022 Association for Computing Machinery.",Datasets; deep learning; logo detection; multi-scale,Copyrights; Large dataset; Baseline methods; Brand management; Copyright infringement; Dataset; Deep learning; Large-scale image datasets; Large-scales; Logo detections; Multi-scales; Social media; Deep learning
Authentication of LINE Chat History Files by Information Hiding,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127850907&doi=10.1145%2f3474225&partnerID=40&md5=eb59d5040fdbeaa3aacd0985a7ece386,"With the prevalence of smartphones, message exchanges via mobile chatting programs like LINE have become popular. The messages in the form of chat records in a LINE chat history, after being downloaded for legal uses, might be tampered with illicitly. A novel method for authenticating the chat history against such attacks is proposed. The signal used for authenticating each chat-record segment is created by concatenating the ID label of the segment and a digest yielded by hashing the segment content. The signal is then encoded by three types of spacing code, namely half space, full space, and tab, and embedded into the blank-space areas created by the tab codes in the chat records of the segment. Authentication of a history file is accomplished by extracting the authentication signals embedded in the file and comparing them with the original signals computed directly from the file. The embedded signals are invisible, arousing no suspicion from the hacker. The signals are fragile, because any modification of the records can be detected by the authentication process. Experiments for testing four types of tampering with text files of four languages have been conducted, yielding correct authentication results that show the feasibility of the proposed method.  © 2022 Association for Computing Machinery.",authentication; half and full spaces; information hiding; LINE chat history; tab code,Codes (symbols); Geometry; Personal computing; Signal processing; Embedded signals; Half and full space; Half spaces; Information hiding; LINE chat history; Message exchange; Novel methods; Original signal; Smart phones; Tab code; Authentication
Optimizing Immersive Video Coding Configurations Using Deep Learning: A Case Study on TMIV,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127909113&doi=10.1145%2f3471191&partnerID=40&md5=d099edef8b3f7fdfa1b1c9d68c82280f,"Immersive video streaming technologies improve Virtual Reality (VR) user experience by providing users more intuitive ways to move in simulated worlds, e.g., with 6 Degree-of-Freedom (6DoF) interaction mode. A naive method to achieve 6DoF is deploying cameras at numerous different positions and orientations that may be required based on users' movement, which unfortunately is expensive, tedious, and inefficient. A better solution for realizing 6DoF interactions is to synthesize target views on-The-fly from a limited number of source views. While such view synthesis is enabled by the recent Test Model for Immersive Video (TMIV) codec, TMIV dictates manually-composed configurations, which cannot exercise the tradeoff among video quality, decoding time, and bandwidth consumption. In this article, we study the limitation of TMIV and solve its configuration optimization problem by searching for the optimal configuration in a huge configuration space. We first identify the critical parameters in the TMIV configurations. Then, we introduce two Neural Network (NN)-based algorithms from two heterogeneous aspects: (i) a Convolutional Neural Network (CNN) algorithm solving a regression problem and (ii) a Deep Reinforcement Learning (DRL) algorithm solving a decision making problem, respectively. We conduct both objective and subjective experiments to evaluate the CNN and DRL algorithms on two diverse datasets: An equirectangular and a perspective projection dataset. The objective evaluations reveal that both algorithms significantly outperform the default configurations. In particular, with the equirectangular (perspective) projection dataset, the proposed algorithms only require 95% (23%) decoding time, stream 79% (23%) views, and improve the utility by 6% (73%) on average. The subjective evaluations confirm the proposed algorithms consume fewer resources while achieving comparable Quality of Experience (QoE) than the default and the optimal TMIV configurations.  © 2022 Association for Computing Machinery.",3DoF+; 6DoF; augmented reality; extended reality; head-mounted displays; optimization; streaming; view synthesis; Virtual reality,Augmented reality; Decision making; Decoding; Degrees of freedom (mechanics); Helmet mounted displays; Quality control; Quality of service; Reinforcement learning; Virtual reality; 3dof+; 6 degree of freedom; Convolutional neural network; Extended reality; Head-mounted-displays; Immersive; Optimisations; Streaming; Test models; View synthesis; Deep learning
MMSUM Digital Twins: A Multi-view Multi-modality Summarization Framework for Sporting Events,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127848930&doi=10.1145%2f3462777&partnerID=40&md5=a4a0041f59a0d832a45b99d7fb3d43c7,"Sporting events generate a massive amount of traffic on social media with live moment-To-moment accounts as any given situation unfolds. The generated data are intensified by fans feelings, reactions, and subjective opinions towards what happens during the event, all of which are based on their individual points of view. Analyzing and summarizing this data will generate a comprehensive overview of the event in terms of how the event evolves and how fans react and view the event based on their perspectives. Previously, most of the summarization works ignore fan reactions and subjective opinions, and focus primarily on generating an objective-view summary. We believe that an effective and useful summary should consider human reactions, sentiment, and point of view, as opposed to simply describing what happens during the event. Accordingly, in this work, we propose MMSUM Digital Twins: A summarization framework that is capable of generating a multi-view multi-modal summary for sporting events in real-Time. The proposed digital twins-based framework consists of four main components: sub-event recognition which detects the event's key moments, tweet categorization, which determines which team the tweets' writers support and assigns tweets to their teams, sentiment analysis to track fans' state of mind, and image popularity prediction for selecting representative images. Furthermore, the MMSUM employs a visual-filtering model to address the issue of noisy images that inundate social media, compromising the summarization quality. We leverage the knowledge of sport fans to evaluate the generated multi-view summarization through an online user study. The experiment results confirm the effectiveness of our proposed approach for summarizing sporting events by considering multimedia data, sentiment, and subjective views of the event.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Digital twins; multi-view; multimedia; popularity prediction; sentiment analysis; social events; social media; sporting events; subjective; summarization,Knowledge management; Social networking (online); Sports; Multi-modality; Multi-views; Multimedium; Popularity predictions; Sentiment analysis; Social events; Social media; Sporting event; Subjective; Summarization; Sentiment analysis
Learning Hierarchical Video Graph Networks for One-Stop Video Delivery,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127846115&doi=10.1145%2f3466886&partnerID=40&md5=c82707f25196561319c8dfe5cbf47175,"The explosive growth of video data has brought great challenges to video retrieval, which aims to find out related videos from a video collection. Most users are usually not interested in all the content of retrieved videos but have a more fine-grained need. In the meantime, most existing methods can only return a ranked list of retrieved videos lacking a proper way to present the video content. In this paper, we introduce a distinctively new task, namely One-Stop Video Delivery (OSVD) aiming to realize a comprehensive retrieval system with the following merits: it not only retrieves the relevant videos but also filters out irrelevant information and presents compact video content to users, given a natural language query and video collection. To solve this task, we propose an end-To-end Hierarchical Video Graph Reasoning framework (HVGR), which considers relations of different video levels and jointly accomplishes the one-stop delivery task. Specifically, we decompose the video into three levels, namely the video-level, moment-level, and the clip-level in a coarse-To-fine manner, and apply Graph Neural Networks (GNNs) on the hierarchical graph to model the relations. Furthermore, a pairwise ranking loss named Progressively Refined Loss is proposed based on prior knowledge that there is a relative order of the similarity of query-video, query-moment, and query-clip due to the different granularity of matched information. Extensive experimental results on benchmark datasets demonstrate that the proposed method achieves superior performance compared with baseline methods.  © 2022 Association for Computing Machinery.",Cross modal; deep learning; graph neural networks; video retrieval,Benchmarking; Deep neural networks; Graph neural networks; Search engines; Cross-modal; Deep learning; Explosive growth; Graph networks; Graph neural networks; One stop; Video collections; Video contents; Video delivery; Video retrieval; Video recording
Mimicking Individual Media Quality Perception with Neural Network based Artificial Observers,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127869387&doi=10.1145%2f3464393&partnerID=40&md5=fadfbd3030d3bcfa5999c9aed137bf5e,"The media quality assessment research community has traditionally been focusing on developing objective algorithms to predict the result of a typical subjective experiment in terms of Mean Opinion Score (MOS) value. However, the MOS, being a single value, is insufficient to model the complexity and diversity of human opinions encountered in an actual subjective experiment. In this work we propose a complementary approach for objective media quality assessment that attempts to more closely model what happens in a subjective experiment in terms of single observers and, at the same time, we perform a qualitative analysis of the proposed approach while highlighting its suitability. More precisely, we propose to model, using neural networks (NNs), the way single observers perceive media quality. Once trained, these NNs, one for each observer, are expected to mimic the corresponding observer in terms of quality perception. Then, similarly to a subjective experiment, such NNs can be used to simulate the users' single opinions, which can be later aggregated by means of different statistical indicators such as average, standard deviation, quantiles, etc. Unlike previous approaches that consider subjective experiments as a black box providing reliable ground truth data for training, the proposed approach is able to consider human factors by analyzing and weighting individual observers. Such a model may therefore implicitly account for users' expectations and tendencies, that have been shown in many studies to significantly correlate with visual quality perception. Furthermore, our proposal also introduces and investigates an index measuring how much inconsistency there would be if an observer was asked to rate many times the same stimulus. Simulation experiments conducted on several datasets demonstrate that the proposed approach can be effectively implemented in practice and thus yielding a more complete objective assessment of end users' quality of experience.  © 2022 Association for Computing Machinery.",human factors; Media quality; neural networks; subjects opinions; user expectations,Quality of service; Mean opinion scores; Media quality; Network-based; Neural-networks; Quality assessment; Quality perceptions; Single observer; Subject opinion; Subjective experiments; User expectations; Human engineering
A Novel Multi-Modal Network-Based Dynamic Scene Understanding,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127896688&doi=10.1145%2f3462218&partnerID=40&md5=465e37afe2520b07c5a1d399a37f5b82,"In recent years, dynamic scene understanding has gained attention from researchers because of its widespread applications. The main important factor in successfully understanding the dynamic scenes lies in jointly representing the appearance and motion features to obtain an informative description. Numerous methods have been introduced to solve dynamic scene recognition problem, nevertheless, a few concerns still need to be investigated. In this article, we introduce a novel multi-modal network for dynamic scene understanding from video data, which captures both spatial appearance and temporal dynamics effectively. Furthermore, two-level joint tuning layers are proposed to integrate the global and local spatial features as well as spatial and temporal stream deep features. In order to extract the temporal information, we present a novel dynamic descriptor, namely, Volume Symmetric Gradient Local Graph Structure (VSGLGS), which generates temporal feature maps similar to optical flow maps. However, this approach overcomes the issues of optical flow maps. Additionally, Volume Local Directional Transition Pattern (VLDTP) based handcrafted spatiotemporal feature descriptor is also introduced, which extracts the directional information through exploiting edge responses. Lastly, a stacked Bidirectional Long Short-Term Memory (Bi-LSTM) network along with a temporal mixed pooling scheme is designed to achieve the dynamic information without noise interference. The extensive experimental investigation proves that the proposed multi-modal network outperforms most of the state-of-The-Art approaches for dynamic scene understanding.  © 2022 Association for Computing Machinery.",Multi-modal network; stacked Bi-LSTM network; temporal mixed pooling; volume local directional transition pattern; volume symmetric gradient local graph structure,Flow graphs; Graphic methods; Optical flows; Dynamic scenes; Graph structures; Memory network; Multimodal network; Stacked bidirectional long short-term memory network; Symmetrics; Temporal mixed pooling; Transition patterns; Volume local directional transition pattern; Volume symmetric gradient local graph structure; Long short-term memory
Hyperspectral Image Reconstruction Using Multi-scale Fusion Learning,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127849767&doi=10.1145%2f3477396&partnerID=40&md5=bffc520221402165fee038e3386e939c,"Hyperspectral imaging is a promising imaging modality that simultaneously captures several images for the same scene on narrow spectral bands, and it has made considerable progress in different fields, such as agriculture, astronomy, and surveillance. However, the existing hyperspectral (HS) cameras sacrifice the spatial resolution for providing the detail spectral distribution of the imaged scene, which leads to low-resolution (LR) HS images compared with the common red-green-blue (RGB) images. Generating a high-resolution HS (HR-HS) image via fusing an observed LR-HS image with the corresponding HR-RGB image has been actively studied. Existing methods for this fusing task generally investigate hand-crafted priors to model the inherent structure of the latent HR-HS image, and they employ optimization approaches for solving it. However, proper priors for different scenes can possibly be diverse, and to figure it out for a specific scene is difficult. This study investigates a deep convolutional neural network (DCNN)-based method for automatic prior learning, and it proposes a novel fusion DCNN model with multi-scale spatial and spectral learning for effectively merging an HR-RGB and LR-HS images. Specifically, we construct an U-shape network architecture for gradually reducing the feature sizes of the HR-RGB image (Encoder-side) and increasing the feature sizes of the LR-HS image (Decoder-side), and we fuse the HR spatial structure and the detail spectral attribute in multiple scales for tackling the large resolution difference in spatial domain of the observed HR-RGB and LR-HS images. Then, we employ multi-level cost functions for the proposed multi-scale learning network to alleviate the gradient vanish problem in long-propagation procedure. In addition, for further improving the reconstruction performance of the HR-HS image, we refine the predicted HR-HS image using an alternating back-projection method for minimizing the reconstruction errors of the observed LR-HS and HR-RGB images. Experiments on three benchmark HS image datasets demonstrate the superiority of the proposed method in both quantitative values and visual qualities.  © 2022 Association for Computing Machinery.",alternative back projection; DCNN; Hyper-spectral image super-resolution; multi-scale fusion; spectral and spatial fusion; U-shape architecture,Backpropagation; Convolutional neural networks; Cost functions; Deep neural networks; Hyperspectral imaging; Image enhancement; Image fusion; Image reconstruction; Network architecture; Alternative back projection; Backprojections; Hyper-spectral image super-resolution; Hyper-spectral images; Image super resolutions; Lower resolution; Multiscale fusion; Spectral and spatial fusion; U shape; U-shape architecture; Spectroscopy
An Empirical Method for Causal Inference of Constructs for QoE in Haptic-Audiovisual Communications,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127893040&doi=10.1145%2f3473986&partnerID=40&md5=3135042b9ae34c657589bd7590dc546c,"This article proposes an empirical method for inferring causal directions in multidimensional Quality of Experience (QoE) in multimedia communications, noting that causation in QoE is perceptual. As an example for modeling framework, we pick up a Bayesian structural equation model (SEM) previously built for haptic audiovisual interactive communications. The SEM includes three constructs (Audiovisual quality, Haptic quality, and User experience quality), which are latent variables each representing a group of observed variables with similar characteristics. In the SEM, the causal directions of the constructs were assumed by resorting to the domain knowledge. This article aims at proposing a methodology for inferring causal directions of constructs in general by verifying the assumption of causal directions in the SEM through their observed data alone. For that purpose, we compare six SEMs each with different causal directions of constructs, one of which is the one from the domain knowledge. The proposed method is based on QoE prediction by a Bayesian approach with Markov chain Monte Carlo (MCMC) simulation. Setting observed scores to the indicators of exogenous variables in each SEM, we predict values of all the indicators; we then assess the mean square error (MSE) between predicted QoE and mean opinion score (MOS) from observed scores and estimate the probability distribution of the MSE in each SEM. We can compare any two SEMs to find which is more plausible by examining the probability that the MSE for one SEM is smaller than or equal to that for the other. These probabilities are estimated with MCMC simulation. The method indicates that the causal directions thus inferred for the haptic audiovisual interactive communications adequately support the original ones drawn from the domain knowledge. In addition, we demonstrate that QoE can behave like the ""impact-perceive-Adapt""model of the effects of delayed haptic and visual feedback on performance in a collaborative environment, which Jay, Glencross, and Hubbold proposed in 2007, and that it accompanies reversal of plausible causal directions like a flip-flop.  © 2022 Association for Computing Machinery.",Bayesian modeling; Causal inference; causation; construct; haptic-Audiovisual interactive communications; latent variables; MCMC; media synchronization; OpenBUGS; Quality of Experience (QoE); SEM,Computer graphics; Markov processes; Mean square error; Monte Carlo methods; Multimedia systems; Probability distributions; Quality of service; Visual communication; Bayesian modelling; Causal inferences; Causation; Construct; Haptic-audiovisual interactive communication; Haptics; Interactive communications; Latent variable; Markov chain Monte Carlo; Markov Chain Monte-Carlo; Media synchronization; OpenBUGS; Quality of experience; Structural equation models; Bayesian networks
Mask-Guided Deformation Adaptive Network for Human Parsing,2022,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127835136&doi=10.1145%2f3467889&partnerID=40&md5=8f227025686d12563d81410e004c1583,"Due to the challenges of densely compacted body parts, nonrigid clothing items, and severe overlap in crowd scenes, human parsing needs to focus more on multilevel feature representations compared to general scene parsing tasks. Based on this observation, we propose to introduce the auxiliary task of human mask and edge detection to facilitate human parsing. Different from human parsing, which exploits the discriminative features of each category, human mask and edge detection emphasizes the boundaries of semantic parsing regions and the difference between foreground humans and background clutter, which benefits the parsing predictions of crowd scenes and small human parts. Specifically, we extract human mask and edge labels from the human parsing annotations and train a shared encoder with three independent decoders for the three mutually beneficial tasks. Furthermore, the decoder feature maps of the human mask prediction branch are further exploited as attention maps, indicating human regions to facilitate the decoding process of human parsing and human edge detection. In addition to these auxiliary tasks, we further alleviate the problem of deformed clothing items under various human poses by tracking the deformation patterns with the deformable convolution. Extensive experiments show that the proposed method can achieve superior performance against state-of-The-Art methods on both single and multiple human parsing datasets. Codes and trained models are available https://github.com/ViktorLiang/MGDAN.  © 2022 Association for Computing Machinery.",deformable convolution; Human parsing; multi-Task learning,Computer vision; Decoding; Deformation; Edge detection; Feature extraction; Learning systems; Semantics; Adaptive networks; Background clutter; Body parts; Deformable convolution; Discriminative features; Feature representation; Human parsing; Multilevels; Multitask learning; Semantic parsing; Convolution
Y-Net: Dual-branch joint network for semantic segmentation,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123319867&doi=10.1145%2f3460940&partnerID=40&md5=d36305e04c31ae55d6a9bb776e28502b,"Most existing segmentation networks are built upon a ""U-shaped""encoder-decoder structure, where the multi-level features extracted by the encoder are gradually aggregated by the decoder. Although this structure has been proven to be effective in improving segmentation performance, there are two main drawbacks. On the one hand, the introduction of low-level features brings a significant increase in calculations without an obvious performance gain. On the other hand, general strategies of feature aggregation such as addition and concatenation fuse features without considering the usefulness of each feature vector, which mixes the useful information with massive noises. In this article, we abandon the traditional ""U-shaped""architecture and propose Y-Net, a dual-branch joint network for accurate semantic segmentation. Specifically, it only aggregates the high-level features with low-resolution and utilizes the global context guidance generated by the first branch to refine the second branch. The dual branches are effectively connected through a Semantic Enhancing Module, which can be regarded as the combination of spatial attention and channel attention. We also design a novel Channel-Selective Decoder (CSD) to adaptively integrate features from different receptive fields by assigning specific channelwise weights, where the weights are input-dependent. Our Y-Net is capable of breaking through the limit of singe-branch network and attaining higher performance with less computational cost than ""U-shaped""structure. The proposed CSD can better integrate useful information and suppress interference noises. Comprehensive experiments are carried out on three public datasets to evaluate the effectiveness of our method. Eventually, our Y-Net achieves state-of-the-art performance on PASCAL VOC 2012, PASCAL Person-Part, and ADE20K dataset without pre-training on extra datasets. © 2021 Association for Computing Machinery.",Channel-selective decoder; Dual-branch; Semantic enhancing module; U-shaped structure; Y-net,Channel coding; Computer vision; Decoding; Information use; Semantic Segmentation; Signal encoding; Channel-selective decoder; Dual-branch; Encoder-decoder; Joint network; Semantic enhancing module; Semantic segmentation; Shaped structures; U-shaped; U-shaped structure; Y-net; Semantics
Task-independent recognition of communication skills in group interaction using time-series modeling,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123285003&doi=10.1145%2f3450283&partnerID=40&md5=da1cdeb890bb37cce552f3287de29847,"Case studies of group discussions are considered an effective way to assess communication skills (CS). This method can help researchers evaluate participants' engagement with each other in a specific realistic context. In this article, multimodal analysis was performed to estimate CS indices using a three-task-type group discussion dataset, the MATRICS corpus. The current research investigated the effectiveness of engaging both static and time-series modeling, especially in task-independent settings. This investigation aimed to understand three main points: first, the effectiveness of time-series modeling compared to nonsequential modeling; second, multimodal analysis in a task-independent setting; and third, important differences to consider when dealing with task-dependent and task-independent settings, specifically in terms of modalities and prediction models. Several modalities were extracted (e.g., acoustics, speaking turns, linguistic-related movement, dialog tags, head motions, and face feature sets) for inferring the CS indices as a regression task. Three predictive models, including support vector regression (SVR), long short-term memory (LSTM), and an enhanced time-series model (an LSTM model with a combination of static and time-series features), were taken into account in this study. Our evaluation was conducted by using the R2 score in a cross-validation scheme. The experimental results suggested that time-series modeling can improve the performance of multimodal analysis significantly in the task-dependent setting (with the best R2 = 0.797 for the total CS index), with word2vec being the most prominent feature. Unfortunately, highly context-related features did not fit well with the task-independent setting. Thus, we propose an enhanced LSTM model for dealing with task-independent settings, and we successfully obtained better performance with the enhanced model than with the conventional SVR and LSTM models (the best R2 = 0.602 for the total CS index). In other words, our study shows that a particular time-series modeling can outperform traditional nonsequential modeling for automatically estimating the CS indices of a participant in a group discussion with regard to task dependency. © 2021 Association for Computing Machinery.",Communication skills; Group discussion; Multimodal analysis; Task independent; Time-series modeling,Long short-term memory; Support vector machines; Time series; Time series analysis; Communication skills; Group discussions; Group interaction; Memory modeling; Multimodal analysis; Nonsequential; Performance; Support vector regressions; Task independent; Times series models; Modal analysis
Dissimilarity-based regularized learning of charts,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123302865&doi=10.1145%2f3458884&partnerID=40&md5=f3d20f4e5c4ad0125188b0eee1c5080b,"Chart images exhibit significant variabilities that make each image different from others even though they belong to the same class or categories. Classification of charts is a major challenge because each chart class has variations in features, structure, and noises. However, due to the lack of affiliation between the dissimilar features and the structure of the chart, it is a challenging task to model these variations for automatic chart recognition. In this article, we present a novel dissimilarity-based learning model for similar structured but diverse chart classification. Our approach jointly learns the features of both dissimilar and similar regions. The model is trained by an improved loss function, which is fused by a structural variation-aware dissimilarity index and incorporated with regularization parameters, making the model more prone toward dissimilar regions. The dissimilarity index enhances the discriminative power of the learned features not only from dissimilar regions but also from similar regions. Extensive comparative evaluations demonstrate that our approach significantly outperforms other benchmark methods, including both traditional and deep learning models, over publicly available datasets. © 2021 Association for Computing Machinery.",Chart image classification; Deep learning; Dissimilarity index; Regularization,Deep learning; Automatic charts; Chart image classification; Deep learning; Dissimilarity index; Feature structure; Images classification; Learn+; Learning models; Loss functions; Regularisation; Image classification
Unsupervised domain expansion for visual categorization,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118351649&doi=10.1145%2f3448108&partnerID=40&md5=b90611948f19994302bf5bec349accd3,"Expanding visual categorization into a novel domain without the need of extra annotation has been a long-term interest for multimedia intelligence. Previously, this challenge has been approached by unsupervised domain adaptation (UDA). Given labeled data from a source domain and unlabeled data from a target domain, UDA seeks for a deep representation that is both discriminative and domain-invariant. While UDA focuses on the target domain, we argue that the performance on both source and target domains matters, as in practice which domain a test example comes from is unknown. In this article, we extend UDA by proposing a new task called unsupervised domain expansion (UDE), which aims to adapt a deep model for the target domain with its unlabeled data, meanwhile maintaining the model's performance on the source domain. We propose Knowledge Distillation Domain Expansion (KDDE) as a general method for the UDE task. Its domain-adaptation module can be instantiated with any existing model. We develop a knowledge distillation-based learning mechanism, enabling KDDE to optimize a single objective wherein the source and target domains are equally treated. Extensive experiments on two major benchmarks, i.e., Office-Home and DomainNet, show that KDDE compares favorably against four competitive baselines, i.e., DDC, DANN, DAAN, and CDAN, for both UDA and UDE tasks. Our study also reveals that the current UDA models improve their performance on the target domain at the cost of noticeable performance loss on the source domain. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Classifier generalization; Domain expansion; Visual categorization,Classifier generalization; Domain adaptation; Domain expansion; Generalisation; Long-term interests; Novel domain; Performance; Target domain; Unlabeled data; Visual categorization; Distillation
Cross-domain object representation via robust low-rank correlation analysis,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123277631&doi=10.1145%2f3458825&partnerID=40&md5=88cda2966721f0f785bb4fdcb877da8e,"Cross-domain data has become very popular recently since various viewpoints and different sensors tend to facilitate better data representation. In this article, we propose a novel cross-domain object representation algorithm (RLRCA) which not only explores the complexity of multiple relationships of variables by canonical correlation analysis (CCA) but also uses a low rank model to decrease the effect of noisy data. To the best of our knowledge, this is the first try to smoothly integrate CCA and a low-rank model to uncover correlated components across different domains and to suppress the effect of noisy or corrupted data. In order to improve the flexibility of the algorithm to address various cross-domain object representation problems, two instantiation methods of RLRCA are proposed from feature and sample space, respectively. In this way, a better cross-domain object representation can be achieved through effectively learning the intrinsic CCA features and taking full advantage of cross-domain object alignment information while pursuing low rank representations. Extensive experimental results on CMU PIE, Office-Caltech, Pascal VOC 2007, and NUS-WIDE-Object datasets, demonstrate that our designed models have superior performance over several state-of-the-art cross-domain low rank methods in image clustering and classification tasks with various corruption levels. © 2021 Association for Computing Machinery.",Correlation analysis; Cross-domain; Low-rank; Object representation,Classification (of information); Computer vision; Canonical correlations analysis; Correlated components; Correlation analysis; Cross-domain; Data representations; Low-rank; Noisy data; Object representations; Rank correlation; Rank modeling; Correlation methods
Residual-guided in-loop filter using convolution neural network,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123284680&doi=10.1145%2f3460820&partnerID=40&md5=72474648374821f7d12fd3ffb1898031,"The block-based coding structure in the hybrid video coding framework inevitably introduces compression artifacts such as blocking, ringing, and so on. To compensate for those artifacts, extensive filtering techniques were proposed in the loop of video codecs, which are capable of boosting the subjective and objective qualities of reconstructed videos. Recently, neural network-based filters were presented with the power of deep learning from a large magnitude of data. Though the coding efficiency has been improved from traditional methods in High-Efficiency Video Coding (HEVC), the rich features and information generated by the compression pipeline have not been fully utilized in the design of neural networks. Therefore, in this article, we propose the Residual-Reconstruction-based Convolutional Neural Network (RRNet) to further improve the coding efficiency to its full extent, where the compression features induced from bitstream in form of prediction residual are fed into the network as an additional input to the reconstructed frame. In essence, the residual signal can provide valuable information about block partitions and can aid reconstruction of edge and texture regions in a picture. Thus, more adaptive parameters can be trained to handle different texture characteristics. The experimental results show that our proposed RRNet approach presents significant BD-rate savings compared to HEVC and the state-of-the-art CNN-based schemes, indicating that residual signal plays a significant role in enhancing video frame reconstruction. © 2021 Association for Computing Machinery.",Convolutional neural network; High efficiency video coding; In-loop filter; Reconstruction; Residual,Convolution; Convolutional neural networks; Deep learning; Efficiency; Image coding; Textures; Video signal processing; Block based; Coding efficiency; Convolution neural network; Convolutional neural network; High-efficiency video coding; In-loop filters; Neural-networks; Reconstruction; Residual; Residual signals; Image reconstruction
Pedestrian-aware panoramic video stitching based on a structured camera array,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123321882&doi=10.1145%2f3460511&partnerID=40&md5=c3067a6a2300fb455b4a2c9f332394f2,"The panorama stitching system is an indispensable module in surveillance or space exploration. Such a system enables the viewer to understand the surroundings instantly by aligning the surrounding images on a plane and fusing them naturally. The bottleneck of existing systems mainly lies in alignment and naturalness of the transition of adjacent images. When facing dynamic foregrounds, they may produce outputs with misaligned semantic objects, which is evident and sensitive to human perception. We solve three key issues in the existing workflow that can affect its efficiency and the quality of the obtained panoramic video and present Pedestrian360, a panoramic video system based on a structured camera array (a spatial surround-view camera system). First, to get a geometrically aligned 360g view in the horizontal direction, we build a unified multi-camera coordinate system via a novel refinement approach that jointly optimizes camera poses. Second, to eliminate the brightness and color difference of images taken by different cameras, we design a photometric alignment approach by introducing a bias to the baseline linear adjustment model and solving it with two-step least-squares. Third, considering that the human visual system is more sensitive to high-level semantic objects, such as pedestrians and vehicles, we integrate the results of instance segmentation into the framework of dynamic programming in the seam-cutting step. To our knowledge, we are the first to introduce instance segmentation to the seam-cutting problem, which can ensure the integrity of the salient objects in a panorama. Specifically, in our surveillance oriented system, we choose the most significant target, pedestrians, as the seam avoidance target, and this accounts for the name Pedestrian360. To validate the effectiveness and efficiency of Pedestrian360, a large-scale dataset composed of videos with pedestrians in five scenes is established. The test results on this dataset demonstrate the superiority of Pedestrian360 compared to its competitors. Experimental results show that Pedestrian360 can stitch videos at a speed of 12 to 26 fps, which depends on the number of objects in the shooting scene and their frequencies of movements. To make our reported results reproducible, the relevant code and collected data are publicly available at https://cslinzhang.github.io/Pedestrian360-Homepage/. © 2021 Association for Computing Machinery.",Extrinsic calibration; Instance segmentation; Panoramic video stitching; Photometric alignment; Seam-cutting,Computer vision; Dynamic programming; Efficiency; Instance Segmentation; Large dataset; Photometry; Security systems; Semantic Segmentation; Semantics; Space research; Statistical tests; A-plane; Camera arrays; Existing systems; Extrinsic calibration; Panoramic video; Panoramic video stitching; Photometric alignment; Seam-cutting; Space explorations; Video stitching; Cameras
Cross-modal hybrid feature fusion for image-sentence matching,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123272823&doi=10.1145%2f3458281&partnerID=40&md5=eb7bf62b45cf0e02be5798c1345b423a,"Image-sentence matching is a challenging task in the field of language and vision, which aims at measuring the similarities between images and sentence descriptions. Most existing methods independently map the global features of images and sentences into a common space to calculate the image-sentence similarity. However, the image-sentence similarity obtained by these methods may be coarse as (1) an intermediate common space is introduced to implicitly match the heterogeneous features of images and sentences in a global level, and (2) only the inter-modality relations of images and sentences are captured while the intra-modality relations are ignored. To overcome the limitations, we propose a novel Cross-Modal Hybrid Feature Fusion (CMHF) framework for directly learning the image-sentence similarity by fusing multimodal features with inter- and intra-modality relations incorporated. It can robustly capture the high-level interactions between visual regions in images and words in sentences, where flexible attention mechanisms are utilized to generate effective attention flows within and across the modalities of images and sentences. A structured objective with ranking loss constraint is formed in CMHF to learn the image-sentence similarity based on the fused fine-grained features of different modalities bypassing the usage of intermediate common space. Extensive experiments and comprehensive analysis performed on two widely used datasets - Microsoft COCO and Flickr30K - show the effectiveness of the hybrid feature fusion framework in CMHF, in which the state-of-the-art matching performance is achieved by our proposed CMHF method. © 2021 Association for Computing Machinery.",Attention mechanism; Cross-modal retrieval; Image-sentence matching; Multimodal feature fusion,Attention mechanisms; Common spaces; Cross-modal; Cross-modal retrieval; Features fusions; Hybrid features; Image-sentence matching; Matchings; Multimodal feature fusions; Sentence similarity; Image fusion
Adaptive compression for online computer vision: An edge reinforcement learning approach,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123299943&doi=10.1145%2f3447878&partnerID=40&md5=10321d47af79b4e0198f5205a5c85a38,"With the growth of computer vision-based applications, an explosive amount of images have been uploaded to cloud servers that host such online computer vision algorithms, usually in the form of deep learning models. JPEG has been used as the de facto compression and encapsulation method for images. However, standard JPEG configuration does not always perform well for compressing images that are to be processed by a deep learning model - for example, the standard quality level of JPEG leads to 50% of size overhead (compared with the best quality level selection) on ImageNet under the same inference accuracy in popular computer vision models (e.g., InceptionNet and ResNet). Knowing this, designing a better JPEG configuration for online computer vision-based services is still extremely challenging. First, cloud-based computer vision models are usually a black box to end-users; thus, it is challenging to design JPEG configuration without knowing their model structures. Second, the ""optimal""JPEG configuration is not fixed; instead, it is determined by confounding factors, including the characteristics of the input images and the model, the expected accuracy and image size, and so forth. In this article, we propose a reinforcement learning (RL)-based adaptive JPEG configuration framework, AdaCompress. In particular, we design an edge (i.e., user-side) RL agent that learns the optimal compression quality level to achieve an expected inference accuracy and upload image size, only from the online inference results, without knowing details of the model structures. Furthermore, we design an explore-exploit mechanism to let the framework fast switch an agent when it detects a performance degradation, mainly due to the input change (e.g., images captured across daytime and night). Our evaluation experiments using real-world online computer vision-based APIs from Amazon Rekognition, Face++, and Baidu Vision show that our approach outperforms existing baselines by reducing the size of images by one-half to one-third while the overall classification accuracy only decreases slightly. Meanwhile, AdaCompress adaptively re-trains or re-loads the RL agent promptly to maintain the performance. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Adaptive compression; Edge computing; Machine learning service; Reinforcement learning,Computer vision; Deep learning; E-learning; Edge computing; Image compression; Model structures; Adaptive compression; Edge computing; Learning models; Machine learning service; Quality levels; Reinforcement learning agent; Reinforcement learning approach; Vision based; Vision model; Vision-based applications; Reinforcement learning
An adaptive bitrate switching algorithm for speech applications in context of webrtc,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123302272&doi=10.1145%2f3458751&partnerID=40&md5=55d91cfcbf5c3cbba30c4465ac85388e,"Web Real-Time Communication (WebRTC) combines a set of standards and technologies to enable high-quality audio, video, and auxiliary data exchange in web browsers and mobile applications. It enables peer-to-peer multimedia sessions over IP networks without the need for additional plugins. The Opus codec, which is deployed as the default audio codec for speech and music streaming in WebRTC, supports a wide range of bitrates. This range of bitrates covers narrowband, wideband, and super-wideband up to fullband bandwidths. Users of IP-based telephony always demand high-quality audio. In addition to users' expectation, their emotional state, content type, and many other psychological factors; network quality of service; and distortions introduced at the end terminals could determine their quality of experience. To measure the quality experienced by the end user for voice transmission service, the E-model standardized in the ITU-T Rec. G.107 (a narrowband version), ITU-T Rec. G.107.1 (a wideband version), and the most recent ITU-T Rec. G.107.2 extension for the super-wideband E-model can be used. In this work, we present a quality of experience model built on the E-model to measure the impact of coding and packet loss to assess the quality perceived by the end user in WebRTC speech applications. Based on the computed Mean Opinion Score, a real-time adaptive codec parameter switching mechanism is used to switch to the most optimum codec bitrate under the present network conditions. We present the evaluation results to show the effectiveness of the proposed approach when compared with the default codec configuration in WebRTC. © 2021 Association for Computing Machinery.",E-model; Opus codec; QoE; speech coding; WebRTC,Audio acoustics; Codes (symbols); Electronic data interchange; Internet protocols; Peer to peer networks; Speech communication; Voice/data communication systems; Web browsers; Bit rates; E-Model; Narrow bands; Opus codec; QoE; Real-time communication; Speech applications; Super-wideband; Web real-time communication; Wide-band; Quality of service
Using multisensory content to impact the quality of experience of reading digital books,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123273976&doi=10.1145%2f3458676&partnerID=40&md5=a7259dd63e2e01a54198d16fd0021144,"Multisensorial books enrich a story with either traditional multimedia content or sensorial effects. The main idea is to increase children's interest in reading by enhancing their QoE while reading. Studies on enriched and/or augmented e-books also propose synchronizing additional content with text. However, they usually focus on audio, video, images, or haptic feedback. In this work, we present MBook, a tool for presenting multisensorial books. It decouples the book's textual content from the additional content, as well as its synchronization, and rendering. Thus, a change in the additional content or its synchronization does not require changes to the book's content. To enable fine-grained synchronization, MBook captures the reading position using an eye-tracker. Experimental results with students within the 13- to 19-year-old age group point to MBook being able to provide good usability. © 2021 Association for Computing Machinery.",E-book; Mulsemedia; Multisensorial reading; Reading interest,Electronic document exchange; Electronic publishing; Eye tracking; Quality of service; Audio videos; Digital books; E-books; Haptic feedbacks; Mulsemedium; Multimedia contents; Multisensorial reading; Multisensory; Reading interest; Video image; Synchronization
Detecting non-aligned double jpeg compression based on amplitude-angle feature,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123272887&doi=10.1145%2f3464388&partnerID=40&md5=9515087f33c9184146e103c5c6e90c38,"Due to the popularity of JPEG format images in recent years, JPEG images will inevitably involve image editing operation. Thus, some tramped images will leave tracks of Non-aligned double JPEG (NA-DJPEG) compression. By detecting the presence of NA-DJPEG compression, one can verify whether a given JPEG image has been tampered with. However, only few methods can identify NA-DJPEG compressed images in the case that the primary quality factor is greater than the secondary quality factor. To address this challenging task, this article proposes a novel feature extraction scheme based optimized pixel difference (OPD), which is a new measure for blocking artifacts. Firstly, three color channels (RGB) of a reconstructed image generated by decompressing a given JPEG color image are mapped into spherical coordinates to calculate amplitude and two angles (azimuth and zenith). Then, 16 histograms of OPD along the horizontal and vertical directions are calculated in the amplitude and two angles, respectively. Finally, a set of features formed by arranging the bin values of these histograms is used for binary classification. Experiments demonstrate the effectiveness of the proposed method, and the results show that it significantly outperforms the existing typical methods in the mentioned task. © 2021 Association for Computing Machinery.",Blocking artifacts; Color image forensics; Non-aligned double JPEG compression; Spherical coordinates,Color; Feature extraction; Graphic methods; Image compression; Angle feature; Blocking artifacts; Color image forensic; Colour image; Double JPEG compressions; Image editing; Image forensics; JPEG image; Non-aligned double JPEG compression; Spherical coordinates; Digital forensics
Trust mechanism of feedback trust weight in multimedia network,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123301732&doi=10.1145%2f3391296&partnerID=40&md5=27023831de849d610c529af647ec8bb1,"It is necessary to solve the inaccurate data arising from data reliability ignored by most data fusion algorithms drawing upon collaborative filtering and fuzzy network theory. Therefore, a model is constructed based on the collaborative filtering algorithm and fuzzy network theory to calculate the node trust value as the weight of weighted data fusion. First, a FTWDF (Feedback Trust Weighted for Data Fusion) is proposed. Second, EEFA (Efficiency unequal Fuzzy clustering Algorithm) is introduced into FTWDF considering the defects of the clustering structure caused by ignoring the randomness of node energy consumption and cluster head selection in the practical application of the existing data fusion algorithm. Besides, the fuzzy logic is applied to cluster head selection and node clustering. Finally, an FTWDF-EEFA clustering algorithm is constructed for generating candidate cluster head nodes, which is verified by simulation experiments. The comparative analysis reveals that the accuracy of the FTWDF-EEFA clustering algorithm is 4.1% higher than that of the TMDF (Trust Multiple attributes Decision-making-based data Fusion) algorithm, and 8.3% higher than that of LDTS (Larger Data fusion based on node Trust evaluation in wireless Sensor networks) algorithm. It performs better in accuracy and recommendation results during the processing of ML100M dataset and NF5M dataset. Besides, the new clustering algorithm increases the survival time of nodes when analyzing the number of death nodes to prolong networks' lifespan. It improves the survival period of nodes, balances the network load, and prolongs networks' lifespan. Furthermore, the FTWDF-EEFA clustering algorithm can balance nodes' energy consumption and effectively save nodes' overall energy through analysis. Therefore, the optimized algorithm can increase the lifespan of network and improve the trust mechanism effectively. The performance of the algorithm has reached the expected effect, providing a reference for the practical application of the trust mechanism in networks. © 2021 Association for Computing Machinery.",Collaborative filtering algorithm; FTWDF; FTWDF-EEFA; Network fuzzy theory; Simulation; Trust mechanism,Clustering algorithms; Collaborative filtering; Decision making; Decision theory; Energy utilization; Feedback; Fuzzy clustering; Fuzzy filters; Fuzzy set theory; Reliability theory; Sensor nodes; Signal filtering and prediction; Collaborative filtering algorithms; Data fusion algorithm; Feedback trust weighted for data fusion; Feedback trust weighted for data fusion-EEFA; Fuzzy networks; Fuzzy theory; Lifespans; Network fuzzy theory; Simulation; Trust mechanism; Fuzzy logic
Fine-grained visual textual alignment for cross-modal retrieval using transformer encoders,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123302427&doi=10.1145%2f3451390&partnerID=40&md5=ecdec63bc4da86824c10e0b85a1390c6,"Despite the evolution of deep-learning-based visual-textual processing systems, precise multi-modal matching remains a challenging task. In this work, we tackle the task of cross-modal retrieval through image-sentence matching based on word-region alignments, using supervision only at the global image-sentence level. Specifically, we present a novel approach called Transformer Encoder Reasoning and Alignment Network (TERAN). TERAN enforces a fine-grained match between the underlying components of images and sentences (i.e., image regions and words, respectively) to preserve the informative richness of both modalities. TERAN obtains state-of-the-art results on the image retrieval task on both MS-COCO and Flickr30k datasets. Moreover, on MS-COCO, it also outperforms current approaches on the sentence retrieval task.Focusing on scalable cross-modal information retrieval, TERAN is designed to keep the visual and textual data pipelines well separated. Cross-attention links invalidate any chance to separately extract visual and textual features needed for the online search and the offline indexing steps in large-scale retrieval systems. In this respect, TERAN merges the information from the two domains only during the final alignment phase, immediately before the loss computation. We argue that the fine-grained alignments produced by TERAN pave the way toward the research for effective and efficient methods for large-scale cross-modal information retrieval. We compare the effectiveness of our approach against relevant state-of-the-art methods. On the MS-COCO 1K test set, we obtain an improvement of 5.7% and 3.5% respectively on the image and the sentence retrieval tasks on the Recall@1 metric. The code used for the experiments is publicly available on GitHub at https://github.com/mesnico/TERAN. © 2021 Association for Computing Machinery.",Computer vision; Cross-modal retrieval; Deep learning; Multi-modal matching; Natural language processing,Computer vision; Deep learning; Image enhancement; Learning algorithms; Natural language processing systems; Online systems; Search engines; Cross-modal; Cross-modal retrieval; Deep learning; Fine grained; Large-scales; Matchings; Modal matching; Multi-modal; Multi-modal matching; Processing systems; Signal encoding
Dual-stream guided-learning via a priori optimization for person re-identification,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123299445&doi=10.1145%2f3447715&partnerID=40&md5=66edb2ce6487c35dcc4e93c2a9bfe709,"The task of person re-identification (re-ID) is to find the same pedestrian across non-overlapping camera views. Generally, the performance of person re-ID can be affected by background clutter. However, existing segmentation algorithms cannot obtain perfect foreground masks to cover the background information clearly. In addition, if the background is completely removed, some discriminative ID-related cues (i.e., backpack or companion) may be lost. In this article, we design a dual-stream network consisting of a Provider Stream (P-Stream) and a Receiver Stream (R-Stream). The R-Stream performs an a priori optimization operation on foreground information. The P-Stream acts as a pusher to guide the R-Stream to concentrate on foreground information and some useful ID-related cues in the background. The proposed dual-stream network can make full use of the a priori optimization and guided-learning strategy to learn encouraging foreground information and some useful ID-related information in the background. Our method achieves Rank-1 accuracy of 95.4% on Market-1501, 89.0% on DukeMTMC-reID, 78.9% on CUHK03 (labeled), and 75.4% on CUHK03 (detected), outperforming state-of-the-art methods. © 2022 Association for Computing Machinery.",A priori optimization; Foreground images; Guided-learning; Person re-identification,A priori optimization; Background clutter; Foreground images; Foreground information; Guided-learning; Non-overlapping camera views; Optimisations; Performance; Person re identifications; Stream networks; Image segmentation
Bi-directional co-attention network for image captioning,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123299562&doi=10.1145%2f3460474&partnerID=40&md5=7a31a09bf269a7bdef484d2bc7464a4f,"Image Captioning, which automatically describes an image with natural language, is regarded as a fundamental challenge in computer vision. In recent years, significant advance has been made in image captioning through improving attention mechanism. However, most existing methods construct attention mechanisms based on singular visual features, such as patch features or object features, which limits the accuracy of generated captions. In this article, we propose a Bidirectional Co-Attention Network (BCAN) that combines multiple visual features to provide information from different aspects. Different features are associated with predicting different words, and there are a priori relations between these multiple visual features. Based on this, we further propose a bottom-up and top-down bi-directional co-attention mechanism to extract discriminative attention information. Furthermore, most existing methods do not exploit an effective multimodal integration strategy, generally using addition or concatenation to combine features. To solve this problem, we adopt the Multivariate Residual Module (MRM) to integrate multimodal attention features. Meanwhile, we further propose a Vertical MRM to integrate features of the same category, and a Horizontal MRM to combine features of the different categories, which can balance the contribution of the bottom-up co-attention and the top-down co-attention. In contrast to the existing methods, the BCAN is able to obtain complementary information from multiple visual features via the bi-directional co-attention strategy, and integrate multimodal information via the improved multivariate residual strategy. We conduct a series of experiments on two benchmark datasets (MSCOCO and Flickr30k), and the results indicate that the proposed BCAN achieves the superior performance. © 2021 Association for Computing Machinery.",Attention mechanism; Bi-directional co-attention mechanism; Image captioning; Multivariate residual strategy,Benchmarking; Attention mechanisms; Bi-directional; Bi-directional co-attention mechanism; Bottom-up and top-down; Image captioning; Mechanism-based; Multivariate residual strategy; Natural languages; Visual feature; Image enhancement
Dual-stream structured graph convolution network for skeleton-based action recognition,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123316823&doi=10.1145%2f3450410&partnerID=40&md5=b0f94d787a8a2824a23657234caf7df2,"In this work, we propose a dual-stream structured graph convolution network (DS-SGCN) to solve the skeleton-based action recognition problem. The spatio-temporal coordinates and appearance contexts of the skeletal joints are jointly integrated into the graph convolution learning process on both the video and skeleton modalities. To effectively represent the skeletal graph of discrete joints, we create a structured graph convolution module specifically designed to encode partitioned body parts along with their dynamic interactions in the spatio-temporal sequence. In more detail, we build a set of structured intra-part graphs, each of which can be adopted to represent a distinctive body part (e.g., left arm, right leg, head). The inter-part graph is then constructed to model the dynamic interactions across different body parts; here each node corresponds to an intra-part graph built above, while an edge between two nodes is used to express these internal relationships of human movement. We implement the graph convolution learning on both intra- and inter-part graphs in order to obtain the inherent characteristics and dynamic interactions, respectively, of human action. After integrating the intra- and inter-levels of spatial context/coordinate cues, a convolution filtering process is conducted on time slices to capture these temporal dynamics of human motion. Finally, we fuse two streams of graph convolution responses in order to predict the category information of human action in an end-to-end fashion. Comprehensive experiments on five single/multi-modal benchmark datasets (including NTU RGB+D 60, NTU RGB+D 120, MSR-Daily 3D, N-UCLA, and HDM05) demonstrate that the proposed DS-SGCN framework achieves encouraging performance on the skeleton-based action recognition task. © 2021 Association for Computing Machinery.",Action recognition; Dual-stream structured graph convolution; Graph convolution network,Benchmarking; Musculoskeletal system; Action recognition; Body parts; Dual-stream structured graph convolution; Dynamic interaction; Graph convolution network; Human actions; Part graph; Skeletal joints; Spatio-temporal; Structured graphs; Convolution
Where are they going? Predicting human behaviors in crowded scenes,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123275652&doi=10.1145%2f3449359&partnerID=40&md5=b6d76d78cbb68ad1fc3f3e9056a9c5af,"In this article, we propose a framework for crowd behavior prediction in complicated scenarios. The fundamental framework is designed using the standard encoder-decoder scheme, which is built upon the long short-term memory module to capture the temporal evolution of crowd behaviors. To model interactions among humans and environments, we embed both the social and the physical attention mechanisms into the long short-term memory. The social attention component can model the interactions among different pedestrians, whereas the physical attention component helps to understand the spatial configurations of the scene. Since pedestrians' behaviors demonstrate multi-modal properties, we use the generative model to produce multiple acceptable future paths. The proposed framework not only predicts an individual's trajectory accurately but also forecasts the ongoing group behaviors by leveraging on the coherent filtering approach. Experiments are carried out on the standard crowd benchmarks (namely, the ETH, the UCY, the CUHK crowd, and the CrowdFlow datasets), which demonstrate that the proposed framework is effective in forecasting crowd behaviors in complex scenarios. © 2021 Association for Computing Machinery.",Attention mechanism; Behavior prediction; Crowd analysis; Multi-modality modeling; Pedestrian grouping,Behavioral research; Brain; Long short-term memory; Attention mechanisms; Behavior prediction; Crowd analysis; Crowd behavior; Encoder-decoder; Human behaviors; Memory modules; Multi-modality; Multi-modality modeling; Pedestrian grouping; Forecasting
Health status prediction with local-global heterogeneous behavior graph,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123275531&doi=10.1145%2f3457893&partnerID=40&md5=974acc3cd1eaa89fa5a1afe30d2273f9,"Health management is getting increasing attention all over the world. However, existing health management mainly relies on hospital examination and treatment, which are complicated and untimely. The emergence of mobile devices provides the possibility to manage people's health status in a convenient and instant way. Estimation of health status can be achieved with various kinds of data streams continuously collected from wearable sensors. However, these data streams are multi-source and heterogeneous, containing complex temporal structures with local contextual and global temporal aspects, which makes the feature learning and data joint utilization challenging. We propose to model the behavior-related multi-source data streams with a local-global graph, which contains multiple local context sub-graphs to learn short-term local context information with heterogeneous graph neural networks and a global temporal sub-graph to learn long-term dependency with self-attention networks. Then health status is predicted based on the structure-aware representation learned from the local-global behavior graph. We take experiments on the StudentLife dataset, and extensive results demonstrate the effectiveness of our proposed model. © 2021 Association for Computing Machinery.",Graph neural networks; Health status prediction; Individual behavior,Health; Behavior graphs; Data stream; Graph neural networks; Health management; Health status; Health status prediction; Individual behavior; Learn+; Local contexts; Subgraphs; Graph neural networks
Bayesian covariance representation with global informative prior for 3d action recognition,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123305674&doi=10.1145%2f3460235&partnerID=40&md5=c9cf00b55d8807835976b2579b1a0510,"For the merits of high-order statistics and Riemannian geometry, covariance matrix has become a generic feature representation for action recognition. An independent action can be represented by an empirical statistics over all of its pose samples. Two major problems of covariance include the following: (1) it is prone to be singular so that actions fail to be represented properly, and (2) it is short of global action/pose-aware information so that expressive and discriminative power is limited. In this article, we propose a novel Bayesian covariance representation by a prior regularization method to solve the preceding problems. Specifically, covariance is viewed as a parametric maximum likelihood estimate of Gaussian distribution over local poses from an independent action. Then, a Global Informative Prior (GIP) is generated over global poses with sufficient statistics to regularize covariance. In this way, (1) singularity is greatly relieved due to sufficient statistics, (2) global pose information of GIP makes Bayesian covariance theoretically equivalent to a saliency weighting covariance over global action poses so that discriminative characteristics of actions can be represented more clearly. Experimental results show that our Bayesian covariance with GIP efficiently improves the performance of action recognition. In some databases, it outperforms the state-of-the-art variant methods that are based on kernels, temporal-order structures, and saliency weighting attentions, among others. © 2021 Association for Computing Machinery.",3D action recognition; Bayesian regularization; Covariance matrix; Riemannian manifold,Geometry; Maximum likelihood estimation; 3d action recognition; Action recognition; Bayesian; Bayesian regularization; Covariance matrices; Global actions; Independent action; Informative Priors; Riemannian manifold; Sufficient statistics; Covariance matrix
Smart director: An event-driven directing system for live broadcasting,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123307207&doi=10.1145%2f3448981&partnerID=40&md5=89da4723284ce1cff4a4f68db8b11c41,"Live video broadcasting normally requires a multitude of skills and expertise with domain knowledge to enable multi-camera productions. As the number of cameras keeps increasing, directing a live sports broadcast has now become more complicated and challenging than ever before. The broadcast directors need to be much more concentrated, responsive, and knowledgeable, during the production. To relieve the directors from their intensive efforts, we develop an innovative automated sports broadcast directing system, called Smart Director, which aims at mimicking the typical human-in-the-loop broadcasting process to automatically create near-professional broadcasting programs in real-time by using a set of advanced multi-view video analysis algorithms. Inspired by the so-called ""three-event""construction of sports broadcast [14], we build our system with an event-driven pipeline consisting of three consecutive novel components: (1) the Multi-View Event Localization to detect events by modeling multi-view correlations, (2) the Multi-View Highlight Detection to rank camera views by the visual importance for view selection, and (3) the Auto-Broadcasting Scheduler to control the production of broadcasting videos. To our best knowledge, our system is the first end-to-end automated directing system for multi-camera sports broadcasting, completely driven by the semantic understanding of sports events. It is also the first system to solve the novel problem of multi-view joint event detection by cross-view relation modeling. We conduct both objective and subjective evaluations on a real-world multi-camera soccer dataset, which demonstrate the quality of our auto-generated videos is comparable to that of the human-directed videos. Thanks to its faster response, our system is able to capture more fast-passing and short-duration events which are usually missed by human directors. © 2021 Association for Computing Machinery.",Highlight detection; Multi-view event detection; Sports-broadcast directing,Cameras; Domain Knowledge; Semantics; Sports; Domain knowledge; Event-driven; Events detection; Highlights detection; Live video; Multi-cameras; Multi-view event detection; Multi-views; Sport-broadcast directing; Video broadcasting; Quality control
Perceptual quality assessment of low-light image enhancement,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123290374&doi=10.1145%2f3457905&partnerID=40&md5=66ee0702457877cffba19ba6632cf5e1,"Low-light image enhancement algorithms (LIEA) can light up images captured in dark or back-lighting conditions. However, LIEA may introduce various distortions such as structure damage, color shift, and noise into the enhanced images. Despite various LIEAs proposed in the literature, few efforts have been made to study the quality evaluation of low-light enhancement. In this article, we make one of the first attempts to investigate the quality assessment problem of low-light image enhancement. To facilitate the study of objective image quality assessment (IQA), we first build a large-scale low-light image enhancement quality (LIEQ) database. The LIEQ database includes 1,000 light-enhanced images, which are generated from 100 low-light images using 10 LIEAs. Rather than evaluating the quality of light-enhanced images directly, which is more difficult, we propose to use the multi-exposure fused (MEF) image and stack-based high dynamic range (HDR) image as a reference and evaluate the quality of low-light enhancement following a full-reference (FR) quality assessment routine. We observe that distortions introduced in low-light enhancement are significantly different from distortions considered in traditional image IQA databases that are well-studied, and the current state-of-the-art FR IQA models are also not suitable for evaluating their quality. Therefore, we propose a new FR low-light image enhancement quality assessment (LIEQA) index by evaluating the image quality from four aspects: luminance enhancement, color rendition, noise evaluation, and structure preserving, which have captured the most key aspects of low-light enhancement. Experimental results on the LIEQ database show that the proposed LIEQA index outperforms the state-of-the-art FR IQA models. LIEQA can act as an evaluator for various low-light enhancement algorithms and systems. To the best of our knowledge, this article is the first of its kind comprehensive low-light image enhancement quality assessment study. © 2021 Association for Computing Machinery.",Light-enhanced image; Low-light image enhancement; Quality assessment; Structure similarity,Database systems; Image quality; Quality control; Full references; Image enhancement algorithm; Image quality assessment; Light enhancement; Light-enhanced image; Low light; Low-light image enhancement; Low-light images; Quality assessment; Structure similarity; Image enhancement
A Fast view synthesis implementation method for light field applications,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123297617&doi=10.1145%2f3459098&partnerID=40&md5=ed9f0670c9b2d7cb5dea76d163876ff9,"View synthesis (VS) for light field images is a very time-consuming task due to the great quantity of involved pixels and intensive computations, which may prevent it from the practical three-dimensional real-time systems. In this article, we propose an acceleration approach for deep learning-based light field view synthesis, which can significantly reduce calculations by using compact-resolution (CR) representation and super-resolution (SR) techniques, as well as light-weight neural networks. The proposed architecture has three cascaded neural networks, including a CR network to generate the compact representation for original input views, a VS network to synthesize new views from down-scaled compact views, and a SR network to reconstruct high-quality views with full resolution. All these networks are jointly trained with the integrated losses of CR, VS, and SR networks. Moreover, due to the redundancy of deep neural networks, we use the efficient light-weight strategy to prune filters for simplification and inference acceleration. Experimental results demonstrate that the proposed method can greatly reduce the processing time and become much more computationally efficient with competitive image quality. © 2021 Association for Computing Machinery.",Compact representation; Deep learning-based view synthesis; Deep neural network compression; Filter pruning; Light field systems; Real-time acceleration,Interactive computer systems; Real time systems; Compact representation; Deep learning-based view synthesis; Deep neural network compression; Filter pruning; Light field system; Light fields; Network compression; Real- time; Real-time acceleration; View synthesis; Deep neural networks
Uncertainty-aware semi-supervised method using large unlabeled and limited labeled COVID-19 data,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122317111&doi=10.1145%2f3462635&partnerID=40&md5=747ed633be3ec98f4f0a9c7c43891974,"The new coronavirus has caused more than one million deaths and continues to spread rapidly. This virus targets the lungs, causing respiratory distress which can be mild or severe. The X-ray or computed tomography (CT) images of lungs can reveal whether the patient is infected with COVID-19 or not. Many researchers are trying to improve COVID-19 detection using artificial intelligence. Our motivation is to develop an automatic method that can cope with scenarios in which preparing labeled data is time consuming or expensive. In this article, we propose a Semi-supervised Classification using Limited Labeled Data (SCLLD) relying on Sobel edge detection and Generative Adversarial Networks (GANs) to automate the COVID-19 diagnosis. The GAN discriminator output is a probabilistic value which is used for classification in this work. The proposed system is trained using 10,000 CT scans collected from Omid Hospital, whereas a public dataset is also used for validating our system. The proposed method is compared with other state-of-the-art supervised methods such as Gaussian processes. To the best of our knowledge, this is the first time a semi-supervised method for COVID-19 detection is presented. Our system is capable of learning from a mixture of limited labeled and unlabeled data where supervised learners fail due to a lack of sufficient amount of labeled data. Thus, our semi-supervised training method significantly outperforms the supervised training of Convolutional Neural Network (CNN) when labeled training data is scarce. The 95% confidence intervals for our method in terms of accuracy, sensitivity, and specificity are 99.56 ± 0.20%, 99.88 ± 0.24%, and 99.40 ± 0.18%, respectively, whereas intervals for the CNN (trained supervised) are 68.34 ± 4.11%, 91.2 ± 6.15%, and 46.40 ± 5.21%.  © 2021 Association for Computing Machinery.",COVID-19; Deep learning; Generative adversarial networks; Semi-supervised learning; Supervised learning,Computer aided diagnosis; Computerized tomography; Convolutional neural networks; Deep learning; Supervised learning; Viruses; Automatic method; Computed tomography images; Convolutional neural network; Coronaviruses; COVID-19; Deep learning; Labeled data; Semi-supervised method; Semisupervised classification (SSC); Uncertainty; Generative adversarial networks
Introduction to the special issue on explainable ai on multimedia computing,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122626889&doi=10.1145%2f3489522&partnerID=40&md5=0e47b1ff8a321da0de2ba52d45db9c95,[No abstract available],,
XCos: An explainable cosine metric for face verification task,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122642619&doi=10.1145%2f3469288&partnerID=40&md5=19ab144bbf0e7cfb624466dbd2aa64b0,"We study the XAI (explainable AI) on the face recognition task, particularly the face verification. Face verification has become a crucial task in recent days and it has been deployed to plenty of applications, such as access control, surveillance, and automatic personal log-on for mobile devices. With the increasing amount of data, deep convolutional neural networks can achieve very high accuracy for the face verification task. Beyond exceptional performances, deep face verification models need more interpretability so that we can trust the results they generate. In this article, we propose a novel similarity metric, called explainable cosine (xCos), that comes with a learnable module that can be plugged into most of the verification models to provide meaningful explanations. With the help of xCos, we can see which parts of the two input faces are similar, where the model pays its attention to, and how the local similarities are weighted to form the output xCos score. We demonstrate the effectiveness of our proposed method on LFW and various competitive benchmarks, not only resulting in providing novel and desirable model interpretability for face verification but also ensuring the accuracy as plugging into existing face recognition models.  © 2021 Association for Computing Machinery.",Explainable AI; Explainable artificial intelligence; Face recognition; Face verification; XAI; XCos,Access control; Convolutional neural networks; Deep neural networks; Cosine metric; Explainable AI; Explainable artificial intelligence; Face Verification; High-accuracy; Interpretability; Verification model; Verification task; XAI; Xcos; Face recognition
Local constraint and label embedding multi-layer dictionary learning for sperm head classification,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122621483&doi=10.1145%2f3458927&partnerID=40&md5=20fd839214113ef6d78c03db51681473,"Morphological classification of human sperm heads is a key technology for diagnosing male infertility. Due to its sparse representation and learning capability, dictionary learning has shown remarkable performance in human sperm head classification. To promote the discriminability of the classification model, a novel local constraint and label embedding multi-layer dictionary learning model called LCLM-MDL is proposed in this study. Based on the multi-layer dictionary learning framework, two dictionaries are built on the basis of Laplacian regularized constraint and label embedding term in each layer, and the two dictionaries are approximated to each other as much as possible, so as to well exploit the nonlinear structure and discriminability features of the morphology of human sperm heads. In addition, to promote the robustness of the model, the asymmetric Huber loss is adopted in the last layer of LCLM-MDL, which approximates the misclassification error by using the absolute error function. Finally, the experimental results on HuSHeM dataset demonstrate the validity of the LCLM-MDL.  © 2021 Association for Computing Machinery.",Dictionary learning; Human sperm head image; Morphological classification; Multi-layer framework,Image classification; Dictionary learning; Discriminability; Embeddings; Human sperm; Human sperm head image; Key technologies; Local constraints; Morphological classifications; Multi-layer framework; Multi-layers; Embeddings
Semantic explanation for deep neural networks using feature interactions,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122641342&doi=10.1145%2f3474557&partnerID=40&md5=9eeea64f280f22eebd074d566d997da6,"Given the promising results obtained by deep-learning techniques in multimedia analysis, the explainability of predictions made by networks has become important in practical applications. We present a method to generate semantic and quantitative explanations that are easily interpretable by humans. The previous work to obtain such explanations has focused on the contributions of each feature, taking their sum to be the prediction result for a target variable; the lack of discriminative power due to this simple additive formulation led to low explanatory performance. Our method considers not only individual features but also their interactions, for a more detailed interpretation of the decisions made by networks. The algorithm is based on the factorization machine, a prediction method that calculates factor vectors for each feature. We conducted experiments on multiple datasets with different models to validate our method, achieving higher performance than the previous work. We show that including interactions not only generates explanations but also makes them richer and is able to convey more information. We show examples of produced explanations in a simple visual format and verify that they are easily interpretable and plausible.  © 2021 Copyright held by the owner/author(s).",Explainability; Multimedia analysis; Semantics,Deep neural networks; Forecasting; Semantic Web; Discriminative power; Explainability; Factorization machines; Feature interactions; Individual features; Learning techniques; Multi-media analysis; Performance; Prediction methods; Simple++; Semantics
Explainable AI: A multispectral palm-vein identification system with new augmentation features,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122624768&doi=10.1145%2f3468873&partnerID=40&md5=c5c16b867309059348ede4777413458d,"Recently, as one of the most promising biometric traits, the vein has attracted the attention of both academia and industry because of its living body identification and the convenience of the acquisition process. State-of-the-art techniques can provide relatively good performance, yet they are limited to specific light sources. Besides, it still has poor adaptability to multispectral images. Despite the great success achieved by convolutional neural networks (CNNs) in various image understanding tasks, they often require large training samples and high computation that are infeasible for palm-vein identification. To address this limitation, this work proposes a palm-vein identification system based on lightweight CNN and adaptive multi-spectral method with explainable AI. The principal component analysis on symmetric discrete wavelet transform (SMDWT-PCA) technique for vein images augmentation method is adopted to solve the problem of insufficient data and multispectral adaptability. The depth separable convolution (DSC) has been applied to reduce the number of model parameters in this work. To ensure that the experimental result demonstrates accurately and robustly, a multispectral palm image of the public dataset (CASIA) is also used to assess the performance of the proposed method. As result, the palm-vein identification system can provide superior performance to that of the former related approaches for different spectrums.  © 2021 Association for Computing Machinery.",Convolutional neural networks; Explainable AI; Palm-vein identification; Principal component analysis on symmetric discrete wavelet transform,Convolution; Convolutional neural networks; Discrete wavelet transforms; Light sources; Palmprint recognition; Signal reconstruction; Convolutional neural network; Discrete-wavelet-transform; Explainable AI; Multi-spectral; Palm Vein; Palm-vein identification; Performance; Principal component analyse on symmetric discrete wavelet transform; Principal-component analysis; Symmetrics; Principal component analysis
Introduction to the special issue on explainable deep learning for medical image computing,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122624530&doi=10.1145%2f3485046&partnerID=40&md5=bc97bf57f640d0ccda728281517511b7,[No abstract available],,
An explainable framework for diagnosis of COVID-19 pneumonia via transfer learning and discriminant correlation analysis,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122619581&doi=10.1145%2f3449785&partnerID=40&md5=72dcd76fb29dad19928b6a2a79ddc2ca,"The new coronavirus COVID-19 has been spreading all over the world in the last six months, and the death toll is still rising. The accurate diagnosis of COVID-19 is an emergent task as to stop the spreading of the virus. In this paper, we proposed to leverage image feature fusion for the diagnosis of COVID-19 in lung window computed tomography (CT). Initially, ResNet-18 and ResNet-50 were selected as the backbone deep networks to generate corresponding image representations from the CT images. Second, the representative information extracted from the two networks was fused by discriminant correlation analysis to obtain refined image features. Third, three randomized neural networks (RNNs): extreme learning machine, Schmidt neural network and random vector functional-link net, were trained using the refined features, and the predictions of the three RNNs were ensembled to get a more robust classification performance. Experiment results based on five-fold cross validation suggested that our method outperformed state-of-the-art algorithms in the diagnosis of COVID-19.  © 2021 Association for Computing Machinery.",Computed tomography; COVID-19; Extreme learning machine; Random vector functional-link net; Randomized neural network; ResNet; Schmidt neural network,Correlation methods; Image representation; Knowledge acquisition; Machine learning; Viruses; COVID-19; Functional-link net; Neural-networks; Random vector functional-link net; Random vectors; Randomized neural network; Resnet; Schmidt; Schmidt neural network; Computerized tomography
An explainable deep learning ensemble model for robust diagnosis of diabetic retinopathy grading,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122078299&doi=10.1145%2f3469841&partnerID=40&md5=8b32e8096812829db91b2121069d5ed4,"Diabetic retinopathy (DR) is one of the most common causes of vision loss in people who have diabetes for a prolonged period. Convolutional neural networks (CNNs) have become increasingly popular for computer-aided DR diagnosis using retinal fundus images. While these CNNs are highly reliable, their lack of sufficient explainability prevents them from being widely used in medical practice. In this article, we propose a novel explainable deep learning ensemble model where weights from different models are fused into a single model to extract salient features from various retinal lesions found on fundus images. The extracted features are then fed to a custom classifier for the final diagnosis of DR severity level. The model is trained on an APTOS dataset containing retinal fundus images of various DR grades using a cyclical learning rates strategy with an automatic learning rate finder for decaying the learning rate to improve model accuracy. We develop an explainability approach by leveraging gradient-weighted class activation mapping and shapely adaptive explanations to highlight the areas of fundus images that are most indicative of different DR stages. This allows ophthalmologists to view our model's decision in a way that they can understand. Evaluation results using three different datasets (APTOS, MESSIDOR, IDRiD) show the effectiveness of our model, achieving superior classification rates with a high degree of precision (0.970), sensitivity (0.980), and AUC (0.978). We believe that the proposed model, which jointly offers state-of-the-art diagnosis performance and explainability, will address the black-box nature of deep CNN models in robust detection of DR grading.  © 2021 Association for Computing Machinery.",Diabetic retinopathy diagnosis; Ensemble model; Explainable deep CNN; Retinal fundus images; Transfer learning,Classification (of information); Computer aided diagnosis; Convolutional neural networks; Deep learning; Grading; Image enhancement; Learning algorithms; Ophthalmology; Transfer learning; Convolutional neural network; Diabetic retinopathy; Diabetic retinopathy diagnose; Diabetic retinopathy grading; Ensemble models; Explainable deep convolutional neural network; Fundus image; Learning rates; Retinal fundus images; Transfer learning; Eye protection
Leveraging deep statistics for underwater image enhancement,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122621373&doi=10.1145%2f3489520&partnerID=40&md5=2866ebeafb1b105a61bc301fbd518e90,"Underwater imaging often suffers from color cast and contrast degradation due to range-dependent medium absorption and light scattering. Introducing image statistics as prior has been proved to be an effective solution for underwater image enhancement. However, relative to the modal divergence of light propagation and underwater scenery, the existing methods are limited in representing the inherent statistics of underwater images resulting in color artifacts and haze residuals. To address this problem, this article proposes a convolutional neural network (CNN)-based framework to learn hierarchical statistical features related to color cast and contrast degradation and to leverage them for underwater image enhancement. Specifically, a pixel disruption strategy is first proposed to suppress intrinsic colors' influence and facilitate modeling a unified statistical representation of underwater image. Then, considering the local variation of depth of field, two parallel sub-networks: Color Correction Network (CC-Net) and Contrast Enhancement Network (CE-Net) are presented. The CC-Net and CE-Net can generate pixel-wise color cast and transmission map and achieve spatial-varied color correction and contrast enhancement. Moreover, to address the issue of insufficient training data, an imaging model-based synthesis method that incorporates pixel disruption strategy is presented to generate underwater patches with global degradation consistency. Quantitative and subjective evaluations demonstrate that our proposed method achieves state-of-the-art performance.  © 2021 Association for Computing Machinery.",Pixel disruption strategy; Two-branch architecture; Underwater image enhancement,Color; Color image processing; Image enhancement; Light scattering; Statistics; Color artifact; Color cast; Colour contrast; Colour corrections; Contrast Enhancement; Effective solution; Image statistics; Pixel disruption strategy; Two-branch architecture; Underwater image enhancements; Pixels
Learning to fool the speaker recognition,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122623326&doi=10.1145%2f3468673&partnerID=40&md5=0d5a55d73041da4f12994ff18116fb4b,"Due to the widespread deployment of fingerprint/face/speaker recognition systems, the risk in these systems, especially the adversarial attack, has drawn increasing attention in recent years. Previous researches mainly studied the adversarial attack to the vision-based systems, such as fingerprint and face recognition. While the attack for speech-based systems has not been well studied yet, although it has been widely used in our daily life. In this article, we attempt to fool the state-of-the-art speaker recognition model and present speaker recognition attacker, a lightweight multi-layer convolutional neural network to fool the well-trained state-of-the-art speaker recognition model by adding imperceptible perturbations onto the raw speech waveform. We find that the speaker recognition system is vulnerable to the adversarial attack, and achieve a high success rate on both the non-targeted attack and targeted attack. Besides, we present an effective method by leveraging a pretrained phoneme recognition model to optimize the speaker recognition attacker to obtain a tradeoff between the attack success rate and the perceptual quality. Experimental results on the TIMIT and LibriSpeech datasets demonstrate the effectiveness and efficiency of our proposed model. And the experiments for frequency analysis indicate that high-frequency attack is more effective than low-frequency attack, which is different from the conclusion drawn in previous image-based works. Additionally, the ablation study gives more insights into our model.  © 2021 Association for Computing Machinery.",Adversarial attack; Audio forensics; Deep neural network,Convolutional neural networks; Deep neural networks; Digital forensics; Multilayer neural networks; Network layers; Speech recognition; Adversarial attack; Audio forensics; Daily lives; Multi-layers; Recognition models; Speaker recognition; Speaker recognition system; Speech-based systems; State of the art; Vision-based system; Face recognition
Hypomimia recognition in Parkinson's disease with semantic features,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122629952&doi=10.1145%2f3476778&partnerID=40&md5=8d26f35fc9f42dd72196582a44ac3886,"Parkinson's disease is the second most common neurodegenerative disorder, commonly affecting elderly people over the age of 65. As the cardinal manifestation, hypomimia, referred to as impairments in normal facial expressions, stays covert. Even some experienced doctors may miss these subtle changes, especially in a mild stage of this disease. The existing methods for hypomimia recognition are mainly dominated by statistical variable-based methods with the help of traditional machine learning algorithms. Despite the success of recognizing hypomimia, they show a limited accuracy and lack the capability of performing semantic analysis. Therefore, developing a computer-aided diagnostic method for semantically recognizing hypomimia is appealing. In this article, we propose a Semantic Feature based Hypomimia Recognition network, named SFHR-NET, to recognize hypomimia based on facial videos. First, a Semantic Feature Classifier (SF-C) is proposed to adaptively adjust feature maps salient to hypomimia, which leads the encoder and classifier to focus more on areas of hypomimia-interest. In SF-C, the progressive confidence strategy (PCS) ensures more reliable semantic features. Then, a two-stream framework is introduced to fuse the spatial data stream and temporal optical stream, which allows the encoder to semantically and progressively characterize the rigid process of hypomimia. Finally, to improve the interpretability of the model, Gradient-weighted Class Activation Mapping (Grad-CAM) is integrated to generate attention maps that cast our engineered features into hypomimia-interest regions. These highlighted regions provide visual explanations for decisions of our network. Experimental results based on real-world data demonstrate the effectiveness of our method in detecting hypomimia.  © 2021 Association for Computing Machinery.",Hypomimia detection; Semantic features; Visualized analysis,Classification (of information); Diagnosis; Learning algorithms; Machine learning; Semantics; Signal encoding; Elderly people; Facial Expressions; Feature classifiers; Hypomimium detection; Machine learning algorithms; Neurodegenerative disorders; Parkinson's disease; Semantic features; Statistical variables; Visualized analyse; Neurodegenerative diseases
Black-box diagnosis and calibration on gan intra-mode collapse: A pilot study,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122616804&doi=10.1145%2f3472768&partnerID=40&md5=853b45e1aedd4bce3c67112202d02b3e,"Generative adversarial networks (GANs) nowadays are capable of producing images of incredible realism. Two concerns raised are whether the state-of-the-art GAN's learned distribution still suffers from mode collapse and what to do if so. Existing diversity tests of samples from GANs are usually conducted qualitatively on a small scale and/or depend on the access to original training data as well as the trained model parameters. This article explores GAN intra-mode collapse and calibrates that in a novel black-box setting: access to neither training data nor the trained model parameters is assumed. The new setting is practically demanded yet rarely explored and significantly more challenging. As a first stab, we devise a set of statistical tools based on sampling that can visualize, quantify, and rectify intra-mode collapse. We demonstrate the effectiveness of our proposed diagnosis and calibration techniques, via extensive simulations and experiments, on unconditional GAN image generation (e.g., face and vehicle). Our study reveals that the intra-mode collapse is still a prevailing problem in state-of-the-art GANs and the mode collapse is diagnosable and calibratable in black-box settings. Our codes are available at https://github.com/VITA-Group/BlackBoxGANCollapse.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Black-box; Calibration; Diagnosis; Hypothesis testing; Mode collapse,Black-box testing; Generative adversarial networks; Statistical mechanics; Black boxes; Calibration techniques; Diagnosis techniques; Hypothesis testing; Modeling parameters; Pilot studies; Small scale; State of the art; Statistical tools; Training data; Calibration
Medical image classification based on an adaptive size deep learning model,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122612967&doi=10.1145%2f3465220&partnerID=40&md5=03ff9ffad7d885fb57e7e5ede658dd50,"With the rapid development of Artificial Intelligence (AI), deep learning has increasingly become a research hotspot in various fields, such as medical image classification. Traditional deep learning models use Bilinear Interpolation when processing classification tasks of multi-size medical image dataset, which will cause the loss of information of the image, and then affect the classification effect. In response to this problem, this work proposes a solution for an adaptive size deep learning model. First, according to the characteristics of the multi-size medical image dataset, the optimal size set module is proposed in combination with the unpooling process. Next, an adaptive deep learning model module is proposed based on the existing deep learning model. Then, the model is fused with the size fine-tuning module used to process multi-size medical images to obtain a solution of the adaptive size deep learning model. Finally, the proposed solution model is applied to the pneumonia CT medical image dataset. Through experiments, it can be seen that the model has strong robustness, and the classification effect is improved by about 4% compared with traditional algorithms.  © 2021 Association for Computing Machinery.",Adaptive size; Classification; Deep learning; Medical images; Multi-size,Computerized tomography; Deep learning; Image classification; Medical imaging; Adaptive size; Bilinear interpolation; Deep learning; Hotspots; Image datasets; Learning models; Medical image; Medical image classification; Model use; Multi sizes; Classification (of information)
Deep active context estimation for automated COVID-19 diagnosis,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122614876&doi=10.1145%2f3457124&partnerID=40&md5=08b33dd063283ce1d982f9f98aed9a0a,"Many studies on automated COVID-19 diagnosis have advanced rapidly with the increasing availability of large-scale CT annotated datasets. Inevitably, there are still a large number of unlabeled CT slices in the existing data sources since it requires considerable consuming labor efforts. Notably, cinical experience indicates that the neighboring CT slices may present similar symptoms and signs. Inspired by such wisdom, we propose DACE, a novel CNN-based deep active context estimation framework, which leverages the unlabeled neighbors to progressively learn more robust feature representations and generate a well-performed classifier for COVID-19 diagnosis. Specifically, the backbone of the proposed DACE framework is constructed by a well-designed Long-Short Hierarchical Attention Network (LSHAN), which effectively incorporates two complementary attention mechanisms, i.e., short-range channel interactions (SCI) module and long-range spatial dependencies (LSD) module, to learn the most discriminative features from CT slices. To make full use of such available data, we design an efficient context estimation criterion to carefully assign the additional labels to these neighbors. Benefiting from two complementary types of informative annotations from -nearest neighbors, i.e., the majority of high-confidence samples with pseudo labels and the minority of low-confidence samples with hand-annotated labels, the proposed LSHAN can be fine-tuned and optimized in an incremental learning manner. Extensive experiments on the Clean-CC-CCII dataset demonstrate the superior performance of our method compared with the state-of-the-art baselines.  © 2021 Association for Computing Machinery.",Automated COVID-19 diagnosis; Deep active context estimation; K-nearest neighbors; Long-range spatial dependencies; Short-range channel interactions,Large dataset; Nearest neighbor search; Automated COVID-19 diagnose; Channel interactions; Context estimation; Deep active context estimation; K-near neighbor; Long-range spatial dependency; Long-range spatials; Nearest-neighbour; Short-range channel interaction; Spatial dependencies; Automation
WTRPNet: An explainable graph feature convolutional neural network for epileptic EEG classification,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122628001&doi=10.1145%2f3460522&partnerID=40&md5=2f52c233a0747e58602c4555618ad47e,"As one of the important tools of epilepsy diagnosis, the electroencephalogram (EEG) is noninvasive and presents no traumatic injury to patients. It contains a lot of physiological and pathological information that is easy to obtain. The automatic classification of epileptic EEG is important in the diagnosis and therapeutic efficacy of epileptics. In this article, an explainable graph feature convolutional neural network named WTRPNet is proposed for epileptic EEG classification. Since WTRPNet is constructed by a recurrence plot in the wavelet domain, it can fully obtain the graph feature of the EEG signal, which is established by an explainable graph features extracted layer called WTRP block. The proposed method shows superior performance over state-of-the-art methods. Experimental results show that our algorithm has achieved an accuracy of 99.67% in classification of focal and nonfocal epileptic EEG, which proves the effectiveness of the classification and detection of epileptic EEG.  © 2021 Association for Computing Machinery.",CNN; EEG classification; Recurrence plot; Wavelet transform,Computer aided diagnosis; Convolution; Convolutional neural networks; Electroencephalography; Automatic classification; CNN; Convolutional neural network; Electroencephalogram classification; Graph features; Recurrence plot; Therapeutic efficacy; Traumatic injury; Wavelet domain; Wavelets transform; Wavelet transforms
Doctor's dilemma: Evaluating an explainable subtractive spatial lightweight convolutional neural network for brain tumor diagnosis,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122631643&doi=10.1145%2f3457187&partnerID=40&md5=5daa85128ab221811f00b6b1309e1f38,"In Medicine Deep Learning has become an essential tool to achieve outstanding diagnosis on image data. However, one critical problem is that Deep Learning comes with complicated, black-box models so it is not possible to analyze their trust level directly. So, Explainable Artificial Intelligence (XAI) methods are used to build additional interfaces for explaining how the model has reached the outputs by moving from the input data. Of course, that's again another competitive problem to analyze if such methods are successful according to the human view. So, this paper comes with two important research efforts: (1) to build an explainable deep learning model targeting medical image analysis, and (2) to evaluate the trust level of this model via several evaluation works including human contribution. The target problem was selected as the brain tumor classification, which is a remarkable, competitive medical image-based problem for Deep Learning. In the study, MR-based pre-processed brain images were received by the Subtractive Spatial Lightweight Convolutional Neural Network (SSLW-CNN) model, which includes additional operators to reduce the complexity of classification. In order to ensure the explainable background, the model also included Class Activation Mapping (CAM). It is important to evaluate the trust level of a successful model. So, numerical success rates of the SSLW-CNN were evaluated based on the peak signal-to-noise ratio (PSNR), computational time, computational overhead, and brain tumor classification accuracy. The objective of the proposed SSLW-CNN model is to obtain faster and good tumor classification with lesser time. The results illustrate that the SSLW-CNN model provides better performance of PSNR which is enhanced by 8%, classification accuracy is improved by 33%, computation time is reduced by 19%, computation overhead is decreased by 23%, and classification time is minimized by 13%, as compared to state-of-the-art works. Because the model provided good numerical results, it was then evaluated in terms of XAI perspective by including doctor-model based evaluations such as feedback CAM visualizations, usability, expert surveys, comparisons of CAM with other XAI methods, and manual diagnosis comparison. The results show that the SSLW-CNN provides good performance on brain tumor diagnosis and ensures a trustworthy solution for the doctors.  © 2021 Association for Computing Machinery.",Brain tumor diagnosis; Deep learning; Explainable artificial intelligence; Medical imaging; Subtractive spatial lightweight convolutional neural networks,Brain; Brain mapping; Computer aided diagnosis; Convolution; Deep learning; Signal to noise ratio; Tumors; Activation mapping; Brain tumor diagnose; Brain tumors; Convolutional neural network; Deep learning; Explainable artificial intelligence; Neural network model; Subtractive spatial lightweight convolutional neural network; Trust level; Tumor diagnosis; Convolutional neural networks
Precise no-reference image quality evaluation based on distortion identification,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120323387&doi=10.1145%2f3468872&partnerID=40&md5=09a545416ab5a25693c69895c8c8c942,"The difficulty of no-reference image quality assessment (NR IQA) often lies in the lack of knowledge about the distortion in the image, which makes quality assessment blind and thus inefficient. To tackle such issue, in this article, we propose a novel scheme for precise NR IQA, which includes two successive steps, i.e., distortion identification and targeted quality evaluation. In the first step, we employ the well-known Inception-ResNet-v2 neural network to train a classifier that classifies the possible distortion in the image into the four most common distortion types, i.e., Gaussian white noise (WN), Gaussian blur (GB), jpeg compression (JPEG), and jpeg2000 compression (JP2K). Specifically, the deep neural network is trained on the large-scale Waterloo Exploration database, which ensures the robustness and high performance of distortion classification. In the second step, after determining the distortion type of the image, we then design a specific approach to quantify the image distortion level, which can estimate the image quality specially and more precisely. Extensive experiments performed on LIVE, TID2013, CSIQ, and Waterloo Exploration databases demonstrate that (1) the accuracy of our distortion classification is higher than that of the state-of-the-art distortion classification methods, and (2) the proposed NR IQA method outperforms the state-of-the-art NR IQA methods in quantifying the image quality.  © 2021 Association for Computing Machinery.",Deep learning; Distortion identification; Image quality assessment (IQA); No-reference (NR)/blind; Noisiness; Sharpness,Classification (of information); Deep neural networks; Image compression; Image quality; Quality control; Deep learning; Distortion identification; Image quality assessment; JPEG compression; No-reference; No-reference /blind; No-reference images; Noisiness; Sharpness; White noise
Deep-based Self-refined Face-top Coordination,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112863723&doi=10.1145%2f3446970&partnerID=40&md5=f1e852700f7a12b37929d3c7654c0db8,"Face-top coordination, which exists in most clothes-fitting scenarios, is challenging due to varieties of attributes, implicit correlations, and tradeoffs between general preferences and individual preferences. We present a Deep-Based Self-Refined (DBSR) system to simulate face-top coordination based on intuition evaluation. To this end, we first establish a well-coordinated face-top (WCFT) dataset from fashion databases and communities. Then, we use a jointly trained CNN Deep Canonical Correlation Analysis (DCCA) method to bridge the semantic face-top gap based on the WCFT dataset to deal with general preferences. Subsequently, an irrelevance-based Optimum-path Forest (OPF) method is developed to adapt the results to individual preferences iteratively. Experimental results and user study demonstrate the effectiveness of our method. © 2021 Association for Computing Machinery.",canonical correlation analysis; deep cross-modal learning; Fashion analysis; optimum-path forest; personalized face-top coordination; relevance feedback,Semantics; Canonical correlation analysis; Individual preference; Optimum-path forests; User study; Iterative methods
Invertible Grayscale with Sparsity Enforcing Priors,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112863569&doi=10.1145%2f3451993&partnerID=40&md5=c9cfd1e4e1b3912c0802e46f49532a36,"Color dimensionality reduction is believed as a non-invertible process, as re-colorization results in perceptually noticeable and unrecoverable distortion. In this article, we propose to convert a color image into a grayscale image that can fully recover its original colors, and more importantly, the encoded information is discriminative and sparse, which saves storage capacity. Particularly, we design an invertible deep neural network for color encoding and decoding purposes. This network learns to generate a residual image that encodes color information, and it is then combined with a base grayscale image for color recovering. In this way, the non-differentiable compression process (e.g., JPEG) of the base grayscale image can be integrated into the network in an end-to-end manner. To further reduce the size of the residual image, we present a specific layer to enhance Sparsity Enforcing Priors (SEP), thus leading to negligible storage space. The proposed method allows color embedding on a sparse residual image while keeping a high, 35dB PSNR on average. Extensive experiments demonstrate that the proposed method outperforms state-of-the-arts in terms of image quality and tolerability to compression. © 2021 Association for Computing Machinery.",colorization; convolutional neural networks; Decolorization; sparsity enforcing priors,Color; Deep neural networks; Dimensionality reduction; Encoding (symbols); Image enhancement; Color information; Compression process; Encoded information; Gray-scale images; Non-differentiable; Residual images; State of the art; Storage capacity; Image compression
Label Consistent Flexible Matrix Factorization Hashing for Efficient Cross-modal Retrieval,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112454268&doi=10.1145%2f3446774&partnerID=40&md5=3b9bba6f8efc154ac1924164edf687b1,"Hashing methods have sparked a great revolution on large-scale cross-media search due to its effectiveness and efficiency. Most existing approaches learn unified hash representation in a common Hamming space to represent all multimodal data. However, the unified hash codes may not characterize the cross-modal data discriminatively, because the data may vary greatly due to its different dimensionalities, physical properties, and statistical information. In addition, most existing supervised cross-modal algorithms preserve the similarity relationship by constructing an n×n pairwise similarity matrix, which requires a large amount of calculation and loses the category information. To mitigate these issues, a novel cross-media hashing approach is proposed in this article, dubbed label flexible matrix factorization hashing (LFMH). Specifically, LFMH jointly learns the modality-specific latent subspace with similar semantic by the flexible matrix factorization. In addition, LFMH guides the hash learning by utilizing the semantic labels directly instead of the large n×n pairwise similarity matrix. LFMH transforms the heterogeneous data into modality-specific latent semantic representation. Therefore, we can obtain the hash codes by quantifying the representations, and the learned hash codes are consistent with the supervised labels of multimodal data. Then, we can obtain the similar binary codes of the corresponding modality, and the binary codes can characterize such samples flexibly. Accordingly, the derived hash codes have more discriminative power for single-modal and cross-modal retrieval tasks. Extensive experiments on eight different databases demonstrate that our model outperforms some competitive approaches. © 2021 Association for Computing Machinery.",cross-modal retrieval; flexible matrix factorization; Hashing,Binary codes; Factorization; Hash functions; Modal analysis; Semantics; Discriminative power; Effectiveness and efficiencies; Flexible matrix; Heterogeneous data; Latent semantics; Multi-modal data; Pairwise similarity matrix; Statistical information; Matrix algebra
Single-shot Semantic Matching Network for Moment Localization in Videos,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112866149&doi=10.1145%2f3441577&partnerID=40&md5=dec138a753fe0cc73b217e55fc6106e8,"Moment localization in videos using natural language refers to finding the most relevant segment from videos given a natural language query. Most of the existing methods require video segment candidates for further matching with the query, which leads to extra computational costs, and they may also not locate the relevant moments under any length evaluated. To address these issues, we present a lightweight single-shot semantic matching network (SSMN) to avoid the complex computations required to match the query and the segment candidates, and the proposed SSMN can locate moments of any length theoretically. Using the proposed SSMN, video features are first uniformly sampled to a fixed number, while the query sentence features are generated and enhanced by GloVe, long-term short memory (LSTM), and soft-attention modules. Subsequently, the video features and sentence features are fed to an enhanced cross-modal attention model to mine the semantic relationships between vision and language. Finally, a score predictor and a location predictor are designed to locate the start and stop indexes of the query moment. We evaluate the proposed method on two benchmark datasets and the experimental results demonstrate that SSMN outperforms state-of-the-art methods in both precision and efficiency. © 2021 Association for Computing Machinery.",moment localization; Multimodal retrieval; natural language understanding; visual comprehension,Semantic Web; Semantics; Benchmark datasets; Complex computation; Computational costs; Natural language queries; Natural languages; Semantic matching; Semantic relationships; State-of-the-art methods; Long short-term memory
Payoff-based Dynamic Segment Replication and Graph Classification Method with Attribute Vectors Adapted to Urban VANET,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112865201&doi=10.1145%2f3440018&partnerID=40&md5=3a2d65c1d101c03361f011e66b42f058,"Due to the number of constraints and the dynamic nature of vehicular ad hoc networks (VANET), effective video broadcasting always remains a difficult task. In this work, we proposed a quality of video visualization guarantee model based on a feedback loop and an efficient algorithm for segmenting and replicating video segments using the Payoff-based Dynamic Segment Replication Policy (P-DSR). In the urban VANET environment, P-DSR is defined by taking into account the position of the vehicles, the speed, the direction, the number of neighboring vehicles, and the reputation of each node to stabilize the urban VANET topology. However, the management of various load control parameters between the different components of the urban VANET network remains a problem to be studied. This work uses a multi-objective problem that takes the parameters of our algorithm based on the Graph Classification Method with Attribute Vectors (GCMAV) as input. This algorithm aims to provide an improved class lifetime, an improved video segment delivery rate, a reduced inter-class overload, and an optimization of a global criterion. A scalable algorithm is used to optimize the parameters of the GCMAV. The simulations were carried out using the NetSim simulator and Multi-Objective Evolutionary Algorithms framework to optimize parameters. Experiments were carried out with realistic maps of Open Street Maps and its results were compared with other algorithms such as Seamless and Authorized Multimedia Streaming and P-DSR. The survey suggests that the proposed methodology works well concerning the average lifetime of the inter-classes and the delivery rate of video segments. © 2021 Association for Computing Machinery.",feedback control; inter-class load balancing; load balancing; optimization; P-DSR; replication; Urban VANET; video streaming,Evolutionary algorithms; Facsimile; Graph algorithms; Image segmentation; Parameter estimation; Attribute vectors; Control parameters; Graph classification; Multi objective evolutionary algorithms; Multi-objective problem; Multimedia streaming; Scalable algorithms; Video broadcasting; Vehicular ad hoc networks
Secure Chaff-less Fuzzy Vault for Face Identification Systems,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112868681&doi=10.1145%2f3442198&partnerID=40&md5=5a1af3c926f97007c3ec099ec4d1eb52,"Biometric cryptosystems such as fuzzy vaults represent one of the most popular approaches for secret and biometric template protection. However, they are solely designed for biometric verification, where the user is required to input both identity credentials and biometrics. Several practical questions related to the implementation of biometric cryptosystems remain open, especially in regard to biometric template protection. In this article, we propose a face cryptosystem for identification (FCI) in which only biometric input is needed. Our FCI is composed of a one-to-N search subsystem for template protection and a one-to-one match chaff-less fuzzy vault (CFV) subsystem for secret protection. The first subsystem stores N facial features, which are protected by index-of-maximum (IoM) hashing, enhanced by a fusion module for search accuracy. When a face image of the user is presented, the subsystem returns the top k matching scores and activates the corresponding vaults in the CFV subsystem. Then, one-to-one matching is applied to the k vaults based on the probe face, and the identifier or secret associated with the user is retrieved from the correct matched vault. We demonstrate that coupling between the IoM hashing and the CFV resolves several practical issues related to fuzzy vault schemes. The FCI system is evaluated on three large-scale public unconstrained face datasets (LFW, VGG2, and IJB-C) in terms of its accuracy, computation cost, template protection criteria, and security. © 2021 Association for Computing Machinery.",biometric template protection; Chaff-less fuzzy vault; face identification system,Biometrics; Cryptography; Large dataset; Network security; Biometric cryptosystems; Biometric template protections; Biometric verification; Computation costs; Fuzzy vault scheme; Practical issues; Search accuracy; Template protection; Large scale systems
IRTS: An Intelligent and Reliable Transmission Scheme for Screen Updates Delivery in DaaS,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112868047&doi=10.1145%2f3440035&partnerID=40&md5=b4f13b71da2963e6ce53cc6c6b726bf7,"Desktop-as-a-service (DaaS) has been recognized as an elastic and economical solution that enables users to access personal desktops from anywhere at any time. During the interaction process of DaaS, users rely on screen updates to perceive execution results remotely, and thus the reliability and timeliness of screen updates transmission have a great influence on users' quality of experience (QoE). However, the efficient transmission of screen updates in DaaS is facing severe challenges: most transmission schemes applied in DaaS determine sending strategies in terms of pre-set rules, lacking the intelligence to utilize bandwidth rationally and fit new network scenarios. Meanwhile, they tend to focus on reliability or timeliness and perform unsatisfactorily in ensuring reliability and timeliness simultaneously, leading to lower transmission efficiency of screen updates and users' QoE when network conditions turn unfavorable. In this article, an intelligent and reliable end-to-end transmission scheme (IRTS) is proposed to cope with the preceding issues. IRTS draws support from reinforcement learning by adopting SARSA, an online learning method based on the temporal difference update rule, to grasp the optimal mapping between network states and sending actions, which extricates IRTS from the reliance on pre-set rules and augments its adaptability to different network conditions. Moreover, IRTS guarantees reliability and timeliness via an adaptive loss recovery method, which intends to recover lost screen updates data automatically with fountain code while controlling the number of redundant packets generated. Extensive performance evaluations are conducted, and numerical results show that IRTS outperforms the reference schemes in display quality, end-to-end delay/delay jitter, and fairness when transferring screen updates under various network conditions, proving that IRTS can enhance the transmission efficiency of screen updates and users' QoE in DaaS. © 2021 Association for Computing Machinery.",Desktop-as-a-service; end-to-end transmission scheme; fountain code; reinforcement learning,Data as a service (DaaS); Efficiency; Learning systems; Quality control; Reinforcement learning; Reliability; User experience; End-to-end transmission; Interaction process; On-line learning methods; Quality of experience (QoE); Reliable transmission; Temporal differences; Transmission efficiency; Transmission schemes; Quality of service
Knowledge-aware Multi-modal Adaptive Graph Convolutional Networks for Fake News Detection,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112866928&doi=10.1145%2f3451215&partnerID=40&md5=dc096376bc0b5604dc36f4708d129aaf,"In this article, we focus on fake news detection task and aim to automatically identify the fake news from vast amount of social media posts. To date, many approaches have been proposed to detect fake news, which includes traditional learning methods and deep learning-based models. However, there are three existing challenges: (i) How to represent social media posts effectively, since the post content is various and highly complicated; (ii) how to propose a data-driven method to increase the flexibility of the model to deal with the samples in different contexts and news backgrounds; and (iii) how to fully utilize the additional auxiliary information (the background knowledge and multi-modal information) of posts for better representation learning. To tackle the above challenges, we propose a novel Knowledge-aware Multi-modal Adaptive Graph Convolutional Networks (KMAGCN) to capture the semantic representations by jointly modeling the textual information, knowledge concepts, and visual information into a unified framework for fake news detection. We model posts as graphs and use a knowledge-aware multi-modal adaptive graph learning principal for the effective feature learning. Compared with existing methods, the proposed KMAGCN addresses challenges from three aspects: (1) It models posts as graphs to capture the non-consecutive and long-range semantic relations; (2) it proposes a novel adaptive graph convolutional network to handle the variability of graph data; and (3) it leverages textual information, knowledge concepts and visual information jointly for model learning. We have conducted extensive experiments on three public real-world datasets and superior results demonstrate the effectiveness of KMAGCN compared with other state-of-the-art algorithms. © 2021 Association for Computing Machinery.",Fake news detection; graph convolutional network; multi-modal learning,Convolution; Convolutional neural networks; Deep learning; Semantics; Social networking (online); Auxiliary information; Back-ground knowledge; Convolutional networks; Learning Based Models; Multi-modal information; Semantic representation; State-of-the-art algorithms; Traditional learning; Learning systems
Is the Reign of Interactive Search Eternal? Findings from the Video Browser Showdown 2020,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112868703&doi=10.1145%2f3445031&partnerID=40&md5=21e4f53b9bf4b2643c356d9b77ecec77,"Comprehensive and fair performance evaluation of information retrieval systems represents an essential task for the current information age. Whereas Cranfield-based evaluations with benchmark datasets support development of retrieval models, significant evaluation efforts are required also for user-oriented systems that try to boost performance with an interactive search approach. This article presents findings from the 9th Video Browser Showdown, a competition that focuses on a legitimate comparison of interactive search systems designed for challenging known-item search tasks over a large video collection. During previous installments of the competition, the interactive nature of participating systems was a key feature to satisfy known-item search needs, and this article continues to support this hypothesis. Despite the fact that top-performing systems integrate the most recent deep learning models into their retrieval process, interactive searching remains a necessary component of successful strategies for known-item search tasks. Alongside the description of competition settings, evaluated tasks, participating teams, and overall results, this article presents a detailed analysis of query logs collected by the top three performing systems, SOMHunter, VIRET, and vitrivr. The analysis provides a quantitative insight to the observed performance of the systems and constitutes a new baseline methodology for future events. The results reveal that the top two systems mostly relied on temporal queries before a correct frame was identified. An interaction log analysis complements the result log findings and points to the importance of result set and video browsing approaches. Finally, various outlooks are discussed in order to improve the Video Browser Showdown challenge in the future. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep learning; interactive search evaluation; Interactive video retrieval,Benchmarking; Deep learning; Information retrieval; Information retrieval systems; Benchmark datasets; Interactive search; Known item searches; Participating systems; Participating teams; Retrieval process; User-oriented systems; Video collections; Search engines
Residual Refinement Network with Attribute Guidance for Precise Saliency Detection,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112864868&doi=10.1145%2f3440694&partnerID=40&md5=8d7993a12b758deb8370438c23828e89,"As an important topic in the multimedia and computer vision fields, salient object detection has been researched for years. Recently, state-of-the-art performance has been witnessed with the aid of the fully convolutional networks (FCNs) and the various pyramid-like encoder-decoder frameworks. Starting from a common encoder-decoder architecture, we enhance a residual refinement network with feature purification for better saliency estimation. To this end, we improve the global knowledge streams with intermediate supervisions for global saliency estimation and design a specific feature subtraction module for residual learning, respectively. On the basis of the strengthened network, we also introduce an attribute encoding sub-network (AENet) with a grid aggregation block (GAB) to guide the final saliency predictor to obtain more accurate saliency maps. Furthermore, the network is trained with a novel constraint loss besides the traditional cross-entropy loss to yield the finer results. Extensive experiments on five public benchmarks show our method achieves better or comparable performance compared with previous state-of-the-art methods. © 2021 Association for Computing Machinery.",attribute encoding; deep intermediate supervision; residual learning; Salient object detection,Benchmarking; Decoding; Object detection; Signal encoding; Convolutional networks; Encoder-decoder; Encoder-decoder architecture; Global knowledge; Saliency detection; Salient object detection; State-of-the-art methods; State-of-the-art performance; Convolutional neural networks
Lifelog Image Retrieval Based on Semantic Relevance Mapping,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112868937&doi=10.1145%2f3446209&partnerID=40&md5=7314d3c14264ba14ca720f726c7c09f0,"Lifelog analytics is an emerging research area with technologies embracing the latest advances in machine learning, wearable computing, and data analytics. However, state-of-the-art technologies are still inadequate to distill voluminous multimodal lifelog data into high quality insights. In this article, we propose a novel semantic relevance mapping (SRM) method to tackle the problem of lifelog information access. We formulate lifelog image retrieval as a series of mapping processes where a semantic gap exists for relating basic semantic attributes with high-level query topics. The SRM serves both as a formalism to construct a trainable model to bridge the semantic gap and an algorithm to implement the training process on real-world lifelog data. Based on the SRM, we propose a computational framework of lifelog analytics to support various applications of lifelog information access, such as image retrieval, summarization, and insight visualization. Systematic evaluations are performed on three challenging benchmarking tasks to show the effectiveness of our method. © 2021 Association for Computing Machinery.",image retrieval; Lifelog; semantic mapping; summarization,Data Analytics; Mapping; Semantics; Wearable technology; Computational framework; High-level queries; Information access; Semantic attribute; Semantic relevance; State-of-the-art technology; Systematic evaluation; Wearable computing; Image retrieval
PGNet: Progressive Feature Guide Learning Network for Three-dimensional Shape Recognition,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112867970&doi=10.1145%2f3443708&partnerID=40&md5=ccf4ba1aace7ee045741bfe89f40e7c2,"Three-dimensional (3D) shape recognition is a popular topic and has potential application value in the field of computer vision. With the recent proliferation of deep learning, various deep learning models have achieved state-of-the-art performance. Among them, multiview-based 3D shape representation has received increased attention in recent years, and related approaches have shown significant improvement in 3D shape recognition. However, these methods focus on feature learning based on the design of the network and ignore the correlation among views. In this article, we propose a novel progressive feature guide learning network (PGNet) that focuses on the correlation among multiple views and integrates multiple modalities for 3D shape recognition. In particular, we propose two information fusion schemes from visual and feature aspects. The visual fusion scheme focuses on the view level and employs the soft-attention model to define the weights of views for visual information fusion. The feature fusion scheme focuses on the feature dimension information and employs the quantified feature as the mask to further optimize the feature. These two schemes jointly construct a PGNet for 3D shape representation. The classic ModelNet40 and ShapeNetCore55 datasets are applied to demonstrate the performance of our approach. The corresponding experiment also demonstrates the superiority of our approach. © 2021 Association for Computing Machinery.",3D model retrieval; 3D shape recognition; multi-modal,Deep learning; Information fusion; 3D shape recognition; 3D shape representation; Feature dimensions; Multiple modalities; State-of-the-art performance; Three dimensional (3D) shapes; Three-dimensional shape; Visual information fusion; Learning systems
Neural-Network-Based Cross-Channel Intra Prediction,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112865032&doi=10.1145%2f3434250&partnerID=40&md5=78ad30602f2a5ac48cbcdafb971504ea,"To reduce the redundancy among different color channels, e.g., YUV, previous methods usually adopt a linear model that tends to be oversimple for complex image content. We propose a neural-network-based method for cross-channel prediction in intra frame coding. The proposed network utilizes twofold cues, i.e., the neighboring reconstructed samples with all channels, and the co-located reconstructed samples with partial channels. Specifically, for YUV video coding, the neighboring samples with YUV are processed by several fully connected layers; the co-located samples with Y are processed by convolutional layers; and the proposed network fuses the twofold cues. We observe that the integration of twofold information is crucial to the performance of intra prediction of the chroma components. We have designed the network architecture to achieve a good balance between compression performance and computational efficiency. Moreover, we propose a transform domain loss for the training of the network. The transform domain loss helps obtain more compact representations of residues in the transform domain, leading to higher compression efficiency. The proposed method is plugged into HEVC and VVC test models to evaluate its effectiveness. Experimental results show that our method provides more accurate cross-channel intra prediction compared with previous methods. On top of HEVC, our method achieves on average 1.3%, 5.4%, and 3.8% BD-rate reductions for Y, Cb, and Cr on common test sequences, and on average 3.8%, 11.3%, and 9.0% BD-rate reductions for Y, Cb, and Cr on ultra-high-definition test sequences. On top of VVC, our method achieves on average 0.5%, 1.7%, and 1.3% BD-rate reductions for Y, Cb, and Cr on common test sequences. © 2021 Association for Computing Machinery.",Convolutional neural network; cross-channel prediction; fully connected network; high-efficiency video coding (HEVC); transform domain loss; versatile video coding (VVC),Chromium; Computational efficiency; Efficiency; Forecasting; Image coding; Network architecture; Ternary alloys; Video signal processing; Channel prediction; Compact representation; Compression efficiency; Compression performance; Intra Prediction; Intraframe coding; Transform domain; Ultra high definitions; Neural networks
A Densely Connected Network Based on U-Net for Medical Image Segmentation,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112868302&doi=10.1145%2f3446618&partnerID=40&md5=11c0c67e09a4c42d1642396ee5ebca78,"The U-Net has become the most popular structure in medical image segmentation in recent years. Although its performance for medical image segmentation is outstanding, a large number of experiments demonstrate that the classical U-Net network architecture seems to be insufficient when the size of segmentation targets changes and the imbalance happens between target and background in different forms of segmentation. To improve the U-Net network architecture, we develop a new architecture named densely connected U-Net (DenseUNet) network in this article. The proposed DenseUNet network adopts a dense block to improve the feature extraction capability and employs a multi-feature fuse block fusing feature maps of different levels to increase the accuracy of feature extraction. In addition, in view of the advantages of the cross entropy and the dice loss functions, a new loss function for the DenseUNet network is proposed to deal with the imbalance between target and background. Finally, we test the proposed DenseUNet network and compared it with the multi-resolutional U-Net (MultiResUNet) and the classic U-Net networks on three different datasets. The experimental results show that the DenseUNet network has significantly performances compared with the MultiResUNet and the classic U-Net networks. © 2021 Association for Computing Machinery.",deep learning; dense block; densely connection; Medical image segmentation; U-Net network,Extraction; Feature extraction; Image segmentation; Medical image processing; Cross entropy; Densely connected networks; Extraction capability; Feature map; Loss functions; Multi features; Net networks; Target and background; Network architecture
Full-reference Screen Content Image Quality Assessment by Fusing Multilevel Structure Similarity,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112866869&doi=10.1145%2f3447393&partnerID=40&md5=52fd3c712c16aeef7c5ebae3530bdfed,"Screen content images (SCIs) usually comprise various content types with sharp edges, in which artifacts or distortions can be effectively sensed by a vanilla structure similarity measurement in a full-reference manner. Nonetheless, almost all of the current state-of-the-art (SOTA) structure similarity metrics are ""locally""formulated in a single-level manner, while the true human visual system (HVS) follows the multilevel manner; such mismatch could eventually prevent these metrics from achieving reliable quality assessment. To ameliorate this issue, this article advocates a novel solution to measure structure similarity ""globally""from the perspective of sparse representation. To perform multilevel quality assessment in accordance with the real HVS, the abovementioned global metric will be integrated with the conventional local ones by resorting to the newly devised selective deep fusion network. To validate its efficacy and effectiveness, we have compared our method with 12 SOTA methods over two widely used large-scale public SCI datasets, and the quantitative results indicate that our method yields significantly higher consistency with subjective quality scores than the current leading works. Both the source code and data are also publicly available to gain widespread acceptance and facilitate new advancement and validation. © 2021 Association for Computing Machinery.",Image quality assessment; screen content images; selective deep fusion,Computer networks; Human visual systems; Image quality assessment; Multi-level structures; Quality assessment; Quantitative result; Sparse representation; Structure similarity; Subjective quality; Large dataset
MFECN: Multi-level Feature Enhanced Cumulative Network for Scene Text Detection,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112869185&doi=10.1145%2f3440087&partnerID=40&md5=95091ae7c3b754aca2c3c74178c071ad,"Recently, many scene text detection algorithms have achieved impressive performance by using convolutional neural networks. However, most of them do not make full use of the context among the hierarchical multi-level features to improve the performance of scene text detection. In this article, we present an efficient multi-level features enhanced cumulative framework based on instance segmentation for scene text detection. At first, we adopt a Multi-Level Features Enhanced Cumulative (MFEC) module to capture features of cumulative enhancement of representational ability. Then, a Multi-Level Features Fusion (MFF) module is designed to fully integrate both high-level and low-level MFEC features, which can adaptively encode scene text information. To verify the effectiveness of the proposed method, we perform experiments on six public datasets (namely, CTW1500, Total-text, MSRA-TD500, ICDAR2013, ICDAR2015, and MLT2017), and make comparisons with other state-of-the-art methods. Experimental results demonstrate that the proposed Multi-Level Features Enhanced Cumulative Network (MFECN) detector can well handle scene text instances with irregular shapes (i.e., curved, oriented, and horizontal) and achieves better or comparable results. © 2021 Association for Computing Machinery.",Feature Fusion; Instance Segmentation; Multi-level Feature; Scene Text Detection,Convolutional neural networks; Horizontal wells; Features fusions; Irregular shape; Multilevels; Scene Text; State-of-the-art methods; Feature extraction
Global-Local Enhancement Network for NMF-Aware Sign Language Recognition,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112867427&doi=10.1145%2f3436754&partnerID=40&md5=012ccd87549fbbec580278988011a235,"Sign language recognition (SLR) is a challenging problem, involving complex manual features (i.e., hand gestures) and fine-grained non-manual features (NMFs) (i.e., facial expression, mouth shapes, etc.). Although manual features are dominant, non-manual features also play an important role in the expression of a sign word. Specifically, many sign words convey different meanings due to non-manual features, even though they share the same hand gestures. This ambiguity introduces great challenges in the recognition of sign words. To tackle the above issue, we propose a simple yet effective architecture called Global-Local Enhancement Network (GLE-Net), including two mutually promoted streams toward different crucial aspects of SLR. Of the two streams, one captures the global contextual relationship, while the other stream captures the discriminative fine-grained cues. Moreover, due to the lack of datasets explicitly focusing on this kind of feature, we introduce the first non-manual-feature-aware isolated Chinese sign language dataset (NMFs-CSL) with a total vocabulary size of 1,067 sign words in daily life. Extensive experiments on NMFs-CSL and SLR500 datasets demonstrate the effectiveness of our method. © 2021 Association for Computing Machinery.",global-local enhancement network; NMFs-CSL dataset; Non-manual features; sign language recognition,Computer networks; Chinese sign language; Contextual relationships; Facial Expressions; Fine grained; Global-local; Hand gesture; Sign Language recognition; Vocabulary size
A Real-Time Effective Fusion-Based Image Defogging Architecture on FPGA,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112863988&doi=10.1145%2f3446241&partnerID=40&md5=054fcde1c4cd748b35bd4c8ec8a787af,"Foggy weather reduces the visibility of photographed objects, causing image distortion and decreasing overall image quality. Many approaches (e.g., image restoration, image enhancement, and fusion-based methods) have been proposed to work out the problem. However, most of these defogging algorithms are facing challenges such as algorithm complexity or real-time processing requirements. To simplify the defogging process, we propose a fusional defogging algorithm on the linear transmission of gray single-channel. This method combines gray single-channel linear transform with high-boost filtering according to different proportions. To enhance the visibility of the defogging image more effectively, we convert the RGB channel into a gray-scale single channel without decreasing the defogging results. After gray-scale fusion, the data in the gray-scale domain should be linearly transmitted. With the increasing real-time requirements for clear images, we also propose an efficient real-time FPGA defogging architecture. The architecture optimizes the data path of the guided filtering to speed up the defogging speed and save area and resources. Because the pixel reading order of mean and square value calculations are identical, the shift register in the box filter after the average and the computation of the square values is separated from the box filter and put on the input terminal for sharing, saving the storage area. What's more, using LUTs instead of the multiplier can decrease the time delays of the square value calculation module and increase efficiency. Experimental results show that the linear transmission can save 66.7% of the total time. The architecture we proposed can defog efficiently and accurately, meeting the real-time defogging requirements on 1920 × 1080 image size. © 2021 Association for Computing Machinery.",FPGA; fusion-based defogging; Single gray channel,Computational complexity; Digital storage; Field programmable gate arrays (FPGA); Filters (for fluids); Image fusion; Image reconstruction; Mathematical transformations; Shift registers; Transmissions; Visibility; Algorithm complexity; Different proportions; Guided filtering; Image distortions; Linear transform; Linear transmission; Real time requirement; Realtime processing; Image enhancement
Distribution Aligned Multimodal and Multi-domain Image Stylization,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112867154&doi=10.1145%2f3450525&partnerID=40&md5=72bb14a2f6271f0e565620fea3dfa8f5,"Multimodal and multi-domain stylization are two important problems in the field of image style transfer. Currently, there are few methods that can perform multimodal and multi-domain stylization simultaneously. In this study, we propose a unified framework for multimodal and multi-domain style transfer with the support of both exemplar-based reference and randomly sampled guidance. The key component of our method is a novel style distribution alignment module that eliminates the explicit distribution gaps between various style domains and reduces the risk of mode collapse. The multimodal diversity is ensured by either guidance from multiple images or random style codes, while the multi-domain controllability is directly achieved by using a domain label. We validate our proposed framework on painting style transfer with various artistic styles and genres. Qualitative and quantitative comparisons with state-of-the-art methods demonstrate that our method can generate high-quality results of multi-domain styles and multimodal instances from reference style guidance or a random sampled style. © 2021 Association for Computing Machinery.",Image stylization; multi-domain; multimodal,Computer networks; Exemplar-based; High quality; Image stylizations; Multi domains; Multiple image; Quantitative comparison; State-of-the-art methods; Unified framework
Video Decolorization Based on the CNN and LSTM Neural Network,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112866188&doi=10.1145%2f3446619&partnerID=40&md5=915338864768b443dfaa12442cc26f0a,"Video decolorization is the process of transferring three-channel color videos into single-channel grayscale videos, which is essentially the decolorization operation of video frames. Most existing video decolorization algorithms directly apply image decolorization methods to decolorize video frames. However, if we only take the single-frame decolorization result into account, it will inevitably cause temporal inconsistency and flicker phenomenon meaning that the same local content between continuous video frames may display different gray values. In addition, there are often similar local content features between video frames, which indicates redundant information. To solve the preceding problems, this article proposes a novel video decolorization algorithm based on the convolutional neural network and the long short-term memory neural network. First, we design a local semantic content encoder to learn and extract the same local content of continuous video frames, which can better preserve the contrast of video frames. Second, a temporal feature controller based on the bi-directional recurrent neural networks with Long short-term memory units is employed to refine the local semantic features, which can greatly maintain temporal consistency of the video sequence to eliminate the flicker phenomenon. Finally, we take advantages of deconvolution to decode the features to produce the grayscale video sequence. Experiments have indicated that our method can better preserve the local contrast of video frames and the temporal consistency over the state of the-art. © 2021 Association for Computing Machinery.",convolution neural network; LSTM; RNN; temporal consistency; Video decolorization,Brain; Convolutional neural networks; Semantics; Video recording; Decolorization methods; Local semantic features; Semantic content; Short term memory; State of the art; Temporal consistency; Temporal features; Temporal inconsistencies; Long short-term memory
Semantic Correspondence with Geometric Structure Analysis,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112865912&doi=10.1145%2f3441576&partnerID=40&md5=807b11faff27290dd9fd705941430cc1,"This article studies the correspondence problem for semantically similar images, which is challenging due to the joint visual and geometric deformations. We introduce the Flip-aware Distance Ratio method (FDR) to solve this problem from the perspective of geometric structure analysis. First, a distance ratio constraint is introduced to enforce the geometric consistencies between images with large visual variations, whereas local geometric jitters are tolerated via a smoothness term. For challenging cases with symmetric structures, our proposed method exploits Curl to suppress the mismatches. Subsequently, image correspondence is formulated as a permutation problem, for which we propose a Gradient Guided Simulated Annealing (GGSA) algorithm to perform a robust discrete optimization. Experiments on simulated and real-world datasets, where both visual and geometric deformations are present, indicate that our method significantly improves the baselines for both visually and semantically similar images. © 2021 Association for Computing Machinery.",bilateral symmetry; Curl; image correspondence,Deformation; Image enhancement; Semantics; Simulated annealing; Correspondence problems; Discrete optimization; Geometric deformations; Image correspondence; Permutation problems; Real-world datasets; Semantic correspondence; Symmetric structures; Geometry
Part-wise Spatiooral Attention Driven CNN-based 3D Human Action Recognition,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112867385&doi=10.1145%2f3441628&partnerID=40&md5=5a5b71ac78a5f962eed208c9e2dc8a11,"Recently, human activity recognition using skeleton data is increasing due to its ease of acquisition and finer shape details. Still, it suffers from a wide range of intra-class variation, inter-class similarity among the actions and view variation due to which extraction of discriminative spatial and temporal features is still a challenging problem. In this regard, we present a novel Residual Inception Attention Driven CNN (RIAC-Net) Network, which visualizes the dynamics of the action in a part-wise manner. The complete skeletonis partitioned into five key parts: Head to Spine, Left Leg, Right Leg, Left Hand, Right Hand. For each part, a Compact Action Skeleton Sequence (CASS) is defined. Part-wise skeleton-based motion dynamics highlights discriminative local features of the skeleton that helps to overcome the challenges of inter-class similarity and intra-class variation with improved recognition performance. The RIAC-Net architecture is inspired by the concept of inception-residual representation that unifies the Attention Driven Residues (ADR) with inception-based Spatiooral Convolution Features (STCF) to learn efficient salient action features. An ablation study is also carried out to analyze the effect of ADR over simple residue-based action representation. The robustness of the proposed framework is evaluated by performing an extensive experiment on four challenging datasets: UT Kinect Action 3D, Florence 3D action, MSR Daily Action3D, and NTU RGB-D datasets, which consistently demonstrate the superiority of the proposed method over other state-of-the-art methods. © 2021 Association for Computing Machinery.",Attention; human action recognition; inception; residues; skeleton; spatiooral action representation,Computer networks; Action representations; Human activity recognition; Human-action recognition; Intra-class variation; Motion dynamics; NET architecture; State-of-the-art methods; Temporal features; Musculoskeletal system
Semi-supervised Intrusive Appliance Load Monitoring in Smart Energy Monitoring System,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122619632&doi=10.1145%2f3448415&partnerID=40&md5=6ef2767d3d762f9d01cca0dc3445f27d,"Intrusive Load Monitoring (ILM) is a method to measure and collect the energy consumption data of individual appliances via smart plugs or smart sockets. A major challenge of ILM is automatic appliance identification, in which the system is able to determine automatically a label of the active appliance connected to the smart device. Existing ILM techniques depend on labels input by end-users and are usually under the supervised learning scheme. However, in reality, end-users labeling is laboriously rendering insufficient training data to fit the supervised learning models. In this work, we propose a semi-supervised learning (SSL) method that leverages rich signals from the unlabeled dataset and jointly learns the classification loss for the labeled dataset and the consistency training loss for unlabeled dataset. The samples fit into consistency learning are generated by a transformation that is built upon weighted versions of DTW Barycenter Averaging algorithm. The work is inspired by two recent advanced works in SSL in computer vision and combines the advantages of the two. We evaluate our method on the dataset collected from our developed Internet-of-Things based energy monitoring system in a smart home environment. We also examine the method's performances on 10 benchmark datasets. As a result, the proposed method outperforms other methods on our smart appliance datasets and most of the benchmarks datasets, while it shows competitive results on the rest datasets.  © 2021 Association for Computing Machinery.",Intrusive loading monitoring; semi-supervised learning; UDA,Automation; Classification (of information); Electric load management; Energy utilization; Learning algorithms; Supervised learning; Benchmark datasets; End-users; Energy consumption datum; Energy monitoring system; Intrusive loading monitoring; Load monitoring; Semi-supervised; Smart energies; Smart plugs; UDA; Monitoring
A Multi-instance Multi-label Dual Learning Approach for Video Captioning,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108535626&doi=10.1145%2f3446792&partnerID=40&md5=e43d80d0bb807e91334a8a769254145a,"Video captioning is a challenging task in the field of multimedia processing, which aims to generate informative natural language descriptions/captions to describe video contents. Previous video captioning approaches mainly focused on capturing visual information in videos using an encoder-decoder structure to generate video captions. Recently, a new encoder-decoder-reconstructor structure was proposed for video captioning, which captured the information in both videos and captions. Based on this, this article proposes a novel multi-instance multi-label dual learning approach (MIMLDL) to generate video captions based on the encoder-decoder-reconstructor structure. Specifically, MIMLDL contains two modules: caption generation and video reconstruction modules. The caption generation module utilizes a lexical fully convolutional neural network (Lexical FCN) with a weakly supervised multi-instance multi-label learning mechanism to learn a translatable mapping between video regions and lexical labels to generate video captions. Then the video reconstruction module synthesizes visual sequences to reproduce raw videos using the outputs of the caption generation module. A dual learning mechanism fine-tunes the two modules according to the gap between the raw and the reproduced videos. Thus, our approach can minimize the semantic gap between raw videos and the generated captions by minimizing the differences between the reproduced and the raw visual sequences. Experimental results on a benchmark dataset demonstrate that MIMLDL can improve the accuracy of video captioning.  © 2021 Association for Computing Machinery.",Deep neural networks; Dual learning; Multimedia processing; Multiple instance learning; Video captioning,Convolutional neural networks; Decoding; Multimedia systems; Semantics; Signal encoding; Benchmark datasets; Learning approach; Learning mechanism; Multi-instance multi-label learning; Multimedia processing; Natural languages; Video reconstruction; Visual information; Image reconstruction
Exploring Uncertainty Measures for Image-caption Embedding-and-retrieval Task,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107938786&doi=10.1145%2f3425663&partnerID=40&md5=7789b499a8d2a716c1e9ad5ef7943d57,"With the significant development of black-box machine learning algorithms, particularly deep neural networks, the practical demand for reliability assessment is rapidly increasing. On the basis of the concept that ""Bayesian deep learning knows what it does not know,""the uncertainty of deep neural network outputs has been investigated as a reliability measure for classification and regression tasks. By considering an embedding task as a regression task, several existing studies have quantified the uncertainty of embedded features and improved the retrieval performance of cutting-edge models by model averaging. However, in image-caption embedding-and-retrieval tasks, well-known samples are not always easy to retrieve. This study shows that the existing method has poor performance in reliability assessment and investigates another aspect of image-caption embedding-and-retrieval tasks. We propose posterior uncertainty by considering the retrieval task as a classification task, which can accurately assess the reliability of retrieval results. The consistent performance of the two uncertainty measures is observed with different datasets (MS-COCO and Flickr30k), different deep-learning architectures (dropout and batch normalization), and different similarity functions. To the best of our knowledge, this is the first study to perform a reliability assessment on image-caption embedding-and-retrieval tasks. © 2021 ACM.",Bayesian deep learning; image-caption retrieval; semantic embedding; Uncertainty quantification,Deep neural networks; Embeddings; Learning algorithms; Neural networks; Reliability analysis; Uncertainty analysis; Classification tasks; Consistent performance; Learning architectures; Posterior uncertainties; Reliability assessments; Retrieval performance; Similarity functions; Uncertainty measures; Deep learning
Integrating Scene Semantic Knowledge into Image Captioning,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108005177&doi=10.1145%2f3439734&partnerID=40&md5=5aee1565be8de941eb77cfb5691c87e0,"Most existing image captioning methods use only the visual information of the image to guide the generation of captions, lack the guidance of effective scene semantic information, and the current visual attention mechanism cannot adjust the focus intensity on the image. In this article, we first propose an improved visual attention model. At each timestep, we calculated the focus intensity coefficient of the attention mechanism through the context information of the model, then automatically adjusted the focus intensity of the attention mechanism through the coefficient to extract more accurate visual information. In addition, we represented the scene semantic knowledge of the image through topic words related to the image scene, then added them to the language model. We used the attention mechanism to determine the visual information and scene semantic information that the model pays attention to at each timestep and combined them to enable the model to generate more accurate and scene-specific captions. Finally, we evaluated our model on Microsoft COCO (MSCOCO) and Flickr30k standard datasets. The experimental results show that our approach generates more accurate captions and outperforms many recent advanced models in various evaluation metrics. © 2021 ACM.",attention mechanism; encoder-decoder framework; Image captioning; scene semantics,Behavioral research; Attention mechanisms; Context information; Evaluation metrics; Semantic information; Semantic knowledge; Visual attention mechanisms; Visual attention model; Visual information; Semantics
eDiaPredict: An Ensemble-based Framework for Diabetes Prediction,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108570180&doi=10.1145%2f3415155&partnerID=40&md5=22a441a9fe4313d6874b82aafc78ea81,"Medical systems incorporate modern computational intelligence in healthcare. Machine learning techniques are applied to predict the onset and reoccurrence of the disease, identify biomarkers for survivability analysis depending upon certain health conditions of the patient. Early prediction of diseases like diabetes is essential as the number of diabetic patients of all age groups is increasing rapidly. To identify underlying reasons for the onset of diabetes in its early stage has become a challenging task for medical practitioners. Continuously increasing diabetic patient data has necessitated for the applications of efficient machine learning algorithms, which learns from the trends of the underlying data and recognizes the critical conditions in patients. In this article, an ensemble-based framework named eDiaPredict is proposed. It uses ensemble modeling, which includes an ensemble of different machine learning algorithms comprising XGBoost, Random Forest, Support Vector Machine, Neural Network, and Decision tree to predict diabetes status among patients. The performance of eDiaPredict has been evaluated using various performance parameters like accuracy, sensitivity, specificity, Gini Index, precision, area under curve, area under convex hull, minimum error rate, and minimum weighted coefficient. The effectiveness of the proposed approach is shown by its application on the PIMA Indian diabetes dataset wherein an accuracy of 95% is achieved.  © 2021 Association for Computing Machinery.",Decision tree; Diabetes prediction; Ensembled models; Random forest; XGBoost,Decision trees; Forecasting; Hospital data processing; Intelligent computing; Learning systems; Support vector machines; Critical condition; Machine learning techniques; Medical practitioner; Minimum error rate; Performance parameters; Pima Indian Diabetes; Survivability analysis; Weighted coefficients; Learning algorithms
A Security and Privacy Validation Methodology for e-Health Systems,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108559720&doi=10.1145%2f3412373&partnerID=40&md5=66bb0238c099cb36c6e3f020b6b8f344,"e-Health applications enable one to acquire, process, and share patient medical data to improve diagnosis, treatment, and patient monitoring. Despite the undeniable benefits brought by the digitization of health systems, the transmission of and access to medical information raises critical issues, mainly related to security and privacy. While several security mechanisms exist that can be applied in an e-Health system, they may not be adequate due to the complexity of involved workflows, and to the possible inherent correlation among health-related concepts that may be exploited by unauthorized subjects. In this article, we propose a novel methodology for the validation of security and privacy policies in a complex e-Health system, that leverages a formal description of clinical workflows and a semantically enriched definition of the data model used by the workflows, in order to build a comprehensive model of the system that can be analyzed with automated model checking and ontology-based reasoning techniques. To validate the proposed methodology, we applied it to two case studies, subjected to the directives of the EU GDPR regulation for the protection of health data, and demonstrated its ability to correctly verify the fulfillment of desired policies in different scenarios.  © 2021 Association for Computing Machinery.",E-Health management systems; Formal methods for security validation; Security and privacy for e-Health data; Security and privacy validation,Diagnosis; eHealth; Model checking; Ontology; Patient monitoring; Patient treatment; Automated modeling; Comprehensive model; e-Health applications; Formal Description; Medical information; Security and privacy; Security mechanism; Validation methodologies; Medical information systems
MILL: Channel Attention–based Deep Multiple Instance Learning for Landslide Recognition,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108538442&doi=10.1145%2f3454009&partnerID=40&md5=b8447c093f74e4f4a5ef5258a07790f8,"Landslide recognition is widely used in natural disaster risk management. Traditional landslide recognition is mainly conducted by geologists, which is accurate but inefficient. This article introduces multiple instance learning (MIL) to perform automatic landslide recognition. An end-to-end deep convolutional neural network is proposed, referred to as Multiple Instance Learning-based Landslide classification (MILL). First, MILL uses a large-scale remote sensing image classification dataset to build pre-train networks for landslide feature extraction. Second, MILL extracts instances and assign instance labels without pixel-level annotations. Third, MILL uses a new channel attention-based MIL pooling function to map instance-level labels to bag-level label. We apply MIL to detect landslides in a loess area. Experimental results demonstrate that MILL is effective in identifying landslides in remote sensing images.  © 2021 Association for Computing Machinery.",Channel attention; Landslide classification; Landslide detection; Landslide recognition; Multiple instance learning,Classification (of information); Convolutional neural networks; Deep neural networks; Disasters; Image processing; Landslides; Large dataset; Learning systems; Remote sensing; Risk management; End to end; Loess area; Multiple instance learning; Natural disaster risk management; New channels; Pixel level; Remote sensing image classification; Remote sensing images; Deep learning
Equivariant Adversarial Network for Image-to-image Translation,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108552069&doi=10.1145%2f3458280&partnerID=40&md5=257ac55083adf71036aefe58a853d9b7,"Image-to-Image translation aims to learn an image from a source domain to a target domain. However, there are three main challenges, such as lack of paired datasets, multimodality, and diversity, that are associated with these problems and need to be dealt with. Convolutional neural networks (CNNs), despite of having great performance in many computer vision tasks, they fail to detect the hierarchy of spatial relationships between different parts of an object and thus do not form the ideal representative model we look for. This article presents a new variation of generative models that aims to remedy this problem. We use a trainable transformer, which explicitly allows the spatial manipulation of data within training. This differentiable module can be augmented into the convolutional layers in the generative model, and it allows to freely alter the generated distributions for image-to-image translation. To reap the benefits of proposed module into generative model, our architecture incorporates a new loss function to facilitate an effective end-to-end generative learning for image-to-image translation. The proposed model is evaluated through comprehensive experiments on image synthesizing and image-to-image translation, along with comparisons with several state-of-the-art algorithms.  © 2021 Association for Computing Machinery.",Domain adaptation; Generative model; Image-to-image translation; Stylistic image generation,Convolution; Adversarial networks; Generative model; Image translation; Loss functions; Multi-modality; Spatial relationships; State-of-the-art algorithms; Target domain; Convolutional neural networks
3D Tensor Auto-encoder with Application to Video Compression,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107958395&doi=10.1145%2f3431768&partnerID=40&md5=37420123907f164a115afcfba060c00f,"Auto-encoder has been widely used to compress high-dimensional data such as the images and videos. However, the traditional auto-encoder network needs to store a large number of parameters. Namely, when the input data is of dimension n, the number of parameters in an auto-encoder is in general O(n). In this article, we introduce a network structure called 3D Tensor Auto-Encoder (3DTAE). Unlike the traditional auto-encoder, in which a video is represented as a vector, our 3DTAE considers videos as 3D tensors to directly pass tensor objects through the network. The weights of each layer are represented by three small matrices, and thus the number of parameters in 3DTAE is just O(n1/3). The compact nature of 3DTAE fits well the needs of video compression. Given an ensemble of high-dimensional videos, we represent them as 3DTAE networks plus some small core tensors, and we further quantize the network parameters and the core tensors to get the final compressed data. Experimental results verify the efficiency of 3DTAE. © 2021 ACM.",ADAM; auto-encoder; tensor; Video compression,Clustering algorithms; Learning systems; Signal encoding; Tensors; Three dimensional computer graphics; Auto encoders; Compressed datum; High dimensional data; High-dimensional; Input datas; Network parameters; Network structures; Tensor objects; Image compression
Output-Bounded and RBFNN-Based Position Tracking and Adaptive Force Control for Security Tele-Surgery,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108542424&doi=10.1145%2f3394920&partnerID=40&md5=211d1f6d4465e6f8caf5ee89ef2ccf64,"In security e-health brain neurosurgery, one of the important processes is to move the electrocoagulation to the appropriate position in order to excavate the diseased tissue.1 However, it has been problematic for surgeons to freely operate the electrocoagulation, as the workspace is very narrow in the brain. Due to the precision, vulnerability, and important function of brain tissues, it is essential to ensure the precision and safety of brain tissues surrounding the diseased part. The present study proposes the use of a robot-assisted tele-surgery system to accomplish the process. With the aim to achieve accuracy, an output-bounded and RBF neural network-based bilateral position control method was designed to guarantee the stability and accuracy of the operation process. For the purpose of accomplishing a minimal amount of bleeding and damage, an adaptive force control of the slave manipulator was proposed, allowing it to be appropriate to contact the susceptible vessels, nerves, and brain tissues. The stability was analyzed, and the numerical simulation results revealed the high performance of the proposed controls.  © 2021 Association for Computing Machinery.",Bilateral position control; Force control; RBFNN; Security tele-surgery,Brain; Coagulation; Force control; Histology; Neurosurgery; Position control; Radial basis function networks; Tissue; Tracking (position); Adaptive force control; Brain tissue; Electro coagulations; Operation process; RBF Neural Network; Slave manipulators; Telesurgery; Robotic surgery
Multi-Tier CloudVR: Leveraging Edge Computing in Remote Rendered Virtual Reality,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107974106&doi=10.1145%2f3429441&partnerID=40&md5=dd2ab9a09d914d567c1f0c87507af627,"The availability of high bandwidth with low-latency communication in 5G mobile networks enables remote rendered real-time virtual reality (VR) applications. Remote rendering of VR graphics in a cloud removes the need for local personal computer for graphics rendering and augments weak graphics processing unit capacity of stand-alone VR headsets. However, to prevent the added network latency of remote rendering from ruining user experience, rendering a locally navigable viewport that is larger than the field of view of the HMD is necessary. The size of the viewport required depends on latency: Longer latency requires rendering a larger viewport and streaming more content. In this article, we aim to utilize multi-access edge computing to assist the backend cloud in such remote rendered interactive VR. Given the dependency between latency and amount and quality of the content streamed, our objective is to jointly optimize the tradeoff between average video quality and delivery latency. Formulating the problem as mixed integer nonlinear programming, we leverage the interpolation between client's field of view frame size and overall latency to convert the problem to integer nonlinear programming model and then design efficient online algorithms to solve it. The results of our simulations supplemented by real-world user data reveal that enabling a desired balance between video quality and latency, our algorithm particularly achieves the improvements of on average about 22% and 12% in term of video delivery latency and 8% in term of video quality compared to respectively order-of-arrival, threshold-based, and random-location strategies. © 2021 ACM.",greedy algorithm; integer nonlinear programming (INLP); joint optimization; Multi-access edge computing (MEC); rendered virtual reality (VR),5G mobile communication systems; Edge computing; Graphics processing unit; Integer programming; Nonlinear programming; Personal computers; Program processors; User experience; Virtual reality; Graphics processing; Graphics rendering; Integer-nonlinear programming; Low-latency communication; Mixed-integer nonlinear programming; Network latencies; On-line algorithms; Remote rendering; Rendering (computer graphics)
Privacy-preserving Decentralized Learning Framework for Healthcare System,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108542972&doi=10.1145%2f3426474&partnerID=40&md5=45bb944fbf3d918b28c7121fd2da530d,"Clinical trials and drug discovery would not be effective without the collaboration of institutions. Earlier, it has been at the cost of individual's privacy. Several pacts and compliances have been enforced to avoid data breaches. The existing schemes collect the participant's data to a central repository for learning predictions as the collaboration is indispensable for research advances. The current COVID pandemic has put a question mark on our existing setup where the existing data repository has proved to be obsolete. There is a need for contemporary data collection, processing, and learning. The smartphones and devices held by the last person of the society have also made them a potential contributor. It demands to design a distributed and decentralized Collaborative Learning system that would make the knowledge inference from every data point. Federated Learning [21], proposed by Google, brings the concept of in-place model training by keeping the data intact to the device. Though it is privacy-preserving in nature, however, it is susceptible to inference, poisoning, and Sybil attacks. Blockchain is a decentralized programming paradigm that provides a broader control of the system, making it attack resistant. It poses challenges of high computing power, storage, and latency. These emerging technologies can contribute to the desired learning system and motivate them to address their security and efficiency issues. This article systematizes the security issues in Federated Learning, its corresponding mitigation strategies, and Blockchain's challenges. Further, a Blockchain-based Federated Learning architecture with two layers of participation is presented, which improves the global model accuracy and guarantees participant's privacy. It leverages the channel mechanism of Blockchain for parallel model training and distribution. It facilitates establishing decentralized trust between the participants and the gateways using the Blockchain, which helps to have only honest participants.  © 2021 Association for Computing Machinery.",Blockchain; Federated learning; Healthcare system; Privacy; Security,Blockchain; Digital storage; mHealth; Privacy by design; Collaborative learning systems; Decentralized learning; Emerging technologies; Health-care system; Learning architectures; Mitigation strategy; Privacy preserving; Programming paradigms; Learning systems
A Fast Defogging Image Recognition Algorithm Based on Bilateral Hybrid Filtering,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107921063&doi=10.1145%2f3391297&partnerID=40&md5=a8082b17a8e762bf557b725a2b309181,"With the rapid advancement of video and image processing technologies in the Internet of Things, it is urgent to address the issues in real-time performance, clarity, and reliability of image recognition technology for a monitoring system in foggy weather conditions. In this work, a fast defogging image recognition algorithm is proposed based on bilateral hybrid filtering. First, the mathematical model based on bilateral hybrid filtering is established. The dark channel is used for filtering and denoising the defogging image. Next, a bilateral hybrid filtering method is proposed by using a combination of guided filtering and median filtering, as it can effectively improve the robustness and transmittance of defogging images. On this basis, the proposed algorithm dramatically decreases the computation complexity of defogging image recognition and reduces the image execution time. Experimental results show that the defogging effect and speed are promising, with the image recognition rate reaching to 98.8% after defogging. © 2021 ACM.",bilateral hybrid filtering; defogging image; IoT; robustness,Image recognition; Median filters; Computation complexity; Hybrid filtering methods; Image processing technology; Image recognition technology; Median filtering; Monitoring system; Real time performance; Recognition algorithm; Image enhancement
Z-Score-Based Secure Biomedical Model for Effective Skin Lesion Segmentation over eHealth Cloud,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108566878&doi=10.1145%2f3430806&partnerID=40&md5=aee40e74c0fb722ad58e64d7684ef639,"This study aims to process the private medical data over eHealth cloud platform. The current pandemic situation, caused by Covid19 has made us to realize the importance of automatic remotely operated independent services, such as cloud. However, the cloud servers are developed and maintained by third parties, and may access user's data for certain benefits. Considering these problems, we propose a specialized method such that the patient's rights and changes in medical treatment can be preserved. The problem arising due to Melanoma skin cancer is carefully considered and a privacy-preserving cloud-based approach is proposed to achieve effective skin lesion segmentation. The work is accomplished by the development of a Z-score-based local color correction method to differentiate image pixels from ambiguity, resulting the segmentation quality to be highly improved. On the other hand, the privacy is assured by partially order homomorphic Permutation Ordered Binary (POB) number system and image permutation. Experiments are performed over publicly available images from the ISIC 2016 and 2017 challenges, as well as PH dataset, where the proposed approach is found to achieve significant results over the encrypted images (known as encrypted domain), as compared to the existing schemes in the plain domain (unencrypted images). We also compare the results with the winners of the ISBI 2016 and 2017 challenges, and show that the proposed approach achieves a very close result with them, even after processing test images in the encrypted domain. Security of the proposed approach is analyzed using a challenge-response game model.  © 2021 Association for Computing Machinery.",EHealth cloud application; Image segmentation; Multimedia security; Privacy-preservation,Cryptography; Dermatology; eHealth; Image segmentation; Medical problems; Numbering systems; Patient treatment; Privacy by design; Bio-medical models; Challenge response; Cloud platforms; Encrypted images; Medical treatment; Privacy preserving; Segmentation quality; Third parties; Image enhancement
Assessment of Machine Learning-Based AudiovisualQuality Predictors:Why Uncertainty Matters,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107911280&doi=10.1145%2f3430376&partnerID=40&md5=3f1311e29dd98bc3bb8fea7580c8eb24,"Quality assessment of audiovisual (AV) signals is important from the perspective of system design, optimization, and management of a modern multimedia communication system. However, automatic prediction of AV quality via the use of computational models remains challenging. In this context, machine learning (ML) appears to be an attractive alternative to the traditional approaches. This is especially when such assessment needs to be made in no-reference (i.e., the original signal is unavailable) fashion. While development of ML-based quality predictors is desirable, we argue that proper assessment and validation of such predictors is also crucial before they can be deployed in practice. To this end, we raise some fundamental questions about the current approach of ML-based model development for AV quality assessment and signal processing for multimedia communication in general. We also identify specific limitations associated with the current validation strategy which have implications on analysis and comparison of ML-based quality predictors. These include a lack of consideration of: (a) data uncertainty, (b) domain knowledge, (c) explicit learning ability of the trained model, and (d) interpretability of the resultant model. Therefore, the primary goal of this article is to shed some light into mentioned factors. Our analysis and proposed recommendations are of particular importance in the light of significant interests in ML methods for multimedia signal processing (specifically in cases where human-labeled data is used), and a lack of discussion of mentioned issues in existing literature. © 2021 ACM.",Audiovisual quality; machine learning; uncertainty; validation,Machine learning; Multimedia signal processing; Multimedia systems; Automatic prediction; Computational model; Explicit learning; Multi-media communications; Multimedia communication systems; Quality assessment; Traditional approaches; Validation strategies; Quality control
Multi-peak Graph-based Multi-instance Learning for Weakly Supervised Object Detection,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108557127&doi=10.1145%2f3432861&partnerID=40&md5=c126f15451ffddd5d4f054373d27360a,"Weakly supervised object detection (WSOD), aiming to detect objects with only image-level annotations, has become one of the research hotspots over the past few years. Recently, much effort has been devoted to WSOD for the simple yet effective architecture and remarkable improvements have been achieved. Existing approaches using multiple-instance learning usually pay more attention to the proposals individually, ignoring relation information between proposals. Besides, to obtain pseudo-ground-truth boxes for WSOD, MIL-based methods tend to select the region with the highest confidence score and regard those with small overlap as background category, which leads to mislabeled instances. As a result, these methods suffer from mislabeling instances and lacking relations between proposals, degrading the performance of WSOD. To tackle these issues, this article introduces a multi-peak graph-based model for WSOD. Specifically, we use the instance graph to model the relations between proposals, which reinforces multiple-instance learning process. In addition, a multi-peak discovery strategy is designed to avert mislabeling instances. The proposed model is trained by stochastic gradients decent optimizer using back-propagation in an end-to-end manner. Extensive quantitative and qualitative evaluations on two publicly challenging benchmarks, PASCAL VOC 2007 and PASCAL VOC 2012, demonstrate the superiority and effectiveness of the proposed approach.  © 2021 Association for Computing Machinery.",Context information; Graph neural network; Multi-instance learning; Weakly supervised object detection,Backpropagation; Graphic methods; Learning systems; Object recognition; Stochastic models; Stochastic systems; Confidence score; Graph-based modeling; Ground truth; Multi-instance learning; Multiple instance learning; Qualitative evaluations; Relation information; Stochastic gradient; Object detection
A Survey on Healthcare Data: A Security Perspective,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108558738&doi=10.1145%2f3422816&partnerID=40&md5=055f2930371a982001f9095c55c4105b,"With the remarkable development of internet technologies, the popularity of smart healthcare has regularly come to the fore. Smart healthcare uses advanced technologies to transform the traditional medical system in an all-round way, making healthcare more efficient, more convenient, and more personalized. Unfortunately, medical data security is a serious issue in the smart healthcare systems. It becomes a fundamental challenge that requires the development of efficient innovative strategies towards fulfilling the healthcare needs and supporting secure healthcare transfer and delivery. This article provides a comprehensive survey on state-of-the-art techniques for health data security and their new trends for solving challenges in real-world applications. We survey the various notable cryptography, biometrics, watermarking, and blockchain-based security techniques for healthcare applications. A comparative analysis is also performed to identify the contribution of reviewed techniques in terms of their objective, methodology, type of medical data, important features, and limitations. At the end, we discuss the open issues and research directions to explore the promising areas for future research.  © 2021 Association for Computing Machinery.",Biometrics; Blockchain; Cryptography; Cybersecurity; Medical records; Security issues; Smart healthcare system; Watermarking,Security of data; Surveys; Advanced technology; Comparative analysis; Health care application; Important features; Innovative strategies; Internet technology; Medical systems; Smart healthcare systems; Health care
Multitarget Tracking Using Siamese Neural Networks,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108568505&doi=10.1145%2f3441656&partnerID=40&md5=c3c8fffc481884df6df1a28a5a611349,"In this article, we detect and track visual objects by using Siamese network or twin neural network. The Siamese network is constructed to classify moving objects based on the associations of object detection network and object tracking network, which are thought of as the two branches of the twin neural network. The proposed tracking method was designed for single-target tracking, which implements multitarget tracking by using deep neural networks and object detection. The contributions of this article are stated as follows. First, we implement the proposed method for visual object tracking based on multiclass classification using deep neural networks. Then, we attain multitarget tracking by combining the object detection network and the single-target tracking network. Next, we uplift the tracking performance by fusing the outcomes of the object detection network and object tracking network. Finally, we speculate on the object occlusion problem based on IoU and similarity score, which effectively diminish the influence of this issue in multitarget tracking.  © 2021 Association for Computing Machinery.",AlexNet; ResNet50; SiamFC; SiamRPN; SSD,Clutter (information theory); Deep neural networks; Neural networks; Object detection; Object recognition; Target tracking; Detection networks; Multi-class classification; Multi-target tracking; Similarity scores; Single target tracking; Tracking networks; Tracking performance; Visual object tracking; Object tracking
Visual Semantic-Based Representation Learning Using Deep CNNs for Scene Recognition,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107996230&doi=10.1145%2f3436494&partnerID=40&md5=c4aec463a29a10d25c7f4d676f81f763,"In this work, we address the task of scene recognition from image data. A scene is a spatially correlated arrangement of various visual semantic contents also known as concepts, e.g., ""chair,""""car,""""sky,""etc. Representation learning using visual semantic content can be regarded as one of the most trivial ideas as it mimics the human behavior of perceiving visual information. Semantic multinomial (SMN) representation is one such representation that captures semantic information using posterior probabilities of concepts. The core part of obtaining SMN representation is the building of concept models. Therefore, it is necessary to have ground-truth (true) concept labels for every concept present in an image. Moreover, manual labeling of concepts is practically not feasible due to the large number of images in the dataset. To address this issue, we propose an approach for generating pseudo-concepts in the absence of true concept labels. We utilize the pre-trained deep CNN-based architectures where activation maps (filter responses) from convolutional layers are considered as initial cues to the pseudo-concepts. The non-significant activation maps are removed using the proposed filter-specific threshold-based approach that leads to the removal of non-prominent concepts from data. Further, we propose a grouping mechanism to group the same pseudo-concepts using subspace modeling of filter responses to achieve a non-redundant representation. Experimental studies show that generated SMN representation using pseudo-concepts achieves comparable results for scene recognition tasks on standard datasets like MIT-67 and SUN-397 even in the absence of true concept labels. © 2021 ACM.",Pseudo-concept; pseudo-concept modeling; semantic multinomial representation; subspace modeling; support vector machine,Chemical activation; Deep learning; Large dataset; Semantics; CNN-based architecture; Filter response; Posterior probability; Scene recognition; Semantic information; Subspace modeling; Visual information; Visual semantics; Behavioral research
Introduction to the Special Issue on Recent Trends in Medical Data Security for e-Health Applications,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108533647&doi=10.1145%2f3459601&partnerID=40&md5=9d8545621fd7a02d50f202187417faa0,[No abstract available],,
A Multiple Sieve Approach Based on Artificial Intelligent Techniques and Correlation Power Analysis,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108517557&doi=10.1145%2f3433165&partnerID=40&md5=00a14bba0025ce2e60e4a118b7cd4d5e,"Side-channel analysis achieves key recovery by analyzing physical signals generated during the operation of cryptographic devices. Power consumption is one kind of these signals and can be regarded as a multimedia form. In recent years, many artificial intelligence technologies have been combined with classical side-channel analysis methods to improve the efficiency and accuracy. A simple genetic algorithm was employed in Correlation Power Analysis (CPA) when apply to cryptographic algorithms implemented in parallel. However, premature convergence caused failure in recovering the whole key, especially when plenty of large S-boxes were employed in the target primitive, such as in the case of AES. In this article, we investigate the reason of premature convergence and propose a Multiple Sieve Method (MS-CPA), which overcomes this problem and reduces the number of traces required in correlation power analysis. Our method can be adjusted to combine with key enumeration algorithms and further improves the efficiency. Simulation experimental results depict that our method reduces the required number of traces by and , compared to classic CPA and the Simple-Genetic-Algorithm-based CPA (SGA-CPA), respectively, when the success rate is fixed to . Real experiments performed on SAKURA-G confirm that the number of traces required for recovering the correct key in our method is almost equal to the minimum number that makes the correlation coefficients of correct keys stand out from the wrong ones and is much less than the numbers of traces required in CPA and SGA-CPA. When combining with key enumeration algorithms, our method has better performance. For the traces number being 200 (noise standard deviation ), the attacks success rate of our method is , which is much higher than the classic CPA with key enumeration ( success rate). Moreover, we adjust our method to work on that DPA contest v1 dataset and achieve a better result (40.04 traces) than the winning proposal (42.42 traces).  © 2021 Association for Computing Machinery.",AES; Correlation power analysis; Genetic algorithm; Multiple sieve; Parallel implementation,Artificial intelligence; Efficiency; Genetic algorithms; Information dissemination; Sieves; Artificial intelligence technologies; Artificial intelligent techniques; Correlation coefficient; Correlation power analysis; Correlation power analysis (CPA); Cryptographic algorithms; Pre-mature convergences; Simple genetic algorithm; Side channel attack
Spatial-temporal Regularized Multi-modality Correlation Filters for Tracking with Re-detection,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107938902&doi=10.1145%2f3430257&partnerID=40&md5=45fc0e52b5e6c5487fb430a222ee807a,"The development of multi-spectrum image sensing technology has brought great interest in exploiting the information of multiple modalities (e.g., RGB and infrared modalities) for solving computer vision problems. In this article, we investigate how to exploit information from RGB and infrared modalities to address two important issues in visual tracking: robustness and object re-detection. Although various algorithms that attempt to exploit multi-modality information in appearance modeling have been developed, they still face challenges that mainly come from the following aspects: (1) the lack of robustness to deal with large appearance changes and dynamic background, (2) failure in re-capturing the object when tracking loss happens, and (3) difficulty in determining the reliability of different modalities. To address these issues and perform effective integration of multiple modalities, we propose a new tracking-by-detection algorithm called Adaptive Spatial-temporal Regulated Multi-Modality Correlation Filter. Particularly, an adaptive spatial-temporal regularization is imposed into the correlation filter framework in which the spatial regularization can help to suppress effect from the cluttered background while the temporal regularization enables the adaptive incorporation of historical appearance cues to deal with appearance changes. In addition, a dynamic modality weight learning algorithm is integrated into the correlation filter training, which ensures that more reliable modalities gain more importance in target tracking. Experimental results demonstrate the effectiveness of the proposed method. © 2021 ACM.",behavior understanding; Multi-modality fusion; tracking,Learning algorithms; Object detection; Target tracking; Cluttered backgrounds; Computer vision problems; Correlation filters; Multi-spectrum images; Multiple modalities; Spatial regularizations; Temporal regularization; Tracking by detections; Object tracking
Synthesising Privacy by Design Knowledge Toward Explainable Internet of Things Application Designing in Healthcare,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108506137&doi=10.1145%2f3434186&partnerID=40&md5=32b6293369a29f387b15c71d1f1c01f5,"Privacy by Design (PbD) is the most common approach followed by software developers who aim to reduce risks within their application designs, yet it remains commonplace for developers to retain little conceptual understanding of what is meant by privacy. A vision is to develop an intelligent privacy assistant to whom developers can easily ask questions to learn how to incorporate different privacy-preserving ideas into their IoT application designs. This article lays the foundations toward developing such a privacy assistant by synthesising existing PbD knowledge to elicit requirements. It is believed that such a privacy assistant should not just prescribe a list of privacy-preserving ideas that developers should incorporate into their design. Instead, it should explain how each prescribed idea helps to protect privacy in a given application design context - this approach is defined as ""Explainable Privacy.""A total of 74 privacy patterns were analysed and reviewed using ten different PbD schemes to understand how each privacy pattern is built and how each helps to ensure privacy. Due to page limitations, we have presented a detailed analysis in Reference [3]. In addition, different real-world Internet of Things (IoT) use-cases, including a healthcare application, were used to demonstrate how each privacy pattern could be applied to a given application design. By doing so, several knowledge engineering requirements were identified that need to be considered when developing a privacy assistant. It was also found that, when compared to other IoT application domains, privacy patterns can significantly benefit healthcare applications. In conclusion, this article identifies the research challenges that must be addressed if one wishes to construct an intelligent privacy assistant that can truly augment software developers' capabilities at the design phase.  © 2021 Association for Computing Machinery.",Explainable privacy; Healthcare; Internet of Things; Knowledge engineering; Privacy; Privacy assistant; Privacy by design,Application programs; Health care; Internet of things; Application design; Conceptual understanding; Health care application; Internet of Things (IOT); IOT applications; Privacy preserving; Research challenges; Software developer; Privacy by design
An Image Privacy Protection Algorithm Based on Adversarial Perturbation Generative Networks,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107985974&doi=10.1145%2f3381088&partnerID=40&md5=16da95ef1bae086f0fcb264e8086abad,"Today, users of social platforms upload a large number of photos. These photos contain personal private information, including user identity information, which is easily gleaned by intelligent detection algorithms. To thwart this, in this work, we propose an intelligent algorithm to prevent deep neural network (DNN) detectors from detecting private information, especially human faces, while minimizing the impact on the visual quality of the image. More specifically, we design an image privacy protection algorithm by training and generating a corresponding adversarial sample for each image to defend DNN detectors. In addition, we propose an improved model based on the previous model by training an adversarial perturbation generative network to generate perturbation instead of training for each image. We evaluate and compare our proposed algorithm with other methods on wider face dataset and others by three indicators: Mean average precision, Averaged distortion, and Time spent. The results show that our method significantly interferes with DNN detectors while causing weak impact to the visual quality of images, and our improved model does speed up the generation of adversarial perturbations. © 2021 ACM.",adversarial perturbation generative network; neural networks; privacy; Social network,Deep neural networks; Privacy by design; Human faces; Intelligent Algorithms; Intelligent detection; Personal private informations; Privacy protection; Private information; User identity; Visual qualities; Image enhancement
Security and Privacy of Patient Information in Medical Systems Based on Blockchain Technology,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108504070&doi=10.1145%2f3408321&partnerID=40&md5=94e263c503e3e5958e4d88f65edcadfa,"The essence of ""blockchain""is a shared database in which information stored is un-falsifiable, traceable, open, and transparent. Therefore, to improve the security of private information in medical systems, this article uses blockchain technology to design a method to protect private information in medical systems and effectively realize anti-theft control of private information. First, the Patient-oriented Privacy Preserving Access Control model is introduced into the access control process of private information in medical systems. Next, a private information storage platform is built by using blockchain technology, and information transmission is realized using standard cryptographic algorithms. In this process, file authorization contracts are also used to guarantee the security of private information and further prevent theft of medical private information. Our simulation results show that the storage response time of this method is kept below 1,000 ms, and the maximum information throughput rate reaches 550 kbit/s, which indicates that this method has strong performance in information storage and transmission efficiency. Moreover, the reliability and bandwidth utilization of data transmission across domains is higher, so the method has higher information security control performance and superior overall performance.  © 2021 Association for Computing Machinery.",Anti-theft; Asymmetric encryption; Blockchain; Cloud database; Medical system; Privacy information,Access control; Blockchain; Crime; Digital storage; Privacy by design; Process control; Security systems; Transmissions; Access control models; Band-width utilization; Cryptographic algorithms; Information security controls; Information transmission; Patient information; Security and privacy; Transmission efficiency; Medical information systems
Introduction to the Special Issue on Advanced Approaches for Multiple Instance Learning on Multimedia Applications,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108540440&doi=10.1145%2f3459603&partnerID=40&md5=7dde7e02948cc5ac8060d41cc1f8c1a1,[No abstract available],,
Pinball Loss Twin Support Vector Clustering,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108572330&doi=10.1145%2f3409264&partnerID=40&md5=14be9d9614d78e2d13392f49554c79a1,"Twin Support Vector Clustering (TWSVC) is a clustering algorithm inspired by the principles of Twin Support Vector Machine (TWSVM). TWSVC has already outperformed other traditional plane based clustering algorithms. However, TWSVC uses hinge loss, which maximizes shortest distance between clusters and hence suffers from noise-sensitivity and low re-sampling stability. In this article, we propose Pinball loss Twin Support Vector Clustering (pinTSVC) as a clustering algorithm. The proposed pinTSVC model incorporates the pinball loss function in the plane clustering formulation. Pinball loss function introduces favorable properties such as noise-insensitivity and re-sampling stability. The time complexity of the proposed pinTSVC remains equivalent to that of TWSVC. Extensive numerical experiments on noise-corrupted benchmark UCI and artificial datasets have been provided. Results of the proposed pinTSVC model are compared with TWSVC, Twin Bounded Support Vector Clustering (TBSVC) and Fuzzy c-means clustering (FCM). Detailed and exhaustive comparisons demonstrate the better performance and generalization of the proposed pinTSVC for noise-corrupted datasets. Further experiments and analysis on the performance of the above-mentioned clustering algorithms on structural MRI (sMRI) images taken from the ADNI database, face clustering, and facial expression clustering have been done to demonstrate the effectiveness and feasibility of the proposed pinTSVC model.  © 2021 Association for Computing Machinery.",Clustering; Convex programming; Noise insensitivity; Optimization; Pinball loss; Quantile distance; Support vector machine; Twin support vector clustering; Twin support vector machine,Magnetic resonance imaging; Support vector machines; Vectors; Artificial datasets; Between clusters; Facial Expressions; Fuzzy C means clustering; Noise sensitivity; Numerical experiments; Support vector clustering; Twin support vector machines; Clustering algorithms
Lightweight Multi-party Authentication and Key Agreement Protocol in IoT-based E-Healthcare Service,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108526552&doi=10.1145%2f3398039&partnerID=40&md5=1b60e7367340987ddbf7944a370461fd,"Internet of Things (IoT) is playing a promising role in e-healthcare applications in the recent decades; nevertheless, security is one of the crucial challenges in the current field of study. Many healthcare devices (for instance, a sensor-augmented insulin pump and heart-rate sensor) collect a user's real-time data (such as glucose level and heart rate) and send them to the cloud for proper analysis and diagnosis of the user. However, the real-time user's data are vulnerable to various authentication attacks while sending through an insecure channel. Besides that, the attacks may further open scope for many other subsequent attacks. Existing security mechanisms concentrate on two-party mutual authentication. However, an IoT-enabled healthcare application involves multiple parties such as a patient, e-healthcare test-equipment, doctors, and cloud servers that requires multi-party authentication for secure communication. Moreover, the design and implementation of a lightweight security mechanism that fits into the resource constraint IoT-enabled healthcare devices are challenging. Therefore, this article proposes a lightweight, multi-party authentication and key-establishment protocol in IoT-based e-healthcare service access network to counter the attacks in resource constraint devices. The proposed multi-party protocol has used a lattice-based cryptographic construct such as Identity-Based Encryption (IBE) to acquire security, privacy, and efficiency. The study provided all-round analysis of the scheme, such as security, power consumption, and practical usage, in the following ways. The proposed scheme is tested by a formal security tool, Scyther, to testify the security properties of the protocol. In addition, security analysis for various attacks and comparison with other existing works are provided to show the robust security characteristics. Further, an experimental evaluation of the proposed scheme using IBE cryptographic construct is provided to validate the practical usage. The power consumption of the scheme is also computed and compared with existing works to evaluate its efficiency.  © 2021 Association for Computing Machinery.",Authentication; Healthcare; Identity-based encryption; Internet of Things; Key establishment; Lattice-based cryptography; Lightweight; Security protocol,Authentication; Cryptography; Electric power utilization; Energy efficiency; Equipment testing; Health care; Heart; Network security; Privacy by design; Authentication and key agreement protocols; Design and implementations; Experimental evaluation; Health care application; Identity Based Encryption; Internet of Things (IOT); Key establishment protocol; Resource-constraint devices; Internet of things
An Augmented Reality Online Assistance Platform for Repair Tasks,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108002657&doi=10.1145%2f3429285&partnerID=40&md5=ac0bab297a0a8eca8575ab870e446261,"Our augmented reality online assistance platform enables an expert to specify 6DoF movements of a component and apply the geometrical and physical constraints in real-time. We track the real components on the expert's side to monitor the operations of an expert. We leverage a remote rendering technique that we proposed previously to relieve the rendering burden of the augmented reality end devices. By conducting a user study, we show that the proposed method outperforms conventional instructional videos and sketches. The answers to the questionnaires show that the proposed method receives higher recommendation than sketching, and, compared to conventional instructional videos, is outstanding in terms of instruction clarity, preference, recommendation, and confidence of task completion. Moreover, as to the overall user experience, the proposed method has an advantage over the video method. © 2021 ACM.",Augmented reality; collaborative tools; performance evaluation; remote assistance,Augmented reality; Surveys; User experience; End-devices; Instructional videos; Physical constraints; Real components; Real time; Remote rendering; User study; Rendering (computer graphics)
Interactive Search vs. Automatic Search: An Extensive Study on Video Retrieval,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107981783&doi=10.1145%2f3429457&partnerID=40&md5=47a91bb0d46eb5cd4e34b5b43dbc3e59,"This article conducts user evaluation to study the performance difference between interactive and automatic search. Particularly, the study aims to provide empirical insights of how the performance landscape of video search changes, with tens of thousands of concept detectors freely available to exploit for query formulation. We compare three types of search modes: free-to-play (i.e., search from scratch), non-free-to-play (i.e., search by inspecting results provided by automatic search), and automatic search including concept-free and concept-based retrieval paradigms. The study involves a total of 40 participants; each performs interactive search over 15 queries of various difficulty levels using two search modes on the IACC.3 dataset provided by TRECVid organizers. The study suggests that the performance of automatic search is still far behind interactive search. Furthermore, providing users with the result of automatic search for exploration does not show obvious advantage over asking users to search from scratch. The study also analyzes user behavior to reveal insights of how users compose queries, browse results, and discover new query terms for search, which can serve as guideline for future research of both interactive and automatic search. © 2021 ACM.",ad hoc video search; automatic search; interactive search; user study; Video retrieval,Search engines; Automatic searches; Concept-based retrieval; Free to plays; Interactive search; Query formulation; User behaviors; User evaluations; Video retrieval; Behavioral research
Entropy Slicing Extraction and Transfer Learning Classification for Early Diagnosis of Alzheimer Diseases with sMRI,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107996979&doi=10.1145%2f3383749&partnerID=40&md5=123568de303f47ccdf15c0d7df4438fd,"Alzheimer's Disease (AD) is an irreversible neurogenerative disorder that undergoes progressive decline in memory and cognitive function and is characterized by structural brain Magnetic Resonance Images (sMRI). In recent years, sMRI data has played a vital role in the evaluation of brain anatomical changes, leading to early detection of AD through deep networks. The existing AD problems such as preprocessing complexity and unreliability are major concerns at present. To overcome these, a model (FEESCTL) has been proposed with an entropy slicing for feature extraction and Transfer Learning for classification. In the present study, the entropy image slicing method is attempted for selecting the most informative MRI slices during training stages. The ADNI dataset is trained on Transfer Learning adopted by VGG-16 network for classifying the AD with normal individuals. The experimental results reveal that the proposed model has achieved an accuracy level of 93.05%, 86.39%, 92.00% for binary classifications (AD/MCI, MCI/CN, AD/CN) and 93.12% for ternary classification (AD/MCI/CN), respectively, and henceforth the efficiency in diagnosing AD is proved through comparative analysis. © 2021 ACM.",Alzheimer's disease; ConvNets; convolutional neural network; deep learning; medical diagnostic imaging; sMRI; transfer learning,Classification (of information); Computer aided diagnosis; Entropy; Extraction; Magnetic resonance; Magnetic resonance imaging; Neurodegenerative diseases; Alzheimer disease; Alzheimer's disease; Anatomical changes; Binary classification; Brain magnetic resonance images; Cognitive functions; Comparative analysis; Early diagnosis; Transfer learning
Fast Accurate and Automatic Brushstroke Extraction,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107963684&doi=10.1145%2f3429742&partnerID=40&md5=6b330039c7da5de21398c3c11cff4475,"Brushstrokes are viewed as the artist's ""handwriting""in a painting. In many applications such as style learning and transfer, mimicking painting, and painting authentication, it is highly desired to quantitatively and accurately identify brushstroke characteristics from old masters' pieces using computer programs. However, due to the nature of hundreds or thousands of intermingling brushstrokes in the painting, it still remains challenging. This article proposes an efficient algorithm for brush Stroke extraction based on a Deep neural network, i.e., DStroke. Compared to the state-of-the-art research, the main merit of the proposed DStroke is to automatically and rapidly extract brushstrokes from a painting without manual annotation, while accurately approximating the real brushstrokes with high reliability. Herein, recovering the faithful soft transitions between brushstrokes is often ignored by the other methods. In fact, the details of brushstrokes in a master piece of painting (e.g., shapes, colors, texture, overlaps) are highly desired by artists since they hold promise to enhance and extend the artists' powers, just like microscopes extend biologists' powers. To demonstrate the high efficiency of the proposed DStroke, we perform it on a set of real scans of paintings and a set of synthetic paintings, respectively. Experiments show that the proposed DStroke is noticeably faster and more accurate at identifying and extracting brushstrokes, outperforming the other methods. © 2021 ACM.",Brushstroke extraction; hard and soft segmentation; painting authentication; Pix2Pix network,Application programs; Deep neural networks; Extraction; Textures; Brush stroke; High reliability; High-efficiency; Manual annotation; Soft-transition; State of the art; Painting
On the Performance Comparisons of Native and Clientless Real-Time Screen-Sharing Technologies,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107932315&doi=10.1145%2f3437881&partnerID=40&md5=c83e351ba0d1e96f5ef0821bc2202415,"Real-time screen-sharing provides users with ubiquitous access to remote applications, such as computer games, movie players, and desktop applications (apps), anywhere and anytime. In this article, we study the performance of different screen-sharing technologies, which can be classified into native and clientless ones. The native ones dictate that users install special-purpose software, while the clientless ones directly run in web browsers. In particular, we conduct extensive experiments in three steps. First, we identify a suite of the most representative native and clientless screen-sharing technologies. Second, we propose a systematic measurement methodology for comparing screen-sharing technologies under diverse and dynamic network conditions using different performance metrics. Last, we conduct extensive experiments and perform in-depth analysis to quantify the performance gap between clientless and native screen-sharing technologies. We found that our WebRTC-based implementation achieves the best overall performance. More precisely, it consumes a maximum of 3 Mbps bandwidth while reaching a high decoding ratio and delivering good video quality. Moreover, it leads to a steadily high decoding ratio and video quality under dynamic network conditions. By presenting the very first rigorous comparisons of the native and clientless screen-sharing technologies, this article will stimulate more exciting studies on the emerging clientless screen-sharing technologies. © 2021 ACM.",Live video streaming; measurements; performance evaluations; performance optimization; real-time encoding,Computer games; Decoding; Web browsers; Desktop applications; In-depth analysis; Measurement methodology; Performance comparison; Performance metrics; Remote applications; Special purpose software; Ubiquitous access; Video signal processing
Automatic Comic Generation with Stylistic Multi-page Layouts and Emotion-driven Text Balloon Generation,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107911353&doi=10.1145%2f3440053&partnerID=40&md5=82c7472d77cddcce40d983d64fae9e7b,"In this article, we propose a fully automatic system for generating comic books from videos without any human intervention. Given an input video along with its subtitles, our approach first extracts informative keyframes by analyzing the subtitles and stylizes keyframes into comic-style images. Then, we propose a novel automatic multi-page layout framework that can allocate the images across multiple pages and synthesize visually interesting layouts based on the rich semantics of the images (e.g., importance and inter-image relation). Finally, as opposed to using the same type of balloon as in previous works, we propose an emotion-aware balloon generation method to create different types of word balloons by analyzing the emotion of subtitles and audio. Our method is able to vary balloon shapes and word sizes in balloons in response to different emotions, leading to more enriched reading experience. Once the balloons are generated, they are placed adjacent to their corresponding speakers via speaker detection. Our results show that our method, without requiring any user inputs, can generate high-quality comic pages with visually rich layouts and balloons. Our user studies also demonstrate that users prefer our generated results over those by state-of-the-art comic generation systems. © 2021 ACM.",Automatic; comic books; keyframes; layout; multi-page; stylizing,Image processing; Semantics; Automatic systems; Generation method; Generation systems; High quality; Human intervention; Speaker detection; State of the art; Word balloons; Balloons
Server Allocation for Massively Multiplayer Online Cloud Games Using Evolutionary Optimization,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107915797&doi=10.1145%2f3433027&partnerID=40&md5=48dbf7a29e17c0df8b503f51868b9df7,"In recent years, Massively Multiplayer Online Games (MMOGs) are becoming popular, partially due to their sophisticated graphics and broad virtual world, and cloud gaming is demanded more than ever especially when entertaining with light and portable devices. This article considers the problem of server allocation for running MMOG on cloud, aiming to reduce the cost on cloud gaming service and meanwhile enhance the quality of service. The problem is formulated into minimizing an objective function involving the cost of server rental, the cost of data transfer and the network latency during the gaming time. A genetic algorithm is developed to solve the minimization problem for processing simultaneous server allocation for the players who log into the system at the same time while many existing players are playing the same game. Extensive experiments based on the player behavior in ""World of Warcraft""are conducted to evaluate the proposed method and compare with the state-of-the-art as well. The experimental results show that the method gives a lower cost and a shorter network latency in most of the time. © 2021 ACM.",Cloud gaming; genetic algorithm; MMOG; optimization; server allocation,Data transfer; Interactive computer graphics; Quality of service; Evolutionary optimizations; Massively multiplayer; Massively multiplayer online game (MMOGs); Minimization problems; Network latencies; Objective functions; State of the art; World of Warcraft; Genetic algorithms
SDN Enabled QoE and Security Framework for Multimedia Applications in 5G Networks,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100009497&doi=10.1145%2f3377390&partnerID=40&md5=01bf84c66f70c83d9e30b5102c142608,"The technologies for real-time multimedia transmission and immersive 3D gaming applications are rapidly emerging, posing challenges in terms of performance, security, authentication, data privacy, and encoding. The communication channel for these multimedia applications must be secure and reliable from network attack vectors and data-contents must employ strong encryption to preserve privacy and confidentiality. Towards delivering secure multimedia application environment for 5G networks, we propose an SDN/NFV (Software-Defined-Networking/Network-Function-Virtualization) framework called STREK, which attempts to deliver highly adaptable Quality-of-Experience (QoE), Security, and Authentication functions for multi-domain Cloud to Edge networks. The STREK architecture consists of a holistic SDNFV dataplane, NFV service-chaining and network slicing, a lightweight adaptable hybrid cipher scheme called TREK, and an open RESTful API for applications to deploy custom policies at runtime for multimedia services. For multi-domain/small-cell deployments, the key-generation scheme is dynamic at flow/session-level, and the handover authentication scheme uses a novel method to exchange security credentials with the Access Points (APs) of neighborhood cells. This scheme is designed to improve authentication function during handover with low overhead, delivering the 5G ultra-low latency requirements. We present the experiments with both software and hardware-based implementations and compare our solution with popular lightweight cryptographic solutions, standard open source software, and SDN-based research proposals for 5G multimedia. In the microbenchmarks, STREK achieves smaller hardware, low overhead, low computation, higher attack resistance, and offers better network performance for multimedia streaming applications. In real-time multimedia use-cases, STREK shows greater level of quality distortion for multimedia contents with minimal encryption bitrate overhead to deliver data confidentiality, immunity to common cryptanalysis, and significant resistance to communication channel attacks, in the context of low-latency 5G networks. © 2021 ACM.",5th Generation network (5G); lightweight cryptography; Multi-Access Edge Computing (MEC); multimedia communication; network security; network slicing; NFV; QoE; SDN,Application programs; Authentication; Communication channels (information theory); Cryptography; Data communication systems; Multimedia services; Network security; Open source software; Open systems; Privacy by design; Quality of service; Queueing networks; Authentication functions; Data confidentiality; Handover authentications; Multimedia applications; Multimedia streaming applications; Quality of experience (QoE); Security credentials; Software and hardwares; 5G mobile communication systems
A Multi-agent Feature Selection and Hybrid Classification Model for Parkinson's Disease Diagnosis,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107281580&doi=10.1145%2f3433180&partnerID=40&md5=0569fccf0d3cc5748ceadd147bbbbcfa,"Parkinson's disease (PD) diagnostics includes numerous analyses related to the neurological, physical, and psychical status of the patient. Medical teams analyze multiple symptoms and patient history considering verified genetic influences. The proposed method investigates the voice symptoms of this disease. The voice files are processed, and the feature extraction is conducted. Several machine learning techniques are used to recognize Parkinson's and healthy patients. This study focuses on examining PD diagnosis through voice data features. A new multi-agent feature filter (MAFT) algorithm is proposed to select the best features from the voice dataset. The MAFT algorithm is designed to select a set of features to improve the overall performance of prediction models and prevent over-fitting possibly due to extreme reduction to the features. Moreover, this algorithm aims to reduce the complexity of the prediction, accelerate the training phase, and build a robust training model. Ten different machine learning methods are then integrated with the MAFT algorithm to form a powerful voice-based PD diagnosis model. Recorded test results of the PD prediction model using the actual and filtered features yielded 86.38% and 86.67% accuracies on average, respectively. With the aid of the MAFT feature selection, the test results are improved by 3.2% considering the hybrid model (HM) and 3.1% considering the Naïve Bayesian and random forest. Subsequently, an HM, which comprises a binary convolutional neural network and three feature selection algorithms (namely, genetic algorithm, Adam optimizer, and mini-batch gradient descent), is proposed to improve the classification accuracy of the PD. The results reveal that PD achieves an overall accuracy of 93.7%. The HM is integrated with the MAFT, and the combination realizes an overall accuracy of 96.9%. These results demonstrate that the combination of the MAFT algorithm and the HM model significantly enhances the PD diagnosis outcomes.  © 2021 Association for Computing Machinery.","Convolutional Neural Network; Feature evaluation; Hybrid Classification Model; Machine learning; Multi-agent feature filter; Parkinson's disease; Voice feature, multi-agent system",Computer aided diagnosis; Convolutional neural networks; Decision trees; Forecasting; Genetic algorithms; Gradient methods; Machine learning; Multi agent systems; Neurodegenerative diseases; Partial discharges; Predictive analytics; Statistical tests; Classification accuracy; Feature selection algorithm; Hybrid classification; Machine learning methods; Machine learning techniques; Overall accuracies; Parkinson's disease; Robust trainings; Feature extraction
TripRes: Traffic Flow Prediction Driven Resource Reservation for Multimedia IoV with Edge Computing,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106695263&doi=10.1145%2f3401979&partnerID=40&md5=6e5ffcabe296eb90f470ed39bac69cd7,"The Internet of Vehicles (IoV) connects vehicles, roadside units (RSUs) and other intelligent objects, enabling data sharing among them, thereby improving the efficiency of urban traffic and safety. Currently, collections of multimedia content, generated by multimedia surveillance equipment, vehicles, and so on, are transmitted to edge servers for implementation, because edge computing is a formidable paradigm for accommodating multimedia services with low-latency resource provisioning. However, the uneven or discrete distribution of the traffic flow covered by edge servers negatively affects the service performance (e.g., overload and underload) of edge servers in multimedia IoV systems. Therefore, how to accurately schedule and dynamically reserve proper numbers of resources for multimedia services in edge servers is still challenging. To address this challenge, a traffic flow prediction driven resource reservation method, called TripRes, is developed in this article. Specifically, the city map is divided into different regions, and the edge servers in a region are treated as a ""big edge server""to simplify the complex distribution of edge servers. Then, future traffic flows are predicted using the deep spatiotemporal residual network (ST-ResNet), and future traffic flows are used to estimate the amount of multimedia services each region needs to offload to the edge servers. With the number of services to be offloaded in each region, their offloading destinations are determined through latency-sensitive transmission path selection. Finally, the performance of TripRes is evaluated using real-world big data with over 100M multimedia surveillance records from RSUs in Nanjing China. © 2021 ACM.",edge computing; multimedia IoV; residual networks; Resource reservation; traffic flow prediction,Data Sharing; Edge computing; Multimedia systems; Network security; Vehicle to vehicle communications; Vehicles; Discrete distribution; Multimedia contents; Number of services; Resource reservations; Service performance; Surveillance equipment; Traffic flow prediction; Transmission paths; Multimedia services
High-quality Frame Recurrent Video De-raining with Multi-contextual Adversarial Network,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119576488&doi=10.1145%2f3444974&partnerID=40&md5=d0eaa314830fc7d529b5f58aefa51b20,"In this article, we address the problem of rain-streak removal in the videos. Unlike the image, challenges in video restoration comprise temporal consistency besides spatial enhancement. The researchers across the world have proposed several effective methods for estimating the de-noised videos with outstanding temporal consistency. However, such methods also amplify the computational cost due to their larger size. By way of analysis, incorporating separate modules for spatial and temporal enhancement may require more computational resources. It motivates us to propose a unified architecture that directly estimates the de-rained frame with maximal visual quality and minimal computational cost. To this end, we present a deep learning-based Frame-recurrent Multi-contextual Adversarial Network for rain-streak removal in videos. The proposed model is built upon a Conditional Generative Adversarial Network (CGAN)-based framework where the generator model directly estimates the de-rained frame from the previously estimated one with the help of its multi-contextual adversary. To optimize the proposed model, we have incorporated the Perceptual loss function in addition to the conventional Euclidean distance. Also, instead of traditional entropy loss from the adversary, we propose to use the Euclidean distance between the features of de-rained and clean frames, extracted from the discriminator model as a cost function for video de-raining. Various experimental observations across 11 test sets, with over 10 state-of-the-art methods, using 14 image-quality metrics, prove the efficacy of the proposed work, both visually and computationally.  © 2021 ACM.",deep learning; generative adversarial network; Video de-raining,Cost benefit analysis; Cost functions; Deep learning; Generative adversarial networks; Image enhancement; Adversarial networks; Computational costs; Computational resources; Deep learning; Euclidean distance; High quality; Spatial enhancement; Temporal consistency; Video de-raining; Video restoration; Rain
Robust ordinal regression: User credit grading with triplet loss-based sampling,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105019107&doi=10.1145%2f3408303&partnerID=40&md5=556269326f9f5a4e4d4a96c584da5107,"With the development of social media sites, user credit grading, which served as an important and fashionable problem, has attracted substantial attention from a slew of developers and operators of mobile applications. In particular, multi-grades of user credit aimed to achieve (1) anomaly detection and risk early warning and (2) personalized information and service recommendation for privileged users. The above two goals still remained as up-to-date challenges. To these ends, in this article, we propose a novel regression-based method. Technically speaking, we define three natural ordered categories including BlockList, GeneralList, and AllowList according to users' registration and behavior information, which preserve both the global hierarchical relationship of user credit and the local coincident features of users, and hence formulate user credit grading as the ordinal regression problem. Our method is inspired by KDLOR (kernel discriminant learning for ordinal regression), which is an effective and efficient model to solve ordinal regression by mapping high-dimension samples to the discriminant region with supervised conditions. However, the performance of KDLOR is fragile to the extreme imbalanced distribution of users. To address this problem, we propose a robust sampling model to balance distribution and avoid overfit or underfit learning, which induces the triplet metric constraint to obtain hard negative samples that well represent the latent ordered class information. A step further, another salient problem lies in ambiguous samples that are noises or located in the classification boundary to impede optimized mapping and embedding. To this problem, we improve sampling by identifying and evading noises in triplets to obtain hard negative samples to enhance robustness and effectiveness for ordinal regression. We organized training and testing datasets for user credit grading by selecting limited items from real-life huge tables of users in the mobile application, which are used in similar problems; moreover, we theoretically and empirically demonstrate the advantages of the proposed model over established datasets.  © 2021 Association for Computing Machinery.",hard negative sampling; metric learning; ordinal regression; User credit evaluation,Anomaly detection; Learning systems; Mapping; Mobile computing; Classification boundary; Mobile applications; Ordinal regression; Personalized information; Risk early warning; Robust ordinal regressions; Service recommendations; Training and testing; Grading
Zero-shot cross-modal retrieval by assembling autoencoder and generative adversarial network,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104987729&doi=10.1145%2f3424341&partnerID=40&md5=7ebe14177c24c08e96dc3f6ad2ad6c75,"Conventional cross-modal retrieval models mainly assume the same scope of the classes for both the training set and the testing set. This assumption limits their extensibility on zero-shot cross-modal retrieval (ZS-CMR), where the testing set consists of unseen classes that are disjoint with seen classes in the training set. The ZS-CMR task is more challenging due to the heterogeneous distributions of different modalities and the semantic inconsistency between seen and unseen classes. A few of recently proposed approaches are inspired by zero-shot learning to estimate the distribution underlying multimodal data by generative models and make the knowledge transfer from seen classes to unseen classes by leveraging class embeddings. However, directly borrowing the idea from zero-shot learning (ZSL) is not fully adaptive to the retrieval task, since the core of the retrieval task is learning the common space. To address the above issues, we propose a novel approach named Assembling AutoEncoder and Generative Adversarial Network (AAEGAN), which combines the strength of AutoEncoder (AE) and Generative Adversarial Network (GAN), to jointly incorporate common latent space learning, knowledge transfer, and feature synthesis for ZS-CMR. Besides, instead of utilizing class embeddings as common space, the AAEGAN approach maps all multimodal data into a learned latent space with the distribution alignment via three coupled AEs. We empirically show the remarkable improvement for ZS-CMR task and establish the state-of-the-art or competitive performance on four image-text retrieval datasets.  © 2021 Association for Computing Machinery.",Cross-modal retrieval; feature synthesis; zero-shot learning,Embeddings; Image enhancement; Knowledge management; Semantics; Adversarial networks; Competitive performance; Feature synthesis; Heterogeneous distributions; Knowledge transfer; Multi-modal data; Semantic inconsistencies; State of the art; Learning systems
RICA-MD: A Refined ICA algorithm for motion detection,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104942893&doi=10.1145%2f3416492&partnerID=40&md5=228a91026fd5b634989d4b76748070a7,"With the rapid development of various computing technologies, the constraints of data processing capabilities gradually disappeared, and more data can be simultaneously processed to obtain better performance compared to conventional methods. As a standard statistical analysis method that has been widely used in many fields, Independent Component Analysis (ICA) provides a new way for motion detection by extracting the foreground without precisely modeling the background. However, most existing ICA-based motion detection algorithms use only two-channel data for source separation and simply generate the observation vectors by decomposing and reconstructing the images by row, hence they cannot obtain an integrated and accurate shape of the moving objects in complex scenes. In this article, we propose a refined ICA algorithm for motion detection (RICA-MD), which fuses a larger number of channels than conventional ICA-based motion detection algorithms to provide more effective information for foreground extraction. Meanwhile, we propose four novel methods for generating observation vectors to further cover the diverse motion styles of the moving objects. These improvements enable RICA-MD to effectively deal with slowly moving objects, which are difficult to detect using conventional methods. Our quantitative evaluation in multiple scenes shows that our proposed method is able to achieve a better performance at an acceptable cost of false alarms.  © 2021 Association for Computing Machinery.",Independent Component Analysis (ICA); Motion detection; multi-channel input; observation signal reconstruction,Data handling; Independent component analysis; Signal detection; Source separation; Computing technology; Conventional methods; Foreground extraction; Independent component analysis(ICA); Observation vectors; Processing capability; Quantitative evaluation; Standard statistical analysis; Motion analysis
Lightweight single image super-resolution with dense connection distillation network,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105013000&doi=10.1145%2f3414838&partnerID=40&md5=f9b126d488d1ca8c30b53d1c40a919f4,"Single image super-resolution attempts to reconstruct a high-resolution (HR) image from its corresponding low-resolution (LR) image, which has been a research hotspot in computer vision and image processing for decades. To improve the accuracy of super-resolution images, many works adopt very deep networks to model the translation from LR to HR, resulting in memory and computation consumption. In this article, we design a lightweight dense connection distillation network by combining the feature fusion units and dense connection distillation blocks (DCDB) that include selective cascading and dense distillation components. The dense connections are used between and within the distillation block, which can provide rich information for image reconstruction by fusing shallow and deep features. In each DCDB, the dense distillation module concatenates the remaining feature maps of all previous layers to extract useful information, the selected features are then assessed by the proposed layer contrast-aware channel attention mechanism, and finally the cascade module aggregates the features. The distillation mechanism helps to reduce training parameters and improve training efficiency, and the layer contrast-aware channel attention further improves the performance of model. The quality and quantity experimental results on several benchmark datasets show the proposed method performs better tradeoff in term of accuracy and efficiency.  © 2021 Association for Computing Machinery.",attention mechanism; Dense connection; single image super-resolution,Benchmarking; Distillation; Distilleries; Efficiency; Image reconstruction; Optical resolving power; Attention mechanisms; Benchmark datasets; Feature fusion; High resolution image; Low resolution images; Super resolution; Training efficiency; Training parameters; Image enhancement
SPGAN: Face forgery using spoofing generative adversarial networks,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104960247&doi=10.1145%2f3432817&partnerID=40&md5=ebebab23634238258c73a28f90f3e677,"Current face spoof detection schemes mainly rely on physiological cues such as eye blinking, mouth movements, and micro-expression changes, or textural attributes of the face images [9]. But none of these methods represent a viable mechanism for makeup-induced spoofing, especially since makeup has been widely used. Compared with face alteration techniques such as plastic surgery, makeup is non-permanent and cost efficient, which makes makeup-induced spoofing become a realistic threat to the integrity of a face recognition system. To solve this problem, we propose a generative model to construct spoofing face images (confusing face images) for improving the accuracy and robustness of automatic face recognition. Our network structure is composed of two separate parts, with one using inter-attention mechanism to obtain interested face region, and another using intra-attention to translate imitation style with preserving imitation style-excluding details. These two attention mechanisms can precisely learn imitation style, where inter-attention pays more attention to imitation regions of image and intra-attention learns face attributes with long distance in image. To effectively discriminate generated images, we introduce an imitation style discriminator. Our model (SPGAN) generates face images that transfer the imitation style from target to subject image and preserve the imitation-excluding features. Experimental results demonstrate the performance of our model in improving quality of imitated face images.  © 2021 Association for Computing Machinery.",Imitated dataset; imitation style transfer; inter-intra attention mechanism; style classification constraint,Eye movements; Image enhancement; Adversarial networks; Attention mechanisms; Automatic face recognition; Face recognition systems; Generative model; Micro-expressions; Network structures; Textural attributes; Face recognition
"A multimodal, multimedia point-of-care deep learning framework for COVID-19 diagnosis",2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104932685&doi=10.1145%2f3421725&partnerID=40&md5=f11c2970699fc59a153d4f0e77bdce26,"In this article, we share our experiences in designing and developing a suite of deep neural network-(DNN) based COVID-19 case detection and recognition framework. Existing pathological tests such as RT-PCR-based pathogen RNA detection from nasal swabbing seem to display low detection rates during the early stages of virus contraction. Moreover, the reliance on a few overburdened laboratories based around an epicenter capable of supplying large numbers of RT-PCR tests makes this testing method non-scalable when the rate of infections is high. Similarly, finding an effective drug or vaccine with which to combat COVID-19 requires a long time and many clinical trials. The development of pathological COVID-19 tests is hindered by shortages in the supply chain of chemical reagents necessary for testing on a large scale. This diminishes the speed of diagnosis and the ability to filter out COVID-19 positive patients from uninfected patients on a national level. Existing research has shown that DNN has been successful in identifying COVID-19 from radiological media such as CT scans and X-ray images, audio media such as cough sounds, optical coherence tomography to identify conjunctivitis and pink eye symptoms on the ocular surface, body temperature measurement using smartphone fingerprint sensors or thermal cameras, the use of live facial detection to identify safe social distancing practices from camera images, and face mask detection from camera images. We also investigate the utility of federated learning in diagnosis cases where private data can be trained via edge learning. These point-of-care modalities can be integrated with DNN-based RT-PCR laboratory test results to assimilate multiple modalities of COVID-19 detection and thereby provide more dimensions of diagnosis. Finally, we will present our initial test results, which are encouraging.  © 2021 Association for Computing Machinery.",COVID-19 pandemic; Deep Learning; point-of-care,Audio acoustics; Cameras; Computerized tomography; Deep neural networks; Diagnosis; Face recognition; Optical tomography; Polymerase chain reaction; Supply chains; Temperature measurement; Viruses; Body temperature measurements; Chemical reagents; Detection rates; Fingerprint sensors; Laboratory test; Learning frameworks; Multiple modalities; Rate of infections; Deep learning
Urban perception: Sensing cities via a deep interactive multi-task learning framework,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105016613&doi=10.1145%2f3424115&partnerID=40&md5=90a5318aa0f94f76964157284ee1bd95,"Social scientists have shown evidence that visual perceptions of urban attributes, such as safe, wealthy, and beautiful perspectives of the given cities, are highly correlated to the residents' behaviors and quality of life. Despite their significance, measuring visual perceptions of urban attributes is challenging due to the following facts: (1) Visual perceptions are subjectively contradistinctive rather than absolute. (2) Perception comparisons between image pairs are usually conducted region by region, and highly related to the specific urban attributes. And (3) the urban attributes have both the shared and specific information. To address these problems, in this article, we present a Deep inteRActive Multi-task leArning scheme, DRAMA for short. DRAMA comparatively quantifies the perceptions of urban attributes by jointly integrating the pairwise comparisons, regional interactions, and urban attribute correlations within a unified deep scheme. In DRAMA, each urban attribute is treated as a task, whereby the task-sharing and the task-specific information is fully explored. By conducting extensive experiments over a public large-scale benchmark dataset, it is demonstrated that our proposed DRAMA scheme outperforms several state-of-the-art baselines. Meanwhile, we applied the pairwise comparisons of our DRAMA model to further quantify the urban attributes and hence rank cities with respect to the given urban attributes. As a byproduct, we have released the codes and parameter settings to facilitate other researches.  © 2021 Association for Computing Machinery.",deep multi-task learning; regional interactions; urban attributes; Urban perception,Deep learning; Large dataset; Learning systems; Vision; Benchmark datasets; Highly-correlated; Pair-wise comparison; Parameter setting; Social scientists; Specific information; State of the art; Visual perception; Multi-task learning
Introduction to the special issue on fine-grained visual computing,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105019326&doi=10.1145%2f3447532&partnerID=40&md5=52527af6238f43ef1a916aae66540efd,[No abstract available],,
Gaussian mixture model clustering with incomplete data,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104966404&doi=10.1145%2f3408318&partnerID=40&md5=39bf8f7e1fc73d537e9489193c58175a,"Gaussian mixture model (GMM) clustering has been extensively studied due to its effectiveness and efficiency. Though demonstrating promising performance in various applications, it cannot effectively address the absent features among data, which is not uncommon in practical applications. In this article, different from existing approaches that first impute the absence and then perform GMM clustering tasks on the imputed data, we propose to integrate the imputation and GMM clustering into a unified learning procedure. Specifically, the missing data is filled by the result of GMM clustering, and the imputed data is then taken for GMM clustering. These two steps alternatively negotiate with each other to achieve optimum. By this way, the imputed data can best serve for GMM clustering. A two-step alternative algorithm with proved convergence is carefully designed to solve the resultant optimization problem. Extensive experiments have been conducted on eight UCI benchmark datasets, and the results have validated the effectiveness of the proposed algorithm.  © 2021 Association for Computing Machinery.",clustering; EM; GMM; incomplete data,Computer networks; Alternative algorithms; Benchmark datasets; Effectiveness and efficiencies; Gaussian Mixture Model; Incomplete data; Learning procedures; Missing data; Optimization problems; Gaussian distribution
Chinese image captioning via fuzzy attention-based DenseNet-BiLSTM,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104945230&doi=10.1145%2f3422668&partnerID=40&md5=f0d46a894a4ff698f16cdfd87bc8e974,"Chinese image description generation tasks usually have some challenges, such as single-feature extraction, lack of global information, and lack of detailed description of the image content. To address these limitations, we propose a fuzzy attention-based DenseNet-BiLSTM Chinese image captioning method in this article. In the proposed method, we first improve the densely connected network to extract features of the image at different scales and to enhance the model's ability to capture the weak features. At the same time, a bidirectional LSTM is used as the decoder to enhance the use of context information. The introduction of an improved fuzzy attention mechanism effectively improves the problem of correspondence between image features and contextual information. We conduct experiments on the AI Challenger dataset to evaluate the performance of the model. The results show that compared with other models, our proposed model achieves higher scores in objective quantitative evaluation indicators, including BLEU, BLEU, METEOR, ROUGEl, and CIDEr. The generated description sentence can accurately express the image content.  © 2021 Association for Computing Machinery.",BiLSTM; DenseNet; fuzzy attention; Image captioning,Image enhancement; Attention mechanisms; Context information; Contextual information; Densely connected networks; Global informations; Image captioning; Image descriptions; Quantitative evaluation; Long short-term memory
Introduction to Big Multimodal Multimedia Data with Deep Analytics,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104960278&doi=10.1145%2f3447530&partnerID=40&md5=da201a673572d5604a66dd0d5c974dfc,[No abstract available],,
Alignment enhancement network for fine-grained visual categorization,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104988856&doi=10.1145%2f3446208&partnerID=40&md5=580f01af2a993ba49033dd9db5efda3e,"Fine-grained visual categorization (FGVC) aims to automatically recognize objects from different sub-ordinate categories. Despite attracting considerable attention from both academia and industry, it remains a challenging task due to subtle visual differences among different classes. Cross-layer feature aggregation and cross-image pairwise learning become prevailing in improving the performance of FGVC by extracting discriminative class-specific features. However, they are still inefficient to fully use the cross-layer information based on the simple aggregation strategy, while existing pairwise learning methods also fail to explore long-range interactions between different images. To address these problems, we propose a novel Alignment Enhancement Network (AENet), including two-level alignments, Cross-layer Alignment (CLA) and Cross-image Alignment (CIA). The CLA module exploits the cross-layer relationship between low-level spatial information and high-level semantic information, which contributes to cross-layer feature aggregation to improve the capacity of feature representation for input images. The new CIA module is further introduced to produce the aligned feature map, which can enhance the relevant information as well as suppress the irrelevant information across the whole spatial region. Our method is based on an underlying assumption that the aligned feature map should be closer to the inputs of CIA when they belong to the same category. Accordingly, we establish Semantic Affinity Loss to supervise the feature alignment within each CIA block. Experimental results on four challenging datasets show that the proposed AENet achieves the state-of-the-art results over prior arts.  © 2021 Association for Computing Machinery.",feature aggregation; Fine-grained visual categorization; image classification,Alignment; Arts computing; Learning systems; Semantics; Aggregation strategy; Class specific features; Feature aggregation; Feature representation; High level semantics; Long range interactions; Spatial informations; Visual categorization; Image enhancement
Exploring image enhancement for salient object detection in low light images,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105006742&doi=10.1145%2f3414839&partnerID=40&md5=73da346fce676249f42c716fc485c724,"Low light images captured in a non-uniform illumination environment usually are degraded with the scene depth and the corresponding environment lights. This degradation results in severe object information loss in the degraded image modality, which makes the salient object detection more challenging due to low contrast property and artificial light influence. However, existing salient object detection models are developed based on the assumption that the images are captured under a sufficient brightness environment, which is impractical in real-world scenarios. In this work, we propose an image enhancement approach to facilitate the salient object detection in low light images. The proposed model directly embeds the physical lighting model into the deep neural network to describe the degradation of low light images, in which the environment light is treated as a point-wise variate and changes with local content. Moreover, a Non-Local-Block Layer is utilized to capture the difference of local content of an object against its local neighborhood favoring regions. To quantitative evaluation, we construct a low light Images dataset with pixel-level human-labeled ground-truth annotations and report promising results on four public datasets and our benchmark dataset.  © 2021 Association for Computing Machinery.",images enhancement; Low light images; non-local-block layer; physical lighting model; salient object detection,Deep neural networks; Object detection; Object recognition; Artificial light; Benchmark datasets; Local neighborhoods; Non-uniform illumination; Object information; Quantitative evaluation; Real-world scenario; Salient object detection; Image enhancement
DLRF-Net: A Progressive Deep Latent Low-Rank Fusion Network for Hierarchical Subspace Discovery,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105001682&doi=10.1145%2f3402030&partnerID=40&md5=c8759fe4577884f237256598cc70a296,"Low-rank coding-based representation learning is powerful for discovering and recovering the subspace structures in data, which has obtained an impressive performance; however, it still cannot obtain deep hidden information due to the essence of single-layer structures. In this article, we investigate the deep low-rank representation of images in a progressive way by presenting a novel strategy that can extend existing single-layer latent low-rank models into multiple layers. Technically, we propose a new progressive Deep Latent Low-Rank Fusion Network (DLRF-Net) to uncover deep features and the clustering structures embedded in latent subspaces. The basic idea of DLRF-Net is to progressively refine the principal and salient features in each layer from previous layers by fusing the clustering and projective subspaces, respectively, which can potentially learn more accurate features and subspaces. To obtain deep hidden information, DLRF-Net inputs shallow features from the last layer into subsequent layers. Then, it aims at recovering the hierarchical information and deeper features by respectively congregating the subspaces in each layer of the network. As such, one can also ensure the representation learning of deeper layers to remove the noise and discover the underlying clean subspaces, which will be verified by simulations. It is noteworthy that the framework of our DLRF-Net is general and is applicable to most existing latent low-rank representation models, i.e., existing latent low-rank models can be easily extended to the multilayer scenario using DLRF-Net. Extensive results on real databases show that our framework can deliver enhanced performance over other related techniques.  © 2021 Association for Computing Machinery.",clustering; deep latent low-rank fusion network; Hierarchical subspace discovery; image representation,Network layers; Hidden information; Hierarchical information; Low-rank representations; Multiple layers; Novel strategies; Projective subspace; Salient features; Single-layer structure; Learning to rank
HCMSL: Hybrid cross-modal similarity learning for cross-modal retrieval,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104989289&doi=10.1145%2f3412847&partnerID=40&md5=4c9a9e80f50e16c3b8a75a282f5ebf90,"The purpose of cross-modal retrieval is to find the relationship between different modal samples and to retrieve other modal samples with similar semantics by using a certain modal sample. As the data of different modalities presents heterogeneous low-level feature and semantic-related high-level features, the main problem of cross-modal retrieval is how to measure the similarity between different modalities. In this article, we present a novel cross-modal retrieval method, named Hybrid Cross-Modal Similarity Learning model (HCMSL for short). It aims to capture sufficient semantic information from both labeled and unlabeled cross-modal pairs and intra-modal pairs with same classification label. Specifically, a coupled deep fully connected networks are used to map cross-modal feature representations into a common subspace. Weight-sharing strategy is utilized between two branches of networks to diminish cross-modal heterogeneity. Furthermore, two Siamese CNN models are employed to learn intra-modal similarity from samples of same modality. Comprehensive experiments on real datasets clearly demonstrate that our proposed technique achieves substantial improvements over the state-of-the-art cross-modal retrieval techniques.  © 2021 Association for Computing Machinery.",Cross-modal retrieval; deep learning; hybrid cross-modal similarity; intra-modal semantic correlation,Classification (of information); Classification labels; Feature representation; Fully connected networks; High-level features; Low-level features; Retrieval techniques; Semantic information; Similarity learning; Semantics
Fine-grained visual computing based on deep learning,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104963855&doi=10.1145%2f3418215&partnerID=40&md5=f4e455b7889f2c5ba90fdd2652cea1dc,"With increasing amounts of information, the image information received by people also increases exponentially. To perform fine-grained categorization and recognition of images and visual calculations, this study combines the Visual Geometry Group Network 16 model of convolutional neural networks and the vision attention mechanism to build a multi-level fine-grained image feature categorization model. Finally, the TensorFlow platform is utilized to simulate the fine-grained image classification model based on the visual attention mechanism. The results show that in terms of accuracy and required training time, the fine-grained image categorization effect of the multi-level feature categorization model constructed by this study is optimal, with an accuracy rate of 85.3% and a minimum training time of 108 s. In the similarity effect analysis, it is found that the chi-square distance between Log Gabor features and the degree of image distortion show a strong positive correlation; in addition, the validity of this measure is verified. Therefore, through the research in this study, it is found that the constructed fine-grained image categorization model has higher accuracy in image recognition categorization, shorter training time, and significantly better performance in similar feature effects, which provides an experimental reference for the visual computing of fine-grained images in the future.  © 2021 Association for Computing Machinery.",convolutional neural network; Fine-grained; image classification; visual attention mechanism; visual computing,Behavioral research; Convolutional neural networks; Image recognition; Imaging systems; Attention mechanisms; Chi Square distance; Classification models; Image Categorization; Image distortions; Image information; Positive correlations; Visual attention mechanisms; Deep learning
Compatibility-aware web API recommendation for mashup creation via textual description mining,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104930322&doi=10.1145%2f3417293&partnerID=40&md5=840916d3ad82c7287917e243a03b9c91,"With the ever-increasing prosperity of web Application Programming Interface (API) sharing platforms, it is becoming an economic and efficient way for software developers to design their interested mashups through web API re-use. Generally, a software developer can browse, evaluate, and select his or her preferred web APIs from the API's sharing platforms to create various mashups with rich functionality. The big volume of candidate APIs places a heavy burden on software developers' API selection decisions. This, in turn, calls for the support of intelligent API recommender systems. However, existing API recommender systems often face two challenges. First, they focus more on the functional accuracy of APIs while neglecting the APIs' actual compatibility. This then creates incompatible mashups. Second, they often require software developers to input a set of keywords that can accurately describe the expected functions of the mashup to be developed. This second challenge tests partial developers who have little background knowledge in the fields. To tackle the above-mentioned challenges, in this article we propose a compatibility-aware and text description-driven web API recommendation approach (named WARtext). WARtext guarantees the compatibility among the recommended APIs by utilizing the APIs' composition records produced by historical mashup creations. Besides, WARtext entitles a software developer to type a simple text document that describes the expected mashup functions as input. Then through textual description mining, WARtext can precisely capture the developers' functional requirements and then return a set of APIs with the highest compatibility. Finally, through a real-world mashup dataset ProgrammableWeb, we validate the feasibility of our novel approach.  © 2021 Association for Computing Machinery.",APIs recommendation; compatibility; mashup creation; Text document; textual description mining,Application programs; Computer software reusability; Recommender systems; Text mining; Back-ground knowledge; Challenge tests; Functional requirement; Selection decisions; Sharing platforms; Software developer; Textual description; WEB application; Application programming interfaces (API)
Dynamic graph learning convolutional networks for semi-supervised classification,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104997452&doi=10.1145%2f3412846&partnerID=40&md5=fd7830e1c03c05fe75424927df2141f4,"Over the past few years, graph representation learning (GRL) has received widespread attention on the feature representations of the non-Euclidean data. As a typical model of GRL, graph convolutional networks (GCN) fuse the graph Laplacian-based static sample structural information. GCN thus generalizes convolutional neural networks to acquire the sample representations with the variously high-order structures. However, most of existing GCN-based variants depend on the static data structural relationships. It will result in the extracted data features lacking of representativeness during the convolution process. To solve this problem, dynamic graph learning convolutional networks (DGLCN) on the application of semi-supervised classification are proposed. First, we introduce a definition of dynamic spectral graph convolution operation. It constantly optimizes the high-order structural relationships between data points according to the loss values of the loss function, and then fits the local geometry information of data exactly. After optimizing our proposed definition with the one-order Chebyshev polynomial, we can obtain a single-layer convolution rule of DGLCN. Due to the fusion of the optimized structural information in the learning process, multi-layer DGLCN can extract richer sample features to improve classification performance. Substantial experiments are conducted on citation network datasets to prove the effectiveness of DGLCN. Experiment results demonstrate that the proposed DGLCN obtains a superior classification performance compared to several existing semi-supervised classification models.  © 2021 Association for Computing Machinery.",graph convolutional networks; Graph representation learning; semi-supervised classification,Classification (of information); Convolution; Polynomials; Semi-supervised learning; Chebyshev polynomials; Classification performance; Convolutional networks; Feature representation; Graph representation; Semi-supervised classification; Structural information; Structural relationship; Convolutional neural networks
"Survey on deep multi-modal data analytics: Collaboration, rivalry, and fusion",2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104953718&doi=10.1145%2f3408317&partnerID=40&md5=5816dd3d0f2416f22c6797183bc7d5a8,"With the development of web technology, multi-modal or multi-view data has surged as a major stream for big data, where each modal/view encodes individual property of data objects. Often, different modalities are complementary to each other. This fact motivated a lot of research attention on fusing the multi-modal feature spaces to comprehensively characterize the data objects. Most of the existing state-of-the-arts focused on how to fuse the energy or information from multi-modal spaces to deliver a superior performance over their counterparts with single modal. Recently, deep neural networks have been exhibited as a powerful architecture to well capture the nonlinear distribution of high-dimensional multimedia data, so naturally does for multi-modal data. Substantial empirical studies are carried out to demonstrate its advantages that are benefited from deep multi-modal methods, which can essentially deepen the fusion from multi-modal deep feature spaces. In this article, we provide a substantial overview of the existing state-of-the-arts in the field of multi-modal data analytics from shallow to deep spaces. Throughout this survey, we further indicate that the critical components for this field go to collaboration, adversarial competition, and fusion over multi-modal spaces. Finally, we share our viewpoints regarding some future directions in this field.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep neural networks; Multi-modal data,Data Analytics; Deep neural networks; Modal analysis; Surveys; Critical component; Empirical studies; High-dimensional; Multi-modal data; Multi-view datum; Non-linear distribution; State of the art; Web technologies; Data streams
Cumulative Quality Modeling for HTTP Adaptive Streaming,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104472797&doi=10.1145%2f3423421&partnerID=40&md5=d68f20d20d672cbf04654feac340fee9,"HTTP Adaptive Streaming has become the de facto choice for multimedia delivery. However, the quality of adaptive video streaming may fluctuate strongly during a session due to throughput fluctuations. So, it is important to evaluate the quality of a streaming session over time. In this article, we propose a model to estimate the cumulative quality for HTTP Adaptive Streaming. In the model, a sliding window of video segments is employed as the basic building block. Through statistical analysis using a subjective dataset, we identify four important components of the cumulative quality model, namely the minimum window quality, the last window quality, the maximum window quality, and the average window quality. Experiment results show that the proposed model achieves high prediction performance and outperforms related quality models. In addition, another advantage of the proposed model is its simplicity and effectiveness for deployment in real-time estimation. Our subjective dataset as well as the source code of the proposed model have been made publicly available at https://sites.google.com/site/huyenthithanhtran1191/cqmdatabase.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adaptive video streaming; Cumulative quality; quality model; quality of experience,HTTP; Adaptive streaming; Adaptive video streaming; Basic building block; Multimedia delivery; Prediction performance; Quality modeling; Real-time estimation; Streaming sessions; Quality control
Improved Jitter Buffer Management for WebRTC,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104478893&doi=10.1145%2f3410449&partnerID=40&md5=f0e023451fcb138678d7de11962d4a5c,"This work studies the jitter buffer management algorithm for Voice over IP in WebRTC. In particular, it details the core concepts of WebRTC's jitter buffer management. Furthermore, it investigates how jitter buffer management algorithm behaves under network conditions with packet bursts. It also proposes an approach, different from the default WebRTC algorithm, to avoid distortions that occur under such network conditions. Under packet bursts, when the packet buffer becomes full, the WebRTC jitter buffer algorithm may discard all the packets in the buffer to make room for incoming packets. The proposed approach offers a novel strategy to minimize the number of packets discarded in the presence of packet bursts. Therefore, voice quality as perceived by the user is improved. ITU-T Rec. P.863, which also confirms the improvement, is employed to objectively evaluate the listening quality.  © 2021 Association for Computing Machinery.",jitter buffer management; voice quality; WebRTC,Voice/data communication systems; Incoming packets; Jitter buffer management; Jitter buffers; Listening qualities; Network condition; Novel strategies; Packet buffers; Voice over IP; Jitter
Multi-task Learning-based All-in-one Collaboration Framework for Degraded Image Super-resolution,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104480696&doi=10.1145%2f3417333&partnerID=40&md5=3163091c2ef72b8f6d677465a2a86870,"In this article, we address the degraded image super-resolution problem in a multi-task learning (MTL) manner. To better share representations between multiple tasks, we propose an all-in-one collaboration framework (ACF) with a learnable ""junction""unit to handle two major problems that exist in MTL-""How to share""and ""How much to share.""Specifically, ACF consists of a sharing phase and a reconstruction phase. Considering the intrinsic characteristic of multiple image degradations, we propose to first deal with the compression artifact, motion blur, and spatial structure information of the input image in parallel under a three-branch architecture in the sharing phase. Subsequently, in the reconstruction phase, we up-sample the previous features for high-resolution image reconstruction with a channel-wise and spatial attention mechanism. To coordinate two phases, we introduce a learnable ""junction""unit with a dual-voting mechanism to selectively filter or preserve shared feature representations that come from sharing phase, learning an optimal combination for the following reconstruction phase. Finally, a curriculum learning-based training scheme is further proposed to improve the convergence of the whole framework. Extensive experimental results on synthetic and real-world low-resolution images show that the proposed all-in-one collaboration framework not only produces favorable high-resolution results while removing serious degradation, but also has high computational efficiency, outperforming state-of-the-art methods. We also have applied ACF to some image-quality sensitive practical task, such as pose estimation, to improve estimation accuracy of low-resolution images.  © 2021 Association for Computing Machinery.",all-in-one collaboration; Degraded image super-resolution; multi-task learning; optimal combination,Computational efficiency; Image enhancement; Image reconstruction; Image segmentation; Learning systems; Linearization; Optical resolving power; Collaboration framework; Compression artifacts; Feature representation; High resolution image reconstruction; Intrinsic characteristics; Low resolution images; Spatial structure information; State-of-the-art methods; Multi-task learning
Socializing the Videos: A Multimodal Approach for Social Relation Recognition,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104402836&doi=10.1145%2f3416493&partnerID=40&md5=57c31359d089e0802f0ef39ba8f9cb6c,"As a crucial task for video analysis, social relation recognition for characters not only provides semantically rich description of video content but also supports intelligent applications, e.g., video retrieval and visual question answering. Unfortunately, due to the semantic gap between visual and semantic features, traditional solutions may fail to reveal the accurate relations among characters. At the same time, the development of social media platforms has now promoted the emergence of crowdsourced comments, which may enhance the recognition task with semantic and descriptive cues. To that end, in this article, we propose a novel multimodal-based solution to deal with the character relation recognition task. Specifically, we capture the target character pairs via a search module and then design a multistream architecture for jointly embedding the visual and textual information, in which feature fusion and attention mechanism are adapted for better integrating the multimodal inputs. Finally, supervised learning is applied to classify character relations. Experiments on real-world data sets validate that our solution outperforms several competitive baselines.  © 2021 Association for Computing Machinery.",multimodal learning; natural language processing; person search; Social relation recognition,Computer networks; Attention mechanisms; Intelligent applications; Multi-modal approach; Multi-stream architecture; Question Answering; Semantic features; Social media platforms; Textual information; Semantics
Attribute-wise Explainable Fashion Compatibility Modeling,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104402032&doi=10.1145%2f3425636&partnerID=40&md5=ca7951f355c9a2276863240c908470ee,"With the boom of the fashion market and people's daily needs for beauty, clothing matching has gained increased research attention. In a sense, tackling this problem lies in modeling the human notions of the compatibility between fashion items, i.e., Fashion Compatibility Modeling (FCM), which plays an important role in a wide bunch of commercial applications, including clothing recommendation and dressing assistant. Recent advances in multimedia processing have shown remarkable effectiveness in accurate compatibility evaluation. However, these studies work like a black box and cannot provide appropriate explanations, which are indeed of importance for gaining users' trust and improving their experience. In fact, fashion experts usually explain the compatibility evaluation through the matching patterns between fashion attributes (e.g., a silk tank top cannot go with a knit dress). Inspired by this, we devise an attribute-wise explainable FCM solution, named ExFCM, which can simultaneously generate the item-level compatibility evaluation for input fashion items and the attribute-level explanations for the evaluation result. In particular, ExFCM consists of two key components: attribute-wise representation learning and attribute interaction modeling. The former works on learning the region-aware attribute representation for each item with the threshold global average pooling. Besides, the latter is responsible for compiling the attribute-level matching signals into the overall compatibility evaluation adaptively with the attentive interaction mechanism. Note that ExFCM is trained without any attribute-level compatibility annotations, which facilitates its practical applications. Extensive experiments on two real-world datasets validate that ExFCM can generate more accurate compatibility evaluations than the existing methods, together with reasonable explanations.  © 2021 Association for Computing Machinery.",attribute-wise learning; explainable compatibility modeling; Fashion analysis,Computer networks; Attribute interactions; Commercial applications; Compatibility evaluations; Evaluation results; Interaction mechanisms; Matching patterns; Multimedia processing; Real-world datasets
GreyReID: A Novel Two-stream Deep Framework with RGB-grey Information for Person Re-identification,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104483176&doi=10.1145%2f3419439&partnerID=40&md5=3d6df40f65ae1f8cbd2daae1409b8aa2,"In this article, we observe that most false positive images (i.e., different identities with query images) in the top ranking list usually have the similar color information with the query image in person re-identification (Re-ID). Meanwhile, when we use the greyscale images generated from RGB images to conduct the person Re-ID task, some hard query images can obtain better performance compared with using RGB images. Therefore, RGB and greyscale images seem to be complementary to each other for person Re-ID. In this article, we aim to utilize both RGB and greyscale images to improve the person Re-ID performance. To this end, we propose a novel two-stream deep neural network with RGB-grey information, which can effectively fuse RGB and greyscale feature representations to enhance the generalization ability of Re-ID. First, we convert RGB images to greyscale images in each training batch. Based on these RGB and greyscale images, we train the RGB and greyscale branches, respectively. Second, to build up connections between RGB and greyscale branches, we merge the RGB and greyscale branches into a new joint branch. Finally, we concatenate the features of all three branches as the final feature representation for Re-ID. Moreover, in the training process, we adopt the joint learning scheme to simultaneously train each branch by the independent loss function, which can enhance the generalization ability of each branch. Besides, a global loss function is utilized to further fine-tune the final concatenated feature. The extensive experiments on multiple benchmark datasets fully show that the proposed method can outperform the state-of-the-art person Re-ID methods. Furthermore, using greyscale images can indeed improve the person Re-ID performance in the proposed deep framework.  © 2021 Association for Computing Machinery.",greyscale person images; Person re-identification; two-stream deep framework,Deep neural networks; Benchmark datasets; Color information; Feature representation; Generalization ability; Grey scale images; Person re identifications; State of the art; Training process; Image enhancement
Market2Dish: Health-aware Food Recommendation,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104451164&doi=10.1145%2f3418211&partnerID=40&md5=0d58d7b734c15249baaef14603c761a6,"With the rising incidence of some diseases, such as obesity and diabetes, the healthy diet is arousing increasing attention. However, most existing food-related research efforts focus on recipe retrieval, user-preference-based food recommendation, cooking assistance, or the nutrition and calorie estimation of dishes, ignoring the personalized health-aware food recommendation. Therefore, in this work, we present a personalized health-aware food recommendation scheme, namely, Market2Dish, mapping the ingredients displayed in the market to the healthy dishes eaten at home. The proposed scheme comprises three components, namely, recipe retrieval, user health profiling, and health-aware food recommendation. In particular, recipe retrieval aims to acquire the ingredients available to the users and then retrieve recipe candidates from a large-scale recipe dataset. User health profiling is to characterize the health conditions of users by capturing the textual health-related information crawled from social networks. Specifically, to solve the issue that the health-related information is extremely sparse, we incorporate a word-class interaction mechanism into the proposed deep model to learn the fine-grained correlations between the textual tweets and pre-defined health concepts. For the health-aware food recommendation, we present a novel category-aware hierarchical memory network-based recommender to learn the health-aware user-recipe interactions for better food recommendation. Moreover, extensive experiments demonstrate the effectiveness of the health-aware food recommendation scheme.  © 2021 Association for Computing Machinery.",health-aware food recommendation; recipe retrieval; User health profiling,Commerce; Large dataset; Nutrition; Calorie estimations; Class interaction; Health condition; Health related informations; Hierarchical memory; Personalized healths; Preference-based; Research efforts; Thermal processing (foods)
Compressed Imaging Reconstruction with Sparse Random Projection,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104396865&doi=10.1145%2f3447431&partnerID=40&md5=dea3d9d2204452f34e1c62a85c9ddf19,"As the Internet of Things thrives, monitors and cameras produce tons of image data every day. To efficiently process these images, many compressed imaging frameworks are proposed. A compressed imaging framework comprises two parts, image signal measurement and reconstruction. Although a plethora of measurement devices have been designed, the development of the reconstruction is relatively lagging behind. Nowadays, most of existing reconstruction algorithms in compressed imaging are optimization problem solvers based on specific priors. The computation burdens of these optimization algorithms are enormous and the solutions are usually local optimums. Meanwhile, it is inconvenient to deploy these algorithms on cloud, which hinders the popularization of compressed imaging. In this article, we dive deep into the random projection to build reconstruction algorithms for compressed imaging. We first fully utilize the information in the measurement procedure and propose a combinatorial sparse random projection (SRP) reconstruction algorithm. Then, we generalize the SRP to a novel distributed algorithm called Cloud-SRP (CSRP), which enables efficient reconstruction on cloud. Moreover, we explore the combination of SRP with conventional optimization reconstruction algorithms and propose the Iterative-SRP (ISRP), which converges to a guaranteed fixed point. With minor modifications on the naive optimization algorithms, the ISRP yields better reconstructions. Experiments on real ghost imaging reconstruction reveal that our algorithms are effective. And simulation experiments show their advantages over the classical algorithms.  © 2021 Association for Computing Machinery.",cloud computing; Compressed sensing; internet of things; random projection; transform domain,Imaging techniques; Optimization; Conventional optimization; Efficient reconstruction; Imaging reconstruction; Measurement procedures; Optimization algorithms; Optimization problems; Reconstruction algorithms; Sparse random projections; Iterative methods
ART-UP: A Novel Method for Generating Scanning-Robust Aesthetic QR Codes,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098122449&doi=10.1145%2f3418214&partnerID=40&md5=07981eef2a13a1b794e62fc030e60aa9,"Quick response (QR) codes are usually scanned in different environments, so they must be robust to variations in illumination, scale, coverage, and camera angles. Aesthetic QR codes improve the visual quality, but subtle changes in their appearance may cause scanning failure. In this article, a new method to generate scanning-robust aesthetic QR codes is proposed, which is based on a module-based scanning probability estimation model that can effectively balance the tradeoff between visual quality and scanning robustness. Our method locally adjusts the luminance of each module by estimating the probability of successful sampling. The approach adopts the hierarchical, coarse-to-fine strategy to enhance the visual quality of aesthetic QR codes, which sequentially generate the following three codes: a binary aesthetic QR code, a grayscale aesthetic QR code, and the final color aesthetic QR code. Our approach also can be used to create QR codes with different visual styles by adjusting some initialization parameters. User surveys and decoding experiments were adopted for evaluating our method compared with state-of-the-art algorithms, which indicates that the proposed approach has excellent performance in terms of both visual quality and scanning robustness.  © 2021 Association for Computing Machinery.",Aesthetic QR codes; error analysis; scanning probability calculation; scanning robustness; visualization optimization,Scanning; Camera angles; Coarse-to-fine strategy; Module-based; Probability estimation; Quick response code; State-of-the-art algorithms; User surveys; Visual qualities; Codes (symbols)
Bi-manual Haptic-based Periodontal Simulation with Finger Support and Vibrotactile Feedback,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104411176&doi=10.1145%2f3421765&partnerID=40&md5=5db9a1c6a0b02b2f0ca78a7153800c78,"The rise of virtual reality and haptic technologies has created exciting new applications in medical training and education. In a dental simulation, haptic technology can create the illusion of substances (teeth, gingiva, bone, etc.) by providing interaction forces within a simulated virtual world of the mouth. In this article, a haptic periodontal training simulation system, named Haptodont, is developed and evaluated for simulating periodontal probing. Thirty-two faculty members from New York University College of Dentistry were recruited and divided into three groups to evaluate three fundamental functionalities: Group 1 evaluated bi-manual 3 Degrees of Freedome (DoF) haptic interaction, Group 2 evaluated bi-manual 3 DoF haptic interaction with a finger support mechanism, and Group 3 evaluated bi-manual 3 DoF haptic interaction with finger support mechanism and vibrotactile feedback. The probe and mirror interactions were simulated with the Geomagic Touch haptic device whereas the finger support was implemented using the Novint Falcon device. The three groups conducted two probing tasks: healthy gingiva scenario with no pockets (2-to 3-mm depth) and periodontitis scenario with deep pockets (4-to 8-mm depth). Results demonstrated that experts performed comparably to clinical settings in terms of probing depth error (within 0.3 to 0.6 mm) and probing forces (less than 0.5 N). Furthermore, the finger support mechanism significantly improved the probing accuracy for periodontitis condition in the lingual region. The argument that probing the lingual region is more difficult than the buccal region is supported by quantitative evidence (significantly higher probing depth error and probing force). Further research is planned to improve the usability of the finger support, integrate the Haptodont system into the pre-clinical curriculum, and evaluate the Haptodont system with dental students as a learning tool.  © 2021 Association for Computing Machinery.",dental simulation; Periodontal training; vibrotactile feedback,Diseases; Virtual reality; Clinical settings; Haptic interactions; Haptic technology; Interaction forces; New York University; Support mechanism; Training simulation systems; Vibro-tactile feedbacks; Clinical research
Robust Secret Image Sharing Resistant to Noise in Shares,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104406150&doi=10.1145%2f3419750&partnerID=40&md5=4467e68b667ebd759c617450aa21236a,"A secret image is split into shares in the generation phase of secret image sharing (SIS) for a threshold. In the recovery phase, the secret image is recovered when any or more shares are collected, and each collected share is generally assumed to be lossless in conventional SIS during storage and transmission. However, noise will arise during real-world storage and transmission; thus, shares will experience data loss, which will also lead to data loss in the secret image being recovered. Secret image recovery in the case of lossy shares is an important issue that must be addressed in practice, which is the overall subject of this article. An SIS scheme that can recover the secret image from lossy shares is proposed in this article. First, robust SIS and its definition are introduced. Next, a robust SIS scheme for a threshold without pixel expansion is proposed based on the Chinese remainder theorem (CRT) and error-correcting codes (ECC). By screening the random numbers, the share generation phase of the proposed robust SIS is designed to implement the error correction capability without increasing the share size. Particularly in the case of collecting noisy shares, our recovery method is to some degree robust to some noise types, such as least significant bit (LSB) noise, JPEG compression, and salt-and-pepper noise. A theoretical proof is presented, and experimental results are examined to evaluate the effectiveness of our proposed method.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Chinese remainder theorem; error-correcting codes; JPEG compression; robust secret image sharing; Secret image sharing,Digital storage; Error correction; Random number generation; Recovery; Transmissions; Chinese remainder theorem; Error correcting code; Error correction capability; Least significant bits; Recovery methods; Salt-and-pepper noise; Secret image sharing; Share generation; Image compression
A Semi-supervised Learning Approach Based on Adaptive Weighted Fusion for Automatic Image Annotation,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104464067&doi=10.1145%2f3426974&partnerID=40&md5=fe4a8c0c097724d619808c920e738923,"To learn a well-performed image annotation model, a large number of labeled samples are usually required. Although the unlabeled samples are readily available and abundant, it is a difficult task for humans to annotate large numbers of images manually. In this article, we propose a novel semi-supervised approach based on adaptive weighted fusion for automatic image annotation that can simultaneously utilize the labeled data and unlabeled data to improve the annotation performance. At first, two different classifiers, constructed based on support vector machine and covolutional neural network, respectively, are trained by different features extracted from the labeled data. Therefore, these two classifiers are independently represented as different feature views. Then, the corresponding features of unlabeled images are extracted and input into these two classifiers, and the semantic annotation of images can be obtained respectively. At the same time, the confidence of corresponding image annotation can be measured by an adaptive weighted fusion strategy. After that, the images and its semantic annotations with high confidence are submitted to the classifiers for retraining until a certain stop condition is reached. As a result, we can obtain a strong classifier that can make full use of unlabeled data. Finally, we conduct experiments on four datasets, namely, Corel 5K, IAPR TC12, ESP Game, and NUS-WIDE. In addition, we measure the performance of our approach with standard criteria, including precision, recall, F-measure, N+, and mAP. The experimental results show that our approach has superior performance and outperforms many state-of-the-art approaches.  © 2021 Association for Computing Machinery.",adaptive weighted fusion; Automatic image annotation; co-training; covolutional neural network; semi-supervised learning; support vector machine,Image analysis; Image enhancement; Image fusion; Labeled data; Semantics; Semi-supervised learning; Support vector machines; Adaptive weighted fusions; Annotation performance; Automatic image annotation; Semantic annotations; Semi-supervised; State-of-the-art approach; Strong classifiers; Unlabeled samples; Image annotation
Multi-human Parsing with a Graph-based Generative Adversarial Model,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104473703&doi=10.1145%2f3418217&partnerID=40&md5=8cbf041a53f2b297b6b4b6d94b5f7b51,"Human parsing is an important task in human-centric image understanding in computer vision and multimedia systems. However, most existing works on human parsing mainly tackle the single-person scenario, which deviates from real-world applications where multiple persons are present simultaneously with interaction and occlusion. To address such a challenging multi-human parsing problem, we introduce a novel multi-human parsing model named MH-Parser, which uses a graph-based generative adversarial model to address the challenges of close-person interaction and occlusion in multi-human parsing. To validate the effectiveness of the new model, we collect a new dataset named Multi-Human Parsing (MHP), which contains multiple persons with intensive person interaction and entanglement. Experiments on the new MHP dataset and existing datasets demonstrate that the proposed method is effective in addressing the multi-human parsing problem compared with existing solutions in the literature.  © 2021 Association for Computing Machinery.",generative adversarial networks; graph convolution network; Human parsing; human-centric image analysis; multi-human parsing,Graphic methods; Multimedia systems; Graph-based; Human-centric; Parsing problems; Real-world; Formal languages
Automated Orchestration of Online Educational Collaboration in Cloud-based Environments,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104391563&doi=10.1145%2f3412381&partnerID=40&md5=05c84aab6fb625ff983350a179b438f4,"Integrated collaboration environments (ICEs) are widely used by corporations to increase productivity by fostering groupwide and interpersonal collaboration. In this article, we discuss the enhancements of such environment needed to build an educational ICE (E-ICE) that addresses the specific needs of educational users. The motivation for the research was the Małopolska Educational Cloud (MEC) project conducted by AGH University and its partners.The E-ICE developed by MEC project fosters collaboration between universities and high schools by creating an immersive virtual collaboration space. MEC is a unique project due to its scale and usage domain. Multiple online collaboration events are organized weekly between over 150 geographically scattered institutions. Such events, aside from videoconferencing, require various services. The MEC E-ICE is a complex composition of a significant number of services and various terminals that require very specific configuration and management.In this article, we focus on a model-driven approach to automating the organization of online meetings in their preparation, execution, and conclusion phases. We present a conceptual model of E-ICE-supported educational courses, introduce a taxonomy of online educational services, identify planes and modes of their operation, as well as discuss the most common collaboration patterns.The MEC E-ICE, which we present as a case study, is built in accordance with the presented, model-driven approach. MEC educational services are described in a way that allows for converting the declarative specification of E-ICE application models into platform-independent models, platform-specific models, and, finally, working sets of orchestrated service instances. Such approach both reduces the level of technical knowledge required from the end-users and considerably speeds up the construction of online educational collaboration environments.  © 2021 Association for Computing Machinery.",cloud-based collaboration; Computer-supported collaboration; integrated collaboration environment; model driven architecture; orchestration; virtual learning environment,Ice; Video conferencing; Collaboration environments; Complex compositions; Educational services; Model driven approach; On-line collaborations; Platform independent model; Platform specific model; Virtual collaboration; E-learning
Conditional LSTM-GAN for Melody Generation from Lyrics,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104397145&doi=10.1145%2f3424116&partnerID=40&md5=9d6d864d52c2cbeb3c90e545795add31,"Melody generation from lyrics has been a challenging research issue in the field of artificial intelligence and music, which enables us to learn and discover latent relationships between interesting lyrics and accompanying melodies. Unfortunately, the limited availability of a paired lyrics-melody dataset with alignment information has hindered the research progress. To address this problem, we create a large dataset consisting of 12,197 MIDI songs each with paired lyrics and melody alignment through leveraging different music sources where alignment relationship between syllables and music attributes is extracted. Most importantly, we propose a novel deep generative model, conditional Long Short-Term Memory (LSTM)-Generative Adversarial Network for melody generation from lyrics, which contains a deep LSTM generator and a deep LSTM discriminator both conditioned on lyrics. In particular, lyrics-conditioned melody and alignment relationship between syllables of given lyrics and notes of predicted melody are generated simultaneously. Extensive experimental results have proved the effectiveness of our proposed lyrics-to-melody generative model, where plausible and tuneful sequences can be inferred from lyrics.  © 2021 Association for Computing Machinery.",conditional LSTM-GAN; Lyrics-conditioned melody generation,Alignment; Large dataset; Adversarial networks; Artificial intelligence and music; Generative model; Melody generation; Research issues; Long short-term memory
360-Degree VR Video Watermarking Based on Spherical Wavelet Transform,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104380520&doi=10.1145%2f3425605&partnerID=40&md5=64f4cdc1bdfd53f1ca54f87f3352b6d8,"Similar to conventional video, the increasingly popular 360 virtual reality (VR) video requires copyright protection mechanisms. The classic approach for copyright protection is the introduction of a digital watermark into the video sequence. Due to the nature of spherical panorama, traditional watermarking schemes that are dedicated to planar media cannot work efficiently for 360 VR video. In this article, we propose a spherical wavelet watermarking scheme to accommodate 360 VR video. With our scheme, the watermark is first embedded into the spherical wavelet transform domain of the 360 VR video. The spherical geometry of the 360 VR video is used as the host space for the watermark so that the proposed watermarking scheme is compatible with the multiple projection formats of 360 VR video. Second, the just noticeable difference model, suitable for head-mounted displays (HMDs), is used to control the imperceptibility of the watermark on the viewport. Third, besides detecting the watermark from the spherical projection, the proposed watermarking scheme also supports detecting watermarks robustly from the viewport projection. The watermark in the spherical domain can protect not only the 360 VR video but also its corresponding viewports. The experimental results show that the embedded watermarks are reliably extracted both from the spherical and the viewport projections of the 360 VR video, and the robustness of the proposed scheme to various copyright attacks is significantly better than that of the competing planar-domain approaches when detecting the watermark from viewport projection.  © 2021 Association for Computing Machinery.",360° VR video; just noticeable difference; spherical wavelet; watermarking,Copyrights; Digital watermarking; Helmet mounted displays; Image quality; Spheres; Wavelet transforms; Copyright protections; Embedded watermarks; Head mounted displays; Just-noticeable difference; Multiple projections; Spherical geometries; Spherical projection; Watermarking schemes; Image compression
Bottom-up and Layerwise Domain Adaptation for Pedestrian Detection in Thermal Images,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104467869&doi=10.1145%2f3418213&partnerID=40&md5=05b3b5befda6b946ab0ad99967c99be5,"Pedestrian detection is a canonical problem for safety and security applications, and it remains a challenging problem due to the highly variable lighting conditions in which pedestrians must be detected. This article investigates several domain adaptation approaches to adapt RGB-trained detectors to the thermal domain. Building on our earlier work on domain adaptation for privacy-preserving pedestrian detection, we conducted an extensive experimental evaluation comparing top-down and bottom-up domain adaptation and also propose two new bottom-up domain adaptation strategies. For top-down domain adaptation, we leverage a detector pre-trained on RGB imagery and efficiently adapt it to perform pedestrian detection in the thermal domain. Our bottom-up domain adaptation approaches include two steps: first, training an adapter segment corresponding to initial layers of the RGB-trained detector adapts to the new input distribution; then, we reconnect the adapter segment to the original RGB-trained detector for final adaptation with a top-down loss. To the best of our knowledge, our bottom-up domain adaptation approaches outperform the best-performing single-modality pedestrian detection results on KAIST and outperform the state of the art on FLIR.  © 2021 Association for Computing Machinery.",bottom-up adaptation; Domain adaptation; fine-tuning; layerwise adaptation; object detection; pedestrian detection; privacy-preserving; thermal; transfer learning,Historic preservation; Privacy by design; Canonical problems; Domain adaptation; Experimental evaluation; Input distributions; Pedestrian detection; Privacy preserving; Safety and securities; Variable lightings; Pedestrian safety
Affinity Derivation for Accurate Instance Segmentation,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104486148&doi=10.1145%2f3407090&partnerID=40&md5=ba5815852261e2dfd8ad83fe960dce30,"Affinity, which represents whether two pixels belong to a same instance, is an equivalent representation to the instance segmentation labels. Conventional works do not make an explicit exploration on the affinity. In this article, we present two instance segmentation schemes based on pixel affinity information and show the effectiveness of affinity in both aspects. For proposal-free method, we predict pixel affinity for each image and then propose a simple yet effective graph merge algorithm to cluster pixels into instances. It shows that the affinity is powerful as an instance-relevant information to guide the clustering procedure in proposal-free instance segmentation. For proposal-based methods, we extend conventional framework with affinity head and introduce affinity as attached supervision in training phase. Without any additional inference cost, we can improve the performance of existing proposal-based instance segmentation methods, which shows that the affinity can also be applied as an auxiliary loss and training with such extra loss is beneficial to the training progress. Experimental results show that our schemes achieve comparable performance to other state-of-the-art instance segmentation methods. With Cityscapes training data, the proposed proposal-free method achieves 28.8 AP and the proposal-based method gets 27.2 AP both on test sets.  © 2021 Association for Computing Machinery.",graph; Instance segmentation; semantic segmentation,Graph algorithms; Clustering procedure; Pixel affinities; Segmentation methods; Segmentation scheme; State of the art; Test sets; Training data; Training phase; Pixels
A weakly supervised semantic segmentation network by aggregating seed cues: The multi-object proposal generation perspective,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101289380&doi=10.1145%2f3419842&partnerID=40&md5=a29ed18d366e59f21f6e98c5baf1f922,"Weakly supervised semantic segmentation under image-level annotations is effectiveness for real-world applications. The small and sparse discriminative regions obtained from an image classification network that are typically used as the important initial location of semantic segmentation also form the bottleneck. Although deep convolutional neural networks (DCNNs) have exhibited promising performances for single-label image classification tasks, images of the real-world usually contain multiple categories, which is still an open problem. So, the problem of obtaining high-confidence discriminative regions from multi-label classification networks remains unsolved. To solve this problem, this article proposes an innovative three-step framework within the perspective of multi-object proposal generation. First, an image is divided into candidate boxes using the object proposal method. The candidate boxes are sent to a single-classification network to obtain the discriminative regions. Second, the discriminative regions are aggregated to obtain a high-confidence seed map. Third, the seed cues grow on the feature maps of high-level semantics produced by a backbone segmentation network. Experiments are carried out on the PASCAL VOC 2012 dataset to verify the effectiveness of our approach, which is shown to outperform other baseline image segmentation methods.  © 2021 Association for Computing Machinery.",high-confidence seed map; image-level annotations; Weakly supervised semantic segmentation,Classification (of information); Convolutional neural networks; Deep neural networks; Image classification; Semantic Web; Semantics; Baseline images; Classification networks; Classification tasks; High confidence; High level semantics; Multi label classification; Segmentation methods; Semantic segmentation; Image segmentation
"A Novel (t, s, k, n)-Threshold Visual Secret Sharing Scheme Based on Access Structure Partition",2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100315788&doi=10.1145%2f3418212&partnerID=40&md5=0def9c91e869b5c218c3171af1c90f43,"Visual secret sharing (VSS) is a new technique for sharing a binary image into multiple shadows. For VSS, the original image can be reconstructed from the shadows in any qualified set, but cannot be reconstructed from those in any forbidden set. In most traditional VSS schemes, the shadows held by participants have the same importance. However, in practice, a certain number of shadows are given a higher importance due to the privileges of their owners. In this article, a novel (t, s, k, n)-threshold VSS scheme is proposed based on access structure partition. First, we construct the basis matrix of the proposed (t, s, k, n)-threshold VSS scheme by utilizing a new access structure partition method and sub-access structure merging method. Then, the secret image is shared by the basis matrix as n shadows, which are divided into s essential shadows and n-s non-essential shadows. To reconstruct the secret image, k or more shadows should be collected, which include at least t essential shadows; otherwise, no information about the secret image can be obtained. Compared with related schemes, our scheme achieves a smaller shadow size and a higher visual quality of the reconstructed image. Theoretical analysis and experiments indicate the effectiveness of the proposed scheme.  © 2020 ACM.",access structure partition; essential participant; Secret sharing; threshold visual secret sharing,Binary images; Access structure; Original images; Partition methods; Reconstructed image; Secret images; Threshold visual secret sharing schemes; Visual qualities; Visual secret sharing; Image reconstruction
Smart Scribbles for Image Matting,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100318129&doi=10.1145%2f3408323&partnerID=40&md5=06ae10025a5d6dd0c9d6b78b09d439cd,"Image matting is an ill-posed problem that usually requires additional user input, such as trimaps or scribbles. Drawing a fine trimap requires a large amount of user effort, while using scribbles can hardly obtain satisfactory alpha mattes for non-professional users. Some recent deep learning-based matting networks rely on large-scale composite datasets for training to improve performance, resulting in the occasional appearance of obvious artifacts when processing natural images. In this article, we explore the intrinsic relationship between user input and alpha mattes and strike a balance between user effort and the quality of alpha mattes. In particular, we propose an interactive framework, referred to as smart scribbles, to guide users to draw few scribbles on the input images to produce high-quality alpha mattes. It first infers the most informative regions of an image for drawing scribbles to indicate different categories (foreground, background, or unknown) and then spreads these scribbles (i.e., the category labels) to the rest of the image via our well-designed two-phase propagation. Both neighboring low-level affinities and high-level semantic features are considered during the propagation process. Our method can be optimized without large-scale matting datasets and exhibits more universality in real situations. Extensive experiments demonstrate that smart scribbles can produce more accurate alpha mattes with reduced additional input, compared to the state-of-the-art matting methods.  © 2020 ACM.",alpha matte; deep learning; Image matting; label propagation; markov chain,Deep learning; Large dataset; Semantics; Smelting; High-level semantic features; Ill posed problem; Improve performance; Natural images; Non-professional users; Propagation process; Real situation; State of the art; Image enhancement
GuessUNeed: Recommending Courses via Neural Attention Network and Course Prerequisite Relation Embeddings,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100316210&doi=10.1145%2f3410441&partnerID=40&md5=f411f35fac304ef1f148822e15e4772c,"Massive Open Online Courses, offering millions of high-quality courses from prestigious universities and prominent experts, are picking up momentum in popularity. Although users enrolling on MOOCs have free access to abundant knowledge, they may easily get overwhelmed by information overload. Therefore, there is a need of recommending technology as a fundamental and well-accepted effective solution. However, differing from many other online recommendations, recommending courses to users on MOOCs faces two challenges. First, users' knowledge background differs, so does their purpose of learning. Second, online courses are not independent but intertwined with prerequisite relations. Therefore, it is necessary to take these two challenges into account when designing a recommending method. To tackle this issue, in this article, we first propose two algorithms for extracting concept-level and course-level prerequisite relations. We then present the recommending method GuessUNeed based on neural attention network and course prerequisite relation embeddings. The experimental results on real-world datasets demonstrate the superiority of the proposed GuessUNeed method.  © 2020 ACM.",MOOCs; prerequisite relation; Recommendation; relation embeddings,Embeddings; Effective solution; Extracting concept; Free access; High quality; Information overloads; Massive open online course; Online course; Real-world datasets; E-learning
Make Full Use of Priors: Cross-View Optimized Filter for Multi-View Depth Enhancement,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100316162&doi=10.1145%2f3408293&partnerID=40&md5=046918a014dbd5d11b5d0a40f1a57bcf,"Multi-view video plus depth (MVD) is the promising and widely adopted data representation for future 3D visual applications and interactive media. However, compression distortions on depth videos impede the development of such applications, and filters are crucially needed for the quality enhancement at the terminal side. Cross-view priors can intuitively be involved in filter design, but these priors are also distorted in compression and thus the contribution of them can hardly be considered in previous research. In this article, we propose a cross-view optimized filter for depth map quality enhancement by making full use of inner- and cross-view priors. We dedicate to evaluate the contributions of distorted cross-view priors in filtering the current view of depth, and then both inner- and cross-view priors can be involved in the filter design. Thus, distortions of cross-view priors are not barriers again as before. For the purpose of that, mutual information guided cross-view consistency is designed to evaluate the contributions of cross-view priors from compression distortions of MVD. After that, under the framework of global optimization, both inner- and cross-view priors are modeled and taken to minimize the designed energy function where both data accuracy and spatial smoothness are modeled. The experimental results show that the proposed model outperforms state-of-the-art methods, where 3.289 dB and 0.0407 average gains on peak signal-to-noise ratio and structural similarity metrics can be obtained, respectively. For the subjective evaluations, object details and structure information are recovered in the compressed depth video. We also verify our method via several practical applications, including virtual view synthesis for smooth interaction and point cloud for 3D modeling for accuracy evaluation. In these verifications, the ringing and malposition artifacts on object contours are properly handled for interactive video, and discontinuous object surfaces are restored for 3D modeling. All of these results suggest that compression distortions in MVD can be properly filtered by the proposed model, which provides a promising solution for future bandwidth constrained 3D and interactive visual applications.  © 2020 ACM.",global optimization; Multi-view video plus depth; view consistency,3D modeling; Bandwidth compression; Global optimization; Image segmentation; Quality control; Signal to noise ratio; Bandwidth-constrained; Discontinuous objects; Multi view video plus depth (MVD); Peak signal to noise ratio; State-of-the-art methods; Structural similarity; Subjective evaluations; Virtual view synthesis; Three dimensional computer graphics
Adaptive Attention-based High-level Semantic Introduction for Image Caption,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100316153&doi=10.1145%2f3409388&partnerID=40&md5=822029889be96d8ce9a09adca65563d6,"There have been several attempts to integrate a spatial visual attention mechanism into an image caption model and introduce semantic concepts as the guidance of image caption generation. High-level semantic information consists of the abstractedness and generality indication of an image, which is beneficial to improve the model performance. However, the high-level information is always static representation without considering the salient elements. Therefore, a semantic attention mechanism is used for the high-level information instead of conventional of static representation in this article. The salient high-level semantic information can be considered as redundant semantic information for image caption generation. Additionally, the generation of visual words and non-visual words can be separated, and an adaptive attention mechanism is employed to realize the guidance information of image caption generation switching between new fusion information (fusion of image feature and high-level semantics) and a language model. Therefore, visual words can be generated according to the image features and high-level semantic information, and non-visual words can be predicted by the language model. The semantics attention, adaptive attention, and previous generated words are fused to construct a special attention module for the input and output of long short-term memory. An image caption can be generated as a concise sentence on the basis of accurately grasping the rich content of the image. The experimental results show that the performance of the proposed model is promising for the evaluation metrics, and the captions can achieve logical and rich descriptions.  © 2020 ACM.",adaptive attention; CNN; high-level semantic; Image caption; LSTM; visual sentinel,Behavioral research; Computational linguistics; Image enhancement; Semantics; Attention mechanisms; Evaluation metrics; Guidance information; High level semantics; High-level information; Semantic information; Static representation; Visual attention mechanisms; Visual languages
MMFN: Multimodal Information Fusion Networks for 3D Model Classification and Retrieval,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100288511&doi=10.1145%2f3410439&partnerID=40&md5=858a85a4f4d41f3206d579f8b9ac4e0e,"In recent years, research into 3D shape recognition in the field of multimedia and computer vision has attracted wide attention. With the rapid development of deep learning, various deep models have achieved state-of-the-art performance based on different representations. There are many modalities for representing a 3D model, such as point cloud, multiview, and panorama view. Deep learning models based on these different modalities have different concerns, and all of them have achieved high performance for 3D shape recognition. However, all of these methods ignore the multimodality information in conditions where the same 3D model is represented by different modalities. Thus, we can obtain a better descriptor by guiding the training to consider these multiple representations. In this article, we propose MMFN, a novel multimodal fusion network for 3D shape recognition that employs correlations between the different modalities to generate a fused descriptor, which is more robust. In particular, we design two novel loss functions to help the model learn the correlation information during training. The first is correlation loss, which focuses on the correlations among different descriptors generated from different structures. This approach reduces the training time and improves the robustness of the fused descriptor of the 3D model. The second is instance loss, which preserves the independence of each modality and utilizes feature differentiation to guide model learning during the training process. More specifically, we use the weighted fusion method, which applies statistical methods to obtain robust descriptors that maximize the advantages of the information from the different modalities. We evaluated the proposed method on the ModelNet40 and ShapeNetCore55 datasets for 3D shape classification and retrieval tasks. The experimental results and comparisons with state-of-the-art methods demonstrate the superiority of our approach.  © 2020 ACM.",3D retrieval; feature fusion; multimodal; point cloud,3D modeling; Classification (of information); Deep learning; Multimedia systems; 3D model classification; 3D shape recognition; Different structure; Multi-modal fusion; Multimodal information fusion; Multiple representation; State-of-the-art methods; State-of-the-art performance; Learning systems
Introduction to the Special Issue on Privacy and Security in Evolving Internet of Multimedia Things,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099849213&doi=10.1145%2f3423955&partnerID=40&md5=315a188a850234eb29e6b7dab8201828,[No abstract available],,
Social-sensed Image Aesthetics Assessment,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099790305&doi=10.1145%2f3414843&partnerID=40&md5=85fd980bcb50958b681219e0b31e2d26,"Image aesthetics assessment aims to endow computers with the ability to judge the aesthetic values of images, and its potential has been recognized in a variety of applications. Most previous studies perform aesthetics assessment purely based on image content. However, given the fact that aesthetic perceiving is a human cognitive activity, it is necessary to consider users' perception of an image when judging its aesthetic quality. In this article, we regard users' social behavior as the reflection of their perception of images and harness these additional clues to improve image aesthetics assessment. Specifically, we first merge the raw social interactions between users and images into clusters as the social labels of images, so the collective social behavioral information associated with an image can be well represented over a structured and compact space. Then, we develop a novel deep multi-task network to jointly learn social labels in different modalities from social images and apply it to common web images. In this manner, our approach is readily generalized to web images without social behavioral information. Finally, we introduce a high-level fusion sub-network to the aesthetics model, in which the social and visual representations of images are well balanced for aesthetics assessment. Experimental results on two benchmark datasets well verify the effectiveness of our approach and highlight the benefits of different types of social behavioral information for image aesthetics assessment. © 2020 ACM.",Image aesthetics assessment; multi-task learning; social sense; user perception modeling,Computer networks; Aesthetic qualities; Benchmark datasets; Cognitive activities; High-level fusions; Image Aesthetics; Social interactions; Users' perception; Visual representations; Image enhancement
MV2Flow: Learning Motion Representation for Fast Compressed Video Action Recognition,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099787416&doi=10.1145%2f3422360&partnerID=40&md5=f9720aabad17e8a45ec6e1f486465afa,"In video action recognition, motion is a very crucial clue, which is usually represented by optical flow. However, optical flow is computationally expensive to obtain, which becomes the bottleneck for the efficiency of traditional action recognition algorithms. In this article, we propose a network called MV2Flow to learn motion representation efficiently from the signals in the compressed domain. To learn the network, three losses are defined. First, we select the classical TV-L1 flow as proxy ground truth to guide the learning. Besides, an unsupervised image reconstruction loss is proposed to further refine it. Moreover, toward the task of action recognition, the above two losses are combined with a motion content loss. To evaluate our approach, extensive experiments on two benchmark datasets UCF-101 and HMDB-51 are conducted. The motion representation generated with our MV2Flow has shown comparable classification performance on action recognition with TV-L1 flow, while operating at an over 200× faster speed. Based on our MV2Flow and 2D-CNN-based network, we have achieved state-of-the-art performance in the compressed domain. With 3D-CNN-based network, we also achieve comparable accuracy with higher inference speed than methods in the decoded domain setting. © 2020 ACM.",action recognition; compressed domain; motion representation; MV2Flow,Image reconstruction; Action recognition; Action recognition algorithms; Benchmark datasets; Classification performance; Compressed domain; Compressed video; Motion representation; State-of-the-art performance; Optical flows
Controlling Neural Learning Network with Multiple Scales for Image Splicing Forgery Detection,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100309363&doi=10.1145%2f3408299&partnerID=40&md5=2ff849803d408945a1f515882f074975,"The guarantee of social stability comes from many aspects of life, and image information security as one of them is being subjected to various malicious attacks. As a means of information attack, image splicing forgery refers to copying some areas of an image to another image to hide the traces of the original information and leads to grave consequences. Image splicing forgery is extremely complex since the attributes of the two images subjected to the pasting and copying operations are greatly different. In order to solve the issue mentioned above, we propose a method by applying a neural learning network controlled by multiple scales (MCNL-Net) based on U-Net to identify whether an image has been tampered and to locate the tampered regions. Firstly, the learning capacity of MCNL-Net is enhanced by the combination of a residual propagation module and a residual feedback module. An ingenious strategy is designed to control the size of local receptive field in each building block of MCNL-Net. The strategy makes MCNL-Net able to achieve properties and superiorities of multi-scale structure and learn specified features. For further improving the detection performance of MCNL-Net, a block attention mechanism is proposed to control the advanced degree of the input information in each building block. In addition, a MaxBlurPool method is applied into image splicing forgery detection for the first time, preserving the shift-equivariance of a convolutional neural network. Through experiments, we demonstrate that MCNL-Net can achieve more promising results and offer stronger robustness than the state-of-the-art splicing forgery detection methods.  © 2020 ACM.",block attention mechanism; Image splicing forgery detection; multi-scale structure; shift-equivariance,Backpropagation; Learning systems; Network security; Attention mechanisms; Detection performance; Forgery detections; Information attacks; Learning capacity; Multi-scale structures; Propagation modules; Receptive fields; Convolutional neural networks
Part-based Structured Representation Learning for Person Re-identification,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100301752&doi=10.1145%2f3412384&partnerID=40&md5=2e4309a4d1324e8a84ccea60228f2001,"Person re-identification aims to match person of interest under non-overlapping camera views. Therefore, how to generate a robust and discriminative representation is crucial for person re-identification. Mining local clues from human body parts to describe pedestrians has been extensively studied in existing methods. However, existing methods locate human body parts coarsely and do not consider the relations among different local parts. To address the above problem, we propose a Part-based Structured Representation Learning (PSRL) for better exploiting local clues to improve the person representation. There are two important modules in our architecture: Local Semantic Feature Extraction and Structured Person Representation Learning. The Local Semantic Feature Extraction module is designed to extract local features from human body semantic regions. After obtaining the local features, the Structured Person Representation Learning is proposed to fuse the local features by considering the person structure. To model the underlying person structure, a graph convolutional network is employed to capture the relations of different semantic regions. The generated structured feature encodes underlying person structure information, and local semantic feature can solve the misalignment problem caused by pose variations in feature matching. By combining them together, we can improve the descriptive ability of the generated representation. Extensive evaluations on four standard benchmarks show that our proposed method achieves competitive performance against state-of-the-art methods.  © 2020 ACM.",graph convolutional network; Person re-identification; representation learning,Benchmarking; Convolutional neural networks; Extraction; Feature extraction; Competitive performance; Convolutional networks; Feature matching; Local semantic features; Non-overlapping camera views; Person re identifications; State-of-the-art methods; Structure information; Semantics
Perceptual Image Compression with Block-Level Just Noticeable Difference Prediction,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100291388&doi=10.1145%2f3408320&partnerID=40&md5=e2df5adeb5136788a4126b2ad25c4ab2,"A block-level perceptual image compression framework is proposed in this work, including a block-level just noticeable difference (JND) prediction model and a preprocessing scheme. Specifically speaking, block-level JND values are first deduced by utilizing the OTSU method based on the variation of block-level structural similarity values between two adjacent picture-level JND values in the MCL-JCI dataset. After the JND value for each image block is generated, a convolutional neural network-based prediction model is designed to forecast block-level JND values for a given target image. Then, a preprocessing scheme is devised to modify the discrete cosine transform coefficients during JPEG compression on the basis of the distribution of block-level JND values of the target test image. Finally, the test image is compressed by the max JND value across all of its image blocks in the light of the initial quality factor setting. The experimental results demonstrate that the proposed block-level perceptual image compression method is able to achieve 16.75% bit saving as compared to the state-of-the-art method with similar subjective quality. The project page can be found at https://mic.tongji.edu.cn/43/3f/c9778a148287/page.htm.  © 2021 ACM.",block-level prediction; convolutional neural network; just noticeable difference; Perceptual image compression,Convolutional neural networks; Discrete cosine transforms; Forecasting; Image coding; Predictive analytics; Discrete cosine transform coefficients; Image compression methods; JPEG compression; Just-noticeable difference; Preprocessing scheme; State-of-the-art methods; Structural similarity; Subjective quality; Image compression
Privacy Protection for Medical Data Sharing in Smart Healthcare,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099777861&doi=10.1145%2f3408322&partnerID=40&md5=079dc70d963625e99d6fd07321221d1e,"In virtue of advances in smart networks and the cloud computing paradigm, smart healthcare is transforming. However, there are still challenges, such as storing sensitive data in untrusted and controlled infrastructure and ensuring the secure transmission of medical data, among others. The rapid development of watermarking provides opportunities for smart healthcare. In this article, we propose a new data-sharing framework and a data access control mechanism. The applications are submitted by the doctors, and the data is processed in the medical data center of the hospital, stored in semi-trusted servers to support the selective sharing of electronic medical records from different medical institutions between different doctors. Our approach ensures that privacy concerns are taken into account when processing requests for access to patients' medical information. For accountability, after data is modified or leaked, both patients and doctors must add digital watermarks associated with their identification when uploading data. Extensive analytical and experimental results are presented that show the security and efficiency of our proposed scheme. © 2020 ACM.",access control; attribute-based encryption; Medical data privacy; smart healthcare,Access control; Data privacy; Digital watermarking; Electronic document exchange; Health care; Medical computing; Data access control; Electronic medical record; Medical information; Medical institutions; Privacy concerns; Privacy protection; Secure transmission; Sensitive datas; Data Sharing
Depth Image Denoising Using Nuclear Norm and Learning Graph Model,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100289878&doi=10.1145%2f3404374&partnerID=40&md5=156bdd1a9a184e025dcfc6e3441722c2,"Depth image denoising is increasingly becoming the hot research topic nowadays, because it reflects the three-dimensional scene and can be applied in various fields of computer vision. But the depth images obtained from depth camera usually contain stains such as noise, which greatly impairs the performance of depth-related applications. In this article, considering that group-based image restoration methods are more effective in gathering the similarity among patches, a group-based nuclear norm and learning graph (GNNLG) model was proposed. For each patch, we find and group the most similar patches within a searching window. The intrinsic low-rank property of the grouped patches is exploited in our model. In addition, we studied the manifold learning method and devised an effective optimized learning strategy to obtain the graph Laplacian matrix, which reflects the topological structure of image, to further impose the smoothing priors to the denoised depth image. To achieve fast speed and high convergence, the alternating direction method of multipliers is proposed to solve our GNNLG. The experimental results show that the proposed method is superior to other current state-of-the-art denoising methods in both subjective and objective criterion.  © 2020 ACM.",ADMM; Learning graph model; low-rank; nonlocal self-similarity,Image reconstruction; Learning systems; Matrix algebra; Topology; Alternating direction method of multipliers; Hot research topics; Low-rank properties; Manifold learning; Objective criteria; Restoration methods; Three-dimensional scenes; Topological structure; Image denoising
Vertical Retargeting for Stereoscopic Images via Stereo Seam Carving,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100308421&doi=10.1145%2f3408295&partnerID=40&md5=c414edc9a79a63c82249d8669d5734ab,"Vertical retargeting for stereoscopic images using seam manipulation-based approaches has remained an open challenge over the years. Even though horizontal retargeting had attracted a huge amount of interest, its seam coupling strategies were not capable to construct valid seam pairs for vertical retargeting. In this article, we propose two seam coupling strategies for vertical retargeting, namely, real mapping and virtual mapping. Our proposed mapping strategies were implemented to address the problems of multiple assignments and missing assignments, which are able to occur in the straightforward generalization from horizontal retargeting to vertical retargeting. On the basis of our proposed method, stereo seams were allowed to lay across occluded regions and occluding regions in stereo images. We maintained the geometric consistency by removing occluded pixels and corresponding occluding pixels in both stereo images. As a result, our method guarantees valid and geometrically consistent stereo seam pairs to be found in the horizontal direction. We generate vertically retargeted stereo images by removing or adding horizontal seam pairs iteratively. We conducted experiments on a number of indoor and outdoor scenes. Experimental results demonstrated that our method overcomes the limitations of vertical retargeting and is effective in preserving the geometric consistency.  © 2020 ACM.",coupling strategies; occluded regions; occluding regions; Stereo image retargeting; vertical retargeting,Iterative methods; Mapping; Pixels; Mapping strategy; Outdoor scenes; Seam carving; Stereo-image; Stereoscopic image; Stereo image processing
Fog-based Secure Service Discovery for Internet of Multimedia Things: A Cross-blockchain Approach,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099792535&doi=10.1145%2f3415151&partnerID=40&md5=16f7aa0f5560610e70cf6628300f8126,"The Internet of Multimedia Things (IoMT) has become the backbone of innumerable multimedia applications in various fields. The wide application of IoMT not only makes our life convenient but also brings challenges to service discovery. Service discovery aims to leverage location information and trust evidence scattered in a variety of multimedia applications to find trusted IoMT devices that can provide specific service in target areas. However, the eavesdropping and tampering to these sensitive IoMT data during the trust propagation process invalidate the service discovery process. To address these challenges, we propose Secure Service Discovery (SSD) for IoMT using cross-blockchain-enabled fog computing. To resist the tampering and eavesdropping during the trust propagation process, a scalable cross-blockchain structure consisting of multiple parallel blockchains is first proposed based on fog, in which different parallel blockchains can be orchestrated to propagate encrypted location information and trust evidence of different applications. Moreover, to enable a cross-blockchain structure to leverage encrypted location information and trust evidence to find trusted IoMT devices in preset areas, a novel privacy-preserving range query is proposed to query and aggregate trust evidence. Security analysis and simulations are carried out to demonstrate the effectiveness and security of the proposed SSD. © 2020 ACM.",blockchain; fog computing; Internet of Multimedia Things (IoMT); privacy-preserving,Cryptography; Fog; Fog computing; Location; Privacy by design; Security systems; Web services; Location information; Multimedia applications; Privacy preserving; Secure service discovery; Security analysis; Service discovery; Service discovery process; Trust propagation; Blockchain
Correlation Discrepancy Insight Network for Video Re-identification,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100313165&doi=10.1145%2f3402666&partnerID=40&md5=b3e7e5fa4faf8f2151532a8ec1116cdb,"Video-based person re-identification (ReID) aims at re-identifying a specified person sequence from videos that were captured by disjoint cameras. Most existing works on this task ignore the quality discrepancy across frames by using all video frames to develop a ReID method. Additionally, they adopt only the person self-characteristic as the representation, which cannot adapt to cross-camera variation effectively. To that end, we propose a novel correlation discrepancy insight network for video-based person ReID, which consists of an unsupervised correlation insight model (CIM) for video purification and a discrepancy description network (DDN) for person representation. Concretely, CIM is constructed by using kernelized correlation filters to encode person half-parts, which evaluates the frame quality by the cross correlation across frames for selecting discriminative video fragments. Furthermore, DDN exploits the selected video fragments to generate a discrepancy descriptor using a compression network, which aims at employing the discrepancies with other persons' to facilitate the representation of the target person rather than only using the self-characteristic. Due to the advantage in handling cross-domain variation, the discrepancy descriptor is expected to provide a new pattern for the object representation in cross-camera tasks. Experimental results on three public benchmarks demonstrate that the proposed method outperforms several state-of-the-art methods.  © 2020 ACM.",correlation insight; cross-domain variation; discrepancy description network; Video re-identification,Correlation filters; Cross correlations; Frame quality; Object representations; Person re identifications; Re identifications; State-of-the-art methods; Video fragments; Cameras
Differentially Private Tensor Train Deep Computation for Internet of Multimedia Things,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099782502&doi=10.1145%2f3421276&partnerID=40&md5=bca5efbb631c5b40e99143428f78abdc,"The significant growth of the Internet of Things (IoT) takes a key and active role in healthcare, smart homes, smart manufacturing, and wearable gadgets. Due to complexness and difficulty in processing multimedia data, the IoT based scheme, namely Internet of Multimedia Things (IoMT) exists that is specialized for services and applications based on multimedia data. However, IoMT generated data are facing major processing and privacy issues. Therefore, tensor-based deep computation models proved a better platform to process IoMT generated data. A differentially private deep computation method working in the tensor space can attest to its efficacy for IoMT. Nevertheless, the deep computation model comprises a multitude of parameters; thus, it requires large units of memory and expensive computing units with higher performance levels, which hinders its performance for IoMT. Motivated by this, therefore, the paper proposes a deep private tensor train autoencoder (dPTTAE) technique to deal with IoMT generated data. Notably, the compression of weight tensors to manageable tensor train format is achieved through Tensor Train (TT) network. Moreover, TT format parameters are trained through higher-order back-propagation and gradient descent. We applied dPTTAE on three representative datasets. Comprehensive experimental evaluations and theoretical analysis show that dPTTAE enhances training time efficiency, and greatly improve memory utilization efficiency, attesting its potential for IoMT. © 2020 ACM.",cyber-physical-social systems; deep computation; deep learning; IoMT; privacy; Tensor train auto-encoder; ϵ-differential privacy,Automation; Backpropagation; Computation theory; Data handling; Efficiency; Gradient methods; Intelligent buildings; Multimedia services; Tensors; Computation methods; Computation model; Experimental evaluation; Internet of thing (IOT); Memory utilization; Performance level; Services and applications; Smart manufacturing; Internet of things
Evaluation of Information Comprehension in Concurrent Speech-based Designs,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100292560&doi=10.1145%2f3409463&partnerID=40&md5=e7b1b36b3fa2d4e65eb18f109919aaa6,"In human-computer interaction, particularly in multimedia delivery, information is communicated to users sequentially, whereas users are capable of receiving information from multiple sources concurrently. This mismatch indicates that a sequential mode of communication does not utilise human perception capabilities as efficiently as possible. This article reports an experiment that investigated various speech-based (audio) concurrent designs and evaluated the comprehension depth of information by comparing comprehension performance across several different formats of questions (main/detailed, implied/stated). The results showed that users, besides answering the main questions, were also successful in answering the implied questions, as well as the questions that required detailed information, and that the pattern of comprehension depth remained similar to that seen to a baseline condition, where only one speech source was presented. However, the participants answered more questions correctly that were drawn from the main information, and performance remained low where the questions were drawn from detailed information. The results are encouraging to explore the concurrent methods further for communicating multiple information streams efficiently in human-computer interaction, including multimedia.  © 2020 ACM.",audio spatial location; audio streams; auditory display; comprehension depth; Concurrent speech; concurrent speech-based information comprehension; concurrent streaming; intermittent 8 continuous speech presentation; listening comprehension; speech perception; voice-based interaction,Computer hardware description languages; Base-line conditions; Comprehension performance; Concurrent design; Human perception; Multimedia delivery; Multiple information streams; Multiple source; Sequential mode; Human computer interaction
Am i Done Predicting Action Progress in Videos,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100298786&doi=10.1145%2f3402447&partnerID=40&md5=88a50e8acbf1b6da322a5286266a13d0,"In this article, we deal with the problem of predicting action progress in videos. We argue that this is an extremely important task, since it can be valuable for a wide range of interaction applications. To this end, we introduce a novel approach, named ProgressNet, capable of predicting when an action takes place in a video, where it is located within the frames, and how far it has progressed during its execution. To provide a general definition of action progress, we ground our work in the linguistics literature, borrowing terms and concepts to understand which actions can be the subject of progress estimation. As a result, we define a categorization of actions and their phases. Motivated by the recent success obtained from the interaction of Convolutional and Recurrent Neural Networks, our model is based on a combination of the Faster R-CNN framework, to make framewise predictions, and LSTM networks, to estimate action progress through time. After introducing two evaluation protocols for the task at hand, we demonstrate the capability of our model to effectively predict action progress on the UCF-101 and J-HMDB datasets.  © 2020 ACM.",Action progress; action recognition,Forecasting; Evaluation protocol; Progress estimations; Long short-term memory
An LSH-based Offloading Method for IoMT Services in Integrated Cloud-Edge Environment,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099781988&doi=10.1145%2f3408319&partnerID=40&md5=3621dd6c263f04c253efcc666dbcf45d,"Benefiting from the massive available data provided by Internet of multimedia things (IoMT), enormous intelligent services requiring information of various types to make decisions are emerging. Generally, the IoMT devices are equipped with limited computing power, interfering with the process of computation-intensive services. Currently, to satisfy a wide range of service requirements, the novel computing paradigms, i.e., cloud computing and edge computing, can potentially be integrated for service accommodation. Nevertheless, the private information (i.e., location, service type, etc.) in the services is prone to spilling out during service offloading in the cloud-edge computing. To avoid privacy leakage while improving service utility, including the service response time and energy consumption for service executions, a <underline>L</underline>ocality-sensitive-hash (LSH)-based <underline>o</underline>ffloading <underline>m</underline>ethod, named LOM, is devised. Specifically, LSH is leveraged to encrypt the feature information for the services offloaded to the edge servers with the intention of privacy preservation. Eventually, comparative experiments are conducted to verify the effectiveness of LOM with respect to promoting service utility. © 2021 ACM.",cloud-edge computing; IoMT; LSH; privacy preservation; service offloading,Edge computing; Energy utilization; Privacy by design; Comparative experiments; Computation intensives; Feature information; Intelligent Services; Privacy preservation; Private information; Service requirements; Service response time; Multimedia services
Knowledge-driven Egocentric Multimodal Activity Recognition,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100294506&doi=10.1145%2f3409332&partnerID=40&md5=06c8324869b29fb4b0b3af4d59e00e9a,"Recognizing activities from egocentric multimodal data collected by wearable cameras and sensors, is gaining interest, as multimodal methods always benefit from the complementarity of different modalities. However, since high-dimensional videos contain rich high-level semantic information while low-dimensional sensor signals describe simple motion patterns of the wearer, the large modality gap between the videos and the sensor signals raises a challenge for fusing the raw data. Moreover, the lack of large-scale egocentric multimodal datasets due to the cost of data collection and annotation processes makes another challenge for employing complex deep learning models. To jointly deal with the above two challenges, we propose a knowledge-driven multimodal activity recognition framework that exploits external knowledge to fuse multimodal data and reduce the dependence on large-scale training samples. Specifically, we design a dual-GCLSTM (Graph Convolutional LSTM) and a multi-layer GCN (Graph Convolutional Network) to collectively model the relations among activities and intermediate objects. The dual-GCLSTM is designed to fuse temporal multimodal features with top-down relation-aware guidance. In addition, we apply a co-attention mechanism to adaptively attend to the features of different modalities at different timesteps. The multi-layer GCN aims to learn relation-aware classifiers of activity categories. Experimental results on three publicly available egocentric multimodal datasets show the effectiveness of the proposed model.  © 2020 ACM.",Egocentric videos; graph neural networks; wearable sensors,Convolution; Convolutional neural networks; Deep learning; Large dataset; Pattern recognition; Semantics; Wearable sensors; Activity recognition; Attention mechanisms; Convolutional networks; External knowledge; High level semantics; Multimodal datasets; Multimodal features; Wearable cameras; Long short-term memory
Learning a Deep Agent to Predict Head Movement in 360-Degree Images,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100305639&doi=10.1145%2f3410455&partnerID=40&md5=50236f32dd2ed174b5431f405e66566a,"Virtual reality adequately stimulates senses to trick users into accepting the virtual environment. To create a sense of immersion, high-resolution images are required to satisfy human visual system, and low latency is essential for smooth operations, which put great demands on data processing and transmission. Actually, when exploring in the virtual environment, viewers only perceive the content in the current field of view. Therefore, if we can predict the head movements that are important behaviors of viewers, more processing resources can be allocated to the active field of view. In this article, we propose a model to predict the trajectory of head movement. Deep reinforcement learning is employed to mimic the decision making. In our framework, to characterize each state, features for viewport images are extracted by convolutional neural networks. In addition, the spherical coordinate maps and visited maps are generated for each viewport image, which facilitate the multiple dimensions of the state information by considering the impact of historical head movement and position information. To ensure the accurate simulation of visual behaviors during the watching of panoramas, we stipulate that the model imitates the behaviors of human demonstrators. To allow the model to generalize to more conditions, the intrinsic motivation is employed to guide the agent's action toward reducing uncertainty, which can enhance robustness during the exploration. The experimental results demonstrate the effectiveness of the proposed stepwise head movement predictor.  © 2020 ACM.",360 degree; deep reinforcement learning (DRL); head movement prediction; omnidirectional; panoramic; saliency; VR,Behavioral research; Convolutional neural networks; Data handling; Decision making; Forecasting; Image processing; Reinforcement learning; Data processing and transmission; High resolution image; Human Visual System; Intrinsic motivation; Multiple dimensions; Position information; Processing resources; Spherical coordinates; Deep learning
Securing Multimedia by Using DNA-Based Encryption in the Cloud Computing Environment,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099781069&doi=10.1145%2f3392665&partnerID=40&md5=881d7cfc6372586087fded10c43262cb,"Today, the size of a multimedia file is increasing day by day from gigabytes to terabytes or even petabytes, mainly because of the evolution of a large amount of real-time data. As most of the multimedia files are transmitted through the internet, hackers and attackers try to access the users' personal and confidential data without any authorization. Thus, maintaining a strong security technique has become a significant concerned to protect the personal information. Deoxyribonucleic Acid (DNA) computing is an advanced field for improving security, which is based on the biological concept of DNA. A novel DNA-based encryption scheme is proposed in this article for protecting multimedia files in the cloud computing environment. Here, a 1024-bit secret key is generated based on DNA computing and the user's attributes and password to encrypt any multimedia file. To generate the secret key, the decimal encoding rule, American Standard Code for Information Interchange value, DNA reference key, and complementary rule are used, which enable the system to protect the multimedia file against many security attacks. Experimental results, as well as theoretical analyses, show the efficiency of the proposed scheme over some well-known existing schemes. © 2020 ACM.",American Standard Code for Information Interchange; Cloud computing; CloudSim; complementary rule; decimal encoding rule; DNA computing,Biology; Character sets; Cloud computing; DNA; Electronic document exchange; Gene encoding; Personal computing; American standard code for information interchanges; Cloud computing environments; Confidential data; Encryption schemes; Multimedia files; Personal information; Security attacks; Strong securities; Cryptography
SDN-Assisted DDoS Defense Framework for the Internet of Multimedia Things,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099793205&doi=10.1145%2f3394956&partnerID=40&md5=fb1271e0a82bc517d051df9769222cae,"The Internet of Things is visualized as a fundamental networking model that bridges the gap between the cyber and real-world entity. Uniting the real-world object with virtualization technology is opening further opportunities for innovation in nearly every individual's life. Moreover, the usage of smart heterogeneous multimedia devices is growing extensively. These multimedia devices that communicate among each other through the Internet form a unique paradigm called the Internet of Multimedia Things (IoMT). As the volume of the collected data in multimedia application increases, the security, reliability of communications, and overall quality of service need to be maintained. Primarily, distributed denial of service attacks unveil the pervasiveness of vulnerabilities in IoMT systems. However, the Software Defined Network (SDN) is a new network architecture that has the central visibility of the entire network, which helps to detect any attack effectively. In this regard, the combination of SDN and IoMT, termed SD-IoMT, has the immense ability to improve the network management and security capabilities of the IoT system. This article proposes an SDN-assisted two-phase detection framework, namely SD-IoMT-Protector, in which the first phase utilizes the entropy technique as the detection metric to verify and alert about the malicious traffic. The second phase has trained with an optimized machine learning technique for classifying different attacks. The outcomes of the experimental results signify the usefulness and effectiveness of the proposed framework for addressing distributed denial of service issues of the SD-IoMT system. © 2020 ACM.",Control plane; entropy; IoMT; machine learning; SDN; security,Internet of things; Learning systems; Network architecture; Network security; Quality of service; Distributed denial of service; Distributed denial of service attack; Machine learning techniques; Multimedia applications; Real-world entities; Real-world objects; Security capability; Virtualization technologies; Denial-of-service attack
Motion-Aware Structured Matrix Factorization for Foreground Detection in Complex Scenes,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100310149&doi=10.1145%2f3407188&partnerID=40&md5=382ba891985e411a4c4c36c434e3aeca,"Foreground detection is one of the key steps in computer vision applications. Many foreground and background models have been proposed and achieved promising performance in static scenes. However, due to challenges such as dynamic background, irregular movement, and noise, most algorithms degrade sharply in complex scenes. To address the problem, we propose a motion-aware structured matrix factorization approach (MSMF), which integrates the structural and spatiotemporal motion information into a unified sparse-low-rank matrix factorization framework. Technologically, it has three main contributions: First, a variant of structured sparsity-inducing norm is proposed to constrain both structure and sparsity of foreground. The model is robust to the statistical variability of the underlying foreground pixels in complex scenes. Second, to capture the ambiguous pixels, a spatiotemporal cube-based motion trajectory is extracted for assisting matrix factorization. Finally, to solve the optimization problem of structured matrix factorization, we develop an augmented Lagrange multiplier method with the alternating direction strategy and Douglas-Rachford monotone operator splitting algorithm. Experiments demonstrate that the proposed approach achieves impressive performance in separating irregular moving foreground while suppressing the dynamic background and the noise, and outperforms some state-of-the-art algorithms.  © 2020 ACM.",foreground detection; lregularization; Matrix factorization; structured sparse-inducing norm,Factorization; Lagrange multipliers; Pixels; Alternating directions; Augmented lagrange multiplier methods; Computer vision applications; Matrix factorizations; Optimization problems; State-of-the-art algorithms; Statistical variability; Structured sparsities; Matrix algebra
Eye-based Recognition for User Identification on Mobile Devices,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100304554&doi=10.1145%2f3399659&partnerID=40&md5=fa44236ebe811b4cb3ca25f0c42ecc9b,"User identification is becoming more and more important for Apps on mobile devices. However, the identity recognition based on eyes, e.g., iris recognition, is rarely used on mobile devices comparing with those based on face and fingerprint due to its extra cost in hardware and complicated operations during recognition. In this article, an eye-based recognition method is designed for identity recognition on mobile devices, which can be implemented just like face recognition. In the proposed method, the eye feature is composed of the static and dynamic features, where the periocular feature extracted by deep neural network from the eye image is used as the static feature, and the motion feature of saccadic velocity is selected as the dynamic feature. The eye images can be captured by the normal camera on mobile devices just like faces, and dynamic features can provide living information to increase the difficulty of forgery. The GazeCapture dataset is used to test the proposed method, because the eye images in this dataset are captured by mobile devices during daily use. The recognition accuracy of the proposed method on the GazeCapture dataset can reach 96.87% only based on the periocular feature and can be enhanced to 97.99% when it is fused with the saccadic feature. The experiment results show that the performance of the proposed method can be comparative to that of iris recognition methods. It demonstrates that the proposed method is a practical reference for the eye-based identity recognition, and the proposed method provides one more biometric choice for mobile devices.  © 2020 ACM.",deep neural network; eye movement; eye recognition; Identity recognition; mobile device,Biometrics; Deep neural networks; Statistical tests; Dynamic features; Identity recognition; Iris recognition; Recognition accuracy; Recognition methods; Saccadic velocity; Static features; User identification; Face recognition
Analysis of the Security of Internet of Multimedia Things,2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096298900&doi=10.1145%2f3398201&partnerID=40&md5=aa04f7b8888c98d512e5e33fcc8ccecd,"To study the security performance of the Internet of multimedia things on the privacy protection of user identity, behavior trajectory, and preference under the new information technology industry wave, in this study, aiming at the problems of the sharing of Internet of things perception data and the exposure of users' privacy information, the Anonymous Batch Authentication Scheme (ABAH) for privacy protection is designed. Hash-based Message Authentication Code is used to cancel the list-checking process and analyze its security performance. Compared with the methods of elliptic curve digital signature algorithm, Bayes least-square method, identity-based bulk verification, anonymous batch authentication and key protocol, conditional privacy authentication scheme, and expert message authentication protocol, the transmission delay, packet loss rate, and computation cost are studied without considering the undo list and during the undo check. The results show that with the increase of information size, the transmission delay and packet loss rate also increase, and the transmission delay of ABAH increases by about 15%, while the correlation between speed and transmission delay is small. In the case of the same amount of validation information, ABAH has the highest validation efficiency, and it still has an efficient validation effect in the case of invalid information. The message packet loss rate for ABAH is always 0 when the undo check validation overhead is considered. It can be found that ABAH can avoid the communication overhead and privacy leakage caused by the revocation list, ensure the integrity of batch verification information, meet the security performance of the vehicular ad hoc network under the Internet of Things, and protect the privacy of users from being disclosed. © 2020 ACM.",an undo list; Internet of things; multimedia VANET; privacy information; security,Authentication; Behavioral research; Cryptography; Electronic document identification systems; Internet of things; Least squares approximations; Packet loss; Privacy by design; Vehicular ad hoc networks; Authentication scheme; Batch authentications; Communication overheads; Conditional privacies; Elliptic curve digital signature algorithm; Information technology industry; Message authentication codes; Message authentication protocols; Network security
"Data Hiding: Current Trends, Innovation and Potential Challenges",2021,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091415206&doi=10.1145%2f3382772&partnerID=40&md5=054bff5cef58713479b78194d19c9b78,"With the widespread growth of digital information and improved internet technologies, the demand for improved information security techniques has significantly increased due to privacy leakage, identity theft, illegal copying, and data distribution. Because of this, data hiding approaches have received much attention in several application areas. However, those approaches are unable to solve many issues that are necessary to measure in future investigations. This survey provides a comprehensive survey on data hiding techniques and their new trends for solving new challenges in real-world applications. The notable applications are telemedicine, 3D objects, mobile devices, cloud/distributed computing and data mining environments, chip and hardware protection, cyber physical systems, internet traffic, fusion of watermarking and encryption, joint compression and watermarking, biometric watermarking, watermarking at the physical layer, and many other perspectives. Further, the potential issues that existing approaches of data hiding face are identified. I believe that this survey will provide a valuable source of information for finding research directions for fledgling researchers and developers. © 2020 ACM.",3D objects; biometric; cloud; compression; copyright; cyber physical systems; encryption; mobile; telemedicine; Watermarking,Crime; Cryptography; Embedded systems; Steganography; Surveys; Watermarking; Application area; Biometric watermarking; Data distribution; Digital information; Hardware protection; Internet technology; Internet traffic; Mining environments; Data mining
Multi-View Graph Matching for 3D Model Retrieval,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092003158&doi=10.1145%2f3387920&partnerID=40&md5=0ef87d464a833baa7530bc08d9fadab7,"3D model retrieval has been widely utilized in numerous domains, such as computer-aided design, digital entertainment, and virtual reality. Recently, many graph-based methods have been proposed to address this task by using multi-view information of 3D models. However, these methods are always constrained by many-to-many graph matching for the similarity measure between pairwise models. In this article, we propose a multi-view graph matching method (MVGM) for 3D model retrieval. The proposed method can decompose the complicated multi-view graph-based similarity measure into multiple single-view graph-based similarity measures and fusion. First, we present the method for single-view graph generation, and we further propose the novel method for the similarity measure in a single-view graph by leveraging both node-wise context and model-wise context. Then, we propose multi-view fusion with diffusion, which can collaboratively integrate multiple single-view similarities w.r.t. different viewpoints and adaptively learn their weights, to compute the multi-view similarity between pairwise models. In this way, the proposed method can avoid the difficulty in the definition and computation of the traditional high-order graph. Moreover, this method is unsupervised and does not require a large-scale 3D dataset for model learning. We conduct evaluations on four popular and challenging datasets. The extensive experiments demonstrate the superiority and effectiveness of the proposed method compared against the state of the art. In particular, this unsupervised method can achieve competitive performances against the most recent supervised and deep learning method.  © 2020 ACM.",3D model retrieval; graph matching; unsupervised learning,3D modeling; Computer aided design; Deep learning; Graphic methods; Large dataset; Virtual reality; 3 d model retrievals; Competitive performance; Digital entertainment; Graph generation; Graph-based methods; Graph-matching methods; Similarity measure; Unsupervised method; Learning systems
Image Captioning with a Joint Attention Mechanism by Visual Concept Samples,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091997004&doi=10.1145%2f3394955&partnerID=40&md5=5a8313f799cbfb74cbd2245b066963b3,"The attention mechanism has been established as an effective method for generating caption words in image captioning; it explores one noticed subregion in an image to predict a related caption word. However, even though the attention mechanism could offer accurate subregions to train a model, the learned captioner may predict wrong, especially for visual concept words, which are the most important parts to understand an image. To tackle the preceding problem, in this article we propose Visual Concept Enhanced Captioner, which employs a joint attention mechanism with visual concept samples to strengthen prediction abilities for visual concepts in image captioning. Different from traditional attention approaches that adopt one LSTM to explore one noticed subregion each time, Visual Concept Enhanced Captioner introduces multiple virtual LSTMs in parallel to simultaneously receive multiple subregions from visual concept samples. Then, the model could update parameters by jointly exploring these subregions according to a composite loss function. Technically, this joint learning is helpful in finding the common characters of a visual concept, and thus it enhances the prediction accuracy for visual concepts. Moreover, by integrating diverse visual concept samples from different domains, our model can be extended to bridge visual bias in cross-domain learning for image captioning, which saves the cost for labeling captions. Extensive experiments have been conducted on two image datasets (MSCOCO and Flickr30K), and superior results are reported when comparing to state-of-the-art approaches. It is impressive that our approach could significantly increase BLUE-1 and F1 scores, which demonstrates an accuracy improvement for visual concepts in image captioning.  © 2020 ACM.",attention; cross-domain; Image captioning; LSTM; visual concept,Forecasting; Image enhancement; Accuracy Improvement; Attention mechanisms; Cross-domain learning; Different domains; Joint-attention mechanisms; Multiple sub-regions; Prediction accuracy; State-of-the-art approach; Long short-term memory
Sketch-guided Deep Portrait Generation,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091994892&doi=10.1145%2f3396237&partnerID=40&md5=90a50815244b8ee11f0d9571316b9d63,"Generating a realistic human class image from a sketch is a unique and challenging problem considering that the human body has a complex structure that must be preserved. Additionally, input sketches often lack important details that are crucial in the generation process, hence making the problem more complicated. In this article, we present an effective method for synthesizing realistic images from human sketches. Our framework incorporates human poses corresponding to locations of key semantic components (e.g., arm, eyes, nose), seeing that its a strong prior for generating human class images. Our sketch-image synthesis framework consists of three stages: Semantic keypoint extraction, coarse image generation, and image refinement. First, we extract the semantic keypoints using Part Affinity Fields (PAFs) and a convolutional autoencoder. Then, we integrate the sketch with semantic keypoints to generate a coarse image of a human. Finally, in the image refinement stage, the coarse image is enhanced by a Generative Adversarial Network (GAN) that adopts an architecture carefully designed to avoid checkerboard artifacts and to generate photo-realistic results. We evaluate our method on 6,300 sketch-image pairs and show that our proposed method generates realistic images and compares favorably against state-of-the-art image synthesis methods.  © 2020 ACM.",convolutional autoencoder; generative adversarial networks; Image synthesis; perceptual loss; semantic keypoints,Semantics; Adversarial networks; Complex structure; Generation process; Image generations; Image synthesis; Realistic images; Semantic components; State of the art; Image enhancement
Improving Multiperson Pose Estimation by Mask-aware Deep Reinforcement Learning,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091970283&doi=10.1145%2f3397340&partnerID=40&md5=6015933d398efc3eb21ab1e6c7897b19,"Research on single-person pose estimation based on deep neural networks has recently witnessed progress in both accuracy and execution efficiency. However, multiperson pose estimation is still a challenging topic, partially because the object regions are selected greedily from proposals via class-agnostic nonmaximum suppression (NMS), and the misalignment in the redundant detection yields inaccurate human poses. Therefore, we consider how to obtain the optimal input in human pose estimation under conditions in which intermediate label information is not available. As supervised learning-based alignment does not generalize well to unseen samples in the human pose space, in this article, we present a mask-aware deep reinforcement learning approach to modify the detection result. We use mask information to remove the adverse effects from the cluttered background and to select the optimal action according to the revised reward function. We also propose a new regularization term to punish joints that are outside of the silhouette region in the human pose estimation stage. We evaluate our approach on the MPII Multiperson dataset and the MS-COCO Keypoints Challenge. The results show that our approach yields competing inference results when it is compared to the other state-of-the-art approaches.  © 2020 ACM.",Computer vision; deep learning; regularization; reinforcement learning,Alignment; Deep neural networks; Reinforcement learning; Cluttered backgrounds; Human pose estimations; Label information; Non-maximum suppression; Optimal actions; Regularization terms; Reinforcement learning approach; State-of-the-art approach; Deep learning
Attention-Based Modality-Gated Networks for Image-Text Sentiment Analysis,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091983758&doi=10.1145%2f3388861&partnerID=40&md5=7080a57e92d15aed84b361c80f6569ff,"Sentiment analysis of social multimedia data has attracted extensive research interest and has been applied to many tasks, such as election prediction and products evaluation. Sentiment analysis of one modality (e.g., text or image) has been broadly studied. However, not much attention has been paid to the sentiment analysis of multimodal data. Different modalities usually have information that is complementary. Thus, it is necessary to learn the overall sentiment by combining the visual content with text description. In this article, we propose a novel method-Attention-Based Modality-Gated Networks (AMGN)-to exploit the correlation between the modalities of images and texts and extract the discriminative features for multimodal sentiment analysis. Specifically, a visual-semantic attention model is proposed to learn attended visual features for each word. To effectively combine the sentiment information on the two modalities of image and text, a modality-gated LSTM is proposed to learn the multimodal features by adaptively selecting the modality that presents stronger sentiment information. Then a semantic self-attention model is proposed to automatically focus on the discriminative features for sentiment classification. Extensive experiments have been conducted on both manually annotated and machine weakly labeled datasets. The results demonstrate the superiority of our approach through comparison with state-of-the-art models.  © 2020 ACM.",attention; deep learning; modality-gated; multimodal; Sentiment analysis,Image analysis; Modal analysis; Semantics; Sentiment analysis; Discriminative features; Labeled datasets; Multi-modal data; Multimodal features; Research interests; Sentiment classification; State of the art; Visual semantics; Long short-term memory
Single-stage Instance Segmentation,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091990502&doi=10.1145%2f3387926&partnerID=40&md5=53defeaa0da3de33e7cf4a5621061dc3,"Albeit the highest accuracy of object detection is generally acquired by multi-stage detectors, like R-CNN and its extension approaches, the single-stage object detectors also achieve remarkable performance with faster execution and higher scalability. Inspired by this, we propose a single-stage framework to tackle the instance segmentation task. Building on a single-stage object detection network in hand, our model outputs the detected bounding box of each instance, the semantic segmentation result, and the pixel affinity simultaneously. After that, we generate the final instance masks via a fast post-processing method with the help of the three outputs above. As far as we know, it is the first attempt to segment instances in a single-stage pipeline on challenging datasets. Extensive experiments demonstrate the efficiency of our post-processing method, and the proposed framework obtains competitive results as a single-stage instance segmentation method. We achieve 32.5 box AP and 26.0 mask AP on the COCO validation set with 500 pixels input scale and 22.9 mask AP on the Cityscapes test set.  © 2020 ACM.",graph merge; Instance segmentation; neural networks; single stage,Image segmentation; Object recognition; Pixels; Processing; Semantics; Bounding box; Detection networks; Model outputs; Object detectors; Pixel affinities; Postprocessing methods; Segmentation methods; Semantic segmentation; Object detection
Few-shot Food Recognition via Multi-view Representation Learning,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091971270&doi=10.1145%2f3391624&partnerID=40&md5=be0dad48cb0bdac712604966a9007948,"This article considers the problem of few-shot learning for food recognition. Automatic food recognition can support various applications, e.g., dietary assessment and food journaling. Most existing works focus on food recognition with large numbers of labelled samples, and fail to recognize food categories with few samples. To address this problem, we propose a Multi-View Few-Shot Learning (MVFSL) framework to explore additional ingredient information for few-shot food recognition. Besides category-oriented deep visual features, we introduce ingredient-supervised deep network to extract ingredient-oriented features. As general and intermediate attributes of food, ingredient-oriented features are informative and complementary to category-oriented features, and thus they play an important role in improving food recognition. Particularly in few-shot food recognition, ingredient information can bridge the gap between disjoint training categories and test categories. To take advantage of ingredient information, we fuse these two kinds of features by first combining their feature maps from their respective deep networks and then convolving combined feature maps. Such convolution is further incorporated into a multi-view relation network, which is capable of comparing pairwise images to enable fine-grained feature learning. MVFSL is trained in an end-to-end fashion for joint optimization on two types of feature learning subnetworks and relation subnetworks. Extensive experiments on different food datasets have consistently demonstrated the advantage of MVFSL in multi-view feature fusion. Furthermore, we extend another two types of networks, namely, Siamese Network and Matching Network, by introducing ingredient information for few-shot food recognition. Experimental results have also shown that introducing ingredient information into these two networks can improve the performance of few-shot food recognition.  © 2020 ACM.",deep learning; few-shot learning; Food recognition; visual recognition,Combined features; Dietary assessments; Feature fusion; Feature learning; Joint optimization; Matching networks; Oriented features; Visual feature; Machine learning
Learning Joint Structure for Human Pose Estimation,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091986412&doi=10.1145%2f3392302&partnerID=40&md5=0b14bb983ed2107862b30fc44cd77dfa,"Recently, tremendous progress has been achieved on human pose estimation with the development of convolutional neural networks (CNNs). However, current methods still suffer from severe occlusion, back view, and large pose variation due to the lack of consideration of the spatial relationship between different joints, which can provide strong cues for localizing the hidden keypoints. In this work, we design a Structural Pose Network (SPN) to take full advantage of joint structure for human pose estimation under unconstrained environment. Specifically, the proposed model is composed of two subnets: Structure Residual Network (SRN) and Structure Improving Network (SIN). Given an input image, SRN first captures rich joint structure as priors through a multi-branch feature extraction module, following a hourglass network with pyramid residual units to enlarge the receptive field and further obtain structural feature representations. SIN, based on coordinate regression, can optimize the spatial relationship of different joints via the attention mechanism, thus refining the initial prediction from SRN. In addition, we propose a novel structure-consistency constraint, which can maintain the structural consistency between the joints and body parts via estimating whether the joints are located in their corresponding parts. At the same time, an online hard regions mining (OHRM) strategy is introduced to drive the network to pay corresponding attention to different body parts. The experimental results on three challenging datasets show that our method outperforms other state-of-the-art algorithms.  © 2020 ACM.",heatmap and offset estimation; Learning joint structure; single person pose estimation; structural consistency,Computer networks; Attention mechanisms; Consistency constraints; Human pose estimations; Severe occlusions; Spatial relationships; State-of-the-art algorithms; Structural feature; Unconstrained environments; Convolutional neural networks
Constrained LSTM and Residual Attention for Image Captioning,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091973861&doi=10.1145%2f3386725&partnerID=40&md5=b122ca8205bb2d5acb6a49eb1f8c8611,"Visual structure and syntactic structure are essential in images and texts, respectively. Visual structure depicts both entities in an image and their interactions, whereas syntactic structure in texts can reflect the part-of-speech constraints between adjacent words. Most existing methods either use visual global representation to guide the language model or generate captions without considering the relationships of different entities or adjacent words. Thus, their language models lack relevance in both visual and syntactic structure. To solve this problem, we propose a model that aligns the language model to certain visual structure and also constrains it with a specific part-of-speech template. In addition, most methods exploit the latent relationship between words in a sentence and pre-extracted visual regions in an image yet ignore the effects of unextracted regions on predicted words. We develop a residual attention mechanism to simultaneously focus on the pre-extracted visual objects and unextracted regions in an image. Residual attention is capable of capturing precise regions of an image corresponding to the predicted words considering both the effects of visual objects and unextracted regions. The effectiveness of our entire framework and each proposed module are verified on two classical datasets: MSCOCO and Flickr30k. Our framework is on par with or even better than the state-of-the-art methods and achieves superior performance on COCO captioning Leaderboard.  © 2020 ACM.",Image captioning; LSTM; object detection; visual attention; visual skeleton,Computational linguistics; Image processing; Long short-term memory; Syntactics; Attention mechanisms; Global representation; Image captioning; Part Of Speech; State-of-the-art methods; Syntactic structure; Visual objects; Visual structure; Visual languages
Deep Triplet Neural Networks with Cluster-CCA for Audio-Visual Cross-Modal Retrieval,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091979849&doi=10.1145%2f3387164&partnerID=40&md5=ea9da5e3e018c4f0b175ad4f393ec96b,"Cross-modal retrieval aims to retrieve data in one modality by a query in another modality, which has been a very interesting research issue in the field of multimedia, information retrieval, and computer vision, and database. Most existing works focus on cross-modal retrieval between text-image, text-video, and lyrics-audio. Little research addresses cross-modal retrieval between audio and video due to limited audio-video paired datasets and semantic information. The main challenge of the audio-visual cross-modal retrieval task focuses on learning joint embeddings from a shared subspace for computing the similarity across different modalities, where generating new representations is to maximize the correlation between audio and visual modalities space. In this work, we propose TNN-C-CCA, a novel deep triplet neural network with cluster canonical correlation analysis, which is an end-to-end supervised learning architecture with an audio branch and a video branch. We not only consider the matching pairs in the common space but also compute the mismatching pairs when maximizing the correlation. In particular, two significant contributions are made. First, a better representation by constructing a deep triplet neural network with triplet loss for optimal projections can be generated to maximize correlation in the shared subspace. Second, positive examples and negative examples are used in the learning stage to improve the capability of embedding learning between audio and video. Our experiment is run over fivefold cross validation, where average performance is applied to demonstrate the performance of audio-video cross-modal retrieval. The experimental results achieved on two different audio-visual datasets show that the proposed learning architecture with two branches outperforms existing six canonical correlation analysis-based methods and four state-of-the-art-based cross-modal retrieval methods.  © 2020 ACM.",cluster-CCA; cross-modal retrieval; Deep triplet neural networks; triplet loss,Cluster computing; Correlation methods; Deep neural networks; Embeddings; Information retrieval; Multimedia systems; Network architecture; Query processing; Semantics; Canonical correlation analysis; Cross validation; Learning architectures; Negative examples; Positive examples; Retrieval methods; Semantic information; Visual modalities; Neural networks
Recurrent Attention Network with Reinforced Generator for Visual Dialog,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091995994&doi=10.1145%2f3390891&partnerID=40&md5=5ebeceec2060549daea25ff4514d3584,"In Visual Dialog, an agent has to parse temporal context in the dialog history and spatial context in the image to hold a meaningful dialog with humans. For example, to answer ""what is the man on her left wearing?""the agent needs to (1) analyze the temporal context in the dialog history to infer who is being referred to as ""her,""(2) parse the image to attend ""her,""and (3) uncover the spatial context to shift the attention to ""her left""and check the apparel of the man. In this article, we use a dialog network to memorize the temporal context and an attention processor to parse the spatial context. Since the question and the image are usually very complex, which makes it difficult for the question to be grounded with a single glimpse, the attention processor attends to the image multiple times to better collect visual information. In the Visual Dialog task, the generative decoder (G) is trained under the word-by-word paradigm, which suffers from the lack of sentence-level training. We propose to reinforce G at the sentence level using the discriminative model (D), which aims to select the right answer from a few candidates, to ameliorate the problem. Experimental results on the VisDial dataset demonstrate the effectiveness of our approach.  © 2020 ACM.",deep learning; reinforcement learning; vision and language; Visual Dialog,Reinforcement; Discriminative models; Sentence level; Spatial context; Visual information; Recurrent neural networks
Posed and Spontaneous Expression Distinction Using Latent Regression Bayesian Networks,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091983437&doi=10.1145%2f3391290&partnerID=40&md5=30410c3a1935a4e84f69d71662da3dc1,"Facial spatial patterns can help distinguish between posed and spontaneous expressions, but this information has not been thoroughly leveraged by current studies. We present several latent regression Bayesian networks (LRBNs) to capture the patterns existing in facial landmark points and to use those points to differentiate posed from spontaneous expressions. The visible nodes of the LRBN represent facial landmark points. Through learning, the LRBN captures the probabilistic dependencies among landmark points as well as latent variables given observations, successfully modeling the spatial patterns inherent in expressions. Current methods tend to ignore gender and expression categories, although these factors can influence spatial patterns. Therefore, we propose to incorporate this as a kind of privileged information. We construct several LRBNs to capture spatial patterns from spontaneous and posed facial expressions given expression-related factors. Facial landmark points are used during testing to classify samples as either posed or spontaneous, depending on which LRBN has the largest likelihood. We conduct experiments to showcase the superiority of the proposed approach in both modeling spatial patterns and classifying expressions as either posed or spontaneous.  © 2020 ACM.",Latent regression Bayesian network; posed and spontaneous expression distinction; privileged information; spatial pattern,Computer networks; Facial Expressions; Facial landmark; Latent variable; Related factors; Spatial patterns; Bayesian networks
Kernel Attention Network for Single Image Super-Resolution,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091983577&doi=10.1145%2f3398685&partnerID=40&md5=4e0146c1aa53be01bf3c6d7204d5d978,"Recently, attention mechanisms have shown a developing tendency toward convolutional neural network (CNN), and some representative attention mechanisms, i.e., channel attention (CA) and spatial attention (SA) have been fully applied to single image super-resolution (SISR) tasks. However, the existing architectures directly apply these attention mechanisms to SISR without much consideration of the nature characteristic, resulting in less strong representational power. In this article, we propose a novel kernel attention module (KAM) for SISR, which enables the network to adjust its receptive field size corresponding to various scales of input by dynamically selecting the appropriate kernel. Based on this, we stack multiple kernel attention modules with group and residual connection to constitute a novel architecture for SISR, which enables our network to learn more distinguishing representations through filtering the information under different receptive fields. Thus, our network is more sensitive to multi-scale features, which enables our single network to deal with multi-scale SR task by predefining the upscaling modules. Besides, other attention mechanisms in super-resolution are also investigated and illustrated in detail in this article. Thanks to the kernel attention mechanism, the extensive benchmark evaluation shows that our method outperforms the other state-of-the-art methods.  © 2020 ACM.",Image super-resolution; kernel attention; multi-scale features; receptive field,Information filtering; Network architecture; Optical resolving power; Attention mechanisms; Benchmark evaluation; Existing architectures; Multi-scale features; Novel architecture; Receptive field sizes; Spatial attention; State-of-the-art methods; Convolutional neural networks
3D Facial Similarity Measurement and Its Application in Facial Organization,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091982760&doi=10.1145%2f3397765&partnerID=40&md5=f979286aee86bf26b6d8b16c57f86d97,"We propose a novel framework for 3D facial similarity measurement and its application in facial organization. The construction of the framework is based on Kendall shape space theory. Kendall shape space is a quotient space that is constructed by shape features. In Kendall shape space, the shape features can be measured and is robust to similarity transformations. In our framework, a 3D face is represented by the facial feature landmarks model (FFLM), which can be regarded as the facial shape features. We utilize the geodesic in Kendall shape space to represent the FFLM similarity measurement, which can be regarded as the 3D facial similarity measurement. The FFLM similarity measurement is robust to facial expressions, head poses, and partial facial data. In our experiments, we compute the distance between different FFLMs in two public facial databases: FRGC2.0 and BosphorusDB. On average, we achieve a rank-one facial recognition rate of 98%. Based on the similarity results, we propose a method to construct the facial organization. The facial organization is a hierarchical structure that is achieved from the facial clustering by FFLM similarity measurement. Based on the facial organization, the performance of face searching in a large facial database can be improved obviously (about 400% improvement in experiments).  © 2020 ACM.",face recognition; facial data organization; facial feature landmarks model; Kendall shape space,Hierarchical clustering; Facial Expressions; Facial recognition; Hierarchical structures; ITS applications; Shape features; Shape space theory; Similarity measurements; Similarity transformation; Face recognition
Upgrading the Newsroom: An Automated Image SelectionSystem for News Articles,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092004738&doi=10.1145%2f3396520&partnerID=40&md5=4e1cc8f94b3caa2869f715c8771527c7,"We propose an automated image selection system to assist photo editors in selecting suitable images for news articles. The system fuses multiple textual sources extracted from news articles and accepts multilingual inputs. It is equipped with char-level word embeddings to help both modeling morphologically rich languages, e.g., German, and transferring knowledge across nearby languages. The text encoder adopts a hierarchical self-attention mechanism to attend more to both key words within a piece of text and informative components of a news article. We extensively experiment our system on a large-scale text-image database containing multimodal multilingual news articles collected from Swiss local news media websites. The system is compared with multiple baselines with ablation studies and is shown to beat existing text-image retrieval methods in a weakly supervised learning setting. Besides, we also offer insights on the advantage of using multiple textual sources and multilingual data.  © 2020 ACM.",deep learning; multimodal machine learning; Multimodal retrieval; natural language processing; neural networks; news image article analysis; news media,Image retrieval; Modeling languages; Attention mechanisms; Image selection; Multi-modal; News articles; Photo editors; Retrieval methods; Text images; Weakly supervised learning; Search engines
Blind Image Quality Assessment by Natural Scene Statistics and Perceptual Characteristics,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091974980&doi=10.1145%2f3414837&partnerID=40&md5=0a74424c1e13f06ed4f737439328ab9a,"Opinion-unaware blind image quality assessment (OU BIQA) refers to establishing a blind quality prediction model without using the expensive subjective quality scores, which is a highly promising direction in the BIQA research. In this article, we focus on OU BIQA and propose a novel OU BIQA method. Specifically, in our proposed method, we deeply investigate the natural scene statistics (NSS) and the perceptual characteristics of the human brain for visual perception. Accordingly, a set of quality-aware NSS and perceptual characteristics-related features are designed to characterize the image quality effectively. For inferring the image quality, we learn a pristine multivariate Gaussian (MVG) model on a collection of pristine images, which serves as the reference information for quality evaluation. At last, the quality of a new given image is defined by measuring the divergence between its MVG model and the learned pristine MVG model. Thorough experiments performed on seven popular image databases demonstrate that the proposed OU BIQA method delivers superior performance to the state-of-the-art OU BIQA methods. The Matlab source code of the proposed method will be made publicly available at https://github.com/YT2015?tab=;repositories.  © 2020 ACM.",Blind image quality assessment (BIQA); free-energy principle; natural scene statistics (NSS); sparse representation,Predictive analytics; Image database; Image quality assessment; Natural scene statistics; Quality evaluation; Quality prediction models; State of the art; Subjective quality; Visual perception; Image quality
A Unified Tensor Framework for Clustering and Simultaneous Reconstruction of Incomplete Imaging Data,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091961717&doi=10.1145%2f3399806&partnerID=40&md5=f81c675fb40f3852b6671a44a968bfec,"Incomplete observations in the data are always troublesome to data clustering algorithms. In fact, most of the well-received techniques are not designed to encounter such imperative scenarios. Hence, clustering of images under incomplete samples is an inquisitive yet unaddressed area of research. Therefore, the aim of this article is to design a single-stage optimization procedure for clustering as well as simultaneous reconstruction of images without breaking the intrinsic spatial structure. The method employs the self-expressiveness property of submodules, and images are stacked as the lateral slices of a three-dimensional tensor. The proposed optimization method is designed to extract a sparse t-linear combination tensor with low multirank constraint, consisting of a unique set of linear coefficients in the form of mode-3 fibers and the spectral clustering is performed on these fibers. Simultaneously, the recovery of lost samples is accomplished by twisting the entire lateral slices of the data tensor and applying a low-rank approximation on each slice. The prominence of the proposed method lies in the simultaneous execution of data clustering and reconstruction of incomplete observations in a single step. Experimental results reveal the excellence of the proposed method over state-of-the-art clustering algorithms in the context of incomplete imaging data.  © 2020 ACM.",Image clustering; image completion; low-rank approximation; subspace clustering; union of free submodules,Approximation theory; Cluster analysis; Image processing; Tensors; Data clustering algorithm; Incomplete observation; Linear coefficients; Linear combinations; Low rank approximations; Optimization method; Optimization procedures; Simultaneous reconstruction; Clustering algorithms
"Design, Analysis, and Implementation of Efficient Framework for Image Annotation",2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091963239&doi=10.1145%2f3386249&partnerID=40&md5=e9bf6052063dc41b03e7501e29340e6e,"In this article, a general framework of image annotation is proposed by involving salient object detection (SOD), feature extraction, feature selection, and multi-label classification. For SOD, Augmented-Gradient Vector Flow (A-GVF) is proposed, which fuses benefits of GVF and Minimum Directional Contrast. The article also proposes to control the background information to be included for annotation. This article brings about a comprehensive study of all major feature selection methods for a study on four publicly available datasets. The study concludes with the proposition of using Fisher's method for reducing the dimension of features. Moreover, this article also proposes a set of features that are found to be strong discriminants by most of the methods. This reduced set for image annotation gives 3-4% better accuracy across all the four datasets. This article also proposes an improved multi-label classification algorithm C-MLFE.  © 2020 ACM.",feature selection; Image annotation; multi-label classification; salient object detection; scene analysis,Classification (of information); Feature extraction; Image segmentation; Object detection; Background information; Directional contrasts; Feature selection methods; Gradient vector flow; Multi label classification; Salient object detection; Image annotation
A New Transfer Function for Volume Visualization of Aortic Stent and Its Application to Virtual Endoscopy,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088975256&doi=10.1145%2f3373358&partnerID=40&md5=c60b81506814d8c68c30154ff11e6405,"Aortic stent has been widely used in restoring vascular stenosis and assisting patients with cardiovascular disease. The effective visualization of aortic stent is considered to be critical to ensure the effectiveness and functions of the aortic stent in clinical practice. Volume rendering with ray casting has been used as an effective approach to enable the effective visualization of aortic stent. The volume rendering relies on the transfer function that converts the medical images into optical attributes including color and transparency. This article proposes a new transfer function, namely, the multi-dimensional transfer function, to provide additional transparency value of a voxel. The proposed approach using the additional transparency value effectively assists the distinguishing of tissues that have the same CT value. The transparency values are simultaneously determined by gray threshold and gray change threshold, which can recognize the unnecessary structures such as bones transparent. A series of experimental results demonstrate that the situation of aorta stent of a patient can be directly observed, and the angle of view can be switched arbitrarily. The proposed method provides a new way for the operation of a virtual endoscopy to reach the place of blood vessels that a traditional endoscopy fails to reach.  © 2020 ACM.",Aortic stent visualization; transfer function; virtual endoscopy; volume rendering,Blood vessels; Computerized tomography; Endoscopy; Medical imaging; Stents; Transparency; Visualization; Volume rendering; Cardio-vascular disease; Clinical practices; Effective approaches; ITS applications; Multi dimensional transfer functions; Vascular stenosis; Virtual endoscopy; Volume visualization; Transfer functions
Spatio-Temporal Deep Residual Network with Hierarchical Attentions for Video Event Recognition,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088962462&doi=10.1145%2f3378026&partnerID=40&md5=186f139528de821989a4811d3f7c1aea,"Event recognition in surveillance video has gained extensive attention from the computer vision community. This process still faces enormous challenges due to the tiny inter-class variations that are caused by various facets, such as severe occlusion, cluttered backgrounds, and so forth. To address these issues, we propose a spatio-temporal deep residual network with hierarchical attentions (STDRN-HA) for video event recognition. In the first attention layer, the ResNet fully connected feature guides the Faster R-CNN feature to generate object-based attention (O-attention) for target objects. In the second attention layer, the O-attention further guides the ResNet convolutional feature to yield the holistic attention (H-attention) in order to perceive more details of the occluded objects and the global background. In the third attention layer, the attention maps use the deep features to obtain the attention-enhanced features. Then, the attention-enhanced features are input into a deep residual recurrent network, which is used to mine more event clues from videos. Furthermore, an optimized loss function named softmax-RC is designed, which embeds the residual block regularization and center loss to solve the vanishing gradient in a deep network and enlarge the distance between inter-classes. We also build a temporal branch to exploit the long- A nd short-term motion information. The final results are obtained by fusing the outputs of the spatial and temporal streams. Experiments on the four realistic video datasets, CCV, VIRAT 1.0, VIRAT 2.0, and HMDB51, demonstrate that the proposed method has good performance and achieves state-of-the-art results.  © 2020 ACM.",deep residual recurrent network; Event recognition; hierarchical attention; spatio-temporal; surveillance video,Recurrent neural networks; Cluttered backgrounds; Event recognition; Motion information; Object-based attention; Recurrent networks; Surveillance video; Vanishing gradient; Vision communities; Security systems
An Adaptive Two-Layer Light Field Compression Scheme Using GNN-Based Reconstruction,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088964865&doi=10.1145%2f3395620&partnerID=40&md5=24ca46ccb76c1d1353389480858dc9dc,"As a new form of volumetric media, Light Field (LF) can provide users with a true six degrees of freedom immersive experience because LF captures the scene with photo-realism, including aperture-limited changes in viewpoint. But uncompressed LF data is too large for network transmission, which is the reason why LF compression has become an important research topic. One of the more recent approaches for LF compression is to reduce the angular resolution of the input LF during compression and to use LF reconstruction to recover the discarded viewpoints during decompression. Following this approach, we propose a new LF reconstruction algorithm based on Graph Neural Networks; we show that it can achieve higher compression and better quality compared to existing reconstruction methods, although suffering from the same problem as those methods-the inability to deal effectively with high-frequency image components. To solve this problem, we propose an adaptive two-layer compression architecture that separates high-frequency and low-frequency components and compresses each with a different strategy so that the performance can become robust and controllable. Experiments with multiple datasets1 show that our proposed scheme is capable of providing a decompression quality of above 40 dB, and can significantly improve compression efficiency compared with similar LF reconstruction schemes.  © 2020 ACM.",light field compression; Light field image; light field reconstruction,Graph algorithms; Angular resolution; Compression efficiency; Graph neural networks; Low-frequency components; Network transmission; Reconstruction algorithms; Reconstruction method; Six degrees of freedom; Degrees of freedom (mechanics)
Proposal Complementary Action Detection,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088926870&doi=10.1145%2f3361845&partnerID=40&md5=ad7d268d3fe490c8785a8f5f7bfabb65,"Temporal action detection not only requires correct classification but also needs to detect the start and end times of each action accurately. However, traditional approaches always employ sliding windows or actionness to predict the actions, and it is different to train to model with sliding windows or actionness by end-to-end means. In this article, we attempt a different idea to detect the actions end-to-end, which can calculate the probabilities of actions directly through one network as one part of the results. We present PCAD, a novel proposal complementary action detector to deal with video streams under continuous, untrimmed conditions. Our approach first uses a simple fully 3D convolutional network to encode the video streams and then generates candidate temporal proposals for activities by using anchor segments. To generate more precise proposals, we also design a boundary proposal network to offer some complementary information for the candidate proposals. Finally, we learn an efficient classifier to classify the generated proposals into different activities and refine their temporal boundaries at the same time. Our model can achieve end-to-end training by jointly optimizing classification loss and regression loss. When evaluating on the THUMOS'14 detection benchmark, PCAD achieves state-of-the-art performance in high-speed models.  © 2020 ACM.",3D convolutional network; boundary proposal network; Temporal action detection,Benchmarking; Video streaming; Convolutional networks; End to end; High Speed; One parts; Sliding Window; State-of-the-art performance; Traditional approaches; Convolutional neural networks
Requet: Real-Time QoE Metric Detection for Encrypted YouTube Traffic,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088964993&doi=10.1145%2f3394498&partnerID=40&md5=1cf38631c1cc2b738a40dd65a8ce0bc1,"As video traffic dominates the Internet, it is important for operators to detect video quality of experience (QoE) to ensure adequate support for video traffic. With wide deployment of end-to-end encryption, traditional deep packet inspection-based traffic monitoring approaches are becoming ineffective. This poses a challenge for network operators to monitor user QoE and improve upon their experience. To resolve this issue, we develop and present a system for REal-time QUality of experience metric detection for Encrypted Traffic-Requet-which is suitable for network middlebox deployment. Requet uses a detection algorithm that we develop to identify video and audio chunks from the IP headers of encrypted traffic. Features extracted from the chunk statistics are used as input to a machine learning algorithm to predict QoE metrics, specifically buffer warning (low buffer, high buffer), video state (buffer increase, buffer decay, steady, stall), and video resolution. We collect a large YouTube dataset consisting of diverse video assets delivered over various WiFi and LTE network conditions to evaluate the performance. We compare Requet with a baseline system based on previous work and show that Requet outperforms the baseline system in accuracy of predicting buffer low warning, video state, and video resolution by 1.12×, 1.53×, and 3.14×, respectively.  © 2020 ACM.",HTTP adaptive streaming; Machine learning,Cryptography; Large dataset; Learning algorithms; Machine learning; Mobile telecommunication systems; Wi-Fi; Deep packet inspection; Detection algorithm; Encrypted traffic; End-to-end encryption; Network middlebox; Real-time quality; Traffic monitoring; Video resolutions; Quality of service
Cross-Domain Brain CT Image Smart Segmentation via Shared Hidden Space Transfer FCM Clustering,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088996773&doi=10.1145%2f3357233&partnerID=40&md5=e5daa26c5c1856693fd3b72e16c6fd91,"Clustering is an important issue in brain medical image segmentation. Original medical images used for clinical diagnosis are often insufficient for clustering in the current domain. As there are sufficient medical images in the related domains, transfer clustering can improve the clustering performance of the current domain by transferring knowledge across the related domains. In this article, we propose a novel shared hidden space transfer fuzzy c-means (FCM) clustering called SHST-FCM for cross-domain brain computed tomography (CT) image segmentation. SHST-FCM projects both the data samples of the source domain and target domain into the shared hidden space, such that the distributions of the two domains are as close as possible. In the learned shared subspace, the data samples of the source domain serve as the auxiliary knowledge to aid the clustering process in the target domain. Extensive experiments on brain CT medical image datasets indicate the effectiveness of the proposed method.  © 2020 ACM.",clinical diagnosis; FCM clustering; Image segmentation; transfer clustering,Brain; Clustering algorithms; Diagnosis; Image enhancement; Image segmentation; Medical imaging; Clinical diagnosis; Clustering process; FCM clustering; Fuzzy C means clustering; Image datasets; Shared hidden spaces; Shared subspaces; Target domain; Computerized tomography
Modeling Long-Term Dependencies from Videos Using Deep Multiplicative Neural Networks,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088965148&doi=10.1145%2f3357797&partnerID=40&md5=a7dfd143f2792b2533d9757d73faa13b,"Understanding temporal dependencies of videos is fundamental for vision problems, but deep learning-based models are still insufficient in this field. In this article, we propose a novel deep multiplicative neural network (DMNN) for learning hierarchical long-term representations from video. The DMNN is built upon the multiplicative block that remembers the pairwise transformations between consecutive frames using multiplicative interactions rather than the regular weighted-sum ones. The block is slided over the timesteps to update the memory of the networks on the frame pairs. Deep architecture can be implemented by stacking multiple layers of the sliding blocks. The multiplicative interactions lead to exact, rather than approximate, modeling of temporal dependencies. The memory mechanism can remember the temporal dependencies for an arbitrary length of time. The multiple layers output multiple-level representations that reflect the multi-timescale structure of video. Moreover, to address the difficulty of training DMNNs, we derive a theoretically sound convergent method, which leads to a fast and stable convergence. We demonstrate a new state-of-the-art classification performance with proposed networks on the UCF101 dataset and the effectiveness of capturing complicate temporal dependencies on a variety of synthetic datasets.  © 2020 ACM.",Deep learning; temporal dependencies; video recognition,Classification (of information); Deep learning; Deep neural networks; Classification performance; Deep architectures; Learning Based Models; Long-term dependencies; Multiplicative neural networks; Stable convergence; Synthetic datasets; Term representation; Neural networks
Evaluation of Shared Resource Allocation Using SAND for ABR Streaming,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088996544&doi=10.1145%2f3388926&partnerID=40&md5=bc7db440cdd6469b9587643248b2dcf1,"Adaptive bitrate media streaming clients adjust the quality of media content depending on the current network conditions. The shared resource allocation (SRA) feature defined in MPEG-SAND (server and network assisted DASH) allows servers to allocate bandwidth to streaming clients. This enables coordination and prioritization of clients that are connected to the same network bottleneck (e.g., to maximize the number of clients that can play back a stream fluently). In this article, we evaluate different bandwidth limitation strategies and analyze the effects on the clients. For this purpose, a testbed using multiple Raspberry Pis was created. The results show that in various scenarios, SRA improves the fairness and the QoE of streaming sessions. Solely allocating a maximum quality level to the client is not sufficient in some cases. Therefore, additional means, such as limiting bandwidth on the client or traffic shaping with software-defined networking for SRA, are evaluated.  © 2020 ACM.",DANE; dash.js; OpenFlow; SAND; shared resource allocation; software defined networking; streaming video metrics,Bandwidth; Resource allocation; Bandwidth limitation; Network bottlenecks; Network condition; Prioritization; Quality levels; Streaming clients; Streaming sessions; Traffic-shaping; Media streaming
Introduction to the Best Papers from the ACM Multimedia Systems (MMSys) 2019 and Co-Located Workshops,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088996428&doi=10.1145%2f3398384&partnerID=40&md5=987b177b1d6e7763d8c6758cfd5d0a37,[No abstract available],,
A Practical Learning-based Approach for Viewer Scheduling in the Crowdsourced Live Streaming,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088961713&doi=10.1145%2f3397226&partnerID=40&md5=426970ef19dbc4b7e07aba3643f555f8,"Scheduling viewers effectively among different Content Delivery Network (CDN) providers is challenging owing to the extreme diversity in the crowdsourced live streaming (CLS) scenarios. Abundant algorithms have been proposed in recent years, which, however, suffer from a critical limitation: Due to their inaccurate feature engineering or naive rules, they cannot optimally schedule viewers. To address this concern, we put forward LTS (Learn to Schedule), a novel scheduling algorithm that can adapt to the dynamics from both viewer traffics and CDN performance. In detail, we first propose LTS-RL, an approach that schedules CLS viewers based on deep reinforcement learning (DRL). Since LTS-RL is trained in an end-to-end way, it can automatically learn scheduling algorithms without any pre-programmed models or assumptions about the environment dynamics. At the same time, to practically deploy LTS-RL, we then use the decision tree and imitation learning to convert LTS-RL into a more light-weighted and interpretable model, which is denoted as Fast-LTS. After the extensive evaluation of the real data from a leading CLS platform in China, we demonstrate that our proposed model (both LTS-RL and Fast-LTS) can improve the average quality of experience (QoE) over state-of-the-art approaches by 8.71-15.63%. At the same time, we also demonstrate that Fast-LTS can faithfully convert the complicated LTS-RL with slight performance degradation (< 2%), while significantly reducing the decision time (×7-10).  © 2020 ACM.",Crowdsourced live streaming; reinforcement learning; scheduling,Crowdsourcing; Decision trees; Deep learning; Quality of service; Scheduling; Content delivery network; Environment dynamics; Feature engineerings; Learning-based approach; Novel scheduling algorithms; Performance degradation; Quality of experience (QoE); State-of-the-art approach; Reinforcement learning
The Impact of Motion and Delay on Selecting Game Targets with a Mouse,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088976128&doi=10.1145%2f3390464&partnerID=40&md5=5563ea1f6a243b6834fb2e1dea56bb52,"All real-time computer games, particularly networked computer games, have a delay from when a player starts an action (e.g., clicking the mouse) until the game renders the result (e.g., firing a projectile). This delay can degrade both player performance (e.g., reduced game score) and quality of experience (e.g., the game is less fun). While previous work has studied the effects of delay on commercial games and individual game actions, a more detailed understanding is needed of the effects of delay on moving target selection with realistic target motion, a common scenario in many games. This paper presents an in-depth study of the effects of delay on the fundamental game action of selecting a moving target with a mouse while varying two parameters for the target motion-turn frequency and turn angle. We design and implement a custom game where players select moving targets using a mouse, while the game controls both the target motion and input delay. Analysis of data gathered in a 56-person user study shows both target selection time and accuracy degrade with delay. However, both selection time and accuracy increase with the frequency and angle of changes in the target's movement, because turning slows targets down even while making them less predictable. We set these results in the context of other studies of delay and target selection by comparing our findings to those in seven other previously published papers that investigated the effects of delay on target selection.  © 2020 ACM.",Lag; target selection; user study,Mammals; Quality of service; Analysis of data; Design and implements; Moving target selections; Networked computers; Quality of experience (QoE); Real-time computer; Target motions; Target selection; Computer games
QoE-Fair DASH Video Streaming Using Server-side Reinforcement Learning,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088974223&doi=10.1145%2f3397227&partnerID=40&md5=1a1d0395ec6d05078caa3ca4b66e70a1,"To design an optimal adaptive video streaming method, video service providers need to consider both the efficiency and the fairness of the Quality of Experience (QoE) of their users. In Reference [8], we proposed a server-side QoE-fair rate adaptation method that considers both efficiency and fairness of the QoE. The server uses Reinforcement Learning (RL) to select a bitrate for each client sharing the same bottleneck link to the server in a way that achieves fairness among concurrent DASH clients and imposes that bitrate by dynamically modifying the client's Media Presentation Description (MPD) file. In this article, we extend that work to minimize the number of actions the server needs to take to keep the system in its equilibrium state. By incorporating a Recurrent Neural Network, specifically an LSTM model, we modify the server's training algorithm to achieve improvements in both the quality and the quantity of actions the server takes to guide the client. Performance evaluation of the modified algorithm for clients running both homogeneous and heterogeneous adaptation algorithms showed that the number of server actions dropped by 14% and 22%, respectively, while QoE-fairness improved by at least 6% and 10%, respectively.  © 2020 ACM.",DASH; DASH fairness; Dec-POMDP; QoE; reinforcement learning; video rate adaptation,Efficiency; Long short-term memory; Reinforcement learning; Video streaming; Adaptation algorithms; Adaptive video streaming; Efficiency and fairness; Equilibrium state; Modified algorithms; Quality of experience (QoE); Training algorithms; Video service providers; Quality of service
Performance Analysis of ACTE,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089008975&doi=10.1145%2f3387921&partnerID=40&md5=35a5e1c40dbedfd8fa2a85791b4b14cb,"HTTP adaptive streaming with chunked transfer encoding can offer low-latency streaming without sacrificing the coding efficiency. This allows media segments to be delivered while still being packaged. However, conventional schemes often make widely inaccurate bandwidth measurements due to the presence of idle periods between the chunks and hence this is causing sub-optimal adaptation decisions. To address this issue, we earlier proposed ACTE (ABR for Chunked Transfer Encoding) [6], a bandwidth prediction scheme for low-latency chunked streaming. While ACTE was a significant step forward, in this study we focus on two still remaining open areas, namely, (i) quantifying the impact of encoding parameters, including chunk and segment durations, bitrate levels, minimum interval between IDR-frames and frame rate on ACTE, and (ii) exploring the impact of video content complexity on ACTE. We thoroughly investigate these questions and report on our findings. We also discuss some additional issues that arise in the context of pursuing very low latency HTTP video streaming.  © 2020 ACM.",ABR; bandwidth measurement and prediction; CMAF; DASH; encoding parameters; FFmpeg; HAS; HTTP chunked transfer encoding; low-latency; RLS,Bandwidth; Encoding (symbols); HTTP; Adaptive streaming; Bandwidth measurements; Bandwidth prediction; Conventional schemes; Encoding parameters; Http video streaming; Optimal adaptation; Performance analysis; Signal encoding
Introduction to the Special Issue on Smart Communications and Networking for Future Video Surveillance,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088946719&doi=10.1145%2f3398382&partnerID=40&md5=1a6ad15e6ea5b70d6b91fbd52d41d381,[No abstract available],,
Smart Diagnosis: A Multiple-Source Transfer TSK Fuzzy System for EEG Seizure Identification,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088972155&doi=10.1145%2f3340240&partnerID=40&md5=048ef91287733e8f61d38081e8c89dd4,"To effectively identify electroencephalogram (EEG) signals in multiple-source domains, a multiple-source transfer learning-based Takagi-Sugeno-Kang (TSK) fuzzy system (FS), called MST-TSK, is proposed, which combines multiple-source transfer learning and manifold regularization (MR) learning mechanisms together into the TSK-FS framework. Specifically, the advantages of MST-TSK include the following: (1) by evaluating the significance of each source domain (SD), a flexible domain entropy-weighting index is presented; (2) using the theory of sample transfer learning, a reweighting strategy is presented to weigh the prediction of unknown samples in the target domain (TD) and the output of the source prediction functions; (3) by taking into account the MR term, the manifold structure of the TD is effectively maintained in the proposed system; and (4) by inheriting the interpretability of TSK-FS, MST-TSK displays good interpretability in identifying EEG signals that are understandable by humans (domain experts). The effectiveness of the proposed FS is demonstrated in several EEG multiple-source transfer learning tasks.  © 2020 ACM.",EEG signals; Epileptic identification; fuzzy system; manifold regularization learning; multiple source transfer learning,Electroencephalography; Flexible displays; Fuzzy systems; Learning systems; Electroencephalogram signals; Entropy weighting; Learning mechanism; Manifold regularization; Manifold structures; Prediction function; Takagi-Sugeno-Kang fuzzy system; TSK fuzzy system; Transfer learning
DenseNet-201-Based Deep Neural Network with Composite Learning Factor and Precomputation for Multiple Sclerosis Classification,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088953408&doi=10.1145%2f3341095&partnerID=40&md5=748f9e7fd911dec61dd67afd1f2dec08,"(Aim) Multiple sclerosis is a neurological condition that may cause neurologic disability. Convolutional neural network can achieve good results, but tuning hyperparameters of CNN needs expert knowledge and are difficult and time-consuming. To identify multiple sclerosis more accurately, this article proposed a new transfer-learning-based approach. (Method) DenseNet-121, DenseNet-169, and DenseNet-201 neural networks were compared. In addition, we proposed the use of a composite learning factor (CLF) that assigns different learning factor to three types of layers: Early frozen layers, middle layers, and late replaced layers. How to allocate layers into those three layers remains a problem. Hence, four transfer learning settings (viz., Settings A, B, C, and D) were tested and compared. A precomputation method was utilized to reduce the storage burden and accelerate the program. (Results) We observed that DenseNet-201-D (the layers from CP to T3 are frozen, the layers of D4 are updated with learning factor of 1, and the final new layers of FCL are randomly initialized with learning factor of 10) can achieve the best performance. The sensitivity, specificity, and accuracy of DenseNet-201-D was 98.27± 0.58, 98.35± 0.69, and 98.31± 0.53, respectively. (Conclusion) Our method gives better performances than state-of-the-art approaches. Furthermore, this composite learning rate gives superior results to traditional simple learning factor (SLF) strategy.  © 2020 ACM.",composite learning factor; deep learning; deep neural network; DenseNet; Multiple sclerosis; precomputation; simple learning factor; transfer learning,Convolutional neural networks; Deep neural networks; Transfer learning; Expert knowledge; Hyperparameters; Learning factor; Learning rates; Learning-based approach; Multiple sclerosis; Pre-computation; State-of-the-art approach; Deep learning
Learning Visual Elements of Images for Discovery of Brand Posts,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086890610&doi=10.1145%2f3385413&partnerID=40&md5=719669d59b30602f5b1fce6b3e8cbab7,"Online Social Network Sites have become a primary platform for brands and organizations to engage their audience by sharing image and video posts on their timelines. Different from traditional advertising, these posts are not restricted to the products or logo but include visual elements that express more in general the values and attributes of the brand, called brand associations. Since marketers are increasingly spending time in discovering and re-posting user generated posts that reflect the brand attributes, there is an increasing demand for such discovery systems. The goal of these systems is to assist brand experts in filtering through online collections of new user media to discover actionable posts, which match the brand value and have the potential to engage the consumers. Driven by this real-life application, we define and formulate a new task of content discovery for brands and propose a framework that learns to rank posts for brands from their historical timeline. We design a Personalized Content Discovery (PCD) framework to address the three challenges of high inter-brand similarity, sparsity of brand - post interactions, and diversification of timeline. To learn fine-grained brand representation and to generate explanations for the ranking, we automatically learn visual elements of posts from the timeline of brands and from a set of brand attributes in the domain of marketing. To test our framework we use two large-scale Instagram datasets that contain a total of more than 1.5 million image and video posts from the historical timeline of hundreds of brands from multiple verticals such as food and fashion. Extensive experiments indicate that our model can effectively learn fine-grained brand representations and outperform the closest state-of-the-art solutions. © 2020 ACM.",computational marketing; Content discovery; image ranking,Commerce; Large dataset; Online systems; Social networking (online); Content discoveries; Discovery systems; On-line social networks; Online collection; Personalized content; Real-life applications; State of the art; Visual elements; Marketing
Human Activity Recognition from Multiple Sensors Data Using Multi-fusion Representations and CNNs,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086889073&doi=10.1145%2f3377882&partnerID=40&md5=812accb0b2ac85854758b9f379fd988d,"With the emerging interest in the ubiquitous sensing field, it has become possible to build assistive technologies for persons during their daily life activities to provide personalized feedback and services. For instance, it is possible to detect an individual's behavioral pattern (e.g., physical activity, location, and mood) by using sensors embedded in smart-watches and smartphones. The multi-sensor environments also come with some challenges, such as how to fuse and combine different sources of data. In this article, we explore several methods of fusion for multi-representations of data from sensors. Furthermore, multiple representations of sensor data were generated and then fused using data-level, feature-level, and decision-level fusions. The presented methods were evaluated using three publicly available human activity recognition (HAR) datasets. The presented approaches utilize Deep Convolutional Neural Networks (CNNs). A generic architecture for fusion of different sensors is proposed. The proposed method shows promising performance, with the best results reaching an overall accuracy of 98.4% for the Context-Awareness via Wrist-Worn Motion Sensors (HANDY) dataset and 98.7% for the Wireless Sensor Data Mining (WISDM version 1.1) dataset. Both results outperform previous approaches. © 2020 ACM.",activity recognition; CNN; Data fusion; deep learning; multimodal sensors,Convolutional neural networks; Deep neural networks; Motion sensors; Pattern recognition; Assistive technology; Daily life activities; Decision level fusion; Generic architecture; Human activity recognition; Multi-representations; Multiple representation; Personalized feedback; Data mining
EGroupNet: A Feature-enhanced Network for Age Estimation with Novel Age Group Schemes,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086905096&doi=10.1145%2f3379449&partnerID=40&md5=9c25846f942b058d10db5414790fdbdb,"Although age estimation is easily affected by smiling, race, gender, and other age-related attributes, most of the researchers did not pay attention to the correlations among these attributes. Moreover, many researchers perform age estimation from a wide range of age; however, conducting an age prediction over a narrow age range may achieve better results. This article proposes a hierarchic approach referred to as EGroupNet for age prediction. The method includes two main stages, i.e., feature enhancement via excavating the correlations among age-related attributes and age estimation based on different age group schemes. First, we apply the multi-task learning model to learn multiple face attributes simultaneously to obtain discriminative features of different attributes. Second, we project the outputs of fully connected layers of several subnetworks into a highly correlated matrix space via the correlation learning process. Third, we classify these enhanced features into narrow age groups using two Extreme Learning Machine models. Finally, we make predictions based on the results of the age groups mergence. We conduct a large number of experiments on MORPH-II, LAP-2016 dataset, and Adience benchmark. The mean absolute errors of the two different settings on MORPH-II are 2.48 and 2.13 years, respectively; the normal score (ϵ) on the LAP-2016 dataset is 0.3578; and the accuracy of age prediction on Adience benchmark is 0.6978. © 2020 ACM.",Age estimation; age groups; correlations; enhancement; multi-task learning,Forecasting; Large dataset; Age estimation; Age predictions; Discriminative features; Extreme learning machine; Feature enhancement; Highly-correlated; Learning process; Mean absolute error; Learning systems
CryptoLesion: A Privacy-preserving Model for Lesion Segmentation Using Whale Optimization over Cloud,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086894029&doi=10.1145%2f3380743&partnerID=40&md5=f3106d03b73d7728c4f7fe220aa0c040,"The low-cost, accessing flexibility, agility, and mobility of cloud infrastructures have attracted medical organizations to store their high-resolution data in encrypted form. Besides storage, these infrastructures provide various image processing services for plain (non-encrypted) images. Meanwhile, the privacy and security of uploaded data depend upon the reliability of the service provider(s). The enforcement of laws towards privacy policies in health-care organizations, for not disclosing their patient's sensitive and private medical information, restrict them to utilize these services. To address these privacy concerns for melanoma detection, we propose CryptoLesion, a privacy-preserving model for segmenting lesion region using whale optimization algorithm (WOA) over the cloud in the encrypted domain (ED). The user's image is encrypted using a permutation ordered binary number system and a random stumble matrix. The task of segmentation is accomplished by dividing an encrypted image into a pre-defined number of clusters whose optimal centroids are obtained by WOA in ED, followed by the assignment of each pixel of an encrypted image to the unique centroid. The qualitative and quantitative analysis of CryptoLesion is evaluated over publicly available datasets provided in The International Skin Imaging Collaboration Challenges in 2016, 2017, 2018, and PH2 dataset. The segmented results obtained by CryptoLesion are found to be comparable with the winners of respective challenges. CryptoLesion is proved to be secure from a probabilistic viewpoint and various cryptographic attacks. To the best of our knowledge, CryptoLesion is first moving towards the direction of lesion segmentation in ED. © 2020 ACM.",cloud computing; melanoma detection; permutation ordered binary number system; Privacy preserving; whale optimization,Cryptography; Digital storage; Numbering systems; Binary number systems; Cloud infrastructures; Healthcare organizations; High resolution data; Lesion segmentations; Optimization algorithms; Privacy and security; Qualitative and quantitative analysis; Image segmentation
SCeVE: A Component-based Framework to Author Mixed Reality Tours,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086894701&doi=10.1145%2f3377353&partnerID=40&md5=97c015bc1c57555876af4e3eb73da68c,"Authoring a collaborative, interactive Mixed Reality (MR) tour requires flexible design and development of various software modules for tasks such as managing geographically distributed participants, adaptable travel and virtual camera techniques, data logging for assessment of the incorporated techniques, as well as for evaluating the Quality of Experiences (QoE). In most cases, authors might have to develop all these software modules, instead of focusing only on the virtual environment design. In this article, we propose SCeVE, a component-based framework that supports flexible design and authoring of interactive MR tours by offering ease of access to four major design choices: (i) Synchronization, (ii) Collaborative exploration, (iii) Visualization, and (iv) Evaluation. Based on tour requirements, an author can access one or more components (or software libraries) of design choices via SCeVE's API (Application Programming Interface) services, as demonstrated by the two case studies on group travel in a plant walk MR tour. SCeVE framework is innovative in the sense that it facilitates group travel in virtual environments involving ""live"" models of participants from geographically distributed sites. SCeVE empowers authors to focus only on the design of the required virtual environments. They can quickly build a diverse set of collaborative MR tours by utilizing the flexibility of SCeVE in terms of the various available options for traveling, rendering on multiple devices, and virtual camera viewpoint computation strategies. By providing data logs of various components, SCeVE facilitates performance evaluation of the various strategies used as well as the user experience in collaborative MR tours. SCeVE is designed in an extensible manner, allowing authors to add devices and software services as additional components. © 2020 ACM.",Authoring; case study; components; framework; gesture-based; mixed reality; modeling; non-natural; services; trave; user experience; virtual reality,Application programming interfaces (API); Application programs; Cameras; Information retrieval; Petroleum reservoir evaluation; User experience; Component-based framework; Distributed sites; Flexible designs; Multiple devices; Quality of experience (QoE); Software libraries; Software modules; Software services; Mixed reality
Dual-path Convolutional Image-Text Embeddings with Instance Loss,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086077560&doi=10.1145%2f3383184&partnerID=40&md5=9f0e5517c7e268eab3172490072e6734,"Matching images and sentences demands a fine understanding of both modalities. In this article, we propose a new system to discriminatively embed the image and text to a shared visual-textual space. In this field, most existing works apply the ranking loss to pull the positive image/text pairs close and push the negative pairs apart from each other. However, directly deploying the ranking loss on heterogeneous features (i.e., text and image features) is less effective, because it is hard to find appropriate triplets at the beginning. So the naive way of using the ranking loss may compromise the network from learning inter-modal relationship. To address this problem, we propose the instance loss, which explicitly considers the intra-modal data distribution. It is based on an unsupervised assumption that each image/text group can be viewed as a class. So the network can learn the fine granularity from every image/text group. The experiment shows that the instance loss offers better weight initialization for the ranking loss, so that more discriminative embeddings can be learned. Besides, existing works usually apply the off-the-shelf features, i.e., word2vec and fixed visual feature. So in a minor contribution, this article constructs an end-to-end dual-path convolutional network to learn the image and text representations. End-to-end learning allows the system to directly learn from the data and fully utilize the supervision. On two generic retrieval datasets (Flickr30k and MSCOCO), experiments demonstrate that our method yields competitive accuracy compared to state-of-the-art methods. Moreover, in language-based person retrieval, we improve the state of the art by a large margin. The code has been made publicly available. © 2020 ACM.",convolutional neural networks; cross-modal retrieval; Image-sentence retrieval; language-based person search,Embeddings; Modal analysis; Convolutional networks; Fine granularity; Heterogeneous features; State of the art; State-of-the-art methods; Text representation; Visual feature; Weight initialization; Convolution
SAMAF: Sequence-to-sequence Autoencoder Model for Audio Fingerprinting,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086902149&doi=10.1145%2f3380828&partnerID=40&md5=a195a50103032e62bf35b65ab9b96289,"Audio fingerprinting techniques were developed to index and retrieve audio samples by comparing a content-based compact signature of the audio instead of the entire audio sample, thereby reducing memory and computational expense. Different techniques have been applied to create audio fingerprints; however, with the introduction of deep learning, new data-driven unsupervised approaches are available. This article presents Sequence-to-Sequence Autoencoder Model for Audio Fingerprinting (SAMAF), which improved hash generation through a novel loss function composed of terms: Mean Square Error, minimizing the reconstruction error; Hash Loss, minimizing the distance between similar hashes and encouraging clustering; and Bitwise Entropy Loss, minimizing the variation inside the clusters. The performance of the model was assessed with a subset of VoxCeleb1 dataset, a""speech in-the-wild"" dataset. Furthermore, the model was compared against three baselines: Dejavu, a Shazam-like algorithm; Robust Audio Fingerprinting System (RAFS), a Bit Error Rate (BER) methodology robust to time-frequency distortions and coding/decoding transformations; and Panako, a constellation-based algorithm adding time-frequency distortion resilience. Extensive empirical evidence showed that our approach outperformed all the baselines in the audio identification task and other classification tasks related to the attributes of the audio signal with an economical hash size of either 128 or 256 bits for one second of audio. © 2020 ACM.",audio fingerprinting; audio identification; Deep learning; sequence-to-sequence autoencoder,Audio systems; Deep learning; Errors; Learning systems; Mean square error; Audio fingerprint; Audio fingerprinting; Audio identification; Classification tasks; Computational expense; Loss functions; Reconstruction error; Unsupervised approaches; Bit error rate
Hyperspectral Reconstruction with Redundant Camera Spectral Sensitivity Functions,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086899845&doi=10.1145%2f3386313&partnerID=40&md5=4ae0585a9a6951b145ba61e2b424f930,"High-resolution hyperspectral (HS) reconstruction has recently achieved significantly progress, among which the method based on the fusion of the RGB and HS images of the same scene can greatly improve the reconstruction performance compared with those based on the individually spectral or spatial enhancement. It is well known that the HS image is obtained only via the costly hypersoectral sensor, whereas the RGB images can be provided by low-price RGB cameras and the spectral sensitivity (SS) functions of RGB cameras are usually different. Thus, this study proposes a HS reconstruction, which fuses merely two RGB images with redundant spectral responses. In this work, we design a new RGB camera via shifting the SS of an existed RGB camera, which can provide similar strength of spectral response with different spectral centers of SS, and fuse the new achieved color image with an existed RGB image by a deep ResNet. Experiments validate that fusion of two existed RGB images can provide impressive HS reconstruction performance and further improvement can be achieved by integrating the color image of the simulated SS with the RGB image. © 2020 ACM.",deep learning; Hyperspectral reconstruction; ResNet; spectral redundancy; spectral resolution enhancement; spectral sensitivity,Cameras; Image fusion; Image reconstruction; Spectroscopy; Color images; High resolution; HyperSpectral; Spatial enhancement; Spectral center; Spectral response; Spectral sensitivity; Spectral sensitivity functions; Image enhancement
Region-Level Visual Consistency Verification for Large-Scale Partial-Duplicate Image Search,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084417214&doi=10.1145%2f3383582&partnerID=40&md5=2c36432a5fe9d91e63a2ffe9602f2313,"Most recent large-scale image search approaches build on a bag-of-visual-words model, in which local features are quantized and then efficiently matched between images. However, the limited discriminability of local features and the BOW quantization errors cause a lot of mismatches between images, which limit search accuracy. To improve the accuracy, geometric verification is popularly adopted to identify geometrically consistent local matches for image search, but it is hard to directly use these matches to distinguish partial-duplicate images from non-partial-duplicate images. To address this issue, instead of simply identifying geometrically consistent matches, we propose a region-level visual consistency verification scheme to confirm whether there are visually consistent region (VCR) pairs between images for partial-duplicate search. Specifically, after the local feature matching, the potential VCRs are constructed via mapping the regions segmented from candidate images to a query image by utilizing the properties of the matched local features. Then, the compact gradient descriptor and convolutional neural network descriptor are extracted and matched between the potential VCRs to verify their visual consistency to determine whether they are VCRs. Moreover, two fast pruning algorithms are proposed to further improve efficiency. Extensive experiments demonstrate the proposed approach achieves higher accuracy than the state of the art and provide comparable efficiency for large-scale partial-duplicate search tasks. © 2020 ACM.",copy detection; digital forensics; image retrieval; near-duplicate image search; Partial-duplicate image search,Convolutional neural networks; Efficiency; Object recognition; Query processing; Bag-of-visual-words; Discriminability; Duplicate images; Geometric verifications; Pruning algorithms; Quantization errors; State of the art; Visual consistency; Image enhancement
Image to Modern Chinese Poetry Creation via a Constrained Topic-aware Model,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086896743&doi=10.1145%2f3381858&partnerID=40&md5=9634976670dc9aaa42a4a79410c4c418,"Artificial creativity has attracted increasing research attention in the field of multimedia and artificial intelligence. Despite the promising work on poetry/painting/music generation, creating modern Chinese poetry from images, which can significantly enrich the functionality of photo-sharing platforms, has rarely been explored. Moreover, existing generation models cannot tackle three challenges in this task: (1) Maintaining semantic consistency between images and poems; (2) preventing topic drift in the generation; (3) avoidance of certain words appearing frequently. These three points are even common challenges in other sequence generation tasks. In this article, we propose a Constrained Topic-aware Model (CTAM) to create modern Chinese poetries from images regarding the challenges above. Without image-poetry paired dataset, we construct a visual semantic vector to embed visual contents via image captions. For the topic-drift problem, we propose a topic-aware poetry generation model. Additionally, we design an Anti-frequency Decoding (AFD) scheme to constrain high-frequency characters in the generation. Experimental results show that our model achieves promising performance and is effective in poetry's readability and semantic consistency. © 2020 ACM.",Image captioning; poetry generation; semantic consistency,Multimedia systems; High frequency character; Image caption; Photo sharing; Semantic consistency; Sequence generation; Topic drift; Visual content; Visual semantics; Semantics
"Listen, Look, and Find the One: Robust Person Search with Multimodality Index",2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086890486&doi=10.1145%2f3380549&partnerID=40&md5=671e78911b63e359a3a3f2666db8041d,"Person search with one portrait, which attempts to search the targets in arbitrary scenes using one portrait image at a time, is an essential yet unexplored problem in the multimedia field. Existing approaches, which predominantly depend on the visual information of persons, cannot solve problems when there are variations in the person's appearance caused by complex environments and changes in pose, makeup, and clothing. In contrast to existing methods, in this article, we propose an associative multimodality index for person search with face, body, and voice information. In the offline stage, an associative network is proposed to learn the relationships among face, body, and voice information. It can adaptively estimate the weights of each embedding to construct an appropriate representation. The multimodality index can be built by using these representations, which exploit the face and voice as long-term keys and the body appearance as a short-term connection. In the online stage, through the multimodality association in the index, we can retrieve all targets depending only on the facial features of the query portrait. Furthermore, to evaluate our multimodality search framework and facilitate related research, we construct the Cast Search in Movies with Voice (CSM-V) dataset, a large-scale benchmark that contains 127K annotated voices corresponding to tracklets from 192 movies. According to extensive experiments on the CSM-V dataset, the proposed multimodality person search framework outperforms the state-of-the-art methods. © 2020 ACM.",associative network; multimodality index; Person search,Computer networks; Associative network; Complex environments; Long-term keys; Multi-modality; Portrait image; State-of-the-art methods; Visual information; Voice informations; Large dataset
A Benchmark Dataset and Comparison Study for Multi-modal Human Action Analytics,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086892025&doi=10.1145%2f3365212&partnerID=40&md5=c0fa925348ddaf9d699e3d2265589ce2,"Large-scale benchmarks provide a solid foundation for the development of action analytics. Most of the previous activity benchmarks focus on analyzing actions in RGB videos. There is a lack of large-scale and high-quality benchmarks for multi-modal action analytics. In this article, we introduce PKU Multi-Modal Dataset (PKU-MMD), a new large-scale benchmark for multi-modal human action analytics. It consists of about 28,000 action instances and 6.2 million frames in total and provides high-quality multi-modal data sources, including RGB, depth, infrared radiation (IR), and skeletons. To make PKU-MMD more practical, our dataset comprises two subsets under different settings for action understanding, namely Part I and Part II. Part I contains 1,076 untrimmed video sequences with 51 action classes performed by 66 subjects, while Part II contains 1,009 untrimmed video sequences with 41 action classes performed by 13 subjects. Compared to Part I, Part II is more challenging due to short action intervals, concurrent actions and heavy occlusion. PKU-MMD can be leveraged in two scenarios: action recognition with trimmed video clips and action detection with untrimmed video sequences. For each scenario, we provide benchmark performance on both subsets by conducting different methods with different modalities under two evaluation protocols, respectively. Experimental results show that PKU-MMD is a significant challenge to many state-of-the-art methods. We further illustrate that the features learned on PKU-MMD can be well transferred to other datasets. We believe this large-scale dataset will boost the research in the field of action analytics for the community. © 2020 ACM.",action detection; action recognition; Benchmark; multi-modal,Benchmarking; Infrared radiation; Large dataset; Modal analysis; Video recording; Action recognition; Benchmark datasets; Comparison study; Evaluation protocol; Large-scale dataset; Multi-modal data; Multi-modal dataset; State-of-the-art methods; Display devices
Do Users Behave Similarly in VR? Investigation of the User Influence on the System Design,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086894457&doi=10.1145%2f3381846&partnerID=40&md5=6315051df9eba648e87918dbb167be9d,"With the overarching goal of developing user-centric Virtual Reality (VR) systems, a new wave of studies focused on understanding how users interact in VR environments has recently emerged. Despite the intense efforts, however, current literature still does not provide the right framework to fully interpret and predict users' trajectories while navigating in VR scenes. This work advances the state-of-the-art on both the study of users' behaviour in VR and the user-centric system design. In more detail, we complement current datasets by presenting a publicly available dataset that provides navigation trajectories acquired for heterogeneous omnidirectional videos and different viewing platforms - namely, head-mounted display, tablet, and laptop. We then present an exhaustive analysis on the collected data to better understand navigation in VR across users, content, and, for the first time, across viewing platforms. The novelty lies in the user-affinity metric, proposed in this work to investigate users' similarities when navigating within the content. The analysis reveals useful insights on the effect of device and content on the navigation, which could be precious considerations from the system design perspective. As a case study of the importance of studying users' behaviour when designing VR systems, we finally propose a user-centric server optimisation. We formulate an integer linear program that seeks the best stored set of omnidirectional content that minimises encoding and storage cost while maximising the user's experience. This is posed while taking into account network dynamics, type of video content, and also user population interactivity. Experimental results prove that our solution outperforms common company recommendations in terms of experienced quality but also in terms of encoding and storage, achieving a savings up to 70%. More importantly, we highlight a strong correlation between the storage cost and the user-affinity metric, showing the impact of the latter in the system architecture design. © 2020 ACM.",integer linear program; Omnidirectional video dataset; user behaviour analysis; viewport-based adaptive streaming,Digital storage; Encoding (symbols); Helmet mounted displays; Integer programming; Navigation; Signal encoding; Systems analysis; User centered design; Head mounted displays; Integer linear programs; Network dynamics; State of the art; Strong correlation; System architecture design; User influences; User-centric systems; User experience
Shuffled ImageNet Banks for Video Event Detection and Search,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086898570&doi=10.1145%2f3377875&partnerID=40&md5=08fe128a34f4f03db98f4747f644a121,"This article aims for the detection and search of events in videos, where video examples are either scarce or even absent during training. To enable such event detection and search, ImageNet concept banks have shown to be effective. Rather than employing the standard concept bank of 1,000 ImageNet classes, we leverage the full 21,841-class dataset. We identify two problems with using the full dataset: (i) there is an imbalance between the number of examples per concept, and (ii) not all concepts are equally relevant for events. In this article, we propose to balance large-scale image hierarchies for pre-training. We shuffle concepts based on bottom-up and top-down operations to overcome the problems of example imbalance and concept relevance. Using this strategy, we arrive at the shuffled ImageNet bank, a concept bank with an order of magnitude more concepts compared to standard ImageNet banks. Compared to standard ImageNet pre-training, our shuffles result in more discriminative representations to train event models from the limited video event examples. For event search, the broad range of concepts enable a closer match between textual queries of events and concept detections in videos. Experimentally, we show the benefit of the proposed bank for event detection and event search, with state-of-the-art performance for both tasks on the challenging TRECVID Multimedia Event Detection and Ad-Hoc Video Search benchmarks. © 2020 ACM.",concepts; Event detection; event search; shuffle,Computer networks; Bottom-up and top-down; Concept detection; Event detection; Multimedia event detections; Pre-training; State-of-the-art performance; Textual query; Video event detections; Benchmarking
Meta-path Augmented Sequential Recommendation with Contextual Co-attention Network,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086890443&doi=10.1145%2f3382180&partnerID=40&md5=573b6f912ef08eff8b88c356bdeb9a92,"It is critical to comprehensively and efficiently learn user preferences for an effective sequential recommender system. Existing sequential recommendation methods mainly focus on modeling local preference from users' historical behaviors, which largely ignore the global context information from the heterogeneous information network. This prevents a comprehensive user preference representation. To address these issues, we propose a joint learning approach to incorporate global context with local preferences efficiently. The proposed approach introduces meta-paths from a heterogeneous information network to capture the global context information, and the position-based self-attention mechanism is adopted to model the local preference representation efficiently. Compared with the methods that only consider the local preference, our proposed method takes the advantages of incorporating global context information, which extracts structural features that captures relevant semantics to construct users' global preference representation for the sequential recommendation. We further adopt a co-attention mechanism to model complex interactions between global context and users' historical behaviors for better user representations. Quantitative and qualitative experimental evaluations are conducted on nine large-scale Amazon datasets and a multi-modal Zhihu dataset. The promising results demonstrate the effectiveness of the proposed model. © 2020 ACM.",co-attention; heterogeneous information network; meta-path; self-attention; sequential recommendation; User modeling,Information services; Large dataset; Semantics; Attention mechanisms; Experimental evaluation; Heterogeneous information; Joint learning; Model complexes; Preference representation; Recommendation methods; Structural feature; Behavioral research
PPNet: Privacy Protected CDN-ISP Collaboration for QoS-aware Multi-CDN Adaptive Video Streaming,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086892585&doi=10.1145%2f3379983&partnerID=40&md5=29ff1471fae783147eab743e68f6f7fd,"Software-defined networking introduces opportunities to optimize the Internet Service Provider's network and to improve client experience for the Video-on-Demand applications. Recent studies on SDN frameworks show that traffic engineering methods allow a fair share of bandwidth between adaptive video streaming clients. Additionally, ISPs can make better estimations of bandwidth and contribute to the bitrate selection for the clients. This study focuses on another aspect of network assistance in video delivery: CDN server selection. In a typical framework where the ISP contributes to the CDN selection, the video provider and the network provider interfaces are merged together. Clients connect to the ISP to get the best CDN server candidate for a given video. This exposes client requests to the ISP. However, video providers have been investing large resources for encrypted video provisioning to preserve their client's information from third parties, especially network providers. The typical approach is not practical due to privacy concerns. In this study, we present a framework called PPNet to allow CDN-ISP collaboration while preventing the ISP's access to the video request and availability information. Our framework introduces an isolation between the video provider's and the ISP's web interfaces. Clients connect to both of the interfaces and deliver information on a need-to-know basis. As a second contribution, PPNet introduces a practical optimization method for CDN selection. Real-time data collection capabilities of a typical OpenFlow network is used as the input for optimization. Congestion-awareness has been the priority. To adapt for changing network conditions, capability of utilizing multiple servers simultaneously for a single video is introduced. Instead of directing each video client into a CDN node, the proposed system performs request routing per video segment. Finally, we present a system prototype of PPNet and show that our multiple-host adaptive streaming method introduces a significant improvement in quality of experience when compared to the state of the art. © 2020 ACM.",content delivery networks; Internet; privacy; request routing; software-defined networks; Video-on-demand,Application programs; Bandwidth; Quality of service; Video on demand; Video streaming; Adaptive streaming; Adaptive video streaming; Cdn-isp collaborations; Optimization method; Quality of experience (QoE); Real time data collections; Traffic Engineering; Video-on-demand applications; Internet service providers
FIN: Feature Integrated Network for Object Detection,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086893159&doi=10.1145%2f3381086&partnerID=40&md5=3366c070845b5533102da46ce3a57420,"Multi-layer detection is a widely used method in the field of object detection. It extracts multiple feature maps with different resolutions from the backbone network to detect objects of different scales, which can effectively cope with the problem of object scale change in object detection. Although the multi-layer detection utilizes multiple detection layers to alleviate the burden of one single detection layer and can improve the detection accuracy to some extent, this method has two limitations. First, manually assigning anchor boxes of different sizes to different feature maps is too dependent on the human experience. Second, there is a semantic gap between each detection layer in multi-layer detection. The same detector needs to simultaneously process the detection layers with inconsistent semantic strength, which increases the optimization difficulty of the detector. In this article, we propose a feature integrated network (FIN) based on single layer detection to deal with the problems mentioned above. Different from the existing methods, we design a series of verification experiments based on the multi-layer detection model, which shows that the shallow high-resolution feature map has the potential to simultaneously and effectively detect objects of various scales. Considering that the semantic information of the shallow feature map is weak, we propose two modules to enhance the representation ability of the single detection layer. First, we propose a detection adaptation network (DANet) to extract powerful feature maps that are useful for object detection tasks. Second, we combine global context information and local detail information with a verified hourglass module (VHM) to generate a single feature map with high resolution and rich semantic information so that we can assign all anchor boxes to this detection layer. In our model, all the detection operations are concentrated on a high-resolution feature map whose semantic information and detailed information are enhanced as much as possible. Therefore, the proposed model can solve the problem of anchor assignment and inconsistent semantic strength between multiple detection layers mentioned above. A large number of experiments on the Pattern Analysis, Statistical Modelling and Computational Learning Visual Object Classes (PASCAL VOC) and Microsoft Common Objects in Context (MS COCO) datasets show that our model has good detection performance for objects of various sizes. The proposed model can achieve<?brk?> 81.9 mAP when the size of the input image is 300 × 300. © 2020 ACM.",deep learning; feature integration; multi-layer detection; Object detection,Feature extraction; Fins (heat exchange); Large dataset; Object recognition; Semantics; Computational learning; Detection accuracy; Detection performance; Different resolutions; Integrated networks; Multiple detection; Semantic information; Statistical modelling; Object detection
Spatiooral Saliency-based Motion Vector Refinement for Frame Rate Up-conversion,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086886992&doi=10.1145%2f3382506&partnerID=40&md5=43c24ae6e7668e071e77d4218f19bde0,"A spatiooral saliency-based frame rate up-conversion (FRUC) approach is proposed, which achieves better quality of interpolated frames and invalidates existing texture variation-based FRUC detectors. A spatiooral saliency model is designed to select salient frames. After obtaining initial motion vector field by texture- and color-based bilateral motion estimation, two motion vector refining (MVR) schemes are adopted for high and low saliency frames to hierarchically refine the motion vectors, respectively. To produce high-quality interpolated frames, image enhancement are performed for salient frames after frame interpolation. Due to distinct MVR schemes, there are different degrees of texture information in interpolated frames. Some edge and texture information is supplemented into salient frames as post-processing, which can invalidate existing texture variation-based FRUC detectors. Experimental results show that the proposed approach outperforms state-of-the-art works in both objective and subjective qualities of interpolated frames, and achieves the purpose of FRUC anti-forensics. © 2020 ACM.",anti-forensics; forensics; Frame rate up-conversion; interpolation post-processing; video-saliency-based hierarchical motion vector refinement,Image coding; Image enhancement; Motion compensation; Textures; Vectors; Bilateral motion estimations; Frame interpolation; Frame rate up conversion; Motion vector field; Motion vector refinement; Saliency modeling; Subjective quality; Texture information; Motion estimation
CovLets: A second-order descriptor for modeling multiple features,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084765291&doi=10.1145%2f3357525&partnerID=40&md5=f28e1e02082e76bd1f80f841c979447b,"State-of-the-art techniques for image and video classification take a bottom-up approach where local features are aggregated into a global final representation. Existing frameworks (i.e., bag of words or Fisher vectors) are specifically designed to aggregate vector-valued features such as SIFT descriptors. In this article, we propose a technique to aggregate local descriptors in the form of covariance descriptors (CovDs) into a rich descriptor, which in essence benefit from the second-order statistics along the coding pipeline. The difficulty in aggregating CovDs arises from the fact that CovDs lie on the Riemannian manifold of symmetric positive definite (SPD) matrices. Therefore, the aggregating scheme must take advantage of metrics and the geometry of the SPD manifolds. In our proposal, we make use of the Stein divergence and Nyström method to embed the SPD manifold into a Hilbert space. We compare our proposal, dubbed CovLets, against state-of-the-art methods on several image and video classification problems including facial expression recognition and action recognition. © 2020 ACM.",Action recognition; covariance descriptor; reproducing kernel Hilbert space; Riemannian manifold,Aggregates; Classification (of information); Geometry; Action recognition; Facial expression recognition; Riemannian manifold; Second order statistics; State-of-the-art methods; State-of-the-art techniques; Symmetric positive definite; Video classification; Image classification
Cell nuclei classification in histopathological images using hybrid OLConvNet,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084760911&doi=10.1145%2f3345318&partnerID=40&md5=e9a1dc7d0e71683e399b2a428653e95b,"Computer-aided histopathological image analysis for cancer detection is a major research challenge in the medical domain. Automatic detection and classification of nuclei for cancer diagnosis impose a lot of challenges in developing state-of-the-art algorithms due to the heterogeneity of cell nuclei and dataset variability. Recently, a multitude of classification algorithms have used complex deep learning models for their dataset. However, most of these methods are rigid, and their architectural arrangement suffers from inflexibility and non-interpretability. In this research article, we have proposed a hybrid and flexible deep learning architecture OLConvNet that integrates the interpretability of traditional object-level features and generalization of deep learning features by using a shallower Convolutional Neural Network (CNN) named as CNN3L. CNN3L reduces the training time by training fewer parameters and hence eliminating space constraints imposed by deeper algorithms. We used F1-score and multiclass Area Under the Curve (AUC) performance parameters to compare the results. To further strengthen the viability of our architectural approach, we tested our proposed methodology with state-of-the-art deep learning architectures AlexNet, VGG16, VGG19, ResNet50, InceptionV3, and DenseNet121 as backbone networks. After a comprehensive analysis of classification results from all four architectures, we observed that our proposed model works well and performs better than contemporary complex algorithms. © 2020 ACM.",cell nuclei classification; class balancing; convolutional neural networks; Deep learning; histopathological images; hybrid networks; multi layer perceptron; object-level features; transfer learning,Classification (of information); Complex networks; Computer aided analysis; Computer aided diagnosis; Convolutional neural networks; Diseases; Image classification; Learning systems; Medical imaging; Network architecture; Architectural approach; Cell nuclei classifications; Classification algorithm; Classification results; Histopathological image analysis; Histopathological images; Performance parameters; State-of-the-art algorithms; Deep learning
Robust visual tracking using kernel sparse coding on multiple covariance descriptors,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084765097&doi=10.1145%2f3360308&partnerID=40&md5=4548109078f14256131fe0429c5e0a48,"In this article, we aim to improve the performance of visual tracking by combing different features of multiple modalities. The core idea is to use covariance matrices as feature descriptors and then use sparse coding to encode different features. The notion of sparsity has been successfully used in visual tracking. In this context, sparsity is used along appearance models often obtained from intensity/color information. In this work, we step outside this trend and propose to model the target appearance by local covariance descriptors (CovDs) in a pyramid structure. The proposed pyramid structure not only enables us to encode local and spatial information of the target appearance but also inherits useful properties of CovDs such as invariance to affine transforms. Since CovDs lie on a Riemannian manifold, we further propose to perform tracking through sparse coding by embedding the Riemannian manifold into an infinite-dimensional Hilbert space. Embedding the manifold into a Hilbert space allows us to perform sparse coding efficiently using the kernel trick. Our empirical study shows that the proposed tracking framework outperforms the existing state-of-the-art methods in challenging scenarios. © 2020 ACM.",covariance descriptor; kernel sparse coding; reproducing kernel Hilbert space; Riemannian manifold; Visual tracking,Affine transforms; Covariance matrix; Embeddings; Encoding (symbols); Vector spaces; Appearance models; Covariance matrices; Feature descriptors; Infinite dimensional; Multiple modalities; Riemannian manifold; Spatial informations; State-of-the-art methods; Hilbert spaces
A decision support system with intelligent recommendation for multi-disciplinary medical treatment,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084763476&doi=10.1145%2f3352573&partnerID=40&md5=a981fcfa71df00b3b825150551fcf621,"Recent years have witnessed an emerging trend for improving disease treatment by forming multi-disciplinary medical teams. The collaboration among specialists from multiple medical domains has been shown to be significantly helpful for designing comprehensive and reliable regimens, especially for incurable diseases. Although this kind of multi-disciplinary treatment has been increasingly adopted by healthcare providers, a new challenge has been introduced to the decision-making process - how to efficiently and effectively develop final regimens by searching for candidate treatments and considering inputs from every expert. In this article, we present a sophisticated decision support system called MdtDSS (a decision support system (DSS) for multi-disciplinary treatment (Mdt)), which is particularly developed to guide the collaborative decision-making in multi-disciplinary treatment scenarios. The system integrates a recommender system that aims to search for personalized candidates from a large-scale high-quality regimen pool and a voting system that helps collect feedback from multiple specialists without potential bias. Our decision support system optimally combines machine intelligence and human experience and helps medical practitioners make informed and accountable regimen decisions. We deployed the proposed system in a large hospital in Shanghai, China, and collected real-world data on large-scale patient cases. The evaluation shows that the proposed system achieves outstanding results in terms of high-quality multi-disciplinary treatment. © 2020 ACM.",breast cancer; Decision support system; DSS; MDT; medical guidelines; medical oncology; recommender system; representation learning; workflow engine,Artificial intelligence; Decision making; Diseases; Recommender systems; Collaborative decision making; Decision making process; Decision support system (dss); Health care providers; Incurable disease; Machine intelligence; Medical practitioner; Medical treatment; Decision support systems
Action recognition using form and motion modalities,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084760903&doi=10.1145%2f3350840&partnerID=40&md5=f742119ca5695bb92e8ac962c3e14e7d,"Action recognition has attracted increasing interest in computer vision due to its potential applications in many vision systems. One of the main challenges in action recognition is to extract powerful features from videos. Most existing approaches exploit either hand-crafted techniques or learning-based methods to extract features from videos. However, these methods mainly focus on extracting the dynamic motion features, which ignore the static form features. Therefore, these methods cannot fully capture the underlying information in videos accurately. In this article, we propose a novel feature representation method for action recognition, which exploits hierarchical sparse coding to learn the underlying features from videos. The learned features characterize the form and motion simultaneously and therefore provide more accurate and complete feature representation. The learned form and motion features are considered as two modalities, which are used to represent both the static and motion features. These modalities are further encoded into a global representation via a pairwise dictionary learning and then fed to an SVM classifier for action classification. Experimental results on several challenging datasets validate that the proposed method is superior to several state-of-the-art methods. © 2020 ACM.",Action recognition; Fisher vector; form and motion; hierarchical sparse coding,Computer networks; Action classifications; Action recognition; Dictionary learning; Dynamic motions; Feature representation; Global representation; Learning-based methods; State-of-the-art methods; Support vector machines
Hybrid wolf-bat algorithm for optimization of connection weights in multi-layer perceptron,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084766973&doi=10.1145%2f3350532&partnerID=40&md5=67f6cbe3ac129fe9dd62779fb0e7c203,"In a neural network, the weights act as parameters to determine the output(s) from a set of inputs. The weights are used to find the activation values of nodes of a layer from the values of the previous layer. Finding the ideal set of these weights for training a Multi-layer Perceptron neural network such that it minimizes the classification error is a widely known optimization problem. The presented article proposes a Hybrid Wolf-Bat algorithm, a novel optimization algorithm, as a solution to solve the discussed problem. The proposed algorithm is a hybrid of two already existing nature-inspired algorithms, Grey Wolf Optimization algorithm and Bat algorithm. The novel introduced approach is tested on ten different datasets of the medical field, obtained from the UCI machine learning repository. The performance of the proposed algorithm is compared with the recently developed nature-inspired algorithms: Grey Wolf Optimization algorithm, Cuckoo Search, Bat Algorithm, and Whale Optimization Algorithm, along with the standard Back-propagation training method available in the literature. The obtained results demonstrate that the proposed method outperforms other bio-inspired algorithms in terms of both speed of convergence and accuracy. © 2020 ACM.",bat algorithm; connection weights; grey wolf optimization; hybrid; MLP; nature inspired; Neural network; optimization,Backpropagation; Biomimetics; Network layers; Optimization; Bio-inspired algorithms; Classification errors; Multi layer perceptron; Multi-layer perceptron neural networks; Nature inspired algorithms; Optimization algorithms; Optimization problems; UCI machine learning repository; Multilayer neural networks
Delving deeper in drone-based person re-id by employing deep decision forest and attributes fusion,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084763984&doi=10.1145%2f3360050&partnerID=40&md5=75e0abe4045bd45a6c80cb0b8523de4d,"Deep learning has revolutionized the field of computer vision and image processing. Its ability to extract the compact image representation has taken the person re-identification (re-id) problem to a new level. However, in most cases, researchers are focused on developing new approaches to extract more fruitful image representation and use it in the re-id task. The extra information about images is rarely taken into account because the traditional person re-id datasets usually do not have it. Nevertheless, the research in multimodal machine learning has demonstrated that the utilization of the information from different sources leads to better performance. In this work, we demonstrate how a person re-id problem can benefit from the utilization of multimodal data. We have used the UAV drone to collect and label the new person re-id dataset, which is composed of pedestrian images and its attributes. We have manually annotated this dataset with attributes, and in contrast to the recent research, we do not use the deep network to classify them. Instead, we employ the continuous bag-of-words model to extract the word embeddings from text descriptions and fuse it with features extracted from images. Then the deep neural decision forest is used for pedestrians classification. The extensive experiments on the collected dataset demonstrate the effectiveness of the proposed model. © 2020 ACM.",attributes; CBOW; Datasets; drones; neural decision forest; neural networks; re-id; word2vec,Classification (of information); Deep learning; Drones; Forestry; Information retrieval; Learning algorithms; Bag-of-words models; Decision forest; Image representations; Multi-modal; Multi-modal data; New approaches; Person re identifications; Recent researches; Image processing
Exploring disorder-aware attention for clinical event extraction,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084752675&doi=10.1145%2f3372328&partnerID=40&md5=f37cbacf1598a98aef0d7a5c64173390,"Event extraction is one of the crucial tasks in biomedical text mining that aims to extract specific information concerning incidents embedded in the texts. In this article, we propose a deep learning framework that aims to identify the attributes (severity, course, temporal expression, and document creation time) associated with the medical concepts extracted from electronic medical records. The bi-directional long short-term memory network assisted by the attention mechanism is utilized to uncover the important aspects of the patient's medical conditions. The attention mechanism specific to the medical disorder mention can focus on various parts of the sentence when different disorders are considered as input. The proposed methodology is evaluated on benchmark ShARe/CLEF eHealth Evaluation Lab 2014 shared task 2 datasets. In addition to the CLEF dataset, we also used the social media text, especially the medical blog posts. Experimental results of the proposed approach illustrate that our proposed approach achieves significant performance improvements over the state-of-the-art techniques and the highly competitive deep learning - based baseline methods. © 2020 ACM.",attention; clinical event extraction; event extraction; Neural networks; social media; temporal event extraction,Deep learning; Extraction; Medical computing; Natural language processing systems; Attention mechanisms; Biomedical text minings; Electronic medical record; Learning frameworks; Medical conditions; Specific information; State-of-the-art techniques; Temporal expressions; Text mining
Textual entailment - Based figure summarization for biomedical articles,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084742470&doi=10.1145%2f3357334&partnerID=40&md5=14eee6a4060651d469536cfff79c7965,"This article proposes a novel unsupervised approach (FigSum++) for automatic figure summarization in biomedical scientific articles using a multi-objective evolutionary algorithm. The problem is treated as an optimization problem where relevant sentences in the summary for a given figure are selected based on various sentence scoring features (or objective functions), such as the textual entailment score between sentences in the summary and a figure's caption, the number of sentences referring to that figure, semantic similarity between sentences and a figure's caption, and the number of overlapping words between sentences and a figure's caption. These objective functions are optimized simultaneously using multi-objective binary differential evolution (MBDE). MBDE consists of a set of solutions, and each solution represents a subset of sentences to be selected in the summary. MBDE generally uses a single differential evolution variant, but in the current study, an ensemble of two different differential evolution variants measuring diversity among solutions and convergence toward global optimal solution, respectively, is employed for efficient search. Usually, in any summarization system, diversity among sentences (called anti-redundancy) in the summary is a very critical feature, and it is calculated in terms of similarity (like cosine similarity) among sentences. In this article, a new way of measuring diversity in terms of textual entailment is proposed. To represent the sentences of the article in the form of numeric vectors, the recently proposed BioBERT pre-trained language model in biomedical text mining is utilized. An ablation study has also been presented to determine the importance of different objective functions. For evaluation of the proposed technique, two benchmark biomedical datasets containing 91 and 84 figures are considered. Our proposed system obtains 5% and 11% improvements in terms of the F-measure metric over two datasets, compared to the state-of-the-art unsupervised methods. © 2020 ACM.",evolutionary computing; Figure-assisted text summarization; multi-objective optimization (MOO); textual entailment,Bioinformatics; Evolutionary algorithms; Optimization; Semantics; Text mining; Biomedical text minings; Differential Evolution; Global optimal solutions; Multi objective evolutionary algorithms; Optimization problems; Summarization systems; Unsupervised approaches; Unsupervised method; Natural language processing systems
Machine learning techniques for the diagnosis of alzheimer's disease: A review,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079890725&doi=10.1145%2f3344998&partnerID=40&md5=75abadadec045f7e076ae9eb43433a64,"Alzheimer's disease is an incurable neurodegenerative disease primarily affecting the elderly population. Efficient automated techniques are needed for early diagnosis of Alzheimer's. Many novel approaches are proposed by researchers for classification of Alzheimer's disease. However, to develop more efficient learning techniques, better understanding of the work done on Alzheimer's is needed. Here, we provide a review on 165 papers from 2005 to 2019, using various feature extraction and machine learning techniques. The machine learning techniques are surveyed under three main categories: support vector machine (SVM), artificial neural network (ANN), and deep learning (DL) and ensemble methods. We present a detailed review on these three approaches for Alzheimer's with possible future directions. © 2020 ACM.",diffusion tensor imaging (DTI); Magnetic resonance imaging (MRI); mild cognitive impairment (MCI); positron emission tomography (PET),Deep learning; Diagnosis; Learning algorithms; Learning systems; Neural networks; Support vector machines; Alzheimer's disease; Automated techniques; Early diagnosis; Efficient learning; Elderly populations; Ensemble methods; Machine learning techniques; Possible futures; Neurodegenerative diseases
ACMNet: Adaptive confidence matching network for human behavior analysis via cross-modal retrieval,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084742120&doi=10.1145%2f3362065&partnerID=40&md5=97a36f02c7e3adac832b300cde8f0cc9,"Cross-modality human behavior analysis has attracted much attention from both academia and industry. In this article, we focus on the cross-modality image-text retrieval problem for human behavior analysis, which can learn a common latent space for cross-modality data and thus benefit the understanding of human behavior with data from different modalities. Existing state-of-the-art cross-modality image-text retrieval models tend to be fine-grained region-word matching approaches, where they begin with measuring similarities for each image region or text word followed by aggregating them to estimate the global image-text similarity. However, it is observed that such fine-grained approaches often encounter the similarity bias problem, because they only consider matched text words for an image region or matched image regions for a text word for similarity calculation, but they totally ignore unmatched words/regions, which might still be salient enough to affect the global image-text similarity. In this article, we propose an Adaptive Confidence Matching Network (ACMNet), which is also a fine-grained matching approach, to effectively deal with such a similarity bias. Apart from calculating the local similarity for each region(/word) with its matched words(/regions), ACMNet also introduces a confidence score for the local similarity by leveraging the global text(/image) information, which is expected to help measure the semantic relatedness of the region(/word) to the whole text(/image). Moreover, ACMNet also incorporates the confidence scores together with the local similarities in estimating the global image-text similarity. To verify the effectiveness of ACMNet, we conduct extensive experiments and make comparisons with state-of-the-art methods on two benchmark datasets, i.e., Flickr30k and MS COCO. Experimental results show that the proposed ACMNet can outperform the state-of-the-art methods by a clear margin, which well demonstrates the effectiveness of the proposed ACMNet in human behavior analysis and the reasonableness of tackling the mentioned similarity bias issue. © 2020 ACM.",adaptive confidence matching network; Cross-modality retrieval; human behavior analysis; image-text retrieval,Behavioral research; Semantics; Benchmark datasets; Fine-grained matching; Human behavior analysis; Matching networks; Measuring similarities; Semantic relatedness; Similarity calculation; State-of-the-art methods; Information retrieval
Active balancing mechanism for imbalanced medical data in deep learning-based classification models,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084731753&doi=10.1145%2f3357253&partnerID=40&md5=9fa1c4ae57562c1fac27235ea1435c27,"Imbalanced data always has a serious impact on a predictive model, and most under-sampling techniques consume more time and suffer from loss of samples containing critical information during imbalanced data processing, especially in the biomedical field. To solve these problems, we developed an active balancing mechanism (ABM) based on valuable information contained in the biomedical data. ABM adopts the Gaussian naïve Bayes method to estimate the object samples and entropy as a query function to evaluate sample information and only retains valuable samples of the majority class to achieve under-sampling. The Physikalisch Technische Bundesanstalt diagnostic electrocardiogram (ECG) database, including 5,173 normal ECG samples and 26,654 myocardial infarction ECG samples, is applied to verify the validity of ABM. At imbalance rates of 13 and 5, experimental results reveal that ABM takes 7.7 seconds and 13.2 seconds, respectively. Both results are significantly faster than five conventional under-sampling methods. In addition, at the imbalance rate of 13, ABM-based data obtained the highest accuracy of 92.23% and 97.52% using support vector machines and modified convolutional neural networks (MCNNs) with eight layers, respectively. At the imbalance rate of 5, the processed data by ABM also achieved the best accuracy of 92.31% and 98.46% based on support vector machines and MCNNs, respectively. Furthermore, ABM has better performance than two compared methods in F1-measure, G-means, and area under the curve. Consequently, ABM could be a useful and effective approach to deal with imbalanced data in general, particularly biomedical myocardial infarction ECG datasets, and the MCNN can also achieve higher performance compared to the state of the art. © 2020 ACM.",Biomedical; entropy; Gaussian naïve Bayes; imbalanced data; myocardial infarction,Balancing; Cardiology; Convolutional neural networks; Data handling; Electrocardiography; Multilayer neural networks; Query processing; Support vector machines; Area under the curves; Biomedical fields; Classification models; Effective approaches; Myocardial Infarction; Physikalisch-technische bundesanstalt; Predictive modeling; Sample information; Deep learning
Random forest with self-paced bootstrap learning in lung cancer prognosis,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082136026&doi=10.1145%2f3345314&partnerID=40&md5=9b2bacf4e722eda5ae9002f42e14bba8,"Training gene expression data with supervised learning approaches can provide an alarm sign for early treatment of lung cancer to decrease death rates. However, the samples of gene features involve lots of noises in a realistic environment. In this study, we present a random forest with self-paced learning bootstrap for improvement of lung cancer classification and prognosis based on gene expression data. To be specific, we propose an ensemble learning with random forest approach to improving the model classification performance by selecting multi-classifiers. Then, we investigate the sampling strategy by gradually embedding from high- to low-quality samples by self-paced learning. The experimental results based on five public lung cancer datasets show that our proposed method could select significant genes exactly, which improves classification performance compared to that of existing approaches. We believe that our proposed method has the potential to assist doctors in gene selections and lung cancer prognosis. © 2020 ACM.",bootstrap; classification; Lung cancer; random forest; self-paced learning,Biological organs; Classification (of information); Decision trees; Gene expression; Random forests; Classification performance; Ensemble learning; Gene Expression Data; Model classification; Realistic environments; Sampling strategies; Self-paced learning; Supervised learning approaches; Diseases
Introduction to the special issue on multimodal machine learning for human behavior analysis,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084728107&doi=10.1145%2f3381917&partnerID=40&md5=65454d9a866d12c096a95003779a8e08,[No abstract available],,
Multi-scale supervised attentive encoder-decoder network for crowd counting,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084757935&doi=10.1145%2f3356019&partnerID=40&md5=10aa5d9e5b198751524d88294b87034b,"Crowd counting is a popular topic with widespread applications. Currently, the biggest challenge to crowd counting is large-scale variation in objects. In this article, we focus on overcoming this challenge by proposing a novel Attentive Encoder-Decoder Network (AEDN), which is supervised on multiple feature scales to conduct crowd counting via density estimation. This work has three main contributions. First, we augment the traditional encoder-decoder architecture with our proposed residual attention blocks, which, beyond skip-connected encoded features, further extend the decoded features with attentive features. AEDN is better at establishing long-range dependencies between the encoder and decoder, therefore promoting more effective fusion of multi-scale features for handling scale-variations. Second, we design a new KL-divergence-based distribution loss to supervise the scale-aware structural differences between two density maps, which complements the pixel-isolated MSE loss and better optimizes AEDN to generate high-quality density maps. Third, we adopt a multi-scale supervision scheme, such that multiple KL divergences and MSE losses are deployed at all decoding stages, providing more thorough supervisions for different feature scales. Extensive experimental results on four public datasets, including ShanghaiTech Part A, ShanghaiTech Part B, UCF-CC-50, and UCF-QNRF, reveal the superiority and efficacy of the proposed method, which outperforms most state-of-the-art competitors. © 2020 ACM.",representation fusion; Self-attention; supervised method,Decoding; Distribution loss; Encoder-decoder; Encoder-decoder architecture; Long-range dependencies; Multi-scale features; Multiple features; State of the art; Structural differences; Network coding
Multichannel attention refinement for video question answering,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084740770&doi=10.1145%2f3366710&partnerID=40&md5=997ff17288122898e73e26a724528116,"Video Question Answering (VideoQA) is the extension of image question answering (ImageQA) in the video domain. Methods are required to give the correct answer after analyzing the provided video and question in this task. Comparing to ImageQA, the most distinctive part is the media type. Both tasks require the understanding of visual media, but VideoQA is much more challenging, mainly because of the complexity and diversity of videos. Particularly, working with the video needs to model its inherent temporal structure and analyze the diverse information it contains. In this article, we propose to tackle the task from a multichannel perspective. Appearance, motion, and audio features are extracted from the video, and question-guided attentions are refined to generate the expressive clues that support the correct answer. We also incorporate the relevant text information acquired from Wikipedia as an attempt to extend the capability of the method. Experiments on TGIF-QA and ActivityNet-QA datasets show the advantages of our method compared to existing methods. We also demonstrate the effectiveness and interpretability of our method by analyzing the refined attention weights during the question-answering procedure. © 2020 ACM.",attention mechanism; multichannel; Video question answering,Computer networks; Audio features; Interpretability; Media types; Multichannel; Question Answering; Temporal structures; Text information; Visual media
Introduction to the special issue on computational intelligence for biomedical data and imaging,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084753209&doi=10.1145%2f3381919&partnerID=40&md5=22419186d99361e4016cfeec639bffe6,[No abstract available],,
Pulmonary nodule detection based on ISODATA-Improved faster RCNN and 3D-CNN with focal loss,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084739393&doi=10.1145%2f3365445&partnerID=40&md5=508fdf92061c847fbbfe2eb97bfa7f79,"The early diagnosis of pulmonary cancer can significantly improve the survival rate of patients, where pulmonary nodules detection in computed tomography images plays an important role. In this article, we propose a novel pulmonary nodule detection system based on convolutional neural networks (CNN). Our system consists of two stages, pulmonary nodule candidate detection and false positive reduction. For candidate detection, we introduce Iterative Self-Organizing Data Analysis Techniques Algorithm (ISODATA) to Faster Region-based Convolutional Neural Network (Faster R-CNN) model. For false positive reduction, a three-dimensional convolutional neural network (3D-CNN) is employed to completely utilize the three-dimensional nature of CT images. In this network, Focal Loss is used to solve the class imbalance problem in this task. Experiments were conducted on LUNA16 dataset. The results show the preferable performance of the proposed system and the effectiveness of using ISODATA and Focal loss in pulmonary nodule detection is proved. © 2020 ACM.",Datasets; gaze detection; neural networks; text tagging,Computerized tomography; Convolution; Diagnosis; Image enhancement; Iterative methods; Mobile telecommunication systems; Positron emission tomography; Class imbalance problems; Computed tomography images; Early diagnosis; False-positive reduction; Iterative self organizing data analysis techniques algorithms; Pulmonary nodule detection; Pulmonary nodules; Pulmonary nodules detections; Convolutional neural networks
AMIL: Adversarial multi-instance learning for human pose estimation,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083983123&doi=10.1145%2f3355612&partnerID=40&md5=802274e6490fb57edae39a824d66d639,"Human pose estimation has an important impact on a wide range of applications, from human-computer interface to surveillance and content-based video retrieval. For human pose estimation, joint obstructions and overlapping upon human bodies result in departed pose estimation. To address these problems, by integrating priors of the structure of human bodies, we present a novel structure-aware network to discreetly consider such priors during the training of the network. Typically, learning such constraints is a challenging task. Instead, we propose generative adversarial networks as our learning model in which we design two residual Multiple-Instance Learning (MIL) models with identical architecture - one is used as the generator, and the other one is used as the discriminator. The discriminator task is to distinguish the actual poses from the fake ones. If the pose generator generates results that the discriminator is not able to distinguish from the real ones, then the model has successfully learned the priors. In the proposed model, the discriminator differentiates the ground-truth heatmaps from the generated ones, and later the adversarial loss back-propagates to the generator. Such procedure assists the generator to learn reasonable body configurations and is proved to be advantageous to improve the pose estimation accuracy. Meanwhile, we propose a novel function for MIL. It is an adjustable structure for both instance selection and modeling to appropriately pass the information between instances in a single bag. In the proposed residual MIL neural network, the pooling action adequately updates the instance contribution to its bag. The proposed adversarial residual multi-instance neural network that is based on pooling has been validated on two datasets for the human pose estimation task and successfully outperforms the other state-of-the-art models. The code will be made available on https://github.com/pshams55/AMIL. © 2020 ACM.",adversarial network; multiple-instance learning; neural networks; Pose estimations,Security systems; Adversarial networks; Content-based video retrieval; Human computer interfaces; Human pose estimations; Instance selection; Multi-instance learning; Multiple-instance learning; State of the art; Learning systems
Intelligent classification and analysis of essential genes using quantitative methods,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084758449&doi=10.1145%2f3343856&partnerID=40&md5=2103ca17d87556aa40d8e5aeb26edfa6,"Essential genes are considered to be the genes required to sustain life of different organisms. These genes encode proteins that maintain central metabolism, DNA replications, translation of genes, and basic cellular structure, and mediate the transport process within and out of the cell. The identification of essential genes is one of the essential problems in computational genomics. In this present study, to discriminate essential genes from other genes from a non-biologists perspective, the purine and pyrimidine distribution over the essential genes of four exemplary species, namely Homo sapiens, Arabidopsis thaliana, Drosophila melanogaster, and Danio rerio are thoroughly experimented using some quantitative methods. Moreover, the Indigent classification method has also been deployed for classification on the essential genes of the said species. Based on Shannon entropy, fractal dimension, Hurst exponent, and purine and pyrimidine bases distribution, 10 different clusters have been generated for the essential genes of the four species. Some proximity results are also reported herewith for the clusters of the essential genes. © 2020 ACM.",Essential genes; fractal dimension; Hurst exponent; purines; pyrimidines; Shannon entropy,Aromatic compounds; Fractal dimension; Population distribution; Arabidopsis thaliana; Cellular structure; Central metabolisms; Classification methods; Drosophila melanogaster; Essential problems; Intelligent classification; Quantitative method; Genes
Spatial preserved graph convolution networks for person re-identification,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084746868&doi=10.1145%2f3362988&partnerID=40&md5=c0b58ef4b83249ee0806a327e092ad95,"Person Re-identification is a very challenging task due to inter-class ambiguity caused by similar appearances, and large intra-class diversity caused by viewpoints, illuminations, and poses. To address these challenges, in this article, a graph convolution network based model for person re-identification is proposed to learn more discriminative feature embeddings, where a graph-structured relationship between person images and person parts are together integrated. Graph convolution networks extract common characteristics of the same person, while pyramid feature embedding exploits parts relations and learns stable representation with each person image. We achieve a very competitive performance respectively on three widely used datasets, indicating that the proposed approach significantly outperforms the baseline methods and achieves the state-of-the-art performance. © 2020 ACM.",feature embedding; graph convolution; Person re-identification,Convolution; Embeddings; Baseline methods; Competitive performance; Discriminative features; Feature embedding; Inter class; Network-based modeling; Person re identifications; State-of-the-art performance; Image processing
DQ-DASH: AQueuing Theory Approach to Distributed Adaptive Video Streaming,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083078124&doi=10.1145%2f3371040&partnerID=40&md5=f8de8b6f630ffa0d4df82eab88d3cee3,"The significant popularity of HTTP adaptive video streaming (HAS), such as Dynamic Adaptive Streaming over HTTP (DASH), over the Internet has led to a stark increase in user expectations in terms of video quality and delivery robustness. This situation creates new challenges for content providers who must satisfy the Quality-of-Experience (QoE) requirements and demands of their customers over a best-effort network infrastructure. Unlike traditional single server DASH, we developed a Distributed Queuing theory bitrate adaptation algorithm for DASH (DQ-DASH) that leverages the availability of multiple servers by downloading segments in parallel. DQ-DASH uses a Mx/D/1/K queuing theory based bitrate selection in conjunction with the request scheduler to download subsequent segments of the same quality through parallel requests to reduce quality fluctuations. DQ-DASH facilitates the aggregation of bandwidth from different servers and increases fault-tolerance and robustness through path diversity. The resulting resilience prevents clients from suffering QoE degradations when some of the servers become congested. DQ-DASH also helps to fully utilize the aggregate bandwidth from the servers and download the imminently required segment from the server with the highest throughput. We have also analyzed the effect of buffer capacity and segment duration for multi-source video streaming. © 2020 ACM.",ABR; DASH; fairness; multi-source; Multiple server; QoE; queuing theory,Bandwidth; Customer satisfaction; Fault tolerance; HTTP; Quality of service; Queueing theory; Adaptation algorithms; Adaptive video streaming; Best effort networks; Distributed queuing theories; Dynamic Adaptive Streaming over HTTP; Quality fluctuations; Quality of experience (QoE); User expectations; Video streaming
Learning Shared Semantic Space with Correlation Alignment for Cross-Modal Event Retrieval,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083078251&doi=10.1145%2f3374754&partnerID=40&md5=5f399ddb781cabfc4ddb88ae37a5f921,"In this article, we propose to learn shared semantic space with correlation alignment (S3CA) for multimodal data representations, which aligns nonlinear correlations of multimodal data distributions in deep neural networks designed for heterogeneous data. In the context of cross-modal (event) retrieval, we design a neural network with convolutional layers and fully connected layers to extract features for images, including images on Flickr-like social media. Simultaneously, we exploit a fully connected neural network to extract semantic features for text documents, including news articles from news media. In particular, nonlinear correlations of layer activations in the two neural networks are aligned with correlation alignment during the joint training of the networks. Furthermore, we project the multimodal data into a shared semantic space for cross-modal (event) retrieval, where the distances between heterogeneous data samples can be measured directly. In addition, we contribute a Wiki-Flickr Event dataset, where the multimodal data samples are not describing each other in pairs like the existing paired datasets, but all of them are describing semantic events. Extensive experiments conducted on both paired and unpaired datasets manifest the effectiveness of S3CA, outperforming the state-of-the-art methods. © 2020 ACM.",Cross-modal retrieval; deep learning; heterogeneous data; Wiki-Flickr Event dataset,Alignment; Convolutional neural networks; Deep learning; Deep neural networks; Network layers; Semantics; Fully connected neural network; Heterogeneous data; Multi-modal data; Non-linear correlations; Semantic event; Semantic features; Semantic Space; State-of-the-art methods; Multilayer neural networks
Adaptive Exploration for Unsupervised Person Re-identification,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083085688&doi=10.1145%2f3369393&partnerID=40&md5=54dfee1f075e7ec531a22308a1aa35c8,"Due to domain bias, directly deploying a deep person re-identification (re-ID) model trained on one dataset often achieves considerably poor accuracy on another dataset. In this article, we propose an Adaptive Exploration (AE) method to address the domain-shift problem for re-ID in an unsupervised manner. Specifically, in the target domain, the re-ID model is inducted to (1) maximize distances between all person images and (2) minimize distances between similar person images. In the first case, by treating each person image as an individual class, a non-parametric classifier with a feature memory is exploited to encourage person images to move far away from each other. In the second case, according to a similarity threshold, our method adaptively selects neighborhoods for each person image in the feature space. By treating these similar person images as the same class, the non-parametric classifier forces them to stay closer. However, a problem of the adaptive selection is that, when an image has too many neighborhoods, it is more likely to attract other images as its neighborhoods. As a result, a minority of images may select a large number of neighborhoods while a majority of images has only a few neighborhoods. To address this issue, we additionally integrate a balance strategy into the adaptive selection. We evaluate our methods with two protocols. The first one is called ""target-only re-ID"", in which only the unlabeled target data is used for training. The second one is called ""domain adaptive re-ID"", in which both the source data and the target data are used during training. Experimental results on large-scale re-ID datasets demonstrate the effectiveness of our method. Our code has been released at https://github.com/dyh127/Adaptive-Exploration-for-Unsupervised-Person-Re-Identification. © 2020 ACM.",deep learning; domain adaptation; Person re-identification; unsupervised learning,Computer networks; Adaptive explorations; Adaptive selection; Balance strategies; Feature space; Non-parametric classifiers; Person re identifications; Similarity threshold; Target domain; Large dataset
Exploring Deep Learning for View-Based 3D Model Retrieval,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081022153&doi=10.1145%2f3377876&partnerID=40&md5=c6b05d123c03e112a8ef4a646c874acd,"In recent years, view-based 3D model retrieval has become one of the research focuses in the field of computer vision and machine learning. In fact, the 3D model retrieval algorithm consists of feature extraction and similarity measurement, and the robust features play a decisive role in the similarity measurement. Although deep learning has achieved comprehensive success in the field of computer vision, deep learning features are used for 3D model retrieval only in a small number of works. To the best of our knowledge, there is no benchmark to evaluate these deep learning features. To tackle this problem, in this work we systematically evaluate the performance of deep learning features in view-based 3D model retrieval on four popular datasets (ETH, NTU60, PSB, and MVRED) by different kinds of similarity measure methods. In detail, the performance of hand-crafted features and deep learning features are compared, and then the robustness of deep learning features is assessed. Finally, the difference between single-view deep learning features and multi-view deep learning features is also evaluated. By quantitatively analyzing the performances on different datasets, it is clear that these deep learning features can consistently outperform all of the hand-crafted features, and they are also more robust than the hand-crafted features when different degrees of noise are added into the image. The exploration of latent relationships among different views in multi-view deep learning network architectures shows that the performance of multi-view deep learning outperforms that of single-view deep learning features with low computational complexity. © 2020 ACM.",3D model retrieval; benchmark; deep learning features; handcrafted features,3D modeling; Computer vision; Learning systems; Network architecture; Three dimensional computer graphics; 3 d model retrievals; Low computational complexity; Multi-views; Research focus; Similarity measure; Similarity measurements; View-based; Deep learning
Joint Stacked Hourglass Network and Salient Region Attention Refinement for Robust Face Alignment,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083081032&doi=10.1145%2f3374760&partnerID=40&md5=21d6d0df86ba81f801ec64b9ed98cb94,"Facial landmark detection aims to locate keypoints for facial images, which typically suffer from variations caused by arbitrary pose, diverse facial expressions, and partial occlusion. In this article, we propose a coarse-to-fine framework that joins a stacked hourglass network and salient region attention refinement for robust face alignment. To achieve this goal, we first present a multi-scale region learning module to analyze the structure information at a different facial region and extract a strong discriminative deep feature. Then we employ a stacked hourglass network for heatmap regression and initial facial landmarks prediction. Specifically, the stacked hourglass network introduces an improved Inception-ResNet unit as a basic building block, which can effectively improve the receptive field and learn contextual feature representations. Meanwhile, a novel loss function takes into account global weights and local weights to make the heatmap regression more accurate. Different from existing heatmap regression models, we present a salient region attention refinement module to extract a precise feature based on the heatmap regression, and utilize the filtered feature for landmarks refinement to achieve accurate prediction. Extensive experimental results of several challenging datasets (including 300 Faces in the Wild, Caltech Occluded Faces in the Wild, and Annotated Facial Landmarks Faces in the Wild) confirm that our approach can achieve more competitive performance than the most advanced algorithms. © 2020 ACM.",hourglass network; inception-ResNet; maximum coordinate loss; Multi-scale learning; salient region attention,Computer networks; Accurate prediction; Basic building block; Competitive performance; Contextual feature; Facial Expressions; Facial landmark detection; Partial occlusions; Structure information; Regression analysis
LFGAN: 4D Light Field Synthesis from a Single RGB Image,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083081718&doi=10.1145%2f3366371&partnerID=40&md5=95c5b1d7f1de2e0f790241c7f6d33992,"We present a deep neural network called the light field generative adversarial network (LFGAN) that synthesizes a 4D light field from a single 2D RGB image. We generate light fields using a single image super-resolution (SISR) technique based on two important observations. First, the small baseline gives rise to the high similarity between the full light field image and each sub-aperture view. Second, the occlusion edge at any spatial coordinate of a sub-aperture view has the same orientation as the occlusion edge at the corresponding angular patch, implying that the occlusion information in the angular domain can be inferred from the sub-aperture local information. We employ the Wasserstein GAN with gradient penalty (WGAN-GP) to learn the color and geometry information from the light field datasets. The network can generate a plausible 4D light field comprising 8×8 angular views from a single sub-aperture 2D image. We propose new loss terms, namely epipolar plane image (EPI) and brightness regularization (BRI) losses, as well as a novel multi-stage training framework to feed the loss terms at different time to generate superior light fields. The EPI loss can reinforce the network to learn the geometric features of the light fields, and the BRI loss can preserve the brightness consistency across different sub-aperture views. Two datasets have been used to evaluate our method: in addition to an existing light field dataset capturing scenes of flowers and plants, we have built a large dataset of toy animals consisting of 2,100 light fields captured with a plenoptic camera. We have performed comprehensive ablation studies to evaluate the effects of individual loss terms and the multi-stage training strategy, and have compared LFGAN to other state-of-the-art techniques. Qualitative and quantitative evaluation demonstrates that LFGAN can effectively estimate complex occlusions and geometry in challenging scenes, and outperform other existing techniques. © 2020 ACM.",generative adversarial network; Light field synthesis; single image super-resolution,Deep neural networks; Geometry; Large dataset; Luminance; Adversarial networks; Epipolar plane images; Geometry information; Plenoptic cameras; Quantitative evaluation; Spatial coordinates; State-of-the-art techniques; Training framework; Image processing
Cloud Gaming with Foveated Video Encoding,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083085469&doi=10.1145%2f3369110&partnerID=40&md5=432de6ec22b80ea13d8f95ed7d0623a4,"Cloud gaming enables playing high-end games, originally designed for PC or game console setups, on low-end devices such as netbooks and smartphones, by offloading graphics rendering to GPU-powered cloud servers. However, transmitting the high-resolution video requires a large amount of network bandwidth, even though it is a compressed video stream. Foveated video encoding (FVE) reduces the bandwidth requirement by taking advantage of the non-uniform acuity of human visual system and by knowing where the user is looking. Based on a consumer-grade real-time eye tracker and an open source cloud gaming platform, we provide a cloud gaming FVE prototype that is game-agnostic and requires no modifications to the underlying game engine. In this article, we describe the prototype and its evaluation through measurements with representative games from different genres to understand the effect of parametrization of the FVE scheme on bandwidth requirements and to understand its feasibility from the latency perspective. We also present results from a user study on first-person shooter games. The results suggest that it is possible to find a ""sweet spot"" for the encoding parameters so the users hardly notice the presence of foveated encoding but at the same time the scheme yields most of the achievable bandwidth savings. © 2020 ACM.",adaptive bitrate encoding; Cloud gaming; foveated video encoding; game streaming; gaze-contingent encoding,Bandwidth; Computer graphics; Encoding (symbols); Eye tracking; Signal encoding; Bandwidth requirement; Bandwidth savings; Compressed video stream; Encoding parameters; First person shooter games; Graphics rendering; Human Visual System; Network bandwidth; Video signal processing
Cross Refinement Techniques for Markerless Human<?brk?> Motion Capture,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083081541&doi=10.1145%2f3372207&partnerID=40&md5=b5b26ae44fb3c91be9a8e21ebe513d2b,"This article presents a global 3D human pose estimation method for markerless motion capture. Given two calibrated images of a person, it first obtains the 2D joint locations in the images using a pre-trained 2D Pose CNN, then constructs the 3D pose based on stereo triangulation. To improve the accuracy and the stability of the system, we propose two efficient optimization techniques for the joints. The first one, called cross-view refinement, optimizes the joints based on epipolar geometry. The second one, called cross-joint refinement, optimizes the joints using bone-length constraints. Our method automatically detects and corrects the unreliable joint, and consequently is robust against heavy occlusion, symmetry ambiguity, motion blur, and highly distorted poses. We evaluate our method on a number of benchmark datasets covering indoors and outdoors, which showed that our method is better than or on par with the state-of-the-art methods. As an application, we create a 3D human pose dataset using the proposed motion capture system, which contains about 480K images of both indoor and outdoor scenes, and demonstrate the usefulness of the dataset for human pose estimation. © 2020 ACM.",camera calibration; convolutional neural network; epipolar geometry; Human pose estimation,System stability; 3D human pose estimation; Human pose estimations; Markerless motion capture; Motion capture system; Optimization techniques; Refinement techniques; State-of-the-art methods; Stereo triangulation; Motion capture
Enforcing Affinity Feature Learning through Self-attention for Person Re-identification,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083083968&doi=10.1145%2f3377352&partnerID=40&md5=690c533eeb32e56d1055f92d88a17f70,"Person re-identification is the task of recognizing an individual across heterogeneous non-overlapping camera views. It has become a crucial capability needed by many applications in public space video surveillance. However, it remains a challenging task due to the subtle inter-class similarity and large intra-class variation found in person images. Current CNN-based approaches have focused and investigated traditional identification or verification frameworks. Such approaches typically use the whole input image including the background and fail to pay attention to specific body parts, deviating the feature representation learning from informative parts. In this article, we introduce a self-attention mechanism coupled with cross-resolution to improve the feature representation learning of person re-identification task. The proposed self-attention module reinforces the most informative parts from a high-resolution image using its internal representation at the low-resolution. In particular, the model is fed with a pair of images on a different scale and consists of two branches. The upper branch processes the high-resolution image and learns high dimensional feature representation while the lower branch processes the low-resolution image and learns a filtering attention heatmap. The feature maps on the lower branch are subsequently weighted to reflect the importance of each patch of the input image using a softmax operation; whereas, on the upper branch, we apply a max pooling operation to downsample the high-resolution feature map before element-wise multiplied with the attention heatmap. Our attention module helps the network learn the most discriminative visual features of multiple regions of the image and is specifically optimized to attend and enforce feature representation at different scales. Extensive experiments on three large-scale datasets show that network architectures augmented with our self-attention module systematically improve their accuracy and outperform various state-of-the-art models by a large margin. © 2020 ACM.",attention mechanism; deep residual network; pedestrian retrieval; Person ReID,Image segmentation; Machine learning; Network architecture; Security systems; Feature representation; High dimensional feature; High resolution image; Internal representation; Low resolution images; Non-overlapping camera views; Person re identifications; Verification framework; Large dataset
Inception U-Net Architecture for Semantic Segmentation to Identify Nuclei in Microscopy Cell Images,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083074507&doi=10.1145%2f3376922&partnerID=40&md5=6314a2bd1542f4ed4676d6c86004a82e,"With the increasing applications of deep learning in biomedical image analysis, in this article we introduce an inception U-Net architecture for automating nuclei detection in microscopy cell images of varying size and modality to help unlock faster cures, inspired from Kaggle Data Science Bowl Challenge 2018 (KDSB18). This study follows from the fact that most of the analysis requires nuclei detection as the starting phase for getting an insight into the underlying biological process and further diagnosis. The proposed architecture consists of a switch normalization layer, convolution layers, and inception layers (concatenated 1x1, 3x3, and 5x5 convolution and the hybrid of a max and Hartley spectral pooling layer) connected in the U-Net fashion for generating the image masks. This article also illustrates the model perception of image masks using activation maximization and filter map visualization techniques. A novel objective function segmentation loss is proposed based on the binary cross entropy, dice coefficient, and intersection over union loss functions. The intersection over union score, loss value, and pixel accuracy metrics evaluate the model over the KDSB18 dataset. The proposed inception U-Net architecture exhibits quite significant results as compared to the original U-Net and recent U-Net++ architecture. © 2020 ACM.",Convolution neural networks; healthcare; medical image analysis; semantic segmentation,Convolution; Deep learning; Semantics; Biological process; Biomedical image analysis; Dice coefficient; Map visualizations; Nuclei detections; Objective functions; Proposed architectures; Semantic segmentation; Image segmentation
A Deep Learning Approach for Face Hallucination Guided by Facial Boundary Responses,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083080523&doi=10.1145%2f3377874&partnerID=40&md5=12f064ceb6733e9ebb90dd5b676c852f,"Face hallucination is a domain-specific super-resolution (SR) problem of learning a mapping between a low-resolution (LR) face image and its corresponding high-resolution (HR) image. Tremendous progress on deep learning has shown exciting potential for a variety of face hallucination tasks. However, most deep-learning-based methods are limited to handle facial appearance information without paying attention to facial structure priors. In this article, we propose an open source1 Boundary-aware Dual-branch Network (BDN) for face hallucination, which simultaneously extracts face features and estimates facial boundary responses from LR inputs, ultimately fusing them to reconstruct HR results. Specifically, we first upsample LR face images to HR feature maps, and then feed the upsampled HR features into a memory unit and an attention unit synchronously to obtain the refined features and predict facial boundary responses. Next, they are fed into a feature map fusion unit to combine facial appearance and structure information by a spatial attention mechanism. Moreover, we employ a series of stacked units to boost performance before recovering HR face images. Finally, a discriminative network is developed to improve visual quality by introducing adversarial learning strategy. Extensive experiments show that the proposed approach achieves superior face hallucination results against the state-of-the-art ones. © 2020 ACM.",Face hallucination; face super-resolution; facial boundary response; feature maps fusion,Learning systems; Adversarial learning; Discriminative networks; Face hallucination; Facial appearance; High resolution image; Learning approach; Learning-based methods; Structure information; Deep learning
An Evaluation of Tile Selection Methods for Viewport-Adaptive Streaming of 360-Degree Video,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083075062&doi=10.1145%2f3373359&partnerID=40&md5=91cc5f0ba5cf3ce79de1e08a525842da,"360-degree video has become increasingly popular nowadays. For effective transmission of bandwidth-intensive 360-degree video over networks, viewport-adaptive streaming has been introduced. In this article, we evaluate, for the first time, ten existing methods to understand the effectiveness of tile-based viewport adaptive streaming of 360-degree video. Experimental results show that tile-based methods can improve the average V-PSNR by up to 4.3 dB compared to a non-tiled method under low delay settings. Here, the V-PSNR is computed as the peak signal-to-noise ratio of the adapted viewport compared to the corresponding origin viewport. Also, different methods show different tradeoffs between average viewport quality and viewport quality variations. Especially, the performances of most tile-based methods decrease quickly as the segment duration and/or buffer size increase for the content with no main focus. Even, under long delay settings like HTTP Adaptive Streaming, it is found that the simple non-tiled method appears to be the best one. For the content with a strong viewing focus, it is found that the tile-based methods are less influenced by the segment duration and the buffer size. In addition, a comparison of the performances of the tile selection methods using two popular viewport estimation methods is conducted. It is interesting that there is only little difference found in performances of tile selection methods. The findings of this study are useful for service providers to make decisions on deployment of streaming solutions. © 2020 ACM.",360-degree Video; Viewport Adaptive Streaming; Virtual Reality,Signal to noise ratio; Adaptive streaming; Effective transmission; Estimation methods; Peak signal to noise ratio; Quality variation; Segment duration; Selection methods; Service provider; Video streaming
Spatio-temporal Segmentation Based Adaptive Compression of Dynamic Mesh Sequences,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083085675&doi=10.1145%2f3377475&partnerID=40&md5=43cff50ca4d865f3aaa1b68b025e2a64,"With the recent advances in data acquisition techniques, the compression of various dynamic mesh sequence data has become an important topic in the computer graphics community. In this article, we present a new spatio-temporal segmentation-based approach for the adaptive compression of the dynamic mesh sequences. Given an input dynamic mesh sequence, we first compute an initial temporal cut to obtain a small subsequence by detecting the temporal boundary of dynamic behavior. Then, we apply a two-stage vertex clustering on the resulting subsequence to classify the vertices into groups with optimal intra-affinities. After that, we design a temporal segmentation step based on the variations of the principal components within each vertex group prior to performing a PCA-based compression. Furthermore, we apply an extra step on the lossless compression of the PCA bases and coefficients to gain more storage saving. Our approach can adaptively determine the temporal and spatial segmentation boundaries to exploit both temporal and spatial redundancies. We have conducted extensive experiments on different types of 3D mesh animations with various segmentation configurations. Our comparative studies show the advantages of our approach for the compression of 3D mesh animations. © 2020 ACM.",adaptive spatio-temporal segmentation; animation compression; data compression; Dynamic mesh sequences,Computer graphics; Data acquisition; Digital storage; Mesh generation; Adaptive compression; Comparative studies; Lossless compression; Principal Components; Spatio-temporal segmentation; Temporal and spatial redundancies; Temporal and spatial segmentation; Temporal segmentations; Image segmentation
Causal Structures of Multidimensional QoE in Haptic-Audiovisual Communications,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083081196&doi=10.1145%2f3375922&partnerID=40&md5=165df16f1584ec324b189dc417f16eb6,"This article proposes a methodology for building and verifying plausible models that can express causation in multidimensional QoE for haptic-audiovisual interactive communications. For the modeling, we utilize subjective experimental data of five-point scores collected in a previous study where a pair of subjects carry out two kinds of interactive tasks (castanets hitting and object movement) in real space (not in virtual space). The multidimensional QoE is composed of 15 measures for the castanets hitting and 14 measures for the object movement. To reduce the dimension, we classify the QoE measures into three groups as indicators of three constructs (latent variables or factors): AVQ (AudioVisual Quality), HQ (Haptic Quality), and UXQ (User eXperience Quality). We then build two models: (1) a structural equation model in which AVQ and HQ correlated with each other give causal effects on UXQ, and (2) a confirmatory factor analysis model in which the three constructs are only correlated with each other. We refer to the former as 3C-SEM and the latter as 3C-CFA. We further introduce a CFA model with a single construct for which all QoE measures are its indicators (1C-CFA). We perform Bayesian analysis of the three models by means of Markov chain Monte Carlo simulation; in each model, the deviance information criterion is obtained for model comparison, and the posterior predictive p-value is calculated for model checking. As a result, we find that 3C-SEM is the most plausible and that HQ has a stronger causal effect on UXQ than AVQ. We also learn that the correlation between AVQ and UXQ is much higher than the direct causal effect and that the increase in the association as correlation is due to the causal effect of HQ on UXQ through the correlation of AVQ with HQ. Thus, it is suggested that improving haptic performance is more effective in enhancement of QoE than improving audiovisual performance. © 2020 ACM.",Bayesian modeling; causation; CFA; correlation; haptic-audiovisual interactive communications; latent variables; MCMC; OpenBUGS; QoE; Quality of experience; SEM,Audiovisual; Factor analysis; Intelligent systems; Markov chains; Monte Carlo methods; User experience; Audiovisual quality; Bayesian Analysis; Confirmatory factor analysis; Deviance information criterion; Interactive communications; Markov chain monte carlo simulation; Model comparison; Structural equation modeling; Model checking
Image Retrieval for Complex Queries Using Knowledge Embedding,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083082038&doi=10.1145%2f3375786&partnerID=40&md5=0b270c5587b06a6592e9ec746ed37e28,"With the increase in popularity of image-based applications, users are retrieving images using more sophisticated and complex queries. We present three types of complex queries, namely, long, ambiguous, and abstract. Each type of query has its own characteristics/complexities and thus leads to imprecise and incomplete image retrieval. Existing methods for image retrieval are unable to deal with the high complexity of such queries. Search engines need to integrate their image retrieval process with knowledge to obtain rich semantics for effective retrieval. We propose a framework, Image Retrieval using Knowledge Embedding (ImReKE), for embedding knowledge with images and queries, allowing retrieval approaches to understand the context of queries and images in a better way. ImReKE (IR-Approach, Knowledge-Base) takes two inputs, namely, an image retrieval approach and a knowledge base. It selects quality concepts (concepts that possess properties such as rarity, newness, etc.) from the knowledge base to provide rich semantic representations for queries and images to be leveraged by the image retrieval approach. For the first time, an effective knowledge base that exploits both the visual and textual information of concepts has been developed. Our extensive experiments demonstrate that the proposed framework improves image retrieval significantly for all types of complex queries. The improvement is remarkable in the case of abstract queries, which have not yet been dealt with explicitly in the existing literature. We also compare the quality of our knowledge base with the existing text-based knowledge bases, such as ConceptNet, ImageNet, and the like. © 2020 ACM.",ambiguous query; complex query; diversity; Image retrieval; knowledge base; knowledge embedding; query expansion,Abstracting; Embeddings; Image enhancement; Knowledge based systems; Knowledge management; Search engines; Semantics; High complexity; Image-based application; Knowledge basis; Knowledge embedding; Quality concepts; Retrieval process; Semantic representation; Textual information; Image retrieval
Frame-level Bit Allocation Optimization Based on<?brk?> Video Content Characteristics for HEVC,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083086361&doi=10.1145%2f3380827&partnerID=40&md5=2a87e335251fddae246da326091eed0b,"Rate control plays an important role in high efficiency video coding (HEVC), and bit allocation is the foundation of rate control. The video content characteristics are significant for bit allocation, and modeling an accurate relationship between video content characteristics and bit allocation is essential for bit allocation optimization. Therefore, in this article, a video content characteristics-based frame-level optimal bit allocation algorithm is proposed for improving the rate distortion (RD) performance of HEVC. First, the number of search points of motion estimation is used to evaluate the motion activity of video content, and the relationship between the search points and bit allocation is modeled as the search-points model. Second, the grey level co-occurrence matrix and temporal perceptual information are used to evaluate the spatial and temporal texture complexity, and the relationship between the video content texture complexity and bit allocation is modeled as the texture-complexity model. Then, the search-points model and texture-complexity model are jointly employed to allocate the coding bits for the second and third layers of the HEVC hierarchical coding structure. Finally, the remaining coding bits of a group-of-pictures (GOP) are allocated to the first layer of HEVC coding structure. To evaluate the performance of the proposed algorithm, the RD performance and bitrate accuracy are used as evaluation criteria, and the experimental results show that when compared with the popularly used R-λ model-based bit allocation algorithm, the proposed algorithm achieves an average of -3.43% BDBR reduction and 0.13 dB BDPSNR gains with only 0.02% loss of bitrate accuracy. © 2020 ACM.",bit allocation; HEVC; rate control; video content characteristics,Electric distortion; Image coding; Motion estimation; Signal distortion; Textures; Video recording; Evaluation criteria; Grey level co-occurrence matrixes; Hierarchical coding structures; High-efficiency video coding; Model-based bit allocation; Optimal bit allocation; Perceptual information; Texture complexity; Video signal processing
RCE-HIL: Recognizing Cross-media Entailment with Heterogeneous Interactive Learning,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083076689&doi=10.1145%2f3365003&partnerID=40&md5=922f9a8cf19a106b4d13fd2fc66ca0b4,"Entailment recognition is an important paradigm of reasoning that judges if a hypothesis can be inferred from given premises. However, previous efforts mainly concentrate on text-based reasoning as recognizing textual entailment (RTE), where the hypotheses and premises are both textual. In fact, humans' reasoning process has the characteristic of cross-media reasoning. It is naturally based on the joint inference with different sensory organs, which represent complementary reasoning cues from unique perspectives as language, vision, and audition. How to realize cross-media reasoning has been a significant challenge to achieve the breakthrough for width and depth of entailment recognition. Therefore, this article extends RTE to a novel reasoning paradigm: recognizing cross-media entailment (RCE), and proposes heterogeneous interactive learning (HIL) approach. Specifically, HIL recognizes entailment relationships via cross-media joint inference, from image-text premises to text hypotheses. It is an end-to-end architecture with two parts: (1) Cross-media hybrid embedding is proposed to perform cross embedding of premises and hypotheses for generating their fine-grained representations. It aims to achieve the alignment of cross-media inference cues via image-text and text-text interactive attention. (2) Heterogeneous joint inference is proposed to construct a heterogeneous interaction tensor space and extract semantic features for entailment recognition. It aims to simultaneously capture the interaction between cross-media premises and hypotheses and distinguish their entailment relationships. Experimental results on widely used Stanford natural language inference (SNLI) dataset with image premises from Flickr30K dataset verify the effectiveness of HIL and the intrinsic inter-media complementarity in reasoning. © 2020 ACM.",cross-media hybrid embedding; Cross-media reasoning; heterogeneous joint inference; recognizing cross-media entailment,Educational technology; Embeddings; Learning systems; Semantics; Heterogeneous interactions; Heterogeneous joints; Interactive learning; Natural languages; Reasoning paradigm; Reasoning process; Recognizing textual entailments; Semantic features; Character recognition
Soul Dancer: Emotion-based human action generation,2020,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078403324&doi=10.1145%2f3340463&partnerID=40&md5=38e6242af6c591667ef8d59760364d50,"Body language is one of the most common ways of expressing human emotion. In this article, we make the first attempt to generate an action video with a specific emotion from a single person image. The goal of the emotion-based action generation task (EBAG) is to generate action videos expressing a specific type of emotion given a single reference image with a full human body. We divide the task into two parts and propose a two-stage framework to generate action videos with specified emotions. At the first stage, we propose an emotion-based pose sequence generation approach (EPOSE-GAN) for translating the emotion to a pose sequence. At the second stage, we generate the target video frames according to the three inputs including the source pose and the target pose as the motion information and the source image as the appearance reference by using conditional GAN model with an online training strategy. Our framework produces the pose sequence and transforms the action independently, which highlights the fundamental role that the high-level pose feature plays in generating action video with a specific emotion. The proposed method has been evaluated on the “Soul Dancer” dataset which is built for action emotion analysis and generation. The experimental results demonstrate that our framework can effectively solve the emotion-based action generation task. However, the gap in the details of the appearance between the generated action video and the real-world video still exists, which indicates that the emotion-based action generation task has great research potential. © 2020 Association for Computing Machinery.",Action generation; Body language; Human emotion; Pose sequence,Computer networks; Action generation; Body language; Human emotion; Motion information; Pose sequence; Real world videos; Research potential; Sequence generation; Man machine systems
Learning discriminative sentiment representation from strongly- And weakly supervised CNNs,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077795156&doi=10.1145%2f3326335&partnerID=40&md5=ecc6ed40bd0251e2099b853030b2da34,"Visual sentiment analysis is attracting increasing attention with the rapidly growing amount of images uploaded to social networks. Learning rich visual representations often requires training deep convolutional neural networks (CNNs) on massive manually labeled data, which is expensive or scarce especially for a subjective task like visual sentiment analysis. Meanwhile, a large quantity of social images is quite available yet noisy by querying social networks using the sentiment categories as keywords, where various types of images related to the specific sentiment can be easily collected. In this article, we propose a multiple kernel network for visual sentiment recognition, which learns representation from strongly- and weakly supervised CNNs. Specifically, the weakly supervised deep model is trained using the large-scale data from social images, whereas the strongly supervised deep model is fine tuned on the affecitve datasets with manual annotation. We employ the multiple kernel scheme on the multiple layers of CNNs, which can automatically select the discriminative representation by learning a linear combination from a set of pre-defined kernels. In addition, we introduce a large-scale dataset collected from popular comics of various countries, such as America, Japan, China, and France, which consists of 11,821 images with various artistic styles. Experimental results show 6 that the multiple kernel network achieves consistent improvements over the state-of-the-art methods on the public affective datasets, as well as the newly established Comics dataset. The Comics dataset can be found at http://cv.nankai.edu.cn/projects/Comic. © 2019 Association for Computing Machinery.",Convolutional neural network; Multiple kernel learning; Visual sentiment analysis,Convolution; Deep neural networks; Neural networks; Sentiment analysis; Social networking (online); Convolutional neural network; Large scale data; Large-scale dataset; Linear combinations; Manual annotation; Multiple Kernel Learning; State-of-the-art methods; Visual representations; Large dataset
Affective content-aware adaptation scheme on QoE optimization of adaptive streaming over HTTP,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077799830&doi=10.1145%2f3328997&partnerID=40&md5=2a5bb2179f7339993d617545b3758f51,"The article presents a novel affective content-aware adaptation scheme (ACAA) to optimize Quality of Experience (QoE) for dynamic adaptive video streaming over HTTP (DASH). Most of the existing DASH adaptation schemes conduct video bit-rate adaptation based on an estimation of available network resources, which ignore user preference on affective content (AC) embedded in video data streaming over the network. Since the personal demands to AC is very different among all viewers, to satisfy individual affective demand is critical to improve the QoE in commercial video services. However, the results of video affective analysis cannot be applied into a current adaptive streaming scheme directly. Correlating the AC distributions in user’s viewing history to each being streamed segment, the affective relevancy can be inferred as an affective metric for the AC related segment. Further, we have proposed an ACAA scheme to optimize QoE for user desired affective content while taking into account both network status and affective relevancy. We have implemented the ACAA scheme over a realistic trace-based evaluation and compared its performance in terms of network performance, QoE with that of Probe and Adaptation (PANDA), buffer-based adaptation (BBA), and Model Predictive Control (MPC). Experimental results show that ACAA can preserve available buffer time for future being delivered affective content preferred by viewer’s individual preference to achieve better QoE in affective contents than those normal contents while remain the overall QoE to be satisfactory. © 2019 Association for Computing Machinery.",Affective content analysis; HTTP adaptive streaming; MPEG-DASH; Quality of experience,HTTP; Model predictive control; Motion Picture Experts Group standards; Predictive control systems; Video streaming; Adaptation scheme; Adaptive streaming; Adaptive streaming over http; Commercial video services; Content analysis; Individual preference; Mpeg dashes; Quality of experience (QoE); Quality of service
Affective computing for large-scale heterogeneous multimedia data: A survey,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077787050&doi=10.1145%2f3363560&partnerID=40&md5=0228832fb973227ca70e23852dc1cdd2,"The wide popularity of digital photography and social networks has generated a rapidly growing volume of multimedia data (i.e., images, music, and videos), resulting in a great demand for managing, retrieving, and understanding these data. Affective computing (AC) of these data can help to understand human behaviors and enable wide applications. In this article, we survey the state-of-the-art AC technologies comprehensively for large-scale heterogeneous multimedia data. We begin this survey by introducing the typical emotion representation models from psychology that are widely employed in AC. We briefly describe the available datasets for evaluating AC algorithms. We then summarize and compare the representative methods on AC of different multimedia types, i.e., images, music, videos, and multimodal data, with the focus on both handcrafted features-based methods and deep learning methods. Finally, we discuss some challenges and future directions for multimedia affective computing. © 2019 Association for Computing Machinery.",Affective computing; Emotion recognition; Large-scale multimedia; Sentiment analysis,Behavioral research; Deep learning; Sentiment analysis; Affective Computing; Digital photography; Emotion recognition; Emotion representation; Large-scale multimedia; Learning methods; Multi-modal data; State of the art; Surveys
A hierarchical CNN-RNN approach for visual emotion classification,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077798457&doi=10.1145%2f3359753&partnerID=40&md5=da6c2c488f654b5b3bf67c9a376a4838,"Visual emotion classification is predicting emotional reactions of people for the given visual content. Psychological studies show that human emotions are affected by various visual stimuli from low level to high level, including contrast, color, texture, scene, object, and association, among others. Traditional approaches regarded different levels of stimuli as independent components and ignored to effectively fuse different stimuli. This article proposes a hierarchical convolutional neural network (CNN)-recurrent neural network (RNN) approach to predict the emotion based on the fused stimuli by exploiting the dependency among different-level features. First, we introduce a dual CNN to extract different levels of visual stimulus, where two related loss functions are designed to learn the stimuli representation under a multi-task learning structure. Further, to model the dependency between the low- and high-level stimulus, a stacked bi-directional RNN is proposed to fuse the preceding learned features from the dual CNN. Comparison experiments on one large-scale and three small scale datasets show that the proposed approach brings significant improvement. Ablation experiments demonstrate the effectiveness of different modules from our model. © 2019 Association for Computing Machinery.",Feature fusing; Hierarchical CNN-RNN; Multi-task learning; Stacked bi-directional RNN; Visual emotion recognition,Behavioral research; Classification (of information); Large dataset; Learning systems; Textures; Bi-directional; Emotion recognition; Feature fusing; Hierarchical CNN-RNN; Multitask learning; Recurrent neural networks
Physiological signals-based emotion recognition via high-order correlation learning,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077798579&doi=10.1145%2f3332374&partnerID=40&md5=35ee9bc668ef110c3a76aa09d4f01dc3,"Emotion recognition by physiological signals is an effective way to discern the inner state of human beings and therefore has been widely adopted in many user-centered applications. The majority of current state-of-the-art methods focus on exploring relationship among emotion and physiological signals. Given some particular features of the natural process of emotional expression, it is still a challenging and urgent issue to efficiently combine such high-order correlations among multimodal physiological signals and subjects. To tackle the problem, a novel multi-hypergraph neural networks is proposed, in which one hypergraph is established with one type of physiological signals to formulate inter-subject correlations. Each one of the vertices in a hypergraph stands for one subject with a description of its related stimuli, and the complex correlations among the vertices can be formulated through hyperedges. With the multi-hypergraph structure of the subjects, emotion recognition is translated into classification of vertices in the multi-hypergraph structure. Experimental results with the DEAP dataset and ASCERTAIN dataset demonstrate that the proposed method outperforms the current state-of-the-art methods. © 2019 Association for Computing Machinery.",Emotion recognition; Multi-hypergraph neural networks; Multi-modal fusion; Physiological signals,Graph theory; Physiology; Speech recognition; Complex correlation; Emotion recognition; Emotional expressions; High order correlation; Hypergraph; Multi-modal fusion; Physiological signals; State-of-the-art methods; Biomedical signal processing
Characterizing subtle facial movements via Riemannian manifold,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077802418&doi=10.1145%2f3342227&partnerID=40&md5=9dbf358d87f416790c2c79e7e76296fa,"Characterizing subtle facial movements from videos is one of the most intensive topics in computer vision research. It is, however, challenging, since (1) the intensity of subtle facial muscle movement is usually low, (2) the duration may be transient, and (3) datasets containing spontaneous subtle movements with reliable annotations are painful to obtain and often of small sizes. This article is targeted at addressing these problems for characterizing subtle facial movements from both the aspects of motion elucidation and description. First, we propose an efficient method for elucidating hidden and repressed movements to make them easier to get noticed. We explore the feasibility of linearizing motion magnification and temporal interpolation, which is obscured by the architecture of existing methods. On this basis, we propose a consolidated framework, termed MOTEL, to expand temporal duration and amplify subtle facial movements simultaneously. Second, we make our contribution to dynamic description. One major challenge is to capture the intrinsic temporal variations caused by movements and omit extrinsic ones caused by different individuals and various environments. To diminish the influences of such extrinsic diversity, we propose the tangent delta descriptor to characterize the dynamics of short-term movements using the differences between points on the tangent spaces to the manifolds, rather than the points themselves. We then relax the trajectory-smooth assumption of the conventional manifold-based trajectory modeling methods and incorporate the tangent delta descriptor with the sequential inference approaches to cover the period of facial movements. The proposed motion modeling approach is validated by a series of experiments on publicly available datasets in the tasks of micro-expression recognition and visual speech recognition. © 2019 Association for Computing Machinery.",Micro-expression recognition; Motion description; Motion magnification; Video representation; Visual speech recognition,Computer networks; Micro-expressions; Motion description; Motion magnification; Video representations; Visual speech recognition; Speech recognition
Sequential cross-modal hashing learning via multi-scale correlation mining,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077796150&doi=10.1145%2f3356338&partnerID=40&md5=f2407a537ecc0d1bd2230669078f9733,"Cross-modal hashing aims to map heterogeneous multimedia data into a common Hamming space through hash function, and achieves fast and flexible cross-modal retrieval. Most existing cross-modal hashing methods learn hash function by mining the correlation among multimedia data, but ignore the important property of multimedia data: Each modality of multimedia data has features of different scales, such as texture, object, and scene features in the image, which can provide complementary information for boosting retrieval task. The correlations among the multi-scale features are more abundant than the correlations between single features of multimedia data, which reveal finer underlying structures of the multimedia data and can be used for effective hashing function learning. Therefore, we propose the Multi-scale Correlation Sequential Cross-modal Hashing (MCSCH) approach, and its main contributions can be summarized as follows: (1) Multi-scale feature guided sequential hashing learning method is proposed to share the information from features of different scales through an RNN-based network and generate the hash codes sequentially. The features of different scales are used to guide the hash codes generation, which can enhance the diversity of the hash codes and weaken the influence of errors in specific features, such as false object features caused by occlusion. (2) Multi-scale correlation mining strategy is proposed to align the features of different scales in different modalities and mine the correlations among aligned features. These correlations reveal the finer underlying structure of multimedia data and can help to boost the hash function learning. (3) Correlation evaluation network evaluates the importance of the correlations to select the worthwhile correlations, and increases the impact of these correlations for hash function learning. Experiments on two widely-used 2-media datasets and a 5-media dataset demonstrate the effectiveness of our proposed MCSCH approach. © 2019 Association for Computing Machinery.",Correlation mining; Cross-modal hashing; Multi-scale; Sequential hash learning,Codes (symbols); Learning systems; Textures; Correlation evaluations; Correlation mining; Cross-modal; Hashing functions; Multi-scale; Multi-scale features; Sequential hash learning; Sequential hashing; Hash functions
Image/Video restoration via multiplanar autoregressive model and low-rank optimization,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076968810&doi=10.1145%2f3341728&partnerID=40&md5=d0f3e03b12d692bb3faa8ed9dbebfdf4,"In this article, we introduce an image/video restoration approach by utilizing the high-dimensional similarity in images/videos. After grouping similar patches from neighboring frames, we propose to build a multiplanar autoregressive (AR) model to exploit the correlation in cross-dimensional planes of the patch group,which has long been neglected by previous AR models. To further utilize the nonlocal self-similarity in images/videos, a joint multiplanar AR and low-rank based approach is proposed (MARLow) to reconstruct patch groups more effectively. Moreover, for video restoration, the temporal smoothness of the restored video is constrained by the Markov random field (MRF), where MRF encodes a priori knowledge about consistency of patches from neighboring frames. Specifically, we treat different restoration results (from different patch groups) of a certain patch as labels of an MRF, and temporal consistency among these restored patches is imposed. The proposed method is also suitable for other restoration applications such as interpolation and text removal. Extensive experimental results demonstrate that the proposed approach obtains encouraging performance comparing with state-of-the-art methods. © 2019 Association for Computing Machinery.",Image/video restoration; Low-rank optimization; Markov random field; Multiplanar autoregressive model,Image segmentation; Magnetorheological fluids; Markov processes; Restoration; Structural frames; Video cameras; Auto regressive models; High-dimensional; Markov Random Fields; Priori knowledge; Self-similarities; State-of-the-art methods; Temporal consistency; Video restoration; Image reconstruction
Video retrieval with similarity-preserving deep temporal hashing,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076997140&doi=10.1145%2f3356316&partnerID=40&md5=e56618757c40bed0a4b47202218c4e34,"Despite the fact that remarkable progress has been made in recent years, Content-based Video Retrieval (CBVR) is still an appealing research topic due to increasing search demands in the Internet era of big data. This article aims to explore an efficient CBVR system by discriminately hashing videos into short binary codes. Existing video hashing methods usually encounter two weaknesses originating from the following sources: (1) Most works adopt the separated stages method or the frame-pooling based end-to-end architecture. However, the spatial-temporal properties of videos cannot be fully explored or kept well in the followup hashing step. (2) Discriminative learning based on pairwise or triplet constraints often suffers from slow convergence and poor local optimization, mainly because of the limited samples for each update. To alleviate these problems, we propose an end-to-end video retrieval framework called the Similarity-Preserving Deep Temporal Hashing (SPDTH) network. Specifically, we equip the model with the ability to capture spatialtemporal properties of videos and to generate binary codes by stacked Gated Recurrent Units (GRUs). It unifies video temporal modeling and learning to hash into one step to allow for maximum retention of information. We also introduce a deep metric learning objective called 2All_loss for network training by preserving intraclass similarity and inter-class separability, and a quantization loss between the real-valued outputs and the binary codes is minimized. Extensive experiments on several challenging datasets demonstrate that SPDTH can consistently outperform state-of-the-art methods. © 2019 Association for Computing Machinery.",Content-based video retrieval; Convolutional neural network-B.; Recurrent neural network; Video hashing,Binary codes; Deep learning; Content-based video retrieval; Convolutional neural network; Discriminative learning; Local optimizations; Similarity preserving; Spatial temporals; State-of-the-art methods; Video hashing; Recurrent neural networks
Dissecting the Performance of VR Video Streaming through the VR-EXP Experimentation Platform,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076993847&doi=10.1145%2f3360286&partnerID=40&md5=85837483df489f997254fba0ada53b83,"To cope with themassive bandwidth demands of Virtual Reality (VR) video streaming, both the scientific community and the industry have been proposing optimization techniques such as viewport-aware streaming and tile-based adaptive bitrate heuristics. As most of the VR video traffic is expected to be delivered through mobile networks, a major problem arises: both the network performance and VR video optimization techniques have the potential to influence the video playout performance and the Quality of Experience (QoE). However, the interplay between them is neither trivial nor has it been properly investigated. To bridge this gap, in this article, we introduce VR-EXP, an open-source platform for carrying out VR video streaming performance evaluation. Furthermore, we consolidate a set of relevant VR video streaming techniques and evaluate them under variable network conditions, contributing to an in-depth understanding of what to expect when different combinations are employed. To the best of our knowledge, this is the first work to propose a systematic approach, accompanied by a software toolkit, which allows one to compare different optimization techniques under the same circumstances. Extensive evaluations carried out using realistic datasets demonstrate that VR-EXP is instrumental in providing valuable insights regarding the interplay between network performance and VR video streaming optimization techniques. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Adaptive streaming; Quality of experience; Quality of service; Virtual reality,Mobile telecommunication systems; Network performance; Open source software; Optimization; Quality of service; Virtual reality; Adaptive streaming; Experimentation platforms; In-depth understanding; Open source platforms; Optimization techniques; Quality of experience (QoE); Scientific community; Software toolkits; Video streaming
HGAN: Holistic generative adversarial networks for two-dimensional image-based three-dimensional object retrieval,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077001921&doi=10.1145%2f3344684&partnerID=40&md5=6157cf5ffce549183d06143ab56214ac,"In this article, we propose a novel method to address the two-dimensional (2D) image-based 3D object retrieval problem. First, we extract a set of virtual views to represent each 3D object. Then, a soft-attention model is utilized to find the weight of each view to select one characteristic view for each 3D object. Second, we propose a novel Holistic Generative Adversarial Network (HGAN) to solve the cross-domain feature representation problem and make the feature space of virtual characteristic view more inclined to the feature space of the real picture. This will effectively mitigate the distribution discrepancies across the 2D image domains and 3D object domains. Finally, we utilize the generative model of the HGAN to obtain the ""virtual real image"" of each 3D object and make the characteristic view of the 3D object and real picture possess the same feature space for retrieval. To demonstrate the performance of our approach, We established a new dataset that includes pairs of 2D images and 3D objects, where the 3D objects are based on the ModelNet40 dataset. The experimental results demonstrate the superiority of our proposed method over the state-of-the-art methods. © 2019 Association for Computing Machinery.",3D retrieval; Characteristic view; Generative adversarial networks; Imagebased,Object recognition; 3D retrieval; Adversarial networks; Characteristic views; Image-based; State-of-the-art methods; Three-dimensional object; Two dimensional (2D) image; Two dimensional images; Computerized tomography
Effi cient image hashing with geometric invariant vector distance for copy detection,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076977968&doi=10.1145%2f3355394&partnerID=40&md5=39a022e2c56f3fdbb2e3d7e1f1532553,"Hashing method is an efficient technique of multimedia security for content protection. It maps an image into a content-based compact code for denoting the image itself. While most existing algorithms focus on improving the classification between robustness and discrimination, little attention has been paid to geometric invariance under normal digital operations, and therefore results in quite fragile to geometric distortion when applied in image copy detection. In this article, a novel effective image hashing method is proposed based on geometric invariant vector distance in both spatial domain and frequency domain. First, the image is preprocessed by some joint operations to extract robust features. Then, the preprocessed image is randomly divided into several overlapping blocks under a secret key, and two different feature matrices are separately obtained in the spatial domain and frequency domain through invariant moment and low frequency discrete cosine transform coefficients. Furthermore, the invariant distances between vectors in feature matrices are calculated and quantified to form a compact hash code.We conduct various experiments to demonstrate that the proposed hashing not only reaches good classification between robustness and discrimination, but also resists most geometric distortion in image copy detection. In addition, both receiver operating characteristics curve comparisons and mean average precision in copy detection clearly illustrate that the proposed hashing method outperforms state-of-the-art algorithms. © 2019 Association for Computing Machinery.",Copy detection; Discrimination; Geometric invariant vector distance; Image hashing; Robustness,Discrete cosine transforms; Facsimile; Frequency domain analysis; Geometry; Image coding; Robustness (control systems); Vectors; Copy detection; Discrete cosine transform coefficients; Discrimination; Geometric invariance; Geometric invariant; Image hashing; Receiver operating characteristics; State-of-the-art algorithms; Image enhancement
AB-LSTM: Attention-based bidirectional lstm model for scene text detection,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076961140&doi=10.1145%2f3356728&partnerID=40&md5=7094b2e37646153a3d6baedc8a9b2e9e,"Detection of scene text in arbitrary shapes is a challenging task in the field of computer vision. Most existing scene text detection methods exploit the rectangle/quadrangular bounding box to denote the detected text, which fails to accurately fit text with arbitrary shapes, such as curved text. In addition, recent progress on scene text detection has benefited from Fully Convolutional Network. Text cues contained in multi-level convolutional features are complementary for detecting scene text objects. How to explore these multi-level features is still an open problem. To tackle the above issues, we propose an Attention-based Bidirectional Long Short-Term Memory (AB-LSTM) model for scene text detection. First, word stroke regions (WSRs) and text center blocks (TCBs) are extracted by two AB-LSTM models, respectively. Then, the union of WSRs and TCBs are used to represent text objects. To verify the effectiveness of the proposed method, we perform experiments on four public benchmarks: CTW1500, Total-text, ICDAR2013, and MSRA-TD500, and compare it with existing state-of-the-art methods. Experiment results demonstrate that the proposedmethod can achieve competitive results, and well handle scene text objects with arbitrary shapes (i.e., curved, oriented, and horizontal forms). © 2019 Association for Computing Machinery.",Attention; B.; Bidirectional; Feature fusion; LSTM; Scene text detection; Semanticsegmentation,Boron; Character recognition; Convolution; Feature extraction; Horizontal wells; Object detection; Attention; Bidirectional; Feature fusion; LSTM; Scene Text; Semanticsegmentation; Long short-term memory
Adaptive chunklets and AQM for higher-performance content streaming,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077012237&doi=10.1145%2f3344381&partnerID=40&md5=64fbd84d67550893ed285f459ac9141c,"Commercial streaming services such as Netflix and YouTube use proprietary HTTP-based adaptive streaming (HAS) techniques to deliver content to consumers worldwide. MPEG recently developed Dynamic Adaptive Streaming over HTTP (DASH) as a unifying standard for HAS-based streaming. In DASH systems, streaming clients employ adaptive bitrate (ABR) algorithms tomaximise user Quality of Experience (QoE) under variable network conditions. In a typical Internet-enabled home, video streams have to compete with diverse application flows for the last-mile Internet Service Provider (ISP) bottleneck capacity. Under such circumstances,ABR algorithms will only act upon the fraction of the network capacity that is available, leading to possible QoE degradation. We have previously explored chunklets as an approach orthogonal to ABR algorithms, which uses parallel connections for intra-video chunk retrieval. Chunklets effectively make more bandwidth available for ABR algorithms in the presence of cross-traffic, especially in environments where Active Queue Management (AQM) schemes such as Proportional Integral controller Enhanced (PIE) and FlowQueue-Controlled Delay (FQ-CoDel) are deployed. However, chunklets consume valuable server/middlebox resources which typically handle hundreds of thousands of requests/connections per second. In this article, we propose 'adaptive chunklets'-a novel chunklet enhancement that dynamically tunes the number of concurrent connections. We demonstrate that the combination of adaptive chunklets and FQ-CoDel is the most effective strategy. Our experiments show that adaptive chunklets can reduce the number of connections by almost 30% and consume almost 8% less bandwidth than fixed chunklets while providing the same QoE. © 2019 Copyright held by the owner/author(s).",Adaptive bitrate algorithm; AQM; Chunklets; CoDel; Content streaming; DASH; FIFO; FQ-CoDel; HTTP; Latency; Packet loss; PIE; QoE; Queue management; TCP,Bandwidth; Controllers; Electric connectors; HTTP; Internet service providers; Packet loss; Queueing networks; Two term control systems; Bit rates; Chunklets; CoDel; DASH; FIFO; FQ-CoDel; Latency; Queue management; Quality of service
Tile-based adaptive streaming for virtual reality video,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077002023&doi=10.1145%2f3362101&partnerID=40&md5=8d6784edda4a8b9f8ac5ac13dac74e2e,"The increasing popularity of head-mounted devices and 360. video cameras allows content providers to provide virtual reality (VR) video streaming over the Internet, using a two-dimensional representation of the immersive content combined with traditional HTTP adaptive streaming (HAS) techniques. However, since only a limited part of the video (i.e., the viewport) is watched by the user, the available bandwidth is not optimally used. Recent studies have shown the benefits of adaptive tile-based video streaming; rather than sending the whole 360. video at once, the video is cut into temporal segments and spatial tiles, each of which can be requested at a different quality level. This allows prioritization of viewable video content and thus results in an increased bandwidth utilization. Given the early stages of research, there are still a number of open challenges to unlock the full potential of adaptive tile-based VR streaming. The aim of this work is to provide an answer to several of these open research questions. Among others, we propose two tile-based rate adaptation heuristics for equirectangular VR video, which use the great-circle distance between the viewport center and the center of each of the tiles to decide upon the most appropriate quality representation. We also introduce a feedback loop in the quality decision process, which allows the client to revise prior decisions based on more recent information on the viewport location. Furthermore,we investigate the benefits of parallel TCP connections and the use of HTTP/2 as an application layer optimization. Through an extensive evaluation,we showthat the proposed optimizations result in a significant improvement in terms of video quality (more than twice the time spent on the highest quality layer), compared to non-tiled HAS solutions. © 2019 Association for Computing Machinery.",HTTP adaptive streaming-E.; Tile-based rate adaptation; Virtual reality,Bandwidth; HTTP; Optimization; Video cameras; Virtual reality; Adaptive streaming; Application layers; Available bandwidth; Band-width utilization; Parallel TCP connections; Rate adaptation; Research questions; Temporal segments; Video streaming
Steganographer detection via multi-scale embedding probability estimation,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076981434&doi=10.1145%2f3352691&partnerID=40&md5=dddd1a2c28979088036f6dfcdfe12e9e,"Steganographer detection aims to identify the guilty user who utilizes steganographic methods to hide secret information in the spread of multimedia data, especially image data, from a large amount of innocent users on social networks. A true embedding probability map illustrates the probability distribution of embedding secret information in the corresponding images by specific steganographic methods and settings, which has been successfully used as the guidance for content-adaptive steganographic and steganalytic methods. Unfortunately, in real-world situation, the detailed steganographic settings adopted by the guilty user cannot be known in advance. It thus becomes necessary to propose an automatic embedding probability estimation method. In this article, we propose a novel content-adaptive steganographer detection method via embedding probability estimation. The embedding probability estimation is first formulated as a learning-based saliency detection problem and the multi-scale estimated map is then integrated into the CNN to extract steganalytic features. Finally, the guilty user is detected via an efficient Gaussian vote method with the extracted steganalytic features. The experimental results prove that the proposed method is superior to the state-of-the-art methods in both spatial and frequency domains. © 2019 Association for Computing Machinery.",Embedding probability estimation; Gaussian vote; Multimedia security; Steganalytic feature extraction; Steganographer detection,Embeddings; Probability distributions; Embedding probability; Gaussians; Multimedia security; Real world situations; Saliency detection; Secret information; Spatial and frequency domain; State-of-the-art methods; Feature extraction
An end-to-end attention-based neural model for complementary clothing matching,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076991734&doi=10.1145%2f3368071&partnerID=40&md5=840adc27e2ec6112d75eebb4033885c9,"In modern society, people tend to prefer fashionable and decent outfits that can meet more than basic physiological needs. In fact, a proper outfit usually relies on good matching among complementary fashion items (e.g., the top, bottom, and shoes) that compose it, which thus propels us to investigate the automatic complementary clothing matching scheme. However, this is non-trivial due to the following challenges. First, the main challenge lies in how to accurately model the compatibility between complementary fashion items (e.g., the top and bottom) that come from the heterogeneous spaces with multi-modalities (e.g., the visual modality and textual modality). Second, since different features (e.g., the color, style, and pattern) of fashion items may contribute differently to compatibility modeling, how to encode the confidence of different pairwise features presents a tough challenge. Third, how to jointly learn the latent representation of multi-modal data and the compatibility between complementary fashion items contributes to the last challenge. Toward this end, in this work, we present an end-to-end attention-based neural framework for the compatibility modeling, where we introduce a feature-level attention model to adaptively learn the confidence for different pairwise features. Extensive experiments on a public available real-world dataset show the superiority of our model over state-of-the-art methods. © 2019 Association for Computing Machinery.",Complementary clothing matching; End-to-end; Feature-level attention,Computer networks; Attention model; Complementary clothing matching; End to end; Feature level; Matching scheme; Multi-modal data; State-of-the-art methods; Visual modalities; Modal analysis
Embedding Distortion Analysis in Wavelet-domain Watermarking,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077013144&doi=10.1145%2f3357333&partnerID=40&md5=35fa9e68a2d0d2bb726a328f6f1fc935,"Imperceptibility and robustness are two complementary fundamental requirements of any watermarking algorithm. Low-strength watermarking yields high imperceptibility, but exhibits poor robustness. Highstrength watermarking schemes achieve good robustness but often infuse distortions resulting in poor visual quality in host images. This article analyses the embedding distortion for wavelet-based watermarking schemes. We derive the relationship between distortion, measured in mean square error (MSE), and the watermark embedding modification and propose the linear proportionality between MSE and the sum of energy of the selected wavelet coefficients for watermark embedding modification. The initial proposition assumes the orthonormality of discrete wavelet transform. It is further extended for non-orthonormal wavelet kernels using a weighting parameter that follows the energy conservation theorems in wavelet frames. The proposed analysis is verified by experimental results for both non-blind and blind watermarking schemes. Such a model is useful to find the optimum input parameters, including the wavelet kernel, coefficient selection, and subband choices for wavelet domain image watermarking. © 2019 Association for Computing Machinery.",Embedding distortion; MSE; Watermarking; Wavelet,Digital watermarking; Discrete wavelet transforms; Embeddings; Mean square error; Blind watermarking scheme; Conservation theorem; Embedding distortion; Orthonormal wavelets; Watermarking algorithms; Wavelet; Wavelet based watermarking; Wavelet coefficients; Image watermarking
An image cues coding approach for 3D human pose estimation,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076948598&doi=10.1145%2f3368066&partnerID=40&md5=548178c53fd38bc745314b354ea85e2a,"Although Deep Convolutional Neural Networks (DCNNs) facilitate the evolution of 3D human pose estimation, ambiguity remains the most challenging problem in such tasks. Inspired by the Human Perception Mechanism (HPM), we propose an image-to-pose coding method to fill the gap between image cues and 3D poses, thereby alleviating the ambiguity of 3D human pose estimation. First, in 3D pose space, we divide the whole 3D pose space into multiple subregions named pose codes, turning a disambiguation problem into a classification problem. The proposed coding mechanism covers multiple camera views and provides a complete description for 3D pose space. Second, it is noteworthy that the articulated structure of the human body lies on a sophisticated product manifold and the error accumulation in the chain structure will undoubtedly affect the coding performance. Therefore, in image space, we extract the image cues from independent local image patches rather than the whole image. The mapping relationship between image cues and 3D pose codes is established by a set of DCNNs. The image-to-pose coding method transforms the implicit image cues into explicit constraints. Finally, the image-to-pose coding method is integrated into a linear matching mechanism to construct a 3D pose estimation method that effectively alleviates the ambiguity. We conduct extensive experiments on widely used public benchmarks. The experimental results show that our method effectively alleviates the ambiguity in 3D pose recovery and is robust to the variations of view. © 2019 Association for Computing Machinery.",Ambiguity in 3D pose recovery; Code of 3D pose; Human perception mechanism; Image cues; Matching mechanism,Deep neural networks; Neural networks; Code of 3D pose; Human perception; Image cues; Matching mechanisms; Pose recovery; Image coding
Unsupervised learning of human action categories in still images with deep representations,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077004549&doi=10.1145%2f3362161&partnerID=40&md5=26f68fd515a276832d8d74dfce52fc59,"In this article, we propose a novel method for unsupervised learning of human action categories in still images. In contrast to previous methods, the proposed method explores distinctive information of actions directly from unlabeled image databases, attempting to learn discriminative deep representations in an unsupervised manner to distinguish different actions. In the proposed method, action image collections can be used without manual annotations. Specifically, (i) to deal with the problem that unsupervised discriminative deep representations are difficult to learn, the proposed method builds a training dataset with surrogate labels from the unlabeled dataset, then learns discriminative representations by alternately updating convolutional neural network (CNN) parameters and the surrogate training dataset in an iterative manner; (ii) to explore the discriminatory information among different action categories, training batches for updating the CNN parameters are built with triplet groups and the triplet loss function is introduced to update the CNN parameters; and (iii) to learn more discriminative deep representations, a Random Forest classifier is adopted to update the surrogate training dataset, and more beneficial triplet groups then can be built with the updated surrogate training dataset. Extensive experiments on four benchmark datasets demonstrate the effectiveness of the proposed method. © 2019 Association for Computing Machinery.",Action categorization; Deep representations; Unsupervised learning,Decision trees; Deep learning; Iterative methods; Machine learning; Neural networks; Unsupervised learning; Action categorization; Benchmark datasets; Convolutional neural network; Deep representations; Image collections; Manual annotation; Random forest classifier; Training dataset; Classification (of information)
Random playlists smoothly commuting between styles,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076981651&doi=10.1145%2f3361742&partnerID=40&md5=e043c4e03980688dd859cc03e457a08c,"Someone enjoys listening to playlists while commuting. He wants a different playlist of n songs each day, but always starting from Locked Out of Heaven, a Bruno Mars song. The list should progress in smooth transitions between successive and randomly selected songs until it ends up at Stairway to Heaven, a Led Zeppelin song. The challenge of automatically generating random and heterogeneous playlists is to find the appropriate balance among several conflicting goals. We propose two methods for solving this problem. One is called ROPE, and it depends on a representation of the songs in a Euclidean space. It generates a random path through a Brownian Bridge that connects any two songs selected by the user in this music space. The second is STRAW, which constructs a graph representation of the music space where the nodes are songs and edges connect similar songs. STRAW creates a playlist by traversing the graph through a steering random walk that starts on a selected song and is directed toward a target song also selected by the user. When compared with the state-of-the-art algorithms, our algorithms are the only ones that satisfy the following quality constraints: heterogeneity, smooth transitions, novelty, scalability, and usability. We demonstrate the usefulness of our proposed algorithms by applying them to a large collection of songs and make available a prototype. © 2019 Association for Computing Machinery.",Knowledge and data engineering tools and techniques; Music; Sound and music computing; System applications and experience,Computer networks; Graph representation; Knowledge and data engineering tools and techniques; Music; Quality constraints; Smooth transitions; Sound and music computing; State-of-the-art algorithms; System applications; Directed graphs
Efficient face alignment with fast normalization and contour fitting loss,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075623973&doi=10.1145%2f3338842&partnerID=40&md5=1c67278d90414f4c031855f8ea0cba80,"Face alignment is a key component of numerous face analysis tasks. In recent years, most existing methods have focused on designing high-performance face alignment systems and paid less attention to efficiency. However more face alignment systems are now applied on low-cost devices, such as mobile phones. In this article, we design a common efficient framework that can team with any face alignment regression network and improve the overall performance with nearly no extra computational cost. First, we discover that the maximum regression error exists in the face contour, where landmarks do not have distinct semantic positions, and thus are randomly labeled along the face contours in training data. To address this problem, we propose a novel contour fitting loss that dynamically adjusts the regression target during training so the network can learn more accurate semantic meanings of the contour landmarks and achieve better localization performance. Second, we decouple the complex sample variations in face alignment task and propose a Fast Normalization Module (FNM) to efficiently normalize considerable variations that can be described by geometric transformation. Finally, a new lightweight network architecture named Lightweight Alignment Module (LAM) is also proposed to achieve fast and precise face alignment on mobile devices. Our method achieves competitive performance with state-of-the-arts on 300W and AFLW2000-3D benchmarks. Meanwhile, the speed of our framework is significantly faster than other CNN-based approaches. © 2019 Association for Computing Machinery.",Convolutional neural networks; Face alignment; Real-time; Semantic meaning,Benchmarking; Global system for mobile communications; Mathematical transformations; Network architecture; Neural networks; Regression analysis; Semantics; Competitive performance; Computational costs; Convolutional neural network; Face alignment; Geometric transformations; Localization performance; Real time; Regression errors; Alignment
Introduction to the special issue on affective computing for large-scale heterogeneous multimedia data,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075594624&doi=10.1145%2f3365845&partnerID=40&md5=0e96ea6b24d3903375960634263db25c,[No abstract available],,
Autonomous semantic community detection via adaptively weighted low-rank approximation,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075574576&doi=10.1145%2f3355393&partnerID=40&md5=bc202c474ec6c1bec62f82a312475264,"Identification of semantic community structures is important for understanding the interactions and sentiments of different groups of people and predicting the social emotion. A robust community detection method needs to autonomously determine the number of communities and community structure for a given network. Nonnegative matrix factorization (NMF), a component decomposition approach for latent sentiment discovery, has been extensively used for community detection. However, the existing NMF-based methods require the number of communities to be determined a priori, limiting their applicability in practice of affective computing. Here, we develop a novel NMF-based method to autonomously determine the number of semantic communities and community structure simultaneously. In our method, we use an initial number of semantic communities, larger than the actual number, in the NMF formulation, and then suppress some of the communities by introducing an adaptively weighted group-sparse low-rank regularization to derive the target number of communities and at the same time the corresponding community structure. Our method not only maintains the efficiency without increasing the complexity compared to the original NMF method but also can be straightforwardly extended to handle the non-network data. We thoroughly examine the new method, showing its superior performance over several competing methods on synthetic and large real-world social networks. Copyright © 2019 held by the owner/author(s).",Community detection; Low-rank approximation; Nonnegative matrix factorization; Rank determination,Approximation theory; Factorization; Population dynamics; Semantics; Affective Computing; Community detection; Community structures; Component decomposition; Low rank approximations; Nonnegative matrix factorization; Rank determination; Weighted low rank approximations; Matrix algebra
Introduction to the special issue on face analysis applications,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073759282&doi=10.1145%2f3359624&partnerID=40&md5=7d6b0834655ffd2871a56a1541e70c15,[No abstract available],,
A unified tensor-based active appearance model,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073741493&doi=10.1145%2f3338841&partnerID=40&md5=49253b05e2c0a2232bc952db45772988,"Appearance variations result in many difficulties in face image analysis. To deal with this challenge, we present a Unified Tensor-based Active Appearance Model (UT-AAM) for jointly modelling the geometry and texture information of 2D faces. For each type of face information, namely shape and texture, we construct a unified tensor model capturing all relevant appearance variations. This contrasts with the variation-specific models of the classical tensor AAM. To achieve the unification across pose variations, a strategy for dealing with self-occluded faces is proposed to obtain consistent shape and texture representations of pose-varied faces. In addition, our UT-AAM is capable of constructing the model from an incomplete training dataset, using tensor completion methods. Last, we use an effective cascaded-regression-based method for UT-AAM fitting. With these advancements, the utility of UT-AAM in practice is considerably enhanced. As an example, we demonstrate the improvements in training facial landmark detectors through the use of UT-AAM to synthesise a large number of virtual samples. Experimental results obtained on a number of well-known face datasets demonstrate the merits of the proposed approach. © 2019 Association for Computing Machinery.",Active appearance model; Cascaded regression; Face image analysis; Missing training samples; Tensor algebra,Image analysis; Image recognition; Textures; Active appearance models; Cascaded regression; Face image analysis; Tensor algebra; Training sample; Tensors
Visual attention analysis and prediction on human faces for children with autism spectrum disorder,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073730912&doi=10.1145%2f3337066&partnerID=40&md5=789ad86f2a24d4d405c45aee2abee858,"The focus of this article is to analyze and predict the visual attention of children with Autism Spectrum Disorder (ASD) when looking at human faces. Social difficulties are the hallmark features of ASD and will lead to atypical visual attention toward various stimuli more or less, especially on human faces. Learning the visual attention of childrenwith ASD could contribute to related research in the field ofmedical science, psychology, and education.We first construct a Visual Attention on Faces for Autism Spectrum Disorder (VAFA) database, which consists of 300 natural scene images with human faces and corresponding eye movement data collected from 13 children with ASD. Compared with matched typically developing (TD) controls, we quantify atypical visual attention on human faces in ASD. Statistics show that some high-level factors such as face size, facial features, face pose, and facial emotions have different impacts on the visual attention of children with ASD. Combining the feature maps extracted from the state-of-the-art saliency models, we get the visual attention model on human faces for individuals with ASD. The proposed model shows the best performance among all competitors. With the help of our proposed model, researchers in related fields could design specialized education contents containing human faces for the children with ASD or produce the specific model for rapidly screening ASD using their eye movement data. © 2019 Association for Computing Machinery.",Autism spectrum disorder (ASD); Human faces; Saliency prediction; Visual attention,Diseases; Eye movements; Forecasting; Autism spectrum disorders; Children with autisms; Eye movement datum; High-level factors; Human faces; Natural scene images; Visual Attention; Visual attention model; Behavioral research
Synthesizing facial photometries and corresponding geometries using generative adversarial networks,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073746677&doi=10.1145%2f3337067&partnerID=40&md5=c4d361b79996809ba245f74f2373173b,"Artificial data synthesis is currently a well-studied topic with useful applications in data science, computer vision, graphics, and many other fields. Generating realistic data is especially challenging, since human perception is highly sensitive to non-realistic appearance. In recent times, new levels of realism have been achieved by advances in GAN training procedures and architectures. These successful models, however, are tuned mostly for use with regularly sampled data such as images, audio, and video. Despite the successful application of the architecture on these types of media, applying the same tools to geometric data poses a far greater challenge. The study of geometric deep learning is still a debated issue within the academic community, as the lack of intrinsic parametrization inherent to geometric objects prohibits the direct use of convolutional filters, a main building block of today's machine learning systems. In this article, we propose a new method for generating realistic human facial geometries coupled with overlayed textures. We circumvent the parametrization issue by utilizing a specialized non-rigid alignment procedure, and imposing a global mapping from our data to the unit rectangle. This mapping enables the representation of our geometric data as regularly sampled 2D images.We further discuss how to design such a mapping to control the distortion and conserve area within the target image. By representing geometric textures and geometries as images,we are able to use advanced GANmethodologies to generate newplausible textures and geometries.We address the often-neglected topic of relationship between texture and geometry and propose different methods for fitting generated geometries to generated textures. In addition, we widen the scope of our discussion and offer a new method for training GAN models on partially corrupted data. Finally, we provide empirical evidence demonstrating our generative model's ability to produce examples of new facial identities, independent from the training data, while maintaining a high level of realism-two traits that are often at odds. © 2019 Association for Computing Machinery.",3D face synthesis; 3D morphable model; Data augmentation; Face recognition; Facial texture and geometry; Generative adversarial networks,3D modeling; Deep learning; Face recognition; Mapping; Network architecture; Textures; 3-D face synthesis; 3D Morphable model; Adversarial networks; Data augmentation; Facial textures; Geometry
U-Net conditional GANs for photo-realistic and identity-preserving facial expression synthesis,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073713718&doi=10.1145%2f3355397&partnerID=40&md5=88d38ef91bd90e68ce71a831ffbf0e9f,"Facial expression synthesis (FES) is a challenging task since the expression changes are highly non-linear and depend on the facial appearance. Person identity should also be well preserved in the synthesized face. In this article, we present a novel U-Net Conditional Generative Adversarial Network for FES. U-Net helps retain the property of the input face, including the identity information and facial details. Category condition is added to the U-Net model so that one-to-many expression synthesis can be achieved simultaneously.We also design constraints for identity preservation during FES to further guarantee that the identity of the input face can be well preserved in the generated face image. Specifically, we pair the generated output with condition image of other identities for the discriminator, so as to encourage it to learn the distinctions between the synthesized and natural images, as well as between input and other identities, which can help improve its discriminating ability. Additionally, we utilize the triplet loss to maintain the generated face images closer to the same identity person by imposing a margin between the positive pairs and negative pairs in feature space. Both qualitative and quantitative evaluations are conducted on the Oulu-CASIA NIR&VIS facial expression database, the Radboud Faces Database, and the Karolinska Directed Emotional Faces database, and the experimental results showthat our method can generate faces with natural and realistic expressionswhile preserving identity information. © 2019 Association for Computing Machinery.",Facial expression synthesis; Generative adversarial networks (GANs); Identity preserving,Database systems; Adversarial networks; Discriminating abilities; Expression synthesis; Facial expression synthesis; Facial Expressions; Identity information; Identity preserving; Quantitative evaluation; Image enhancement
Features-enhanced multi-attribute estimation with convolutional tensor correlation fusion network,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073757067&doi=10.1145%2f3355542&partnerID=40&md5=44178a09286765501a9da7b74be6d853,"To achieve robust facial attribute estimation, a hierarchical prediction system referred to as tensor correlation fusion network (TCFN) is proposed for attribute estimation. The system includes feature extraction, correlation excavation among facial attribute features, score fusion, and multi-attribute prediction. Subnetworks (Age-Net, Gender-Net, Race-Net, and Smile-Net) are used to extract corresponding features while Main-Net extracts features not only from an input image but also from corresponding pooling layers of subnetworks. Dynamic tensor canonical correlation analysis (DTCCA) is proposed to explore the correlation of different targets' features in the F7 layers. Then, for binary classifications of gender, race, and smile, corresponding robust decisions are achieved by fusing the results of subnetworks with those of TCFN while for age prediction, facial image into one of age groups, and then ELM regressor performs the final age estimation. Experimental results on benchmarks with multiple face attributes (MORPH-II, Adience Benchmark datasets, LAP-2016, and CelebA) show that the proposed approach has superior performance compared to state of the art. © 2019 Association for Computing Machinery.",Attribute estimation; Correlation; Score fusion; Subnetwork,Benchmarking; Correlation methods; Forecasting; Benchmark datasets; Binary classification; Canonical correlation analysis; Multi-attributes; Prediction systems; Score fusion; State of the art; Sub-network; Tensors
Paillier cryptosystem based mean value computation for encrypted domain image processing operations,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075565072&doi=10.1145%2f3325194&partnerID=40&md5=5b92df21343edfcdf347e7c1cb3c0c8f,"Due to its large storage facility and high-end computing capability, cloud computing has received great attention as a huge amount of personal multimedia data and computationally expensive tasks can be outsourced to the cloud. However, the cloud being third-party semi-Trusted, is prone to information leakage, raising privacy risks. Signal processing in the encrypted domain has emerged as a new research paradigm on privacypreserving processing over outsourced data by semi-Trusted cloud. In this article, we propose a solution for non-integer mean value computation in the homomorphic encrypted domain without any interactive protocol between the client and the service provider. Using the proposed solution, various image processing operations, such as local smoothing filter, un-sharp masking, and histogram equalization, can be performed in the encrypted domain at the cloud server without any privacy concerns. Our experimental results from standard test images reveal that these image processing operations can be performed without pre-processing, without client-server interactive protocol, and without any error between the encrypted domain and the plain domain. © 2019 Association for Computing Machinery. © 2019 Association for Computing Machinery.",Cloud computing; Encrypted domain; Paillier cryptosystem; Secure signal processing (SSP),Cloud computing; Cryptography; Digital storage; Trusted computing; Encrypted domain; Histogram equalizations; Information leakage; Interactive protocols; Paillier cryptosystem; Personal multimedias; Secure signal processing (SSP); Signal processing in the encrypted domains; Image processing
Stochastic optimization for green multimedia services in dense 5G networks,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075567890&doi=10.1145%2f3328996&partnerID=40&md5=142134c8b90496839e89f9eb337d079e,"The manyfold capacity magnification promised by dense 5G networks will make possible the provisioning of broadband multimedia services, including virtual reality, augmented reality, and mobile immersive video, to name a few. These new applications will coexist with classic ones and contribute to the exponential growth of multimedia services in mobile networks. At the same time, the different requirements of past and old services pose new challenges to the effective usage of 5G resources. In response to these challenges, a novel Stochastic Optimization framework for Green Multimedia Services named SOGMS is proposed herein that targets the maximization of system throughput and the minimization of energy consumption in data delivery. In particular, Lyapunov optimization is leveraged to face this optimization objective, which is formulated and decomposed into three tractable subproblems. For each subproblem, a distinct algorithm is conceived, namely quality of experience based admission control, cooperative resource allocation, and multimedia services scheduling. Finally, extensive simulations are carried out to evaluate the proposedmethod against stateof-art solutions in dense 5G networks. © 2019 Association for Computing Machinery.",Admission control; Dense 5G networks; Green multimedia; Resource allocation; Stochastic optimizing,Access control; Augmented reality; Energy utilization; Multimedia services; Multimedia systems; Optimization; Quality control; Quality of service; Queueing networks; Resource allocation; Stochastic systems; Virtual reality; Broadband multimedia services; Exponential growth; Extensive simulations; G-networks; Green multimedia; Quality of experience (QoE); Stochastic optimizations; Stochastic optimizing; 5G mobile communication systems
Rethinking the combined and individual orders of derivative of states for differential recurrent neural networks: Deep differential recurrent neural networks,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075564184&doi=10.1145%2f3337928&partnerID=40&md5=f7f2074869115385750954428d6cf54e,"Due to their special gating schemes, Long Short-Term Memory (LSTM) has shown greater potential to process complex sequential information than the traditional Recurrent Neural Network (RNN). The conventional LSTM, however, fails to take into consideration the impact of salient spatio-Temporal dynamics present in the sequential input data. This problem was first addressed by the differential Recurrent Neural Network (dRNN), which uses a differential gating scheme known as Derivative of States (DoS). DoS uses higher orders of internal state derivatives to analyze the change in information gain originated from the salient motions between the successive frames. The weighted combination of several orders of DoS is then used to modulate the gates in dRNN. While each individual order of DoS is good at modeling a certain level of salient spatio-Temporal sequences, the sum of all the orders of DoS could distort the detected motion patterns. To address this problem, we propose to control the LSTM gates via individual orders of DoS. To fully utilize the different orders of DoS, we further propose to stack multiple levels of LSTM cells in an increasing order of state derivatives. The proposed model progressively builds up the ability of the LSTM gates to detect salient dynamical patterns in deeper stacked layers modeling higher orders of DoS; thus, the proposed LSTM model is termed deep differential Recurrent Neural Network (d2RNN). The effectiveness of the proposed model is demonstrated on three publicly available human activity datasets: NUS-HGA, Violent-Flows, and UCF101. The proposed model outperforms both LSTM and non-LSTM based state-of-The-Art algorithms. © 2019 Association for Computing Machinery.",,Deep neural networks; Multilayer neural networks; Dynamical pattern; Human activities; Information gain; Recurrent neural network (RNN); Sequential information; Spatio temporal; Spatio-temporal dynamics; State-of-the-art algorithms; Long short-term memory
A framework for adaptive residual streaming for single-player cloud gaming,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071107024&doi=10.1145%2f3336498&partnerID=40&md5=5bd9e0ead4b290375b87895f0b505661,"Applying cloud technology to 3D interactive multimedia applications is a promising way to provide flexible and cost-efficient online high-bandwidth immersive services to a large population of end users. One main reason cloud systems are popular among users is the fact that it relaxes the hardware requirements for highend interactive visual applications. As most of the computational tasks are done on cloud servers, users no longer need to upgrade their hardware as frequently to keep up with the ever-increasing high-end computing requirements of the latest applications. Moreover, cloud systems make it easier for a user to enjoy applications on different platforms, including mobile devices that are usually not powerful enough to run high-end, memory-intensive services. In short, applying cloud technology to high-end immersive applications has advantages in cost-efficiency and flexibility both for the end users and the service providers. However, there are two main drawbacks to applying cloud technology to 3D interactive multimedia services: (1) high-bandwidth utilization and (2) latency. In this article, we propose a framework that addresses the two problems for singleplayer cloud gaming by using a combination of collaborative rendering, progressive meshes, and 3D image warping techniques. The experimental results show that the proposed system can reduce the bandwidth usage and improve the visual quality by utilizing local computing power on the client. The results also show that the interaction latency can be reduced somewhat by sacrificing some degree of visual quality in the end system. © 2019 Association for Computing Machinery.",3D image warping; Collaborative rendering; Progressive meshes,Bandwidth; Computer hardware; Interactive computer systems; Multimedia systems; Rendering (computer graphics); Three dimensional computer graphics; 3d image warping; Collaborative rendering; Computational task; High end computing; Immersive application; Interactive multimedia; Progressive Mesh; Visual applications; Multimedia services
Introduction to the best papers of the ACM multimedia systems (MMSys) conference 2018 and the ACM workshop on network and operating system support for digital audio and video (NOSSDAV) 2018 and the International Workshop on Mixed and Virtual Environment Systems (MMVE) 2018,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071090741&doi=10.1145%2f3339846&partnerID=40&md5=076d2b985012058bcd85e192ffebf189,[No abstract available],,
A simplistic global median filtering forensics based on frequency domain analysis of image residuals,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071927496&doi=10.1145%2f3321508&partnerID=40&md5=d689e4c96825edd3012f3b72d2490cf9,"Sophisticated image forgeries introduce digital image forensics as an active area of research. In this area, many researchers have addressed the problem of median filtering forensics. Existing median filtering detectors are adequate to classify median filtered images in uncompressed mode and in compressed mode at high-quality factors. Despite that, the field is lacking a robust method to detect median filtering in low-resolution images compressed with low-quality factors. In this article, a novel feature set (four feature dimensions), based on first-order statistics of frequency contents of median filtered residuals (MFRs) of original and median filtered images, has been proposed. The proposed feature set outperforms handcrafted features-based state-of-the-art detectors in terms of feature set dimensions and detection results obtained for low-resolution images at all quality factors. Also, results reveal the efficacy of proposed method over deep-learning-based median filtering detector. Comprehensive results expose the efficacy of the proposed detector to detect median filtering against other similar manipulations. Additionally, generalization ability test on cross-database images support the cross-validation results on four different databases. Thus, our proposed detector meets the current challenges in the field, to a great extent. © 2019 Association for Computing Machinery.",Global DCT coefficients; Image forensics; Median filter; Median filtered residual; Spatial frequency,Deep learning; Digital forensics; Frequency domain analysis; DCT coefficients; Digital image forensics; First-order statistics; Generalization ability; Image forensics; Low resolution images; Median filtered residual; Spatial frequency; Median filters
Multi-source multi-level attention networks for visual question answering,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071091166&doi=10.1145%2f3316767&partnerID=40&md5=e3487dc48c09e1661b661f98d152d022,"In recent years, Visual Question Answering (VQA) has attracted increasing attention due to its requirement on cross-modal understanding and reasoning of vision and language. VQA is proposed to automatically answer natural language questions with reference to a given image. VQA is challenging, because the reasoning process on a visual domain needs a full understanding of the spatial relationship, semantic concepts, as well as the common sense for a real image. However, most existing approaches jointly embed the abstract lowlevel visual features and high-level question features to infer answers. These works have limited reasoning ability due to the lack of modeling of the rich spatial context of regions, high-level semantics of images, and knowledge across multiple sources. To solve the challenges, we propose multi-source multi-level attention networks for visual question answering that can benefit both spatial inferences by visual attention on contextaware region representation and reasoning by semantic attention on concepts as well as external knowledge. Indeed, we learn to reason on image representation by question-guided attention at different levels across multiple sources, including region and concept level representation from image source as well as sentence level representation from the external knowledge base. First, we encode region-based middle-level outputs from Convolutional Neural Networks (CNNs) into spatially embedded representation by a multi-directional two-dimensional recurrent neural network and, further, locate the answer-related regions by Multiple Layer Perceptron as visual attention. Second, we generate semantic concepts from high-level semantics in CNNs and select those question-related concepts as concept attention. Third, we query semantic knowledge from the general knowledge base by concepts and selected question-related knowledge as knowledge attention. Finally, we jointly optimize visual attention, concept attention, knowledge attention, and question embedding by a softmax classifier to infer the final answer. Extensive experiments show the proposed approach achieved significant improvement on two very challenging VQA datasets. © 2019 Association for Computing Machinery.",Attentionmodel; Multi-modal representations; Visual question answering; Visual relationship,Behavioral research; Knowledge based systems; Multilayer neural networks; Natural language processing systems; Recurrent neural networks; Semantics; Attentionmodel; Convolutional neural network; Embedded representation; Multi-modal; Multiple layer perceptron; Natural language questions; Question Answering; Visual relationship; Visual languages
Spatiotemporal-textual co-attention network for video question answering,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071094794&doi=10.1145%2f3320061&partnerID=40&md5=5a36388d6f7449eb7291d2974980167e,"Visual Question Answering (VQA) is to provide a natural language answer for a pair of an image or video and a natural language question. Despite recent progress on VQA, existingworks primarily focus on image question answering and are suboptimal for video question answering. This article presents a novel Spatiotemporal- Textual Co-Attention Network (STCA-Net) for video question answering. The STCA-Net jointly learns spatially and temporally visual attention on videos as well as textual attention on questions. It concentrates on the essential cues in both visual and textual spaces for answering question, leading to effective question-video representation. In particular, a question-guided attention network is designed to learn question-aware video representation with a spatial-temporal attention module. It concentrates the network on regions of interest within the frames of interest across the entire video. A video-guided attention network is proposed to learn video-aware question representation with a textual attention module, leading to fine-grained understanding of question. The learned video and question representations are used by an answer predictor to generate answers. Extensive experiments on two challenging datasets of video question answering, i.e., MSVD-QA and MSRVTT-QA, have shown the effectiveness of the proposed approach. © 2019 Association for Computing Machinery.",Attention mechanism; Video question answering,Behavioral research; Video recording; Visual languages; Attention mechanisms; Natural language questions; Natural languages; Question Answering; Regions of interest; Spatial temporals; Video representations; Visual Attention; Natural language processing systems
"Introduction to the special issue on big data, machine learning, and AI technologies for art and design",2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071113685&doi=10.1145%2f3338002&partnerID=40&md5=c26ed82bdfd0d196412a3ee15ba5a74f,[No abstract available],,
Harvesting visual objects from internet images via deep-learning-based objectness assessment,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071938396&doi=10.1145%2f3318463&partnerID=40&md5=a840ea8fdbb649d87466b68149928e5c,"The collection of internet images has been growing in an astonishing speed. It is undoubted that these images contain rich visual information that can be useful in many applications, such as visual media creation and data-driven image synthesis. In this article, we focus on the methodologies for building a visual object database from a collection of internet images. Such database is built to contain a large number of high-quality visual objects that can help with various data-driven image applications. Our method is based on dense proposal generation and objectness-based re-ranking. A novel deep convolutional neural network is designed for the inference of proposal objectness, the probability of a proposal containing optimally located foreground object. In our work, the objectness is quantitatively measured in regard of completeness and fullness, reflecting two complementary features of an optimal proposal: a complete foreground and relatively small background. Our experiments indicate that object proposals re-ranked according to the output of our network generally achieve higher performance than those produced by other state-of-the-art methods. As a concrete example, a database of over 1.2 million visual objects has been built using the proposed method, and has been successfully used in various data-driven image applications. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Convolutional neural networks; Internet images; Object detection; Object proposals; Objectness,Convolution; Neural networks; Object detection; Object-oriented databases; Complementary features; Convolutional neural network; Foreground objects; Image applications; Internet images; Object proposals; Objectness; State-of-the-art methods; Deep neural networks
Deep scalable supervised quantization by self-organizing map,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071937196&doi=10.1145%2f3328995&partnerID=40&md5=d947e52e7ff9638432144e6b18aac349,"Approximate Nearest Neighbor (ANN) search is an important research topic in multimedia and computer vision fields. In this article, we propose a new deep supervised quantization method by Self-Organizing Map to address this problem. Our method integrates the Convolutional Neural Networks and Self-Organizing Map into a unified deep architecture. The overall training objective optimizes supervised quantization loss as well as classification loss. With the supervised quantization objective, we minimize the differences on the maps between similar image pairs and maximize the differences on the maps between dissimilar image pairs. By optimization, the deep architecture can simultaneously extract deep features and quantize the features into suitable nodes in self-organizing map. To make the proposed deep supervised quantization method scalable for large datasets, instead of constructing a larger self-organizing map, we propose to divide the input space into several subspaces and construct self-organizing map in each subspace. The self-organizing maps in all the subspaces implicitly construct a large self-organizing map, which costs less memory and training time than directly constructing a self-organizing map with equal size. The experiments on several public standard datasets prove the superiority of our approaches over the existing ANN search methods. Besides, as a byproduct, our deep architecture can be directly applied to visualization with little modification, and promising performance is demonstrated in the experiments. © 2019 Association for Computing Machinery.",Approximate nearest neighbor search; Self-organizing map; Supervised quantization,Conformal mapping; Large dataset; Nearest neighbor search; Network architecture; Approximate nearest neighbors (ANN); Convolutional neural network; Deep architectures; Large datasets; Quantization loss; Research topics; Similar image; Supervised quantization; Self organizing maps
Learning click-based deep structure-preserving embeddings with visual attention,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071946239&doi=10.1145%2f3328994&partnerID=40&md5=dc967673d102c672bc475a79dafc9f6e,"One fundamental problem in image search is to learn the ranking functions (i.e., the similarity between query and image). Recent progress on this topic has evolved through two paradigms: the text-based model and image ranker learning. The former relies on image surrounding texts, making the similarity sensitive to the quality of textual descriptions. The latter may suffer from the robustness problem when human-labeled query-image pairs cannot represent user search intent precisely. We demonstrate in this article that the preceding two limitations can be well mitigated by learning a cross-view embedding that leverages click data. Specifically, a novel click-based Deep Structure-Preserving Embeddings with visual Attention (DSPEA) model is presented, which consists of two components: deep convolutional neural networks followed by image embedding layers for learning visual embedding, and a deep neural networks for generating query semantic embedding. Meanwhile, visual attention is incorporated at the top of the convolutional neural network to reflect the relevant regions of the image to the query. Furthermore, considering the high dimension of the query space, a new click-based representation on a query set is proposed for alleviating this sparsity problem. The whole network is end-to-end trained by optimizing a large margin objective that combines cross-view ranking constraints with in-view neighborhood structure preservation constraints. On a large-scale click-based image dataset with 11.7 million queries and 1 million images, our model is shown to be powerful for keyword-based image search with superior performance over several state-of-the-art methods and achieves, to date, the best reported NDCG@25 of 52.21%. © 2019 Association for Computing Machinery.",Click data; CNN; Cross-view embedding; Image search,Behavioral research; Convolution; Embeddings; Large dataset; Multilayer neural networks; Semantics; Click data; Convolutional neural network; Cross-view embedding; Image search; Neighborhood structure; Sparsity problems; State-of-the-art methods; Textual description; Deep neural networks
Subtitle region selection of S3D images in consideration of visual discomfort and viewing habit,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071931667&doi=10.1145%2f3325197&partnerID=40&md5=a6480a9e7c1366ad7ae4935b85cac6f1,"Subtitles, serving as a linguistic approximation of the visual content, are an essential element in stereoscopic advertisement and the film industry. Due to the vergence accommodation conflict, the stereoscopic 3D (S3D) subtitle inevitably causes visual discomfort. To meet the viewing experience, the subtitle region should be carefully arranged. Unfortunately, very few works have been dedicated to this area. In this article, we propose a method for S3D subtitle region selection in consideration of visual discomfort and viewing habit. First, we divide the disparity map into multiple depth layers according to the disparity value. The preferential processed depth layer is determined by considering the disparity value of the foremost object. Second, the optimal region and coarse disparity value for S3D subtitle insertion are chosen by convolving the selective depth layer with the mean filter. Specifically, the viewing habit is considered during the region selection. Finally, after region selection, the disparity value of the subtitle is further modified by using the just noticeable depth difference (JNDD) model. Given that there is no public database reported for the evaluation of S3D subtitle insertion, we collect 120 S3D images as the test platform. Both objective and subjective experiments are conducted to evaluate the comfort degree of the inserted subtitle. Experimental results demonstrate that the proposed method can obtain promising performance in improving the viewing experience of the inserted subtitle. © 2019 Association for Computing Machinery.",Evaluation; Horizontal disparity; Just noticeable depth difference (JNDD); Stereoscopic 3D; Subtitle; Visual discomfort,Computer networks; Evaluation; Horizontal disparity; Just noticeable depth difference (JNDD); Subtitle; Visual discomfort; Stereo image processing
Chunk duration-aware SDN-assisted DASH,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071925825&doi=10.1145%2f3337681&partnerID=40&md5=af60504e461aab4cce1262380e339b51,"Although Dynamic Adaptive Streaming over HTTP (DASH) is the pillar of multimedia content delivery mechanisms, its purely client-based adaptive video bitrate mechanisms have quality-of-experience fairness and stability problems in the existence of multiple DASH clients and highly fluctuating background traffic on the same shared bottleneck link. Varying chunk duration among different titles of multiple video providers exacerbates this problem. With the help of the global network view provided by the software-defined networking paradigm, we propose a centralized joint optimization module-assisted adaptive video bitrate mechanism that takes diversity of chunk sizes among different content into account. Our system collects possible video bitrate levels and chunk duration from DASH clients and simply calculates the optimal video bitrates per client based on the available capacity and chunk duration of each client's selected content while not invading users' privacy. By continuously following the background traffic flows, it asynchronously updates the target video bitrate levels to avoid both buffer stall events and network underutilization issues rather than bandwidth slicing, which brings about scalability problems in practice. It also guarantees fair startup delays for video sessions with various chunk duration. Our experiments clearly show that our proposed approach considering diversity of chunk duration and that background traffic fluctuations can significantly provide a better and fair quality of experience in terms of structural similarity-based video quality and startup delay compared to both purely client-based and state-of-the-art software-defined networking-based adaptive bitrate mechanisms. © 2019 Association for Computing Machinery.",Central adaptive video bitrate optimization; Chunk duration diversity; Dynamic adaptive streaming HTTP; QoE; SDN,Dynamics; HTTP; Software defined networking; Chunk duration diversity; Dynamic Adaptive Streaming over HTTP; Dynamic-adaptive; Multimedia content delivery; Quality of experience (QoE); Scalability problems; Structural similarity; Video bitrate; Quality of service
Beauty is in the eye of the beholder: Demographically oriented analysis of aesthetics in photographs,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071121710&doi=10.1145%2f3328993&partnerID=40&md5=135722f81dd81a3ff0afb2fc5b5ee99a,"Aesthetics is a subjective concept that is likely to be perceived differently among people of different ages, genders, and cultural backgrounds. While techniques that directly compute this concept in images has seen increasing attention by the multimedia and machine-learning community, there are very few attempts at encoding the influences from the photographer's viewpoint. This work demonstrates how the aesthetic quality of photos can be better learned by accounting for the demographic background of a photographer. A new AVA-PD (Photographer Demographic) dataset is created to supplement the AVA dataset by providing photographers' age, gender and location attributes. Two deep convolutional neural network (CNN) architectures are proposed to utilize demographic information for aesthetic prediction of photos; both are shown to yield better prediction capabilities compared to most existing approaches. By leveraging on AVA-PD meta-data, we also present some additional machine-learnable tasks such as identifying the photographer and predicting photography styles from a person's gallery of photos. © 2019 Association for Computing Machinery.",AVA; Convolutional neural networks; Demographic attributes; Image aesthetic evaluation; Photographer demographics,Convolution; Deep neural networks; Forecasting; Image coding; Neural networks; Photography; Convolutional neural network; Cultural backgrounds; Demographic attributes; Demographic information; Image Aesthetics; Machine learning communities; Photographer demographics; Prediction capability; Population statistics
Game of streaming players: Is consensus viable or an Illusion?,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071115560&doi=10.1145%2f3336496&partnerID=40&md5=c941a505a30e75278463542ca78b1eb1,"The dramatic growth of HTTP adaptive streaming (HAS) traffic represents a practical challenge for service providers in satisfying the demand from their customers. Achieving this in a network where multiple players share the network capacity has so far proved hard because of the bandwidth competition among the HAS players. This competition is exacerbated by the bandwidth overestimation that is introduced due to the isolated and selfish behavior of theHAS players. Each player strives individually to select themaximum bitratewithout considering the co-existing players or network resource dynamics. As a result, the HAS players suffer from video quality instability, quality unfairness, and network underutilization or oversubscription, and the players observe a poor quality of experience (QoE). To address this issue, we propose a fully distributed game theory and consensus-based collaborative adaptive bitrate solution for shared network environments, termed Game Theory and consensus-based Approach for Cooperative HAS delivery systems (GTAC). Our solution consists of two-stage games that run in parallel during a streaming session. We extensively evaluate GTAC on a broad set of trace-driven and real-world experiments. Results show that GTAC enhances the viewer QoE by up to 22%, presentation quality stability by up to 24%, fairness by at least 31%, and network utilization by 28% compared to the well-known schemes. © 2019 Association for Computing Machinery.",ABR; Consensus theory; Game theory; HAS; Instability; QoE; Scalability; Underutilization; Unfairness,Bandwidth; Plasma stability; Quality of service; Scalability; Adaptive streaming; Consensus theory; Net work utilization; Quality of experience (QoE); Real world experiment; Streaming sessions; Underutilization; Unfairness; Game theory
Moving foreground-aware visual attention and key volume mining for human action recognition,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071919544&doi=10.1145%2f3321511&partnerID=40&md5=38f39a40d138e3fbd82425513f2c6b3e,"Recently, many deep learning approaches have shown remarkable progress on human action recognition. However, it remains unclear how to extract the useful information in videos since only video-level labels are available in the training phase. To address this limitation, many efforts have been made to improve the performance of action recognition by applying the visual attention mechanism in the deep learning model. In this article, we propose a novel deep model called Moving Foreground Attention (MFA) that enhances the performance of action recognition by guiding the model to focus on the discriminative foreground targets. In our work, MFA detects the moving foreground through a proposed variance-based algorithm. Meanwhile, an unsupervised proposal is utilized to mine the action-related key volumes and generate corresponding correlation scores. Based on these scores, a newly proposed stochastic-out scheme is exploited to train the MFA. Experiment results show that action recognition performance can be significantly improved by using our proposed techniques, and our model achieves state-of-the-art performance on UCF101 and HMDB51. © 2019 Association for Computing Machinery.",Action-relevant key volume; Human action recognition; Variance-based scheme; Visual attention,Deep learning; Stochastic systems; Action recognition; Action-relevant key volume; Human-action recognition; Learning approach; State-of-the-art performance; Variance-based scheme; Visual Attention; Visual attention mechanisms; Behavioral research
Interpretable partitioned embedding for intelligent multi-item fashion outfit composition,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071081381&doi=10.1145%2f3326332&partnerID=40&md5=219bda819ff4243de9d9cefb629631ae,"Intelligent fashion outfit composition has become more popular in recent years. Some deep-learning-based approaches reveal competitive composition. However, the uninterpretable characteristic makes such a deep-learning-based approach fail to meet the businesses', designers', and consumers' urges to comprehend the importance of different attributes in an outfit composition. To realize interpretable and intelligent multi-item fashion outfit compositions, we propose a partitioned embedding network to learn interpretable embeddings from clothing items. The network contains two vital components: attribute partition module and partition adversarial module. In the attribute partition module, multiple attribute labels are adopted to ensure that different parts of the overall embedding correspond to different attributes. In the partition adversarial module, adversarial operations are adopted to achieve the independence of different parts. With the interpretable and partitioned embedding, we then construct an outfit-composition graph and an attribute matching map. Extensive experiments demonstrate that (1) the partitioned embedding have unmingled parts that correspond to different attributes and (2) outfits recommended by our model are more desirable in comparison with the existing methods. © 2019 Association for Computing Machinery.",Adversarial; Embedding; Interpretable; Outfit composition,Deep learning; Adversarial; Attribute matching; Embedding; Embedding network; Intelligent fashion; Interpretable; Learning-based approach; Multiple attributes; Embeddings
6K and 8K effective resolution with 4K HEVC decoding capability for 360 video streaming,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071141770&doi=10.1145%2f3335053&partnerID=40&md5=db433281c3f1660440aa65f7056a6b5a,"The recent Omnidirectional MediA Format (OMAF) standard, which specifies the delivery of 360° video content, supports only equirectangular projection (ERP) and cubemap projection and their region-wise packing with a limitation on video decoding capability to the maximum resolution of 4K (e.g., 4,096 × 2,048). Streaming of 4K ERP content allows only a limited viewport resolution, which is lower than the resolution of many current head-mounted displays (HMDs). Therefore, to take full advantage of high-resolution HMDs, delivery of 360° video content beyond 4K resolution needs to be enabled. In this regard, we propose two specific mixed-resolution packing schemes of 6K (e.g., 6,144 × 3,072) and 8K (e.g., 8,192 × 4,096) ERP content and their realization in tile-based streaming, while complying with the 4K decoding constraint and the High Efficiency Video Coding standard. The proposed packing schemes offer 6K and 8K effective resolution at the viewport. Using our proposed test methodology, experimental results indicate that the proposed layouts significantly decrease streaming bitrates when compared to mixed-quality viewport-adaptive streaming of 4K ERP. Our results further indicate that 8K-effective packing outperforms 6K-effective packing especially in high-quality videos. © 2019 Association for Computing Machinery.",360° video coding and streaming; Adaptive streaming; HEVC; OMAF; Virtual reality,Decoding; Helmet mounted displays; Video recording; Virtual reality; Adaptive streaming; Effective resolutions; Head mounted displays; HEVC; High quality video; High-efficiency video coding; Maximum resolution; OMAF; Video streaming
A pseudo-likelihood approach for geo-localization of events from crowd-sourced sensor-metadata,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071906067&doi=10.1145%2f3321701&partnerID=40&md5=18dc8a453dc29a05bc31e21809ea177b,"Events such as live concerts, protest marches, and exhibitions are often video recorded by many people at the same time, typically using smartphone devices. In this work, we address the problem of geo-localizing such events from crowd-generated data. Traditional approaches for solving such a problem using multiple video sequences of the event would require highly complex computer vision (CV) methods, which are computation intensive and are not robust under the environment where visual data are collected through crowd-sourced medium. In the present work, we approach the problem in a probabilistic framework using only the sensor metadata obtained from smartphones. We model the event location and camera locations and orientations (camera parameters) as the hidden states in a Hidden Markov Model. The sensor metadata from GPS and the digital compass from user smartphones are used as the observations associated with the hidden states of the model. We have used a suitable potential function to capture the complex interaction between the hidden states (i.e., event location and camera parameters). The non-Gaussian densities involved in the model, such as the potential function involving hidden states, make the maximum-likelihood estimation intractable. We propose a pseudo-likelihood-based approach to maximize the approximate-likelihood, which provides a tractable solution to the problem. The experimental results on the simulated as well as real data show correct event geo-localization using the proposed method. When compared with several baselines the proposed method shows a superior performance. The overall computation time required is much smaller, since only the sensor metadata are used instead of visual data. © 2019 Association for Computing Machinery.",Crowd sourcing; Digital compass; Event localization; GPS; HMM; Pseudo-likelihood; Smartphones; Video analysis; Von-Mises distribution,Cameras; Crowdsourcing; Global positioning system; Hidden Markov models; Location; Metadata; Smartphones; Trellis codes; Digital compass; Event localizations; Pseudo-likelihood; Video analysis; Von Mises distribution; Maximum likelihood estimation
Advanced stereo seam carving by considering occlusions on both sides,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071929807&doi=10.1145%2f3321513&partnerID=40&md5=b842c6a0ae9a244afbde2e4ef76e6fe9,"Stereo image retargeting plays a significant role in the field of image processing, which aims at making major objects as prominent as possible when the resolution of an image is changed, including maintaining disparity and depth information at the same time. Some seam carving methods are proposed to preserve the geometric consistency of the images. However, the regions of occlusion on both sides are not considered properly. In this article, we propose a solution to solve this problem. A new strategy of seams finding is designed by considering occluded and occluding regions on both of the input images, and leaving geometric consistency in both images intact. We also introduced the method of line segment detection and superpixel segmentation to further improve the quality of the images. Imaging effects are optimized in the process and visual comfort, which is also influenced by other factors, can be boosted as well. © 2019 Association for Computing Machinery.",Avoiding strategy; Line segment detection; Occluded regions; Occluding regions; Stereo image retargeting,Image enhancement; Image segmentation; Avoiding strategy; Line segment detection; Occluded regions; Occluding regions; Stereo-image; Stereo image processing
"Artificial intelligence, artists, and art: Attitudes toward artwork produced by humans vs. artificial intelligence",2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071105227&doi=10.1145%2f3326337&partnerID=40&md5=6a7ecc04c490eeb4cb8308649b15dc0e,"This study examines how people perceive artwork created by artificial intelligence (AI) and how presumed knowledge of an artist's identity (Human vs. AI) affects individuals' evaluation of art. Drawing on Schema theory and theory of Computers Are Social Actors (CASA), this study used a survey-experiment that controlled for the identity of the artist (AI vs. Human) and presented participants with two types of artworks (AI-created vs. Human-created). After seeing images of six artworks created by either AI or human artists, participants (n = 288) were asked to evaluate the artistic value using a validated scale commonly employed among art professionals. The study found that human-created artworks and AI-created artworks were not judged to be equivalent in their artistic value. Additionally, knowing that a piece of art was created by AI did not, in general, influence participants' evaluation of art pieces' artistic value. However, having a schema that AI cannot make art significantly influenced evaluation. Implications of the findings for application and theory are discussed. © 2019 Association for Computing Machinery.",Art; Artificial intelligence; CASA; Creativity; Human-machine communication; Humancomputer interaction; Schema theory,Computer networks; Human computer interaction; Artistic value; CASA; Creativity; Human-machine communication; Schema theory; Theory of computers; Artificial intelligence
Image captioning by asking questions,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071092066&doi=10.1145%2f3313873&partnerID=40&md5=cdc08dbbe1c03be1b5df8bea8efd36f0,"Image captioning and visual question answering are typical tasks that connect computer vision and natural language processing. Both of them need to effectively represent the visual content using computer vision methods and smoothly process the text sentence using natural language processing skills. The key problem of these two tasks is to infer the target result based on the interactive understanding of the word sequence and the image. Though they practically use similar algorithms, they are studied independently in the past few years. In this article, we attempt to exploit the mutual correlation between these two tasks. We propose the first VQA-improved image-captioning method that transfers the knowledge learned from the VQA corpora to the image-captioning task. A VQA model is first pretrained on image-question-answer instances. Then, the pretrained VQA model is used to extract VQA-grounded semantic representations according to selected free-form open-ended visual question-answer pairs. The VQA-grounded features are complementary to the visual features, because they interpret images from a different perspective. We incorporate the VQA model into the image-captioning model by adaptively fusing the VQA-grounded feature and the attended visual feature. We show that such simple VQA-improved image-captioning (VQA-IIC) models perform better than conventional image-captioning methods on large-scale public datasets. © 2019 Association for Computing Machinery. © 2019 Association for Computing Machinery.",Attention networks; Image captioning; Visual question answering,Computer vision; Large dataset; Natural language processing systems; Semantics; Visual languages; Image captioning; Mutual correlations; NAtural language processing; Question Answering; Question-answer pairs; Semantic representation; Visual content; Visual feature; Image enhancement
From theory to practice: Improving Bitrate adaptation in the DASH reference player,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071089081&doi=10.1145%2f3336497&partnerID=40&md5=ee6c99837b89d52c3d6c2730183e7977,"Modern video streaming uses adaptive bitrate (ABR) algorithms that run inside video players and continually adjust the quality (i.e., bitrate) of the video segments that are downloaded and rendered to the user. To maximize the quality-of-experience (QoE) of the user, ABR algorithms must stream at a high bitrate with low rebuffering and low bitrate oscillations. Further, a good ABR algorithm is responsive to user and network events and can be used in demanding scenarios such as low-latency live streaming. Recent research papers provide an abundance of ABR algorithms but fall short on many of the above real-world requirements. We develop Sabre, an open-source publicly available simulation tool that enables fast and accurate simulation of adaptive streaming environments.We empirically validated Sabre to show that it accurately simulates real-world environments. We used Sabre to design and evaluate BOLA-E and DYNAMIC, two novel ABR algorithms. We also developed a FAST SWITCHING algorithm that can replace segments that have already been downloaded with higher-bitrate (thus, higher-quality) segments. The new algorithms provide higher QoE to the user in terms of higher bitrate, fewer rebuffers, and lesser bitrate oscillations. In addition, these algorithms react faster to user events such as startup and seek, and they respond more quickly to network events such as improvements in throughput. Further, they perform very well for live streams that require low latency, a challenging scenario for ABR algorithms. Overall, our algorithms offer superior video QoE and responsiveness for real-life adaptive video streaming, in comparison to the state-of-the-art. Importantly, all three algorithms presented in this article are now part of the official DASH reference player dash.js and are being used by video providers in production environments. While our evaluation and implementation are focused on the DASH environment, our algorithms are equally applicable to other adaptive streaming formats such as Apple HLS. © 2019 Copyright held by the owner/author(s).",Bitrate adaptation; Video QoE; Video streaming,Video streaming; Adaptive streaming; Adaptive video streaming; Bit rates; Fast and accurate simulations; Production environments; Quality of experience (QoE); Real world environments; Video qoe; Quality of service
Spatial structure preserving feature pyramid network for semantic image segmentation,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071950366&doi=10.1145%2f3321512&partnerID=40&md5=0f2def9e8cccd180b8abf9ce154b7823,"Recently, progress on semantic image segmentation is substantial, benefiting from the rapid development of Convolutional Neural Networks. Semantic image segmentation approaches proposed lately have been mostly based on Fully convolutional Networks (FCNs). However, these FCN-based methods use large receptive fields and too many pooling layers to depict the discriminative semantic information of the images. Specifically, on one hand, convolutional kernel with large receptive field smooth the detailed edges, since too much contexture information is used to depict the “center pixel.” However, the pooling layer increases the receptive field through zooming out the latest feature maps, which loses many detailed information of the image, especially in the deeper layers of the network. These operations often cause low spatial resolution inside deep layers, which leads to spatially fragmented prediction. To address this problem, we exploit the inherent multi-scale and pyramidal hierarchy of deep convolutional networks to extract the feature maps with different resolutions and take full advantages of these feature maps via a gradually stacked fusing way. Specifically, for two adjacent convolutional layers, we upsample the features from deeper layer with stride of 2 and then stack them on the features from shallower layer. Then, a convolutional layer with kernels of 1 × 1 is followed to fuse these stacked features. The fused feature preserves the spatial structure information of the image; meanwhile, it owns strong discriminative capability for pixel classification. Additionally, to further preserve the spatial structure information and regional connectivity of the predicted category label map, we propose a novel loss term for the network. In detail, two graph model-based spatial affinity matrixes are proposed, which are used to depict the pixel-level relationships in the input image and predicted category label map respectively, and then their cosine distance is backward propagated to the network. The proposed architecture, called spatial structure preserving feature pyramid network, significantly improves the spatial resolution of the predicted category label map for semantic image segmentation. The proposed method achieves state-of-the-art results on three public and challenging datasets for semantic image segmentation. © 2019 Association for Computing Machinery.",Discriminative capability; Feature pyramid network; Semantic image segmentation; Spatial resolution,Classification (of information); Convolution; Image enhancement; Image resolution; Network layers; Neural networks; Pixels; Semantic Web; Semantics; Convolutional networks; Convolutional neural network; Discriminative capability; Feature pyramid; Proposed architectures; Semantic image segmentations; Spatial resolution; Spatial structure information; Image segmentation
Eigenvector-based distance metric learning for image classification and retrieval,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071903056&doi=10.1145%2f3340262&partnerID=40&md5=8ed6d9689c518147d01724c6ef6ab80d,"Distance metric learning has been widely studied in multifarious research fields. The mainstream approaches learn a Mahalanobis metric or learn a linear transformation. Recent related works propose learning a linear combination of base vectors to approximate the metric. In this way, fewer variables need to be determined, which is efficient when facing high-dimensional data. Nevertheless, such works obtain base vectors using additional data from related domains or randomly generate base vectors. However, obtaining base vectors from related domains requires extra time and additional data, and random vectors introduce randomness into the learning process, which requires sufficient random vectors to ensure the stability of the algorithm. Moreover, the random vectors cannot capture the rich information of the training data, leading to a degradation in performance. Considering these drawbacks, we propose a novel distance metric learning approach by introducing base vectors explicitly learned from training data. Given a specific task, we can make a sparse approximation of its objective function using the top eigenvalues and corresponding eigenvectors of a predefined integral operator on the reproducing kernel Hilbert space. Because the process of generating eigenvectors simply refers to the training data of the considered task, our proposed method does not require additional data and can reflect the intrinsic information of the input features. Furthermore, the explicitly learned eigenvectors do not result in randomness, and we can extend our method to any kernel space without changing the objective function. We only need to learn the coefficients of these eigenvectors, and the only hyperparameter that we need to determine is the number of eigenvectors that we utilize. Additionally, an optimization algorithm is proposed to efficiently solve this problem. Extensive experiments conducted on several datasets demonstrate the effectiveness of our proposed method. © 2019 Association for Computing Machinery.",Distance metric learning; Nonsmooth function optimization; Reproducing kernel Hilbert space,Clustering algorithms; Eigenvalues and eigenfunctions; Hilbert spaces; Image classification; Learning algorithms; Linear transformations; Mathematical transformations; Random processes; Vector spaces; Distance Metric Learning; High dimensional data; Linear combinations; Non-smooth functions; Objective functions; Optimization algorithms; Reproducing Kernel Hilbert spaces; Sparse approximations; Vectors
Pseudo-3D attention transfer network with content-aware strategy for image captioning,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071928618&doi=10.1145%2f3336495&partnerID=40&md5=2e11a47a457adb88f036dc03f90e4a45,"In this article, we propose a novel Pseudo-3D Attention Transfer network with Content-aware Strategy (P3DAT-CAS) for the image captioning task. Our model is composed of three parts: the Pseudo-3D Attention (P3DA) network, the P3DA-based Transfer (P3DAT) network, and the Content-aware Strategy (CAS). First, we propose P3DA to take full advantage of three-dimensional (3D) information in convolutional feature maps and capture more details. Most existing attention-based models only extract the 2D spatial representation from convolutional feature maps to decide which area should be paid more attention to. However, convolutional feature maps are 3D and different channel features can detect diverse semantic attributes associated with images. P3DA is proposed to combine 2D spatial maps with 1D semantic-channel attributes and generate more informative captions. Second, we design the transfer network to maintain and transfer the key previous attention information. The traditional attention-based approaches only utilize the current attention information to predict words directly, whereas transfer network is able to learn long-term attention dependencies and explore global modeling pattern. Finally, we present CAS to provide a more relevant and distinct caption for each image. The captioning model trained by maximum likelihood estimation may generate the captions that have a weak correlation with image contents, resulting in the cross-modal gap between vision and linguistics. However, CAS is helpful to convey the meaningful visual contents accurately. P3DAT-CAS is evaluated on Flickr30k and MSCOCO, and it achieves very competitive performance among the state-of-the-art models. © 2019 Association for Computing Machinery.",Content-aware strategy; Image captioning; Pseudo-3D attention network; Pseudo-3D attention transfer network; Transfer network,Convolution; Maximum likelihood estimation; Semantics; Competitive performance; Content-aware; Image captioning; Semantic attribute; Spatial representations; State of the art; Three-dimensional (3D) information; Transfer network; Data communication systems
Statistical early termination and early skip models for fast mode decision in HEVC intra coding,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070058003&doi=10.1145%2f3321510&partnerID=40&md5=955172d0b9ef52c525132a0c434e4ba8,"In this article, statistical Early Termination (ET) and Early Skip (ES) models are proposed for fast Coding Unit (CU) and prediction mode decision in HEVC INTRA coding, in which three categories of ET and ES sub-algorithms are included. First, the CU ranges of the current CU are recursively predicted based on the texture and CU depth of the spatial neighboring CUs. Second, the statistical model based ET and ES schemes are proposed and applied to optimize the CU and INTRA prediction mode decision, in which the coding complexities over different decision layers are jointly minimized subject to acceptable rate-distortion degradation. Third, the mode correlations among the INTRA prediction modes are exploited to early terminate the full rate-distortion optimization in each CU decision layer. Extensive experiments are performed to evaluate the coding performance of each sub-algorithm and the overall algorithm. Experimental results reveal that the overall proposed algorithm can achieve 45.47% to 74.77%, and 58.09% on average complexity reduction, while the overall Bjøntegaard delta bit rate increase and Bjøntegaard delta peak signal-to-noise ratio degradation are 2.29% and −0.11 dB, respectively. © 2019 Association for Computing Machinery.",Coding unit; Early skip; Early termination; HEVC; Intra angular prediction; Intra coding; Mode decision,Codes (symbols); Electric distortion; Forecasting; Signal distortion; Signal to noise ratio; Textures; Coding unit; Early skip; Early termination; HEVC; Intra coding; Mode Decision; Image coding
BTDP: Toward sparse fusion with block term decomposition pooling for visual question answering,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068644321&doi=10.1145%2f3282469&partnerID=40&md5=a41340246899986fa8c93418cfd322d2,"Bilinear models are very powerful in multimodal fusion tasks like Visual Question Answering. The predominant bilinear methods can all be seen as a kind of tensor-based decomposition operation that contains a key kernel called “core tensor.” Current approaches usually focus on reducing the computation complexity by applying low-rank constraint on the core tensor. In this article, we propose a novel bilinear architecture called Block Term Decomposition Pooling (BTDP), which not only maintains the advantages of previous bilinear methods but also conducts sparse bilinear interactions between modalities. Our method is based on Block Term Decompositions theory of tensor, which will result in a sparse and learnable block-diagonal core tensor for multimodal fusion. We prove that using such a block-diagonal core tensor is equivalent to conducting many “tiny” bilinear operations in different feature spaces. Thus, introducing sparsity into the bilinear operation can significantly increase the performance of feature fusion and improve VQA models. What is more, our BTDP is very flexible in design. We develop several variants of BTDP and discuss the effects of the diagonal blocks of core tensor. Extensive experiments on two challenging VQA-v1 and VQA-v2 datasets show that our BTDP method outperforms current bilinear models, achieving state-of-the-art performance. © 2019 Association for Computing Machinery.",Block term decomposition pooling; Sparse bilinear pooling; Visual question answering,Computer networks; Bilinear models; Block term decompositions; Computation complexity; Multi-modal fusion; Question Answering; Rank constraints; Sparse bilinear pooling; State-of-the-art performance; Tensors
Visual arts search on mobile devices,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068681912&doi=10.1145%2f3326336&partnerID=40&md5=95a6a7528216e91f0dd3de805a978487,"Visual arts, especially paintings, appear everywhere in our daily lives. They are not only liked by art lovers but also by ordinary people, both of whom are curious about the stories behind these artworks and also interested in exploring related artworks. Among various methods, the mobile visual search has its merits in providing an alternative solution to text and voice searches, which are not always applicable. Mobile visual search for visual arts is far more challenging than the general image visual search. Conventionally, visual search, such as searching products and plant, focuses on locating images containing similar objects. Hence, approaches are designed to locate objects and extract scale-invariant features from distorted photos that are captured by the mobile camera. However, the objects are only part of the visual art piece; the background and the painting style are both important factors that are not considered in the conventional approaches. In this article, an empirical investigation is conducted to study issues in photos taken by mobile cameras, such as orientation variance and motion blur, and how they influence the results of the mobile visual arts search. Based on the empirical investigation results, a photo-rectification pipeline is designed to rectify the photos into perfect images for feature extraction. A new method is proposed to learn high discriminative features for visual arts, which considers both the content information and style information in visual arts. Apart from conducting solid experiments, a real-world system is built to prove the effectiveness of the proposed methods. To the best of our knowledge, this is the first article to solve problems for visual arts search on mobile devices. © 2019 Association for Computing Machinery.",Feature learning; Mobile devices; Visual arts search,Cameras; Mobile computing; Alternative solutions; Conventional approach; Discriminative features; Empirical investigation; Feature learning; Mobile visual searches; Scale invariant features; Visual arts; Image processing
Introduction to the special issue on the cross-media analysis for visual question answering,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068690482&doi=10.1145%2f3337985&partnerID=40&md5=cd81377e9a35e9a3b56f4c79cacf9c30,[No abstract available],,
Video question answering via knowledge-based progressive spatial-temporal attention network,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068661419&doi=10.1145%2f3321505&partnerID=40&md5=6abd3a5afb91e613a158a1608cd45c26,"Visual Question Answering (VQA) is a challenging task that has gained increasing attention from both the computer vision and the natural language processing communities in recent years. Given a question in natural language, a VQA system is designed to automatically generate the answer according to the referenced visual content. Though there recently has been much intereset in this topic, the existing work of visual question answering mainly focuses on a single static image, which is only a small part of the dynamic and sequential visual data in the real world. As a natural extension, video question answering (VideoQA) is less explored. Because of the inherent temporal structure in the video, the approaches of ImageQA may be ineffectively applied to video question answering. In this article, we not only take the spatial and temporal dimension of video content into account but also employ an external knowledge base to improve the answering ability of the network. More specifically, we propose a knowledge-based progressive spatial-temporal attention network to tackle this problem. We obtain both objects and region features of the video frames from a region proposal network. The knowledge representation is generated by a word-level attention mechanism using the comment information of each object that is extracted from DBpedia. Then, we develop a question-knowledge-guided progressive spatial-temporal attention network to learn the joint video representation for video question answering task. We construct a large-scale video question answering dataset. The extensive experiments based on two different datasets validate the effectiveness of our method. © 2019 Association for Computing Machinery.",Attention; Knowledge; Spatial-temporal; Video question answering,Knowledge based systems; Knowledge representation; Large dataset; Video recording; Visual languages; Attention; Attention mechanisms; Knowledge; NAtural language processing; Question Answering; Question Answering Task; Spatial temporals; Video representations; Natural language processing systems
Cross-modality retrieval by joint correlation learning,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068657840&doi=10.1145%2f3314577&partnerID=40&md5=ea13e1cb5d3f32e934693c7f38557e6f,"As an indispensable process of cross-media analyzing, comprehending heterogeneous data faces challenges in the fields of visual question answering (VQA), visual captioning, and cross-modality retrieval. Bridging the semantic gap between the two modalities is still difficult. In this article, to address the problem in cross-modality retrieval, we propose a cross-modal learning model with joint correlative calculation learning. First, an auto-encoder is used to embed the visual features by minimizing the error of feature reconstruction and a multi-layer perceptron (MLP) is utilized to model the textual features embedding. Then we design a joint loss function to optimize both the intra- and the inter-correlations among the image-sentence pairs, i.e., the reconstruction loss of visual features, the relevant similarity loss of paired samples, and the triplet relation loss between positive and negative examples. In the proposed method, we optimize the joint loss based on a batch score matrix and utilize all mutual mismatched paired samples to enhance its performance. Our experiments in the retrieval tasks demonstrate the effectiveness of the proposed method. It achieves comparable performance to the state-of-the-art on three benchmarks, i.e., Flickr8k, Flickr30k, and MS-COCO. © 2019 Association for Computing Machinery.",Auto-encoder; Cross-modality retrieval; Joint loss; MLP,Benchmarking; Semantics; Signal encoding; Auto encoders; Cross modality; Feature reconstruction; Heterogeneous data; Multi layer perceptron; Negative examples; Question Answering; State of the art; Learning systems
"Show, reward, and tell: Adversarial visual story generation",2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068667648&doi=10.1145%2f3291925&partnerID=40&md5=4658c99cf0aa7620ecb0b6ef834f6af1,"Despite the promising progress made in visual captioning and paragraphing, visual storytelling is still largely unexplored. This task is more challenging due to the difficulty in modeling an ordered photo sequence and in generating a relevant paragraph with expressive language style for storytelling. To deal with these challenges, we propose an Attribute-based Hierarchical Generative model with Reinforcement Learning and adversarial training (AHGRL). First, to model the ordered photo sequence and the complex story structure, we propose an attribute-based hierarchical generator. The generator incorporates semantic attributes to create more accurate and relevant descriptions. The hierarchical framework enables the generator to learn from the complex paragraph structure. Second, to generate story-style paragraphs, we design a language-style discriminator, which provides word-level rewards to optimize the generator by policy gradient. Third, we further consider the story generator and the reward critic as adversaries. The generator aims to create indistinguishable paragraphs to human-level stories, whereas the critic aims at distinguishing them and further improving the generator. Extensive experiments on the widely used dataset well demonstrate the advantages of the proposed method over state-of-the-art methods. © 2019 Association for Computing Machinery.",Adversarial training; Policy gradient; Reinforcement learning; Visual storytelling,Machine learning; Modeling languages; Semantics; Attribute-based; Generative model; Human levels; Policy gradient; Semantic attribute; State-of-the-art methods; Story generations; Visual storytellings; Reinforcement learning
Semantic concept network and deep walk-based visual question answering,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068649652&doi=10.1145%2f3300938&partnerID=40&md5=a4c616ad05f0ce914480457498b64c77,"Visual Question Answering (VQA) is a hot-spot in the intersection of computer vision and natural language processing research and its progress has enabled many in high-level applications. This work aims to describe a novel VQA model based on semantic concept network construction and deep walk. Extracting visual image semantic representation is a significant and effective method for spanning the semantic gap. Moreover, current research has shown that co-occurrence patterns of concepts can enhance semantic representation. This work is motivated by the challenge that semantic concepts have complex interrelations and the relationships are similar to a network. Therefore, we construct a semantic concept network adopted by leveraging Word Activation Forces (WAFs), and mine the co-occurrence patterns of semantic concepts using deep walk. Then the model performs polynomial logistic regression on the basis of the extracted deep walk vector along with the visual image feature and question feature. The proposed model effectively integrates visual and semantic features of the image and natural language question. The experimental results show that our algorithm outperforms competitive baselines on three benchmark image QA datasets. Furthermore, through experiments in image annotation refinement and semantic analysis on pre-labeled LabelMe dataset, we test and verify the effectiveness of our constructed concept network for mining concept co-occurrence patterns, sensible concept clusters, and hierarchies. © 2019 Association for Computing Machinery.",And Phrases: VQA; Deep walk; Low-dimensional feature representation; Semantic concept network; Word activation forces,Chemical activation; Image processing; Natural language processing systems; Semantic Web; Semantics; Statistical tests; And Phrases: VQA; Deep walk; Feature representation; Semantic concept; Word activation forces; Visual languages
Color theme–based aesthetic enhancement algorithm to emulate the human perception of beauty in photos,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068639045&doi=10.1145%2f3328991&partnerID=40&md5=d209300ee2d1bfce4c33aff67be5db33,"Fine Art Photography is one of the most popular art forms, which creates lasting impressions that elicit various human emotional reactions. Photo aesthetic enhancement aims at improving the aesthetic level of the photo to please humans by updating color appearance or modifying the geometry structure of objects within that photo. Even though several aesthetic enhancement methods have been proposed, to our knowledge, there is no research to explore, highlight, and accentuate photos’ intrinsic aesthetic value to elicit a stronger response from the human observer about the photos’ theme. To meet this challenge, a new multimedia technology called automatic color theme–based aesthetic enhancement (CT-AEA) is proposed by leveraging big online data to perform timely collection and learning of humans’ current aesthetic perception-behavior over photos and color themes in art, fashion, and design. Unlike existing aesthetic enhancement that examines the composition, such as the geometric structure of the image contents and color/luminance-related (color tone and luminance distribution) characteristics, this CT-AEA takes into consideration the importance of a suitable color theme, namely a set of dominant colors for the design when assessing the aesthetic appearance of a photo. This algorithm is composed of (1) utilizing the knowledge gained from the human evaluator’s perception of beauty from existing online datasets, rather than simply applying prior existing knowledge of color harmony theory; (2) developing a new color theme difference equation that exhibits order-invariance and percentage-sensitive properties; (3) designing an optimal color theme recommendation to maximize the aesthetic performance, while minimizing the color modification cost to solve the problems of color inconsistencies and distortion. Experimental results, quantitative measure, and comparison tests demonstrate the algorithm’s effectiveness, advantages, and potential for use in many color-related art and design applications. © 2019 Association for Computing Machinery.",Aesthetic enhancement; Big data; Color theme; Fine art photography; Human aesthetic perception,Big data; Color; Computerized tomography; Difference equations; Image enhancement; Luminance; Multimedia systems; Structural design; Aesthetic enhancements; Aesthetic perception; Color harmony theory; Color themes; Fine arts; Luminance distributions; Multimedia technologies; Quantitative measures; Color photography
Art by computing machinery: Is machine art acceptable in the artworld?,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068695681&doi=10.1145%2f3326338&partnerID=40&md5=c8b05ee651511a895c139f27e4778937,"When does a machine-created work becomes art? What is art? Can machine artworks fit in to the historical and present discourse? Do machine artworks demonstrate creativity, or are they a type of new media from which artists extend their creativity with? Will solely machine-created artworks be acceptable by our artworlds? This article probes these questions by first identifying the frameworks for defining and explaining art and evaluating its suitability for explaining machine artworks. It then explores how artworks have a necessary relationship with their human artists and the wider context of history, institutions, styles, and approaches and with audiences and artworlds. The article then questions whether machines have such a relational context and whether machines will ever live up to our standard of what constitutes an artwork as defined by us or whether machines are good only for assisting creativity. The question of intellectual property, rights, and ownership are also discussed for human–machine artworks and purely machine-produced works of art. The article critically assesses the viability of machines as artists as the central question in the historical discourse, extended through art and the artworld and evaluates machine-produced work from such a basis. © 2019 Association for Computing Machinery.",Art theory; Artworlds; Machine art; Machine artist; Machine artworks,Computation theory; Machinery; Art theory; Artworlds; Computing machinery; New media; Works of art; Arts computing
Watch Me from Distance (WMD): A privacy-preserving long-distance video surveillance system,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067260997&doi=10.1145%2f3312574&partnerID=40&md5=737bfc9d9e8f53c4da90e1a2111c0fb1,"Preserving the privacy of people in video surveillance systems is quite challenging, and a significant amount of research has been done to solve this problem in recent times. Majority of existing techniques are based on detecting bodily cues such as face and/or silhouette and obscuring them so that people in the videos cannot be identified.We observe that merely hiding bodily cues is not enough for protecting identities of the individuals in the videos. An adversary, who has prior contextual knowledge about the surveilled area, can identify people in the video by exploiting the implicit inference channels such as behavior, place, and time. This article presents an anonymous surveillance system, calledWatch Me from Distance (WMD), which advocates for outsourcing of surveillance video monitoring (similar to call centers) to the long-distance sites where professional security operators watch the video and alert the local site when any suspicious or abnormal event takes place. We find that long-distance monitoring helps in decoupling the contextual knowledge of security operators. Since security operators at the remote site could turn into adversaries, a trust computation model to determine the credibility of the operators is presented as an integral part of the proposed system. The feasibility study and experiments suggest that the proposed system provides more robust measures of privacy yet maintains surveillance effectiveness. © 2019 Association for Computing Machinery.",Context decoupling; Privacy; Surveillance; Trust model,Data privacy; Monitoring; Space surveillance; Watches; Context decoupling; Contextual knowledge; Distance Monitoring; Feasibility studies; Privacy preserving; Surveillance systems; Trust modeling; Video surveillance systems; Security systems
Resilient color image watermarking using accurate quaternion radial substituted chebyshev moments,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067263751&doi=10.1145%2f3325193&partnerID=40&md5=63d36e3e80b1651a613736b99c585b78,"In this work, a new quaternion-based method for color image watermarking is proposed. In this method, a novel set of quaternion radial substituted Chebyshev moments (QRSCMs) is presented for robust geometrically invariant image watermarking. An efficient computational method is proposed for highly accurate, fast, and numerically stable QRSCMs in polar coordinates. The proposed watermarking method consists of three stages. In the first stage, the Arnold transform is used to improve the security of the watermarking scheme by scrambling the binary watermark. In the second stage, the proposed accurate and stable QRSCMs of the host color image are computed. In the third stage, the encrypted binary watermark is embedded into the host image by employing the quantization technique on selected-magnitude QRSCMs where the watermarked color image is obtained by adding the original host color image to the compensation image. Then, the binary watermark can be extracted directly without using the original image from the magnitudes of QRSCMs. Numerical experiments are performed where the performance of proposed method is compared with the existing quaternion moment-based watermarking methods. The comparison clearly shows that the proposed method is very efficient in terms of the visual imperceptibility capability and the robustness under different attacks compared to the existing quaternion moment-based watermarking algorithms. © 2019 Association for Computing Machinery.",Color image watermarking; Geometric attacks; JPEG compression; Noise resistance; Quaternion radial substituted Chebyshev moments,Color; Image watermarking; Numerical methods; Chebyshev moments; Color image watermarking; Geometric attacks; JPEG compression; Noise resistance; Image compression
Appearance-consistent video object segmentation based on a multinomial event model,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067255716&doi=10.1145%2f3321507&partnerID=40&md5=6bcaf6df59d315b16bdd82b240ea25a1,"In this study, we propose an effective and efficient algorithm for unconstrained video object segmentation, which is achieved in a Markov random field (MRF). In the MRF graph, each node is modeled as a superpixel and labeled as either foreground or background during the segmentation process. The unary potential is computed for each node by learning a transductive SVM classifier under supervision by a few labeled frames. The pairwise potential is used for the spatial-temporal smoothness. In addition, a high-order potential based on the multinomial event model is employed to enhance the appearance consistency throughout the frames. To minimize this intractable feature, we also introduce a more efficient technique that simply extends the originalMRF structure. The proposed approach was evaluated in experiments with different measures and the results based on a benchmark demonstrated its effectiveness comparedwith other state-of-the-art algorithms. © 2019 Association for Computing Machinery.",Appearance consistency; Markov random field; Multinomial event model,Magnetorheological fluids; Markov processes; Motion compensation; Structural frames; Appearance consistency; Event model; High-order potentials; Markov Random Fields; Segmentation process; Spatial temporals; State-of-the-art algorithms; Video-object segmentation; Image segmentation
Alone versus in-a-group: A multi-modal framework for automatic affect recognition,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067240920&doi=10.1145%2f3321509&partnerID=40&md5=8e2562ec2be3f2d9ee264d2ae4c03299,"Recognition and analysis of human affect has been researched extensively within the field of computer science in the past two decades. However, most of the past research in automatic analysis of human affect has focused on the recognition of affect displayed by people in individual settings and little attention has been paid to the analysis of the affect expressed in group settings. In this article, we first analyze the affect expressed by each individual in terms of arousal and valence dimensions in both individual and group videos and then propose methods to recognize the contextual information, i.e., whether a person is alone or in-agroup by analyzing their face and body behavioral cues. For affect analysis, we first devise affect recognition models separately in individual and group videos and then introduce a cross-condition affect recognition model that is trained by combining the two different types of data. We conduct a set of experiments on two datasets that contain both individual and group videos. Our experiments show that (1) the proposed Volume Quantized Local ZernikeMoments Fisher Vector outperforms other unimodal features in affect analysis; (2) the temporal learning model, Long-Short Term Memory Networks, works better than the static learning model, Support Vector Machine; (3) decision fusion helps to improve affect recognition, indicating that body behaviors carry emotional information that is complementary rather than redundant to the emotion content in facial behaviors; and (4) it is possible to predict the context, i.e., whether a person is alone or in-a-group, using their non-verbal behavioral cues. © 2019 Association for Computing Machinery.",Affect analysis; Context analysis; Group settings; Multimodal interaction; Non-verbal behaviours,Computer networks; Affect analysis; Affect recognition; Automatic analysis; Context analysis; Contextual information; Emotional information; Multi-Modal Interactions; Non-verbal behaviours; Modal analysis
How deep features have improved event recognition in multimedia: A survey,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067237516&doi=10.1145%2f3306240&partnerID=40&md5=7e99d524261c5eda7be2bf82437ea740,"Event recognition is one of the areas in multimedia that is attracting great attention of researchers. Being applicable in a wide range of applications, from personal to collective events, a number of interesting solutions for event recognition using multimedia information sources have been proposed. On the other hand, following their immense success in classification, object recognition, and detection, deep learning has been shown to perform well in event recognition tasks also. Thus, a large portion of the literature on event analysis relies nowadays on deep learning architectures. In this article, we provide an extensive overview of the existing literature in this field, analyzing how deep features and deep learning architectures have changed the performance of event recognition frameworks. The literature on event-based analysis of multimedia contents can be categorized into four groups, namely (i) event recognition in single images; (ii) event recognition in personal photo collections; (iii) event recognition in videos; and (iv) event recognition in audio recordings. In this article, we extensively review different deep-learning-based frameworks for event recognition in these four domains. Furthermore, we also review some benchmark datasets made available to the scientific community to validate novel event recognition pipelines. In the final part of the manuscript, we also provide a detailed discussion on basic insights gathered from the literature review, and identify future trends and challenges. © 2019 Association for Computing Machinery.",Audio event analysis; Deep features; Deep learning; Event detection; Information retrieval; Natural disaster; Social events detection; Social media; Video analysis,Disasters; Information retrieval; Object detection; Object recognition; Audio events; Deep features; Event detection; Natural disasters; Social events; Social media; Video analysis; Deep learning
A deep learning system for recognizing facial expression in real-time,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067241336&doi=10.1145%2f3311747&partnerID=40&md5=e2997845316113eec96c07175dca648e,"This article presents an image-based real-time facial expression recognition system that is able to recognize the facial expressions of several subjects on a webcam at the same time. Our proposed methodology combines a supervised transfer learning strategy and a joint supervision method with center loss, which is crucial for facial tasks. A newly proposed Convolutional Neural Network (CNN) model, MobileNet, which has both accuracy and speed, is deployed in both offline and in a real-time framework that enables fast and accurate real-time output. Evaluations towards two publicly available datasets, JAFFE and CK+, are carried out respectively. The JAFFE dataset reaches an accuracy of 95.24%, while an accuracy of 96.92% is achieved on the 6-class CK+ dataset, which contains only the last frames of image sequences. At last, the average run-time cost for the recognition of the real-time implementation is around 3.57ms/frame on a NVIDIA Quadro K4200 GPU. © 2019 Association for Computing Machinery.",Deep learning networks; Facial expression recognition,Face recognition; Neural networks; Optical character recognition; Real time control; Response time (computer systems); Convolutional neural network; Facial expression recognition; Facial Expressions; Image sequence; Image-based; Learning network; Real-time implementations; Transfer learning; Deep learning
Look at Me! Correcting eye gaze in live video communication,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067239938&doi=10.1145%2f3311784&partnerID=40&md5=3aae1b05d05411b2587e5691b8642f1d,"Although live video communication is widely used, it is generally less engaging than face-to-face communication because of limitations on social, emotional, and haptic feedback. Missing eye contact is one such problem caused by the physical deviation between the screen and camera on a device. Manipulating video frames to correct eye gaze is a solution to this problem. In this article, we introduce a system to rotate the eyeball of a local participant before the video frame is sent to the remote side. It adopts a warping-based convolutional neural network to relocate pixels in eye regions. To improve visual quality, we minimize the L2 distance between the ground truths and warped eyes. We also present several newly designed loss functions to help network training. These new loss functions are designed to preserve the shape of eye structures and minimize color changes around the periphery of eye regions. To evaluate the presented network and loss functions, we objectively and subjectively compared results generated by our system and the state-of-the-art, DeepWarp, in relation to two datasets. The experimental results demonstrated the effectiveness of our system. In addition, we showed that our system can perform eye-gaze correction in real time on a consumer-level laptop. Because of the quality and efficiency of the system, gaze correction by postprocessing through this system is a feasible solution to the problem of missing eye contact in video communication. © 2019 Association for Computing Machinery.",Convolutional neural network; Eye contact; Gaze correction; Image processing; Live video communication,Convolution; Neural networks; Convolutional neural network; Eye contact; Face-to-face communications; Feasible solution; Live video; Network training; Video communications; Visual qualities; Image processing
Rich visual and language representation with complementary semantics for video captioning,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067243991&doi=10.1145%2f3303083&partnerID=40&md5=1b24e576424c35c1f6e44c5d4596e072,"It is interesting and challenging to translate a video to natural description sentences based on the video content. In this work, an advanced framework is built to generate sentences with coherence and rich semantic expressions for video captioning. A long short term memory (LSTM) network with an improved factored way is first developed, which takes the inspiration of LSTM with a conventional factored way and a common practice to feed multi-modal features into LSTM at the first time step for visual description. Then, the incorporation of the LSTM network with the proposed improved factored way and un-factored way is exploited, and a voting strategy is utilized to predict candidate words. In addition, for robust and abstract visual and language representation, residuals are employed to enhance the gradient signals that are learned from the residual network (ResNet), and a deeper LSTM network is constructed. Furthermore, three convolutional neural network based features extracted from GoogLeNet, ResNet101, and ResNet152, are fused to catch more comprehensive and complementary visual information. Experiments are conducted on two benchmark datasets, including MSVD and MSR-VTT2016, and competitive performances are obtained by the proposed techniques as compared to other state-of-the-art methods. © 2019 Association for Computing Machinery.",Complementary features; Convolutional neural network; Long short term memory; Sequential voting; Video captioning,Brain; Convolution; Semantics; Visual languages; Benchmark datasets; Competitive performance; Complementary features; Convolutional neural network; Sequential voting; State-of-the-art methods; Video captioning; Visual information; Long short-term memory
A multi-sensor framework for personal presentation analytics,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067240351&doi=10.1145%2f3300941&partnerID=40&md5=9114cb417e537f959a7a30a9cb8ce5f4,"Presentation has been an effective method for delivering information to an audience for many years. Over the past few decades, technological advancements have revolutionized the way humans deliver presentation. Conventionally, the quality of a presentation is usually evaluated through painstaking manual analysis with experts. Although the expert feedback is effective in assisting users to improve their presentation skills, manual evaluation suffers from high cost and is often not available to most individuals. In this work, we propose a novel multi-sensor self-quantification system for presentations, which is designed based on a new proposed assessment rubric. We present our analytics model with conventional ambient sensors (i.e., static cameras and Kinect sensor) and the emerging wearable egocentric sensors (i.e., Google Glass). In addition, we performed a cross-correlation analysis of speaker's vocal behavior and body language. The proposed framework is evaluated on a new presentation dataset, namely, NUS Multi-Sensor Presentation dataset, which consists of 51 presentations covering a diverse range of topics. To validate the efficacy of the proposed system, we have conducted a series of user studies with the speakers and an interview with an English communication expert, which reveals positive and promising feedback. © 2019 Association for Computing Machinery.",Learning analytics; Multi-modal analysis; Presentations; Quantified self,Modal analysis; Quality control; Wearable sensors; Assessment rubrics; Cross-correlation analysis; Learning analytics; Manual analysis; Presentation skills; Presentations; Quantified self; Technological advancement; Feedback
Detecting online counterfeit-goods seller using connection discovery,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067246487&doi=10.1145%2f3311785&partnerID=40&md5=d9c63965575db75ad9bc3bfc91d7807d,"With the advancement of social media and mobile technology, any smartphone user can easily become a seller on social media and e-commerce platforms, such as Instagram and Carousell in Hong Kong or Taobao in China. A seller shows images of their products and annotates their images with suitable tags that can be searched easily by others. Those images could be taken by the seller, or the seller could use images shared by other sellers. Among sellers, some sell counterfeit goods, and these sellers may use disguising tags and language, which make detecting them a difficult task. This article proposes a framework to detect counterfeit sellers by using deep learning to discover connections among sellers from their shared images. Based on 473K shared images from Taobao, Instagram, and Carousell, it is proven that the proposed framework can detect counterfeit sellers. The framework is 30% better than approaches using object recognition in detecting counterfeit sellers. To the best of our knowledge, this is the first work to detect online counterfeit sellers from their shared images. © 2019 Association for Computing Machinery.",Counterfeit seller detection; Deep learning; Social network,Mobile commerce; Object detection; Object recognition; Social networking (online); Hong-kong; Mobile Technology; Social media; Deep learning
Increasing image memorability with neural style transfer,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067225680&doi=10.1145%2f3311781&partnerID=40&md5=4b550a17cbda05b98edcf7b5f1e29a8e,"Recent works in computer vision and multimedia have shown that image memorability can be automatically inferred exploiting powerful deep-learning models. This article advances the state of the art in this area by addressing a novel and more challenging issue: ""Given an arbitrary input image, can we make it more memorable? "" To tackle this problem, we introduce an approach based on an editing-by-applying-filters paradigm: Given an input image, we propose to automatically retrieve a set of ""style seeds,"" i.e., a set of style images that, applied to the input image through a neural style transfer algorithm, provide the highest increase in memorability. We show the effectiveness of the proposed approach with experiments on the publicly available LaMem dataset, performing both a quantitative evaluation and a user study. To demonstrate the flexibility of the proposed framework, we also analyze the impact of different implementation choices, such as using different state-of-the-art neural style transfer methods. Finally, we show several qualitative results to provide additional insights on the link between image style and memorability. © 2019 Association for Computing Machinery.",Deep learning; Memorability; Style transfer,Computer networks; Arbitrary inputs; Input image; Learning models; Memorability; Quantitative evaluation; State of the art; Style transfer; Transfer method; Deep learning
Using eye tracking and heart-rate activity to examine crossmodal correspondences QoE in Mulsemedia,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067236346&doi=10.1145%2f3303080&partnerID=40&md5=3ad73e5a2b99fa77498ff12893e82bbf,"Different senses provide us with information of various levels of precision and enable us to construct a more precise representation of the world. Rich multisensory simulations are thus beneficial for comprehension, memory reinforcement, or retention of information. Crossmodal mappings refer to the systematic associations often made between different sensory modalities (e.g., high pitch is matched with angular shapes) and govern multisensory processing. A great deal of research effort has been put into exploring cross-modal correspondences in the field of cognitive science. However, the possibilities they open in the digital world have been relatively unexplored. Multiple sensorial media (mulsemedia) provides a highly immersive experience to the users and enhances their Quality of Experience (QoE) in the digital world. Thus, we consider that studying the plasticity and the effects of cross-modal correspondences in a mulsemedia setup can bring interesting insights about improving the human computer dialogue and experience. In our experiments, we exposed users to videos with certain visual dimensions (brightness, color, and shape), and we investigated whether the pairing with a cross-modal matching sound (high and low pitch) and the corresponding autogenerated vibrotactile effects (produced by a haptic vest) lead to an enhanced QoE. For this, we captured the eye gaze and the heart rate of users while experiencing mulsemedia, and we asked them to fill in a set of questions targeting their enjoyment and perception at the end of the experiment. Results showed differences in eye-gaze patterns and heart rate between the experimental and the control group, indicating changes in participants' engagement when videos were accompanied by matching cross-modal sounds (this effect was the strongest for the video displaying angular shapes and high-pitch audio) and transitively generated crossmodal vibrotactile effects. © 2019 Association for Computing Machinery.",Audio; Cross-modal correspondence; Gaze tracking; Haptic; Heart-rate variability; Mulsemedia; Quality of experience; Video,Audio acoustics; Color matching; Heart; Human computer interaction; Quality of service; Audio; Cross-modal; Gaze tracking; Haptic; Heart rate variability; Mulsemedia; Quality of experience (QoE); Video; Eye tracking
A2CMHNE: Attention-aware collaborative multimodal heterogeneous network embedding,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067253588&doi=10.1145%2f3321506&partnerID=40&md5=17d7f3e9088b70c061db5392b3317935,"Network representation learning is playing an important role in network analysis due to its effectiveness in a variety of applications. However, most existing network embedding models focus on homogeneous networks and neglect the diverse properties such as different types of network structures and associated multimedia content information. In this article, we learn node representations for multimodal heterogeneous networks, which contain multiple types of nodes and/or links as well as multimodal content such as texts and images. We propose a novel attention-aware collaborative multimodal heterogeneous network embedding method (A2CMHNE), where an attention-based collaborative representation learning approach is proposed to promote the collaboration of structure-based embedding and content-based embedding, and generate the robust node representation by introducing an attention mechanism that enables informative embedding integration. In experiments, we compare our model with existing network embedding models on two real-world datasets. Our method leads to dramatic improvements in performance by 5%, and 9% compared with five state-of-the-art embedding methods on one benchmark (M10 Dataset), and on a multi-modal heterogeneous network dataset (WeChat dataset) for node classification, respectively. Experimental results demonstrate the effectiveness of our proposed method on both node classification and link prediction tasks. © 2019 Association for Computing Machinery.",Heterogeneous network; Multimodal; Network embedding,Benchmarking; Classification (of information); Heterogeneous networks; Attention mechanisms; Collaborative representations; In-network analysis; Multi-modal; Multimedia contents; Network embedding; Network representation; Real-world datasets; Embeddings
From selective deep convolutional features to compact binary representations for image retrieval,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067241925&doi=10.1145%2f3314051&partnerID=40&md5=79d8bc27513092e372d324485e0284ef,"In the large-scale image retrieval task, the two most important requirements are the discriminability of image representations and the efficiency in computation and storage of representations. Regarding the former requirement, Convolutional Neural Network is proven to be a very powerful tool to extract highly discriminative local descriptors for effective image search. Additionally, to further improve the discriminative power of the descriptors, recent works adopt fine-tuned strategies. In this article, taking a different approach, we propose a novel, computationally efficient, and competitive framework. Specifically, we first propose various strategies to compute masks, namely, SIFT-masks, SUM-mask, and MAX-mask, to select a representative subset of local convolutional features and eliminate redundant features. Our in-depth analyses demonstrate that proposed masking schemes are effective to address the burstiness drawback and improve retrieval accuracy. Second, we propose to employ recent embedding and aggregating methods that can significantly boost the feature discriminability. Regarding the computation and storage efficiency, we include a hashing module to produce very compact binary image representations. Extensive experiments on six image retrieval benchmarks demonstrate that our proposed framework achieves the state-of-the-art retrieval performances. © 2019 Association for Computing Machinery.",Aggregating; Content based image retrieval; Deep convolutional features; Embedding; Image hashing; Unsupervised,Computational efficiency; Content based retrieval; Convolution; Efficiency; Embeddings; Neural networks; Aggregating; Content based image retrieval; Deep convolutional features; Embedding; Image hashing; Unsupervised; Binary images
Multi-level similarity perception network for person re-identification,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067228733&doi=10.1145%2f3309881&partnerID=40&md5=8b434d15bb09f64dc0e42025664a11ca,"In this article, we propose a novel deep Siamese architecture based on a convolutional neural network (CNN) and multi-level similarity perception for the person re-identification (re-ID) problem. According to the distinct characteristics of diverse feature maps, we effectively apply different similarity constraints to both lowlevel and high-level feature maps during training stage. Due to the introduction of appropriate similarity comparison mechanisms at different levels, the proposed approach can adaptively learn discriminative local and global feature representations, respectively, while the former is more sensitive in localizing part-level prominent patterns relevant to re-identifying people across cameras. Meanwhile, a novel strong activation pooling strategy is utilized on the last convolutional layer for abstract local-feature aggregation to pursue more representative feature representations. Based on this, we propose final feature embedding by simultaneously encoding original global features and discriminative local features. In addition, our framework has two other benefits: First, classification constraints can be easily incorporated into the framework, forming a unified multi-task network with similarity constraints. Second, as similarity-comparable information has been encoded in the network's learning parameters via back-propagation, pairwise input is not necessary at test time. That means we can extract features of each gallery image and build an index in an off-line manner, which is essential for large-scale real-world applications. Experimental results on multiple challenging benchmarks demonstrate that our method achieves splendid performance compared with the current stateof-the-art approaches. © 2019 Association for Computing Machinery.",CNN; Deep Siamese architecture; Multi-level similarity perception; Person re-identification,Abstracting; Backpropagation; Benchmarking; Convolution; Neural networks; Convolutional neural network; Feature representation; Global feature representations; High-level features; Learning parameters; Multilevels; Person re identifications; State-of-the-art approach; Network architecture
"Design, large-scale usage testing, and important metrics for augmented reality gaming applications",2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067251903&doi=10.1145%2f3311748&partnerID=40&md5=53b0ca132ad2280ac1d6f58fcd2e6527,"Augmented Reality (AR) offers the possibility to enrich the real world with digital mediated content, increasing in this way the quality of many everyday experiences. While in some research areas such as cultural heritage, tourism, or medicine there is a strong technological investment, AR for game purposes struggles to become awidespread commercial application. In this article, a novel framework for AR kid games is proposed, already developed by the authors for other AR applications such as Cultural Heritage and Arts. In particular, the framework includes different layers such as the development of a series of AR kid puzzle games in an intermediate structure which can be used as a standard for different applications development, the development of a smart configuration tool, together with general guidelines and long-life usage tests and metrics. The proposed application is designed for augmenting the puzzle experience, but can be easily extended to other AR gaming applications. Once the user has assembled the real puzzle, AR functionality within the mobile application can be unlocked, bringing to life puzzle characters, creating a seamless game that merges AR interactions with the puzzle reality. The main goals and benefits of this framework can be seen in the development of a novel set of AR tests and metrics in the pre-release phase (in order to help the commercial launch and developers), and in the release phase by introducing the measures for long-life app optimization, usage tests and hint on final users together with a measure to design policy, providing a method for automatic testing of quality and popularity improvements. Moreover, smart configuration tools, as part of the general framework, enabling multi-app and eventually also multi-user development, have been proposed, facilitating the serialization of the applications. Results were obtained from a large-scale user test with about 4 million users on a set of eight gaming applications, providing the scientific community a workflow for implicit quantitative analysis in AR gaming. Different data analytics developed on the data collected by the framework prove that the proposed approach is affordable and reliable for long-life testing and optimization. © 2019 Association for Computing Machinery.",Augmented reality; Gaming framework; Large scale testing; Mobile gaming; Puzzle,Automatic testing; Data Analytics; Applications development; Augmented reality gaming; Commercial applications; Gaming framework; Intermediate structures; Large scale testing; Mobile gaming; Puzzle; Augmented reality
QoE for mobile clients with Segment-aware Rate Adaptation Algorithm (SARA) for DASH video streaming,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067227412&doi=10.1145%2f3311749&partnerID=40&md5=62547f973a829899d03a3f9c9f9ef857,"Dynamic adaptive streaming over HTTP (DASH) is widely used for video streaming on mobile devices. Ensuring a good quality of experience (QoE) for mobile video streaming is essential, as it severely impacts both the network and content providers' revenue. Thus, a good rate adaptation algorithm at the client end that provides high QoE is critically important. Recently, a segment size-aware rate adaptation (SARA) algorithm was proposed for DASH clients. However, its performance on mobile clients has not been investigated so far. The main contributions of this article are twofold: (1) We discuss SARA's implementation for mobile clients to improve the QoE in mobile video streaming, one that accurately predicts the download time for the next segment and makes an informed bitrate selection, and (2) we developed a new parametric QoE model to compute a cumulative score that helps in fair comparison of different adaptation algorithms. Based on our subjective and objective evaluation, we observed that SARA for mobile clients outperforms others by 17% on average, in terms of the Mean Opinion Score, while achieving, on average, a 76% improvement in terms of the interruption ratio. The score obtained from our new parametric QoE model also demonstrates that the SARA algorithm for mobile clients gives a better QoE among all the algorithms. © 2019 Association for Computing Machinery.",Bitrate adaptation algorithm; Dynamic adaptive streaming over HTTP; Parametric QoE model; Quality of experience; Segment awareness; Subjective quality assessment,HTTP; Parameter estimation; Video streaming; Adaptation algorithms; Dynamic Adaptive Streaming over HTTP; QoE models; Quality of experience (QoE); Segment awareness; Subjective quality assessments; Quality of service
Low-complexity scalable extension of the High-Efficiency Video Coding (SHVC) encoding system,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067230421&doi=10.1145%2f3313185&partnerID=40&md5=9091a6e96e0943fdaf30df7b66feeeb9,"The scalable extension of the high-efficiency video coding (SHVC) system adopts a hierarchical quadtreebased coding unit (CU) that is suitable for various texture and motion properties of videos. Currently, the test model of SHVC identifies the optimal CU size by performing an exhaustive quadtree depth-level search,which achieves a high compression efficiency at a heavy cost in terms of the computational complexity. However, many interactive multimedia applications, such as remote monitoring and video surveillance, which are sensitive to time delays, have insufficient computational power for coding high-definition (HD) and ultra-highdefinition (UHD) videos. Therefore, it is important, yet challenging, to optimize the SHVC coding procedure and accelerate video coding. In this article, we propose a fast CU quadtree depth-level decision algorithm for inter-frames on enhancement layers that is based on an analysis of inter-layer, spatial, and temporal correlations. When motion/texture properties of coding regions can be identified early, a fast algorithm can be designed for adapting CU depth-level decision procedures to video contents and avoiding unnecessary computations during CU depth-level traversal. The proposed algorithm determines the motion activity level at the treeblock size of the hierarchical quadtree by utilizing motion vectors from its corresponding blocks at the base layer. Based on the motion activity level, neighboring encoded CUs that have larger correlations are preferentially selected to predict the optimal depth level of the current treeblock. Finally, two parameters, namely, the motion activity level and the predicted CU depth level, are used to identify a subset of candidate CU depth levels and adaptively optimize CU depth-level decision processes. The experimental results demonstrate that the proposed scheme can run approximately three times faster than the most recent SHVC reference software, with a negligible loss of compression efficiency. The proposed scheme is efficient for all types of scalable video sequences under various coding conditions and outperforms state-of-the-art fast SHVC and HEVC algorithms. Our scheme is a suitable candidate for interactive HD/UHD video applications that are expected to operate in real-time and power-constrained scenarios. © 2019 Association for Computing Machinery.",Inter-layer correlation; Low-complexity; Mode decision; Scalable video coding; SHVC,Codes (symbols); Digital television; Efficiency; Forestry; Interactive computer systems; Motion analysis; Multimedia systems; Scalable video coding; Security systems; Signal encoding; Textures; Video recording; Compression efficiency; High-efficiency video coding; Inter-layers; Interactive multimedia; Low-complexity; Mode Decision; SHVC; Temporal correlations; Video signal processing
Expression robust 3D facial landmarking via progressive coarse-to-fine tuning,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062374158&doi=10.1145%2f3282833&partnerID=40&md5=2c1494b57e318f1d4736a4d5a60625d4,"Facial landmarking is a fundamental task in automatic machine-based face analysis. The majority of existing techniques for such a problem are based on 2D images; however, they suffer from illumination and pose variations that may largely degrade landmarking performance. The emergence of 3D data theoretically provides an alternative to overcome these weaknesses in the 2D domain. This article proposes a novel approach to 3D facial landmarking, which combines both the advantages of feature-based methods as well as model-based ones in a progressive three-stage coarse-to-fine manner (initial, intermediate, and fine stages). For the initial stage, a few fiducial landmarks (i.e., the nose tip and two inner eye corners) are robustly detected through curvature analysis, and these points are further exploited to initialize the subsequent stage. For the intermediate stage, a statistical model is learned in the feature space of three normal components of the facial point-cloud rather than the smooth original coordinates, namely Active Normal Model (ANM). For the fine stage, cascaded regression is employed to locally refine the landmarks according to their geometry attributes. The proposed approach can accurately localize dozens of fiducial points on each 3D face scan, greatly surpassing the feature-based ones, and it also improves the state of the art of the model-based ones in two aspects: sensitivity to initialization and deficiency in discrimination. The proposed method is evaluated on the BU-3DFE, Bosphorus, and BU-4DFE databases, and competitive results are achieved in comparison with counterparts in the literature, clearly demonstrating its effectiveness. © 2019 Association for Computing Machinery.",3D facial landmarking; Active normal model; Cascaded regression; Coarse-to-fine tuning; Curvature analysis,Edge detection; Cascaded regression; Coarse to fine; Curvature analysis; Facial landmarking; Normal model; 3D modeling
Efficient QoE-aware scheme for video quality switching operations in dynamic adaptive streaming,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062369659&doi=10.1145%2f3269494&partnerID=40&md5=82170f231bcad777630cb8cb445b6be6,"Dynamic Adaptive Streaming over HTTP (DASH) is a popular over-the-top video content distribution technique that adapts the streaming session according to the user's network condition typically in terms of downlink bandwidth. This video quality adaptation can be achieved by scaling the frame quality, spatial resolution or frame rate. Despite the flexibility on the video quality scaling methods, each of these quality scaling dimensions has varying effects on the Quality of Experience (QoE) for end users. Furthermore, in video streaming, the changes in motion over time along with the scaling method employed have an influence on QoE, hence the need to carefully tailor scaling methods to suit streaming applications and content type. In this work, we investigate an intelligent DASH approach for the latest video coding standard H.265 and propose a heuristic QoE-aware cost-efficient adaptation scheme that does not switch unnecessarily to the highest quality level but rather stays temporarily at an intermediate quality level in certain streaming scenarios. Such an approach achieves a comparable and consistent level of quality under impaired network conditions as commonly found in Internet and mobile networks while reducing bandwidth requirements and quality switching overhead. The rationale is based on our empirical experiments, which show that an increase in bitrate does not necessarily mean noticeable improvement in QoE. Furthermore, our work demonstrates that the Signal-to-Noise Ratio (SNR) and the spatial resolution scalability types are the best fit for our proposed algorithm. Finally, we demonstrate an innovative interaction between quality scaling methods and the polarity of switching operations. The proposed QoE-aware scheme is implemented and empirical results show that it is able to reduce bandwidth requirements by up to 41% whilst achieving equivalent QoE compared with a representative DASH reference implementation. © 2019 Association for Computing Machinery.",Adaptation algorithm; DASH; QoE-aware; Quality scaling dimension,Bandwidth; HTTP; Image coding; Image resolution; Quality of service; Signal to noise ratio; Adaptation algorithms; DASH; Dynamic Adaptive Streaming over HTTP; QoE-aware; Quality of experience (QoE); Quality scaling; Reference implementation; Streaming applications; Video streaming
Deep cross-modal correlation learning for audio and lyrics in music retrieval,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062369607&doi=10.1145%2f3281746&partnerID=40&md5=2ead30125c6e18f5ce359e71000bb1f9,"Deep cross-modal learning has successfully demonstrated excellent performance in cross-modal multimedia retrieval, with the aim of learning joint representations between different data modalities. Unfortunately, little research focuses on cross-modal correlation learning where temporal structures of different data modalities, such as audio and lyrics, should be taken into account. Stemming from the characteristic of temporal structures of music in nature, we are motivated to learn the deep sequential correlation between audio and lyrics. In this work, we propose a deep cross-modal correlation learning architecture involving two-branch deep neural networks for audio modality and text modality (lyrics). Data in different modalities are converted to the same canonical space where intermodal canonical correlation analysis is utilized as an objective function to calculate the similarity of temporal structures. This is the first study that uses deep architectures for learning the temporal correlation between audio and lyrics. A pretrained Doc2Vec model followed by fully connected layers is used to represent lyrics. Two significant contributions are made in the audio branch, as follows: (i) We propose an end-to-end network to learn cross-modal correlation between audio and lyrics, where feature extraction and correlation learning are simultaneously performed and joint representation is learned by considering temporal structures. (ii) And, as for feature extraction, we further represent an audio signal by a short sequence of local summaries (VGG16 features) and apply a recurrent neural network to compute a compact feature that better learns the temporal structures of music audio. Experimental results, using audio to retrieve lyrics or using lyrics to retrieve audio, verify the effectiveness of the proposed deep correlation learning architectures in cross-modal music retrieval. © 2019 Association for Computing Machinery.",Convolutional neural networks; Correlation learning between audio and lyrics; Cross-modal music retrieval; Deep cross-modal models; Music knowledge discovery,Deep neural networks; Extraction; Feature extraction; Network architecture; Recurrent neural networks; Canonical correlation analysis; Convolutional neural network; Cross-modal; Cross-modal correlation; Learning architectures; Music retrieval; Sequential correlations; Temporal correlations; Audio acoustics
Cm-GANS: Cross-modal Generative Adversarial Networks for Common Representation Learning,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062334594&doi=10.1145%2f3284750&partnerID=40&md5=34a444d8addce3795719fd9ece098fd1,"It is known that the inconsistent distributions and representations of different modalities, such as image and text, cause the heterogeneity gap, which makes it very challenging to correlate heterogeneous data and measure their similarities. Recently, generative adversarial networks (GANs) have been proposed and have shown their strong ability to model data distribution and learn discriminative representation. It has also been shown that adversarial learning can be fully exploited to learn discriminative common representations for bridging the heterogeneity gap. Inspired by this, we aim to effectively correlate large-scale heterogeneous data of different modalities with the power of GANs to model cross-modal joint distribution. In this article, we propose Cross-modal Generative Adversarial Networks (CM-GANs) with the following contributions. First, a cross-modal GAN architecture is proposed to model joint distribution over the data of different modalities. The inter-modality and intra-modality correlation can be explored simultaneously in generative and discriminative models. Both compete with each other to promote cross-modal correlation learning. Second, the cross-modal convolutional autoencoders with weight-sharing constraint are proposed to form the generative model. They not only exploit the cross-modal correlation for learning the common representations but also preserve reconstruction information for capturing the semantic consistency within each modality. Third, a cross-modal adversarial training mechanism is proposed, which uses two kinds of discriminative models to simultaneously conduct intra-modality and inter-modality discrimination. They can mutually boost to make the generated common representations more discriminative by the adversarial training process. In summary, our proposed CM-GAN approach can use GANs to perform cross-modal common representation learning by which the heterogeneous data can be effectively correlated. Extensive experiments are conducted to verify the performance of CM-GANs on cross-modal retrieval compared with 13 state-of-the-art methods on 4 cross-modal datasets. © 2019 Association for Computing Machinery.",Common representation learning; Cross-modal adversarial mechanism; Cross-modal retrieval; Generative adversarial network,Computer networks; Adversarial learning; Adversarial networks; Common representation learning; Cross-modal; Cross-modal correlation; Discriminative models; Semantic consistency; State-of-the-art methods; Semantics
HTTP/2-based Frame Discarding for Low-Latency Adaptive Video Streaming,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062340628&doi=10.1145%2f3280854&partnerID=40&md5=b79e60ebfa6acbac56a519c35637571e,"In this article, we propose video delivery schemes insuring around 1s delivery latency with Dynamic Adaptive Streaming over HTTP (DASH), which is a standard version of HTTP Live Streaming (HLS), so as to benefit from the video representation switching between successive video segments. We also propose HTTP/2-based algorithms to apply video frame discarding policies inside a video segment when a selected DASH representation does not match with the available network resources. The current solutions with small buffer suffer from rebuffering events. Rebuffering not only impacts the Quality of Experience (QoE) but also increases the delivery delay between the displayed and the original video streams. In this work, we completely eliminate rebuffering events by developing optimal and practical video frame discarding algorithms to meet the 1s latency constraint. In all our algorithms, we request the video frames individually through HTTP/2 multiple streams, and we selectively drop the least meaningful video frames thanks to HTTP/2 stream resetting feature. Our simulations show that the proposed algorithms eliminate rebuffering while insuring an acceptable video quality with at least a Peak Signal to Noise Ratio (PSNR) of 35dB compared to 25dB of the basic First In First Out (FIFO) algorithm. We also quantify and qualify the resulting temporal distortion of the video segments per algorithm. An important number of missing video frames results in a temporal fluidity break known as video jitter. The displayed video looks like a series of snapshots. We show that both the optimal Integer Linear Program (ILP) and practical algorithms decrease the frequency and duration of the jitters. For example, practical algorithms reduce the number of crashed displayed videos (presenting one jitter longer than 1,350ms) with 22% compared to the basic FIFO algorithm. We also show that requesting video frames separately with HTTP/2 slightly increases the overhead from 4.34% to 5.76%. © 2019 Association for Computing Machinery.",DASH; HTTP/2; Low latency; Video delivery; Video frame discarding; Wireless networks,Communication channels (information theory); Image segmentation; Integer programming; Jitter; Quality of service; Scheduling algorithms; Signal to noise ratio; Video streaming; Wireless networks; DASH; Dynamic Adaptive Streaming over HTTP; Integer linear programs; Low latency; Peak signal to noise ratio; Quality of experience (QoE); Video delivery; Video frame; HTTP
Symmetrical residual connections for single image super-resolution,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062350417&doi=10.1145%2f3282445&partnerID=40&md5=bbadfe6032ced2edd12e0f7f5b0902be,"Single-image super-resolution (SISR) methods based on convolutional neural networks (CNN) have shown great potential in the literature. However, most deep CNN models don't have direct access to subsequent layers, seriously hindering the information flow. Furthermore, they fail to make full use of the hierarchical features from different low-level layers, thereby resulting in relatively low accuracy. In this article, we present a new SISR CNN, called SymSR, which incorporates symmetrical nested residual connections to improve both the accuracy and the execution speed. SymSR takes a larger image region for contextual spreading. It symmetrically combines multiple short paths for the forward propagation to improve the accuracy and for the backward propagation of gradient flow to accelerate the convergence speed. Extensive experiments based on open challenge datasets show the effectiveness of symmetrical residual connections. Compared with four other state-of-the-art super-resolution CNN methods, SymSR is superior in both accuracy and runtime. © 2019 Association for Computing Machinery.",Convolutional neural network; Residual networks; Single-image super-resolution; Vanishing gradients,Convolution; Neural networks; Backward propagation; Convergence speed; Convolutional neural network; Forward propagation; Hierarchical features; Information flows; Single images; Vanishing gradient; Optical resolving power
Photorealistic face completion with semantic parsing and face identity-preserving features,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062353357&doi=10.1145%2f3300940&partnerID=40&md5=8d11e0d36d883ed10d8e0d62cf80ed0b,"Tremendous progress on deep learning has shown exciting potential for a variety of face completion tasks. However, most learning-based methods are limited to handle general or structure specified face images (e.g., well-aligned faces). In this article, we propose a novel face completion algorithm, called Learning and Preserving Face Completion Network (LP-FCN), which simultaneously parses face images and extracts face identity-preserving (FIP) features. By tackling these two tasks in a mutually boosting way, the LP-FCN can guide an identity preserving inference and ensure pixel faithfulness of completed faces. In addition, we adopt a global discriminator and a local discriminator to distinguish real images from synthesized ones. By training with a combined identity preserving, semantic parsing and adversarial loss, the LP-FCN encourages the completion results to be semantically valid and visually consistent for more complicated image completion tasks. Experiments show that our approach obtains similar visual quality, but achieves better performance on unaligned faces completion and fine detailed synthesis against the state-of-the-art methods. © 2019 Association for Computing Machinery.",Adversarial loss; Deep learning; Face Completion; Face identity-preserving feature; Local details; Semantic parsing,Deep learning; Semantics; Completion algorithms; Face identity-preserving feature; Image completion; Learning-based methods; Local details; Semantic parsing; State-of-the-art methods; Visual qualities; Image processing
Understanding the dynamics of social interactions: A multi-modal multi-view approach,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062342616&doi=10.1145%2f3300937&partnerID=40&md5=fb981c9960f969cb8051f31b77084880,"In this article, we deal with the problem of understanding human-to-human interactions as a fundamental component of social events analysis. Inspired by the recent success of multi-modal visual data in many recognition tasks, we propose a novel approach to model dyadic interaction by means of features extracted from synchronized 3D skeleton coordinates, depth, and Red Green Blue (RGB) sequences. From skeleton data, we extract new view-invariant proxemic features, named Unified Proxemic Descriptor (UProD), which is able to incorporate intrinsic and extrinsic distances between two interacting subjects. A novel key frame selection method is introduced to identify salient instants of the interaction sequence based on the joints' energy. From Red Green Blue Depth (RGBD) videos, more holistic CNN features are extracted by applying an adaptive pre-trained Convolutional Neural Networks (CNNs) on optical flow frames. For better understanding the dynamics of interactions, we expand the boundaries of dyadic interactions analysis by proposing a fundamentally new modeling for non-treated problem aiming to discern the active from the passive interactor. Extensive experiments have been carried out on four multi-modal and multi-view interactions datasets. The experimental results demonstrate the superiority of our proposed techniques against the state-of-the-art approaches. © 2019 Association for Computing Machinery.",Active/passive subjects; CNN; Interaction recognition; Multi-modal data; RGBD; Skeleton,3D modeling; Modal analysis; Musculoskeletal system; Neural networks; Active/passive; Interaction recognition; Multi-modal data; RGBD; Skeleton; Data mining
Interactive search or sequential browsing? A detailed analysis of the video browser showdown 2018,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062354960&doi=10.1145%2f3295663&partnerID=40&md5=b5bba007ee7e6520171c956060314a0f,"This work summarizes the findings of the 7th iteration of the Video Browser Showdown (VBS) competition organized as a workshop at the 24th International Conference on Multimedia Modeling in Bangkok. The competition focuses on video retrieval scenarios in which the searched scenes were either previously observed or described by another person (i.e., an example shot is not available). During the event, nine teams competed with their video retrieval tools in providing access to a shared video collection with 600 hours of video content. Evaluation objectives, rules, scoring, tasks, and all participating tools are described in the article. In addition, we provide some insights into how the different teams interacted with their video browsers, which was made possible by a novel interaction logging mechanism introduced for this iteration of the VBS. The results collected at the VBS evaluation server confirm that searching for one particular scene in the collection when given a limited time is still a challenging task for many of the approaches that were showcased during the event. Given only a short textual description, finding the correct scene is even harder. In ad hoc search with multiple relevant scenes, the tools were mostly able to find at least one scene, whereas recall was the issue for many teams. The logs also reveal that even though recent exciting advances in machine learning narrow the classical semantic gap problem, user-centric interfaces are still required to mediate access to specific content. Finally, open challenges and lessons learned are presented for future VBS events. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Content-based methods; Evaluation campaigns; Interactive video retrieval; Video browsing,Learning systems; Semantics; Classical semantics; Content-based methods; Evaluation campaigns; Interactive video; Multimedia modeling; Textual description; User centric interface; Video browsing; Iterative methods
Modality-invariant image-text embedding for image-sentence matching,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062321489&doi=10.1145%2f3300939&partnerID=40&md5=261936221d2d07d154c217101d98f9e0,"Performing direct matching among different modalities (like image and text) can benefit many tasks in computer vision, multimedia, information retrieval, and information fusion. Most of existing works focus on class-level image-text matching, called cross-modal retrieval, which attempts to propose a uniform model for matching images with all types of texts, for example, tags, sentences, and articles (long texts). Although cross-model retrieval alleviates the heterogeneous gap among visual and textual information, it can provide only a rough correspondence between two modalities. In this article, we propose a more precise image-text embedding method, image-sentence matching, which can provide heterogeneous matching in the instance level. The key issue for image-text embedding is how to make the distributions of the two modalities consistent in the embedding space. To address this problem, some previous works on the cross-model retrieval task have attempted to pull close their distributions by employing adversarial learning. However, the effectiveness of adversarial learning on image-sentence matching has not been proved and there is still not an effective method. Inspired by previous works, we propose to learn a modality-invariant image-text embedding for image-sentence matching by involving adversarial learning. On top of the triplet loss-based baseline, we design a modality classification network with an adversarial loss, which classifies an embedding into either the image or text modality. In addition, the multi-stage training procedure is carefully designed so that the proposed network not only imposes the image-text similarity constraints by ground-truth labels, but also enforces the image and text embedding distributions to be similar by adversarial learning. Experiments on two public datasets (Flickr30k and MSCOCO) demonstrate that our method yields stable accuracy improvement over the baseline model and that our results compare favorably to the state-of-the-art methods. © 2019 Association for Computing Machinery.",Adversarial learning; Image-text embedding; Retrieval,Embeddings; Text processing; Accuracy Improvement; Adversarial learning; Classification networks; Image texts; Retrieval; State-of-the-art methods; Textual information; Training procedures; Information retrieval
Applying deep learning for epilepsy seizure detection and brain mapping visualization,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062208356&doi=10.1145%2f3241056&partnerID=40&md5=5d08d1ef95a5d702d2368f21031e0e73,"Deep Convolutional Neural Network (CNN) has achieved remarkable results in computer vision tasks for end-to-end learning. We evaluate here the power of a deep CNN to learn robust features from raw Electroencephalogram (EEG) data to detect seizures. Seizures are hard to detect, as they vary both inter- and intra-patient. In this article, we use a deep CNN model for seizure detection task on an open-access EEG epilepsy dataset collected at the Boston Children's Hospital. Our deep learning model is able to extract spectral, temporal features from EEG epilepsy data and use them to learn the general structure of a seizure that is less sensitive to variations. For cross-patient EEG data, our method produced an overall sensitivity of 90.00%, specificity of 91.65%, and overall accuracy of 98.05% for the whole dataset of 23 patients. The system can detect seizures with an accuracy of 99.46%. Thus, it can be used as an excellent cross-patient seizure classifier. The results show that our model performs better than the previous state-of-the-art models for patient-specific and cross-patient seizure detection task. The method gave an overall accuracy of 99.65% for patient-specific data. The system can also visualize the special orientation of band power features. We use correlation maps to relate spectral amplitude features to the output in the form of images. By using the results from our deep learning model, this visualization method can be used as an effective multimedia tool for producing quick and relevant brain mapping images that can be used by medical experts for further investigation. © 2019 Association for Computing Machinery.",Deep learning; Electroencephalogram; Epileptic seizure detection,Deep learning; Deep neural networks; Electroencephalography; Mapping; Neural networks; Neurology; Open access; Visualization; Convolutional neural network; Electroencephalogram (EEG) datum; Epileptic seizure detection; General structures; Overall accuracies; Special orientation; Spectral amplitude; Visualization method; Brain mapping
Visual content recognition by exploiting semantic feature map with atention and multi-task learning,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061200742&doi=10.1145%2f3231739&partnerID=40&md5=da0961f097c07c3225eb55e9439fddb8,"Recent studies have shown that spatial relationships among objects are very important for visual recognition, since they can provide rich clues on object contexts within the images. In this article, we introduce a novel method to learn the Semantic Feature Map (SFM) with attention-based deep neural networks for image and video classifcation in an end-to-end manner, aiming to explicitly model the spatial object contexts within the images. In particular, we explicitly apply the designed gate units to the extracted object features for important objects selection and noise removal. These selected object features are then organized into the proposed SFM, which is a compact and discriminative representation with the spatial information among objects preserved. Finally, we employ either Fully Convolutional Networks (FCN) or Long-Short Term Memory (LSTM) as the classifers on top of the SFM for content recognition. A novel multi-task learning framework with image classifcation loss, object localization loss, and grid labeling loss are also introduced to help better learn the model parameters. We conduct extensive evaluations and comparative studies to verify the effectiveness of the proposed approach on Pascal VOC 2007/2012 and MS-COCO benchmarks for image classifcation. In addition, the experimental results also show that the SFMs learned from the image domain can be successfully transferred to CCV and FCVID benchmarks for video classifcation. © 2019 Association for Computing Machinery.",Contextual fusion; Image classifcation; Image representation; Video classifcation,Deep neural networks; Image classification; Semantics; Classifcation; Comparative studies; Content recognition; Convolutional networks; Image representations; Object localization; Spatial informations; Spatial relationships; Long short-term memory
Deep learning-based multimedia analytics: A review,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061214809&doi=10.1145%2f3279952&partnerID=40&md5=5c6f4ff4021aa49ea9b97452c065bcbd,"The multimedia community has witnessed the rise of deep learning-based techniques in analyzing multimedia content more effectively. In the past decade, the convergence of deep-learning and multimedia analytics has boosted the performance of several traditional tasks, such as classification, detection, and regression, and has also fundamentally changed the landscape of several relatively new areas, such as semantic segmentation, captioning, and content generation. This article aims to review the development path of major tasks in multimedia analytics and take a look into future directions. We start by summarizing the fundamental deep techniques related to multimedia analytics, especially in the visual domain, and then review representative high-level tasks powered by recent advances. Moreover, the performance review of popular benchmarks gives a pathway to technology advancement and helps identify both milestone works and future directions. © 2019 Association for Computing Machinery.",Deep learning; Multimedia analytics; Neural networks,Benchmarking; Neural networks; Semantics; Development path; Multimedia analytics; Multimedia community; Multimedia contents; Performance reviews; Semantic segmentation; Technology advancement; Deep learning
Dense 3D-convolutional neural network for person re-identification in videos,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061215758&doi=10.1145%2f3231741&partnerID=40&md5=1a8c5915e3d7aea2caa71f54508e8c30,"Person re-identification aims at identifying a certain pedestrian across non-overlapping multi-camera networks in different time and places. Existing person re-identification approaches mainly focus on matching pedestrians on images; however, little attention has been paid to re-identify pedestrians in videos. Compared to images, video clips contain motion patterns of pedestrians, which is crucial to person re-identification. Moreover, consecutive video frames present pedestrian appearance with different body poses and from different viewpoints, providing valuable information toward addressing the challenge of pose variation, occlusion, and viewpoint change, and so on. In this article, we propose a Dense 3D-Convolutional Network (D3DNet) to jointly learn spatio-temporal and appearance representation for person re-identification in videos. The D3DNet consists of multiple three-dimensional (3D) dense blocks and transition layers. The 3D dense blocks enlarge the receptive fields of visual neurons in both spatial and temporal dimensions, leading to discriminative appearance representation as well as short-term and long-term motion patterns of pedestrians without the requirement of an additional motion estimation module. Moreover, we formulate a loss function consisting of an identification loss and a center loss to minimize intra-class variance and maximize inter-class variance simultaneously, toward addressing the challenge of large intra-class variance and small inter-class variance. Extensive experiments on two real-world video datasets of person identification, i.e., MARS and iLIDS-VID, have shown the effectiveness of the proposed approach. © 2019 Association for Computing Machinery.",Deep learning; Network structure; Person re-identification,Convolution; Deep learning; Neural networks; Time and motion study; Video cameras; Convolutional networks; Convolutional neural network; Multi-camera networks; Network structures; Person identification; Person re identifications; Temporal dimensions; Threedimensional (3-d); Motion estimation
Deep patch representations with shared codebook for scene classification,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061214934&doi=10.1145%2f3231738&partnerID=40&md5=b2c8dc271fc075869af03433d49f53e7,"Scene classification is a challenging problem. Compared with object images, scene images are more abstract, as they are composed of objects. Object and scene images have different characteristics with different scales and composition structures. How to effectively integrate the local mid-level semantic representations including both object and scene concepts needs to be investigated, which is an important aspect for scene classification. In this article, the idea of a sharing codebook is introduced by organically integrating deep learning, concept feature, and local feature encoding techniques. More specifically, the shared local feature codebook is generated from the combined ImageNet1K and Places365 concepts (Mixed1365) using convolutional neural networks. As the Mixed1365 features cover all the semantic information including both object and scene concepts, we can extract a shared codebook from the Mixed1365 features, which only contain a subset of the whole 1,365 concepts with the same codebook size. The shared codebook can not only provide complementary representations without additional codebook training but also be adaptively extracted toward different scene classification tasks. A method of fusing the encoded features with both the original codebook and the shared codebook is proposed for scene classification. In this way, more comprehensive and representative image features can be generated for classification. Extensive experimentations conducted on two public datasets validate the effectiveness of the proposed method. Besides, some useful observations are also revealed to show the advantage of shared codebook. © 2019 Association for Computing Machinery.",Convolutional neural network; Feature encoding; Scene classification; Shared codebook,Abstracting; Convolution; Deep learning; Encoding (symbols); Neural networks; Semantics; Signal encoding; Codebook training; Codebooks; Composition structure; Convolutional neural network; Encoding techniques; Scene classification; Semantic information; Semantic representation; Classification (of information)
"Orchestrating caching, transcoding and request routing for adaptive video streaming over ICN",2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061216439&doi=10.1145%2f3289184&partnerID=40&md5=22557b97abb5f32eb6db09a0674224d8,"Information-centric networking (ICN) has been touted as a revolutionary solution for the future of the Internet, which will be dominated by video traffic. This work investigates the challenge of distributing video content of adaptive bitrate (ABR) over ICN. In particular, we use the in-network caching capability of ICN routers to serve users; in addition, with the help of named function, we enable ICN routers to transcode videos to lower-bitrate versions to improve the cache hit ratio. Mathematically, we formulate this design challenge into a constrained optimization problem, which aims to maximize the cache hit ratio for service providers and minimize the service delay for endusers. We design a two-step iterative algorithm to find the optimum. First, given a content management scheme, we minimize the service delay via optimally configuring the routing scheme. Second, we maximize the cache hits for a given routing policy. Finally, we rigorously prove its convergence. Through extensive simulations, we verify the convergence and the performance gains over other algorithms. We also find that more resources should be allocated to ICN routers with a heavier request rate, and the routing scheme favors the shortest path to schedule more traffic. © 2019 Copyright held by the owner/author(s).",Adaptive video streaming; Information Centric Networking (ICN); Partial caching; Video transcoding,Constrained optimization; Iterative methods; Network routing; Routing protocols; Adaptive video streaming; Constrained optimi-zation problems; Content management; Extensive simulations; Information-centric networkings (ICN); Iterative algorithm; Partial caching; Video-transcoding; Video streaming
Discovering latent topics by Gaussian latent dirichlet allocation and spectral clustering,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061213078&doi=10.1145%2f3290047&partnerID=40&md5=741e9f7add25c00e0b45fad38e03c927,"Today, diversifying the retrieval results of a certain query will improve customers' search efficiency. Showing the multiple aspects of information provides users an overview of the object, which helps them fast target their demands. To discover aspects, research focuses on generating image clusters from initially retrieved results. As an effective approach, latent Dirichlet allocation (LDA) has been proved to have good performance on discovering high-level topics. However, traditional LDA is designed to process textual words, and it needs the input as discrete data. When we apply this algorithm to process continuous visual images, a common solution is to quantize the continuous features into discrete form by a bag-of-visual-words algorithm. During this process, quantization error will lead to information that inevitably is lost. To construct a topic model with complete visual information, this work applies Gaussian latent Dirichlet allocation (GLDA) on the diversity issue of image retrieval. In this model, traditional multinomial distribution is substituted with Gaussian distribution to model continuous visual features. In addition, we propose a two-phase spectral clustering strategy, called dual spectral clustering, to generate clusters from region level to image level. The experiments on the challenging landmarks of the DIV400 database show that our proposal improves relevance and diversity by about 10% compared to traditional topic models. © 2019 Association for Computing Machinery.",Diversity; Gaussian; Image retrieval; Latent Dirichlet allocation; Spectral clustering,Clustering algorithms; Gaussian distribution; Query processing; Statistics; Bag-of-visual-words; Diversity; Effective approaches; Gaussians; Latent Dirichlet allocation; Latent dirichlet allocations; Multinomial distributions; Spectral clustering; Image retrieval
Modeling dyadic and group impressions with intermodal and interperson features,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061215420&doi=10.1145%2f3265754&partnerID=40&md5=57463f989a6cfee5382c1acd8ccb3b2c,"This article proposes a novel feature-extraction framework for inferring impression personality traits, emergent leadership skills, communicative competence, and hiring decisions. The proposed framework extracts multimodal features, describing each participant's nonverbal activities. It captures intermodal and interperson relationships in interactions and captures how the target interactor generates nonverbal behavior when other interactors also generate nonverbal behavior. The intermodal and interperson patterns are identified as frequent co-occurring events based on clustering from multimodal sequences. The proposed framework is applied to the SONVB corpus, which is an audiovisual dataset collected from dyadic job interviews, and the ELEA audiovisual data corpus, which is a dataset collected from group meetings. We evaluate the framework on a binary classification task involving 15 impression variables from the two data corpora. The experimental results show that the model trained with co-occurrence features is more accurate than previous models for 14 out of 15 traits. © 2019 Association for Computing Machinery.",Impression; Inference; Multimodal interaction; Personality trait,Computer networks; Binary classification; Co-occurrence features; Communicative competences; Impression; Inference; Multi-Modal Interactions; Multimodal features; Personality traits
Convolutional atention networks for scene text recognition,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061216011&doi=10.1145%2f3231737&partnerID=40&md5=f8b49cec9bb8c74e072699e70bfa3fc0,"In this article, we present Convoluitional Attention Networks (CAN) for unconstrained scene text recognition. Recent dominant approaches for scene text recognition are mainly based on Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN), where the CNN encodes images and the RNN generates character sequences. Our CAN is different from these methods; our CAN is completely built on CNN and includes an attention mechanism. The distinctive characteristics of our method include (i) CAN follows encoder-decoder architecture, in which the encoder is a deep two-dimensional CNN and the decoder is a one-dimensional CNN; (ii) the attention mechanism is applied in every convolutional layer of the decoder, and we propose a novel spatial attention method using average pooling; and (iii) position embeddings are equipped in both a spatial encoder and a sequence decoder to give our networks a sense of location. We conduct experiments on standard datasets for scene text recognition, including Street View Text, IIIT5K, and ICDAR datasets. The experimental results validate the effectiveness of different components and show that our convolutional-based method achieves state-of-the-art or competitive performance over prior works, even without the use of RNN. © 2019 Association for Computing Machinery.",Attention model; Convolutional neural networks; Multilevel supervised information; Text detection; Text recognition,Convolution; Decoding; Recurrent neural networks; Signal encoding; Attention model; Convolutional neural network; Multilevel supervised information; Text detection; Text recognition; Character recognition
Structure-aware deep learning for product image classification,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061198077&doi=10.1145%2f3231742&partnerID=40&md5=232489cdca7ac708cee20265ebefa071,"Automatic product image classification is a task of crucial importance with respect to the management of online retailers. Motivated by recent advancements of deep Convolutional Neural Networks (CNN) on image classification, in this work we revisit the problem in the context of product images with the existence of a predefined categorical hierarchy and attributes, aiming to leverage the hierarchy and attributes to improve classification accuracy. With these structure-aware clues, we argue that more advanced deep models could be developed beyond the flat one-versus-all classification performed by conventional CNNs. To this end, novel efforts of this work include a salient-sensitive CNN that gazes into the product foreground by inserting a dedicated spatial attention module; a multiclass regression-based refinement that is expected to predict more accurately by merging prediction scores from multiple preceding CNNs, each corresponding to a distinct classifier in the hierarchy; and a multitask deep learning architecture that effectively explores correlations among categories and attributes for categorical label prediction. Experimental results on nearly 1 million real-world product images basically validate the effectiveness of the proposed efforts individually and jointly, from which performance gains are observed. © 2019 Association for Computing Machinery.",Category hierarchy; Convolutional neural network; Image classification; Multi-class regression; Multi-task learning,Convolution; Deep neural networks; Forecasting; Image enhancement; Neural networks; Categorical hierarchy; Category hierarchy; Classification accuracy; Convolutional neural network; Learning architectures; Multi-class regression; Multitask learning; Product image classifications; Image classification
Editorial to special issue on deep learning for intelligent multimedia analytics,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061187973&doi=10.1145%2f3292059&partnerID=40&md5=1472a739b67330b39d376bf345a2aea8,[No abstract available],,
Image captioning with visual-semantic double atention,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061196844&doi=10.1145%2f3292058&partnerID=40&md5=d7ff856a013c0452e1b6666bb1736f5a,"In this article, we propose a novel Visual-Semantic Double Attention (VSDA) model for image captioning. In our approach, VSDA consists of two parts: a modified visual attention model is used to extract sub-region image features, then a new SEmantic Attention (SEA) model is proposed to distill semantic features. Traditional attribute-based models always neglect the distinctive importance of each attribute word and fuse all of them into recurrent neural networks, resulting in abundant irrelevant semantic features. In contrast, at each timestep, our model selects the most relevant word that aligns with current context. In other words, the real power of VSDA lies in the ability of not only leveraging semantic features but also eliminating the influence of irrelevant attribute words to make the semantic guidance more precise. Furthermore, our approach solves the problem that visual attention models cannot boost generating non-visual words. Considering that visual and semantic features are complementary to each other, our model can leverage both of them to strengthen the generations of visual and non-visual words. Extensive experiments are conducted on famous datasets: MS COCO and Flickr30k. The results show that VSDA outperforms other methods and achieves promising performance. © 2019 Association for Computing Machinery.",Image captioning; Semantic attention; Visual-semantic double attention,Behavioral research; Recurrent neural networks; Attribute-based; Image captioning; Image features; Non visuals; Semantic features; Sub-regions; Visual attention model; Visual semantics; Semantics
Reconstructing 3D face models by incremental aggregation and refinement of depth frames,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061189013&doi=10.1145%2f3287309&partnerID=40&md5=b14174ca209c1c0dde3f8e3b04c0bfad,"Face recognition from two-dimensional (2D) still images and videos is quite successful even with ""in the wild"" conditions. Instead, less consolidated results are available for the cases in which face data come from non-conventional cameras, such as infrared or depth. In this article, we investigate this latter scenario assuming that a low-resolution depth camera is used to perform face recognition in an uncooperative context. To this end, we propose, first, to automatically select a set of frames from the depth sequence of the camera because they provide a good view of the face in terms of pose and distance. Then, we design a progressive refinement approach to reconstruct a higher-resolution model from the selected low-resolution frames. This process accounts for the anisotropic error of the existing points in the current 3D model and the points in a newly acquired frame so that the refinement step can progressively adjust the point positions in the model using a Kalman-like estimation. The quality of the reconstructed model is evaluated by considering the error between the reconstructed models and their corresponding high-resolution scans used as ground truth. In addition, we performed face recognition using the reconstructed models as probes against a gallery of reconstructed models and a gallery with high-resolution scans. The obtained results confirm the possibility to effectively use the reconstructed models for the face recognition task. © 2019 Copyright held by the owner/author(s).",3D face recognition; 3D reconstruction; Anisotropic error; Depth data,Anisotropy; Cameras; Errors; Face recognition; Image reconstruction; 3D face recognition; 3D reconstruction; Anisotropic errors; Conventional camera; Depth data; High resolution scans; Progressive refinement; Two Dimensional (2 D); Three dimensional computer graphics
Virtual portraitist: An intelligent tool for taking well-posed selfies,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061174524&doi=10.1145%2f3288760&partnerID=40&md5=77297614cbdf5140f3d76446e4388640,"Smart photography carries the promise of quality improvement and functionality extension in making aesthetically appealing pictures. In this article, we focus on self-portrait photographs and introduce newmethods that guide a user in how to best pose while taking a selfie. While most of the current solutions use a post processing procedure to beautify a picture, the developed tool enables a novel function of recommending a good look before the photo is captured. Given an input face image, the tool automatically estimates the pose-based aesthetic score, finds the most attractive angle of the face, and suggests how the pose should be adjusted. The recommendation results are determined adaptively to the appearance and initial pose of the input face. We apply a data mining approach to find distinctive, frequent itemsets and association rules from online profile pictures, upon which the aesthetic estimation and pose recommendation methods are developed. A simulated and a real image set are used for experimental evaluation. The results show the proposed aesthetic estimation method can effectively select user-favorable photos. Moreover, the recommendation performance for the vertical adjustment is moderately related to the degree of conformity among the professional photographers' recommendations. This study echoes the trend of instant photo sharing, in which a user takes a picture and then immediately shares it on a social network without engaging in tedious editing. © 2019 Association for Computing Machinery.",Computational aesthetics; Pose recommendation; Selfie; Smart photography,Data mining; Photography; Computational aesthetics; Experimental evaluation; Pose recommendation; Post-processing procedure; Professional photographers; Recommendation methods; Recommendation performance; Selfie; Image enhancement
Personalized emotion recognition by personality-aware high-order learning of physiological signals,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061193683&doi=10.1145%2f3233184&partnerID=40&md5=afeb208e2feca9bcc252f982a368ac39,"Due to the subjective responses of different subjects to physical stimuli, emotion recognition methodologies from physiological signals are increasingly becoming personalized. Existing works mainly focused on modeling the involved physiological corpus of each subject, without considering the psychological factors, such as interest and personality. The latent correlation among different subjects has also been rarely examined. In this article, we propose to investigate the influence of personality on emotional behavior in a hypergraph learning framework. Assuming that each vertex is a compound tuple (subject, stimuli), multi-modal hyper-graphs can be constructed based on the personality correlation among different subjects and on the physiological correlation among corresponding stimuli. To reveal the different importance of vertices, hyperedges, and modalities, we learn the weights for each of them. As the hypergraphs connect different subjects on the compound vertices, the emotions of multiple subjects can be simultaneously recognized. In this way, the constructed hypergraphs are vertex-weighted multi-modal multi-task ones. The estimated factors, referred to as emotion relevance, are employed for emotion recognition. We carry out extensive experiments on the ASCERTAIN dataset and the results demonstrate the superiority of the proposed method, as compared to the state-of-the-art emotion recognition approaches. © 2019 Association for Computing Machinery.",Hypergraph learning; Multi-modal fusion; Personality-sensitive learning; Personalized emotion recognition; Physiological signal analysis,Biomedical signal processing; Physiological models; Physiology; Speech recognition; Hypergraph; Multi-modal fusion; Personality-sensitive learning; Personalized emotion recognition; Physiological signals; Graph theory
Deep semantic mapping for heterogeneous multimedia transfer learning using co-occurrence data,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061195348&doi=10.1145%2f3241055&partnerID=40&md5=b09c1a43d3fc64b750090090d62e8ec2,"Transfer learning, which focuses on finding a favorable representation for instances of different domains based on auxiliary data, can mitigate the divergence between domains through knowledge transfer. Recently, increasing efforts on transfer learning have employed deep neural networks (DNN) to learn more robust and higher level feature representations to better tackle cross-media disparities. However, only a few articles consider the correction and semantic matching between multi-layer heterogeneous domain networks. In this article, we propose a deep semantic mapping model for heterogeneous multimedia transfer learning (DHTL) using co-occurrence data. More specifically, we integrate the DNN with canonical correlation analysis (CCA) to derive a deep correlation subspace as the joint semantic representation for associating data across different domains. In the proposed DHTL, a multi-layer correlation matching network across domains is constructed, in which the CCA is combined to bridge each pair of domain-specific hidden layers. To train the network, a joint objective function is defined and the optimization processes are presented. When the deep semantic representation is achieved, the shared features of the source domain are transferred for task learning in the target domain. Extensive experiments for three multimedia recognition applications demonstrate that the proposed DHTL can effectively find deep semantic representations for heterogeneous domains, and it is superior to the several existing state-of-the-art methods for deep transfer learning. © 2019 Association for Computing Machinery.",Canonical correlation analysis; Deep neural networks; Deep semantic mapping; Heterogeneous multimedia; Transfer learning,Correlation methods; Knowledge management; Mapping; Natural language processing systems; Network layers; Semantics; Canonical correlation analysis; Feature representation; Heterogeneous multimedia; Joint objective function; Semantic mapping; Semantic representation; State-of-the-art methods; Transfer learning; Deep neural networks
"Special section on multimodal understanding of social, afective, and subjective atributes",2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061175280&doi=10.1145%2f3292061&partnerID=40&md5=13bd86c04490526de2a4cf5c2380900e,"Multimedia scientists have largely focused their research on the recognition of tangible properties of data such as objects and scenes. Recently, the field has started evolving toward the modeling of more complex properties. For example, the understanding of social, affective, and subjective attributes of visual data has attracted the attention of many research teams at the crossroads of computer vision, multimedia, and social sciences. These intangible attributes include, for example, visual beauty, video popularity, or user behavior. Multiple, diverse challenges arise when modeling such properties from multimedia data. The sections concern technical aspects such as reliable groundtruth collection, the effective learning of subjective properties, or the impact of context in subjective perception; see Refs. [2] and [3]. © 2019 Copyright held by the owner/author(s).",Multimodal data; Subjective attributes,Computer networks; Complex properties; Effective learning; Multi-modal data; Multimedia data; Special sections; Subjective attributes; Subjective perceptions; Technical aspects; Behavioral research
Cross-modality feature learning via convolutional autoencoder,2019,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061185622&doi=10.1145%2f3231740&partnerID=40&md5=c3bdcb9b2312a158284a637a400083dd,"Learning robust and representative features across multiple modalities has been a fundamental problem in machine learning and multimedia fields. In this article, we propose a novel MUltimodal Convolutional AutoEncoder (MUCAE) approach to learn representative features from visual and textual modalities. For each modality, we integrate the convolutional operation into an autoencoder framework to learn a joint representation from the original image and text content. We optimize the convolutional autoencoders of different modalities jointly by exploiting the correlation between the hidden representations from the convolutional autoencoders, in particular by minimizing both the reconstructing error of each modality and the correlation divergence between the hidden feature of different modalities. Compared to the conventional solutions relying on hand-crafted features, the proposed MUCAE approach encodes features from image pixels and text characters directly and produces more representative and robust features. We evaluate MUCAE on crossmedia retrieval as well as unimodal classification tasks over real-world large-scale multimedia databases. Experimental results have shown that MUCAE performs better than the state-of-the-arts methods. © 2019 Association for Computing Machinery.",Convolutional autoencoder; Cross modality; Feature learning,Classification (of information); Machine learning; Auto encoders; Classification tasks; Cross modality; Cross-media retrieval; Feature learning; Multimedia database; Multiple modalities; State of the art; Convolution
A hybrid approach for spatio-temporal validation of declarative multimedia documents,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061208363&doi=10.1145%2f3267127&partnerID=40&md5=0464d589f62e6d3dc10484dd331272c5,"Declarative multimedia documents represent the description of multimedia applications in terms of media items and relationships among them. Relationships specify how media items are dynamically arrangedintime and space during runtime. Although a declarative approach usually facilitates the authoring task, authors can still make mistakes due to incorrect use of language constructs or inconsistent or missing relationships in a document. In order to properly support multimedia application authoring, it is important to provide tools with validation capabilities. Document validation can indicate possible inconsistencies in a given document to an author so that it can be revised before deployment. Although very useful, multimedia validation tools are not often provided by authoring tools. This work proposes a multimedia validation approach that relies on a formal model called Simple Hypermedia Model (SHM). SHM is used for representing a document for the purpose of validation. An SHM document is validated using a hybrid approach based on two complementary techniques. The first one captures the document's spatio-temporal layout in terms of its state throughout its execution by means of a rewrite theory, and validation is performed through model-checking. The second one captures the document's layout in terms of intervals and event occurrences by means of Satisfiability Modulo Theories (SMT) formulas, and validation is performed through SMT solving. Due to different characteristics of both approaches, each validation technique complements the other in terms of expressiveness of SHM and tests to be checked. We briefly present validation tools that use our approach. They were evaluated with real NCL documents and by usability tests. © 2018 Association for Computing Machinery.",Interactive multimedia applications; Multimedia authoring; Multimedia document validation; Spatio-temporal validation,Interactive computer systems; Multimedia systems; Complementary techniques; Interactive multimedia; Multi-Media authoring; Multimedia applications; Multimedia documents; Satisfiability modulo Theories; Spatio temporal; Validation capability; Model checking
Unsupervised similarity learning through rank correlation and kNN sets,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061196963&doi=10.1145%2f3241053&partnerID=40&md5=8aa3ad06d7a906620146f8c2523c335a,"The increasing amount of multimedia data collections available today evinces the pressing need for methods capable of indexing and retrieving this content. Despite the continuous advances in multimedia features and representation models, to establish an effective measure for comparing different multimedia objects still remains a challenging task. While supervised and semi-supervised techniques made relevant advances on similarity learning tasks, scenarios where labeled data are non-existent require different strategies. In such situations, unsupervised learning has been established as a promising solution, capable of considering the contextual information and the dataset structure for computing new similarity/dissimilarity measures. This article extends a recent unsupervised learning algorithm that uses an iterative re-ranking strategy to take advantage of different k-Nearest Neighbors (kNN) sets and rank correlation measures. Two novel approaches are proposed for computing the kNN sets and their corresponding top-k lists. The proposed approaches were validated in conjunction with various rank correlation measures, yielding superior effectiveness results in comparison with previous works. In addition, we also evaluate the ability of the method in considering different multimedia objects, conducting an extensive experimental evaluation on various image and video datasets. © 2018 Association for Computing Machinery.",Content-based image retrieval; KNN sets; Rank correlation; Unsupervised learning,Content based retrieval; Iterative methods; Learning algorithms; Machine learning; Unsupervised learning; Content based image retrieval; Contextual information; Experimental evaluation; K nearest neighbor (KNN); KNN sets; Multimedia features; Rank correlation; Representation model; Nearest neighbor search
Robust electric network frequency estimation with rank reduction and linear prediction,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061208490&doi=10.1145%2f3241058&partnerID=40&md5=373548d5b6185c717ce8b2ab63e7508b,"This article deals with the problem of Electric Network Frequency (ENF) estimation where Signal to Noise Ratio (SNR) is an essential challenge. By exploiting the low-rank structure of the ENF signal from the audio spectrogram, we propose an approach based on robust principle component analysis to get rid of the interference from speech contents and some of the background noise, which in our case can be regarded as sparse in nature. Weighted linear prediction is enforced on the low-rank signal subspace to gain accurate ENF estimation. The performance of the proposed scheme is analyzed and evaluated as a function of SNR, and the Cramér-Rao Lower Bound (CRLB) is approached at an SNR level above-10 dB. Experiments on real datasets have demonstrated the advantages of the proposed method over state-of-the-art work in terms of estimation accuracy. Specifically, the proposed scheme can effectively capture the ENF fluctuations along the time axis using small numbers of signal observations while preserving sufficient frequency precision. © 2018 Association for Computing Machinery.",Cramér-Rao lower bound; Electric network frequency; Low rank; Weighted linear prediction,Audio acoustics; Electric network parameters; Forecasting; Frequency estimation; Principal component analysis; Background noise; Linear prediction; Low rank; Lower bounds; Network frequency; Principle component analysis; Signal sub-space; State of the art; Signal to noise ratio
Joint head atribute classifier and domain-specific refinement networks for face alignment,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061185828&doi=10.1145%2f3241059&partnerID=40&md5=df9be3fb89a11c01ec2b1335ce861711,"In this article, a two-stage refinement network is proposed for facial landmarks detection on unconstrained conditions. Our model can be divided into two modules, namely the Head Attribude Classifier (HAC) module and the Domain-Specific Refinement (DSR) module. Given an input facial image, HAC adopts multi-task learning mechanism to detect the head pose and obtain an initial shape. Based on the obtained head pose, DSR designs three different CNN-based refinement networks trained by specific domain, respectively, and automatically selects the most approximate network for the landmarks refinement. Different from existing two-stage models, HAC combines head pose prediction with facial landmarks estimation to improve the accuracy of head pose prediction, as well as obtaining a robust initial shape. Moreover, an adaptive subnetwork training strategy applied in the DSR module can effectively solve the issue of traditional multi-view methodsthat animproperly selected sub-network may result inalignment failure. The extensive experimental results on two public datasets, AFLW and 300W, confirm the validity of our model. © 2018 Association for Computing Machinery.",Domain-specific; Face alignment; Head attribute classifier; Two-stage estimation,Computer networks; Domain specific; Face alignment; Facial images; Facial landmark; Multitask learning; Training strategy; Two stage model; Two-stage estimations
Image captioning via semantic guidance atention and consensus selection strategy,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061206372&doi=10.1145%2f3271485&partnerID=40&md5=edb1f2fb5a8a842f19bd4a393c3def01,"Recently, a series of attempts have incorporated spatial attention mechanisms into the task of image captioning, which achieves a remarkable improvement in the quality of generative captions. However, the traditional spatial attention mechanism adopts latent and delayed semantic representations to decide which area should be paid more attention to, resulting in inaccurate semantic guidance and the introduction of redundant information. In order to optimize the spatial attention mechanism, we propose the Semantic Guidance Attention (SGA) mechanism in this article. Specifically, SGA utilizes semantic word representations to provide an intuitive semantic guidance that focuses accurately on semantic-related regions. Moreover, we reduce the difficulty of generating fluent sentences by updating the attention information in time. At the same time, the beam search algorithm is widely used to predict words during sequence generation. This algorithm generates a sentence according to the probabilities of words, so it is easy to push out a generic sentence and discard some distinctive captions. In order to overcome this limitation, we design the Consensus Selection (CS) strategy to choose the most descriptive and informative caption, which is selected by the semantic similarity of captions instead of the probabilities of words. The consensus caption is determined by selecting the one with the highest cumulative semantic similarity with respect to the reference captions. Our proposed model (SGA-CS) is validated on Flickr30k and MSCOCO, which shows that SGA-CS outperforms state-of-the-art approaches. To our best knowledge, SGA-CS is the first attempt to jointly produce semantic attention guidance and select descriptive captions for image captioning tasks, achieving one of the best performance ratings among any cross-entropy training methods. © 2018 Association for Computing Machinery.",Beam search; Consensus selection strategy; Image captioning; Semantic guidance attention mechanism; Spatial attention mechanism,Image enhancement; Natural language processing systems; Attention mechanisms; Beam search; Consensus selection strategy; Image captioning; Spatial attention; Semantics
OmniArt: A large-scale artistic benchmark,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061213466&doi=10.1145%2f3273022&partnerID=40&md5=5f376376c4803f72e084ee48230337ce,"Baselines are the starting point of any quantitative multimedia research, and benchmarks are essential for pushing those baselines further. In this article, we present baselines for the artistic domain with a new benchmark dataset featuring over 2 million images with rich structured metadata dubbed OmniArt. OmniArt contains annotations for dozens of attribute types and features semantic context information through concepts, IconClass labels, color information, and (limited) object-level bounding boxes. For our dataset we establish and present baseline scores on multiple tasks such as artist attribution, creation period estimation, type, style, and school prediction. In addition to our metadata related experiments, we explore the color spaces of art through different types and evaluate a transfer learning object recognition pipeline. © 2018 Association for Computing Machinery.",Artistic data; Computer vision; Dataset; Multimedia,Computer vision; Metadata; Object recognition; Semantics; Artistic data; Benchmark datasets; Color information; Dataset; Multimedia; Multimedia research; Period estimation; Structured metadatas; Color
Thinking like a director: Film editing paterns for virtual cinematographic storytelling,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061179859&doi=10.1145%2f3241057&partnerID=40&md5=f8c13738416ef546b77a6d9ec760de6a,"This article introduces Film Editing Patterns (FEP), a language to formalize film editing practices and stylistic choices found in movies. FEP constructs are constraints, expressed over one or more shots from a movie sequence, that characterize changes in cinematographic visual properties, such as shot sizes, camera angles, or layout of actors on the screen. We present the vocabulary of the FEP language, introduce its usage in analyzing styles from annotated film data, and describe how it can support users in the creative design of film sequences in 3D. More specifically, (i) we define the FEP language, (ii) we present an application to craft filmic sequences from 3D animated scenes that uses FEPs as a high level mean to select cameras and perform cuts between cameras that follow best practices in cinema, and (iii) we evaluate the benefits of FEPs by performing user experiments in which professional filmmakers and amateurs had to create cinematographic sequences. The evaluation suggests that users generally appreciate the idea of FEPs, and that it can effectively help novice and medium experienced users in crafting film sequences with little training. © 2018 Association for Computing Machinery.",3D animation; Assisted creativity; Editing; Film storytelling; Virtual cinematography,Cameras; Motion pictures; 3D animation; Assisted creativity; Best practices; Creative design; Editing; User experiments; Virtual cinematography; Visual properties; High level languages
SKEPRID: Pose and illumination change-resistant skeleton-based person re-identification,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061200068&doi=10.1145%2f3243217&partnerID=40&md5=a47ff95bea041677e9f9b0b40fb77741,"                             Currently, the surveillance camera-based person re-identification is still challenging because of diverse factors such as people's changing poses and various illumination. The various poses make it hard to conduct feature matching across images, and the illumination changes make color-based features unreliable. In this article, we present SKEPRID,                             1                              a skeleton-based person re-identification method that handles strong pose and illumination changes jointly. To reduce the impacts of pose changes on re-identification, we estimate the joints' positions of a person based on the deep learning technique and thus make it possible to extract features on specific body parts with high accuracy. Based on the skeleton information, we design a set of local color comparison-based cloth-type features, which are resistant to various lighting conditions. Moreover, to better evaluate SKEPRID, we build the PO&LI                             2                              dataset, which has large pose and illumination diversity. Our experimental results show that SKEPRID outperforms state-of-the-art approaches in the case of strong pose and illumination variation.                          © 2018 Association for Computing Machinery.",Cloth-type features; Person re-identification; Pose recognition,Color matching; Deep learning; Large dataset; Security systems; Illumination changes; Learning techniques; Lighting conditions; Person re identifications; Pose and illumination variations; Pose recognition; State-of-the-art approach; Surveillance cameras; Musculoskeletal system
Collaborations on YouTube: From unsupervised detection to the impact on video and channel popularity,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061178146&doi=10.1145%2f3241054&partnerID=40&md5=1f5a0aa438b12be8448d4204a6963d70,"YouTube is the most popular platform for streaming of user-generated videos. Nowadays, professional YouTubers are organized in so-called multichannel networks (MCNs). These networks offer services such as brand deals, equipment, and strategic advice in exchange for a share of the YouTubers' revenues. A dominant strategy to gain more subscribers and, hence, revenue is collaborating with other YouTubers. Yet, collaborations on YouTube have not been studied in a detailed quantitative manner. To close this gap, first, we collect a YouTube dataset covering video statistics over 3 months for 7,942 channels. Second, we design a framework for collaboration detection given a previously unknown number of persons featured in YouTube videos. We denote this framework, for the detection and analysis of collaborations in YouTube videos using a Deep Neural Network (DNN)-based approach, as CATANA. Third, we analyze about 2.4 years of video content and use CATANA to answer research questions guiding YouTubers and MCNs for efficient collaboration strategies. Thereby, we focus on (1) collaboration frequency and partner selectivity, (2) the influence of MCNs on channel collaborations, (3) collaborating channel types, and (4) the impact of collaborations on video and channel popularity. Our results show that collaborations are in many cases significantly beneficial regarding viewers and newly attracted subscribers for both collaborating channels, often showing more than 100% popularity growth compared with noncollaboration videos. © 2018 Association for Computing Machinery.",Collaborations; Face clustering; Face detection; Face recognition; Multichannel networks; Unsupervised learning; YouTube,Deep neural networks; Unsupervised learning; Collaborations; Dominant strategy; Face clustering; Multi-channel network; Research questions; Unsupervised detection; User-generated video; YouTube; Face recognition
Unsupervised person re-identification: Clustering and fine-tuning,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056531584&doi=10.1145%2f3243316&partnerID=40&md5=55a551a74fc37aca5b11f99174289a80,"The superiority of deeply learned pedestrian representations has been reported in very recent literature of person re-identification (re-ID). In this article, we consider the more pragmatic issue of learning a deep feature with no or only a few labels. We propose a progressive unsupervised learning (PUL) method to transfer pretrained deep representations to unseen domains. Our method is easy to implement and can be viewed as an effective baseline for unsupervised re-ID feature learning. Specifically, PUL iterates between (1) pedestrian clustering and (2) fine-tuning of the convolutional neural network (CNN) to improve the initialization model trained on the irrelevant labeled dataset. Since the clustering results can be very noisy, we add a selection operation between the clustering and fine-tuning. At the beginning, when the model is weak, CNN is fine-tuned on a small amount of reliable examples that locate near to cluster centroids in the feature space. As the model becomes stronger, in subsequent iterations, more images are being adaptively selected as CNN training samples. Progressively, pedestrian clustering and the CNN model are improved simultaneously until algorithm convergence. This process is naturally formulated as self-paced learning. We then point out promising directions that may lead to further improvement. Extensive experiments on three large-scale re-ID datasets demonstrate that PUL outputs discriminative features that improve the re-ID accuracy. Our code has been released at https://github.com/hehefan/Unsupervised-Person-Re-identification-Clustering-and-Fine-tuning. © 2018 Association for Computing Machinery.",Clustering; Convolutional neural network; Large-scale person re-identification; Unsupervised learning,Clustering algorithms; Convolution; Machine learning; Neural networks; Unsupervised learning; Algorithm convergence; Cluster centroids; Clustering; Clustering results; Convolutional neural network; Discriminative features; Person re identifications; Self-paced learning; Large dataset
Probability model-based early merge mode decision for dependent views coding in 3D-HEVC,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054852386&doi=10.1145%2f3267128&partnerID=40&md5=9f770989a612a8d3f48c8ab2771ce5b8,"As a 3D extension to the High Efficiency Video Coding (HEVC) standard, 3D-HEVC was developed to improve the coding efficiency of multiview videos. It inherits the prediction modes from HEVC, yet both Motion Estimation (ME) and Disparity Estimation (DE) are required for dependent views coding. This improves coding efficiency at the cost of huge computational costs. In this article, an early Merge mode decision approach is proposed for dependent texture views and dependent depth maps coding in 3D-HEVC based on priori and posterior probability models. First, the priori probability model is established by exploiting the hierarchical and interview correlations from those previously encoded blocks. Second, the posterior probability model is built by using the Coded Block Flag (CBF) of the current coding block. Finally, the joint priori and posterior probability model is adopted to early terminate the Merge mode decision for both dependent texture views and dependent depth maps coding. Experimental results show that the proposed approach saves 45.2% and 30.6% encoding time on average for dependent texture views and dependent depth maps coding while maintaining negligible loss of coding efficiency, respectively. © 2018 Association for Computing Machinery.",3D-HEVC; Early mode decision; Merge mode; Priori and posterior probabilities; Real-time applications,Codes (symbols); Efficiency; Image coding; Mergers and acquisitions; Motion estimation; Probability; 3D-HEVC; Computational costs; Disparity estimations; High Efficiency Video Coding (HEVC); Mode Decision; Posterior probability; Probability modeling; Real-time application; Video signal processing
User-click-data-based fine-grained image recognition via weakly supervised metric learning,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053427001&doi=10.1145%2f3209666&partnerID=40&md5=398e24b7d0fe40b066c2305f907e0bc5,"We present a novel fine-grained image recognition framework using user click data, which can bridge the semantic gap in distinguishing categories that are similar in visual. As query set in click data is usually largescale and redundant, we first propose a click-feature-based query-merging approach to merge queries with similar semantics and construct a compact click feature. Afterward, we utilize this compact click feature and convolutional neural network (CNN)-based deep visual feature to jointly represent an image. Finally, with the combined feature, we employ the metriclearning-based template-matching scheme for efficient recognition. Considering the heavy noise in the training data, we introduce a reliability variable to characterize the image reliability, and propose a weakly-supervised metric and template leaning with smooth assumption and click prior (WMTLSC) method to jointly learn the distance metric, object templates, and image reliability. Extensive experiments are conducted on a public Clickture-Dog dataset and our newly established Clickture- Bird dataset. It is shown that the click-data-based query merging helps generating a highly compact (the dimension is reduced to 0.9%) and dense click feature for images, which greatly improves the computational efficiency. Also, introducing this click feature into CNN feature further boosts the recognition accuracy. The proposed framework performs much better than previous state-of-the-arts in fine-grained recognition tasks. © 2018 ACM.",Convolutional neural network; Fine-grained image recognition; Metric learning; User click data; Weakly supervised learning,Computational efficiency; Convolution; Image enhancement; Image recognition; Neural networks; Query processing; Reliability; Semantics; Template matching; Convolutional neural network; Fine grained; Metric learning; User click data; Weakly supervised learning; Merging
Efficient video encoding for automatic video analysis in distributed wireless surveillance systems,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053420694&doi=10.1145%2f3226036&partnerID=40&md5=9347fb50150781cafca757def4a2aad2,"In many distributed wireless surveillance applications, compressed videos are used for performing automatic video analysis tasks. The accuracy of object detection, which is essential for various video analysis tasks, can be reduced due to video quality degradation caused by lossy compression. This article introduces a video encoding framework with the objective of boosting the accuracy of object detection for wireless surveillance applications. The proposed video encoding framework is based on systematic investigation of the effects of lossy compression on object detection. It has been found that current standardized video encoding schemes cause temporal domain fluctuation for encoded blocks in stable background areas and spatial texture degradation for encoded blocks in dynamic foreground areas of a raw video, both of which degrade the accuracy of object detection. Two measures, the sum-of-absolute frame difference (SFD) and the degradation of texture in 2D transform domain (TXD), are introduced to depict the temporal domain fluctuation and the spatial texture degradation in an encoded video, respectively. The proposed encoding framework is designed to suppress unnecessary temporal fluctuation in stable background areas and preserve spatial texture in dynamic foreground areas based on the two measures, and it introduces new mode decision strategies for both intraand interframes to improve the accuracy of object detection while maintaining an acceptable rate distortion performance. Experimental results show that, compared with traditional encoding schemes, the proposed scheme improves the performance of object detection and results in lower bit rates and significantly reduced complexity with comparable quality in terms of PSNR and SSIM. © 2018 ACM.",Surveillance systems; Video analysis; Video encoding; Wireless systems,Electric distortion; Encoding (symbols); Image coding; Image compression; Monitoring; Object detection; Object recognition; Quality control; Security systems; Signal distortion; Signal encoding; Distributed wireless; Rate distortion performance; Surveillance applications; Surveillance systems; Temporal fluctuation; Video analysis; Video encodings; Wireless systems; Video signal processing
A survey on content-aware image and video retargeting,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053420590&doi=10.1145%2f3231598&partnerID=40&md5=2702a1fcfea1b26dc23d0990050f830b,"This survey introduces the current state of the art in image and video retargeting and describes important ideas and technologies that have influenced the recent work. Retargeting is the process of adapting an image or video from one screen resolution to another to fit different displays, for example, when watching a wide screen movie on a normal television screen or a mobile device. As there has been considerable work done in this field already, this survey provides an overview of the techniques. It is meant to be a starting point for new research in the field.We include explanations of basic terms and operators, as well as the basic workflow of the different methods. © 2018 ACM.",Image resizing; Image retargeting; Seam carving; Video resizing; Video retargeting; Warping,Display devices; Mobile devices; Image resizing; Image retargeting; Seam carving; Video resizing; Video retargeting; Warping; Surveys
A network-based virtual reality simulation training approach for orthopedic surgery,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053421689&doi=10.1145%2f3232678&partnerID=40&md5=0b585b468866acd891ec12ec4238d008,"The focus of this article is on the adoption of immersive and haptic simulators for training of medical residents in a surgical process called Less Invasive Stabilization System (LISS) plating surgery. LISS surgery is an orthopedic surgical procedure to treat fractures of the femur bone. Development of such simulators is a complex task which involves multiple systems, technologies, and human experts. Emerging Next Generation Internet technologies were used to develop the standalone on-line haptic-based simulator accessible to the students 24/7. A standalone immersive surgical simulator was also developed using HTC Vive. Expert surgeons played an important role in developing the simulator system; use cases of the target surgical processes were built using a modeling language called the engineering Enterprise Modeling Language (eEML). A detailed study presenting the comparison between the haptic-based simulator and the immersive simulator has been also presented. The outcomes of this study underscore the potential of using such simulators in surgical training. © 2018 ACM.",Immersive simulator; Medical simulation; Next Generation Internet technologies; Orthopedic surgery; Virtual reality,Bone; E-learning; Modeling languages; Orthopedics; Simulators; Transplantation (surgical); Virtual reality; Engineering enterprise; Haptic-based simulators; Immersive; Medical simulations; Next generation Internet; Orthopedic surgery; Stabilization systems; Virtual reality simulations; Surgical equipment
Automatic data augmentation from massiveweb images for deep visual recognition,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053416142&doi=10.1145%2f3204941&partnerID=40&md5=f1bfecfde89cfaada8537c8b6368685a,"Large-scale image datasets and deep convolutional neural networks (DCNNs) are the two primary driving forces for the rapid progress in generic object recognition tasks in recent years. While lots of network architectures have been continuously designed to pursue lower error rates, few efforts are devoted to enlarging existing datasets due to high labeling costs and unfair comparison issues. In this article, we aim to achieve lower error rates by augmenting existing datasets in an automatic manner. Our method leverages both the web and DCNN, where the web provides massive images with rich contextual information, and DCNN replaces humans to automatically label images under the guidance of web contextual information. Experiments show that our method can automatically scale up existing datasets significantly from billions of web pages with high accuracy. The performance on object recognition tasks and transfer learning tasks have been significantly improved by using the automatically augmented datasets, which demonstrates that more supervisory information has been automatically gathered from theweb. Both the dataset and models trained on the dataset have been made publicly available. © 2018 ACM.",Dataset augmentation; Dataset construction; Deep convolutional neural network,Convolution; Network architecture; Neural networks; Object recognition; Websites; Contextual information; Data augmentation; Dataset augmentation; Deep convolutional neural networks; Generic object recognition; Large-scale image datasets; Transfer learning; Visual recognition; Deep neural networks
Image captioning with affective guiding and selective attention,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053399831&doi=10.1145%2f3226037&partnerID=40&md5=5cd066bc21378d7d7d0e6038f3fe4196,"Image captioning is an increasingly important problem associated with artificial intelligence, computer vision, and natural language processing. Recent works revealed that it is possible for amachine to generate meaningful and accurate sentences for images. However, most existing methods ignore latent emotional information in an image. In this article, we propose a novel image captioning model with Affective Guiding and Selective Attention Mechanism named AG-SAM. In our method, we aim to bridge the affective gap between image captioning and the emotional response elicited by the image. First, we introduce affective components that capture higher-level concepts encoded in images into AG-SAM. Hence, our language model can be adapted to generate sentences that are more passionate and emotive. In addition, a selective gate acting on the attention mechanism controls the degree of how much visual information AG-SAM needs. Experimental results have shown that our model outperforms most existing methods, clearly reflecting an association between images and emotional components that is usually ignored in existing works. © 2018 ACM.",Affective guiding and selective attention mechanism; Convolutional neutral network; Image captioning; Visual attention,Behavioral research; Affective components; Attention mechanisms; Emotional information; Image captioning; Neutral network; Selective attention; Selective attention mechanism; Visual Attention; Natural language processing systems
ORL-SDN: Online reinforcement learning for SDN-enabled HTTP adaptive streaming,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053428656&doi=10.1145%2f3219752&partnerID=40&md5=5fd928a017357863cdf71779dc87b6b2,"In designing an HTTP adaptive streaming (HAS) system, the bitrate adaptation scheme in the player is a key component to ensure a good quality of experience (QoE) for viewers.We propose a new online reinforcement learning optimization framework, called ORL-SDN, targeting HAS players running in a software-defined networking (SDN) environment. We leverage SDN to facilitate the orchestration of the adaptation schemes for a set of HAS players. To reach a good level of QoE fairness in a large population of players, we cluster them based on a perceptual quality index. We formulate the adaptation process as a Partially Observable Markov Decision Process and solve the per-cluster optimization problem using an online Q-learning technique that leveragesmodel predictive control and parallelism via aggregation to avoid a per-cluster suboptimal selection and to accelerate the convergence to an optimum. This framework achieves maximum long-term revenue by selecting the optimal representation for each cluster under time-varying network conditions. The results show that ORL-SDN delivers substantial improvements in viewer QoE, presentation quality stability, fairness, and bandwidth utilization over well-known adaptation schemes. © 2018 ACM.",FastMPC; HAS; HAS scalability issues; POMDP; QoE optimization; Reinforcement learning; SDN,E-learning; HTTP; Learning algorithms; Markov processes; Quality of service; Band-width utilization; FastMPC; Http adaptive streaming; Partially observable Markov decision process; POMDP; Quality of experience (QoE); Scalability issue; Software defined networking (SDN); Reinforcement learning
Properties and design of variable-to-variable length codes,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053414358&doi=10.1145%2f3230653&partnerID=40&md5=c0dfad97e6458075720792bed20be03c,"For the entropy coding of independent and identically distributed (i.i.d.) binary sources, variable-to-variable length (V2V) codes are an interesting alternative to arithmetic coding. Such a V2V code translates variable length words of the source into variable length code words by employing two prefix-free codes. In this article, several properties of V2V codes are studied, and new concepts are developed. In particular, it is shown that the redundancy of a V2V code cannot be zero for a binary i.i.d. source {X} with 0 < pX (1) < 0.5. Furthermore, the concept of prime and composite V2V codes is proposed, and it is shown why composite V2V codes can be disregarded in the search for particular classes of minimum redundancy codes. Moreover, a canonical representation for V2V codes is proposed, which identifies V2V codes that have the same average code length function. It is shown how these concepts can be employed to greatly reduce the complexity of a search for minimum redundancy (size-limited) V2V codes. © 2018 ACM.",Entropy coding; Huffman coding; Package merge algorithm; V2V coding; Variable length coding,Decoding; Entropy; Average code length; Canonical representations; Entropy coding; Huffman coding; Prefix-free codes; V2V coding; Variable length codes; Variable length coding; Redundancy
Learning multiple kernel metrics for iterative person re-identification,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053435643&doi=10.1145%2f3234929&partnerID=40&md5=9219aa0fec6789a7679028102675c88c,"In person re-identification most metric learning methods learn from training data only once, and then they are deployed for testing. Although impressive performance has been achieved, the discriminative information from successfully identified test samples are ignored. In this work, we present a novel re-identification framework termed Iterative Multiple Kernel Metric Learning (IMKML). Specifically, there are two main modules in IMKML. In the first module, multiple metrics are learned via a new derived Kernel Marginal Nullspace Learning (KMNL) algorithm. Taking advantage of learning a discriminative nullspace from neighborhood manifold, KMNL can well tackle the Small Sample Size (SSS) problem in re-identification distance metric learning. The second module is to construct a pseudo training set by performing re-identification on the testing set. The pseudo training set, which consists of the test image pairs that are highly probable correct matches, is then inserted into the labeled training set to retrain themetrics. By iteratively alternating between the two modules, many more samples will be involved for training and significant performance gains can be achieved. Experiments on four challenging datasets, including VIPeR, PRID450S, CUHK01, and Market-1501, show that the proposed method performs favorably against the state-of-the-art approaches, especially on the lower ranks. © 2018 ACM.",Kernel method; Metric learning; Nullspace; Person re-identification,Silicon compounds; Kernel methods; Metric learning; Multiple kernels; Null space; Person re identifications; Re identifications; Small sample size problems; State-of-the-art approach; Iterative methods
Aesthetic highlight detection in movies based on synchronization of spectators' reactions,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053417836&doi=10.1145%2f3175497&partnerID=40&md5=3ddc20d62b0788715c34b68982c09c21,"Detection of aesthetic highlights is a challenge for understanding the affective processes taking place during movie watching. In this article, we study spectators' responses to movie aesthetic stimuli in a social context. Moreover, we look for uncovering the emotional component of aesthetic highlights in movies. Our assumption is that synchronized spectators' physiological and behavioral reactions occur during these highlights because: (i) aesthetic choices of filmmakers are made to elicit specific emotional reactions (e.g., special effects, empathy, and compassion toward a character) and (ii) watching a movie together causes spectators' affective reactions to be synchronized through emotional contagion. We compare different approaches to estimation of synchronization among multiple spectators' signals, such as pairwise, group, and overall synchronization measures to detect aesthetic highlights in movies. The results show that the unsupervised architecture relying on synchronization measures is able to capture different properties of spectators' synchronization and detect aesthetic highlights based on both spectators' electrodermal and acceleration signals.We discover that pairwise synchronization measures perform the most accurately independently of the category of the highlights and movie genres. Moreover, we observe that electrodermal signals have more discriminative power than acceleration signals for highlight detection. © 2018 ACM.",Aesthetic experience; Aesthetic highlight detection; Affective computing; Behavioral signals; Dynamical systems; Physiological signals; Synchronization; Video summarization,Dynamical systems; Electrodes; Motion pictures; Physiology; Aesthetic experience; Affective Computing; Highlight detection; Physiological signals; Video summarization; Synchronization
Soundscape of an archaeological site recreated with audio augmented reality,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053387389&doi=10.1145%2f3230652&partnerID=40&md5=bd5de2970f95d33ee305a0ed44ad2047,"This article investigates the use of an audio augmented reality (AAR) system to recreate the soundscape of a medieval archaeological site. The aim of our work was to explore whether it is possible to enhance a tourist's archaeological experience, which is often derived from only scarce remains. We developed a smartphonebased AAR system, which uses location and orientation sensors to synthesize the soundscape of a site and plays it to the user via headphones. We recreated the ancient soundscape of a medieval archaeological site in Croatia and tested it in situ on two groups of participants using the soundwalk method. One test group performed the soundwalk while listening to the recreated soundscape using the AAR system, while the second control group did not use the AAR equipment. We measured the experiences of the participants using two methods: The standard soundwalk questionnaire and affective computing equipment for detecting the emotional state of participants. The results of both test methods show that participants who were listening to the ancient soundscape using our AAR system experienced higher arousal than those visiting the site without AAR. © 2018 ACM.",Affective computing; Augmented reality; Auralization,Architecture; Affective Computing; Archaeological site; Audio augmented reality; Auralizations; Control groups; Emotional state; Orientation sensors; Soundscapes; Augmented reality
Characterizing user behaviors in mobile personal livecast: Towards an edge computing-assisted paradigm,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053870050&doi=10.1145%2f3219751&partnerID=40&md5=4b5583a44cd5290b29e733e4a49f03f9,"Mobile personal livecast (MPL) services are emerging and have received great attention recently. In MPL, numerous and geo-distributed ordinary people broadcast their video contents to worldwide viewers. Different from conventional social networking services like Twitter and Facebook, which have a tolerance for interaction delay, the interactions (e.g., chat messages) in a personal livecast must be in real-time with low feedback latency. These unique characteristics inspire us to: (1) investigate how the relationships (e.g., social links and geo-locations) between viewers and broadcasters influence the user behaviors, which has yet to be explored in depth; and (2) explore insights to benefit the improvement of system performance. In this article, we carry out extensive measurements of a representative MPL system, with a large-scale dataset containing 11M users. In the current costly and limited cloud-based MPL system, which is faced with scalability problem, we find: (1) the long content uploading distances between broadcasters and cloud ingesting servers result in an impaired system QoS, including a high broadcast latency and a frequently buffering events; and (2) most of the broadcasters in MPL are geographically locally popular (the majority of the views come from the same region of the broadcaster), which consume vast computation and bandwidth resources of the clouds and Content Delivery Networks. Fortunately, the emergence of edge computing, which provides cloud-computing capabilities at the edge of the mobile network, naturally sheds new light on the MPL system; i.e., localized ingesting, transcoding, and delivering locally popular live content is possible. Based on these critical observations, we propose an edge-assisted MPL system that collaboratively utilizes the core-cloud and abundant edge computing resources to improve the system efficiency and scalability. In our framework, we consider a dynamic broadcaster assignment to minimize the broadcast latency while keeping the resource lease cost low. We formulate the broadcaster scheduling as a stable matching with migration problem to solve it effectively. Compared with the current pure cloud-based system, our edge-assisted delivery approach reduces the broadcast latency by about 35%. © 2018 ACM.",Edge computing paradigm; Mobile personal livecast; User behavior analysis,Edge computing; Scalability; Social networking (online); Computing paradigm; Computing resource; Content delivery network; Large-scale dataset; Mobile personal livecast; Scalability problems; Social networking services; User behavior analysis; Behavioral research
Delay-sensitive video computing in the cloud: A survey,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053882557&doi=10.1145%2f3212804&partnerID=40&md5=39f8e394c480d3cbd0ec6626fd45418e,"While cloud servers provide a tremendous amount of resources for networked video applications, most successful stories of cloud-assisted video applications are presentational video services, such as YouTube and NetFlix. This article surveys the recent advances on delay-sensitive video computations in the cloud, which are crucial to cloud-assisted conversational video services, such as cloud gaming, Virtual Reality (VR), Augmented Reality (AR), and telepresence. Supporting conversational video services with cloud resources is challenging because most cloud servers are far away from the end users while these services incur the following stringent requirements: high bandwidth, short delay, and high heterogeneity. In this article, we cover the literature with a top-down approach: from applications and experience, to architecture and management, and to optimization in and outside of the cloud. We also point out major open challenges, hoping to stimulate more research activities in this emerging and exciting direction. © 2018 ACM.",Applications; Architecture; Cloud computing; Latency; Multimedia; Networking; Optimization; Quality of experience (QoE); Quality of service (QoS); Virtualization,Applications; Architecture; Augmented reality; Cloud computing; Optimization; Quality of service; Surveys; Virtual reality; Virtualization; Visual communication; Conversational video services; Latency; Multimedia; Networking; Quality of experience (QoE); Research activities; Stringent requirement; Top down approaches; Computer architecture
Best papers of the ACM multimedia systems (MMSys) conference 2017 and the ACM workshop on network and operating system support for digital audio and video (NOSSDAV) 2017,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053925895&doi=10.1145%2f3214700&partnerID=40&md5=eb101d75f557efa48a9f3c97794d45c5,[No abstract available],,
Prototyping a web-scale multimedia retrieval service using spark,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053905243&doi=10.1145%2f3209662&partnerID=40&md5=d251b0c84ad14ad19a088ef9954a527f,"The world has experienced phenomenal growth in data production and storage in recent years, much of which has taken the form of media files. At the same time, computing power has become abundant with multi-core machines, grids, and clouds. Yet it remains a challenge to harness the available power and move toward gracefully searching and retrieving from web-scale media collections. Several researchers have experimented with using automatically distributed computing frameworks, notably Hadoop and Spark, for processing multimedia material, but mostly using small collections on small computing clusters. In this article, we describe a prototype of a (near) web-scale throughput-oriented MM retrieval service using the Spark framework running on the AWS cloud service. We present retrieval results using up to 43 billion SIFT feature vectors from the public YFCC 100M collection, making this the largest high-dimensional feature vector collection reported in the literature. We also present a publicly available demonstration retrieval system, running on our own servers, where the implementation of the Spark pipelines can be observed in practice using standard image benchmarks, and downloaded for research purposes. Finally, we describe a method to evaluate retrieval quality of the ever-growing high-dimensional index of the prototype, without actually indexing a web-scale media collection. © 2018 ACM.",Cloud computing; Content-based image retrieval; Distributed computing; Scalability; Spark,Cloud computing; Content based retrieval; Digital storage; Distributed computer systems; Electric sparks; Scalability; Computing clusters; Content based image retrieval; Distributed computing frameworks; High dimensional feature; High-dimensional index; Multi-core machines; Multimedia materials; Multimedia Retrieval; Search engines
Designing and evaluating a mesh simplification algorithm for virtual reality,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053887050&doi=10.1145%2f3209661&partnerID=40&md5=0b0a30fed8bde6427f41a6b1482cc3eb,"With the increasing accessibility of the mobile head-mounted displays (HMDs), mobile virtual reality (VR) systems are finding applications in various areas. However, mobile HMDs are highly constrained with limited graphics processing units (GPUs) and low processing power and onboard memory. Hence, VR developers must be cognizant of the number of polygons contained within their virtual environments to avoid rendering at low frame rates and inducing simulator sickness. The most robust and rapid approach to keeping the overall number of polygons low is to use mesh simplification algorithms to create low-poly versions of preexisting, high-poly models. Unfortunately, most existing mesh simplification algorithms cannot adequately handle meshes with lots of boundaries or nonmanifold meshes, which are common attributes of many 3D models. In this article, we present QEM4VR, a high-fidelity mesh simplification algorithm specifically designed for VR. This algorithm addresses the deficiencies of prior quadric error metric (QEM) approaches by leveraging the insight that the most relevant boundary edges lie along curvatures while linear boundary edges can be collapsed. Additionally, our algorithm preserves key surface properties, such as normals, texture coordinates, colors, and materials, as it preprocesses 3D models and generates their low-poly approximations offline. We evaluated the effectiveness of our QEM4VR algorithm by comparing its simplified-mesh results to those of prior QEM variations in terms of geometric approximation error, texture error, progressive approximation errors, frame rate impact, and perceptual quality measures. We found that QEM4VR consistently yielded simplified meshes with less geometric approximation error and texture error than the prior QEM variations. It afforded better frame rates than QEM variations with boundary preservation constraints that create unnecessary lower bounds on overall polygon count reduction. Our evaluation revealed that QEM4VR did not fair well in terms of existing perceptual distance measurements, but human-based inspections demonstrate that these algorithmic measurements are not suitable substitutes for actual human perception. In turn, we present a user-based methodology for evaluating the perceptual qualities of mesh simplification algorithms. © 2018 ACM.",Mesh simplification; Quadric error metric; Virtual reality,Approximation algorithms; Errors; Graphic methods; Graphics processing unit; Helmet mounted displays; Image coding; Mesh generation; Petroleum reservoir evaluation; Program processors; Quality control; Sensory perception; Virtual reality; Approximation errors; Geometric approximations; Head mounted displays; Mesh simplifications; Perceptual quality measures; Quadric error metric (QEM); Quadric error metrics; Texture coordinates; Three dimensional computer graphics
Game input with delay-moving target selection with a game controller thumbstick,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053864620&doi=10.1145%2f3187288&partnerID=40&md5=4b3b7fa29982694bbe0e7bc6bbfacce0,"Hosting interactive video-based services, such as computer games, in the Cloud poses particular challenges given user sensitivity to delay. A better understanding of the impact of delay on player-game interactions can help design cloud systems and games that accommodate delays inherent in cloud systems. Previous top-down studies of delay using full-featured games have helped understand the impact of delay, but often do not generalize or lend themselves to analytic modeling. Bottom-up studies isolating user input and delay can better generalize and be used in models, but have yet to be applied to cloud-hosted computer games. In order to better understand delay impact in cloud-hosted computer games, we conduct a large bottom-up user study centered on a fundamental game interaction-selecting a moving target with user input impeded by delay. Our work builds a custom game that controls both the target speed and input delay and has players select the target using a game controller analog thumbstick. Analysis of data from over 50 users shows target selection time exponentially increases with delay and target speed and is well-fit by an exponential model that includes a delay and target speed interaction term. A comparison with two previous studies, both using a mouse instead of a thumbstick, suggests the model's relationship between selection time, delay, and target speed holds more broadly, providing a foundation for a potential law explaining moving target selection with delay encountered in cloud-hosted games. © 2018 ACM.",Delay; Lag,Mammals; Analytic modeling; Delay; Exponential models; Interaction term; Interactive video; Moving target selections; Sensitivity to delay; Target selection; Computer games
ASAP: Adaptive stall-aware pacing for improved DASH video experience in cellular networks,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053913645&doi=10.1145%2f3219750&partnerID=40&md5=d8fa4d4a2534142e22b28fc903be890b,"The dramatic growth of video traffic represents a practical challenge for cellular network operators in providing a consistent streaming Quality of Experience (QoE) to their users. Satisfying this objective has so-far proved elusive, due to the inherent characteristics of wireless networks and varying channel conditions as well as variability in the video bitrate that can degrade streaming performance. In this article, we propose stall-aware pacing as a novel MPEG DASH video traffic management solution that reduces playback stalls and seeks to maintain a consistent QoE for cellular users, even those with diverse channel conditions. These goals are achieved by leveraging both network and client state information to optimize the pacing of individual video flows. We evaluate the performance of two versions of stall-aware pacing techniques extensively, including stall-aware pacing (SAP) and adaptive stall-aware pacing (ASAP), using real video content and clients, operating over a simulated LTE network. We implement state-of-the-art client adaptation and traffic management strategies for direct comparisons with SAP and ASAP. Our results, using a heavily loaded base station, show that SAP reduces the number of stalls and the average stall duration per session by up to 95%. Additionally, SAP ensures that clients with good channel conditions do not dominate available wireless resources, evidenced by a reduction of up to 40% in the standard deviation of the QoE metric across clients. We also show that ASAP achieves additional performance gains by adaptively pacing video streams based on the application buffer state. © 2018 ACM.",Adaptive bitrate video streaming; DASH; QoE; Separable programming,Mobile telecommunication systems; Quality of service; Video streaming; Wireless networks; Wireless telecommunication systems; Bit-rate video; Channel conditions; DASH; Inherent characteristics; Quality of experience (QoE); Standard deviation; Traffic management strategies; Wireless resources; Motion Picture Experts Group standards
Cloud baking: Collaborative scene illumination for dynamic Web3D scenes,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053912317&doi=10.1145%2f3206431&partnerID=40&md5=1de7fdc2046e382f40a40eab6572912d,"We propose Cloud Baking, a collaborative rendering architecture for dynamic Web3D scenes. In our architecture, the cloud renderer renders the scene with the global illumination (GI) information in a GI map; the web-based client renderer renders the scene with ambient lighting only and blends it with the GI map received from the cloud for the final scene. This approach allows the users to interact with the web scene and change the scene dynamically through the web interface end, yet move the computationally heavy tasks of global illumination computation to the cloud. A challenge we face is the interaction delay that causes the frames rendered on the cloud and the client to go out of sync. We propose to use 3D warping and a hole-filling algorithm designed for GI map to predict the late GI map. We show both quantitatively and visually the quality of the GI map produced using our method. Our prediction algorithm allows us to further reduce the frequency at which the GI map is computed and sent from the server, reducing both computational needs and bandwidth usage. © 2018 ACM.",Global illumination; Interactive 3D applications; Light baking; Remote rendering; WebGL,Memory architecture; Three dimensional computer graphics; Web services; 3D application; Ambient lighting; Bandwidth usage; Global illumination; Hole filling algorithm; Prediction algorithms; Remote rendering; WebGL; Rendering (computer graphics)
Novel hybrid-cast approach to reduce bandwidth and latency for cloud-based virtual space,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053872653&doi=10.1145%2f3205864&partnerID=40&md5=b9d65ca634869e66d27393a03a3d14ac,"In this article, we explore the possibility of enabling cloud-based virtual space applications for better computational scalability and easy access from any end device, including future lightweight wireless head-mounted displays. In particular, we investigate virtual space applications such as virtual classroom and virtual gallery, in which the scenes and activities are rendered in the cloud, with multiple views captured and streamed to each end device. A key challenge is the high bandwidth requirement to stream all the user views, leading to high operational cost and potential large delay in a bandwidth-restricted wireless network. We propose a novel hybrid-cast approach to save bandwidth in a multi-user streaming scenario. We identify and broadcast the common pixels shared by multiple users, while unicasting the residual pixels for each user. We formulate the problem of minimizing the total bitrate needed to transmit the user views using hybrid-casting and describe our approach. A common view extraction approach and a smart grouping algorithm are proposed and developed to achieve our hybrid-cast approach. Simulation results show that the hybrid-cast approach can significantly reduce total bitrate by up to 55% and avoid congestion-related latency, compared to traditional cloud-based approach of transmitting all the views as individual unicast streams, hence addressing the bandwidth challenges of the cloud, with additional benefits in cost and delay. © 2018 ACM.",Cloud-based mobile multimedia; Hybrid-cast; Virtual reality; Virtual space,Computer aided instruction; Helmet mounted displays; Pixels; Space applications; Street traffic control; Virtual reality; Computational scalability; Grouping algorithm; Head mounted displays; High bandwidth; Mobile multimedia; Residual pixel; Virtual Classroom; Virtual spaces; Bandwidth
On the effectiveness of offset projections for 360-degree video streaming,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053940575&doi=10.1145%2f3209660&partnerID=40&md5=0c1d302786a8957182e7e5ad9c306a8c,"A new generation of video streaming technology, 360-degree video, promises greater immersiveness than standard video streams. This level of immersiveness is similar to that produced by virtual reality devices-users can control the field of view using head movements rather than needing to manipulate external devices. Although 360-degree video could revolutionize the streaming experience, its large-scale adoption is hindered by a number of factors: 360-degree video streams have larger bandwidth requirements and require faster responsiveness to user inputs, and users may be more sensitive to lower quality streams. In this article, we review standard approaches toward 360-degree video encoding and compare these to families of approaches that distort the spherical surface to allow oriented concentrations of the 360-degree view. We refer to these distorted projections as offset projections. Our measurement studies show that most types of offset projections produce rendered views with better quality than their nonoffset equivalents when view orientations are within 40 or 50 degrees of the offset orientation. Offset projections complicate adaptive 360-degree videostreaming because they requirea combination of bitrate and view orientation adaptations. We es-timate that this combination of streaming adaptation in two dimensions can cause over 57% extra segments to be downloaded compared to an ideal downloading strategy, wasting 20% of the total downloading bandwidth. © 2018 ACM.",360-degree video streaming; Adaptive streaming; Offset projection; Visual quality,Bandwidth; Multimedia systems; Virtual reality; Adaptive streaming; Bandwidth requirement; Measurement study; Offset projection; Spherical surface; Video streaming technology; Virtual reality devices; Visual qualities; Video streaming
Cost-efficient server provisioning for cloud gaming,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053909909&doi=10.1145%2f3190838&partnerID=40&md5=872498286a1cd32f1458c09e3fa0571f,"Cloud gaming has gained significant popularity recently due to many important benefits such as removal of device constraints, instant-on, and cross-platform. The properties of intensive resource demands and dynamic workloads make cloud gaming appropriate to be supported by an elastic cloud platform. Facing a large user population, a fundamental problem is how to provide satisfactory cloud gaming service at modest cost. We observe that the software storage cost could be substantial compared to the server running cost in cloud gaming using elastic cloud resources. Therefore, in this article, we address the server provisioning problem for cloud gaming to optimize both the server running cost and the software storage cost. We find that the distribution of game software among servers and the selection of server types both trigger tradeoffs between the software storage cost and the server running cost in cloud gaming. We formulate the problem with a stochastic model and employ queueing theory to conduct a solid theoretical analysis of the system behaviors under different request dispatching policies. We then propose several classes of algorithms to approximate the optimal solution. The proposed algorithms are evaluated by extensive experiments using real-world parameters. The results show that the proposed Ordered and Genetic algorithms are computationally efficient, nearly cost-optimal, and highly robust to dynamic changes. © 2018 ACM.",Cloud gaming; Heuristic algorithms; Server provisioning; Service cost; Software distribution,Genetic algorithms; Queueing theory; Stochastic models; Stochastic systems; Cloud gamings; Cloud platforms; Computationally efficient; Optimal solutions; Resource demands; Service costs; Software distributions; System behaviors; Heuristic algorithms
User behavior analysis and video popularity prediction on a large-scale VoD system,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053878950&doi=10.1145%2f3226035&partnerID=40&md5=865cea65958a4bc2f824f9163105b2d3,"Understanding streaming user behavior is crucial to the design of large-scale Video-on-Demand (VoD) systems. In this article, we begin with the measurement of individual viewing behavior from two aspects: the temporal characteristics and user interest. We observe that active users spend more hours on each active day, and their daily request time distribution is more scattered than that of the less active users, while the interview time distribution differs negligibly between two groups. The common interest in popular videos and the latest uploaded videos is observed in both groups. We then investigate the predictability of video popularity as a collective user behavior through early views. In the light of the limitations of classical approaches, the Autoregressive-Moving-Average (ARMA) model is employed to forecast the popularity dynamics of individual videos at fine-grained time scales, thus achieving much higher prediction accuracy. When applied to video caching, the ARMA-assisted Least Frequently Used (LFU) algorithm can outperform the Least Recently Used (LRU) by 11-16%, the well-tuned LFU by 6-13%, and the LFU is only 2-4% inferior to the offline LFU in terms of hit ratio. © 2018 ACM.",Cache replacement; Popularity prediction; User behavior analysis; Video-on-demand,Behavioral research; Forecasting; Autoregressive moving average model; Cache replacement; Least frequently used; Popularity predictions; Prediction accuracy; Temporal characteristics; User behavior analysis; Videoon-demand systems (VoD); Video on demand
Enabling live video analytics with a scalable and privacy-aware framework,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053896017&doi=10.1145%2f3209659&partnerID=40&md5=4dfd3269ba8dc49d03c24c17ca9b1b1f,"We show how to build the components of a privacy-aware, live video analytics ecosystem from the bottom up, starting with OpenFace, our new open-source face recognition system that approaches state-of-the-art accuracy. Integrating OpenFace with interframe tracking, we build RTFace, a mechanism for denaturing video streams that selectively blurs faces according to specified policies at full frame rates. This enables privacy management for live video analytics while providing a secure approach for handling retrospective policy exceptions. Finally, we present a scalable, privacy-aware architecture for large camera networks using RTFace and show how it can be an enabler for a vibrant ecosystem and marketplace of privacy-aware video streams and analytics services. © ACM.",Cloud computing; Cloudlet; Edge computing; Face recognition; Privacy mediator,Cloud computing; Data privacy; Ecosystems; Edge computing; Open systems; Video streaming; Camera network; Cloudlet; Face recognition systems; Inter-frame; Open sources; Privacy aware; Privacy management; State of the art; Face recognition
Game categorization for deriving QoE-driven video encoding configuration strategies for cloud gaming,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053878201&doi=10.1145%2f3132041&partnerID=40&md5=7ba3f872f8a2c6891a515ab429e7a7c3,"Cloud gaming has been recognized as a promising shift in the online game industry, with the aim of implementing the ""on demand"" service concept that has achieved market success in other areas of digital entertainment such as movies and TV shows. The concepts of cloud computing are leveraged to render the game scene as a video stream that is then delivered to players in real-time. The main advantage of this approach is the capability of delivering high-quality graphics games to any type of end user device; however, at the cost of high bandwidth consumption and strict latency requirements. A key challenge faced by cloud game providers lies in configuring the video encoding parameters so as to maximize player Quality of Experience (QoE) while meeting bandwidth availability constraints. In this article, we tackle one aspect of this problem by addressing the following research question: Is it possible to improve service adaptation based on information about the characteristics of the game being streamed? To answer this question, two main challenges need to be addressed: the need for different QoE-driven video encoding (re-)configuration strategies for different categories of games, and how to determine a relevant game categorization to be used for assigning appropriate configuration strategies. We investigate these problems by conducting two subjective laboratory studies with a total of 80 players and three different games. Results indicate that different strategies should likely be applied for different types of games, and show that existing game classifications are not necessarily suitable for differentiating game types in this context. We thus further analyze objective video metrics of collected game play video traces as well as player actions per minute and use this as input data for clustering of games into two clusters. Subjective results verify that different video encoding configuration strategies may be applied to games belonging to different clusters. © 2018 ACM.",Cloud gaming; Game categorization; Quality of Experience; Video codec configuration strategies,Bandwidth; Computer games; Encoding (symbols); Quality of service; Signal encoding; Web browsers; Bandwidth availability; Cloud gamings; Digital entertainment; Game categorization; High-quality graphics; Online game industries; Quality of experience (QoE); Video codecs; Video signal processing
Introduction to the special issue on delay-sensitive video computing in the cloud,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053867948&doi=10.1145%2f3214698&partnerID=40&md5=3a6b6bac0b079eb42276d4708e3617fb,[No abstract available],,
Contrast enhancement estimation for digital image forensics,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051983402&doi=10.1145%2f3183518&partnerID=40&md5=7d3a14af1769abbd8a76e710eec3acdc,"Inconsistency in contrast enhancement can be used to expose image forgeries. In this work, we describe a new method to estimate contrast enhancement operations from a single image. Our method takes advantage of the nature of contrast enhancement as a mapping between pixel values and the distinct characteristics it introduces to the image pixel histogram. Our method recovers the original pixel histogram and the contrast enhancement simultaneously from a single image with an iterative algorithm. Unlike previous works, our method is robust in the presence of additive noise perturbations that are used to hide the traces of contrast enhancement. Furthermore, we also develop an effective method to detect image regions undergone contrast enhancement transformations that are different from the rest of the image, and we use this method to detect composite images. We perform extensive experimental evaluations to demonstrate the efficacy and efficiency of our method. © 2018 ACM.",Contrast enhancement; Media forensics; Pixel histogram,Additive noise; Contrast media; Graphic methods; Image enhancement; Iterative methods; Pixels; Composite images; Contrast Enhancement; Digital image forensics; Experimental evaluation; Image regions; Iterative algorithm; Media forensics; Noise perturbation; Digital forensics
Multifeature selection for 3D human action recognition,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051982611&doi=10.1145%2f3177757&partnerID=40&md5=0f6faebaf5dc7b3568afa3a1124083d5,"In mainstream approaches for 3D human action recognition, depth and skeleton features are combined to improve recognition accuracy. However, this strategy results in high feature dimensions and low discrimination due to redundant feature vectors. To solve this drawback, a multi-feature selection approach for 3D human action recognition is proposed in this paper. First, three novel single-modal features are proposed to describe depth appearance, depth motion, and skeleton motion. Second, a classification entropy of random forest is used to evaluate the discrimination of the depth appearance based features. Finally, one of the three features is selected to recognize the sample according to the discrimination evaluation. Experimental results show that the proposed multi-feature selection approach significantly outperforms other approaches based on single-modal feature and feature fusion. © 2018 ACM.",Action recognition; Feature selection,Decision trees; Musculoskeletal system; Action recognition; Appearance based; Feature dimensions; Human-action recognition; Random forests; Recognition accuracy; Redundant features; Skeleton motion; Feature extraction
Guest editorial: Special issue on “QoE management for multimedia services”,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047145004&doi=10.1145%2f3192332&partnerID=40&md5=0038364517a68b8c030d8e733abc6ad3,[No abstract available],,
Image Captioning with Deep Bidirectional LSTMs and Multi-Task Learning,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047127377&doi=10.1145%2f3115432&partnerID=40&md5=950dea126962fd978bfbcf20598c33c4,"Generating a novel and descriptive caption of an image is drawing increasing interests in computer vision, natural language processing, and multimedia communities. In this work, we propose an end-to-end trainable deep bidirectional LSTM (Bi-LSTM (Long Short-Term Memory)) model to address the problem. By combining a deep convolutional neural network (CNN) and two separate LSTM networks, our model is capable of learning long-term visual-language interactions by making use of history and future context information at high-level semantic space. We also explore deep multimodal bidirectional models, in which we increase the depth of nonlinearity transition in different ways to learn hierarchical visual-language embeddings. Data augmentation techniques such as multi-crop, multi-scale, and vertical mirror are proposed to prevent overfitting in training deep models. To understand how our models “translate” image to sentence, we visualize and qualitatively analyze the evolution of Bi-LSTM internal states over time. The effectiveness and generality of proposed models are evaluated on four benchmark datasets: Flickr8K, Flickr30K, MSCOCO, and Pascal1K datasets. We demonstrate that Bi-LSTM models achieve highly competitive performance on both caption generation and image-sentence retrieval even without integrating an additional mechanism (e.g., object detection, attention model). Our experiments also prove that multi-task learning is beneficial to increase model generality and gain performance. We also demonstrate the performance of transfer learning of the Bi-LSTM model significantly outperforms previous methods on the Pascal1K dataset. © 2018 ACM.",Additional Key Words; Image captioning; LSTM; Multimodal representations; Mutli-task learning; Phrases: Deep learning,Deep neural networks; Natural language processing systems; Object detection; Semantics; Visual languages; Image captioning; Key words; LSTM; Multi-modal; Task learning; Long short-term memory
"Measuring individual video QoE: A survey, and proposal for future directions using social media",2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047118572&doi=10.1145%2f3183512&partnerID=40&md5=6cfd585bb95776cf2b9a24702f66f050,"The next generation of multimedia services have to be optimized in a personalized way, taking user factors into account for the evaluation of individual experience. Previous works have investigated the influence of user factors mostly in a controlled laboratory environment which often includes a limited number of users and fails to reflect real-life environment. Social media, especially Facebook, provide an interesting alternative for Internet-based subjective evaluation. In this article, we develop (and open-source) a Facebook application, named YouQ1, as an experimental platform for studying individual experience for videos. Our results show that subjective experiments based on YouQ can produce reliable results as compared to a controlled laboratory experiment. Additionally, YouQ has the ability to collect user information automatically from Facebook, which can be used for modeling individual experience. © 2018 ACM.",Additional Key Words; Facebook; Phrases: Individual Quality of Experience; User factors,Multimedia services; Quality of service; Controlled laboratories; Facebook; Facebook applications; Key words; Quality of experience (QoE); Subjective evaluations; Subjective experiments; User factors; Social networking (online)
Multimodal multiplatform social media event summarization,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047147328&doi=10.1145%2f3115433&partnerID=40&md5=02b04ffcc900347387001840270cf00e,"Social media platforms are turning into important news sources since they provide real-time information from different perspectives. However, high volume, dynamism, noise, and redundancy exhibited by social media data make it difficult to comprehend the entire content. Recent works emphasize on summarizing the content of either a single social media platform or of a single modality (either textual or visual). However, each platform has its own unique characteristics and user base, which brings to light different aspects of real-world events. This makes it critical as well as challenging to combine textual and visual data from different platforms. In this article, we propose summarization of real-world events with data stemming from different platforms and multiple modalities. We present the use of a Markov Random Fields based similarity measure to link content across multiple platforms. This measure also enables the linking of content across time, which is useful for tracking the evolution of long-running events. For the final content selection, summarization is modeled as a subset selection problem. To handle the complexity of the optimal subset selection, we propose the use of submodular objectives. Facets such as coverage, novelty, and significance are modeled as submodular objectives in a multimodal social media setting. We conduct a series of quantitative and qualitative experiments to illustrate the effectiveness of our approach compared to alternative methods. © 2018 ACM.",Additional Key Words; Evaluation; Markov random fields; Multimedia; Phrases: Social media summarization; Topic modeling; User study,Markov processes; Natural language processing systems; Evaluation; Key words; Markov Random Fields; Multimedia; Social media; Topic Modeling; User study; Social networking (online)
Ensemble of deep models for event recognition,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047143886&doi=10.1145%2f3199668&partnerID=40&md5=690affda0bc77ae4d7d3fd0b6f08ae86,"In this article, we address the problem of recognizing an event from a single related picture. Given the large number of event classes and the limited information contained in a single shot, the problem is known to be particularly hard. To achieve a reliable detection, we propose a combination of multiple classifiers, and we compare three alternative strategies to fuse the results of each classifier, namely: (i) induced order weighted averaging operators, (ii) genetic algorithms, and (iii) particle swarm optimization. Each method is aimed at determining the optimal weights to be assigned to the decision scores yielded by different deep models, according to the relevant optimization strategy. Experimental tests have been performed on three event recognition datasets, evaluating the performance of various deep models, both alone and selectively combined. Experimental results demonstrate that the proposed approach outperforms traditional multiple classifier solutions based on uniform weighting, and outperforms recent state-of-the-art approaches. © 2018 ACM.",CNN; Deep neural networks; Event recognition; Fusion; Genetic algorithms; IOWA; Multimedia indexing; Multiple classifiers; PSO; Retrieval,Fusion reactions; Genetic algorithms; Particle swarm optimization (PSO); Event recognition; IOWA; Multimedia indexing; Multiple classifiers; Retrieval; Deep neural networks
Guest editorial: Special section on “multimedia understanding via multimodal analytics”,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047143765&doi=10.1145%2f3192334&partnerID=40&md5=bdfc540d91174e92b658bdcdb37f3786,[No abstract available],,
Learning a multi-concept video retrieval model with multiple latent variables,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047115902&doi=10.1145%2f3176647&partnerID=40&md5=7ebbbe154ff8ecb18b9687947c7abf5b,"Effective and efficient video retrieval has become a pressing need in the “big video” era. The objective of this work is to provide a principled model for computing the ranking scores of a video in response to one or more concepts, where the concepts could be directly supplied by users or inferred by the system from the user queries. Indeed, how to deal with multi-concept queries has become a central component in modern video retrieval systems that accept text queries. However, it has been long overlooked and simply implemented by weighted averaging of the corresponding concept detectors’ scores. Our approach, which can be considered as a latent ranking SVM, integrates the advantages of various recent works in text and image retrieval, such as choosing ranking over structured prediction, modeling inter-dependencies between querying concepts, and so on. Videos consist of shots, and we use latent variables to account for the mutually complementary cues within and across shots. Concept labels of shots are scarce and noisy. We introduce a simple and effective technique to make our model robust to outliers. Our approach gives superior performance when it is tested on not only the queries seen at training but also novel queries, some of which consist of more concepts than the queries used for training. © 2018 ACM.",Multi-concept retrieval; Structural learning; Video indexing; Video retrieval,Indexing (of information); Concept retrieval; Inter-dependencies; Structural learning; Structured prediction; Video indexing; Video retrieval; Video retrieval system; Weighted averaging; Image retrieval
QoE-aware OTT-ISP collaboration in service management: Architecture and approaches,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047104904&doi=10.1145%2f3183517&partnerID=40&md5=7a8c6fe32312ea2069894edb51352f38,"It is a matter of fact that quality of experience (QoE) has become one of the key factors determining whether a new multimedia service will be successfully accepted by the final users. Accordingly, several QoE models have been developed with the aim of capturing the perception of the user by considering as many influencing factors as possible. However, when it comes to adopting these models in the management of the services and networks, it frequently happens that no single provider has access to all of the tools to either measure all influencing factors parameters or control over the delivered quality. In particular, it often happens to the over-the-top (OTT) and Internet service providers (ISPs), which act with complementary roles in the service delivery over the Internet. On the basis of this consideration, in this article we first highlight the importance of a possible OTT-ISP collaboration for a joint service management in terms of technical and economic aspects. Then we propose a general reference architecture for a possible collaboration and information exchange among them. Finally, we define three different approaches, namely joint venture, customer lifetime value based, and QoE fairness based. The first aims to maximize the revenue by providing better QoE to customers paying more. The second aims to maximize the profit by providing better QoE to the most profitable customers (MPCs). The third aims to maximize QoE fairness among all customers. Finally, we conduct simulations to compare the three approaches in terms of QoE provided to the users, profit generated for the providers, and QoE fairness. © 2018 ACM.",Additional Key Words; Internet service providers; ISP; OTT; OTT-ISP collaboration; Phrases: Over The Top service providers; QoE; QoE management; Quality of experience,Multimedia services; Network architecture; Profitability; Quality of service; Sales; Web services; Key words; OTT-ISP collaboration; Qoe managements; Quality of experience (QoE); Service provider; Internet service providers
Toward personalized activity level prediction in community question answering websites,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047112524&doi=10.1145%2f3187011&partnerID=40&md5=d2eee6718dcc9cd1c393316325422bac,"Community Question Answering (CQA) websites have become valuable knowledge repositories. Millions of internet users resort to CQA websites to seek answers to their encountered questions. CQA websites provide information far beyond a search on a site such as Google due to (1) the plethora of high-quality answers, and (2) the capabilities to post new questions toward the communities of domain experts. While most research efforts have been made to identify experts or to preliminarily detect potential experts of CQA websites, there has been a remarkable shift toward investigating how to keep the engagement of experts. Experts are usually the major contributors of high-quality answers and questions of CQA websites. Consequently, keeping the expert communities active is vital to improving the lifespan of these websites. In this article, we present an algorithm termed PALP to predict the activity level of expert users of CQA websites. To the best of our knowledge, PALP is the first approach to address a personalized activity level prediction model for CQA websites. Furthermore, it takes into consideration user behavior change over time and focuses specifically on expert users. Extensive experiments on the Stack Overflow website demonstrate the competitiveness of PALP over existing methods. © 2018 ACM.",Activity level prediction; Additional Key Words; Logistic regression; Personalized model; Phrases: Question answering website,Behavioral research; Forecasting; Search engines; Activity levels; Key words; Logistic regressions; Personalized model; Question Answering; Websites
Paying more attention to saliency: Image captioning with saliency and context attention,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047156748&doi=10.1145%2f3177745&partnerID=40&md5=85d3ec16b26ef7a2194e6482662f5413,"Image captioning has been recently gaining a lot of attention thanks to the impressive achievements shown by deep captioning architectures, which combine Convolutional Neural Networks to extract image representations and Recurrent Neural Networks to generate the corresponding captions. At the same time, a significant research effort has been dedicated to the development of saliency prediction models, which can predict human eye fixations. Even though saliency information could be useful to condition an image captioning architecture, by providing an indication of what is salient and what is not, research is still struggling to incorporate these two techniques. In this work, we propose an image captioning approach in which a generative recurrent neural network can focus on different parts of the input image during the generation of the caption, by exploiting the conditioning given by a saliency prediction model on which parts of the image are salient and which are contextual. We show, through extensive quantitative and qualitative experiments on large-scale datasets, that our model achieves superior performance with respect to captioning baselines with and without saliency and to different state-of-the-art approaches combining saliency and captioning. © 2018 ACM.",Deep learning; Image captioning; Saliency; Visual saliency prediction,Deep learning; Forecasting; Network architecture; Convolutional neural network; Image captioning; Image representations; Large-scale datasets; Qualitative experiments; Saliency; State-of-the-art approach; Visual saliency; Recurrent neural networks
Can you see what I see? Quality-of-experience measurements of mobile live video broadcasting,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047169016&doi=10.1145%2f3165279&partnerID=40&md5=ee8ddcff772f066e2a95e597d0de19a9,"Broadcasting live video directly from mobile devices is rapidly gaining popularity with applications like Periscope and Facebook Live. The quality of experience (QoE) provided by these services comprises many factors, such as quality of transmitted video, video playback stalling, end-to-end latency, and impact on battery life, and they are not yet well understood. In this article, we examine mainly the Periscope service through a comprehensive measurement study and compare it in some aspects to Facebook Live. We shed light on the usage of Periscope through analysis of crawled data and then investigate the aforementioned QoE factors through statistical analyses as well as controlled small-scale measurements using a couple of different smartphones and both versions, Android and iOS, of the two applications. We report a number of findings including the discrepancy in latency between the two most commonly used protocols, RTMP and HLS, surprising surges in bandwidth demand caused by the Periscope app’s chat feature, substantial variations in video quality, poor adaptation of video bitrate to available upstream bandwidth at the video broadcaster side, and significant power consumption caused by the applications. © 2018 ACM.",Adaptive streaming; Additional Key Words; DASH; Facebook Live; Latency; Live video; Mobile video streaming; Periscope; Phrases: QoE; Video quality,Bandwidth; Image coding; Mobile telecommunication systems; Optical instruments; Social networking (online); Adaptive streaming; DASH; Facebook; Key words; Latency; Live video; Mobile video streaming; Periscope; Phrases: QoE; Video quality; Quality of service
Quality of experience-centric management of adaptive video streaming services: Status and challenges,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047150526&doi=10.1145%2f3165266&partnerID=40&md5=7419cbb3e5d8d904bc078ee951faf758,"Video streaming applications currently dominate Internet traffic. Particularly, HTTP Adaptive Streaming (HAS) has emerged as the dominant standard for streaming videos over the best-effort Internet, thanks to its capability of matching the video quality to the available network resources. In HAS, the video client is equipped with a heuristic that dynamically decides the most suitable quality to stream the content, based on information such as the perceived network bandwidth or the video player buffer status. The goal of this heuristic is to optimize the quality as perceived by the user, the so-called Quality of Experience (QoE). Despite the many advantages brought by the adaptive streaming principle, optimizing users’ QoE is far from trivial. Current heuristics are still suboptimal when sudden bandwidth drops occur, especially in wireless environments, thus leading to freezes in the video playout, the main factor influencing users’ QoE. This issue is aggravated in case of live events, where the player buffer has to be kept as small as possible in order to reduce the playout delay between the user and the live signal. In light of the above, in recent years, several works have been proposed with the aim of extending the classical purely client-based structure of adaptive video streaming, in order to fully optimize users’ QoE. In this article, a survey is presented of research works on this topic together with a classification based on where the optimization takes place. This classification goes beyond client-based heuristics to investigate the usage of server- and network-assisted architectures and of new application and transport layer protocols. In addition, we outline the major challenges currently arising in the field of multimedia delivery, which are going to be of extreme relevance in future years. © 2018 ACM.",Additional Key Words; HTTP adaptive streaming; MPEG-DASH; Phrases: Quality of experience,Bandwidth; HTTP; Multimedia systems; Network architecture; Optimization; Quality of service; Video streaming; Adaptive video streaming; Http adaptive streaming; Key words; Mpeg dashes; Quality of experience (QoE); Transport layer protocols; Video Streaming Applications; Wireless environment; Motion Picture Experts Group standards
Data musicalization,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047112417&doi=10.1145%2f3184742&partnerID=40&md5=09a0c2deb325dec18d2719fe7c3bc810,"Data musicalization is the process of automatically composing music based on given data as an approach to perceptualizing information artistically. The aim of data musicalization is to evoke subjective experiences in relation to the information rather than merely to convey unemotional information objectively. This article is written as a tutorial for readers interested in data musicalization. We start by providing a systematic characterization of musicalization approaches, based on their inputs, methods, and outputs. We then illustrate data musicalization techniques with examples from several applications: one that perceptualizes physical sleep data as music, several that artistically compose music inspired by the sleep data, one that musicalizes on-line chat conversations to provide a perceptualization of liveliness of a discussion, and one that uses musicalization in a gamelike mobile application that allows its users to produce music. We additionally provide a number of electronic samples of music produced by the different musicalization applications. © 2018 ACM.",Automated composition; Computational creativity; Data analysis; Data musicalization; Music; Sonification,Computer networks; Data reduction; Automated composition; Computational creativities; Data musicalization; Music; Sonifications; Hardware
DeepProduct: Mobile product search with portable deep features,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047135639&doi=10.1145%2f3184745&partnerID=40&md5=0fb4b547acc8317837b5b767c3da12de,"Features extracted by deep networks have been popular in many visual search tasks. This article studies deep network structures and training schemes for mobile visual search. The goal is to learn an effective yet portable feature representation that is suitable for bridging the domain gap between mobile user photos and (mostly) professionally taken product images while keeping the computational cost acceptable for mobile-based applications. The technical contributions are twofold. First, we propose an alternative of the contrastive loss popularly used for training deep Siamese networks, namely robust contrastive loss, where we relax the penalty on some positive and negative pairs to alleviate overfitting. Second, a simple multitask fine-tuning scheme is leveraged to train the network, which not only utilizes knowledge from the provided training photo pairs but also harnesses additional information from the large ImageNet dataset to regularize the fine-tuning process. Extensive experiments on challenging real-world datasets demonstrate that both the robust contrastive loss and the multitask fine-tuning scheme are effective, leading to very promising results with a time cost suitable for mobile product search scenarios. © 2018 ACM.",Contrastive loss; Deep learning; Efficiency; Mobile product search,Efficiency; Knowledge management; Computational costs; Feature representation; Mobile products; Mobile visual searches; Product images; Real-world datasets; Technical contribution; Training schemes; Deep learning
Improved audio steganalytic feature and its applications in audio forensics,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047138938&doi=10.1145%2f3190575&partnerID=40&md5=406821ea4282fe7e86a52565acfee56c,"Digital multimedia steganalysis has attracted wide attention over the past decade. Currently, there are many algorithms for detecting image steganography. However, little research has been devoted to audio steganalysis. Since the statistical properties of image and audio files are quite different, features that are effective in image steganalysis may not be effective for audio. In this article, we design an improved audio steganalytic feature set derived from both the time and Mel-frequency domains for detecting some typical steganography in the time domain, including LSB matching, Hide4PGP, and Steghide. The experiment results, evaluated on different audio sources, including various music and speech clips of different complexity, have shown that the proposed features significantly outperform the existing ones. Moreover, we use the proposed features to detect and further identify some typical audio operations that would probably be used in audio tampering. The extensive experiment results have shown that the proposed features also outperform the related forensic methods, especially when the length of the audio clip is small, such as audio clips with 800 samples. This is very important in real forensic situations. © 2018 ACM.",Audio forensics; Audio steganalysis; Markov transition probability,Audio recordings; Feature extraction; Forensic science; Steganography; Audio forensics; Audio steganalysis; Digital multimedia; Image steganalysis; Image steganography; ITS applications; Statistical properties; Transition probabilities; Audio acoustics
Analytical global median filtering forensics based on moment histograms,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047163811&doi=10.1145%2f3176650&partnerID=40&md5=857064b797f28d8900761a85a9bfba83,"Median filtering forensics in images has gained wide attention from researchers in recent years because of its inherent nature of preserving visual traces. Although many forensic methods are developed for median filtering detection, probability of detection reduces under JPEG compression at low-quality factors and for low-resolution images. The feature set reduction is also a challenging issue among existing detectors. In this article, a 19-dimensional feature set is analytically derived from image skewness and kurtosis histograms. This new feature set is exploited for the purpose of global median filtering forensics and verified with exhaustive experimental results. The efficacy of the method is tested on six popular databases (UCID, BOWS2, BOSSBase, NRCS, RAISE, and DID) and found that the new feature set uncovers filtering traces for moderate, low JPEG post-compression and low-resolution operation. Our proposed method yields lowest probability of error and largest area under the ROC curve for most of the test cases in comparison with previous approaches. Some novel test cases are introduced to thoroughly assess the benefits and limitations of the proposed method. The obtained results indicate that the proposed method would provide an important tool to the field of passive image forensics. © 2018 ACM.",Kurtosis histograms; Median filter; Passive image forensics; Skewness; Tamper detection,Digital forensics; Graphic methods; Higher order statistics; Image compression; Passive filters; Area under the ROC curve; Kurtosis histograms; Low resolution images; Passive image forensics; Probability of detection; Probability of errors; Skewness; Tamper detection; Median filters
SABR: Network-assisted content distribution for QoE-driven ABR video streaming,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047149665&doi=10.1145%2f3183516&partnerID=40&md5=2b9ad1f747bd701dfee5d57dc5bb26d9,"State-of-the-art software-defined wide area networks (SD-WANs) provide the foundation for flexible and highly resilient networking. In this work, we design, implement, and evaluate a novel architecture (denoted as SABR) that leverages the benefits of software-defined networking (SDN) to provide network-assisted adaptive bitrate streaming. With clients retaining full control of their streaming algorithms, we clearly show that by this network assistance, both the clients and the content providers benefit significantly in terms of quality of experience (QoE) and content origin offloading. SABR utilizes information on available bandwidths per link and network cache contents to guide video streaming clients with the goal of improving the viewer’s QoE. In addition, SABR uses SDN capabilities to dynamically program flows to optimize the utilization of content delivery network caches. Backed by our study of SDN-assisted streaming, we discuss the change in the requirements for network-to-player APIs that enables flexible video streaming. We illustrate the difficulty of the problem and the impact of SDN-assisted streaming on QoE metrics using various well-established player algorithms. We evaluate SABR together with state-of-the-art dynamic adaptive streaming over HTTP (DASH) quality adaptation algorithms through a series of experiments performed on a real-world, SDN-enabled testbed network with minimal modifications to an existing DASH client. In addition, we compare the performance of different caching strategies in combination with SABR. Our trace-based measurements show the substantial improvement in cache hit rates and QoE metrics in conjunction with SABR indicating a rich design space for jointly optimized SDN-assisted caching architectures for adaptive bitrate video streaming applications. © 2018 ACM.",ABR streaming; Additional Key Words; Caching; DASH; Network-assisted streaming; OpenFlow; Phrases: SDN; QoE; Video quality metrics,Bandwidth; Distributed computer systems; Network architecture; Quality control; Quality of service; Video streaming; Caching; DASH; Key words; Openflow; Phrases: SDN; Video quality; Wide area networks
Over-and under-exposure reconstruction of a single plenoptic capture,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051870610&doi=10.1145%2f3199514&partnerID=40&md5=b74826df2e27ea4a9dccf60f1fede19e,"Light field images, for example, takenwith plenoptic cameras, offer interesting post-processing opportunities, including depth-of-field management, depth estimation, viewpoint selection, and 3D image synthesis. Like most capture devices, however, plenoptic cameras have a limited dynamic range, so that over- A nd underexposed areas in plenoptic images are commonplace. We therefore present a straightforward and robust plenoptic reconstruction technique based on the observation that vignetting causes peripheral views to receive less light than central views. Thus, corresponding pixels in different views can be used to reconstruct illumination, especially in areas where information missing in one view is present in another. Our algorithm accurately reconstructs under- A nd over-exposed regions (known as declipping), additionally affording an increase in peak luminance by up to two f-stops, and a comparable lowering of the noise floor. The key advantages of this approach are that no hardware modifications are necessary to improve the dynamic range, that no multiple exposure techniques are required, and therefore that no ghosting or other artifacts are introduced. © 2018 ACM.",Declipping; Dynamic Range; Light Field Acquisition; Light Field Reconstruction; Overexposure; Under-Exposure,Cameras; Declipping; Dynamic range; Light field acquisitions; Light field reconstruction; Overexposure; Under-Exposure; Image processing
A generic approach to video buffer modeling using discrete-time analysis,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047159173&doi=10.1145%2f3183511&partnerID=40&md5=52489ab485dc341c57e3396eafaa7bc6,"The large share of traffic in the Internet generated by video streaming services puts high loads on access and aggregation networks, resulting in high costs for the content delivery infrastructure. To reduce the bandwidth consumed while maintaining a high playback quality, video players use policies that control and limit the buffer level by using thresholds for pausing and continuing the video download. This allows shaping the bandwidth consumed by video streams and limiting the traffic wasted in case of playback abortion. Especially in mobile scenarios, where the throughput can be highly variant, the buffer policy can have a high impact on the probability of interruptions during video playback. To find the optimal setting for the buffer policy in each network condition, the relationship between the parameters of the buffer policy, the network throughput dynamics, and the corresponding video playback behavior needs to be understood. To this end, we model the video buffer as GI/GI/1 queue with pq-policy using discrete-time analysis. By studying the stochastic properties of the buffer-level distribution, we are able to accurately evaluate the impact of network and video bitrate dynamics on the video playback quality based on the buffer policy. We find a fundamental relationship between the bandwidth variation and the expected interarrival time of segments, meaning that overproportionately more bandwidth is necessary to prevent stalling events for high bandwidth variation. The proposed model further allows to optimize the trade-off between the traffic wasted in case of video abortion and video streaming quality experienced by the user. © 2018 ACM.",Additional Key Words; Bandwidth variation; Buffering policy; Performance analysis; Phrases: Adaptive video streaming; Quality of experience,Bandwidth; Economic and social effects; Quality of service; Stochastic systems; Video streaming; Adaptive video streaming; Bandwidth variation; Key words; Performance analysis; Quality of experience (QoE); Quality control
Spatially coherent feature learning for pose-invariant facial expression recognition,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045207087&doi=10.1145%2f3176646&partnerID=40&md5=11a09100cdadbf5179c6b5d1e5a0cb32,"Feature learning has enjoyed much attention and achieved good performance in recent studies of image processing. Unlike the required training conditions often assumed there, far less labeled data is available for training emotion classification systems. In addition, current feature learning is typically performed on an entire face image without considering the dependency between features. These approaches ignore the fact that faces are structured and the neighboring features are dependent. Thus, the learned features lack the power to describe visually coherent facial images. Our method is therefore designed with the goal of simplifying the problem domain by removing expression-irrelevant factors from the input images, with a key region-based mechanism, which is an effort to reduce the amount of data required to effectively train the feature-learning methods. Meanwhile, we can construct geometric constraints between the key regions and its detected positions. To this end, we introduce a Spatially Coherent featurelearning method for Poseinvariant Facial Expression Recognition (SC-PFER). In our model, we first perform face frontalization through a 3D pose-normalization technique, which could normalize poses while preserving the identity information through synthesizing frontal faces for facial images with arbitrary views. Subsequently, we select a sequence of key regions around 51 key points in the synthetic frontal face images for efficient unsupervised feature learning. Finally, we introduce a linkage structure over the learning-based features and the corresponding geometry information of each key region to encode the dependencies of the regions. Our method, on the whole, does not require training multiple models for each specific pose and avoids separating training and parameter tuning for each pose. The proposed framework has been evaluated on two benchmark databases, BU-3DFE and SFEW, for pose-invariant Facial Expression Recognition (FER). The experimental results demonstrate that our algorithm outperforms current state-of-the-art FER methods. Specifically, our model achieves an improvement of 1.72% and 1.11% FER accuracy, on average, on BU-3DFE and SFEW, respectively. © 2018 ACM.",Facial expression recognition; Learning-based features; Pose-invariant; Spatially coherent feature learning,Human computer interaction; Emotion classification systems; Facial expression recognition; Feature learning; Geometric constraint; Geometry information; Learning-based features; Pose invariant; Unsupervised feature learning; Face recognition
Sequential articulated motion reconstruction from a monocular image sequence,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045234736&doi=10.1145%2f3180420&partnerID=40&md5=0c3a24ed98033c20f6d56bf1d3489311,"In this article, we present a sequential approach for articulated motion estimation from a 2D skeleton sequence. This is a challenging task due to the complexity of human movements and the inherent depth ambiguities. The proposed approach models the human movement on a kinematic manifold with the tangent bundle, which is a natural geometrical representation of articulated motion. Combined with a second-order stochastic dynamic model based on the Markov hypothesis, we generalize the Extended Rauch Tung Striebel smoother to a Riemannian manifold to simulate the process of human movement. The human motor system might violate the Markov hypothesis when the human body is subject to external forces, and therefore a refinement stage is introduced to correct the estimation error. Specifically, the current estimation is refined in a feasible solution region consisting of a set of local estimations. This region is called a simplex, in which each element can be represented by a convex hull of all ingredients. We have proved that the refinement problem can be converted into a convex optimization problem with the simplicial constraint. Since the proposed formulation conforms to the principles of kinematic and spatio-temporal continuity of articulated motion, the reconstruction ambiguity can be alleviated essentially. The performance of the proposed algorithm is conducted on multiple synthetic sequences from the CMU and the HDM05 MoCap databases. The results show that, without requiring any training data, the proposed approach achieves greater accuracy over state-ofthe- Art baselines. Furthermore, the proposed approach outperforms two baselines on real sequences from the Human3.6m MoCap database. © 2018 ACM.",Articulated motion; Convex hull; Extend rauch tung striebel smoother; Riemannian manifold; Simplex,Computational geometry; Convex optimization; Estimation; Image reconstruction; Kinematics; Stochastic models; Stochastic systems; Articulated motion; Convex hull; Rauch-tung-striebel smoothers; Riemannian manifold; Simplex; Motion estimation
Full 3D reconstruction of non-rigidly deforming objects,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045262193&doi=10.1145%2f3177756&partnerID=40&md5=29aaf86cc85fda5a8817d22deddb26d9,"In this article, we discuss enhanced full 360° 3D reconstruction of dynamic scenes containing non-rigidly deforming objects using data acquired from commodity depth or 3D cameras. Several approaches for enhanced and full 3D reconstruction of non-rigid objects have been proposed in the literature. These approaches suffer from several limitations due to requirement of a template, inability to tackle large local deformations and topology changes, inability to tackle highly noisy and low-resolution data, and inability to produce online results. We target online and template-free enhancement of the quality of noisy and low-resolution full 3D reconstructions of dynamic non-rigid objects. For this purpose, we propose a view-independent recursive and dynamic multi-frame 3D super-resolution scheme for noise removal and resolution enhancement of 3D measurements. The proposed scheme tracks the position and motion of each 3D point at every timestep by making use of the current acquisition and the result of the previous iteration. The effects of system blur due to per-point tracking are subsequently tackled by introducing a novel and efficient multi-level 3D bilateral total variation regularization. These characteristics enable the proposed scheme to handle large deformations and topology changes accurately. A thorough evaluation of the proposed scheme on both real and simulated data is carried out. The results show that the proposed scheme improves upon the performance of the state-of-the-art methods and is able to accurately enhance the quality of low-resolution and highly noisy 3D reconstructions while being robust to large local deformations. © 2018 ACM.",3D bilateral total variation; 3D point tracking; 3D reconstruction; Non-rigid registration; Point-cloud enhancement; Super-resolution,Cameras; Deformation; Iterative methods; Optical resolving power; Repair; Rigidity; Topology; 3D reconstruction; Bilateral total variations; Nonrigid registration; Point cloud; Point tracking; Super resolution; Image reconstruction
Deformation-based 3D facial expression representation,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045201238&doi=10.1145%2f3176649&partnerID=40&md5=59857487104209db0406c30f6d22e2b8,"We propose a deformation-based representation for analyzing expressions fromthree-dimensional (3D) faces. A point cloud of a 3D face is decomposed into an ordered deformable set of curves that start from a fixed point. Subsequently, a mapping function is defined to identify the set of curves with an element of a highdimensional matrix Lie group, specifically the direct product of SE(3). Representing 3D faces as an element of a high-dimensional Lie group has two main advantages. First, using the group structure, facial expressions can be decoupled from a neutral face. Second, an underlying non-linear facial expression manifold can be captured with the Lie group and mapped to a linear space, Lie algebra of the group. This opens up the possibility of classifying facial expressions with linear models without compromising the underlying manifold. Alternatively, linear combinations of linearised facial expressions can be mapped back from the Lie algebra to the Lie group. The approach is tested on the Binghamton University 3D Facial Expression (BU-3DFE) and the Bosphorus datasets. The results show that the proposed approach performed comparably, on the BU-3DFE dataset, without using features or extensive landmark points. © 2018 ACM.",3D face deformation; 3D facial expression representation; Expression modelling,Deformation; 3-d facial expressions; 3D faces; Binghamton University; Facial Expressions; High-dimensional; Linear combinations; Mapping functions; Matrix lie groups; Lie groups
Introduction to the special section on multimedia computing and applications of socio-affective behaviors in the wild,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045282911&doi=10.1145%2f3181711&partnerID=40&md5=3b2d63392384c4eaf17fd133523f54ab,[No abstract available],,
"Detection of human, legitimate bot, and malicious bot in online social networks based on wavelets",2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045180136&doi=10.1145%2f3183506&partnerID=40&md5=a3038e44cf7e2496b2c66a15e493c654,"Social interactions take place in environments that influence people's behaviours and perceptions. Nowadays, the users of Online Social Network (OSN) generate a massive amount of content based on social interactions. However, OSNs wide popularity and ease of access created a perfect scenario to practice malicious activities, compromising their reliability. To detect automatic information broadcast in OSN, we developed a waveletbased model that classifies users as being human, legitimate robot, or malicious robot, as a result of spectral patterns obtained from users' textual content.We create the feature vector from the DiscreteWavelet Transform along with a weighting scheme called Lexicon-based Coefficient Attenuation. In particular, we induce a classificationmodel using the Random Forest algorithm over two real Twitter datasets. The corresponding results show the developed model achieved an average accuracy of 94.47% considering two different scenarios: Single theme and miscellaneous one. © 2018 ACM.",Bots; OSN frauds; Text mining; Wavelets; Writing style,Classification (of information); Data mining; Decision trees; Online systems; Tellurium compounds; Bots; OSN frauds; Text mining; Wavelets; Writing style; Social networking (online)
Texture and geometry scattering representation-based facial expression recognition in 2D+3D videos,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045237620&doi=10.1145%2f3131345&partnerID=40&md5=1b1643ed5d483f5f46485af373eb4108,"Facial Expression Recognition (FER) is one of the most important topics in the domain of computer vision and pattern recognition, and it has attracted increasing attention for its scientific challenges and application potentials. In this article, we propose a novel and effective approach to FER using multi-model two-dimensional (2D) and 3D videos, which encodes both static and dynamic clues by scattering convolution network. First, a shape-based detection method is introduced to locate the start and the end of an expression in videos; segment its onset, apex, and offset states; and sample the important frames for emotion analysis. Second, the frames in Apex of 2D videos are represented by scattering, conveying static texture details. Those of 3D videos are processed in a similar way, but to highlight static shape details, several geometric maps in terms of multiple order differential quantities, i.e., Normal Maps and Shape Index Maps, are generated as the input of scattering, instead of original smooth facial surfaces. Third, the average of neighboring samples centred at each key texture frame or shape map in Onset is computed, and the scattering features extracted from all the average samples of 2D and 3D videos are then concatenated to capture dynamic texture and shape cues, respectively. Finally, Multiple Kernel Learning is adopted to combine the features in the 2D and 3D modalities and compute similarities to predict the expression label. Thanks to the scattering descriptor, the proposed approach not only encodes distinct local texture and shape variations of different expressions as by several milestone operators, such as SIFT, HOG, and so on, but also captures subtle information hidden in high frequencies in both channels, which is quite crucial to better distinguish expressions that are easily confused. The validation is conducted on the BU-4DFE and BP-4D databa ses, and the accuracies reached are very competitive, indicating its competency for this issue. © 2018 ACM.",2D and 3D videos; Facial expression recognition; Multi-modal fusion; The scattering descriptor,Encoding (symbols); Maps; 3-D videos; Descriptors; Detection methods; Effective approaches; Facial expression recognition; Multi-modal fusion; Multiple Kernel Learning; Two Dimensional (2 D); Face recognition
"Introduction to the special issue on representation, analysis, and recognition of 3D humans",2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045264856&doi=10.1145%2f3181709&partnerID=40&md5=7c3a9b7667c43279c6fc42d49f970307,[No abstract available],,
Gait recognition from motion capture data,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042907000&doi=10.1145%2f3152124&partnerID=40&md5=d6cb9569bea4b7200cc6a65f3754d099,"Gait recognition from motion capture data, as a pattern classification discipline, can be improved by the use of machine learning. This article contributes to the state of the art with a statistical approach for extracting robust gait features directly from raw data by a modification of Linear Discriminant Analysis with Maximum Margin Criterion. Experiments on the CMU MoCap database show that the suggested method outperforms 13 relevant methods based on geometric features and a method to learn the features by a combination of Principal Component Analysis and Linear Discriminant Analysis. The methods are evaluated in terms of the distribution of biometric templates in respective feature spaces expressed in a number of class separability coefficients and classification metrics. Results also indicate a high portability of learned features, what means that we can learn what aspects of walk people generally differ in and extract those as general gait features. Recognizing people without needing group-specific features is convenient, as particular people might not always provide annotated learning data. As a contribution to reproducible research, our evaluation framework and database have been made publicly available. This research makes motion capture technology directly applicable for human recognition. © 2018 ACM",Gait recognition; Maximal margin criterion; MoCap,Discriminant analysis; Gait analysis; Learning algorithms; Learning systems; Pattern recognition; Principal component analysis; Evaluation framework; Gait recognition; Linear discriminant analysis; Maximal margin; Maximum margin criterions; MoCap; Reproducible research; Statistical approach; Motion estimation
"Combining facial parts for learning gender, ethnicity, and emotional state based on RGB-D information",2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045199316&doi=10.1145%2f3152125&partnerID=40&md5=257b0a0ee3669200ea305c09e801a0d6,"With the success of emerging RGB-D cameras such as the Kinect sensor, combining the shape (depth) and texture information to improve the quality of recognition became a trend among computer vision researchers. In this work, we address the problem of face classification in the context of RGB images and depth data. Inspired by the psychological results for human face perception, this article focuses on (i) finding out which facial parts are most effective at making the difference for some social aspects of face perception (gender, ethnicity, and emotional state), (ii) determining the optimal decision by combining the decision rendered by the individual parts, and (iii) extracting the promising features from RGB-D faces to exploit all the potential that this data provide. Experimental results on EurecomKinect Face and CurtinFaces databases show that the proposed approach improves the recognition quality in many use cases. © 2018 ACM.",Face recognition; Facial parts; Kinect; RGB-D data,Behavioral research; Social aspects; Emotional state; Face classification; Face perceptions; Facial parts; Kinect; Optimal decisions; RGB-D data; Texture information; Face recognition
A unified framework for multi-modal isolated gesture recognition,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042912584&doi=10.1145%2f3131343&partnerID=40&md5=e73e61f5ed08a366dd408259ee6a7460,"In this article, we focus on isolated gesture recognition and explore different modalities by involving RGB stream, depth stream, and saliency stream for inspection. Our goal is to push the boundary of this realm even further by proposing a unified framework that exploits the advantages of multi-modality fusion. Specifically, a spatial-temporal network architecture based on consensus-voting has been proposed to explicitly model the long-term structure of the video sequence and to reduce estimation variance when confronted with comprehensive inter-class variations. In addition, a three-dimensional depth-saliency convolutional network is aggregated in parallel to capture subtle motion characteristics. Extensive experiments are done to analyze the performance of each component and our proposed approach achieves the best results on two public benchmarks, ChaLearn IsoGD and RGBD-HuDaAct, outperforming the closest competitor by a margin of over 10% and 15%, respectively. Our project and codes will be released at https://davidsonic.github.io/index/acm_tomm_2017.html. © 2018 ACM",3D convolution; Consensus-voting; Isolated gesture recognition; Multi-modal,Benchmarking; Convolution; Network architecture; Architecture-based; Consensus voting; Convolutional networks; Estimation variance; Long-term structures; Multi-modal; Multi-modality fusion; Spatial temporals; Gesture recognition
Early recognition of 3D human actions,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045233196&doi=10.1145%2f3131344&partnerID=40&md5=0d864402f0da5883a195bd2d41dca348,"Action recognition is an important research problem of human motion analysis (HMA). In recent years, 3D observation-based action recognition has been receiving increasing interest in the multimedia and computer vision communities, due to the recent advent of cost-effective sensors, such as depth camera Kinect. Thiswork takes this one step further, focusing on early recognition of ongoing 3D human actions, which is beneficial for a large variety of time-critical applications, e.g., gesture-based human machine interaction, somatosensory games, and so forth. Our goal is to infer the class label information of 3D human actions with partial observation of temporally incomplete action executions. By considering 3D action data as multivariate time series (m.t.s.) synchronized to a shared common clock (frames), we propose a stochastic process called dynamic marked point process (DMP) to model the 3D action as temporal dynamic patterns, where both timing and strength information are captured. To achieve even more early and better accuracy of recognition, we also explore the temporal dependency patterns between feature dimensions. A probabilistic suffix tree is constructed to represent sequential patterns among features in terms of the variable-order Markov model (VMM). Our approach and several baselines are evaluated on five 3D human action datasets. Extensive results show that our approach achieves superior performance for early recognition of 3D human actions. © 2018 ACM.",3D action recognition; Marked point process; Motion analysis,Cost effectiveness; Markov processes; Motion analysis; Stochastic models; Stochastic systems; Action recognition; Class label informations; Human machine interaction; Marked point process; Multivariate time series; Probabilistic suffix trees; Time-critical applications; Variable Order Markov Models; Image recognition
"Representation, analysis, and recognition of 3D humans: A survey",2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042683665&doi=10.1145%2f3182179&partnerID=40&md5=1796ffbb184b75bc6b633c096ca52d33,"Computer Vision and Multimedia solutions are now offering an increasing number of applications ready for use by end users in everyday life. Many of these applications are centered for detection, representation, and analysis of face and body. Methods based on 2D images and videos are the most widespread, but there is a recent trend that successfully extends the study to 3D human data as acquired by a new generation of 3D acquisition devices. Based on these premises, in this survey, we provide an overview on the newly designed techniques that exploit 3D human data and also prospect the most promising current and future research directions. In particular, we first propose a taxonomy of the representation methods, distinguishing between spatial and temporal modeling of the data. Then, we focus on the analysis and recognition of 3D humans from 3D static and dynamic data, considering many applications for body and face. © 2018 ACM.",3D face and body analysis and retrieval; 3D face and body representation; 3D humans; 3D shape representation,Computer networks; Hardware; 3-d acquisitions; 3D faces; 3D humans; 3D shape representation; Body representations; Future research directions; Representation method; Spatial and temporal modeling; Surveys
Joint estimation of age and expression by combining scattering and convolutional networks,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042552102&doi=10.1145%2f3152118&partnerID=40&md5=c006c617f78e9ae63d37167049d54f05,"This article tackles the problem of joint estimation of human age and facial expression. This is an important yet challenging problem because expressions can alter face appearances in a similar manner to human aging. Different from previous approaches that deal with the two tasks independently, our approach trains a convolutional neural network (CNN) model that unifies ordinal regression and multi-class classification in a single framework. We demonstrate experimentally that our method performs more favorably against state-of-the-art approaches. © 2018 ACM.",Age estimation; Convolutional networks; Deep learning; Expression recognition; Multi-level regression; Multi-task learning; Scattering network; Transfer learning,Convolution; Learning systems; Neural networks; Age estimation; Convolutional networks; Expression recognition; Multilevels; Multitask learning; Scattering networks; Transfer learning; Deep learning
Adaptive fractional-Pixel motion estimation skipped algorithm for efficient HEVC motion estimation,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042523346&doi=10.1145%2f3159170&partnerID=40&md5=69bf3fa749650100502eb384d8046255,"High-Efficiency Video Coding (HEVC) efficiently addresses the storage and transmit problems of high-definition videos, especially for 4K videos. The variable-size Prediction Units (PUs)–based Motion Estimation (ME) contributes a significant compression rate to the HEVC encoder and also generates a huge computation load. Meanwhile, high-level encoding complexity prevents widespread adoption of the HEVC encoder in multimedia systems. In this article, an adaptive fractional-pixel ME skipped scheme is proposed for low-complexity HEVC ME. First, based on the property of the variable-size PUs–based ME process and the video content partition relationship among variable-size PUs, all inter- PU modes during a coding unit encoding process are classified into root-type PU mode and children-type PU modes. Then, according to the ME result of the root-type PU mode, the fractional-pixel ME of its children-type PU modes is adaptively skipped. Simulation results show that, compared to the original ME in HEVC reference software, the proposed algorithm reduces ME encoding time by an average of 63.22% while encoding efficiency performance is maintained. © 2017 ACM.",Fractional-pixel motion estimation; HEVC; Pharses: fast motion estimation; Video coding; Visual communication,Codes (symbols); Computer software; Efficiency; Encoding (symbols); Image coding; Multimedia systems; Pixels; Signal encoding; Video cameras; Video signal processing; Visual communication; Encoding complexity; Encoding efficiency; Fast motion estimation; Fractional pixel motion estimation; HEVC; High definition video; High Efficiency Video Coding (HEVC); Reference software; Motion estimation
Robust privacy-Preserving image sharing over online social networks (OSNs),2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042524033&doi=10.1145%2f3165265&partnerID=40&md5=211adaf734f9bd3f06b4ce89f45b504f,"Sharing images online has become extremely easy and popular due to the ever-increasing adoption of mobile devices and online social networks (OSNs). The privacy issues arising from image sharing over OSNs have received significant attention in recent years. In this article, we consider the problem of designing a secure, robust, high-fidelity, storage-efficient image-sharing scheme over Facebook, a representative OSN that is widely accessed. To accomplish this goal, we first conduct an in-depth investigation on the manipulations that Facebook performs to the uploaded images. Assisted by such knowledge, we propose a DCT-domain image encryption/decryption framework that is robust against these lossy operations. As verified theoretically and experimentally, superior performance in terms of data privacy, quality of the reconstructed images, and storage cost can be achieved. © 2018 ACM.",Image sharing; JPEG; Online social networks; Privacy-preserving,Cryptography; Data privacy; Digital storage; Image compression; Image encryptions; Image sharing; JPEG; On-line social networks; Online social networks (OSNs); Privacy preserving; Reconstructed image; Storage costs; Social networking (online)
Robust multi-Variate temporal features of multi-Variate time series,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042553058&doi=10.1145%2f3152123&partnerID=40&md5=771b6fb001e835eaa4994745429daf59,"Many applications generate and/or consume multi-variate temporal data, and experts often lack the means to adequately and systematically search for and interpret multi-variate observations. In this article, we first observe that multi-variate time series often carry localized multi-variate temporal features that are robust against noise. We then argue that these multi-variate temporal features can be extracted by simultaneously considering, at multiple scales, temporal characteristics of the time series along with external knowledge, including variate relationships that are known a priori. Relying on these observations, we develop data models and algorithms to detect robust multi-variate temporal (RMT) features that can be indexed for efficient and accurate retrieval and can be used for supporting data exploration and analysis tasks. Experiments confirm that the proposed RMT algorithm is highly effective and efficient in identifying robust multi-scale temporal features of multi-variate time series. © 2018 ACM.",Multi-variate time series; Robust multi-variate temporal features,Search engines; Data exploration; External knowledge; Models and algorithms; Multi-scale; Multiple scale; Temporal characteristics; Temporal Data; Temporal features; Time series
Visual background recommendation for dance performances using deep matrix factorization,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042530013&doi=10.1145%2f3152463&partnerID=40&md5=1c7969d26e611810fc48e8ea1d639e6d,"The stage background is one of the most important features for a dance performance, as it helps to create the scene and atmosphere. In conventional dance performances, the background images are usually selected or designed by professional stage designers according to the theme and the style of the dance. In new media dance performances, the stage effects are usually generated by media editing software. Selecting or producing a dance background is quite challenging and is generally carried out by skilled technicians. The goal of the research reported in this article is to ease this process. Instead of searching for background images from the sea of available resources, dancers are recommended images that they are more likely to use. This work proposes the idea of a novel system to recommend images based on content-based social computing. The core part of the system is a probabilistic prediction model to predict a dancer’s interests in candidate images through social platforms. Different from traditional collaborative filtering or content-based models, the model proposed here effectively combines a dancer’s social behaviors (rating action, click action, etc.) with the visual content of images shared by the dancer using deep matrix factorization (DMF). With the help of such a system, dancers can select from the recommended images and set them as the backgrounds of their dance performances through a media editor. According to the experiment results, the proposed DMF model outperforms the previous methods, and when the dataset is very sparse, the proposed DMF model shows more significant results. © 2018 ACM.",Content-based social computing; Dance background; Image recommendation; Interactive dance,Collaborative filtering; Factorization; Background image; Dance background; Image recommendation; Important features; Interactive dance; Matrix factorizations; Probabilistic prediction; Social computing; Matrix algebra
Emotion recognition using multiple kernel learning toward E-learning applications,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042548723&doi=10.1145%2f3131287&partnerID=40&md5=a0f484f77f53e11a8454d7d0f2c97133,"Adaptive Educational Hypermedia (AEH) e-learning models aim to personalize educational content and learning resources based on the needs of an individual learner. The Adaptive Hypermedia Architecture (AHA) is a specific implementation of the AEH model that exploits the cognitive characteristics of learner feedback to adapt resources accordingly. However, beside cognitive feedback, the learning realm generally includes both the affective and emotional feedback of the learner, which is often neglected in the design of e-learning models. This article aims to explore the potential of utilizing affect or emotion recognition research in AEH models. The framework is referred to as Multiple Kernel Learning Decision Tree Weighted Kernel Alignment (MKLDT-WFA). The MKLDT-WFA has two merits over classical MKL. First, the WFA component only preserves the relevant kernel weights to reduce redundancy and improve the discrimination for emotion classes. Second, training via the decision tree reduces the misclassification issues associated with the SimpleMKL. The proposed work has been evaluated on different emotion datasets and the results confirm the good performances. Finally, the conceptual Emotion-based E-learning Model (EEM) with the proposed emotion recognition framework is proposed for future work. © 2018 ACM.",Decision tree; E-learning; Emotion recognition; Multiclass classification; Multiple kernel learning; SimpleMKL,Decision trees; E-learning; Hypermedia systems; Speech recognition; Adaptive educational hypermedia; Cognitive characteristics; e-Learning application; Educational contents; Emotion recognition; Multi-class classification; Multiple Kernel Learning; SimpleMKL; Learning systems
Deep bidirectional cross-Triplet embedding for online clothing shopping,2018,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042536711&doi=10.1145%2f3152114&partnerID=40&md5=035bb5ad4933dd7ab3e35205ad6fdfaa,"In this article, we address the cross-domain (i.e., street and shop) clothing retrieval problem and investigate its real-world applications for online clothing shopping. It is a challenging problem due to the large discrepancy between street and shop domain images. We focus on learning an effective feature-embedding model to generate robust and discriminative feature representation across domains. Existing triplet embedding models achieve promising results by finding an embedding metric in which the distance between negative pairs is larger than the distance between positive pairs plus a margin. However, existing methods do not address the challenges in the cross-domain clothing retrieval scenario sufficiently. First, the intradomain and cross-domain data relationships need to be considered simultaneously. Second, the number of matched and nonmatched cross-domain pairs are unbalanced. To address these challenges, we propose a deep cross-triplet embedding algorithm together with a cross-triplet sampling strategy. The extensive experimental evaluations demonstrate the effectiveness of the proposed algorithms well. Furthermore, we investigate two novel online shopping applications, clothing trying on and accessories recommendation, based on a unified cross-domain clothing retrieval framework. © 2018 ACM.",Accessory recommendation; Clothing retrieval; Cross domain; Deep learning; Triplet embedding,Computer networks; Hardware; Clothing retrievals; Cross-domain; Data relationships; Discriminative features; Embedding algorithms; Experimental evaluation; Sampling strategies; Triplet embedding; Deep learning
Delay-aware quality optimization in cloud-assisted video streaming system,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040082586&doi=10.1145%2f3152116&partnerID=40&md5=6daabfc19e8fc4b2af5588f583b273d1,"Cloud-assisted video streaming has emerged as a new paradigm to optimize multimedia content distribution over the Internet. This article investigates the problem of streaming cloud-assisted real-time video to multiple destinations (e.g., cloud video conferencing, multi-player cloud gaming, etc.) over lossy communication networks. The user diversity and network dynamics result in the delay differences among multiple destinations. This research proposes Differentiated cloud-Assisted VIdeo Streaming (DAVIS) framework,which proactively leverages such delay differences in video coding and transmission optimization. First, we analytically formulate the optimization problem of joint coding and transmission to maximize received video quality. Second, we develop a quality optimization framework that integrates the video representation selection and FEC (Forward Error Correction) packet interleaving. The proposed DAVIS is able to effectively perform differentiated quality optimization for multiple destinations by taking advantage of the delay differences in cloud-assisted video streaming system. We conduct the performance evaluation through extensive experiments with the Amazon EC2 instances and Exata emulation platform. Evaluation results show that DAVIS outperforms the reference cloud-assisted streaming solutions in video quality and delay performance. © 2017 ACM.",Burst loss; Cloud-assisted video streaming; Delay-awareness; Differentiated transmission; Quality optimization,Error correction; Image coding; Optimization; Quality control; Video conferencing; Video streaming; Burst loss; Delay awareness; Multimedia content distribution; Multiple destinations; Optimization problems; Quality optimization; Video coding and transmission; Video representations; Video signal processing
Implicit emotion communication: EEG classification and haptic feedback,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040311565&doi=10.1145%2f3152128&partnerID=40&md5=7fdb19f2ef07f10627c81595189da20d,"Today, ubiquitous digital communication systems do not have an intuitive, natural way of communicating emotion, which, in turn, affects the degree to which humans can emotionally connect and interact with one another. To address this problem, a more natural, intuitive, and implicit emotion communication system was designed and created that employs asymmetry-based EEG emotion classification for detecting the emotional state of the sender and haptic feedback (in the form of tactile gestures) for displaying emotions for a receiver. Emotions are modeled in terms of valence (positive/negative emotions) and arousal (intensity of the emotion). Performance analysis shows that the proposed EEG subject-dependent emotion classification model with Free Asymmetry features allows for more flexible feature-generation schemes than other existing algorithms and attains an average accuracy of 92.5% for valence and 96.5% for arousal, outperforming previous-generation schemes in high feature space. As for the haptic feedback, a tactile gesture authoring tool and a haptic jacket were developed to design tactile gestures that can intensify emotional reactions in terms of valence and arousal. Experimental study demonstrated that subject-independent emotion transmission through tactile gestures is effective for the arousal dimension of an emotion but is less effective for valence. Consistency in subject-dependent responses for both valence and arousal suggests that personalized tactile gestures would be more effective. © 2017 ACM.",Affective computing; Affective haptics; Multimodal interaction; Tactile gestures,Classification (of information); Affective Computing; Emotion classification; Emotional reactions; Feature generation; Haptics; Multi-Modal Interactions; Performance analysis; Tactile gestures; Digital communication systems
DeepSearch: A fast image search framework for mobile devices,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040084742&doi=10.1145%2f3152127&partnerID=40&md5=4caf6b7f5821e0f00af61532b358f14e,"Content-based image retrieval (CBIR) is one of the most important applications of computer vision. In recent years, there have been many important advances in the development of CBIR systems, especially Convolutional Neural Networks (CNNs) and other deep-learning techniques. On the other hand, current CNN-based CBIR systems suffer from high computational complexity of CNNs. This problem becomes more severe as mobile applications become more and more popular. The current practice is to deploy the entire CBIR systems on the server side while the client side only serves as an image provider. This architecture can increase the computational burden on the server side, which needs to process thousands of requests per second.Moreover, sending images have the potential of personal information leakage. As the need of mobile search expands, concerns about privacy are growing. In this article, we propose a fast image search framework, named DeepSearch, which makes complex image search based on CNNs feasible on mobile phones. To implement the huge computation of CNN models, we present a tensor Block Term Decomposition (BTD) approach as well as a nonlinear response reconstruction method to accelerate the CNNs involving in object detection and feature extraction. The extensive experiments on the ImageNet dataset and Alibaba Large-scale Image Search Challenge dataset show that the proposed accelerating approach BTD can significantly speed up the CNN models and further makes CNN-based image search practical on common smart phones. © 2017 ACM.",Acceleration; Convolutional neural networks; Image retrieval; Tensor decomposition,Acceleration; Complex networks; Content based retrieval; Convolution; Feature extraction; Mobile telecommunication systems; Neural networks; Object detection; Smartphones; Telephone sets; Tensors; Block term decompositions; Computational burden; Content-Based Image Retrieval; Convolutional neural network; Learning techniques; Non-linear response; Personal information; Tensor decomposition; Image retrieval
Online early-late fusion based on adaptive HMM for sign language recognition,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040312162&doi=10.1145%2f3152121&partnerID=40&md5=232f711e73364d7d47e610637846cc38,"In sign language recognition (SLR) with multimodal data, a signword can be represented by multiply features, for which there exist an intrinsic property and a mutually complementary relationship among them. To fully explore those relationships, we propose an online early-late fusion method based on the adaptive Hidden Markov Model (HMM). In terms of the intrinsic property, we discover that inherent latent change states of each sign are related not only to the number of key gestures and body poses but also to their translation relationships. We propose an adaptive HMM method to obtain the hidden state number of each sign by affinity propagation clustering. For the complementary relationship, we propose an online early-late fusion scheme. The early fusion (feature fusion) is dedicated to preserving useful information to achieve a better complementary score, while the late fusion (score fusion) uncovers the significance of those features and aggregates them in a weighting manner. Different from classical fusion methods, the fusion is query adaptive. For different queries, after feature selection (including the combined feature), the fusion weight is inversely proportional to the area under the curve of the normalized query score list for each selected feature. Thewhole fusion process is effective and efficient. Experiments verify the effectiveness on the signer-independent SLR with large vocabulary. Compared either on different dataset sizes or to different SLR models, our method demonstrates consistent and promising performance. © 2017 ACM.",HMM; Multi-modal feature fusion; Online algorithm; Query-adaptive; Sign language recognition,Markov processes; Adaptive hidden Markov models; Affinity propagation clustering; Area under the curves; Complementary relationship; Feature fusion; On-line algorithms; Query-adaptive; Sign Language recognition; Hidden Markov models
Learning label preserving binary codes for multimedia retrieval: A general approach,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040309396&doi=10.1145%2f3152126&partnerID=40&md5=cd300e3f1ccae44856711a26cdc066fd,"Learning-based hashing has been researched extensively in the past few years due to its great potential in fast and accurate similarity search among huge volumes of multimedia data. In this article, we present a novel multimedia hashing framework, called Label Preserving Multimedia Hashing (LPMH) for multimedia similarity search. In LPMH, a general optimization method is used to learn the joint binary codes of multiple media types by explicitly preserving semantic label information. Compared with existing hashing methods which are typically developed under and thus restricted to some specific objective functions, the proposed optimization strategy is not tied to any specific loss function and can easily incorporate bit balance constraints to produce well-balanced binary codes. Specifically, our formulation leads to a set of Binary Integer Programming (BIP) problems that have exact solutions both with and without bit balance constraints. These problems can be solved extremely fast and the solution can easily scale up to large-scale datasets. In the hash function learning stage, the boosted decision trees algorithm is utilized to learn multiple media-specific hash functions that can map heterogeneous data sources into a homogeneous Hamming space for cross-media retrieval.We have comprehensively evaluated the proposed method using a range of large-scale datasets in both singlemedia and cross-media retrieval tasks. The experimental results demonstrate that LPMH is competitive with state-of-the-art methods in both speed and accuracy. © 2017 ACM.",Binary integer programming; Discrete optimization; Large-scale similarity search; Learning to hash; Multimedia retrieval; Supervised learning,Binary codes; Bins; Codes (symbols); Decision trees; Hash functions; Multimedia systems; Optimization; Semantics; Supervised learning; Trees (mathematics); Binary integer programming; Discrete optimization; Learning to hash; Multimedia Retrieval; Scale similarity; Integer programming
A discriminatively learned CNN embedding for person reidentification,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040036929&doi=10.1145%2f3159171&partnerID=40&md5=35c72e5e8f5f33534fabb81132b2e617,"In this article, we revisit two popular convolutional neural networks in person re-identification (re-ID): verification and identification models. The two models have their respective advantages and limitations due to different loss functions. Here, we shed light on how to combine the two models to learn more discriminative pedestrian descriptors. Specifically, we propose a Siamese network that simultaneously computes the identification loss and verification loss. Given a pair of training images, the network predicts the identities of the two input images and whether they belong to the same identity. Our network learns a discriminative embedding and a similarity measurement at the same time, thus taking full usage of the re-ID annotations. Our method can be easily applied on different pretrained networks. Albeit simple, the learned embedding improves the state-of-the-art performance on two public person re-ID benchmarks. Further, we show that our architecture can also be applied to image retrieval. © 2017 ACM.",Convolutional neural networks; Person reidentification,Benchmarking; Convolution; Neural networks; Convolutional neural network; Identification model; Input image; Loss functions; Person re identifications; Similarity measurements; State-of-the-art performance; Training image; Image retrieval
Egocentric hand detection via dynamic region growing,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040067703&doi=10.1145%2f3152129&partnerID=40&md5=6bcd44da19b39c3cba36d5cb59ef9068,"Egocentric videos, which mainly record the activities carried out by the users of wearable cameras, have drawn much research attention in recent years. Due to its lengthy content, a large number of ego-related applications have been developed to abstract the captured videos. As the users are accustomed to interacting with the target objects using their own hands, while their hands usually appear within their visual fields during the interaction, an egocentric hand detection step is involved in tasks like gesture recognition, action recognition, and social interaction understanding. In this work, we propose a dynamic region-growing approach for hand region detection in egocentric videos, by jointly considering hand-related motion and egocentric cues. We first determine seed regions that most likely belong to the hand, by analyzing the motion patterns across successive frames. The hand regions can then be located by extending from the seed regions, according to the scores computed for the adjacent superpixels. These scores are derived from four egocentric cues: contrast, location, position consistency, and appearance continuity. We discuss how to apply the proposed method in real-life scenarios, where multiple hands irregularly appear and disappear from the videos. Experimental results on public datasets show that the proposed method achieves superior performance compared with the state-of-the-art methods, especially in complicated scenarios. © 2017 ACM.",Egocentric hand detection; Egocentric videos; Hand region growing; Seed region generation,Computer networks; Hardware; Action recognition; Dynamic region growing; Egocentric videos; Hand detection; Region detection; Region growing; Social interactions; State-of-the-art methods; Palmprint recognition
Semantic reasoning in zero example video event retrieval,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033233261&doi=10.1145%2f3131288&partnerID=40&md5=22e7cbe39776561f67f53fea06b93631,"Searching in digital video data for high-level events, such as a parade or a car accident, is challenging when the query is textual and lacks visual example images or videos. Current research in deep neural networks is highly beneficial for the retrieval of high-level events using visual examples, but without examples it is still hard to (1) determine which concepts are useful to pre-train (Vocabulary challenge) and (2) which pre-trained concept detectors are relevant for a certain unseen high-level event (Concept Selection challenge). In our article, we present our Semantic Event Retrieval System which (1) shows the importance of high-level concepts in a vocabulary for the retrieval of complex and generic high-level events and (2) uses a novel concept selection method (i-w2v) based on semantic embeddings. Our experiments on the international TRECVID Multimedia Event Detection benchmark show that a diverse vocabulary including high-level concepts improves performance on the retrieval of high-level events in videos and that our novel method outperforms a knowledge-based concept selection method. © 2017 ACM.",Content-based visual information retrieval; Multimedia event detection; Semantics; Zero shot,Accidents; Benchmarking; Deep neural networks; Image retrieval; Knowledge based systems; Multimedia systems; Semantics; Concept selection; Content-based visual information retrieval; Digital video data; Knowledge based; Multimedia event detections; Retrieval systems; Semantic reasoning; Zero shot; Computer graphics
A distributed streaming framework for connection discovery using shared videos,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029805736&doi=10.1145%2f3120996&partnerID=40&md5=a6e83d8699632701df90b2cf4fa81291,"With the advances in mobile devices and the popularity of social networks, users can share multimedia content anytime, anywhere. One of the most important types of emerging content is video, which is commonly shared on platforms such as Instagram and Facebook. User connections, which indicate whether two users are follower/followee or have the same interests, are essential to improve services and information relevant to users for many social media applications. But they are normally hidden due to users' privacy concerns or are kept confidential by social media sites. Using user-shared content is an alternative way to discover user connections. This article proposes to use user-shared videos for connection discovery with the Bag of Feature Tagging method and proposes a distributed streaming computation framework to facilitate the analytics. Exploiting the uniqueness of shared videos, the proposed framework is divided into Streaming processing and Online and Offline Computation. With experiments using a dataset from Twitter, it has been proved that the proposed method using user-shared videos for connection discovery is feasible. And the proposed computation framework significantly accelerates the analytics, reducing the processing time to only 32% for follower/followee recommendation. It has also been proved that comparable performance can be achieved with only partial data for each video and leads to more efficient computation. © 2017 ACM.",Bag-of-features tagging; Computation framework; Connection discovery; Social networks; Streaming; User shared videos,Acoustic streaming; Network function virtualization; User interfaces; Video signal processing; Bag of features; Connection discovery; Distributed streaming; Efficient computation; Multimedia contents; Off-line computation; Streaming processing; User shared videos; Social networking (online)
Performance analysis of game engines on mobile and fixed devices,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029794497&doi=10.1145%2f3115934&partnerID=40&md5=fb3ded21e71c9a403818ecbf73158338,"Mobile gaming is an emerging concept wherein gamers are using mobile devices, like smartphones and tablets, to play best-seller games. Compared to dedicated gaming boxes or PCs, these devices still fall short of executing newly complex 3D video games with a rich immersion. Three novel solutions, relying on cloud computing infrastructure, namely, computation offloading, cloud gaming, and client-server architecture, will represent the next generation of game engine architecture aiming at improving the gaming experience. The basis of these aforementioned solutions is the distribution of the game code over different devices (including set-top boxes, PCs, and servers). In order to know how the game code should be distributed, advanced knowledge of game engines is required. By consequence, dissecting and analyzing game engine performances will surely help to better understand how to move in these new directions (i.e., distribute game code), which is so far missing in the literature. Aiming at filling this gap, we propose in this article to analyze and evaluate one of the famous engines in the market, that is, ""Unity 3D.""We begin by detailing the architecture and the game logic of game engines. Then, we propose a test-bed to evaluate the CPU and GPU consumption per frame and per module for nine representative games on three platforms, namely, a stand-alone computer, embedded systems, and web players. Based on the obtained results and observations, we build a valued graph of each module, composing the Unity 3D architecture, which reflects the internal flow and CPU consumption. Finally, we made a comparison in terms of CPU consumption between these architectures. © 2017 ACM.",Cloud gaming; Computation offloading; Games; Rendering; Unity 3D,Client server computer systems; Codes (symbols); Computation theory; Computer games; Embedded systems; Flow graphs; Human computer interaction; Set-top boxes; Technology transfer; Client-server architectures; Cloud computing infrastructures; Cloud gamings; Computation offloading; Games; Gaming experiences; Performance analysis; Rendering; Three dimensional computer graphics
Modeling and analysis of power consumption in live video streaming systems,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029804056&doi=10.1145%2f3115505&partnerID=40&md5=2b5551499647757858c2df37d9d9a974,"This article develops an aggregate power consumption model for live video streaming systems, including many-to-many systems. In many-to-one streaming systems, multiple video sources (i.e., cameras and/or sensors) stream videos to a monitoring station. We model the power consumed by the video sources in the capturing, encoding, and transmission phases and then provide an overall model in terms of the main capturing and encoding parameters, including resolution, frame rate, number of reference frames, motion estimation range, and quantization.We also analyze the power consumed by the monitoring station due to receiving, decoding, and upscaling the received video streams. In addition to modeling the power consumption, we model the achieved bitrate of video encoding. We validate the developed models through extensive experiments using two types of systems and different video contents. Furthermore, we analyze many-to-one systems in terms of bitrate, video quality, and the power consumed by the sources, as well as that by the monitoring station, considering the impacts of multiple parameters simultaneously. © 2017 ACM.",Live video streaming; Power consumption modeling; Video bitrate modeling; Video surveillance systems,Electric power utilization; Encoding (symbols); Mobile devices; Motion estimation; Signal encoding; Video streaming; Encoding parameters; Live video streaming; Model and analysis; Monitoring stations; Multiple parameters; Power consumption model; Video bitrate; Video surveillance systems; Security systems
An Efficient Motion Detection and Tracking Scheme for Encrypted Surveillance Videos,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029806864&doi=10.1145%2f3131342&partnerID=40&md5=e6159caf7f7452d77c921a592576c45d,"Performing detection on surveillance videos contributes significantly to the goals of safety and security. However, performing detection on unprotected surveillance video may reveal the privacy of innocent people in the video. Therefore, striking a proper balance between maintaining personal privacy while enhancing the feasibility of detection is an important issue. One promising solution to this problem is to encrypt the surveillance videos and perform detection on the encrypted videos. Most existing encrypted signal processing methods focus on still images or small data volumes; however, because videos are typically much larger, investigating how to process encrypted videos is a significant challenge. In this article, we propose an efficient motion detection and tracking scheme for encrypted H.264/AVC video bitstreams, which does not require the previous decryption on the encrypted video. The main idea is to first estimate motion information from the bitstream structure and codeword length and, then, propose a region update (RU) algorithm to deal with the loss and error drifting of motion caused by the video encryption. The RU algorithm is designed based on the prior knowledge that the object motion in the video is continuous in space and time. Compared to the existing scheme, which is based on video encryption that occurs at the pixel level, the proposed scheme has the advantages of requiring only a small storage of the encrypted video and has a low computational cost for both encryption and detection. Experimental results show that our scheme performs better regarding detection accuracy and execution speed. Moreover, the proposed scheme can work with more than one format-compliant video encryption method, provided that the positions of the macroblocks can be extracted from the encrypted video bitstream. Due to the coupling of video stream encryption and detection algorithms, our scheme can be directly connected to the video stream output (e.g., surveillance cameras) without requiring any camera modifications. © 2017 ACM.",Cloud; Encrypted bitstream; H.264/AVC; Motion detection and tracking; Partial encryption; Surveillance video; Video processing on the encrypted videos,Binary sequences; Cameras; Clouds; Digital storage; Monitoring; Motion analysis; Motion Picture Experts Group standards; Processing; Security systems; Signal processing; Video signal processing; Video streaming; Bit stream; H.264/AVC; Motion detection; Partial encryption; Surveillance video; Video processing; Cryptography
PLACID: A platform for FPGA-based accelerator creation for DCNNs,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029810872&doi=10.1145%2f3131289&partnerID=40&md5=0c63e9dfdfeb1ceb62d25166d3deb619,"Deep Convolutional Neural Networks (DCNNs) exhibit remarkable performance in a number of pattern recognition and classification tasks. Modern DCNNs involve many millions of parameters and billions of operations. Inference using such DCNNs, if implemented as software running on an embedded processor, results in considerable execution time and energy consumption, which is prohibitive in many mobile applications. Field-programmable gate array (FPGA)-based acceleration of DCNN inference is a promising approach to improve both energy consumption and classification throughput. However, the engineering effort required for development and verification of an optimized FPGA-based architecture is significant. In this article, we present PLACID, an automated PLatform for Accelerator CreatIon for DCNNs. PLACID uses an analytical approach to characterization and exploration of the implementation space. PLACID enables generation of an accelerator with the highest throughput for a given DCNN on a specific target FPGA platform. Subsequently, it generates an RTL level architecture in Verilog, which can be passed onto commercial tools for FPGA implementation. PLACID is fully automated, and reduces the accelerator design time from a few months down to a few hours. Experimental results show that architectures synthesized by PLACID yield 2× higher throughput density than the best competing approach. © 2017 ACM.",Accelerator design; Convolutional neural networks; Deep learning; Design automation,Acceleration; Application programs; Automation; Computer aided design; Convolution; Deep learning; Deep neural networks; Energy utilization; Network architecture; Neural networks; Pattern recognition; Throughput; Accelerator design; Convolutional neural network; Design automations; Embedded processors; FPGA implementations; FPGA-based architectures; Mobile applications; Pattern recognition and classification; Field programmable gate arrays (FPGA)
An efficient computation framework for connection discovery using shared images,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030150381&doi=10.1145%2f3115951&partnerID=40&md5=c968535560b5d0fdc10388665184dda7,"With the advent and popularity of the social network, social graphs become essential to improve services and information relevance to users for many social media applications to predict follower/followee relationship, community membership, and so on. However, the social graphs could be hidden by users due to privacy concerns or kept by social media. Recently, connections discovered from user-shared images using machine-generated labels are proved to be more accessible alternatives to social graphs. But real-time discovery is difficult due to high complexity, and many applications are not possible. This article proposes an efficient computation framework for connection discovery using user-shared images, which is suitable for any image processing and computer vision techniques for connection discovery on the fly. The framework includes the architecture of online computation to facilitate real-time processing, offline computation for a complete processing, and online/offline communication. The proposed framework is implemented to demonstrate its effectiveness by speeding up connection discovery through user-shared images. By studying 300K+ user-shared images from two popular social networks, it is proven that the proposed computation framework reduces 90% of runtime with a comparable accurate with existing frameworks. © 2017 ACM.",Bag-of-features tagging; Computation framework; Connection discovery; Social networks; User-shared images,Computational efficiency; Image processing; Network function virtualization; Bag of features; Connection discovery; Efficient computation; Image processing and computer vision; Information relevances; Off-line computation; Online computations; User-shared images; Social networking (online)
Interactive film recombination,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028574224&doi=10.1145%2f3103241&partnerID=40&md5=5a3d7cb8c955609e59145371b5699025,"In this article,we discuss an innovative media entertainment application called Interactive Movietelling. As an offspring of Interactive Storytelling applied to movies, we propose to integrate narrative generation through artificial intelligence (AI) planning with video processing and modeling to construct filmic variants starting from the baseline content. The integration is possible thanks to content description using semantic attributes pertaining to intermediate-level concepts shared between video processing and planning levels. The output is a recombination of segments taken from the input movie performed so as to convey an alternative plot. User tests on the prototype proved how promising Interactive Movietelling might be, even if it was designed at a proof of concept level. Possible improvements that are suggested here lead to many challenging research issues. © 2017 ACM.",Interactive Storytelling; Logical story unit; Markov chains; Narrative modeling; Semantic description,Markov processes; Semantics; Content description; Entertainment application; Interactive storytelling; Intermediate level; Logical story units; Semantic attribute; Semantic descriptions; Video processing; Video signal processing
Securing speech noise reduction in outsourced environment,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028577261&doi=10.1145%2f3105970&partnerID=40&md5=9185aca16c7abcbd7f0bb0387f68c9ab,"Cloud data centers (CDCs) are becoming a cost-effective method for processing and storage of multimedia data including images, video, and audio. Since CDCs are physically located in different jurisdictions, and are managed by external parties, data security is a growing concern. Data encryption at CDCs is commonly practiced to improve data security. However, to process the data at CDCs, data must often be decrypted,which raises issues in security. Thus, there is a growing demand for data processing techniques in encrypted domain in such an outsourced environment. In this article, we analyze encrypted domain speech content processing techniques for noise reduction. Noise contaminates speech during transmission or during the acquisition process by recording. As a result, the quality of the speech content is degraded. We apply Shamir's secret sharing as the cryptosystem to encrypt speech data before uploading it to a CDC. We then propose finite impulse response digital filters to reduce white and wind noise in the speech in the encrypted domain. We prove that our proposed schemes meet the security requirements of efficiency, accuracy, and checkability for both semi-honest and malicious adversarial models. Experimental results show that our proposed filtering techniques for speech noise reduction in the encrypted domain produce similar results when compared to plaintext domain processing. © 2017 ACM.",Encrypted domain; Homomorphic encryption; Secret sharing; Secure noise reduction,Cost effectiveness; Cryptography; Data handling; Data reduction; Digital filters; Digital storage; Impulse response; Noise abatement; Speech; Speech communication; Video signal processing; Cost-effective methods; Data processing techniques; Encrypted domain; Filtering technique; Finite impulse response digital filters; Ho-momorphic encryptions; Secret sharing; Security requirements; Speech transmission
Complexity correlation-based CTU-level rate control with direction selection for HEVC,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028565930&doi=10.1145%2f3107616&partnerID=40&md5=29b8a314133d6bfdf199994c7ce0a811,"Rate control is a crucial consideration in high-efficiency video coding (HEVC). The estimation of model parameters is very important for coding tree unit (CTU)-level rate control, as it will significantly affect bit allocation and thus coding performance. However, the model parameters in the CTU-level rate control sometimes fails because of inadequate consideration of the correlation between model parameters and complexity characteristic. In this study, we establish a novel complexity correlation-based CTU-level rate control for HEVC. First, we formulate the model parameter estimation scheme as a multivariable estimation problem; second, based on the complexity correlation of the neighbouring CTU, an optimal direction is selected in five directions for reference CTU set selection during model parameter estimation to further improve the prediction accuracy of the complexity of the current CTU. Third, to improve their precision, the relationship between the model parameters and the complexity of the reference CTU set in the optimal direction is established by using least square method (LS), and the model parameters are solved via the estimated complexity of the current CTU. Experimental results show that the proposed algorithm can significantly improve the accuracy of the CTU-level rate control and thus the coding performance; the proposed scheme consistently outperforms HM 16.0 and other state-of-the-art algorithms in a variety of testing configurations. More specifically, up to 8.4% and on average 6.4% BD-Rate reduction is achieved compared to HM 16.0 and up to 4.7% and an average of 3.4% BD-Rate reduction is achieved compared to other algorithms, with only a slight complexity overhead. 2017 Copyright is held by the owner/author(s).",Complexity correlation; CTU level; HEVC; Model parameter; Optimal direction; Reference CTU,Codes (symbols); Image coding; Least squares approximations; CTU level; HEVC; Model parameters; Optimal direction; Reference CTU; Parameter estimation
Multimodal retrieval with diversification and relevance feedback for tourist attraction images,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028551120&doi=10.1145%2f3103613&partnerID=40&md5=b24775fc389e7f3ebec9850d5d9cd470,"In this article, we present a novel framework that can produce a visual description of a tourist attraction by choosing the most diverse pictures from community-contributed datasets, which describe different details of the queried location. The main strength of the proposed approach is its flexibility that permits us to filter out non-relevant images and to obtain a reliable set of diverse and relevant images by first clustering similar images according to their textual descriptions and their visual content and then extracting images from different clusters according to a measure of the user's credibility. Clustering is based on a two-step process, where textual descriptions are used first and the clusters are then refined according to the visual features. The degree of diversification can be further increased by exploiting users' judgments on the results produced by the proposed algorithm through a novel approach, where users not only provide a relevance feedback but also a diversity feedback. Experimental results performed on the MediaEval 2015 ""Retrieving Diverse Social Images"" dataset show that the proposed framework can achieve very good performance both in the case of automatic retrieval of diverse images and in the case of the exploitation of the users' feedback. The effectiveness of the proposed approach has been also confirmed by a small case study involving a number of real users. © 2017 ACM.",Diversification; Tourist attraction images retrieval,Computer networks; Automatic retrieval; Diversification; Relevance feedback; Textual description; Tourist attractions; Two-step process; Visual content; Visual feature; Hardware
"O-mopsi: Mobile orienteering game for sightseeing, exercising, and education",2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028545928&doi=10.1145%2f3115935&partnerID=40&md5=be1854e7d8138fcb598046d3f66da8e5,"Location-based games have been around already since 2000 but only recently when PokemonGo came to markets it became clear that they can reach wide popularity. In this article, we perform a literature-based analytical study of what kind of issues location-based game design faces, and how they can be solved. We study how to use and verify the location, the role of the games as exergames, use in education, and study technical and safety issues. As a case study, we present O-Mopsi game that combines physical activity with problem solving. It includes three challenges: (1) navigating to the next target, (2) deciding the order of targets, (3) physical movement. All of them are unavoidable and relevant. For guiding the players, we use three types of multimedia: images (targets and maps), sound (user guidance), and GPS (for positioning).We discuss motivational aspects, analysis of the playing, and content creation. The quality of experiences is reported based on playing in SciFest Science festivals during 2011-2016. © 2017 ACM.",Competition; GPS; Location; Orienteering; Target finding,Competition; Location; Problem solving; Analytical studies; Content creation; Location based games; Orienteering; Physical activity; Physical movements; Quality of experience (QoE); Target finding; Global positioning system
Mobile multi-food recognition using deep learning,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028536006&doi=10.1145%2f3063592&partnerID=40&md5=02d7b1f69bc080ee45802ea3594171b8,"In this article, we propose a mobile food recognition system that uses the picture of the food, taken by the user's mobile device, to recognize multiple food items in the same meal, such as steak and potatoes on the same plate, to estimate the calorie and nutrition of themeal. To speed up and make the processmore accurate, the user is asked to quickly identify the general area of the food by drawing a bounding circle on the food picture by touching the screen. The system then uses image processing and computational intelligence for food item recognition. The advantage of recognizing items, instead of the whole meal, is that the system can be trained with only single item food images. At the training stage, we first use region proposal algorithms to generate candidate regions and extract the convolutional neural network (CNN) features of all regions. Second, we perform region mining to select positive regions for each food category using maximum cover by our proposed submodular optimization method. At the testing stage, we first generate a set of candidate regions. For each region, a classification score is computed based on its extracted CNN features and predicted food names of the selected regions. Since fast response is one of the important parameters for the user who wants to eat the meal, certain heavy computational parts of the application are offloaded to the cloud. Hence, the processes of food recognition and calorie estimation are performed in cloud server. Our experiments, conducted with the FooDD dataset, show an average recall rate of 90.98%, precision rate of 93.05%, and accuracy of 94.11% compared to 50.8% to 88% accuracy of other existing food recognition systems. © 2017 ACM.",Cloud computing; Deep learning; Mobile food recognition,Cloud computing; Deep learning; Mobile devices; Neural networks; Touch screens; Calorie estimations; Convolutional neural network; Item recognition; Mobile food recognition; Positive region; Proposal algorithm; Recognition systems; Submodular optimizations; Image processing
When Smart Devices Interact With Pervasive Screens: A Survey,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028537631&doi=10.1145%2f3115933&partnerID=40&md5=f8843ea1cd09bd3ea78d731614c894aa,"The meeting of pervasive screens and smart devices has witnessed the birth of screen-smart device interaction (SSI), a key enabler to many novel interactive use cases. Most current surveys focus on direct human-screen interaction, and to the best of our knowledge, none have studied state-of-the-art SSI. This survey identifies three core elements of SSI and delivers a timely discussion on SSI oriented around the screen, the smart device, and the interaction modality. Two evaluation metrics (i.e., interaction latency and accuracy) have been adopted and refined to match the evaluation criterion of SSI. The bottlenecks that hinder the further advancement of the current SSI in connection with this metrics are studied. Last, future research challenges and opportunities are highlighted in the hope of inspiring continuous research efforts to realize the next generation of SSI. © 2017 ACM.",Interactive technology; Pervasive screen; Smart device,Computer networks; Hardware; Evaluation criteria; Evaluation metrics; Interactive technology; Research challenges; Research efforts; Screen interaction; Smart devices; State of the art; Surveys
A Tucker deep computation model for mobile multimedia feature learning,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028540529&doi=10.1145%2f3063593&partnerID=40&md5=9b8be6d1d8ec91a138bfa0a1f7c8d7d2,"Recently, the deep computation model, as a tensor deep learning model, has achieved super performance for multimedia feature learning. However, the conventional deep computation model involves a large number of parameters. Typically, training a deep computation model with millions of parameters needs highperformance servers with large-scale memory and powerful computing units, limiting the growth of the model size for multimedia feature learning on common devices such as portable CPUs and conventional desktops. To tackle this problem, this article proposes a Tucker deep computation model by using the Tucker decomposition to compress the weight tensors in the full-connected layers for multimedia feature learning. Furthermore, a learning algorithm based on the back-propagation strategy is devised to train the parameters of the Tucker deep computation model. Finally, the performance of the Tucker deep computation model is evaluated by comparing with the conventional deep computation model on two representative multimedia datasets, that is, CUAVE and SNAE2, in terms of accuracy drop, parameter reduction, and speedup in the experiments. Results imply that the Tucker deep computation model can achieve a large-parameter reduction and speedup with a small accuracy drop for multimedia feature learning. © 2017 ACM.",Back-propagation; Deep computation; Deep learning; Mobile multimedia; Tucker decomposition,Backpropagation; Backpropagation algorithms; Computation theory; Deep learning; Drops; Learning algorithms; Program processors; Tensors; Computation model; Computing units; Highperformance; Learning models; Mobile multimedia; Multimedia features; Parameter reduction; Tucker decompositions; Parameter estimation
Caching online video: Analysis and proposed algorithm,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028550779&doi=10.1145%2f3106157&partnerID=40&md5=dc4f2adb6264903f4e26f9edccee6434,"Online video presents new challenges to traditional caching with over a thousand-fold increase in number of assets, rapidly changing popularity of assets and much higher throughput requirements. We propose a newhierarchical filtering algorithm for caching online video-HiFi. Our algorithm is designed to optimize hit rate, replacement rate and cache throughput. It has an associated implementation complexity comparable to that of LRU. Our results show that, under typical operator conditions, HiFi can increase edge cache byte hit rate by 5%-24% over an LRU policy, but more importantly can increase the RAM or memory byte hit rate by 80% to 200% and reduce the replacement rate by more than 100 times! These two factors combined can dramatically increase throughput for most caches. If SSDs are used for storage, the much lower replacement rate may also allow substitution of lower-cost MLC-based SSDs instead of SLC-based SSDs. We extend previous multi-tier analytical models for LRU caches to caches with filtering. We analytically show how HiFi can approach the performance of an optimal caching policy and how to tune HiFi to reach as close to optimal performance as the traffic conditions allow. We develop a realistic simulation environment for online video using statistics from operator traces. We show that HiFi performs within a few percentage points from the optimal solution which was simulated by Belady's MIN algorithm under typical operator conditions. © 2017 ACM.",GDSF; Hierarchical cache; Hit-rate; LFU; LRU; Online video,Random access storage; Filtering algorithm; GDSF; Hierarchical caches; Hit rate; Implementation complexity; Online video; Optimal performance; Realistic simulation; Throughput
Mixtape: Using real-time user feedback to navigate large media collections,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027100981&doi=10.1145%2f3105969&partnerID=40&md5=c1be56d33673f503fd48b699b0710ffb,"In this work, we explore the increasing demand for novel user interfaces to navigate large media collections. We implement a geometric data structure to store and retrieve item-to-item similarity information and propose a novel navigation framework that uses vector operations and real-time user feedback to direct the outcome. The framework is scalable to large media collections and is suitable for computationally constrained devices. In particular, we implement this framework in the domain of music. To evaluate the effectiveness of the navigation process, we propose an automatic evaluation framework, based on synthetic user profiles, which allows us to quickly simulate and compare navigation paths using different algorithms and datasets. Moreover, we perform a real user study. To do that, we developed and launched Mixtape, a simple web application that allows users to create playlists by providing real-time feedback through liking and skipping patterns. © 2017 ACM.",collaborative filtering; content similarity; graph embedding; Media collection; navigation,Collaborative filtering; Network function virtualization; User interfaces; Automatic evaluation; Constrained devices; Content similarity; Geometric data structures; Graph embeddings; Navigation paths; Real-time feedback; Vector operations; Navigation
Saliency detection on light field: A multi-cue approach,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027408673&doi=10.1145%2f3107956&partnerID=40&md5=2c4805645c37d35d6cc9efed642620bf,"Saliency detection has recently received increasing research interest on using high-dimensional datasets beyond two-dimensional images. Despite the many available capturing devices and algorithms, there still exists a wide spectrum of challenges that need to be addressed to achieve accurate saliency detection. Inspired by the success of the light-field technique, in this article, we propose a new computational scheme to detect salient regions by integrating multiple visual cues from light-field images. First, saliency prior maps are generated from several light-field features based on superpixel-level intra-cue distinctiveness, such as color, depth, and flow inherited from different focal planes and multiple viewpoints. Then, we introduce the location prior to enhance the saliency maps. These maps will finally be merged into a single map using a random-search-based weighting strategy. Besides, we refine the object details by employing a two-stage saliency refinement to obtain the final saliency map. In addition, we present a more challenging benchmark dataset for light-field saliency analysis, named HFUT-Lytro, which consists of 255 light fields with a range from 53 to 64 images generated from each light-field image, therein spanning multiple occurrences of saliency detection challenges such as occlusions, cluttered background, and appearance changes. Experimental results show that our approach can achieve 0.66.7% relative improvements over state-of-The-Art methods in terms of the F-measure and Precision metrics, which demonstrates the effectiveness of the proposed approach. © 2017 ACM.",Light field; Multi-cue; Saliency detection,Computer networks; Hardware; Cluttered backgrounds; Computational schemes; High dimensional datasets; Light fields; Multi cues; Saliency detection; State-of-the-art methods; Two dimensional images; Image segmentation
Sparse representation-based semi-supervised regression for people counting,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027171009&doi=10.1145%2f3106156&partnerID=40&md5=da039c2d20a4fe0100e3ea1b3a16fba5,"Label imbalance and the insufficiency of labeled training samples are major obstacles in most methods for counting people in images or videos. In this work, a sparse representation-based semi-supervised regression method is proposed to count people in images with limited data. The basic idea is to predict the unlabeled training data, select reliable samples to expand the labeled training set, and retrain the regression model. In the algorithm, the initial regression model, which is learned from the labeled training data, is used to predict the number of people in the unlabeled training dataset. Then, the unlabeled training samples are regarded as an over-complete dictionary. Each feature of the labeled training data can be expressed as a sparse linear approximation of the unlabeled data. In turn, the labels of the labeled training data can be estimated based on a sparse reconstruction in feature space. The label confidence in labeling an unlabeled sample is estimated by calculating the reconstruction error. The training set is updated by selecting unlabeled samples with minimal reconstruction errors, and the regression model is retrained on the new training set. A co-training style method is applied during the training process. The experimental results demonstrate that the proposed method has a low mean square error and mean absolute error compared with those of state-ofthe- art people-counting benchmarks. © 2017 ACM.",Counting people; reconstruction error; semi-supervised regression; sparse reconstruction; sparse representation,Errors; Mean square error; Sampling; Counting people; Reconstruction error; Semi-supervised; Sparse reconstruction; Sparse representation; Regression analysis
Cloud-assisted crowdsourced livecast,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026482916&doi=10.1145%2f3095755&partnerID=40&md5=407ac501503a837459fb2b4e40f01fe3,"The past two years have witnessed an explosion of a new generation of livecast services, represented by Twitch.tv, GamingLive, and Dailymotion, to name but a few. With such a livecast service, geo-distributed Internet users can broadcast any event in real-time, for example, game, cooking, drawing, and so on, to viewers of interest. Its crowdsourced nature enables rich interactions among broadcasters and viewers but also introduces great challenges to accommodate their great scales and dynamics. To fulfill the demands from a large number of heterogeneous broadcasters and geo-distributed viewers, expensive server clusters have been deployed to ingest and transcode live streams. Yet our Twitch-based measurement shows that a significant portion of the unpopular and dynamic broadcasters are consuming considerable system resources; in particular, 25% of bandwidth resources and 30% of computational capacity are used by the broadcasters who do not have any viewers at all. In this article, through the real-world measurement and data analysis, we show that the public cloud has great potentials to address these scalability challenges. We accordingly present the design of Cloud-assisted Crowdsourced Livecast (CACL) and propose a comprehensive set of solutions for broadcaster partitioning. Our trace-driven evaluations show that our CACL design can smartly assign ingesting and transcoding tasks to the elastic cloud virtual machines, providing flexible and costeffective system deployment. ©2017 ACM.",Crowdsourced livecast; Public clouds; Resource allocation; Workload migration,Computer networks; Hardware; Resource allocation; Bandwidth resource; Computational capacity; Crowdsourced livecast; Public clouds; Server cluster; System deployment; System resources; Workload migration; Virtual machine
Enhancing transmission collision detection for distributed TDMA in vehicular networks,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026470507&doi=10.1145%2f3092833&partnerID=40&md5=960cdf2ac4f734ff86c35ee23f33213c,"The increasing number of road accidents has led to the evolution of vehicular ad hoc networks (VANETs), which allow vehicles and roadside infrastructure to continuously broadcast safety messages, including necessary information to avoid undesired events on the road. To support reliable broadcast of safety messages, distributed time division multiple access (D-TDMA) protocols are proposed for medium access control in VANETs. Existing D-TDMA protocols react to a transmission failure without distinguishing whether the failure comes from a transmission collision or from a poor radio channel condition, resulting in degraded performance. In this article, we present the importance of transmission failure differentiation due to a poor channel or due to a transmission collision for D-TDMA protocols in vehicular networks. We study the effects of such a transmission failure differentiation on the performance of a node when reserving a time slot to access the transmission channel. Furthermore, we propose a method for transmission failure differentiation, employing the concept of deep-learning techniques, for a node to decide whether to release or continue using its acquired time slot. The proposed method is based on the application of a Markov chain model to estimate the channel state when a transmission failure occurs. The Markov model parameters are dynamically updated by each node (i.e., vehicle or roadside unit) based on information included in the safety messages that are periodically received from neighboring nodes. In addition, from the D-TDMA protocol headers of received messages, a node approximately determines the error in estimating the channel state based on the proposed Markov model and then uses this channel estimation error to further improve subsequent channel state estimations. Through mathematical analysis, we show that transmission failure differentiation, or transmission collision detection, helps a node to efficiently reserve a time slot even with a large number of nodes contending for time slots. Furthermore, through extensive simulations in a highway scenario, we demonstrate that the proposed solution significantly improves the performance of D-TDMA protocols by reducing unnecessary contention on the available time slots, thus increasing the number of nodes having unique time slots for successful broadcast of safety messages. © 2017 ACM.",Channel estimation; Collision detection; Distributed time division multiple access (D-TDMA); Medium access control (MAC); Transmission failure differentiation; VANETs,Access control; Accidents; Ad hoc networks; Channel estimation; Deep learning; Estimation; Highway accidents; Markov processes; Medium access control; Radio broadcasting; Radio transmission; Roads and streets; Roadsides; Transportation; Vehicular ad hoc networks; Collision detection; Distributed time; Medium access control(MAC); Transmission failures; VANETs; Time division multiple access
Pithos: Distributed storage for massive multi-user virtual environments,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026434770&doi=10.1145%2f3105577&partnerID=40&md5=7e07ddf16bfb88a55fb41a1ad269299b,"There has been significant research effort into peer-to-peer (P2P) massively multi-user virtual environments (MMVEs). A number of architectures have been proposed to implement the P2P approach; however, the development of fully distributed MMVEs has met with a number of challenges. In this work, we address one of the key remaining challenges of state consistency and persistency in P2P MMVEs. Having reviewed state management and persistency architectures currently receiving research attention, we have identified deficiencies such as lack of load balancing, responsiveness, and scalability. To address these deficiencies, we present Pithos-a reliable, responsive, secure, load-balanced, and scalable distributed storage system, suited to P2P MMVEs. Pithos is designed specifically for P2P MMVEs, and we show that it improves the reliability and responsiveness of storage architectures as compared to existing P2P state persistency architectures. Pithos is implemented as an OverSim simulation running on the OMNeT++ network simulation framework. It is evaluated using up to 10,400 peers, with realistic latency profiles, with up to 15.8 million storage and retrieval requests that are generated to store a total of 2.4 million objects. Each peer in Pithos uses a maximum of 1,950Bps bandwidth to achieve 99.98% storage reliability, while the most reliable overlay storage configuration tested only achieved 93.65% reliability, using 2,182Bps bandwidth. Pithos is also more responsive than overlay storage, with an average responsiveness of 0.192s, compared with the average overlay responsiveness of 1.4s when retrieving objects from storage. © 2017 ACM.",Distributed storage; Massive multiplayer virtual environments; State consistency; State management; State persistency,Bandwidth; Distributed computer systems; Multiprocessing systems; Network architecture; Reliability; Virtual reality; Distributed storage; Multiplayers; State consistency; State management; State persistency; Peer to peer networks
Design & analysis of QoE-aware quality adaptation for DASH: A spectrum-based approach,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026452193&doi=10.1145%2f3092839&partnerID=40&md5=ea95ecdf00955726c83061f1909b1f3a,"The dynamics of the application-layer-based control loop of dynamic adaptive streaming over HTTP (DASH) make video bitrate selection for DASH a difficult problem. In this work, we provide a DASH quality adaptation algorithm, named SQUAD, that is specifically tailored to provide a high quality of experience (QoE). We review and provide new insights into the challenges for DASH rate estimation. We found that in addition to the ON-OFF behavior of DASH clients, there exists a discrepancy in the timescales that form the basis of the rate estimates across (i) different video segments and (ii) the rate control loops of DASH and Transmission Control Protocol (TCP). With these observations in mind, we design SQUAD aiming to maximize the average quality bitrate while minimizing the quality variations. We test our implementation of SQUAD together with a number of different quality adaptation algorithms under various conditions in the Global Environment for Networking Innovation testbed, as well as, in a series of measurements over the public Internet. Through a measurement study, we show that by sacrificing little to nothing in average quality bitrate, SQUAD can provide significantly better QoE in terms of quality switching and magnitude. In addition, we show that retransmission of higher-quality segments that were originally received in low-quality is feasible and improves the QoE. ©2017 ACM.",Adaptive bitrate streaming; DASH; Quality of experience; TCP,HTTP; Quality of service; Transmission control protocol; Video streaming; Application layers; Bit rates; DASH; Dynamic Adaptive Streaming over HTTP; Global environment; Measurement study; Quality adaptation; Quality of experience (QoE); Quality control
Design and performance evaluation of network-assisted control strategies for HTTP adaptive streaming,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022320104&doi=10.1145%2f3092836&partnerID=40&md5=ad38260b668c7818636fc234ec7ea10e,"This article investigates several network-assisted streaming approaches that rely on active cooperation between video streaming applications and the network. We build a Video Control Plane that enforces Video Quality Fairness among concurrent video flows generated by heterogeneous client devices. For this purpose, a max-min fairness optimization problem is solved at runtime. We compare two approaches to actuate the optimal solution in an Software Defined Networking network: The first one allocates network bandwidth slices to video flows, and the second one guides video players in the video bitrate selection. We assess performance through several QoE-related metrics, such as Video Quality Fairness, video quality, and switching frequency. The impact of client-side adaptation algorithms is also investigated. © 2017 ACM.",Adaptive video streaming; Control plane; DASH; Fairness; Network-assistance; Quality of experience,Adaptive control systems; Concurrency control; Quality of service; Video streaming; Adaptive video streaming; Control planes; DASH; Fairness; Network assistance; Quality of experience (QoE); Quality control
On optimizing adaptive algorithms based on rebuffering probability,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022325003&doi=10.1145%2f3092837&partnerID=40&md5=bf82150016af682318e27b7e45ec07e3,"Traditionally, video adaptive algorithms aim to select the representation that better fits to the current download rate. In recent years, a number of new approaches appeared that take into account the buffer occupancy and the probability of video rebuffering as important indicators of the representation to be selected. We propose an optimization of the existing algorithm based on rebuffering probability and argue that the algorithm should avoid the situations when the client buffer is full and the download is stopped, since these situations decrease the efficiency of the algorithm. Reducing full buffer states does not increase the rebuffering probability thanks to a clever management of the client buffer, which analyses the buffer occupancy and downloads higher bitrate representations only in the case of high buffer occupancy. © 2017 ACM.",Adaptation algorithms; Adaptive streaming; ERA,Adaptive algorithms; Probability; Adaptation algorithms; Adaptive streaming; Bit rates; Buffer occupancy; Buffer state; Client buffer; New approaches; Optimization
An SDN architecture for privacy-friendly network-assisted DASH,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022207355&doi=10.1145%2f3092838&partnerID=40&md5=b9936ff4ac3c17a7db5e9466a408d2bb,"Dynamic Adaptive Streaming over HTTP (DASH) is the premier technology for Internet video streaming. DASH efficiently uses existing HTTP-based delivery infrastructures implementing adaptive streaming. However, DASH traffic is bursty in nature. This causes performance problems when DASH players share a network connection or in networks with heavy background traffic. The result is unstable and lower quality video. In this article, we present the design and implementation of a so-called DASH Assisting Network Element (DANE). Our system provides target bitrate signaling and dynamic traffic control. These two mechanisms realize proper bandwidth sharing among clients. Our system is privacy friendly and fully supports encrypted video streams. Trying to improve the streaming experience for users who share a network connection, our system increases the video bitrate and reduces the number of quality switches. We show this through evaluations in our Wi-Fi testbed. © 2017 ACM.",DASH; HTTP adaptive streaming; Network assistance; Performance; Video streaming; Wi-Fi,Bandwidth; HTTP; Video streaming; Adaptive streaming; Background traffic; DASH; Design and implementations; Dynamic Adaptive Streaming over HTTP; Network assistance; Performance; Performance problems; Wi-Fi
Deep artwork detection and retrieval for automatic context-aware audio guides,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022334753&doi=10.1145%2f3092832&partnerID=40&md5=f70d83cb7dff50df87725041965b70e6,"In this article, we address the problem of creating a smart audio guide that adapts to the actions and interests of museum visitors. As an autonomous agent, our guide perceives the context and is able to interact with users in an appropriate fashion. To do so, it understands what the visitor is looking at, if the visitor is moving inside the museum hall, or if he or she is talking with a friend. The guide performs automatic recognition of artworks, and it provides configurable interface features to improve the user experience and the fruition of multimedia materials through semi-automatic interaction. Our smart audio guide is backed by a computer vision system capable of working in real time on a mobile device, coupled with audio and motion sensors. We propose the use of a compact Convolutional Neural Network (CNN) that performs object classification and localization. Using the same CNN features computed for these tasks, we perform also robust artwork recognition. To improve the recognition accuracy, we perform additional video processing using shape-based filtering, artwork tracking, and temporal filtering. The system has been deployed on an NVIDIA Jetson TK1 and a NVIDIA Shield Tablet K1 and tested in a real-world environment (Bargello Museum of Florence). © 2017 ACM.",Audio guide; Computer vision; Cultural heritage; Deep learning; Image retrieval; Mobile computing; Object detection,Autonomous agents; Computer vision; Image retrieval; Interfaces (materials); Mobile computing; Mobile devices; Museums; Neural networks; Object detection; Video signal processing; Audio guide; Automatic recognition; Computer vision system; Convolutional neural network; Cultural heritages; Multimedia materials; Object classification; Real world environments; Deep learning
Intensifying emotional reactions via tactile gestures in immersive films,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022320965&doi=10.1145%2f3092840&partnerID=40&md5=caf271b7581b811841541fc63f7dcc19,"The film industry continuously strives to make visitors' movie experience more immersive and thus, more captivating. This is realized through larger screens, sophisticated speaker systems, and high quality 2D and 3D content. Moreover, a recent trend in the film industry is to incorporate multiple interaction modalities, such as 4D film, to simulate rain, wind, vibration, and heat, in order to intensify viewers' emotional reactions. In this context, humans' sense of touch possesses significant potential for intensifying emotional reactions for the film experience beyond audio-visual sensory modalities. This article presents a framework for authoring tactile cues (tactile gestures as used in this article) and enabling automatic rendering of said gestures to intensify emotional reactions in an immersive film experience. To validate the proposed framework, we conducted an experimental study where tactile gestures are designed and evaluated for the ability to intensify four emotional reactions: high valence-high arousal, high valence-low arousal, low valence-high arousal, and low valence-low arousal. Using a haptic jacket, participants felt tactile gestures that are synchronized with the audio-visual contents of a film. Results demonstrated that (1) any tactile feedback generated a positive user experience; (2) the tactile feedback intensifies emotional reactions when the audio-visual stimuli elicit clear emotional responses, except for low arousal emotional response since tactile gestures seem to always generate excitement; (3) purposed tactile gestures do not seem to significantly outperform randomized tactile gesture for intensifying specific emotional reactions; and (4) using a haptic jacket is not distracting for the users. © 2017 ACM.",Affective haptics; Immersive virtual reality; Multimodal interaction; Tactile gestures,Virtual reality; Audio-visual content; Audio-visual stimulus; Emotional reactions; Haptics; Immersive virtual reality; Multi-Modal Interactions; Multiple interactions; Tactile gestures; User interfaces
Creating segments and effects on comics by clustering gaze data,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022343718&doi=10.1145%2f3078836&partnerID=40&md5=92b69c84422b8806275be9ed72066a98,"Traditional comics are increasingly being augmented with digital effects, such as recoloring, stereoscopy, and animation. An open question in this endeavor is identifying where in a comic panel the effects should be placed. We propose a fast, semi-automatic technique to identify effects-worthy segments in a comic panel by utilizing gaze locations as a proxy for the importance of a region. We take advantage of the fact that comic artists influence viewer gaze towards narrative important regions. By capturing gaze locations from multiple viewers, we can identify important regions and direct a computer vision segmentation algorithm to extract these segments. The challenge is that these gaze data are noisy and difficult to process. Our key contribution is to leverage a theoretical breakthrough in the computer networks community towards robust and meaningful clustering of gaze locations into semantic regions, without needing the user to specify the number of clusters. We present a method based on the concept of relative eigen quality that takes a scanned comic image and a set of gaze points and produces an image segmentation. We demonstrate a variety of effects such as defocus, recoloring, stereoscopy, and animations. We also investigate the use of artificially generated gaze locations from saliency models in place of actual gaze locations. © 2017 ACM.",Comics; Effects,Location; Semantics; Stereo image processing; Comic panels; Comics; Digital effects; Effects; Gaze point; Number of clusters; Segmentation algorithms; Semi-automatics; Image segmentation
Secure cloud-based image tampering detection and localization using POB number system,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022340688&doi=10.1145%2f3077140&partnerID=40&md5=9e74f4aafc99ba84492066b097e95be5,"The benefits of high-end computation infrastructure facilities provided by cloud-based multimedia systems are attracting people all around the globe. However, such cloud-based systems possess security issues as third party servers become involved in them. Rendering data in an unreadable form so that no information is revealed to the cloud data centers will serve as the best solution to these security issues. One such image encryption scheme based on a Permutation Ordered Binary Number System has been proposed in this work. It distributes the image information in totally random shares, which can be stored at the cloud data centers. Further, the proposed scheme authenticates the shares at the pixel level. If any tampering is done at the cloud servers, the scheme can accurately identify the altered pixels via authentication bits and localizes the tampered area. The tampered portion is also reflected back in the reconstructed image that is obtained at the authentic user end. The experimental results validate the efficacy of the proposed scheme against various kinds of possible attacks, tested with a variety of images. The tamper detection accuracy has been computed on a pixel basis and found to be satisfactorily high for most of the tampering scenarios. © 2017 ACM.",Encrypted domain; Permutation ordered binary (POB) number system; Secret sharing,Bins; Cryptography; Image processing; Multimedia systems; Numbering systems; Pixels; Authentication bits; Binary number systems; Cloud data centers; Encrypted domain; Image encryption scheme; Number system; Reconstructed image; Secret sharing; Distributed computer systems
Introduction to special issue on deep learning for mobile multimedia,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022328612&doi=10.1145%2f3088340&partnerID=40&md5=0dd0742bc115ed62c483dfc2f82fbd11,[No abstract available],,
An analytic system for user gender identification through user shared images,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022336486&doi=10.1145%2f3095077&partnerID=40&md5=2015a7aca3ab3900fc388d0e48356275,"Many social media applications, such as recommendation, virality prediction, and marketing, make use of user gender, which may not be explicitly specified or kept privately. Meanwhile, advancedmobile devices have become part of our lives and a huge amount of content is being generated by users every day, especially user shared images shared by individuals in social networks. This particular form of user generated content is widely accessible to others due to the sharing nature.When user gender is only accessible to exclusive parties, these user shared images are proved to be an easier way to identify user gender. This work investigated 3,152,344 images by 7,450 users from Fotolog and Flickr, two image-oriented social networks. It is observed that users who share visually similar images are more likely to have the same gender. A multimedia big data system that utilizes this phenomenon is proposed for user gender identification with 79% accuracy. These findings are useful for information or services in any social network with intensive image sharing. © 2017 ACM.",Big data; Gender; Mobile; Recommendation; Social network analysis; User shared images,Big data; Social sciences; Gender; Gender identification; Image sharing; Mobile; Recommendation; Similar image; User shared images; User-generated content; Social networking (online)
Distributed rate allocation in switch-based multiparty videoconferencing system,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022338663&doi=10.1145%2f3092835&partnerID=40&md5=31b0821f8ac822dc12aa30570928ab89,"Multiparty videoconferences, or more generally multiparty video calls, are gaining a lot of popularity as they offer a rich communication experience. These applications have, however, large requirements in terms of both network and computational resources and have to deal with sets of heterogenous clients. The multiparty videoconferencing systems are usually either based on expensive central nodes, called Multipoint Control Units (MCU), with transcoding capabilities, or on a peer-to-peer architecture where users cooperate to distribute more efficiently the different video streams. Whereas the first class of systems requires an expensive central hardware, the second one depends completely on the redistribution capacity of the users, which sometimes might neither provide sufficient bandwidth nor be reliable enough. In this work, we propose an alternative solution where we use a central node to distribute the video streams, but at the same time we maintain the hardware complexity and the computational requirements of this node as low as possible, for example, it has no video decoding capabilities. We formulate the rate allocation problem as an optimization problem that aims at maximizing the Quality of Service (QoS) of the videoconference. We propose two different distributed algorithms for solving the optimization problem: the first algorithm is able to find an approximate solution of the problem in a one-shot execution, whereas the second algorithm, based on Lagrangian relaxation, performs iterative updates of the optimization variables in order to gradually increase the value of the objective function. The two algorithms, though being disjointed, nicely complement each other. If executed in sequence, they allow us to achieve both a quick approximate rate reallocation, in case of a sudden change of the system conditions, and a precise refinement of the variables, which avoids problems caused by possible faulty approximate solutions. We have further implemented our solution in a network simulator where we show that our rate allocation algorithm is able to properly optimize users' QoS. We also illustrate the benefits of our solution in terms of network usage and overall utility when compared to a baseline heuristic method operating on the same system architecture. © 2017 ACM.",QoS optimization; Rate allocation; Videoconference,Complex networks; Computer hardware description languages; Distributed computer systems; Hardware; Heuristic methods; Iterative methods; Multimedia services; Network architecture; Peer to peer networks; Problem solving; Quality of service; Video conferencing; Video streaming; Computational requirements; Distributed rate allocation; Peer-to-peer architectures; QoS optimization; Rate allocation; Rate allocation algorithm; Videoconference; Videoconferencing systems; Optimization
Deep learning for mobile multimedia: A survey,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022331462&doi=10.1145%2f3092831&partnerID=40&md5=318857b57f1b1822af241a6659376fa6,"Deep Learning (DL) has become a crucial technology for multimedia computing. It offers a powerful instrument to automatically produce high-level abstractions of complex multimedia data, which can be exploited in a number of applications, including object detection and recognition, speech-to- text, media retrieval, multimodal data analysis, and so on. The availability of affordable large-scale parallel processing architectures, and the sharing of effective open-source codes implementing the basic learning algorithms, caused a rapid diffusion of DL methodologies, bringing a number of new technologies and applications that outperform, in most cases, traditional machine learning technologies. In recent years, the possibility of implementing DL technologies on mobile devices has attracted significant attention. Thanks to this technology, portable devices may become smart objects capable of learning and acting. The path toward these exciting future scenarios, however, entangles a number of important research challenges. DL architectures and algorithms are hardly adapted to the storage and computation resources of a mobile device. Therefore, there is a need for new generations of mobile processors and chipsets, small footprint learning and inference algorithms, new models of collaborative and distributed processing, and a number of other fundamental building blocks. This survey reports the state of the art in this exciting research area, looking back to the evolution of neural networks, and arriving to the most recent results in terms of methodologies, technologies, and applications for mobile environments. © 2017 ACM.",Deep learning; Deep neural networks; Mobile multimedia computing,Character recognition; Complex networks; Computer architecture; Deep neural networks; Digital storage; Distributed computer systems; Education; Engineering education; Human computer interaction; Inference engines; Learning algorithms; Mobile devices; Network architecture; Object detection; Object recognition; Speech recognition; Surveys; Distributed processing; Fundamental building blocks; Large-scale parallel processing; Machine learning technology; Mobile multimedia computing; Multimodal data analysis; Object detection and recognition; Technologies and applications; Deep learning
From annotation to computer-aided diagnosis: Detailed evaluation of a medical multimedia system,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021829593&doi=10.1145%2f3079765&partnerID=40&md5=8c029bff9cb76e6dd4890409c5b9ee54,"Holisticmedical multimedia systems covering end-to-end functionality from data collection to aided diagnosis are highly needed, but rare. In many hospitals, the potential value of multimedia data collected through routine examinations is not recognized. Moreover, the availability of the data is limited, as the health care personnel may not have direct access to stored data. However, medical specialists interact with multimedia content daily through their everyday work and have an increasing interest in finding ways to use it to facilitate their work processes. In this article, we present a novel, holistic multimedia system aiming to tackle automatic analysis of video from gastrointestinal (GI) endoscopy. The proposed system comprises the whole pipeline, including data collection, processing, analysis, and visualization. It combines filters using machine learning, image recognition, and extraction of global and local image features. The novelty is primarily in this holistic approach and its real-time performance, where we automate a complete algorithmic GI screening process. We built the system in a modular way to make it easily extendable to analyze various abnormalities, and we made it efficient in order to run in real time. The conducted experimental evaluation proves that the detection and localization accuracy are comparable or even better than existing systems, but it is by far leading in terms of real-time performance and efficient resource consumption. © 2017 ACM.",Evaluation; Gastrointestinal tract; Medical multimedia system,Data acquisition; Data handling; Data visualization; Diagnosis; Image recognition; Multimedia systems; Real time systems; Detection and localization; Evaluation; Experimental evaluation; Gastrointestinal endoscopies; Gastrointestinal tract; Local image features; Real time performance; Resource consumption; Computer aided diagnosis
Enhancing person re-identification in a self-trained subspace,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022322208&doi=10.1145%2f3089249&partnerID=40&md5=3c8433e75836704c406ffbc5ba2ad567,"Despite the promising progress made in recent years, person re-identification (re-ID) remains a challenging task due to the complex variations in human appearances from different camera views. For this challenging problem, a large variety of algorithms have been developed in the fully supervised setting, requiring access to a large amount of labeled training data. However, the main bottleneck for fully supervised re-ID is the limited availability of labeled training samples. To address this problem, we propose a self-trained subspace learning paradigm for person re-ID that effectively utilizes both labeled and unlabeled data to learn a discriminative subspace where person images across disjoint camera views can be easily matched. The proposed approach first constructs pseudo-pairwise relationships among unlabeled persons using the k-nearest neighbors algorithm. Then, with the pseudo-pairwise relationships, the unlabeled samples can be easily combined with the labeled samples to learn a discriminative projection by solving an eigenvalue problem. In addition,we refine the pseudo-pairwise relationships iteratively, which further improves learning performance. Amulti-kernel embedding strategy is also incorporated into the proposed approach to cope with the non-linearity in a person's appearance and explore the complementation of multiple kernels. In this way, the performance of person re-ID can be greatly enhanced when training data are insufficient. Experimental results on six widely used datasets demonstrate the effectiveness of our approach, and its performance can be comparable to the reported results of most state-of-the-art fully supervised methods while using much fewer labeled data. © 2017 ACM.",Person re-identification; Self-training; Semi-supervised learning,Cameras; Eigenvalues and eigenfunctions; Iterative methods; Learning systems; Nearest neighbor search; Problem solving; Supervised learning; Embedding strategies; K-nearest neighbors; Labeled and unlabeled data; Labeled training data; Learning performance; Person re identifications; Self training; Semi- supervised learning; Education
Best papers of the 2016 ACM multimedia systems (MMSys) conference and workshop on network and operating system support for digital audio and video (NOSSDAV) 2016,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022337268&doi=10.1145%2f3084539&partnerID=40&md5=d47eb9f9166fae0209d554ac0b10693c,[No abstract available],,
Recognizing human actions with outlier frames by observation filtering and completion,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022330341&doi=10.1145%2f3089250&partnerID=40&md5=658cd0d138aa2c0b3fb62ca4bf13b83d,"This article addresses the problem of recognizing partially observed human actions. Videos of actions acquired in the realworld often contain corrupt frames caused by various factors. These frames may appear irregularly, and make the actions only partially observed. They change the appearance of actions and degrade the performance of pretrained recognition systems. In this article, we propose an approach to address the corrupt-frame problem without knowing their locations and durations in advance. The proposed approach includes two key components: outlier filtering and observation completion. The former identifies and filters out unobserved frames, and the latter fills up the filtered parts by retrieving coherent alternatives from training data. Hidden Conditional Random Fields (HCRFs) are then used to recognize the filtered and completed actions. Our approach has been evaluated on three datasets, which contain both fully observed actions and partially observed actions with either real or synthetic corrupt frames. The experimental results show that our approach performs favorably against the other state-of-the-art methods, especially when corrupt frames are present. © 2017 ACM.",And conditional random fields; Early prediction; Gap filling; Human action recognition; Observation completion; Outlier filtering,Random processes; Statistics; Conditional random field; Early prediction; Frame problems; Gap filling; Hidden conditional random fields; Human-action recognition; Recognition systems; State-of-the-art methods; Image recognition
Spott: On-the-spot e-commerce for television using deep learning-based video analysis techniques,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022341806&doi=10.1145%2f3092834&partnerID=40&md5=4206a29b0414aa0e5bdfdaa80da15693,"Spott is an innovative second screen mobile multimedia application which offers viewers relevant information on objects (e.g., clothing, furniture, food) they see and like on their television screens. The application enables interaction between TV audiences and brands, so producers and advertisers can offer potential consumers tailored promotions, e-shop items, and/or free samples. In line with the current views on innovation management, the technological excellence of the Spott application is coupled with iterative user involvement throughout the entire development process. This article discusses both of these aspects and how they impact each other. First, we focus on the technological building blocks that facilitate the (semi-) automatic interactive tagging process of objects in the video streams. The majority of these building blocks extensively make use of novel and state-of-the-art deep learning concepts and methodologies. We show how these deep learning based video analysis techniques facilitate video summarization, semantic keyframe clustering, and (similar) object retrieval. Secondly, we provide insights in user tests that have been performed to evaluate and optimize the application's user experience. The lessons learned from these open field tests have already been an essential input in the technology development and will further shape the future modifications to the Spott application. © 2017 ACM.",Deep learning; Experience studies; Interactive television; Metadata enrichment; Object recognition; User-validation; Video summarization,Education; Interactive television; Iterative methods; Object recognition; Semantics; Video recording; Video signal processing; Video streaming; Development process; Experience studies; Innovation management; Interactive tagging; Mobile multimedia applications; Technology development; User-validation; Video summarization; Deep learning
Query expansion for content-based similarity search using local and global features,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022320005&doi=10.1145%2f3063595&partnerID=40&md5=61b7eb9762cae40e05af399162e35432,"This article presents an efficient and totally unsupervised content-based similarity search method for multimedia data objects represented by high-dimensional feature vectors. The assumption is that the similarity measure is applicable to feature vectors of arbitrary length. During the offline process, different sets of features are selected by a generalized version of the Laplacian Score in an unsupervised way for individual data objects in the database. Online retrieval is performed by ranking the query object in the feature spaces of candidate objects. Those candidates for which the query object is ranked highly are selected as the query results. The ranking scheme is incorporated into an automated query expansion framework to further improve the semantic quality of the search result. Extensive experiments were conducted on several datasets to show the capability of the proposed method in boosting effectiveness without losing efficiency. © 2017 ACM.",Content-based similarity search; Flexible aggregation; Query expansion; Subjective feature space; Unsupervised feature selection,Semantics; Feature space; Flexible aggregation; Query expansion; Similarity search; Unsupervised feature selection; Query processing
V-JAUNE: A framework for joint action recognition and video summarization,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019912816&doi=10.1145%2f3063532&partnerID=40&md5=a1339e3fa8a540a243385668fe2ba3e1,"Video summarization and action recognition are two important areas of multimedia video analysis. While these two areas have been tackled separately to date, in this article, we present a latent structural SVM framework to recognize the action and derive the summary of a video in a joint, simultaneous fashion. Efficient inference is provided by a submodular score function that accounts for the action and summary jointly. In this article, we also define a novel measure to evaluate the quality of a predicted video summary against the annotations of multiple annotators. Quantitative and qualitative results over two challenging action datasets-the ACE and MSR DailyActivity3D datasets-show that the proposed joint approach leads to higher action recognition accuracy and equivalent or better summary quality than comparable approaches that perform these tasks separately. © 2017 ACM.",Action recognition; latent structural SVM; sub-modular inference; video summarization,Video recording; Video signal processing; Action recognition; Joint actions; latent structural SVM; Multimedia video; Score function; sub-modular inference; Video summaries; Video summarization; Quality control
A multiplexing scheme for multimodal teleoperation,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017582077&doi=10.1145%2f3063594&partnerID=40&md5=2771dd7d28866e1685550b9f5acf0a0e,"This article proposes an application-layer multiplexing scheme for teleoperation systems with multimodal feedback (video, audio, and haptics). The available transmission resources are carefully allocated to avoid delay-jitter for the haptic signal potentially caused by the size and arrival time of the video and audio data. The multiplexing scheme gives high priority to the haptic signal and applies a preemptive-resume scheduling strategy to stream the audio and video data. The proposed approach estimates the available transmission rate in real time and adapts the video bitrate, data throughput, and force buffer size accordingly. Furthermore, the proposed scheme detects sudden transmission rate drops and applies congestion control to avoid abrupt delay increases and converge promptly to the altered transmission rate. The performance of the proposed scheme is measured objectively in terms of end-to-end signal latencies, packet rates, and peak signal-to-noise ratio (PSNR) for visual quality. Moreover, peak-delay and convergence time measurements are carried out to investigate the performance of the congestion control mode of the system. © 2017 ACM.",Congestion control; Haptic compression and communication; Haptics; Human-robot interaction over communication networks; Multiplexing; Rate control; Teleoperation,Audio systems; Congestion control (communication); Human robot interaction; Image coding; Multiplexing; Remote control; Robots; Haptics; Multimodal feedback; Multiplexing schemes; Peak signal to noise ratio; Rate controls; Scheduling strategies; Teleoperation systems; Transmission resources; Signal to noise ratio
A dual-domain perceptual framework for generating visual inconspicuous counterparts,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019882222&doi=10.1145%2f3068427&partnerID=40&md5=e3b362fe120abd45ade029f46e470255,"For a given image, it is a challenging task to generate its corresponding counterpart with visual inconspicuous modification. The complexity of this problem reasons from the high correlativity between the editing operations and vision perception. Essentially, a significant requirement that should be emphasized is how to make the object modifications hard to be found visually in the generative counterparts. In this article, we propose a novel dual-domain perceptual framework to generate visual inconspicuous counterparts, which applies the perceptual bidirectional similarity metric (PBSM) and appearance similarity metric (ASM) to create the dual-domain perception error minimization model. The candidate targets are yielded by the well-known PatchMatch model with the strokes-based interactions and selective object library. By the dual-perceptual evaluation index, all candidate targets are sorted to select out the best result. For demonstration, a series of objective and subjective measurements are used to evaluate the performance of our framework.",Bidirectional similarity; Image editing; Image quality assessment; Object manipulation; Visual perception,Computer networks; Bidirectional similarity; Image editing; Image quality assessment; Object manipulation; Visual perception; Hardware
Congestion control for network-aware Telehaptic communication,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017171933&doi=10.1145%2f3052821&partnerID=40&md5=bf4166bcdb6ae5907fb643def289404b,"Telehaptic applications involve delay-sensitive multimedia communication between remote locations with distinct Quality of Service (QoS) requirements for different media components. These QoS constraints pose a variety of challenges, especially when the communication occurs over a shared network, with unknown and time-varying cross-traffic. In this work, we propose a transport layer congestion control protocol for telehaptic applications operating over shared networks, termed as Dynamic Packetization Module (DPM). DPM is a lossless, network-aware protocol that tunes the telehaptic packetization rate based on the level of congestion in the network. To monitor the network congestion, we devise a novel network feedback module, which communicates the end-to-end delays encountered by the telehaptic packets to the respective transmitters with negligible overhead. Via extensive simulations, we show that DPM meets the QoS requirements of telehaptic applications over a wide range of network cross-traffic conditions. We also report qualitative results of a real-time telepottery experiment with several human subjects, which reveal that DPM preserves the quality of telehaptic activity even under heavily congested network scenarios. Finally, we compare the performance of DPM with several previously proposed telehaptic communication protocols and demonstrate that DPM outperforms these protocols. © 2017 ACM.",Congestion control; Dynamic rate adaptation; Multimedia; QoS; Telehaptic communication; Transport layer,Congestion control (communication); Internet protocols; Multimedia systems; Traffic congestion; Congestion control protocols; Dynamic rate adaptation; Extensive simulations; Multi-media communications; Multimedia; Network congestions; Qualityof-service requirement (QoS); Transport layers; Quality of service
Multichannel-Kernel canonical correlation analysis for cross-view person reidentification,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015180860&doi=10.1145%2f3038916&partnerID=40&md5=2fc2ddc91b05487f0d316c5332d9927a,"In this article, we introduce a method to overcome one of the main challenges of person reidentification in multicamera networks, namely cross-view appearance changes. The proposed solution addresses the extreme variability of person appearance in different camera views by exploiting multiple feature representations. For each feature, kernel canonical correlation analysis with different kernels is employed to learn several projection spaces in which the appearance correlation between samples of the same person observed from different cameras is maximized. An iterative logistic regression is finally used to select and weight the contributions of each projection and perform the matching between the two views. Experimental evaluation shows that the proposed solution obtains comparable performance on the VIPeR and PRID 450s datasets and improves on the PRID and CUHK01 datasets with respect to the state of the art. © 2017 ACM.",KCCA; Late fusion; Person reidentification,Cameras; Iterative methods; Experimental evaluation; KCCA; Kernel canonical correlation analysis; Late fusion; Logistic regressions; Multi-camera networks; Multiple features; Person re identifications; Kcca; Correlation methods
Multi-class Latent Concept Pooling for computer-aided endoscopy diagnosis,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017103449&doi=10.1145%2f3051481&partnerID=40&md5=31ebfa0d724ddfad80f5b47b4d8e3172,"Successful computer-aided diagnosis systems typically rely on training datasets containing sufficient and richly annotated images. However, detailed image annotation is often time consuming and subjective, especially for medical images, which becomes the bottleneck for the collection of large datasets and then building computer-aided diagnosis systems. In this article, we design a novel computer-aided endoscopy diagnosis system to deal with the multi-classification problem of electronic endoscopy medical records (EEMRs) containing sets of frames, while labels of EEMRs can be mined from the corresponding text records using an automatic text-matching strategy without human special labeling. With unambiguous EEMR labels and ambiguous frame labels, we propose a simple but effective pooling scheme called Multi-class Latent Concept Pooling, which learns a codebook from EEMRs with different classes step by step and encodes EEMRs based on a soft weighting strategy. In our method, a computer-aided diagnosis system can be extended to new unseen classes with ease and applied to the standard single-instance classification problem even though detailed annotated images are unavailable. In order to validate our system, we collect 1,889 EEMRs with more than 59K frames and successfully mine labels for 348 of them. The experimental results show that our proposed system significantly outperforms the state-of-the-art methods. Moreover, we apply the learned latent concept codebook to detect the abnormalities in endoscopy images and compare it with a supervised learning classifier, and the evaluation shows that our codebook learning method can effectively extract the true prototypes related to different classes from the ambiguous data. © 2017 ACM.",Computer-aided diagnosis; Endoscopy; Latent Concept Pooling; Multi-class; Sparse dictionary learning,Computer aided instruction; Diagnosis; Endoscopy; Image retrieval; Medical imaging; Text processing; Computer aided diagnosis systems; Instance classifications; Latent Concept Pooling; Multi-class; Multi-classification problems; Sparse dictionaries; State-of-the-art methods; Weighting strategies; Computer aided diagnosis
Crowd scene understanding from video: A survey,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017165434&doi=10.1145%2f3052930&partnerID=40&md5=87d9eb36b14d7635095bd333796132fe,"Crowd video analysis has applications in crowd management, public space design, and visual surveillance. Example tasks potentially aided by automated analysis include anomaly detection (such as a person walking against the grain of traffic or rapid assembly/dispersion of groups of people), population and density measurements, and interactions between groups of people. This survey explores crowd analysis as it relates to two primary research areas: crowd statistics and behavior understanding. First, we survey methods for counting individuals and approximating the density of the crowd. Second, we showcase research efforts on behavior understanding as related to crowds. These works focus on identifying groups, interactions within small groups, and abnormal activity detection such as riots and bottlenecks in large crowds.Works presented in this section also focus on tracking groups of individuals, either as a single entity or a subset of individuals within the frame of reference. Finally, a summary of datasets available for crowd activity video research is provided. © 2017 ACM 1551-6857/2017/03-ART19 15.00.",Crowd analysis; Datasets; Human Activity,Security systems; Surveys; Abnormal activity detection; Automated analysis; Behavior understanding; Crowd analysis; Datasets; Human activities; Scene understanding; Visual surveillance; Behavioral research
A temporal order modeling approach to human action recognition from multimodal sensor data,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015181520&doi=10.1145%2f3038917&partnerID=40&md5=6ab9a8c6001a61e47fb3ee24e1b34a76,"From wearable devices to depth cameras, researchers have exploited various multimodal data to recognize human actions for applications, such as video gaming, education, and healthcare. Although there many successful techniques have been presented in the literature, most current approaches have focused on statistical or local spatiotemporal features and do not explicitly explore the temporal dynamics of the sensor data. However, human action data contain rich temporal structure information that can characterize the unique underlying patterns of different action categories. From this perspective, we propose a novel temporal order modeling approach to human action recognition. Specifically, we explore subspace projections to extract the latent temporal patterns from different human action sequences. The temporal order between these patterns are compared, and the index of the pattern that appears first is used to encode the entire sequence. This process is repeated multiple times and produces a compact feature vector representing the temporal dynamics of the sequence. Human action recognition can then be efficiently solved by the nearest neighbor search based on the Hamming distance between these compact feature vectors. We further introduce a sequential optimization algorithm to learn the optimized projections that preserve the pairwise label similarity of the action sequences. Experimental results on two public human action datasets demonstrate the superior performance of the proposed technique in both accuracy and efficiency. © 2017 ACM.",Human action recognition; Multimodal sensor data; Optimization; Temporal order modeling,Nearest neighbor search; Optimization; Human-action recognition; Multimodal sensor; Nearest neighbors; Sequential optimization algorithms; Spatio temporal features; Subspace projection; Temporal order; Temporal structures; Hamming distance
A video bitrate adaptation and prediction mechanism for HTTP Adaptive Streaming,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017126495&doi=10.1145%2f3052822&partnerID=40&md5=639d91d47c3879f485e9a00217473be6,"The Hypertext Transfer Protocol (HTTP) Adaptive Streaming (HAS) has now become ubiquitous and accounts for a large amount of video delivery over the Internet. But since the Internet is prone to bandwidth variations, HAS's up and down switching between different video bitrates to keep up with bandwidth variations leads to a reduction in Quality of Experience (QoE). In this article, we propose a video bitrate adaptation and prediction mechanism based on Fuzzy logic for HAS players, which takes into consideration the estimate of available network bandwidth as well as the predicted buffer occupancy level in order to proactively and intelligently respond to current conditions. This leads to two contributions: First, it allows HAS players to take appropriate actions, sooner than existing methods, to prevent playback interruptions caused by buffer underrun, reducing the ON-OFF traffic phenomena associated with current approaches and increasing the QoE. Second, it facilitates fair sharing of bandwidth among competing players at the bottleneck link. We present the implementation of our proposed mechanism and provide both empirical/QoE analysis and performance comparison with existing work. Our results show that, compared to existing systems, our system has (1) better fairness among multiple competing players by almost 50% on average and as much as 80% as indicated by Jain's fairness index and (2) better perceived quality of video by almost 8% on average and as much as 17%, according to the estimate the Mean Opinion Score (eMOS) model. © 2017 ACM.",Adaptation; Adaptive moving average; Fuzzy-based controller; Grey model; HAS; ON-OFF traffic; Prediction; Video bitrate,Bandwidth; Forecasting; Fuzzy logic; Hypertext systems; Quality of service; Adaptation; Adaptive moving averages; Grey Model; ON-OFF traffic; Video bitrate; HTTP
Machine learning-based parametric audiovisual quality prediction models for real-time communications,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017094855&doi=10.1145%2f3051482&partnerID=40&md5=828fe145edee303de374477cbe5bf41d,"In order to mechanically predict audiovisual quality in interactive multimedia services, we have developed machine learning-based no-reference parametric models. We have compared Decision Trees-based ensemble methods, Genetic Programming and Deep Learning models that have one and more hidden layers. We have used the Institut national de la recherche scientifique (INRS) audiovisual quality dataset specifically designed to include ranges of parameters and degradations typically seen in real-time communications. Decision Trees-based ensemble methods have outperformed both Deep Learning- and Genetic Programming-based models in terms of Root-Mean-Square Error (RMSE) and Pearson correlation values. We have also trained and developed models on various publicly available datasets and have compared our results with those of these original models. Our studies show that Random Forests-based prediction models achieve high accuracy for both the INRS audiovisual quality dataset and other publicly available comparable datasets. © 2017 ACM.",Audiovisual quality dataset; Machine learning; MOS; No-reference models; Perceived quality estimation,Artificial intelligence; Correlation methods; Decision trees; Deep learning; Forecasting; Forestry; Genetic algorithms; Genetic programming; Interactive computer systems; Mean square error; Molybdenum; Multimedia services; Multimedia systems; Audiovisual quality; Interactive multimedia; No references; Parametric models; Pearson correlation; Perceived quality; Real-time communication; Root mean square errors; Learning systems
An efficient framework for compressed domain watermarking in p frames of high-efficiency video coding (HEVC)-encoded video,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011360839&doi=10.1145%2f3002178&partnerID=40&md5=247961f127e4ec8f8516e95ee16d12f5,"Digital watermarking has received much attention in recent years as a promising solution to copiright protection. Video watermarking in compressed domain has gained importance since videos are stored and transmitted in a compressed format. This decreases the overhead to fully decode and re-encode the video for embedding and extraction of the watermark. High Efficiency Video Coding (HEVC/H.265) is the latest and most efficient video compression standard and a successor to H.264 Advanced Video Coding. In this article, we propose a robust watermarking framework for HEVC-encoded video using informed detector. A readable watermark is embedded invisibly in P frames for better perceptual quality. Our framework imposes security and robustness by selecting appropriate blocks using a random key and the spatio-temporal characteristics of the compressed video. A detail analysis of the strengths of different compressed domain features is performed for implementing the watermarking framework. We experimentally demonstrate the utility of the proposed work. The results show that the proposed work effectively limits the increase in video bitrate and degradation in perceptual quality. The proposed framework is robust against re-encoding and image processing attacks. © 2017 ACM.",Compressed domain; High-efficiency video coding (HEVC); Video watermarking,Codes (symbols); Digital watermarking; Efficiency; Encoding (symbols); Image processing; Video signal processing; Advanced video coding; Compressed domain; Compressed domain watermarking; High-efficiency video coding; Robust watermarking; Spatiotemporal characteristics; Video compression standards; Video watermarking; Image compression
Prediction of virality timing using cascades in social media,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006983672&doi=10.1145%2f2978771&partnerID=40&md5=8c80f0de699b7b8eef6c9c212aea33d1,"Predicting content going viral in social networks is attractive for viral marketing, advertisement, entertainment, and other applications, but it remains a challenge in the big data era today. Previous works mainly focus on predicting the possible popularity of content rather than the timing of reaching such popularity. This work proposes a novel yet practical iterative algorithm to predict virality timing, in which the correlation between the timing and growth of content popularity is captured by using its own big data naturally generated from users' sharing. Such data is not only able to correlate the dynamics and associated timings in social cascades of viral content but also can be useful to self-correct the predicted timing against the actual timing of the virality in each iterative prediction. The proposed prediction algorithm is verified by datasets from two popular social networks-Twitter and Digg-as well as two synthesized datasets with extreme network densities and infection rates. With about 50% of the required content virality data available (i.e., halfway before reaching its actual virality timing), the error of the predicted timing is proven to be bounded within a 40% deviation from the actual timing. To the best of our knowledge, this is the first work that predicts content virality timing iteratively by capturing social cascades dynamics. © 2016 ACM 1551-6857/2016/12-ART2 $15.00.",Social cascade; Social media and networks; Virality prediction; Virality timing,Forecasting; Iterative methods; Social networking (online); Timing circuits; Content popularities; Extreme Networks; Infection rates; Iterative algorithm; Prediction algorithms; Social media; Viral marketing; Virality timing; Big data
Identification of reconstructed speech,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011382783&doi=10.1145%2f3004055&partnerID=40&md5=3a2a875e32753a8064f2d9018bcae7c8,"Both voice conversion and hidden Markov model-(HMM) based speech synthesis can be used to produce artificial voices of a target speaker. They have shown great negative impacts on speaker verification (SV) systems. In order to enhance the security of SV systems, the techniques to detect converted/synthesized speech should be taken into consideration. During voice conversion and HMM-based synthesis, speech reconstruction is applied to transform a set of acoustic parameters to reconstructed speech. Hence, the identification of reconstructed speech can be used to distinguish converted/synthesized speech from human speech. Several related works on such identification have been reported. The equal error rates (EERs) lower than 5% of detecting reconstructed speech have been achieved. However, through the cross-database evaluations on different speech databases, we find that the EERs of several testing cases are higher than 10%. The robustness of detection algorithms to different speech databases needs to be improved. In this article, we propose an algorithm to identify the reconstructed speech. Three different speech databases and two different reconstruction methods are considered in our work, which has not been addressed in the reported works. The high-dimensional data visualization approach is used to analyze the effect of speech reconstruction on Melfrequency cepstral coefficients (MFCC) of speech signals. The Gaussian mixturemodel supervectors of MFCC are used as acoustic features. Furthermore, a set of commonly used classification algorithms are applied to identify reconstructed speech. According to the comparison among different classification methods, linear discriminant analysis-ensemble classifiers are chosen in our algorithm. Extensive experimental results show that the EERs lower than 1% can be achieved by the proposed algorithm in most cases, outperforming the reported state-of-the-art identification techniques. © 2017 ACM.",Audio forensics; GMM supervectors; Identification; LDA-ensemble classification; MFCC; Reconstructed speech; Speaker verification,Clustering algorithms; Data mining; Data visualization; Database systems; Discriminant analysis; Hidden Markov models; Identification (control systems); Markov processes; Natural language processing systems; Speech; Speech processing; Speech synthesis; Trellis codes; Audio forensics; Ensemble classification; GMM supervectors; MFCC; Speaker verification; Speech recognition
Drift-compensated robust watermarking algorithm for H.265/HEVC video stream,2017,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011344605&doi=10.1145%2f3009910&partnerID=40&md5=47cc68f5d09012aa520752de19209264,"It has been observed in the recent literature that the drift error due to watermarking degrades the visual quality of the embedded video. The existing drift error handling strategies for recent video standards such as H.264 may not be directly applicable for upcoming high-definition video standards (such as High Efficiency Video Coding (HEVC)) due to different compression architecture. In this article, a compressed domain watermarking scheme is proposed for H.265/HEVC bit stream that can handle drift error propagation both for intra-and interprediction process. Additionally, the proposed scheme shows adequate robustness against recompression attack as well as common image processing attacks while maintaining decent visual quality. A comprehensive set of experiments has been carried out to justify the efficacy of the proposed scheme over the existing literature. © 2017 ACM.",Compressed domainwatermarking; Drift compensation; H.265/HEVC; High efficiency video coding; Robustwatermarking,Digital watermarking; Efficiency; Image processing; Video signal processing; Video streaming; Compressed domainwatermarking; Drift compensation; H.265/HEVC; High-efficiency video coding; Robustwatermarking; Errors
Audiovisual tool for solfège assessment,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008187233&doi=10.1145%2f3007194&partnerID=40&md5=1caaa9899afed88c0c675a367659d059,"Solfège is a general technique used in the music learning process that involves the vocal performance of melodies, regarding the time and duration of musical sounds as specified in the music score, properly associated with the meter-mimicking performed by hand movement. This article presents an audiovisual approach for automatic assessment of this relevant musical study practice. The proposed system combines the gesture of meter-mimicking (video information) with the melodic transcription (audio information), where hand movement works as a metronome, controlling the time flow (tempo) of the musical piece. Thus, meter-mimicking is used to align the music score (ground truth) with the sung melody, allowing assessment even in time-dynamic scenarios. Audio analysis is applied to achieve the melodic transcription of the sung notes and the solfège performances are evaluated by a set of Bayesian classifiers that were generated from real evaluations done by experts listeners. © 2016 ACM.",Automatic assessment; Melodic transcription; Meter-mimicking; Music education; Sight-singing; Solfège,Transcription; Audio information; Automatic assessment; Bayesian classifier; Learning process; Music education; Sight-singing; Video information; Vocal performance; Audio acoustics
A tensor-based framework for software-defined cloud data center,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008457768&doi=10.1145%2f2983640&partnerID=40&md5=8184b99613e271144fa638a37e2383a5,"Multimedia has been exponentially increasing as the biggest big data, which consist of video clips, images, and audio files. Processing and analyzing them on a cloud data center have become a preferred solution that can utilize the large pool of cloud resources to address the problems caused by the tremendous amount of unstructured multimedia data. However, there exist many challenges in processing multimedia big data on a cloud data center, such as multimedia data representation approach, an efficient networking model, and an estimation method for traffic patterns. The primary purpose of this article is to develop a novel tensor-based software-defined networking model on a cloud data center for multimedia big-data computation and communication. First, an overview of the proposed framework is provided, in which the functions of the representative modules are briefly illustrated. Then, three models,-forwarding tensor, control tensor, and transition tensor-are proposed for management of networking devices and prediction of network traffic patterns. Finally, two algorithms about single-mode and multimode tensor eigen-decomposition are developed, and the incremental method is employed for efficiently updating the generated eigen-vector and eigen-tensor. Experimental results reveal that the proposed framework is feasible and efficient to handle multimedia big data on a cloud data center. © 2016 ACM.",Big data; Data center; Software defined networks; Tensor,Data handling; Distributed computer systems; Multimedia systems; Software defined networking; Tensors; Cloud data centers; Data centers; Eigen decomposition; Estimation methods; Incremental method; Networking devices; Networking model; Preferred solutions; Big data
Consistent synchronization of action order with least noticeable delays in fast-paced multiplayer online games,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008145374&doi=10.1145%2f3003727&partnerID=40&md5=fe75b6dd915fede384a938ee2eb5f0a1,"When running multiplayer online games on IP networks with losses and delays, the order of actions may be changed when compared to the order run on an ideal network with no delays and losses. To maintain a proper ordering of events, traditional approaches either use rollbacks to undo certain actions or local lags to introduce additional delays. Both may be perceived by players because their changes are beyond the just-noticeable-difference (JND) threshold. In this article, we propose a novel method for ensuring a strongly consistent completion order of actions, where strong consistency refers to the same completion order as well as the same interval between any completion time and the corresponding ideal reference completion time under no network delay. We find that small adjustments within the JND on the duration of an action would not be perceivable, as long as the duration is comparable to the network round-trip time. We utilize this property to control the vector of durations of actions and formulate the search of the vector as a multidimensional optimization problem. By using the property that players are generally more sensitive to the most prominent delay effect (with the highest probability of noticeability Pnotice or the probability of correctly noticing a change when compared to the reference), we prove that the optimal solution occurs when Pnotice of the individual adjustments are equal. As this search can be done efficiently in polynomial time (∼5ms) with a small amount of space (∼160KB), the search can be done at runtime to determine the optimal control. Last, we evaluate our approach on the popular open-source online shooting game BZFlag. © 2016 ACM.",Consistency; Human factors; Internet; Quality of experience,Human engineering; Internet; Optimization; Polynomial approximation; Quality of service; Consistency; Just-noticeable difference; Multi-player online games; Multidimensional optimization; Ordering of actions; Quality of experience (QoE); Strong consistency; Traditional approaches; Social networking (online)
Predicting occupation from images by combining face and body context information,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006998125&doi=10.1145%2f3009911&partnerID=40&md5=1636af57aed9daf5306f0b7fc8fc3d39,"Facial images embed age, gender, and other rich information that is implicitly related to occupation. In this work, we advocate that occupation prediction from a single facial image is a doable computer vision problem. We extract multilevel hand-crafted features associated with locality-constrained linear coding and convolutional neural network features as image occupation descriptors. To avoid the curse of dimensionality and overfitting, a boost strategy called multichannel SVM is used to integrate features from face and body. Intra- and interclass visual variations are jointly considered in the boosting framework to further improve performance. In the evaluation, we verify the effectiveness of predicting occupation from face and demonstrate promising performance obtained by combining face and body information. More importantly, our work further integrates deep features into the multichannel SVM framework and shows significantly better performance over the state of the art. © 2016 ACM 1551-6857/2016/12-ART7 $15.00.",Adaptive weighting; Body context; Convolutional neural network; Discriminant multi-channel SVM; Locality-constrained linear coding; Occupation prediction; Spatial pyramid,Computer vision; Convolution; Forecasting; Image coding; Neural networks; Adaptive weighting; Body context; Convolutional neural network; Linear coding; Multi channel; Spatial pyramids; Employment
Content-adaptive display power saving for internet video applications on mobile devices,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84999176810&doi=10.1145%2f2996461&partnerID=40&md5=63b5b7e33bb3bb9a2614ae6b9c02ff98,"Backlight scaling is a technique proposed to reduce the display panel power consumption by strategically dimming the backlight. However, for mobile video applications, a computationally intensive luminance compensation step must be performed in combination with backlight scaling to maintain the perceived appearance of video frames. This step, if done by the Central Processing Unit (CPU), could easily offset the power savings via backlight dimming. Furthermore, computing the backlight scaling values requires per-frame luminance information, which is typically too energy intensive to compute on mobile devices. In this article, we propose Content-Adaptive Display (CAD) for two typical Internet mobile video applications: video streaming and real-time video communication. CAD uses the mobile device's Graphics Processing Unit (GPU) rather than the CPU to perform luminance compensation at reduced power consumption. For video streaming where video frames are available in advance, we compute the backlight scaling schedule using a more efficient dynamic programming algorithm than existing work. For real-time video communication where video frames are generated on the fly, we propose a greedy algorithm to determine the backlight scaling at runtime. We implement CAD in one video streaming application and one real-time video call application on the Android platform and use a Monsoon power meter to measure the real power consumption. Experiment results show that CAD can save more than 10% overall power consumption for up to 55.7% videos during video streaming and up to 31.0% overall power consumption in real-time video calls. © 2016 ACM.",,Computer graphics; Computer graphics equipment; Dynamic programming; Electric power utilization; Luminance; Mobile devices; Program processors; Video streaming; Android platforms; Backlight dimming; Dynamic programming algorithm; Graphics Processing Unit; Mobile video applications; Real-time video communication; Reduced power consumption; Video Streaming Applications; Display devices
Congestion-aware MAC layer adaptation to improve video telephony over Wi-Fi,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000763589&doi=10.1145%2f2983634&partnerID=40&md5=c66bf67e336b0dc977a22c8bedf31f15,"In wireless networks such as those based on IEEE 802.11, packet losses due to fading and interference are often misinterpreted as indications of congestion by the congestion control protocol at higher layers, causing an unnecessary decrease in the data sending rate. For delay-constrained applications such as video telephony, packet losses may result in excessive artifacts or freeze in the decoded video. We propose a simple and yet effective mechanism to detect and reduce channel-caused packet losses by adjusting the retry limit parameter of the IEEE 802.11 protocol while taking into account the delay requirement of the traffic. Since the retry limit is left configurable in the IEEE 802.11 standard and does not require cross-layer coordination, our scheme can be easily implemented and incrementally deployed. Experimental results of applying the proposed scheme to a WebRTC-based real-time video communication prototype show significant performance gain compared to the case where the retry limit is configured statically. © 2016 ACM.",802.11 MaC; Congestion detection; Google congestion control (GCC); Real-time video system; Retry limit; Video telephony; WebRTC; Wi-Fi,Packet loss; Real time systems; Standards; Wi-Fi; Wireless local area networks (WLAN); 802.11 MAC; Congestion detection; Google congestion control (GCC); Real time videos; Retry limits; Video telephony; WebRTC; Packet networks
Learning from collective intelligence: Feature learning using social images and tags,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000842805&doi=10.1145%2f2978656&partnerID=40&md5=4f33b1a3a1651cccd0f1863627e2a49d,"Feature representation for visual content is the key to the progress of many fundamental applications such as annotation and cross-modal retrieval. Although recent advances in deep feature learning offer a promising route towards these tasks, they are limited in application domains where high-quality and large-scale training data are expensive to obtain. In this article, we propose a novel deep feature learning paradigm based on social collective intelligence, which can be acquired from the inexhaustible social multimedia content on the Web, in particular, largely social images and tags. Differing from existing feature learning approaches that relyon high-quality image-label supervision, our weak supervision isacquired by mining the visual-semantic embeddings from noisy, sparse, and diverse social image collections. The resultant imageword embedding space can be used to (1) fine-tune deep visual models for low-level feature extractions and (2) seek sparse representations as high-level cross-modal features for both image and text. We offer an easy-to-use implementation for the proposed paradigm, which is fast and compatible with any state-of-the-art deep architectures. Extensive experiments on several benchmarks demonstrate that the cross-modal features learned by our paradigm significantly outperforms others in various applications such as content-based retrieval, classification, and image captioning. © 2016 ACM.",Cross-media analysis; Representation learning; Visual-semantic embedding,Content based retrieval; Semantics; Collective intelligences; Cross-media; Deep feature learning; Feature representation; Multimedia contents; Representation learning; Sparse representation; Visual semantics; Benchmarking
"Design, implementation, and measurement of a crowdsourcing-based content distribution platform",2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997416735&doi=10.1145%2f2978655&partnerID=40&md5=83967b973d331fd497c73b23675fb995,"Content distribution, especially the distribution of video content, unavoidably consumes bandwidth resources heavily. Internet content providers invest heavily in purchasing content distribution network (CDN) services. By deploying tens of thousands of edge servers close to end users, CDN companies are able to distribute content efficiently and effectively, but at considerable cost. Thus, it is of great importance to develop a new system that distributes content at a lower cost but comparable service quality. In lieu of expensive CDN systems, we implement a crowdsourcing-based content distribution system, Thunder Crystal, by renting bandwidth for content upload/download and storage for content cache from agents. This is a large-scale system with tens of thousands of agents, whose resources significantly amplify Thunder Crystal's content distribution capacity. The involved agents are either from ordinary Internet users or enterprises. Monetary rewards are paid to agents based on their upload traffic so as to motivate them to keep contributing resources. As far as we know, this is a novel system that has not been studied or implemented before. This article introduces the design principles and implementation details before presenting the measurement study. In summary, with the help of agent devices, Thunder Crystal is able to reduce the content distribution cost by one half and amplify the content distribution capacity by 11 to 15 times. © 2016 ACM.",Agent; CDN; Crowdsourcing; Video distribution,Agents; Bandwidth; Costs; Large scale systems; Bandwidth resource; Content distribution; Content distribution networks; Content distribution systems; Design Principles; Internet content providers; Measurement study; Video distribution; Crowdsourcing
Deep learning at scale and at ease,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997254867&doi=10.1145%2f2996464&partnerID=40&md5=3786d4ccaca1b561c70e2c53c6a24313,"Recently, deep learning techniques have enjoyed success in various multimedia applications, such as image classification and multimodal data analysis. Large deep learning models are developed for learning rich representations of complex data. There are two challenges to overcome before deep learning can be widely adopted in multimedia and other applications. One is usability, namely the implementation of different models and training algorithms must be done by nonexperts without much effort, especially when the model is large and complex. The other is scalability, namely the deep learning system must be able to provision for a huge demand of computing resources for training large models with massive datasets. To address these two challenges, in this article we design a distributed deep learning platform called SINGA, which has an intuitive programming model based on the common layer abstraction of deep learning models. Good scalability is achieved through flexible distributed training architecture and specific optimization techniques. SINGA runs on both GPUs and CPUs, and we show that it outperforms many other state-of-the-art deep learning systems. Our experience with developing and training deep learning models for real-life multimedia applications in SINGA shows that the platform is both usable and scalable. 2016 Copyright is held by the owner/author(s).",Deep learning; Distributed training; Multimedia,Program processors; Scalability; Computing resource; Intuitive programming; Learning techniques; Multimedia; Multimedia applications; Multimodal data analysis; Optimization techniques; Training algorithms; Deep learning
Generalized deep transfer networks for knowledge propagation in heterogeneous domains,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997194375&doi=10.1145%2f2998574&partnerID=40&md5=a279647ea6a85667df7035bbc22dd73b,"In recent years, deep neural networks have been successfully applied to model visual concepts and have achieved competitive performance on many tasks. Despite their impressive performance, traditional deep networks are subjected to the decayed performance under the condition of lacking sufficient training data. This problem becomes extremely severe for deep networks trained on a very small dataset, making them overfitting by capturing nonessential or noisy information in the training set. Toward this end, we propose a novel generalized deep transfer networks (DTNs), capable of transferring label information across heterogeneous domains, textual domain to visual domain. The proposed framework has the ability to adequately mitigate the problem of insufficient training images by bringing in rich labels from the textual domain. Specifically, to share the labels between two domains, we build parameter- and representation-shared layers. They are able to generate domain-specific and shared interdomain features, making this architecture flexible and powerful in capturing complex information from different domains jointly. To evaluate the proposed method, we release a new dataset extended from NUS-WIDE at http://imag.njust.edu.cn/NUS-WIDE-128.html. Experimental results on this dataset show the superior performance of the proposed DTNs compared to existing state-of-the-art methods. © 2016 ACM.","Algorithms; Cross-domain label transfer; Deep transfer network; Design; Experimentation; H.2.5 [database applications]: image representation; Heterogeneous-domain knowledge propagation; I.4.7 [learning]: parameter learning, concept learning, knowledge acquisition; Image classification; Performance",Algorithms; Deep neural networks; Delay tolerant networks; Design; Image classification; Concept learning; Cross-domain; Experimentation; Heterogeneous domains; Image representations; Performance; Transfer network; Classification (of information)
A content-aware video adaptation service to support mobile video,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000797830&doi=10.1145%2f2983636&partnerID=40&md5=6475b16e317898e82670b1eafe084fb2,"Adaptive video streaming systems rely on the availability of different quality versions of a video. Such a system can dynamically adjust the quality of a video stream during its playback depending on the available network throughput. Even if the necessary throughput is available, mobile users can benefit from limiting the generated data traffic as most cellular network contracts have data caps. Usually, if the cap is reached, the throughput is throttled to a speed that does not allow video streaming. Existing systems react to varying network conditions but often neglect content-specific adaptation needs. Content inspection can help to save data traffic when a higher bitrate representation would not increase the perceived quality. In this work, we present the Video Adaptation Service (VAS), a support service for a content-aware video adaptation for mobile devices. Based on the video content, the adaptation process is improved for both the available network resources and the perception of the user. By leveraging the content properties of a video stream, the system is able to maintain a stable video quality and at the same time reduce the generated data traffic. The system is evaluated with different adaptation schemes and shows that content-specific adaptation can both increase the perceived quality as well as reduce the data traffic. Additionally, we demonstrate the practical feasibility of this approach by integrating the VAS into Dynamic Adaptive Streaming over the Hypertext Transfer Protocol. © 2016 ACM.",Adaptation; Content-awareness; DASH; Video quality,Hypertext systems; Mobile telecommunication systems; Throughput; Adaptation; Adaptive video streaming; Content inspections; Content-awareness; DASH; Network throughput; Specific adaptations; Video quality; Video streaming
Toward an adaptive screencast platform: Measurement and optimization,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997281840&doi=10.1145%2f2886778&partnerID=40&md5=236e3073323184e56631caeeef6be2ef,"The binding between computing devices and displays is becoming dynamic and adaptive, and screencast technologies enable such binding over wireless networks. In this article, we design and conduct the first detailed measurement study on the performance of the state-of-the-art screencast technologies. Several commercial and one open-source screencast technologies are considered in our detailed analysis, which leads to several insights: (1) there is no single winning screencast technology, indicating room to further enhance the screencast technologies; (2) hardware video encoders significantly reduce the CPU usage at the expense of slightly higher GPU usage and end-to-end delay, and should be adopted in future screencast technologies; (3) comprehensive error resilience tools are needed as wireless communication is vulnerable to packet loss; (4) emerging video codecs designed for screen contents lead to a better Quality of Experience (QoE) of screencast; and (5) rate adaptation mechanisms are critical to avoiding degraded QoE due to network dynamics. As a case study, we propose a nonintrusive yet accurate available bandwidth estimation mechanism. Real experiments demonstrate the practicality and efficiency of our proposed solution. Our measurement methodology, open-source screencast platform, and case study allow researchers and developers to quantitatively evaluate other design considerations, which will lead to optimized screencast technologies. © 2016 ACM.",Design; H.5 [information systems applications]: multimedia information systems; Live video streaming; Measurement; Performance evaluation; Performance optimization; Real-time encoding,Bandwidth; Bins; Design; Display devices; Information systems; Measurements; Mobile telecommunication systems; Open systems; Quality of service; Real time systems; Video streaming; Wireless telecommunication systems; Live video streaming; MultiMedia Information Systems; Performance evaluation; Performance optimizations; Real-time encoding; Quality control
Mobile video streaming over dynamic single-frequency networks,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000786883&doi=10.1145%2f2983635&partnerID=40&md5=ed9bba9ff25fc2bcf47a1d8e3a4ac5e2,"The demand for multimedia streaming over mobile networks has been steadily increasing over the past several years. For instance, it has become common for mobile users to stream full TV episodes, sports events, and movies while on the go. Unfortunately, this growth in demand has strained the wireless networks despite the significant increase of their capacities with recent generations. Hence, efficient utilization of the expensive and limited wireless spectrum remains an important problem, especially in the context of multimedia streaming services that consume a large portion of the bandwidth capacity. In this article, we introduce the idea of dynamically configuring cells in wireless cellular networks to form single-frequency networks based on the multimedia traffic demands from users in each cell. We formulate the resource allocation problem in such complex networks with the goal of maximizing the number of served multimedia streams, and we prove that this problem is NP-Complete. Then we present an optimal solution to maximize the number of served multimedia streams within a cellular network. This optimal solution, however, may suffer from an exponential time complexity in the worst case, which is not practical for real-time streaming over large-scale networks. Therefore, we propose a heuristic algorithm with polynomial running time to provide faster and more practical solution for real-time deployments. Through detailed packet-level simulations, we assess the performance of the proposed algorithms with respect to the average service ratio, energy saving, video quality, frame loss rate, initial buffering time, rate of re-buffering events, and bandwidth overhead. We show that the proposed algorithms achieve substantial improvements in all of these performance metrics compared to the state-of-the-art approaches. For example, for the service ratio metric, our algorithms can serve up to 11 times more users compared to the unicast approach, and they achieve up to 54% improvement over the closest multicast approaches in the literature. © 2016 ACM.",Mobile multimedia; Single-frequency network; Wireless streaming,Bandwidth; Energy conservation; Heuristic algorithms; Media streaming; Mobile telecommunication systems; Multimedia services; Multimedia systems; Optimal systems; Optimization; Polynomials; Signal receivers; Video streaming; Wireless networks; Exponential time complexity; Mobile multimedia; Multimedia streaming services; Packet level simulation; Resource allocation problem; Single frequency networks; State-of-the-art approach; Wireless cellular networks; Complex networks
Introduction to special issue MMSys/NOSSDAV 2015,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997428981&doi=10.1145%2f3003439&partnerID=40&md5=4805892ab6ad430cfa4710f9f4b35fac,[No abstract available],,
Video management and resource allocation for a Large-Scale VoD cloud,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994533196&doi=10.1145%2f2983638&partnerID=40&md5=bba956ad0203e4ef78877e0dca7fadfa,"We consider providing large-scale Netflix-like video-on-demand (VoD) service on a cloud platform, where cloud proxy servers are placed close to user pools. Videos may have heterogeneous popularity at different geo-locations. A repository provides video backup for the network, and the proxy servers collaboratively store and stream videos. To deploy the VoD cloud, the content provider rents resources consisting of link capacities among servers, server storage, and server processing capacity to handle remote requests. We study how to minimize the deployment cost by jointly optimizing video management (in terms of video placement and retrieval at servers) and resource allocation (in terms of link, storage, and processing capacities), subject to a certain user delay requirement on video access. We first formulate the joint optimization problem and show that it is NP-hard. To address it, we propose Resource allocation And Video management Optimization (RAVO), a novel and efficient algorithm based on linear programming with proven optimality gap. For a large video pool, we propose a video clustering algorithm to substantially reduce the run-Time computational complexity without compromising performance. Using extensive simulation and trace-driven real data, we show that RAVO achieves close-To-optimal performance, outperforming other advanced schemes significantly (often by multiple times). © 2016 ACM.",Distributed video-on-demand; Linear programming; Optimization; Resource allocation; Spectral clustering,Clustering algorithms; Digital storage; Linear programming; Optimization; Resource allocation; Distributed video-on-demand; Extensive simulations; Joint optimization; Optimal performance; Processing capacities; Server processing; Spectral clustering; Video on demand services; Video on demand
Device-To-Device communication for mobile multimedia in emerging 5G networks,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994495822&doi=10.1145%2f2983641&partnerID=40&md5=d07a32d63938b2cd6a4ef976ff11a712,"Device-To-device (D2D) communication, which utilizes mobile devices located within close proximity for direct connection and data exchange, holds great promise for improving energy and spectrum efficiency of mobile multimedia in 5G networks. It has been observed that most available D2D-based works-considered only the single-cell scenario with a single BS. Such scenario-based schemes, although tractable and able to illustrate the relationship between D2D links and cellular links, failed to take into account the distribution of surrounding base stations and user equipments (UEs), as well as the accumulated interference from ongoing transmissions in other cells. Furthermore, the single-Tier network with one BS considered in available works is far from the real 5G scenario in which multi-Tier BSS are heterogeneously distributed among the whole network area. In light of such observations, we present in this article a model for network coverage probability and average rate analysis in a D2D communication overlaying a two-Tier downlink cellular network, where nineteen macro base stations (MBSS) with pico base stations (PBSS) placed at the end point of macro cell (hexagons) borders are employed according to the 3GPP specifications, and mobile users are spatially distributed according to the homogeneous Poisson Point Process model. Each mobile UE is able to establish a D2D link with adjacent UEs or connect to a nearby macro or pico base station. Stochastic geometric analysis is adopted to characterize the intratier interference distribution within the MBS-Tier, PBS-Tier, and D2D-Tier based on which network coverage probability and per-user average rate are derived with a careful consideration of important issues such as threshold value, SINR value, user density, content hit rate, spectrum allocation, and cell coverage range. Our results show that, even for the overlaying case, D2D communication can significantly improve network coverage probability and per-user average downlink rate. Another finding is that the frequency allocation for D2D communications should be carefully tuned according to network settings, which may result in totally different varying behaviors for the per-user average rate. © 2016 ACM.",5G; Device-To-device communication; Downlink; Multichannel cellular network; Overlay; Stochastic geometry,Cells; Cytology; Electronic data interchange; Frequency allocation; Mobile devices; Mobile telecommunication systems; Probability; Probability distributions; Queueing networks; Stochastic systems; Wireless networks; Cellular network; Device-to-Device communications; Downlink; Overlay; Stochastic geometry; Base stations
Digital Lollipop: Studying Electrical Stimulation on the Human Tongue to Simulate Taste Sensations,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994479879&doi=10.1145%2f2996462&partnerID=40&md5=dab04dc963f41639d68d8f43e148ac5f,"Among the five primary senses, the sense of taste is the least explored as a form of digital media applied in Human-Computer Interface. This article presents an experimental instrument, the Digital Lollipop, for digitally simulating the sensation of taste (gustation) by utilizing electrical stimulation on the human tongue. The system is capable ofmanipulating the properties of electric currents (magnitude, frequency, and polarity) to formulate different stimuli. To evaluate the effectiveness of this method, the system was experimentally tested in two studies. The first experiment was conducted using separate regions of the human tongue to record occurrences of basic taste sensations and their respective intensity levels. The results indicate occurrences of sour, salty, bitter, and sweet sensations from different regions of the tongue. One of the major discoveries of this experiment was that the sweet taste emerges via an inverse-current mechanism, which deserves further research in the future. The second study was conducted to compare natural and artificial (virtual) sour taste sensations and examine the possibility of effectively controlling the artificial sour taste at three intensity levels (mild, medium, and strong). The proposed method is attractive since it does not require any chemical solutions and facilitates further research opportunities in several directions including human-computer interaction, virtual reality, food and beverage, as well as medicine. © 2016 ACM.",Digital lollipop; Digital taste; Gustation; Taste simulation; User interfaces; Virtual reality,Digital storage; Inverse problems; User interfaces; Virtual reality; Chemical solutions; Digital lollipop; Digital tastes; Electrical stimulations; Gustation; Human computer interfaces; Research opportunities; Taste simulation; Human computer interaction
Enhanced user context-aware reputation measurement of multimedia service,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994052469&doi=10.1145%2f2978569&partnerID=40&md5=f533d0a7dd489f7da016b8aebaeb7bf5,"Reputation plays an important role for users in choosing or paying for multimedia applications or services. Some efficient multimedia reputation-measurement approaches have been proposed to achieve accurate reputation measurement based on feedback ratings that users give to a multimedia service after invoking. However, the implementation of these approaches suffers from the problems of wide abuse and low utilization of user context. In this article, we study the relationship between user context and feedback ratings according to which one user often gives different feedback ratings to the same multimedia service in different user contexts. We further propose an enhanced user context-aware reputation-measurement approach for multimedia services that is accurate in two senses: (1) Each multimedia service has three reputation values with three different user context levels when its feedback ratings are sufficient and (2) the reputation of a multimedia service with different user context levels is found using user context sensitivity and user similarity when its feedback ratings are limited or not available. Experimental results based on a real-world dataset show that our approach outperforms other approaches in terms of accuracy. © 2016 ACM.",Feedback rating; Multimedia service; Reputation; User context; User similarity,Computer networks; Hardware; Measurement-based; Multimedia applications; nocv1; Real-world; Reputation; User context; User similarity; Multimedia services
PPHOCFS: Privacy preserving high-order CFS algorithm on the cloud for clustering multimedia data,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84993949465&doi=10.1145%2f2886779&partnerID=40&md5=37a513e5da51b7b107fbaa3a25edb30b,"Clustering is a commonly used technique for multimedia data analysis and management. In this article, we propose a high-order clustering algorithm by fast search and find of density peaks (HOCFS) by extending the traditional clustering scheme by fast search and find of density peaks (CFS) algorithm from the vector space to the tensor space for multimedia data clustering. Furthermore, we propose a privacy preserving HOCFS algorithm (PPHOCFS) which improves the efficiency of the HOCFS algorithm by using the cloud computing to perform most of the clustering operations. To protect the private data in the multimedia data sets during the clustering process on the cloud, the raw data is encrypted by the Brakerski-Gentry-Vaikuntanathan (BGV) strategy before being uploaded to the cloud for performing the HOCFS clustering algorithm efficiently. In the proposed method, the client is required to only execute the encryption/decryption operations and the cloud servers are employed to perform all the computing operations. Finally, the performance of our scheme is evaluated on two representative multimedia data sets, namely NUS-WIDE and SNAE2, in terms of clustering accuracy, execution time, and speedup in the experiments. The results demonstrate that the proposed PPHOCFS scheme can save at least 40% running time compared with HOCFS, without disclosing the private data on the cloud, making our scheme securely suitable for multimedia big data clustering. © 2016 ACM.",BGV encryption; CFS clustering algorithm; Cloud computing; Multimedia data; Privacy preserving,Big data; Cloud computing; Cluster analysis; Cryptography; Data privacy; Vector spaces; Clustering accuracy; Clustering operation; Clustering process; Encryption/decryption; Multimedia data; Multimedia data analysis; Privacy preserving; Traditional clustering; Clustering algorithms
Real-Time load reduction in multimedia big data for mobile internet,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994531567&doi=10.1145%2f2990473&partnerID=40&md5=2b3943f3b174d43103510102e9d33d68,"In the age of multimedia big data, the popularity of mobile devices has been in an unprecedented growth, the speed of data increasing is faster than ever before, and Internet traffic is rapidly increasing, not only in volume but also in heterogeneity. Therefore, data processing and network overload have become two urgent problems. To address these problems, extensive papers have been published on image analysis using deep learning, but only a few works have exploited this approach for video analysis. In this article, a hybrid-stream model is proposed to solve these problems for video analysis. Functionality of this model covers Data Preprocessing, Data Classification, and Data-Load-Reduction Processing. Specifically, an improved Convolutional Neural Networks (CNN) classification algorithm is designed to evaluate the importance of each video frame and video clip to enhance classification precision. Then, a reliable keyframe extraction mechanism will recognize the importance of each frame or clip, and decide whether to abandon it automatically by a series of correlation operations. The model will reduce data load to a dynamic threshold changed by σ, control the input size of the video in mobile Internet, and thus reduce network overload. Through experimental simulations, we find that the size of processed video has been effectively reduced and the quality of experience (QoE) has not been lowered due to a suitably selected parameter η. The simulation also shows that the model has a steady performance and is powerful enough for continuously growing multimedia big data. © 2016 ACM.",Big data; Load reduction; Mobile internet; Multimedia; Networking; Real-Time,Big data; Classification (of information); Deep learning; Mobile devices; Neural networks; Problem solving; Quality of service; Load reduction; Mobile Internet; Multimedia; Networking; Real time; Data reduction
A deployment optimization scheme over multimedia big data for Large-Scale media streaming application,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994495804&doi=10.1145%2f2983642&partnerID=40&md5=d7ffecff926355ef17467cad6b8de162,"With the prosperity of media streaming applications over the Internet in the past decades, multimedia data has sharply increased (categorized as multimedia big data), which exerts more pressure on the infrastructure, such as networking of the application provider. In order to move this hurdle, an increasing number of traditional media streaming applications have migrated from a private server cluster onto the cloud. With the elastic resource provisioning and centralized management of the cloud, the operational costs of media streaming application providers can decrease dramatically. However, to the best of our knowledge, existing migration solutions do not fully take viewer information such as hardware condition into consideration. In this article, we consider the deployment optimization problem named ODP by leveraging local memories at each viewer. Considering the NP-hardness of calculating the optimal solution, we turn to propose computationally tractable algorithms. Specifically, we unfold the original problem into two interactive subproblems: coarse-grained migration subproblem and fine-grained scheduling subproblem. Then, the corresponding offline approximation algorithms with performance guarantee and computational efficiency are given. The results of extensive evaluation show that compared with the baseline algorithm without leveraging local memories at viewers, our proposed algorithms and their online versions can decrease total bandwidth reservation and enhance the utilization of bandwidth reservation dramatically. © 2016 ACM.",Cloud; Deployment optimization; Large-scale media streaming application; Local memory; Multimedia big data,Approximation algorithms; Bandwidth; Big data; Clouds; Computational efficiency; Optimization; Application providers; Bandwidth reservation; Centralized management; Deployment optimization; Local memory; Performance guarantees; Scheduling sub-problem; Streaming applications; Media streaming
Fixation prediction through multimodal analysis,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994530204&doi=10.1145%2f2996463&partnerID=40&md5=51bd7ef4c30c47a84d34770f1a663b02,"In this article, we propose to predict human eye fixation through incorporating both audio and visual cues. Traditional visual attention models generally make the utmost of stimuli's visual features, yet they bypass all audio information. In the real world, however, we not only direct our gaze according to visual saliency, but also are attracted by salient audio cues. Psychological experiments show that audio has an influence on visual attention, and subjects tend to be attracted by the sound sources. Therefore, we propose fusing both audio and visual information to predict eye fixation. In our proposed framework, we first localize the moving-soundgenerating objects through multimodal analysis and generate an audio attention map. Then, we calculate the spatial and temporal attention maps using the visual modality. Finally, the audio, spatial, and temporal attention maps are fused to generate the final audiovisual saliency map. The proposed method is applicable to scenes containing moving-sound-generating objects. We gather a set of video sequences and collect eyetracking data under an audiovisual test condition. Experiment results show that we can achieve better eye fixation prediction performance when taking both audio and visual cues into consideration, especially in some typical scenes in which object motion and audio are highly correlated. © 2016 ACM.",Attention fusion; Audiovisual attention; Eye fixation prediction; Multimodal analysis; Saliency,Audio acoustics; Behavioral research; Forecasting; Speech recognition; Audio and visual information; Audiovisual attention; Eye fixations; Multimodal analysis; Prediction performance; Psychological experiment; Saliency; Visual attention model; Modal analysis
Approximate asymmetric search for binary embedding codes,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994509799&doi=10.1145%2f2990504&partnerID=40&md5=193be32c913bcf14eff73bd127e6ca45,"In this article, we propose a method of approximate asymmetric nearest-neighbor search for binary embedding codes. The asymmetric distance takes advantage of less information loss at the query side. However, calculating asymmetric distances through exhaustive search is prohibitive in a large-scale dataset. We present a novel method, called multi-index voting, that integrates the multi-index hashing technique with a voting mechanism to select appropriate candidates and calculate their asymmetric distances. We show that the candidate selection scheme can be formulated as the tail of the binomial distribution function. In addition, a binary feature selection method based on minimal quantization error is proposed to address the memory insufficiency issue and improve the search accuracy. Substantial experimental evaluations were made to demonstrate that the proposed method can yield an approximate accuracy to the exhaustive search method while significantly accelerating the runtime. For example, one result shows that in a dataset of one billion 256-bit binary codes, examining only 0.5% of the dataset, can reach 95-99% close accuracy to the exhaustive search method and accelerate the search by 73-128 times. It also demonstrates an excellent tradeoff between the search accuracy and time efficiency compared to the state-of-The-Art nearest-neighbor search methods. Moreover, the proposed feature selection method shows its effectiveness and improves the accuracy up to 8.35% compared with other feature selection methods. © 2016 ACM.",Asymmetric distance; Multi-index hashing and voting; Nearest neighbor search,Codes (symbols); Distribution functions; Feature extraction; Asymmetric distances; Binomial distribution; Candidate selection; Experimental evaluation; Feature selection methods; Large-scale dataset; Multi-index hashing; Quantization errors; Nearest neighbor search
QoE-based low-delay live streaming using throughput predictions,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994506911&doi=10.1145%2f2990505&partnerID=40&md5=e7fe1660ad171d7ba4f6259f62deabfe,"Recently,Hypertext Transfer Protocol (HTTP)-based adaptive streaming has become the de facto standard for video streaming over the Internet. It allows clients to dynamically adaptmedia characteristics to the varying network conditions to ensure a high quality of experience (QoE)-That is, minimize playback interruptions while maximizing video quality at a reasonable level of quality changes. In the case of live streaming, this task becomes particularly challenging due to the latency constraints. The challenge further increases if a client uses a wireless access network, where the throughput is subject to considerable fluctuations. Consequently, live streams often exhibit latencies of up to 20 to 30 seconds. In the present work, we introduce an adaptation algorithm for HTTP-based live streaming called LOLYPOP (short for low-latency predictionbased adaptation), which is designed to operate with a transport latency of a few seconds. To reach this goal, LOLYPOP leverages Transmission Control Protocol throughput predictions on multiple time scales, from 1 to 10 seconds, along with estimations of the relative prediction error distributions. In addition to satisfying the latency constraint, the algorithm heuristically maximizes the QoE by maximizing the average video quality as a function of the number of skipped segments and quality transitions. To select an efficient prediction method, we studied the performance of several time series prediction methods in IEEE 802.11 wireless access networks.We evaluated LOLYPOP under a large set of experimental conditions, limiting the transport latency to 3 seconds, against a state-of-The-Art adaptation algorithm called FESTIVE. We observed that the average selected video representation index is by up to a factor of 3 higher than with the baseline approach. We also observed that LOLYPOP is able to reach points from a broader region in the QoE space, and thus it is better adjustable to the user profile or service provider requirements. © 2016 ACM.",Adaptive streaming; MPEG-Dash; TCP throughput prediction,Communication channels (information theory); Forecasting; HTTP; Hypertext systems; Motion Picture Experts Group standards; Quality of service; Standards; Throughput; Transmission control protocol; Adaptation algorithms; Adaptive streaming; Experimental conditions; Mpeg dashes; TCP throughput prediction; Time series prediction; Video representations; Wireless access networks; Video streaming
Introduction to special issue on multimedia big data: Networking,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994494241&doi=10.1145%2f2989214&partnerID=40&md5=0a39b9cd5d0adc8c658ba9ddb9945d70,[No abstract available],,
Performance modelling and analysis of software-defined networking under bursty multimedia traffic,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994504443&doi=10.1145%2f2983637&partnerID=40&md5=87b8fd606f62b5c61f639f920f340098,"Software-Defined Networking (SDN) is an emerging architecture for the next-generation Internet, providing unprecedented network programmability to handle the explosive growth of big data driven by the popularisation of smart mobile devices and the pervasiveness of content-rich multimedia applications. In order to quantitatively investigate the performance characteristics of SDN networks, several research efforts from both simulation experiments and analytical modelling have been reported in the current literature. Among those studies, analytical modelling has demonstrated its superiority in terms of cost-effectiveness in the evaluation of large-scale networks. However, for analytical tractability and simplification, existing analytical models are derived based on the unrealistic assumptions that the network traffic follows the Poisson process, which is suitable to model nonbursty text data, and the data plane of SDN is modelled by one simplified Single-Server Single-Queue (SSSQ) system. Recent measurement studies have shown that, due to the features of heavy volume and high velocity, the multimedia big data generated by real-world multimedia applications reveals the bursty and correlated nature in the network transmission. With the aim of capturing such features of realistic traffic patterns and obtaining a comprehensive and deeper understanding of the performance behaviour of SDN networks, this article presents a new analytical model to investigate the performance of SDN in the presence of the bursty and correlated arrivals modelled by the Markov Modulated Poisson Process (MMPP). The Quality-of-Service performance metrics in terms of the average latency and average network throughput of the SDN networks are derived based on the developed analytical model. To consider a realistic multiqueue system of forwarding elements, a Priority-Queue (PQ) system is adopted to model the SDN data plane. To address the challenging problem of obtaining the key performance metrics, for example, queue-length distribution of a PQ system with a given service capacity, a versatile methodology extending the Empty Buffer Approximation (EBA) method is proposed to facilitate the decomposition of such a PQ system to two SSSQ systems. The validity of the proposed model is demonstrated through extensive simulation experiments. To illustrate its application, the developed model is then utilised to study the strategy of the network configuration and resource allocation in SDN networks. © 2016 ACM.",Multimedia big data; Performance modelling and analysis; Queueing decomposition; Resource allocation; Software-defined networking,Application programs; Big data; Cost effectiveness; Mobile devices; Multimedia systems; Poisson distribution; Quality of service; Queueing theory; Resource allocation; Software defined networking; Telecommunication traffic; Analytical tractability; Markov modulated Poisson process; Next generation Internet; Performance characteristics; Performance modelling; Quality of service performance; Queue length distribution; Software defined networking (SDN); Analytical models
Toward Delay-Efficient Game-Aware data centers for cloud gaming,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994520403&doi=10.1145%2f2983639&partnerID=40&md5=081852a716851c99b65b744a84299210,"Gaming on demand is an emerging service that has recently started to garner prominence in the gaming industry. Cloud-based video games provide affordable, flexible, and high-performance solutions for end-users with constrained computing resources and enables them to play high-end graphic games on low-end thin clients. Despite its advantages, cloud gaming's Quality of Experience (QoE) suffers from high and varying end-To-end delay. Since the significant part of computational processing, including game rendering and video compression, is performed in data centers, controlling the transfer of information within the cloud has an important impact on the quality of cloud gaming services. In this article, a novel method for minimizing the end-To-end latency within a cloud gaming data center is proposed.We formulate an optimization problem for reducing delay, and propose a Lagrangian Relaxation (LR) time-efficient heuristic algorithm as a practical solution. Simulation results indicate that the heuristic method can provide close-To-optimal solutions. Also, the proposed model reduces end-To-end delay and delay variation by almost 11% and 13.5%, respectively, and outperforms the existing server-centric and network-centric models. As a byproduct, our proposed method also achieves better fairness among multiple competing players by almost 45%, on average, in comparison with existing methods. © 2016 ACM.",Cloud gaming; Software defined network (SDN),Computer games; Heuristic algorithms; Human computer interaction; Optimization; Quality of service; Video signal processing; Cloud gamings; Computational processing; End to end latencies; Lagrangian relaxations; Network centric models; Optimization problems; Quality of experience (QoE); Transfer of information; Heuristic methods
Evaluating the privacy risk of user-shared images,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989332671&doi=10.1145%2f2978568&partnerID=40&md5=b67f15f5f9270acaf945c8133e996793,"User-shared images are shared on social media about a user's life and interests that are widely accessible to others due to their sharing nature. Unlike for online profiles and social graphs, most users are unaware of the privacy risks relating to shared images, as they do not directly disclose characteristics such as gender and origin. Recently, however, user-shared images have been proven to be an accessible alternative to social graphs for online friendship recommendation and gender identification. This article evaluates 1.6M usershared images from an image-oriented social network, Fotolog, and concludes how they can create privacy risks by proposing a system for de-anonymization, as well as inferring information on online profiles with the user-shared images. It is concluded that given user-shared images, using social graphs is 2 and 2.5 times more effective in de-anonymization than using origins or genders. With two showcases, it is also proven that using user-shared images is effective in online friendship recommendation, gender identification, and origin inference. To the best of our knowledge, this is the first article to evaluate the privacy issue qualitatively with big multimedia data from a real social network. © 2016 ACM.",Big data; De-anonymization; Privacy; Social network analysis; User-shared images,Big data; Data privacy; Image processing; Anonymization; Gender identification; Multimedia data; Privacy issue; Privacy risks; Social graphs; Social media; User-shared images; Social networking (online)
Robust hashing based on quaternion zernike moments for image authentication,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989311682&doi=10.1145%2f2978572&partnerID=40&md5=529f1db07f74bc1f9ca7cab45d52c2e4,"The reliability and security of multimedia contents in transmission, communications, storage, and usage have attracted special attention. Robust image hashing, also referred to as perceptual image hashing, is widely applied in multimedia authentication and forensics, image retrieval, image indexing, and digital image watermarking. In this work, a novel robust image hashing method based on quaternion Zernike moments (QZMs) is proposed. QZMs offer a sound way to jointly deal with the three channels of color images without discarding chrominance information; the generated hash is thus shorter than the hash of three channels separately processing. The proposed approach's performance was evaluated on the color images database of UCID and compared with several recent and efficient methods. These experiments show that the proposed scheme provides a short hash in length that is robust to most common image content-preserving manipulations like JPEG compression, filtering, noise, scaling, and large angle rotation operations. © 2016 ACM.",Image forensics; Multimedia authentication; Quaternion Zernike moments; Robust image hashing,Authentication; Color; Color image processing; Digital watermarking; Image retrieval; Image watermarking; Digital image watermarking; Image authentication; Image forensics; Image hashing; Multimedia authentication; Multimedia contents; Perceptual image hashing; Zernike moments; Feature extraction
Image encryption based on compressive sensing and scrambled index for secure multimedia transmission,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989333156&doi=10.1145%2f2903717&partnerID=40&md5=3457dffacb3eac86d1e808c3642e8c1e,"With the rapid growth of multimedia message exchange and digital communication, multimedia big data has become a research hotspot in various fields. The storage and transmission of multimedia big data have high requirements for security. Images, covering the highest proportion of multimedia data, should be processed and transmitted with high security. Compressive sensing (CS) has a beneficial property for the encryption that the image can be recovered with fewer samples than conventional approaches use. In recent years, CS has been studied not only to reduce the resource requirements for signal acquisition but also to ensure the security of data. It is still an open challenge to improve security and enhance the quality of the decrypted image simultaneously using the key with small size. In this article, a CS-based encryption method is presented that associates the quantization with random measurement permutation. An enormous number of experiments have been conducted on both standard test images and face images chosen from the big database LFW. Experimental results show that our proposal has dramatic improvements on ensuring the security, enhancing the quality of the decrypted image, and raising the efficiency. Additionally, this proposal remarkably reduces storage and transmission resources. Accordingly, this encryption scheme can be applied to ensure the security of multimedia transmission. © 2016 ACM.",Compressive sensing; Image encryption; Multimedia security; Random permutation; Split Bregman algorithm,Big data; Cryptography; Digital communication systems; Digital storage; Image processing; Security of data; Signal processing; Signal reconstruction; Compressive sensing; Image encryptions; Multimedia security; Random permutations; Split bregman algorithms; Compressed sensing
Privacy-preserving multimedia big data aggregation in large-scale wireless sensor networks,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989291132&doi=10.1145%2f2978570&partnerID=40&md5=d808e64159b897040ea2d03f2cf73bab,"To preserve the privacy of multimedia big data and achieve the efficient data aggregation in wireless multimedia sensor networks (WMSNs), a distributed compressed sensing'based privacy-preserving data aggregation (DCSPDA) approach is proposed in this article. First, in this approach, the original multimedia sensor data are compressed and measured by distributed compressed sensing (DCS) and the compressed data measurements are uploaded to the sink, by which the inherent characteristics between sensor data can be obtained. Second, the original multimedia data are jointly recovered and the common and innovation sparse components are obtained through solving the optimization problem and linear equations at the sink. Third, through least squares support vector machine (LSSVM) learning of the sparse components, the sparse position configuration can be determined and disseminated for each node to conduct the privacypreserving data configuration. After receiving the configuration message, original multimedia sensor data are accordingly customized, compressed, and measured by the common measurement matrix, aggregated at the cluster heads, and transmitted to the sink. Finally, the aggregated multimedia sensor data are recovered by the sink according to the data configuration to achieve the privacy-preserving data aggregation and transmission. Our comparative simulation results validate the efficiency and scalability of DCSPDA and demonstrate that the proposed approach can effectively reduce the communication overheads and provide reliable privacy-preserving with low computational complexity for WMSNs. © 2016 ACM.",Data aggregation; Distributed compressed sensing; Privacy-preserving method; Wireless multimedia sensor networks,Big data; Complex networks; Compressed sensing; Machine components; Optimization; Signal reconstruction; Wireless sensor networks; Data aggregation; Distributed Compressed Sensing; Large-scale wireless sensor networks; Least squares support vector machines; Low computational complexity; Privacy preserving; Wireless multimedia sensor network; Wireless multimedia sensor network (WMSNS); Data privacy
SecSIFT: Secure image SIFT feature extraction in cloud computing,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989313033&doi=10.1145%2f2978574&partnerID=40&md5=74b3731621fe64200279fca762eb2ef9,"The image and multimedia data produced by individuals and enterprises is increasing every day. Motivated by the advances in cloud computing, there is a growing need to outsource such computational intensive image feature detection tasks to cloud for its economic computing resources and on-demand ubiquitous access. However, the concerns over the effective protection of private image and multimedia data when outsourcing it to cloud platform become the major barrier that impedes the further implementation of cloud computing techniques over massive amount of image and multimedia data. To address this fundamental challenge, we study the state-of-the-art image feature detection algorithms and focus on Scalar Invariant Feature Transform (SIFT), which is one of the most important local feature detection algorithms and has been broadly employed in different areas, including object recognition, image matching, robotic mapping, and so on. We analyze and model the privacy requirements in outsourcing SIFT computation and propose Secure Scalar Invariant Feature Transform (SecSIFT), a high-performance privacy-preserving SIFT feature detection system. In contrast to previous works, the proposed design is not restricted by the efficiency limitations of current homomorphic encryption scheme. In our design, we decompose and distribute the computation procedures of the original SIFT algorithm to a set of independent, co-operative cloud servers and keep the outsourced computation procedures as simple as possible to avoid utilizing a computationally expensive homomorphic encryption scheme. The proposed SecSIFT enables implementation with practical computation and communication complexity. Extensive experimental results demonstrate that SecSIFT performs comparably to original SIFT on image benchmarks while capable of preserving the privacy in an efficient way. © 2016 ACM.",Cloud computing; Image feature detection; Privacy-preserving; SIFT,Cloud computing; Cryptography; Data privacy; Image matching; Object recognition; Outsourcing; Signal detection; Ubiquitous computing; Communication complexity; Efficiency limitations; Homomorphic Encryption Schemes; Image features; Local feature detections; Privacy preserving; Privacy requirements; SIFT; Feature extraction
Trustworthy authentication on scalable surveillance video with background model support,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989311409&doi=10.1145%2f2978573&partnerID=40&md5=7fb046308f35a8d9b161015ac694f772,"H.264/SVC (Scalable Video Coding) codestreams, which consist of a single base layer and multiple enhancement layers, are designed for quality, spatial, and temporal scalabilities. They can be transmitted over networks of different bandwidths and seamlessly accessed by various terminal devices. With a huge amount of video surveillance and various devices becoming an integral part of the security infrastructure, the industry is currently starting to use the SVC standard to process digital video for surveillance applications such that clients with different network bandwidth connections and display capabilities can seamlessly access various SVC surveillance (sub)codestreams. In order to guarantee the trustworthiness and integrity of received SVC codestreams, engineers and researchers have proposed several authentication schemes to protect video data. However, existing algorithms cannot simultaneously satisfy both efficiency and robustness for SVC surveillance codestreams. Hence, in this article, a highly efficient and robust authentication scheme, named TrustSSV (Trust Scalable Surveillance Video), is proposed. Based on quality/spatial scalable characteristics of SVC codestreams, TrustSSV combines cryptographic and content-based authentication techniques to authenticate the base layer and enhancement layers, respectively. Based on temporal scalable characteristics of surveillance codestreams, TrustSSV extracts, updates, and authenticates foreground features for each access unit dynamically with background model support. Using SVC test sequences, our experimental results indicate that the scheme is able to distinguish between content-preserving and content-changing manipulations and to pinpoint tampered locations. Compared with existing schemes, the proposed scheme incurs very small computation and communication costs. © 2016 ACM.",Authentication; Background model; H.264/SVC; Integrity; Surveillance application,Authentication; Bandwidth; Computer graphics; Digital devices; Display devices; Image coding; Image segmentation; Monitoring; Multimedia systems; Network security; Scalable video coding; Static Var compensators; Video signal processing; Authentication scheme; Background model; Content-based authentication; H.264/SVC; Integrity; Security infrastructure; Surveillance applications; Temporal scalability; Security systems
Editorial: Trust management for multimedia big data,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989284368&doi=10.1145%2f2978431&partnerID=40&md5=b06c153ba5afc7d564926c75bd516608,[No abstract available],,
Intercrossed access controls for secure financial services on multimedia big data in cloud systems,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989338233&doi=10.1145%2f2978575&partnerID=40&md5=d3810301ceb5d316d89b09e86774f58e,"The dramatically growing demand of Cyber Physical and Social Computing (CPSC) has enabled a variety of novel channels to reach services in the financial industry. Combining cloud systems with multimedia big data is a novel approach for Financial Service Institutions (FSIs) to diversify service offerings in an efficient manner. However, the security issue is still a great issue in which the service availability often conflicts with the security constraints when the service media channels are varied. This paper focuses on this problem and proposes a novel approach using the Semantic-Based Access Control (SBAC) techniques for acquiring secure financial services on multimedia big data in cloud computing. The proposed approach is entitled IntercroSsed Secure Big Multimedia Model (2SBM), which is designed to secure accesses between various media through the multiple cloud platforms. The main algorithms supporting the proposed model include the Ontology-Based Access Recognition (OBAR) Algorithm and the Semantic Information Matching (SIM) Algorithm.We implement an experimental evaluation to prove the correctness and adoptability of our proposed scheme. © 2016 ACM.",Cloud computing; Intercrossed access control; Multimedia big data; Secure financial services; Trustmanagement,Access control; Cloud computing; Finance; Multimedia services; Semantics; Experimental evaluation; Financial industry; Financial service; Multimedia modeling; Security constraint; Semantic information; Service availability; TrustManagement; Big data
Secure social multimedia big data sharing using scalable JFE in the TSHWT Domain,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989311358&doi=10.1145%2f2978571&partnerID=40&md5=1c30ed81e7192082131e8124afcb6aba,"With the advent of social networks and cloud computing, the amount of multimedia data produced and communicated within social networks is rapidly increasing. In the meantime, social networking platforms based on cloud computing have made multimedia big data sharing in social networks easier and more efficient. The growth of social multimedia, as demonstrated by social networking sites such as Facebook and YouTube, combined with advances in multimedia content analysis, underscores potential risks for malicious use, such as illegal copying, piracy, plagiarism, and misappropriation. Therefore, secure multimedia sharing and traitor tracing issues have become critical and urgent in social networks. In this article, a joint fingerprinting and encryption (JFE) scheme based on tree-structured Haar wavelet transform (TSHWT) is proposed with the purpose of protecting media distribution in social network environments. The motivation is to map hierarchical community structure of social networks into a tree structure of Haar wavelet transform for fingerprinting and encryption. First, fingerprint code is produced using social network analysis (SNA). Second, the content is decomposed based on the structure of fingerprint code by the TSHWT. Then, the content is fingerprinted and encrypted in the TSHWT domain. Finally, the encrypted contents are delivered to users via hybrid multicast-unicast. The proposed method, to the best of our knowledge, is the first scalable JFE method for fingerprinting and encryption in the TSHWT domain using SNA. The use of fingerprinting along with encryption using SNA not only provides a double layer of protection for social multimedia sharing in social network environment but also avoids big data superposition effect. Theory analysis and experimental results show the effectiveness of the proposed JFE scheme. © 2016 ACM.",Collusion attack; Joint fingerprinting and encryption; Social multimedia big data sharing; Social network; Tree-structured Haar wavelet transform (TSHWT),Cloud computing; Computation theory; Crime; Cryptography; Distributed computer systems; Forestry; Network security; Risk assessment; Social networking (online); Social sciences computing; Trees (mathematics); Wavelet transforms; Websites; Collusion attack; Data Sharing; Haar wavelet transform; Hierarchical community structures; Multimedia content analysis; Network environments; Social networking sites; Superposition effect; Big data
A unified video recommendation by cross-network user modeling,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983084277&doi=10.1145%2f2957755&partnerID=40&md5=76aeaba626597c4618d88d8dae63bc22,"Online video sharing sites are increasingly encouraging their users to connect to the social network venues such as Facebook and Twitter, with goals to boost user interaction and better disseminate the high-quality video content. This in turn provides huge possibilities to conduct cross-network collaboration for personalized video recommendation. However, very few efforts have been devoted to leveraging users' social media profiles in the auxiliary network to capture and personalize their video preferences, so as to recommend videos of interest. In this article, we propose a unified YouTube video recommendation solution by transferring and integrating users' rich social and content information in Twitter network. While general recommender systems often suffer from typical problems like cold-start and data sparsity, our proposed recommendation solution is able to effectively learn from users' abundant auxiliary information on Twitter for enhanced user modeling and well address the typical problems in a unified framework. In this framework, two stages are mainly involved: (1) auxiliary-network data transfer, where user preferences are transferred from an auxiliary network by learning cross-network knowledge associations; and (2) cross-network data integration, where transferred user preferences are integrated with the observed behaviors on a target network in an adaptive fashion. Experimental results show that the proposed cross-network collaborative solution achieves superior performance not only in terms of accuracy, but also in improving the diversity and novelty of the recommended videos. © 2016 ACM.",Algorithms; Cross-network collaboration; Experimentation; H.3.3 [information storage and retrieval]: information search and retrieval; H.4.m [information systems]: miscellaneous; Performance; Personalized video recommendation; User modeling,Algorithms; Data integration; Data transfer; Digital storage; Distributed computer systems; Social networking (online); User interfaces; Websites; Cross networks; Experimentation; H.3.3 [information storage and retrieval]: information search and retrievals; H.4.m [information systems]: miscellaneous; Performance; Personalized video; User Modeling; Search engines
Audio masking effect on inter-component skews in olfaction-enhanced multimedia presentations,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983035993&doi=10.1145%2f2957753&partnerID=40&md5=21fb53c05264b07a3b32047c9c9d49a4,"Media-rich content plays a vital role in consumer applications today, as these applications try to find new and interesting ways to engage their users. Video, audio, and the more traditional forms of media content continue to dominate with respect to the use of media content to enhance the user experience. Tactile interactivity has also now become widely popular in modern computing applications, while our olfactory and gustatory senses continue to have a limited role. However, in recent times, there have been significant advancements regarding the use of olfactory media content (i.e., smell), and there are a variety of devices now available to enable its computer-controlled emission. This paper explores the impact of the audio stream on user perception of olfactory-enhanced video content in the presence of skews between the olfactory and video media. This research uses the results from two experimental studies of user-perceived quality of olfactory-enhanced multimedia, where audio was present and absent, respectively. Specifically, the paper shows that the user Quality of Experience (QoE) is generally higher in the absence of audio for nearly perfect synchronized olfactory-enhanced multimedia presentations (i.e., an olfactory media skew of between {-10,+10s}); however, for greater olfactory media skews (ranging between {-30s;-10s} and {+10s, +30s}) user QoE is higher when the audio stream is present. It can be concluded that the presence of the audio has the ability to mask larger synchronization skews between the other media components in olfaction-enhanced multimedia presentations. © 2016 ACM.","Audio; Design; Experimentation; H.1.2 [user/machine systems]: human factors; H.5.1 [multimedia information systems]: artificial, augmented, and virtual realities; H.5.2 [information interfaces and presentation]: user interfaces - evaluation/methodology; Human factors; Masking effect; Olfaction; Quality of experience; Synchronization","Design; Human engineering; Quality control; Quality of service; Synchronization; User interfaces; Virtual reality; Audio; Experimentation; H.5.1 [multimedia information systems]: artificial , augmented , and virtual realities; H.5.2 [Information Interfaces and Presentation]: User Interfaces - Evaluation/methodology; Olfaction; Quality of experience (QoE); User/machine systems; Audio systems"
Semantic feature mining for video event understanding,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983078670&doi=10.1145%2f2962719&partnerID=40&md5=02b58606ae2bfb4274ea70fac550848d,"Content-based video understanding is extremely difficult due to the semantic gap between low-level vision signals and the various semantic concepts (object, action, and scene) in videos. Though feature extraction from videos has achieved significant progress, most of the previous methods rely only on low-level features, such as the appearance and motion features. Recently, visual-feature extraction has been improved significantly with machine-learning algorithms, especially deep learning. However, there is still not enough work focusing on extracting semantic features from videos directly. The goal of this article is to adopt unlabeled videos with the help of text descriptions to learn an embedding function, which can be used to extract more effective semantic features from videos when only a few labeled samples are available for video recognition. To achieve this goal, we propose a novel embedding convolutional neural network (ECNN). We evaluate our algorithm by comparing its performance on three challenging benchmarks with several popular state-of-the-art methods. Extensive experimental results show that the proposed ECNN consistently and significantly outperforms the existing methods. © 2016 ACM.",Event; Video recognition,Artificial intelligence; Benchmarking; Character recognition; Extraction; Feature extraction; Learning algorithms; Learning systems; Neural networks; Convolutional neural network; Event; Low-level features; Semantic features; State-of-the-art methods; Video recognition; Video understanding; Visual feature extraction; Semantics
Applying seamful design in location-based mobile museum applications,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984846115&doi=10.1145%2f2962720&partnerID=40&md5=bf1687d9c37d498588ea7a9ca2fd1ff0,"The application of mobile computing is currently altering patterns of our behavior to a greater degree than perhaps any other invention. In combination with the introduction of power-efficient wireless communication technologies, such as Bluetooth Low Energy (BLE), designers are today increasingly empowered to shape the way we interact with our physical surroundings and thus build entirely new experiences. However, our evaluations of BLE and its abilities to facilitate mobile location-based experiences in public environments revealed a number of potential problems. Most notably, the position and orientation of the user in combination with various environmental factors, such as crowds of people traversing the space, were found to cause major fluctuations of the received BLE signal strength. These issues are rendering a seamless functioning of any location-based application practically impossible. Instead of achieving seamlessness by eliminating these technical issues, we thus choose to advocate the use of a seamful approach, that is, to reveal and exploit these problems and turn them into a part of the actual experience. In order to demonstrate the viability of this approach, we designed, implemented, and evaluated the Ghost Detector-an educational location-based museum game for children. By presenting a qualitative evaluation of this game and by motivating our design decisions, this article provides insight into some of the challenges and possible solutions connected to the process of developing location-based BLE-enabled experiences for public cultural spaces. © 2016 ACM.",Game design; Human-computer interaction; Location based gaming; Mobile; Mobile games; Multimodal interaction; Museum studies; Seamful design; Smartphone games,Human computer interaction; Location; Museums; Wireless telecommunication systems; Game design; Location based; Mobile; Mobile games; Multi-Modal Interactions; Museum studies; Seamful designs; Computer games
Progressive Visual Cryptography with unexpanded meaningful shares,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984797281&doi=10.1145%2f2935618&partnerID=40&md5=3defb030a679b8d0dd4de98b51879c6f,"The traditional k-out-of-n Visual Cryptography (VC) scheme is the conception of ""all or nothing"" for n participants to share a secret image. The original secret image can be visually revealed only when a subset of k or more shares are superimposed together, but if the number of stacked shares are less than k, nothing will be revealed. On the other hand, a Progressive Visual Cryptography (PVC) scheme differs from the traditional VC with respect to decoding. In PVC, clarity and contrast of the decoded secret image will be increased progressively with the number of stacked shares. Much of the existing state-of-the-art research on PVC has problems with pixel expansion and random pattern of the shares. In this article, a novel scheme of progressive visual cryptography with four or more number of unexpanded as well as meaningful shares has been proposed. For this, a novel and efficient Candidate Block Replacement preprocessing approach and a basis matrix creation algorithm have also been introduced. The proposed method also eliminates many unnecessary encryption constraints like a predefined codebook for encoding and decoding the secret image, restriction on the number of participants, and so on. From the experiments, it is observed that the reconstruction probability of black pixels in the decoded image corresponding to the black pixel in the secret image is always 1, whereas that of white pixels is 0.5 irrespective of the meaningful contents visible in the shares, thus ensuring the value of contrast to alwasys be 50%. Therefore, a reconstructed image can be easily identified by a human visual system without any computation. © 2016 ACM.",Basis matrix; Meaningful shares; Progressive Visual Cryptography; Secret sharing; Unexpanded shares; Visual Cryptography,Chromium compounds; Decoding; Image reconstruction; Matrix algebra; Pixels; Basis matrix; Meaningful shares; Secret sharing; Unexpanded shares; Visual cryptography; Cryptography
Field effect deep networks for image recognition with incomplete data,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983048011&doi=10.1145%2f2957754&partnerID=40&md5=c99c0288f34684cc981282322a6e7c74,"Image recognition with incomplete data is a well-known hard problem in computer vision and machine learning. This article proposes a novel deep learning technique called Field Effect Bilinear Deep Networks (FEBDN) for this problem. To address the difficulties of recognizing incomplete data, we design a novel second-order deep architecture with the Field Effect Restricted Boltzmann Machine, which models the reliability of the delivered information according to the availability of the features. Based on this new architecture, we propose a new three-stage learning procedure with field effect bilinear initialization, field effect abstraction and estimation, and global fine-tuning with missing features adjustment. By integrating the reliability of features into the new learning procedure, the proposed FEBDN can jointly determine the classification boundary and estimate the missing features. FEBDN has demonstrated impressive performance on recognition and estimation tasks in various standard datasets. © 2016 ACM.",Deep learning; Image recognition; Incomplete data; Missing features,Computer vision; Image recognition; Network architecture; Classification boundary; Deep architectures; Deep networks; Incomplete data; Learning procedures; Learning techniques; Missing features; Restricted boltzmann machine; Deep learning
When game becomes life: The creators and spectators of online game replays and live streaming,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983087107&doi=10.1145%2f2957750&partnerID=40&md5=d18a56cf5e7ab3a6460f13185a007eb0,"Online gaming franchises such as World of Tanks, Defense of the Ancients, and StarCraft have attracted hundreds of millions of users who, apart from playing the game, also socialize with each other through gaming and viewing gamecasts. As a form of User Generated Content (UGC), gamecasts play an important role in user entertainment and gamer education. They deserve the attention of both industrial partners and the academic communities, corresponding to the large amount of revenue involved and the interesting research problems associated with UGC sites and social networks. Although previous work has put much effort into analyzing general UGC sites such as YouTube, relatively little is known about the gamecast sharing sites. In this work, we provide the first comprehensive study of gamecast sharing sites, including commercial streaming-based sites such as Amazon's Twitch.tv and community-maintained replay-based sites such as WoTreplays. We collect and share a novel dataset on WoTreplays that includes more than 380,000 game replays, shared by more than 60,000 creators with more than 1.9 million gamers. Together with an earlier published dataset on Twitch.tv, we investigate basic characteristics of gamecast sharing sites, and we analyze the activities of their creators and spectators. Among our results, we find that (i) WoTreplays and Twitch.tv are both fast-consumed repositories, with millions of gamecasts being uploaded, viewed, and soon forgotten; (ii) both the gamecasts and the creators exhibit highly skewed popularity, with a significant heavy tail phenomenon; and (iii) the upload and download preferences of creators and spectators are different: while the creators emphasize their individual skills, the spectators appreciate team-wise tactics. Our findings provide important knowledge for infrastructure and service improvement, for example, in the design of proper resource allocation mechanisms that consider future gamecasting and in the tuning of incentive policies that further help player retention. © 2016 ACM.",Gamecast sharing sites; Online game communities; Popularity dynamics; Repository characteristics; User behaviors,Behavioral research; Economics; Industrial research; Social networking (online); Video streaming; Allocation mechanism; Basic characteristics; Gamecast sharing sites; Industrial partners; On-line games; Repository characteristics; User behaviors; User generated content (UGC); Internet
Depth-based view-invariant blind 3D image watermarking,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983028459&doi=10.1145%2f2957751&partnerID=40&md5=7f8ad2929ef10d7d653cc08fdda8f180,"With the huge advance in Internet technology as well as the availability of low-cost 3D display devices, 3D image transmission has become popular in recent times. Since watermarking has become regarded as a potential Digital Rights Management (DRM) tools in the past decade, 3D image watermarking is an emerging research topic. With the introduction of the Depth Image-Based Rendering (DIBR) technique, 3D image watermarking is a more challenging task, especially for synthetic view generation. In this article, synthetic view generation is regarded as a potential attack, and a blind watermarking scheme is proposed that can resist it. In the proposed scheme, the watermark is embedded into the low-pass filtered dependent view region of 3D images. Block Discrete Cosine Transformation (DCT) is used for spatial-filtration of the dependent view region to find the DC coefficient with horizontally shifted coherent regions from the left and right view to make the scheme robust against synthesis view attack. A comprehensive set of experiments have been carried out to justify the robustness of the proposed scheme over related existing schemes with respect to Stereo JPEG compression and different noise addition attacks. © 2016 ACM.",3D; Dependent view region; Depth; DIBR; Image; Synthesis view; Watermarking,Copyrights; Digital watermarking; Discrete cosine transforms; Image coding; Image understanding; Image watermarking; Low pass filters; Stereo image processing; Three dimensional computer graphics; Three dimensional displays; Watermarking; Dependent view region; Depth; DIBR; Image; Synthesis views; Image compression
Covert voice over internet protocol communications with packet loss based on fractal interpolation,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984996544&doi=10.1145%2f2961053&partnerID=40&md5=dc0f4e99d3ce95920944cf84f5f8e8cd,"The last few years have witnessed an explosive growth in the research of information hiding in multimedia objects, but few studies have taken into account packet loss in multimedia networks. As one of the most popular real-time services in the Internet, Voice over Internet Protocol (VoIP) contributes to a large part of network traffic for its advantages of real time, high flow, and low cost. So packet loss is inevitable in multimedia networks and affects the performance of VoIP communications. In this study, a fractal-based VoIP steganographic approach was proposed to realize covert VoIP communications in the presence of packet loss. In the proposed scheme, secret data to be hidden were divided into blocks after being encrypted with the block cipher, and each block of the secret data was then embedded into VoIP streaming packets. The VoIP packets went through a packet-loss system based on Gilbert model which simulates a real network situation. And a prediction model based on fractal interpolationwas built to decide whether a VoIP packetwas suitable for data hiding. The experimental results indicated that the speech quality degradation increased with the escalating packet-loss level. The average variance of speech quality metrics (PESQ score) between the ""no-embedding"" speech samples and the ""with-embedding"" stego-speech samples was about 0.717, and the variances narrowed with the increasing packet-loss level. Both the average PESQ scores and the SNR values of stego-speech samples and the data-retrieving rates had almost the same varying trends when the packet-loss level increased, indicating that the success rate of the fractal prediction model played an important role in the performance of covert VoIP communications. © 2016 ACM.",Covert communications; Fractal interpolation; Packet loss; Steganography; VoIP,Cryptography; Fractals; Information services; Internet; Internet protocols; Internet telephony; Interpolation; Packet loss; Speech; Steganography; Covert communications; Fractal interpolation; Information hiding; Multimedia networks; Multimedia object; Real time service; Voice over Internet protocol; VoIP; Voice/data communication systems
MobiCoop: An incentive-based cooperation solution for mobile applications,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984993674&doi=10.1145%2f2957752&partnerID=40&md5=71bb3f0a86348ea190cfb7a8242ee32f,"Network architectures based on mobile devices and wireless communications present several constraints (e.g., processor, energy storage, bandwidth, etc.) that affect the overall network performance. Cooperation strategies have been considered as a solution to address these network limitations. In the presence of unstable network infrastructures, mobile nodes cooperate with each other, forwarding data and performing other specific network functionalities. This article proposes a generalized incentive-based cooperation solution for mobile services and applications called MobiCoop. This reputation-based scheme includes an application framework for mobile applications that uses a Web service to handle all the nodes reputation and network permissions. The main goal of MobiCoop is to provide Internet services to mobile devices without network connectivity through cooperation with neighbor devices. The article includes a performance evaluation study of MobiCoop considering both a real scenario (using a prototype) and a simulation-based study. Results show that the proposed approach provides network connectivity independency to users with mobile apps when Internet connectivity is unavailable. Then, it is concluded that MobiCoop improved significantly the overall system performance and the service provided for a given mobile application. © 2016 ACM.",Cooperation; Incentive-based cooperation; Mobile applications; Mobile computing; Reputation-based cooperation,Digital storage; Mobile computing; Mobile devices; Network architecture; Wireless telecommunication systems; Application frameworks; Cooperation; Incentive-based cooperation; Mobile applications; Network infrastructure; Performance evaluations; Reputation-based cooperation; Wireless communications; Web services
From the Past Editor-In-Chief,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976875036&doi=10.1145%2f2903774&partnerID=40&md5=500d875629f2186a555017f5371f6c54,[No abstract available],,
Atom decomposition with adaptive basis selection strategy for matrix completion,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976448846&doi=10.1145%2f2903716&partnerID=40&md5=50f98f60a93180cdfcff758cdd8ba8d4,"Estimating missing entries in matrices has attracted much attention due to its wide range of applications like image inpainting and video denoising, which are usually considered as low-rank matrix completion problems theoretically. It is common to consider nuclear norm as a surrogate of the rank operator since it is the tightest convex lower bound of the rank operator under certain conditions. However, most approaches based on nuclear norm minimization involve a number of singular value decomposition (SVD) operations. Given a matrix X ∈ Rmxn, the time complexity of the SVD operation is O(mn2), which brings prohibitive computational burden on large-scale matrices, limiting the further usage of these methods in real applications. Motivated by this observation, a series of atom-decomposition-based matrix completion methods have been studied. The key to these methods is to reconstruct the target matrix by pursuit methods in a greedy way, which only involves the computation of the top SVD and has great advantages in efficiency compared with the SVD-based matrix completion methods. However, due to gradually serious accumulation errors, atom-decomposition-based methods usually result in unsatisfactory reconstruction accuracy. In this article, we propose a new efficient and scalable atom decomposition algorithm for matrix completion called Adaptive Basis Selection Strategy (ABSS). Different from traditional greedy atom decomposition methods, a two-phase strategy is conducted to generate the basis separately via different strategies according to their different nature. At first, we globally prune the basis space to eliminate the unimportant basis as much as possible and locate the probable subspace containing the most informative basis. Then, another group of basis spaces are learned to improve the recovery accuracy based on local information. In this way, our proposed algorithm breaks through the accuracy bottleneck of traditional atom-decomposition-based matrix completion methods; meanwhile, it reserves the innate efficiency advantages over SVD-based matrix completion methods. We empirically evaluate the proposed algorithm ABSS on real visual image data and large-scale recommendation datasets. Results have shown that ABSS has much better reconstruction accuracy with comparable cost to atom-decomposition-based methods. At the same time, it outperforms the state-of-the-art SVD-based matrix completion algorithms by similar or better reconstruction accuracy with enormous advantages on efficiency. © 2016 ACM.",Atom decomposition; Basis selection; Matrix completion,Atoms; Efficiency; Accumulation errors; Atom decomposition; Basis selection; Computational burden; Low-rank matrix completions; Matrix completion; Nuclear norm minimizations; Reconstruction accuracy; Singular value decomposition
Semantic photo retargeting under noisy image labels,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971207205&doi=10.1145%2f2886775&partnerID=40&md5=c899c6e5a7a39120810da52ebf179769,"With the popularity of mobile devices, photo retargeting has become a useful technique that adapts a high-resolution photo onto a low-resolution screen. Conventional approaches are limited in two aspects. The first factor is the de-emphasized role of semantic content that is many times more important than lowlevel features in photo aesthetics. Second is the importance of image spatial modeling: Toward a semantically reasonable retargeted photo, the spatial distribution of objects within an image should be accurately learned. To solve these two problems, we propose a new semantically aware photo retargeting that shrinks a photo according to region semantics. The key technique is a mechanism transferring semantics of noisy image labels (inaccurate labels predicted by a learner like an SVM) into different image regions. In particular, we first project the local aesthetic features (graphlets in this work) onto a semantic space, wherein image labels are selectively encoded according to their noise level. Then, a category-sharing model is proposed to robustly discover the semantics of each image region. The model is motivated by the observation that the semantic distribution of graphlets from images tagged by a common label remains stable in the presence of noisy labels. Thereafter, a spatial pyramid is constructed to hierarchically encode the spatial layout of graphlet semantics. Based on this, a probabilistic model is proposed to enforce the spatial layout of a retargeted photo to be maximally similar to those from the training photos. Experimental results show that (1) noisy image labels predicted by different learners can improve the retargeting performance, according to both qualitative and quantitative analysis, and (2) the category-sharing model stays stable even when 32.36% of image labels are incorrectly predicted. ©2016 ACM.",Aesthetics evaluation; Graphlet; Image label; Retargeting; Semantic,Semantics; Aesthetics evaluation; Conventional approach; Graphlet; High-resolution photos; Probabilistic modeling; Qualitative and quantitative analysis; Retargeting; Semantic distribution; Image enhancement
Adaptive streaming in P2P live video systems: A distributed rate control approach,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971440047&doi=10.1145%2f2912123&partnerID=40&md5=2dbc79b255836235a3fd39e35aa2ac42,"Dynamic Adaptive Streaming over HTTP (DASH) is a recently proposed standard that offers different versions of the same media content to adapt the delivery process over the Internet to dynamic bandwidth fluctuations and different user device capabilities. The peer-to-peer (P2P) paradigm for video streaming allows us to leverage the cooperation among peers, guaranteeing the service of video requests with increased scalability and reduced cost. We propose to combine these two approaches in a P2P-DASH architecture, exploiting the potentiality of both. The new platform is made of several swarms and a different DASH representation is streamed within each of them; unlike client-server DASH architectures, where each client autonomously selects which version to download according to current network conditions and to its device resources, we put forth a new rate control strategy implemented at peer site to maintain a good viewing quality to the local user and to simultaneously guarantee the successful operation of the P2P swarms. The effectiveness of the solution is demonstrated through simulation and it indicates that the P2P-DASH platform is able to provide its users with very good performance, much more satisfying than in a conventional P2P environment where DASH is not employed. Through a comparison with a reference DASH system modeled via the Integer Linear Programming (ILP) approach, the new system is shown to outperform such reference architecture. To further validate the proposal, in terms of both robustness and scalability, system behavior is investigated in the critical condition of a flash crowd, showing that the strong upsurge of new users can be successfully revealed and gradually accommodated. © 2016 ACM.",DASH; Flash-crowd; Integer linear programming; Peer-to-peer video streaming,Adaptive control systems; HTTP; Integer programming; Media streaming; Memory architecture; Network architecture; Quality control; Scalability; Video streaming; Critical condition; DASH; Dynamic Adaptive Streaming over HTTP; Flash crowd; Integer Linear Programming; Peer-to-peer paradigm; Peer-to-peer video streaming; Reference architecture; Peer to peer networks
Collaborative annotation of videos relying on weak consistency,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971432345&doi=10.1145%2f2907983&partnerID=40&md5=c3026a1fb7c9f9a31224c251524371f2,"This work discusses a distributed interactive video system that supports video annotation using simultaneous hyperlinking by multiple users. The users mark and annotate objects within the video with links to other media such as text, images, websites, or other videos. Annotations are visualized on the client user interface as an overlay close to the objects. Our system is intuitive to use; for example, it contains automatic object-tracking functionality that correctly positions the annotations, even when the form or location of an object changes. Thus, our first contribution discusses the adaptive object-tracking algorithm used for this repositioning. It shows improved precision and reliability in comparison to nonadaptive algorithms. A second key issue is to keep the system responsive when the number of concurrent annotators increases. Thus, we rely on the concept of eventual consistency between different network entities. While this weak form of consistency allows temporary inconsistencies, it ensures that a consistent state can be reached. Thus, the second contribution is the design and evaluation of our distributed interactive video system, which relies on the weak consistency paradigm. © 2016 ACM.",Eventual consistency; Hypervideo; Interactive system; Weak consistency,Distributed computer systems; Tracking (position); Adaptive object tracking; Design and evaluations; Eventual consistency; Hypervideo; Interactive system; Nonadaptive algorithm; Video annotations; Weak consistency; User interfaces
A high-fidelity and low-interaction-delay screen sharing system,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971473558&doi=10.1145%2f2897395&partnerID=40&md5=3cc965a62113dc6fe0d68a43b6e2e679,"The pervasive computing environment and wide network bandwidth provide users more opportunities to share screen content among multiple devices. In this article, we introduce a remote display system to enable screen sharing among multiple devices with high fidelity and responsive interaction. In the developed system, the frame-level screen content is compressed and transmitted to the client side for screen sharing, and the instant control inputs are simultaneously transmitted to the server side for interaction. Even if the screen responds immediately to the control messages and updates at a high frame rate on the server side, it is difficult to update the screen content with low delay and high frame rate in the client side due to non-negligible time consumption on the whole screen frame compression, transmission, and display buffer updating. To address this critical problem, we propose a layered structure for screen coding and rendering to deliver diverse screen content to the client side with an adaptive frame rate.More specifically, the interaction content with small region screen update is compressed by a blockwise screen codec and rendered at a high frame rate to achieve smooth interaction, while the natural video screen content is compressed by standard video codec and rendered at a regular frame rate for a smooth video display. Experimental results with real applications demonstrate that the proposed system can successfully reduce transmission bandwidth cost and interaction delay during screen sharing. Especially for user interaction in small regions, the proposed system can achieve a higher frame rate than most previous counterparts. © 2016 ACM.",Bandwidth consumption; Interaction delay; Layered structure; Screen coding; Screen rendering; Video quality,Bandwidth; Display devices; Internet; Ubiquitous computing; Bandwidth consumption; Critical problems; Frame compression; Interaction delay; Layered Structures; Pervasive computing environment; Transmission bandwidth; Video quality; Rendering (computer graphics)
Measuring Collectiveness via Refined Topological Similarity,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968830522&doi=10.1145%2f2854000&partnerID=40&md5=688fd0d2615adb5581458ef4273af049,"Crowd system has motivated a surge of interests in many areas of multimedia, as it contains plenty of information about crowd scenes. In crowd systems, individuals tend to exhibit collective behaviors, and the motion of all those individuals is called collective motion. As a comprehensive descriptor of collective motion, collectiveness has been proposed to reflect the degree of individuals moving as an entirety. Nevertheless, existing works mostly have limitations to correctly find the individuals of a crowd system and precisely capture the various relationships between individuals, both of which are essential to measure collectiveness. In this article, we propose a collectiveness-measuring method that is capable of quantifying collectiveness accurately. Our main contributions are threefold: (1) we compute relatively accurate collectiveness bymaking the tracked feature points represent the individuals more precisely with a point selection strategy; (2) we jointly investigate the spatial-temporal information of individuals and utilize it to characterize the topological relationship between individuals by manifold learning; (3) we propose a stability descriptor to deal with the irregular individuals, which influence the calculation of collectiveness. Intensive experiments on the simulated and real world datasets demonstrate that the proposed method is able to compute relatively accurate collectiveness and keep high consistency with human perception. © 2016 Copyright held by the owner/author(s).",collectiveness; crowd analysis; feature extraction; manifold; Multimedia,Feature extraction; Collective behavior; collectiveness; Crowd analysis; manifold; Multimedia; Real-world datasets; Topological relationships; Topological similarity; Topology
Forensic analysis of linear and nonlinear image filtering using quantization noise,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961123769&doi=10.1145%2f2857069&partnerID=40&md5=ba1ce5315659c1afd6acc28222431d3c,"The availability of intelligent image editing techniques and antiforensic algorithms, make it convenient to manipulate an image and to hide the artifacts that itmight have produced in the process. Real world forgeries are generally followed by the application of enhancement techniques such as filtering and/or conversion of the image format to suppress the forgery artifacts. Though several techniques evolved in the direction of detecting some of these manipulations, additional operations like recompression, nonlinear filtering, and other antiforensic methods during forgery are not deeply investigated. Toward this, we propose a robust method to detect whether a given image has undergone filtering (linear or nonlinear) based enhancement, possibly followed by format conversion after forgery. In the proposed method, JPEG quantization noise is obtained using natural image prior and quantization noise models. Transition probability features extracted from the quantization noise are used for machine learning based detection and classification. We test the effectiveness of the algorithm in classifying the class of the filter applied and the efficacy in detecting filtering in low resolution images. Experiments are performed to compare the performance of the proposed technique with state-of-the-art forensic filtering detection algorithms. It is found that the proposed technique is superior in most of the cases. Also, experiments against popular antiforensic algorithms show the counter antiforensic robustness of the proposed technique. © 2016 ACM.",Counter antiforensics; Filtering classification; GMM; HMRF; Markov process; Quantization noise,Artificial intelligence; Learning systems; Markov processes; Counter anti-forensics; Detection algorithm; HMRF; Low resolution images; Nonlinear image filtering; Quantization noise; Quantization noise model; Transition probabilities; Algorithms
Depth personalization and streaming of stereoscopic sports videos,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961195400&doi=10.1145%2f2890103&partnerID=40&md5=e67d575f801d185979dc21a2af16a70f,"Current three-dimensional displays cannot fully reproduce all depth cues used by a human observer in the real world. Instead, they create only an illusion of looking at a three-dimensional scene. This leads to a number of challenges during the content creation process. To assure correct depth reproduction and visual comfort, either the acquisition setup has to be carefully controlled or additional postprocessing techniques have to be applied. Furthermore, these manipulations need to account for a particular setup that is used to present the content, for example, viewing distance or screen size. This creates additional challenges in the context of personal use when stereoscopic content is shown on TV sets, desktop monitors, or mobile devices. We address this problem by presenting a new system for streaming stereoscopic content. Its key feature is a computationally efficient depth adjustment technique which can automatically optimize viewing experience for videos of field sports such as soccer, football, and tennis. Additionally, the method enables depth personalization to allow users to adjust the amount of depth according to their preferences. Our stereoscopic video streaming system was implemented, deployed, and tested with real users. © 2016 ACM.",3d video; 3D Video streaming; Depth personalization; Stereoscopic retargeting; Video storage systems,Mobile devices; Sports; Video streaming; 3-D videos; 3D video streaming; Personalizations; Stereoscopic retargeting; Video storage; Stereo image processing
Secure nonlocal denoising in outsourced images,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961189925&doi=10.1145%2f2886777&partnerID=40&md5=a6ec44aa3c8239074bb20b8433f18e19,"Signal processing in the encrypted domain becomes a desired technique to protect privacy of outsourced data in cloud. In this article, we propose a double-cipher scheme to implement nonlocal means (NLM) denoising in encrypted images. In this scheme, one ciphertext is generated by the Paillier scheme, which enables the mean filter, and the other is obtained by a privacy-preserving transform, which enables the nonlocal search. By the privacy-preserving transform, the cloud server can search the similar pixel blocks in the ciphertexts with the same speed as in the plaintexts; thus, the proposed method can be executed fast. To enhance the security, we randomly permutate both ciphertexts. To reduce the denoising complexity caused by random permutation, a random NLM method is exploited in the encrypted domain. The experimental results show that the quality of denoised images in the encrypted domain is comparable to that obtained in the plain domain. © 2016 ACM.",Image denoising; Johnson-Lindenstrauss transform; Nonlocal means; Paillier homomorphic encryption,Cryptography; Data privacy; Security of data; Signal processing; Ho-momorphic encryptions; Johnson-lindenstrauss transforms; Non local means (NLM); Non-local denoising; Non-local means; Privacy preserving; Random permutations; Signal processing in the encrypted domains; Image denoising
Enhanced Reweighted MRFs for Efficient Fashion Image Parsing,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961157652&doi=10.1145%2f2890104&partnerID=40&md5=747d63020ff21dcc41d77f0c55dd4c39,"Previous image parsing methods usually model the problem in a conditional random field which describes a statistical model learned from a training dataset and then processes a query image using the conditional probability. However, for clothing images, fashion items have a large variety of layering and configuration, and it is hard to learn a certain statistical model of features that apply to general cases. In this article, we take fashion images as an example to show how Markov Random Fields (MRFs) can outperform Conditional Random Fields when the application does not follow a certain statistical model learned from the training data set. We propose a new method for automatically parsing fashion images in high processing efficiency with significantly less training time by applying a modification of MRFs, named reweighted MRF (RW-MRF), which resolves the problem of over smoothing infrequent labels. We further enhance RW-MRF with occlusion prior and background prior to resolve two other common problems in clothing parsing, occlusion, and background spill. Our experimental results indicate that our proposed clothing parsing method significantly improves processing time and training time over state-of-the-art methods, while ensuring comparable parsing accuracy and improving label recall rate. © 2016 ACM.",Conditional random field; Fashion parsing; Image parsing; Image segmentation; Markov random field,Image segmentation; Markov processes; Random processes; Conditional probabilities; Conditional random field; Fashion parsing; Image parsing; Markov random field; Markov Random Fields; State-of-the-art methods; Statistical modeling; Image processing
Mobile device-to-device video distribution: Theory and application,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961181445&doi=10.1145%2f2886776&partnerID=40&md5=507df32d37d28bd1e479753f23856103,"As video traffic has dominated the data flow of smartphones, traditional cellular communications face substantial transmission challenges. In this work, we study mobile device-to-device (D2D) video distribution that leverages the storage and communication capacities of smartphones. In such a mobile distributed framework, D2D communication represents an opportunistic process to selectively store and transmit local videos to meet the future demand of others. The performance is measured by the service time, which denotes the elapsed period for fulfilling the demand, and the corresponding implementation of each device depends on the video's demand, availability, and size. The main contributions of this work lie in (1) considering the impact of video size in a practical mobile D2D video distribution scenario and proposing a general global estimation of the video distribution based on limited and local observations; (2) designing a purely distributed D2D video distribution scheme without the monitoring of any central controller; and (3) providing a practical implementation of the scheme, which does not need to know the video availability, user demand, and device mobility. Numerical results have demonstrated the efficiency and robustness of the proposed scheme. © 2016 ACM.",Application; Device-to-device; Smartphone; Video distribution,Applications; Digital storage; Mobile devices; Communication capacity; D2D communications; Device mobilities; Device-to-device; Distributed framework; Local observations; Numerical results; Video distribution; Smartphones
Automatic generation of visual-textual presentation layout,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968783396&doi=10.1145%2f2818709&partnerID=40&md5=5aedd4bd7516b5f3df485a26e436b913,"Visual-textual presentation layout (e.g., digital magazine cover, poster, Power Point slides, and any other rich media), which combines beautiful image and overlaid readable texts, can result in an eye candy touch to attract users' attention. The designing of visual-textual presentation layout is therefore becoming ubiquitous in both commercially printed publications and online digital magazines. However, handcrafting aesthetically compelling layouts still remains challenging for many small businesses and amateur users. This article presents a system to automatically generate visual-textual presentation layouts by investigating a set of aesthetic design principles, through which an average user can easily create visually appealing layouts. The system is attributed with a set of topic-dependent layout templates and a computational framework integrating high-level aesthetic principles (in a top-down manner) and low-level image features (in a bottom-up manner). The layout templates, designed with prior knowledge from domain experts, define spatial layouts, semantic colors, harmonic color models, and font emotion and size constraints. We formulate the typography as an energy optimization problem by minimizing the cost of text intrusion, the utility of visual space, and the mismatch of information importance in perception and semantics, constrained by the automatically selected template and further preserving color harmonization. We demonstrate that our designs achieve the best reading experience compared with the reimplementation of parts of existing state-of-the-art designs through a series of user studies. © 2016 ACM.",Color harmonization; Design templates; Rich media presentation; Typography; Visual-textual presentation layout,Color; Constrained optimization; Content based retrieval; Optimization; Semantics; Typesetting; Color harmonization; Design templates; Rich media presentation; Typography; Visual-textual presentation layout; Design
Finding social points of interest from georeferenced and oriented online photographs,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971595450&doi=10.1145%2f2854004&partnerID=40&md5=211348748668738576d39fe8e4bd3935,"Points of interest are an important requirement for location-based services, yet they are editorially curated and maintained, either professionally or through community. Beyond the laborious manual annotation task, further complications arise as points of interest may appear, relocate, or disappear over time, and may be relevant only to specific communities. To assist, complement, or even replace manual annotation, we propose a novel method for the automatic localization of points of interest depicted in photos taken by people across the world. Our technique exploits the geographic coordinates and the compass direction supplied by modern cameras, while accounting for possible measurement errors due to the variability in accuracy of the sensors that produced them.We statistically demonstrate that our method significantly outperforms techniques from the research literature on the task of estimating the geographic coordinates and geographic footprints of points of interest in various cities, even when photos are involved in the estimation process that do not show the point of interest at all. © 2016 ACM 1551-6857/2016/01-ART36 $15.00.",,Telecommunication services; Automatic localization; Estimation process; Geographic coordinates; Geographic footprints; Manual annotation; Point of interest; Points of interest; Location based services
Measurements and analysis of a major adult video portal,2016,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964528223&doi=10.1145%2f2854003&partnerID=40&md5=3e6da85283db03f48dc0ec2de5a001bc,"Today, the Internet is a largemultimedia delivery infrastructure, withwebsites such as YouTube appearing at the top of most measurement studies. However,most traffic studies have ignored an important domain: Adult multimedia distribution.Whereas, traditionally, such services were provided primarily via bespoke websites, recently these have converged towards what is known as ""Porn 2.0"". These services allow users to upload, view, rate, and comment on videos for free (much like YouTube). Despite their scale, we still lack even a basic understanding of their operation. This article addresses this gap by performing a large-scale study of one of the most popular Porn 2.0 websites: YouPorn. Our measurements reveal a global delivery infrastructure that we have repeatedly crawled to collect statistics (on 183k videos). We use this data to characterise the corpus, as well as to inspect popularity trends and how they relate to other features, for example, categories and ratings. To explore our discoveries further, we use a small-scale user study, highlighting key system implications. © 2016 ACM.",Adult video content; Media streaming; Porn 2.0; User behaviour,Behavioral research; Media streaming; Websites; KeY systems; Large-scale studies; Measurement study; Porn 2.0; Small scale; Traffic studies; User behaviour; Video contents; Multimedia services
A Novel Framework for Joint Learning of City Region Partition and Representation,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193684907&doi=10.1145%2f3652857&partnerID=40&md5=e76deb296723ca58e8290b931ca034e3,"The proliferation of multimodal big data in cities provides unprecedented opportunities for modeling and forecasting urban problems, such as crime prediction and house price prediction, through data-driven approaches. A fundamental and critical issue in modeling and forecasting urban problems lies in identifying suitable spatial analysis units, also known as city region partition. Existing works rely on subjective domain knowledge for static partitions, which is general and universal for all tasks. In fact, different tasks may need different city region partitions. To address this issue, we propose JLPR, a task-oriented framework for Joint Learning of region Partition and Representation. To make partitions fit tasks, JLPR integrates the region partition into the representation model training and learns region partitions using the supervision signal from the downstream task. We evaluate the framework on two prediction tasks (i.e., crime prediction and housing price prediction) in Chicago. Experiments show that JLPR consistently outperforms state-of-the-art partitioning methods in both tasks, which achieves above 25% and 70% performance improvements in terms of mean absolute error for crime prediction and house price prediction tasks, respectively. Additionally, we meticulously undertake three visualization case studies, which yield profound and illuminating findings from diverse perspectives, demonstrating the remarkable effectiveness and superiority of our approach.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",multimodal big data; partition learning; prediction task; Region partition; representation learning,Crime; Data mining; Domain Knowledge; Forecasting; Housing; Joint learning; Modeling and forecasting; Multi-modal; Multimodal big data; Partition learning; Prediction tasks; Price prediction; Region partition; Representation learning; Urban problems; Big data
Suitable and Style-Consistent Multi-Texture Recommendation for Cartoon Illustrations,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193716143&doi=10.1145%2f3652518&partnerID=40&md5=e76e0a6fad0a16018a4670a6923a450c,"Texture plays an important role in cartoon illustrations to display object materials and enrich visual experiences. Unfortunately, manually designing and drawing an appropriate texture is not easy even for proficient artists, let alone novice or amateur people. While there exist tons of textures on the Internet, it is not easy to pick an appropriate one using traditional text-based search engines. Although several texture pickers have been proposed, they still require the users to browse the textures by themselves, which is still labor-intensive and time-consuming. In this article, an automatic texture recommendation system is proposed for recommending multiple textures to replace a set of user-specified regions in a cartoon illustration with visually pleasant look. Two measurements, the suitability measurement and the style-consistency measurement, are proposed to make sure that the recommended textures are suitable for cartoon illustration and at the same time mutually consistent in style. The suitability is measured based on the synthesizability, cartoonity, and region fitness of textures. The style-consistency is predicted using a learning-based solution since it is subjective to judge whether two textures are consistent in style. An optimization problem is formulated and solved via the genetic algorithm. Our method is validated on various cartoon illustrations, and convincing results are obtained. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cartoon texture; Texture recommendation; texture replacement; texture style-consistency; texture synthesis,Genetic algorithms; Search engines; Cartoon texture; Labor time; Labour-intensive; Multi-texture; Style consistency; Texture recommendation; Texture replacement; Texture style-consistency; Texture synthesis; Visual experiences; Textures
Optimizing Camera Motion with MCTS and Target Motion Modeling in Multi-Target Active Object Tracking,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193805026&doi=10.1145%2f3648369&partnerID=40&md5=450f5291848dfaeb69af53c274ecae96,"In this work, we are dedicated to multi-target active object tracking (AOT), where the goal is to achieve continuous tracking of targets through real-time control of camera. This form of active camera control can be applied to unmanned aerial vehicles (UAV), intelligent robots, and sports events. Our work is conducted in an environment featuring multiple cameras and targets, where our goal is to maximize target coverage. Contrasting with previous research, our work introduces additional degrees of freedom for the cameras, allowing them not only to rotate but also to move along boundary lines. In addition, we model the motion of target to predict the future position of the target in environment. With target’s future position, we use Monte Carlo Tree Search (MCTS) method to find the optimal action of camera. Since the action space is large, we propose to leverage the action selection from multi-agent reinforcement learning (MARL) network to prune the search tree of Monte Carlo Tree Search method, so as to find the optimal action more efficiently. We establish a multi-target 2D environment to simulate several sports games, and experimental results demonstrate that our method can effectively improve the target coverage. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Active object tracking; Monte Carlo tree search; reinforcement learning; target motion modeling,Antennas; Cameras; Degrees of freedom (mechanics); Intelligent robots; Learning systems; Monte Carlo methods; Multi agent systems; Real time control; Sports; Unmanned aerial vehicles (UAV); Active object; Active object tracking; Future position; Monte carlo tree search; Multi-targets; Object Tracking; Reinforcement learnings; Target coverage; Target motion models; Tree-search; Reinforcement learning
Multi-agent DRL-based Multipath Scheduling for Video Streaming with QUIC,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193759320&doi=10.1145%2f3649139&partnerID=40&md5=d4762975fe1c78694b018d7da2591c38,"The popularization of video streaming brings challenges in satisfying diverse Quality of Service (QoS) requirements. The multipath extension of the Quick UDP Internet Connection (QUIC) protocol, also called MPQUIC, has the potential to improve video streaming performance with multiple simultaneously transmitting paths. The multipath scheduler of MPQUIC determines how to distribute the packets onto different paths. However, while applying current multipath schedulers into MPQUIC, our experimental results show that they fail to adapt to various receive buffer sizes of different devices and comprehensive QoS requirements of video streaming. These problems are especially severe under heterogeneous and dynamic network environments. To tackle these problems, we propose MARS, a Multi-agent deep Reinforcement learning (MADRL)-based Multipath QUIC Scheduler, which is able to promptly adapt to dynamic network environments. It exploits the MADRL method to learn a neural network for each path and generate scheduling policy. Besides, it introduces a novel multi-objective reward function that takes out-of-order queue size and different QoS metrics into consideration to realize adaptive scheduling optimization. We implement MARS in an MPQUIC prototype and deploy in Dynamic Adaptive Streaming over HTTP system. Then, we compare it with the state-of-the-art multipath schedulers in both emulated and real-world networks. Experimental results show that MARS outperforms the other schedulers with better adaptive capability regarding the receive buffer sizes and QoS. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adaptive packet scheduling; multi-agent deep reinforcement learning; Multipath QUIC; video streaming,Deep learning; HTTP; Multi agent systems; Quality of service; Video streaming; Adaptive packet scheduling; Internet connection; Multi agent; Multi-agent deep reinforcement learning; Multipath; Multipath quick UDP internet connection; Packet scheduling; Quality-of-service; Reinforcement learnings; Video-streaming; Reinforcement learning
Discriminative Segment Focus Network for Fine-grained Video Action Recognition,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193709748&doi=10.1145%2f3654671&partnerID=40&md5=de2f505bacd56c93ebd1aefe4ef7c010,"Fine-grained video action recognition aims at identifying minor and discriminative variations among fine categories of actions. While many recent action recognition methods have been proposed to better model spatio-temporal representations, how to model the interactions among discriminative atomic actions to effectively characterize inter-class and intra-class variations has been neglected, which is vital for understanding fine-grained actions. In this work, we devise a Discriminative Segment Focus Network (DSFNet) to mine the discriminability of segment correlations and localize discriminative action-relevant segments for fine-grained video action recognition. Firstly, we propose a hierarchic correlation reasoning (HCR) module which explicitly establishes correlations between different segments at multiple temporal scales and enhances each segment by exploiting the correlations with other segments. Secondly, a discriminative segment focus (DSF) module is devised to localize the most action-relevant segments from the enhanced representations of HCR by enforcing the consistency between the discriminability and the classification confidence of a given segment with a consistency constraint. Finally, these localized segment representations are combined with the global action representation of the whole video for boosting final recognition. Extensive experimental results on two fine-grained action recognition datasets, i.e., FineGym and Diving48, and two action recognition datasets, i.e., Kinetics400 and Something-Something, demonstrate the effectiveness of our approach compared with the state-of-the-art methods.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",correlation; discriminative segment; Fine-grained action recognition,Action recognition; Atomic actions; Correlation; Discriminability; Discriminative segment; Fine grained; Fine-grained action recognition; Recognition methods; Spatio-temporal; Temporal representations
"ISF-GAN: Imagine, Select, and Fuse with GPT-Based Text Enrichment for Text-to-Image Synthesis",2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193683789&doi=10.1145%2f3650033&partnerID=40&md5=bf1d6e0486b72f1df93c710d70a15da1,"Text-to-Image synthesis aims to generate an accurate and semantically consistent image from a given text description. However, it is difficult for existing generative methods to generate semantically complete images from a single piece of text. Some works try to expand the input text to multiple captions via retrieving similar descriptions of the input text from the training set but still fail to fill in missing image semantics. In this article, we propose a GAN-based approach to Imagine, Select, and Fuse for Text-to-image synthesis, named ISF-GAN. The proposed ISF-GAN contains Imagine Stage and Select and Fuse Stage to solve the above problems. First, the Imagine Stage proposes a text completion and enrichment module. This module guides a GPT-based model to enrich the text expression beyond the original dataset. Second, the Select and Fuse Stage selects qualified text descriptions and then introduces a cross-modal attentional mechanism to interact these different sentence embeddings with the image features at different scales. In short, our proposed model enriches the input text information for completing missing semantics and introduces a cross-modal attentional mechanism to maximize the utilization of enriched text information to generate semantically consistent images. Experimental results on CUB, Oxford-102, and CelebA-HQ datasets prove the effectiveness and superiority of the proposed network. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adversarial learning; generative pre-trained model; Text-to-image synthesis,Image processing; Adversarial learning; Attentional mechanism; Cross-modal; Generative methods; Generative pre-trained model; Image semantics; Images synthesis; Text information; Text-to-image synthesis; Training sets; Semantics
Heterogeneous Fusion and Integrity Learning Network for RGB-D Salient Object Detection,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193760056&doi=10.1145%2f3656476&partnerID=40&md5=e3cd2ca0c35c8efa0222d7df10cd1557,"While significant progress has been made in recent years in the field of salient object detection, there are still limitations in heterogeneous modality fusion and salient feature integrity learning. The former is primarily attributed to a paucity of attention from researchers to the fusion of cross-scale information between different modalities during processing multi-modal heterogeneous data, coupled with an absence of methods for adaptive control of their respective contributions. The latter constraint stems from the shortcomings in existing approaches concerning the prediction of salient region's integrity. To address these problems, we propose a Heterogeneous Fusion and Integrity Learning Network for RGB-D Salient Object Detection (HFIL-Net). In response to the first challenge, we design an Advanced Semantic Guidance Aggregation (ASGA) module, which utilizes three fusion blocks to achieve the aggregation of three types of information: within-scale cross-modal, within-modal cross-scale, and cross-modal cross-scale. In addition, we embed the local fusion factor matrices in the ASGA module and utilize the global fusion factor matrices in the Multi-modal Information Adaptive Fusion module to control the contributions adaptively from different perspectives during the fusion process. For the second issue, we introduce the Feature Integrity Learning and Refinement Module. It leverages the idea of ""part-whole""relationships from capsule networks to learn feature integrity and further refine the learned features through attention mechanisms. Extensive experimental results demonstrate that our proposed HFIL-Net outperforms over 17 state-of-the-art detection methods in testing across seven challenging standard datasets. Codes and results are available on https://github.com/BojueGao/HFIL-Net.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",capsule network; heterogeneous modality fusion; integrity learning; Salient object detection,Adaptive control systems; Learning systems; Object detection; Object recognition; Capsule network; Cross-modal; Fusion factors; Fusion features; Heterogeneous modality fusion; Integrity learning; Learning network; matrix; Modality Fusion; Salient object detection; Semantics
StepNet: Spatial-temporal Part-aware Network for Isolated Sign Language Recognition,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193741537&doi=10.1145%2f3656046&partnerID=40&md5=c255e697800b2d9b68a633b32d76bf4d,"The goal of sign language recognition (SLR) is to help those who are hard of hearing or deaf overcome the communication barrier. Most existing approaches can be typically divided into two lines, i.e., Skeleton-based, and RGB-based methods, but both lines of methods have their limitations. Skeleton-based methods do not consider facial expressions, while RGB-based approaches usually ignore the fine-grained hand structure. To overcome both limitations, we propose a new framework called the Spatial-temporal Part-aware network (StepNet), based on RGB parts. As its name suggests, it is made up of two modules: Part-level Spatial Modeling and Part-level Temporal Modeling. Part-level Spatial Modeling, in particular, automatically captures the appearance-based properties, such as hands and faces, in the feature space without the use of any keypoint-level annotations. On the other hand, Part-level Temporal Modeling implicitly mines the long short-term context to capture the relevant attributes over time. Extensive experiments demonstrate that our StepNet, thanks to spatial-temporal modules, achieves competitive Top-1 Per-instance accuracy on three commonly used SLR benchmarks, i.e., 56.89% on WLASL, 77.2% on NMFs-CSL, and 77.1% on BOBSL. Additionally, the proposed method is compatible with the optical flow input and can produce superior performance if fused. For those who are hard of hearing, we hope that our work can act as a preliminary step.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Sign language recognition; video analysis,Musculoskeletal system; Communication barriers; Facial Expressions; Hard of hearings; Part levels; Sign Language recognition; Spatial modelling; Spatial temporals; Temporal models; Two-line; Video analysis; Audition
Automatic Lyric Transcription and Automatic Music Transcription from Multimodal Singing,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193683324&doi=10.1145%2f3651310&partnerID=40&md5=068656292d5bd05f0df98ea4fbfc24f3,"Automatic lyric transcription (ALT) refers to transcribing singing voices into lyrics, while automatic music transcription (AMT) refers to transcribing singing voices into note events, i.e., musical MIDI notes. Despite these two tasks having significant potential for practical application, they are still nascent. This is because the transcription of lyrics and note events solely from singing audio is notoriously difficult due to the presence of noise contamination, e.g., musical accompaniment, resulting in a degradation of both the intelligibility of sung lyrics and the recognizability of sung notes. To address this challenge, we propose a general framework for implementing multimodal ALT and AMT systems. Additionally, we curate the first multimodal singing dataset, comprising N20EMv1 and N20EMv2, which encompasses audio recordings and videos of lip movements, together with ground truth for lyrics and note events. For model construction, we propose adapting self-supervised learning models from the speech domain as acoustic encoders and visual encoders to alleviate the scarcity of labeled data. We also introduce a residual cross-attention mechanism to effectively integrate features from the audio and video modalities. Through extensive experiments, we demonstrate that our single-modal systems exhibit state-of-the-art performance on both ALT and AMT tasks. Subsequently, through single-modal experiments, we also explore the individual contributions of each modality to the multimodal system. Finally, we combine these and demonstrate the effectiveness of our proposed multimodal systems, particularly in terms of their noise robustness. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",automatic lyric transcription; automatic music transcription; dataset; multimodality; self-supervised-learning; Singing,Acoustic noise; Audio acoustics; Audio recordings; Learning systems; Music; Signal encoding; Automatic lyric transcription; Automatic music transcription; Dataset; Multi-modal; Multi-modality; Multimodal system; Self-supervised-learning; Singing; Singing voices; Single-modal; Supervised learning
A Unified Framework for Jointly Compressing Visual and Semantic Data,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193740059&doi=10.1145%2f3654800&partnerID=40&md5=4523fb03cb1bde866c200a83db15c6a6,"The rapid advancement of multimedia and imaging technologies has resulted in increasingly diverse visual and semantic data. A large range of applications such as remote-assisted driving requires the amalgamated storage and transmission of various visual and semantic data. However, existing works suffer from the limitation of insufficiently exploiting the redundancy between different types of data. In this article, we propose a unified framework to jointly compress a diverse spectrum of visual and semantic data, including images, point clouds, segmentation maps, object attributes, and relations. We develop a unifying process that embeds the representations of these data into a joint embedding graph according to their categories, which enables flexible handling of joint compression tasks for various visual and semantic data. To fully leverage the redundancy between different data types, we further introduce an embedding-based adaptive joint encoding process and a Semantic Adaptation Module to efficiently encode diverse data based on the learned embeddings in the joint embedding graph. Experiments on the Cityscapes, MSCOCO, and KITTI datasets demonstrate the superiority of our framework, highlighting promising steps toward scalable multimedia processing.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Joint visual and semantic data compression; multimedia processing; unified compression framework; visual semantic data,Data compression; Embeddings; Encoding (symbols); Image segmentation; Redundancy; Semantics; Embeddings; Joint visual and semantic data compression; Multimedia processing; Semantic data; Unified compression framework; Unified framework; Visual data; Visual semantic data; Visual semantics; Digital storage
Temporal Scene Montage for Self-Supervised Video Scene Boundary Detection,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193744383&doi=10.1145%2f3654669&partnerID=40&md5=6b80e5d784224beedab0661f718c6905,"Once a video sequence is organized as basic shot units, it is of great interest to temporally link shots into semantic-compact scene segments to facilitate long video understanding. However, it still challenges existing video scene boundary detection methods to handle various visual semantics and complex shot relations in video scenes. We proposed a novel self-supervised learning method, Video Scene Montage for Boundary Detection (VSMBD), to extract rich shot semantics and learn shot relations using unlabeled videos. More specifically, we present Video Scene Montage (VSM) to synthesize reliable pseudo scene boundaries, which learns task-related semantic relations between shots in a self-supervised manner. To lay a solid foundation for modeling semantic relations between shots, we decouple visual semantics of shots into foreground and background. Instead of costly learning from scratch as in most previous self-supervised learning methods, we build our model upon large-scale pre-trained visual encoders to extract the foreground and background features. Experimental results demonstrate VSMBD trains a model with strong capability in capturing shot relations, surpassing previous methods by significant margins. The code is available at https://github.com/mini-mind/VSMBD.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Self-supervised learning; video scene boundary detection; video structuring; video understanding,Learning systems; Semantic Segmentation; Supervised learning; Boundary detection; Learn+; Scene boundary detections; Self-supervised learning; Supervised learning methods; Video scene; Video scene boundary detection; Video structuring; Video understanding; Visual semantics; Semantics
Universal Relocalizer for Weakly Supervised Referring Expression Grounding,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193748237&doi=10.1145%2f3656045&partnerID=40&md5=56707a7c662f57123c25ce3af1cc65b4,"This article introduces the Universal Relocalizer, a novel approach designed for weakly supervised referring expression grounding. Our method strives to pinpoint a target proposal that corresponds to a specific query, eliminating the need for region-level annotations during training. To bolster the localization precision and enrich the semantic understanding of the target proposal, we devise three key modules: the category module, the color module, and the spatial relationship module. The category and color modules assign respective category and color labels to region proposals, enabling the computation of category and color scores. Simultaneously, the spatial relationship module integrates spatial cues, yielding a spatial score for each proposal to enhance localization accuracy further. By adeptly amalgamating the category, color, and spatial scores, we derive a refined grounding score for every proposal. Comprehensive evaluations on the RefCOCO, RefCOCO+, and RefCOCOg datasets manifest the prowess of the Universal Relocalizer, showcasing its formidable performance across the board.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",refined grounding score; universal relocalizer; Weakly supervised referring expression grounding,Color; Color labels; Color module; Color scores; Localisation; Referring expressions; Refined grounding score; Semantics understanding; Spatial relationships; Universal relocalizer; Weakly supervised referring expression grounding; Semantics
Learning Nighttime Semantic Segmentation the Hard Way,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193723435&doi=10.1145%2f3650032&partnerID=40&md5=e0346ade6c6974e0dbfad2c6c29166a8,"Nighttime semantic segmentation is an important but challenging research problem for autonomous driving. The major challenges lie in the small objects or regions from the under-/over-exposed areas or suffer from motion blur caused by the camera deployed on moving vehicles. To resolve this, we propose a novel hard-class-aware module that bridges the main network for full-class segmentation and the hard-class network for segmenting aforementioned hard-class objects. In specific, it exploits the shared focus of hard-class objects from the dual-stream network, enabling the contextual information flow to guide the model to concentrate on the pixels that are hard to classify. In the end, the estimated hard-class segmentation results will be utilized to infer the final results via an adaptive probabilistic fusion refinement scheme. Moreover, to overcome over-smoothing and noise caused by extreme exposures, our model is modulated by a carefully crafted pretext task of constructing an exposure-aware semantic gradient map, which guides the model to faithfully perceive the structural and semantic information of hard-class objects while mitigating the negative impact of noises and uneven exposures. In experiments, we demonstrate that our unique network design leads to superior segmentation performance over existing methods, featuring the strong ability of perceiving hard-class objects under adverse conditions. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",dual-branch; fusion refinement scheme; hard class; Nighttime semantic segmentation; pretext task,Classification (of information); Semantics; Autonomous driving; Class objects; Dual-branch; Fusion refinement scheme; Hard class; Nighttime semantic segmentation; Pretext task; Research problems; Semantic segmentation; Small objects; Semantic Segmentation
Recoverable Privacy-Preserving Image Classification through Noise-like Adversarial Examples,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193720161&doi=10.1145%2f3653676&partnerID=40&md5=46f229a2803985003b5e67efc1dc70b1,"With the increasing prevalence of cloud computing platforms, ensuring data privacy during the cloud-based image-related services such as classification has become crucial. In this study, we propose a novel privacy-preserving image classification scheme that enables the direct application of classifiers trained in the plaintext domain to classify encrypted images without the need of retraining a dedicated classifier. Moreover, encrypted images can be decrypted back into their original form with high fidelity (recoverable) using a secret key. Specifically, our proposed scheme involves utilizing a feature extractor and an encoder to mask the plaintext image through a newly designed Noise-like Adversarial Example (NAE). Such an NAE not only introduces a noise-like visual appearance to the encrypted image but also compels the target classifier to predict the ciphertext as the same label as the original plaintext image. At the decoding phase, we adopt a Symmetric Residual Learning (SRL) framework for restoring the plaintext image with minimal degradation. Extensive experiments demonstrate that (1) the classification accuracy of the classifier trained in the plaintext domain remains the same in both the ciphertext and plaintext domains; (2) the encrypted images can be recovered into their original form with an average PSNR of up to 51+ dB for the SVHN dataset and 48+ dB for the VGGFace2 dataset; (3) our system exhibits satisfactory generalization capability on the encryption, decryption, and classification tasks across datasets that are different from the training one; and (4) a high-level of security is achieved against three potential threat models. The code is available at https://github.com/csjunjun/RIC.git.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep neural networks; encryption; image classification; Privacy-preserving,Classification (of information); Image classification; Privacy-preserving techniques; Ciphertexts; Classification scheme; Cloud computing platforms; Cloud-based; Encrypted images; High-fidelity; Images classification; Plaintext; Privacy preserving; Secret key; Deep neural networks
Multimodal Score Fusion with Sparse Low-rank Bilinear Pooling for Egocentric Hand Action Recognition,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193720043&doi=10.1145%2f3656044&partnerID=40&md5=5ce65fd42362aa429448f4f53572e21a,"With the advent of egocentric cameras, there are new challenges where traditional computer vision is not sufficient to handle this kind of video. Moreover, egocentric cameras often offer multiple modalities that need to be modeled jointly to exploit complimentary information. In this article, we propose a sparse low-rank bilinear score pooling approach for egocentric hand action recognition from RGB-D videos. It consists of five blocks: a baseline CNN to encode RGB and depth information for producing classification probabilities; a novel bilinear score pooling block to generate a score matrix; a sparse low-rank matrix recovery block to reduce redundant features, which is common in bilinear pooling; a one-layer CNN for frame-level classification; and an RNN for video-level classification. We proposed to fuse classification probabilities instead of traditional CNN features from RGB and depth modality, involving an effective yet simple sparse low-rank bilinear score pooling to produce a fused RGB-D score matrix. To demonstrate the efficacy of our method, we perform extensive experiments over two large-scale hand action datasets, namely, THU-READ and FPHA, and two smaller datasets, GUN-71 and HAD. We observe that the proposed method outperforms state-of-the-art methods and achieves accuracies of 78.55% and 96.87% over the THU-READ dataset in cross-subject and cross-group settings, respectively. Further, we achieved accuracies of 91.59% and 43.87% over the FPHA and Gun-71 datasets, respectively.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Bilinear score pooling; CNN; egocentric hand action recognition; low rank; RGB-D videos; RNN; sparse,Cameras; Large datasets; Palmprint recognition; Action recognition; Bilinear score pooling; Egocentric hand action recognition; Low rank; Multi-modal; RGB-D video; RNN; Score fusion; Score matrixes; Sparse; Classification (of information)
Effective Video Summarization by Extracting Parameter-Free Motion Attention,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193733866&doi=10.1145%2f3654670&partnerID=40&md5=8862873181b019bc4d9e51a057029c57,"Video summarization remains a challenging task despite increasing research efforts. Traditional methods focus solely on long-range temporal modeling of video frames, overlooking important local motion information that cannot be captured by frame-level video representations. In this article, we propose the Parameter-free Motion Attention Module (PMAM) to exploit the crucial motion clues potentially contained in adjacent video frames, using a multi-head attention architecture. The PMAM requires no additional training for model parameters, leading to an efficient and effective understanding of video dynamics. Moreover, we introduce the Multi-feature Motion Attention Network (MMAN), integrating the PMAM with local and global multi-head attention based on object-centric and scene-centric video representations. The synergistic combination of local motion information, extracted by the proposed PMAM, with long-range interactions modeled by the local and global multi-head attention mechanism, can significantly enhance the performance of video summarization. Extensive experimental results on the benchmark datasets, SumMe and TVSum, demonstrate that the proposed MMAN outperforms other state-of-the-art methods, resulting in remarkable performance gains.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",feature fusion; motion attention; multi-head attention; parameter-free; Video summarization,Benchmarking; Features fusions; Free motion; Local motions; Motion attention; Motion information; Multi-head attention; Parameter-free; Video frame; Video representations; Video summarization; Video recording
Dual Dynamic Threshold Adjustment Strategy,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193754485&doi=10.1145%2f3656047&partnerID=40&md5=e478e0909b261d61a5c56c0ee8c7f412,"Loss functions and sample mining strategies are essential components in deep metric learning algorithms. However, the existing loss function or mining strategy often necessitates the incorporation of additional hyperparameters, notably the threshold, which defines whether the sample pair is informative. The threshold provides a stable numerical standard for determining whether to retain the pairs. It is a vital parameter to reduce the redundant sample pairs participating in training. Nonetheless, finding the optimal threshold can be a time-consuming endeavor, often requiring extensive grid searches. Because the threshold cannot be dynamically adjusted in the training stage, we should conduct plenty of repeated experiments to determine the threshold. Therefore, we introduce a novel approach for adjusting the thresholds associated with both the loss function and the sample mining strategy. We design a static Asymmetric Sample Mining Strategy (ASMS) and its dynamic version, the Adaptive Tolerance ASMS (AT-ASMS), tailored for sample mining methods. ASMS utilizes differentiated thresholds to address the problems (too few positive pairs and too many redundant negative pairs) caused by only applying a single threshold to filter samples. The AT-ASMS can adaptively regulate the ratio of positive and negative pairs during training according to the ratio of the currently mined positive and negative pairs. This meta-learning-based threshold generation algorithm utilizes a single-step gradient descent to obtain new thresholds. We combine these two threshold adjustment algorithms to form the Dual Dynamic Threshold Adjustment Strategy (DDTAS). Experimental results show that our algorithm achieves competitive performance on the CUB200, Cars196, and SOP datasets. Our codes are available at https://github.com/NUST-Machine-Intelligence-Laboratory/DDTAS.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep metric learning; image retrieval; sample mining strategy,Data mining; Deep learning; Gradient methods; Learning algorithms; A-stable; Adaptive tolerance; Deep metric learning; Dual dynamics; Dynamic threshold; Hyper-parameter; Loss functions; Metric learning; Sample mining strategy; Threshold adjustments; Image retrieval
Multi-Domain Image-to-Image Translation with Cross-Granularity Contrastive Learning,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193748000&doi=10.1145%2f3656048&partnerID=40&md5=f5fff6238d855ba534c220e46fd885a6,"The objective of multi-domain image-to-image translation is to learn the mapping from a source domain to a target domain in multiple image domains while preserving the content representation of the source domain. Despite the importance and recent efforts, most previous studies disregard the large style discrepancy between images and instances in various domains, or fail to capture instance details and boundaries properly, resulting in poor translation results for rich scenes. To address these problems, we present an effective architecture for multi-domain image-to-image translation that only requires one generator. Specifically, we provide detailed procedures for capturing the features of instances throughout the learning process, as well as learning the relationship between the style of the global image and that of a local instance in the image by enforcing the cross-granularity consistency. In order to capture local details within the content space, we employ a dual contrastive learning strategy that operates at both the instance and patch levels. Extensive studies on different multi-domain image-to-image translation datasets reveal that our proposed method outperforms state-of-the-art approaches.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",contrastive learning; cross-granularity; GAN; Image-to-image translation; multi-domain,Contrastive learning; Cross-granularity; GAN; Image domain; Image translation; Image-to-image translation; Learn+; Multi-domains; Multiple image; Target domain; Learning systems
WaRENet: A Novel Urban Waterlogging Risk Evaluation Network,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193724590&doi=10.1145%2f3651163&partnerID=40&md5=b093aa37638a171331fb2525deac6e23,"In this article, we propose a novel urban waterlogging risk evaluation network (WaRENet) to evaluate the risk of waterlogging. The WaRENet distinguishes whether an urban image involves waterlogging by classification module, and estimates the waterlogging risk levels by multi-class reference objects detection module (MCROD). First, in the waterlogging scene classification, ResNet combined with Se-block is used to identify the waterlogging scene, and lightweight gradient-weighted class activation mapping (Grad-CAM) is also integrated to roughly locate overall waterlogging areas with low computational burden. Second, in the MCROD module, we detect reference objects, e.g., cars and persons in waterlogging scenes. The positional relationship between water depths and reference objects serves as risk indicators for accurately evaluating waterlogging risk. Specifically, we incorporate switchable atrous convolution (SAC) into YOLOv5 to solve occlusions and varying scales problems in complex waterlogging scenes. Moreover, we construct a large-scale urban waterlogging dataset called UrbanWaterloggingRiskDataset (UWRDataset) with 6,351 images for waterlogging scene classification and 3,217 images for reference objects detection. Experimental results on the dataset show that our WaRENet outperforms all comparison methods. The waterlogging scene classification module achieves accuracy of 95.99%. The MCROD module obtains mAP of 54.9%, while maintaining a high processing speed of 70.04 fps. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",object detection; Urban waterlogging; waterlogging risk evaluation,Classification (of information); Image classification; Large datasets; Activation mapping; Computational burden; Detection modules; Objects detection; Reference objects; Risk evaluation; Risk levels; Scene classification; Urban waterlogging; Waterlogging risk evaluation; Object detection
Deep Network for Image Compressed Sensing Coding Using Local Structural Sampling,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193707970&doi=10.1145%2f3649441&partnerID=40&md5=e1a603f991c98afc980b994092e40045,"Existing image compressed sensing (CS) coding frameworks usually solve an inverse problem based on measurement coding and optimization-based image reconstruction, which still exist the following two challenges: (1) the widely used random sampling matrix, such as the Gaussian Random Matrix (GRM), usually leads to low measurement coding efficiency, and (2) the optimization-based reconstruction methods generally maintain a much higher computational complexity. In this article, we propose a new convolutional neural network based image CS coding framework using local structural sampling (dubbed CSCNet) that includes three functional modules: local structural sampling, measurement coding, and Laplacian pyramid reconstruction. In the proposed framework, instead of GRM, a new local structural sampling matrix is first developed, which is able to enhance the correlation between the measurements through a local perceptual sampling strategy. Besides, the designed local structural sampling matrix can be jointly optimized with the other functional modules during the training process. After sampling, the measurements with high correlations are produced, which are then coded into final bitstreams by the third-party image codec. Last, a Laplacian pyramid reconstruction network is proposed to efficiently recover the target image from the measurement domain to the image domain. Extensive experimental results demonstrate that the proposed scheme outperforms the existing state-of-the-art CS coding methods while maintaining fast computational speed. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Compressed sensing (CS); compressed sensing coding; convolutional neural network (CNN); local structural sampling; third-party image codec,Convolution; Convolutional neural networks; Image coding; Image reconstruction; Inverse problems; Laplace transforms; Matrix algebra; Network coding; Compressed sensing; Compressed sensing coding; Compressed-Sensing; Convolutional neural network; Image codecs; Local structural sampling; Third parties; Third-party image codec; Compressed sensing
DATRA-MIV: Decoder-Adaptive Tiling and Rate Allocation for MPEG Immersive Video,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193753284&doi=10.1145%2f3648371&partnerID=40&md5=a2b2f21e0b6ca8633816d45eb3ed2673,"The emerging immersive video coding standard moving picture experts group (MPEG) immersive video (MIV), which is ongoing standardization by MPEG-Immersive (MPEG-I) group, enables six degrees of freedom in a virtual reality environment that represents both natural and computer-generated scenes using multi-view video compression. The MIV eliminates the redundancy between multi-view videos and merges the residuals into multiple pictures, called an atlas. Thus, bitstreams with encoded atlases are generated and corresponding number of decoders are needed, which is challenging for the lightweight device with a single decoder. This article proposes a decoder-adaptive tiling and rate allocation method for MIV to overcome the challenge. First, the proposed method divides atlases into subpictures considering two aspects: (i) subpicture bitstream extracting and merging into one bitstream to use a single decoder and (ii) separation of each source view from the atlases for rate allocation. Second, the atlases are encoded by versatile video coding (VVC), using an extractable subpicture to divide the atlases into subpictures. Third, each subpicture bitstream is extracted, and asymmetric quality allocation for each subpictures is conducted by considering the residuals in the subpicture. Fourth, mixed-quality subpictures were merged by using the proposed bitstream merger. Fifth, the merged bitstream is decoded by using a single decoder. Finally, the viewing area of the user is synthesized by using the reconstructed atlases. Experimental results with the VVC test model (VTM) show that the proposed method achieves a 21.37% Bjøntegaard delta rate saving for immersive video peak signal-to-noise ratio and a 26.76% decoding runtime saving compared to the VTM anchor configuration. Moreover, it supports bitstreams for multiple decoders and single decoder without re-encoding, transcoding, or a substantial increase of the server-side storage. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",frame packing; MPEG immersive video; six degrees of freedom; Spatial computing,Binary sequences; Decoding; Degrees of freedom (mechanics); Image coding; Image compression; Mergers and acquisitions; Motion Picture Experts Group standards; Signal to noise ratio; Video signal processing; Virtual reality; Bitstreams; Frame packing; Immersive; Moving picture expert group immersive video; Moving pictures experts groups; Multiview video; Rate allocation; Six degrees of freedom; Spatial computing; Test models; Merging
Efficient Brain Tumor Segmentation with Lightweight Separable Spatial Convolutional Network,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193735078&doi=10.1145%2f3653715&partnerID=40&md5=e3f531a9a2fb340bc181c04744c71f0b,"Accurate and automated segmentation of lesions in brain MRI scans is crucial in diagnostics and treatment planning. Despite the significant achievements of existing approaches, they often require substantial computational resources and fail to fully exploit the synergy between low-level and high-level features. To address these challenges, we introduce the Separable Spatial Convolutional Network (SSCN), an innovative model that refines the U-Net architecture to achieve efficient brain tumor segmentation with minimal computational cost. SSCN integrates the PocketNet paradigm and replaces standard convolutions with depthwise separable convolutions, resulting in a significant reduction in parameters and computational load. Additionally, our feature complementary module enhances the interaction between features across the encoder-decoder structure, facilitating the integration of multi-scale features while maintaining low computational demands. The model also incorporates a separable spatial attention mechanism, enhancing its capability to discern spatial details. Empirical validations on standard datasets demonstrate the effectiveness of our proposed model, especially in segmenting small and medium-sized tumors, with only 0.27M parameters and 3.68 GFlops. Our code is available at https://github.com/zzpr/SSCN.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Brain Tumor Segmentation; Separable Spatial Convolutional Network,Brain; Diagnosis; Image segmentation; Magnetic resonance imaging; Tumors; Automated segmentation; Brain MRI; Brain tumor segmentation; Computational resources; Convolutional networks; High-level features; Innovative models; MRI scan; Separable spatial convolutional network; Treatment planning; Convolution
Building Category Graphs Representation with Spatial and Temporal Attention for Visual Navigation,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193688420&doi=10.1145%2f3653714&partnerID=40&md5=50fa7b730eea0a652b592fa5e6a78c7d,"Given an object of interest, visual navigation aims to reach the object's location based on a sequence of partial observations. To this end, an agent needs to (1) acquire specific knowledge about the relations of object categories in the world during training and (2) locate the target object based on the pre-learned object category relations and its trajectory in the current unseen environment. In this article, we propose a Category Relation Graph (CRG) to learn the knowledge of object category layout relations and a Temporal-Spatial-Region attention (TSR) architecture to perceive the long-term spatial-temporal dependencies of objects, aiding navigation. We establish CRG to learn prior knowledge of object layout and deduce the positions of specific objects. Subsequently, we propose the TSR architecture to capture relationships among objects in temporal, spatial, and regions within observation trajectories. Specifically, we implement a Temporal attention module (T) to model the temporal structure of the observation sequence, implicitly encoding historical moving or trajectory information. Then, a Spatial attention module (S) uncovers the spatial context of the current observation objects based on CRG and past observations. Last, a Region attention module (R) shifts the attention to the target-relevant region. Leveraging the visual representation extracted by our method, the agent accurately perceives the environment and easily learns a superior navigation policy. Experiments on AI2-THOR demonstrate that our CRG-TSR method significantly outperforms existing methods in both effectiveness and efficiency. The supplementary material includes the code and will be publicly available.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",attention network; Object visual navigation; reinforcement learning; relation graph,Navigation; Network architecture; Reinforcement learning; Attention network; Learn+; Object based; Object categories; Object visual navigation; Objects-based; Reinforcement learnings; Relation graph; Spatial regions; Visual Navigation; Trajectories
MS-GDA: Improving Heterogeneous Recipe Representation via Multinomial Sampling Graph Data Augmentation,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193715457&doi=10.1145%2f3648620&partnerID=40&md5=33ac98c2e7d80a6e8a0e6b4b54531385,"We study the problem of classifying different cooking styles, based on the recipe. The difficulty is that the same food ingredients, seasoning, and the very similar instructions result in different flavors, with different cooking styles. Existing methods have limitations: they mainly focus on homogeneous data (e.g., instruction or image), ignoring heterogeneous data (e.g., flavor compound or ingredient), which certainly hurts the classification performance. This is because collecting enough available heterogeneous data of a recipe is a non-trivial task. In this paper, we present a new heterogeneous data augmentation method to improve classification performance. Specifically, we first construct a heterogeneous recipe graph network to represent heterogeneous data, which includes four main-stream types of heterogeneous data: ingredient, flavor compound, image, and instruction. Then, we draw a sequence of augmented graphs for Semi-Supervised learning through multinomial sampling. The probability distribution of sampling depends on the Cosine distance between the nodes of graph. In this way, we name our approach as Multinomial Sampling Graph Data Augmentation (MS-GDA). Extensive experiments demonstrate that MS-GDA significantly outperforms SOTA baselines on cuisine classification and region prediction with the recipe benchmark dataset. Code is available at https://github.com/LiangzheChen/MS-GDA. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",graph neural network; heterogeneous representation; network embedding; recipe data augmentation,Cooking; Flavor compounds; Graph neural networks; Machine learning; Probability distributions; Classification performance; Data augmentation; Food ingredients; Graph data; Graph neural networks; Heterogeneous data; Heterogeneous representation; Multinomials; Network embedding; Recipe data augmentation; Classification (of information)
Learning Scene Representations for Human-assistive Displays Using Self-attention Networks,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193684992&doi=10.1145%2f3650111&partnerID=40&md5=7ca0ccfca65dc32e4b070ff9db362a04,"Video-see-through (VST) augmented reality (AR) is widely used to present novel augmentative visual experiences by processing video frames for viewers. Among VST AR systems, assistive vision displays aim to compensate for low vision or blindness, presenting enhanced visual information to support activities of daily living for the vision impaired/deprived. Despite progress, current assistive displays suffer from a visual information bottleneck, limiting their functional outcomes compared to healthy vision. This motivates the exploration of methods to selectively enhance and augment salient visual information. Traditionally, vision processing pipelines for assistive displays rely on hand-crafted, single-modality filters, lacking adaptability to time-varying and environment-dependent needs. This article proposes the use of Deep Reinforcement Learning (DRL) and Self-attention (SA) networks as a means to learn vision processing pipelines for assistive displays. SA networks selectively attend to task-relevant features, offering a more parameter - and compute-efficient approach to RL-based task learning. We assess the feasibility of using SA networks in a simulation-trained model to generate relevant representations of real-world states for navigation with prosthetic vision displays. We explore two prosthetic vision applications, vision-to-auditory encoding, and retinal prostheses, using simulated phosphene visualisations. This article introduces SA-px, a general-purpose vision processing pipeline using self-attention networks, and SA-phos, a display-specific formulation targeting low-resolution assistive displays. We present novel scene visualisations derived from SA image patches importance rankings to support mobility with prosthetic vision devices. To the best of our knowledge, this is the first application of self-attention networks to the task of learning vision processing pipelines for prosthetic vision or assistive displays. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep reinforcement learning; display algorithms; prosthetic vision; Vision processing,Augmented reality; Deep learning; Pipeline processing systems; Pipelines; Prosthetics; Visualization; Assistive; Deep reinforcement learning; Display algorithm; Prosthetic vision; Reinforcement learnings; Scene representation; Video frame; Vision processing; Visual experiences; Visual information; Reinforcement learning
Instance-level Adversarial Source-free Domain Adaptive Person Re-identification,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193689373&doi=10.1145%2f3649900&partnerID=40&md5=60287fcd12a27bfa6a8f01f77336d2d3,"Domain adaption (DA) for person re-identification (ReID) has attained considerable progress by transferring knowledge from a source domain with labels to a target domain without labels. Nonetheless, most of the existing methods require access to source data, which raises privacy concerns. Source-free DA has recently emerged as a response to these privacy challenges, yet its direct application to open-set pedestrian re-identification tasks is hindered by the reliance on a shared category space in existing methods. Current source-free DA approaches for person ReID still encounter several obstacles, particularly the divergence-agnostic problem and the notable domain divergence due to the absent source data. In this article, we introduce an Instance-level Adversarial Mutual Teaching (IAMT) framework, which utilizes adversarial views to tackle the challenges mentioned above. Technically, we first elaborately develop a variance-based division (VBD) module to segregate the target data into instance-level subsets based on their similarity and dissimilarity to the source using the source-trained model, implicitly tackling the divergence-agnostic problem. To mitigate domain divergence, we additionally introduce a dynamic adversarial alignment (DAA) strategy, aiming to enhance the consistence of feature distribution across domains by employing adversarial instances from the target data to confuse the discriminators. Experiments reveal the superiority of the IAMT over state-of-the-art methods for DA person ReID tasks, while preserving the privacy of the source data. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adversarial attacks; mutual teaching; Person re-identification; source-free domain adaptation,Adversarial attack; Domain adaptation; Domain adaptions; Mutual teaching; Pedestrian re-identification; Person re identifications; Privacy concerns; Source data; Source-free domain adaptation; Target domain; Data privacy
Robust Image Hashing via CP Decomposition and DCT for Copy Detection,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193697576&doi=10.1145%2f3650112&partnerID=40&md5=2616bd3cdc2a2ece06fd7670192abd03,"Copy detection is a key task of image copyright protection. This article proposes a robust image hashing algorithm by CP decomposition and discrete cosine transform (DCT) for copy detection. The first contribution is the third-order tensor construction with low-frequency coefficients in the DCT domain. Since the low-frequency DCT coefficients contain most of the image energy, they can reflect the basic visual content of the image and are less disturbed by noise. Hence, the third-order tensor construction with the low-frequency DCT coefficients can ensure robustness of our algorithm. Another contribution is the application of the CP decomposition to the third-order tensor for learning a short binary hash. As the factor matrices learned from the CP decomposition can preserve the topology of the original tensor, the binary hash derived from the factor matrices can reach good discrimination. Lots of experiments and comparisons are done to validate effectiveness and advantage of our algorithm. The results demonstrate that our algorithm has superior classification and copy detection performances than several baseline algorithms. In addition, our algorithm is also better than some baseline algorithms with regard to hash length and computational time. © 2024 Copyright held by the owner/author(s).",copy detection; CP decomposition; dimensionality reduction; image hashing; tensor construction,Copyrights; Discrete cosine transforms; Image compression; Copy detection; CP decomposition; Dimensionality reduction; Discrete cosine transform coefficients; Image copyright protection; Image hashing; Lower frequencies; matrix; Tensor construction; Third-order tensors; Tensors
Enhanced Video Super-Resolution Network towards Compressed Data,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193718316&doi=10.1145%2f3651309&partnerID=40&md5=8adc50f867900122c7ea10ac0c4f274f,"Video super-resolution (VSR) algorithms aim at recovering a temporally consistent high-resolution (HR) video from its corresponding low-resolution (LR) video sequence. Due to the limited bandwidth during video transmission, most available videos on the internet are compressed. Nevertheless, few existing algorithms consider the compression factor in practical applications. In this paper, we propose an enhanced VSR model towards compressed videos, termed as ECVSR, to simultaneously achieve compression artifacts reduction and SR reconstruction end-to-end. ECVSR contains a motion-excited temporal adaption network (METAN) and a multi-frame SR network (SRNet). The METAN takes decoded LR video frames as input and models inter-frame correlations via bidirectional deformable alignment and motion-excited temporal adaption, where temporal differences are calculated as motion prior to excite the motion-sensitive regions of temporal features. In SRNet, cascaded recurrent multi-scale blocks (RMSB) are employed to learn deep spatio-temporal representations from adapted multi-frame features. Then, we build a reconstruction module for spatio-temporal information integration and HR frame reconstruction, which is followed by a detail refinement module for texture and visual quality enhancement. Extensive experimental results on compressed videos demonstrate the superiority of our method for compressed VSR. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Compressed video super-resolution; motion-excited temporal adaption; multi-frame SR network; video quality enhancement,Image coding; Image communication systems; Optical resolving power; Textures; Compressed datum; Compressed video; Compressed video super-resolution; High resolution; Motion-excited temporal adaption; Multi-frame; Multi-frame SR network; Super resolution algorithms; Video quality enhancements; Video super-resolution; Image compression
An Optimal Edge-weighted Graph Semantic Correlation Framework for Multi-view Feature Representation Learning,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193732789&doi=10.1145%2f3649466&partnerID=40&md5=0a72b344d718ca73d5a63ac72700cb89,"In this article, we present an optimal edge-weighted graph semantic correlation (EWGSC) framework for multi-view feature representation learning. Different from most existing multi-view representation methods, local structural information and global correlation in multi-view feature spaces are exploited jointly in the EWGSC framework, leading to a new and high-quality multi-view feature representation. Specifically, a novel edge-weighted graph model is first conceptualized and developed to preserve local structural information in each of the multi-view feature spaces. Then, the explored structural information is integrated with a semantic correlation algorithm, labeled multiple canonical correlation analysis (LMCCA), to form a powerful platform for effectively exploiting local and global relations across multi-view feature spaces jointly. We then theoretically verified the relation between the upper limit on the number of projected dimensions and the optimal solution to the multi-view feature representation problem. To validate the effectiveness and generality of the proposed framework, we conducted experiments on five datasets of different scales, including visual-based (University of California Irvine (UCI) iris database, Olivetti Research Lab (ORL) face database, and Caltech 256 database), text-image-based (Wiki database), and video-based (Ryerson Multimedia Lab (RML) audio-visual emotion database) examples. The experimental results show the superiority of the proposed framework on multi-view feature representation over state-of-the-art algorithms. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",data visualization; emotion recognition; face recognition; graph model; Multi-view feature representation; object recognition; semantic correlation; text-image recognition,Character recognition; Data visualization; Database systems; Emotion Recognition; Graphic methods; Object recognition; Semantics; Speech recognition; Edge-weighted graph; Emotion recognition; Feature representation; Graph model; Multi-view feature representation; Multi-views; Objects recognition; Semantic correlation; Text images; Text-image recognition; Face recognition
ReFID: Reciprocal Frequency-aware Generalizable Person Re-identification via Decomposition and Filtering,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193699148&doi=10.1145%2f3643684&partnerID=40&md5=52ccdc2f71285e9e3ce1b4742a93d593,"Domain generalization of person re-identification aims to conduct testing across domains that have not been previously encountered, without utilizing target domain data during the training stage. As the number of source domains increases, the relationships between training samples become more complex. This can lead to domain-invariant features that include certain instance-level spurious correlations, which can impact the model's ability to generalize further. To overcome this limitation, the Reciprocal Frequency-aware Generalizable Person Re-identification method is proposed in this article, which aims to utilize spectral feature correlation learning to transmit frequency component information and generate more discriminative hybrid features. A module called Bilateral Frequency Component-guided Attention is developed to help the network understand high-level semantic and texture information from various frequency features. Furthermore, to reduce the impact of noise from the frequency domain, this article proposes an innovative module called Fourier Noise Masquerade Filtering. This module enhances the portability of frequency domain components while simultaneously suppressing elements that do not contribute to generalization. Extensive experimental results on various datasets demonstrate that our method is effective and superior to the state-of-the-art methods.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Domain generalization; frequency domain learning; person re-identification,Semantics; Textures; Domain generalization; Domain learning; Frequency components; Frequency domain learning; Frequency domains; Generalisation; Number of sources; Person re identifications; Target domain; Training sample; Frequency domain analysis
Delay Threshold for Social Interaction in Volumetric eXtended Reality Communication,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193722458&doi=10.1145%2f3651164&partnerID=40&md5=c89f6b907bda2e46a6316fc388c33d28,"Immersive technologies like eXtended Reality (XR) are the next step in videoconferencing. In this context, understanding the effect of delay on communication is crucial. This article presents the first study on the impact of delay on collaborative tasks using a realistic Social XR system. Specifically, we design an experiment and evaluate the impact of end-to-end delays of 300, 600, 900, 1,200, and 1,500 ms on the execution of a standardized task involving the collaboration of two remote users that meet in a virtual space and construct block-based shapes. To measure the impact of the delay in this communication scenario, objective and subjective data were collected. As objective data, we measured the time required to execute the tasks and computed conversational characteristics by analyzing the recorded audio signals. As subjective data, a questionnaire was prepared and completed by every user to evaluate different factors such as overall quality, perception of delay, annoyance using the system, level of presence, cybersickness, and other subjective factors associated with social interaction. The results show a clear influence of the delay on the perceived quality and a significant negative effect as the delay increases. Specifically, the results indicate that the acceptable threshold for end-to-end delay should not exceed 900 ms. This article additionally provides guidelines for developing standardized XR tasks for assessing interaction in Social XR environments. © 2024 Copyright held by the owner/author(s).",delay; eXtended reality; volumetric Social XR,Quality control; Collaborative tasks; Delay; Effect of delays; End to end delay; Extended reality; Immersive technologies; Remote users; Social interactions; Volumetric social XR; Volumetrics; Video conferencing
Continuous Image Outpainting with Neural ODE,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193687841&doi=10.1145%2f3648367&partnerID=40&md5=59afa4c6f960812afc7540606d0ba591,"Generalised image outpainting is an important and active research topic in computer vision, which aims to extend appealing content all-side around a given image. Existing state-of-the-art outpainting methods often rely on discrete extrapolation to extend the feature map in the bottleneck. They thus suffer from content unsmoothness, especially in circumstances where the outlines of objects in the extrapolated regions are incoherent with the input sub-images. To mitigate this issue, we design a novel bottleneck with Neural ODEs to make continuous extrapolation in latent space, which could be a plug-in for many deep learning frameworks. Our ODE-based network continuously transforms the state and makes accurate predictions by learning the incremental relationship among latent points, leading to both smooth and structured feature representation. Experimental results on three real-world datasets both applied on transformer-based and CNN-based frameworks show that our methods could generate more realistic and coherent images against the state-of-the-art image outpainting approaches. Our code is available at https://github.com/PengleiGao/Continuous-Image-Outpainting-with-Neural-ODE. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Image outpainting; neural ODE; transformer; u-shaped structure,Deep learning; Ordinary differential equations; Feature map; Image outpainting; Neural ODE; Research topics; Shaped structures; State of the art; Subimages; Transformer; U-shaped; U-shaped structure; Extrapolation
Perceptual Quality-Oriented Rate Allocation via Distillation from End-to-End Image Compression,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193743480&doi=10.1145%2f3650034&partnerID=40&md5=5f30a56b8fc859daac9a2e128a189bea,"Mainstream image/video coding standards, exemplified by the state-of-the-art H.266/VVC, AVS3, and AV1, follow the block-based hybrid coding framework. Due to the block-based framework, encoders designed for these standards are easily optimized for peak signal-to-noise ratio (PSNR) but have difficulties optimizing for the metrics more aligned to perceptual quality, e.g., multi-scale structural similarity (MS-SSIM), since these metrics cannot be accurately evaluated at the small block level. We address this problem by leveraging inspiration from the end-to-end image compression built on deep networks, which is easily optimized through network training for any metric as long as the metric is differentiable. We compared the trained models using the same network structure but different metrics and observed that the models allocate rates in different ratios. We then propose a distillation method to obtain the rate allocation rule from end-to-end image compression models with different metrics and to utilize such a rule in the block-based encoders. We implement the proposed method on the VVC reference software - VTM and the AVS3 reference software - HPM, focusing on intraframe coding. Experimental results show that the proposed method on top of VTM achieves more than 10% BD-rate reduction than the anchor when evaluated with MS-SSIM or LPIPS, which leads to concrete perceptual quality improvement. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",AVS3; block-based coding; end-to-end image compression; H.266/VVC; perceptual quality; rate allocation.,Distillation; Image coding; Image quality; Quality control; Signal encoding; Signal to noise ratio; AVS3; Block based; Block-based coding; End to end; End-to-end image compression; H.266/VVC; Images compression; Perceptual quality; Rate allocation; Rate allocation.; Image compression
MIS: A Multi-Identifier Management and Resolution System in the Metaverse,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181993180&doi=10.1145%2f3597641&partnerID=40&md5=41440d0d54c2e6ae0a1e35cef204978d,"The metaverse gradually evolves into a virtual world containing a series of interconnected sub-metaverses. Diverse digital resources, including identities, contents, services, and supporting data, are key components of the sub-metaverse. Therefore, a Domain Name System (DNS)-like system is necessary for efficient management and resolution. However, the legacy DNS was designed with security vulnerabilities and trust risks due to centralized issues. Blockchain is used to mitigate these concerns due to its decentralized features. Additionally, it supports identity management as a default feature, making it a natural fit for the metaverse. While there are several DNS alternatives based on the blockchain, they either manage only a single type of identifiers or isolate identities from other sorts of identifiers, making it difficult for sub-metaverses to coexist and connect with each other. This article proposes a Multi-Identifier management and resolution System (MIS) in the metaverse, supporting the registration, resolution, and inter-translation functions. The basic MIS is portrayed as a four-tier architecture on a consortium blockchain due to its manageability, enhanced security, and efficiency properties. On-chain data is lightweight and compressed to save on storage while accelerating reading and writing operations. The resource data is encrypted based on the attributes of the sub-metaverse in the storage tier for privacy protection and access control. For users with decentralization priorities, a modification named EMIS is built on top of Ethereum. Finally, MIS is implemented on two testbeds and is available online as the open-source system. The first testbed consists of 4 physical servers located in the UK and Malaysia while the second is made up of 200 virtual machines (VMs) spread over 26 countries across all 5 continents on Google Cloud. Experiments indicate that MIS provides efficient reading and writing performance than the legacy DNS and other public blockchain-based workarounds including EMIS and Ethereum Name Service (ENS).  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",blockchain; Domain Name System; identifier management; Metaverse,Access control; Digital storage; Ethereum; Open systems; Testbeds; Block-chain; Content services; Digital resources; Domain name system; Efficient managements; Identifier management; Management systems; Metaverses; Resolution systems; Virtual worlds; Blockchain
RAC-Chain: An Asynchronous Consensus-based Cross-chain Approach to Scalable Blockchain for Metaverse,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193716956&doi=10.1145%2f3586011&partnerID=40&md5=3b4d73be237b375a76a0075de0466933,"The metaverse, as an emerging technical term, conceptually aims to construct a virtual digital space that runs parallel to the physical world. Due to human behaviors and interactions being represented in the virtual world, security in the metaverse is a challenging issue in which the traditional centralized service model is one of the threat sources. To conquer the obstacle caused by centralized computing, blockchain-based solutions are potential problem-solving methods. However, it is difficult for a single blockchain to support large-scale data and business services in the metaverse, due to the scalability restrictions. Moreover, multi-chain settings also encounter the interoperability issues. In this work, we propose a Relay chain and Asynchronous consensus-based Consortium blockchain cross-Chain model, which realizes message transmission and cross-chain transactions in multiple chains by adopting the relay chain and cross-chain gateways. All nodes of the application chains and the relay chain execute cross-chain transactions in sequence and reach a consensus on transactions at any transmission delay. Our experiment evaluations demonstrate that our approach performs well in atomicity, security, and functionality (cross-chain transactions), such that the performance of blockchain scalability in the metaverse can be improved, compared with the traditional relay chain schemes. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",asynchronous consensus; blockchain interoperability; blockchain scalability; cross-chain technology; Relay chain,Behavioral research; Blockchain; Interoperability; Virtual reality; Asynchronous consensus; Block-chain; Blockchain interoperability; Blockchain scalability; Centralised; Cross-chain technology; Metaverses; Relay chain; Technical terms; Scalability
Multimodal Visual-Semantic Representations Learning for Scene Text Recognition,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193707049&doi=10.1145%2f3646551&partnerID=40&md5=aeb52f8c529ddb2a32699201e761899f,"Scene Text Recognition (STR), the critical step in OCR systems, has attracted much attention in computer vision. Recent research on modeling textual semantics with Language Model (LM) has witnessed remarkable progress. However, LM only optimizes the joint probability of the estimated characters generated from the Vision Model (VM) in a single language modality, ignoring the visual-semantic relations in different modalities. Thus, LM-based methods can hardly generalize well to some challenging conditions, in which the text has weak or multiple semantics, arbitrary shape, and so on. To migrate the above issue, in this paper, we propose Multimodal Visual-Semantic Representations Learning for Text Recognition Network (MVSTRN) to reason and combine the multimodal visual-semantic information for accurate Scene Text Recognition. Specifically, our MVSTRN builds a bridge between vision and language through its unified architecture and has the ability to reason visual semantics by guiding the network to reconstruct the original image from the latent text representation, breaking the structural gap between vision and language. Finally, the tailored multimodal Fusion (MMF) module is motivated to combine the multimodal visual and textual semantics from VM and LM to make the final predictions. Extensive experiments demonstrate our MVSTRN achieves state-of-the-art performance on several benchmarks.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Scene text recognition; self-supervised learning; vision transformer,Benchmarking; Computer vision; Modeling languages; Network architecture; Optical character recognition; Supervised learning; Visual languages; Language model; Multi-modal; Scene Text; Scene text recognition; Self-supervised learning; Semantic representation; Text recognition; Vision model; Vision transformer; Visual semantics; Semantics
Scene Graph Lossless Compression with Adaptive Prediction for Objects and Relations,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193687637&doi=10.1145%2f3649503&partnerID=40&md5=f8902145badd7412cec1ea1f571b9adb,"The scene graph is a novel data structure describing objects and their pairwise relationship within image scenes. As the size of scene graphs in vision and multimedia applications increases, the need for lossless storage and transmission of such data becomes more critical. However, the compression of scene graphs is less studied because of the complicated data structures involved and complex distributions. Existing solutions usually involve general-purpose compressors or graph structure compression methods, which are weak at reducing the redundancy in scene graph data. This article introduces a novel lossless compression framework with adaptive predictors for the joint compression of objects and relations in scene graph data. The proposed framework comprises a unified prior extractor and specialized element predictors to adapt to different data elements. Furthermore, to exploit the context information within and between graph elements, Graph Context Convolution is proposed to support different graph context modeling schemes for different graph elements. Finally, an overarching framework incorporates the learned distribution model to predict numerical data under complicated conditional constraints. Experiments conducted on labeled or generated scene graphs demonstrate the effectiveness of the proposed framework for scene graph lossless compression. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",image compression; lossless compression; Scene graph compression,Digital storage; Graphic methods; Image compression; Adaptive predictions; Graph compressions; Graph data; Image scene; Images compression; Lossless compression; Multimedia applications; Scene graph compression; Scene-graphs; Vision applications; Data structures
A Privacy-preserving Auction Mechanism for Learning Model as an NFT in Blockchain-driven Metaverse,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175684546&doi=10.1145%2f3599971&partnerID=40&md5=80af295b7dfcb30de5ee7e65c34fbfdf,"The Metaverse, envisioned as the next-generation Internet, will be constructed via twining a practical world in a virtual form, wherein Meterverse service providers (MSPs) are required to collect massive data from Meterverse users (MUs). In this regard, a critical demand exists for MSPs to motivate MUs to contribute computing resources and data while preserving user privacy. Federated learning (FL), as a privacy-preserving collaborative machine learning paradigm, can support distributed intensive computation in the Metaverse. In this work, we first investigate minting the machine learning models into NFT with FL assistance (referred to as FL-NFT), such that MUs as stakeholders can control the ownership and share the economic value of user-generated content (UGC). Specifically, MUs are encouraged to establish a decentralized autonomous organization (i.e., MU-DAO) to aggregate local models and mint FL-NFT. MUs and MSPs optimize the strategies by formulating an imperfect information Stackelberg game to trade off the cost and benefit. We apply the backward induction to derive the equilibrium solution. Then, we construct a privacy-preserving multi-winner sealed-bid auction mechanism (PMS-AM), in which the Hidden Markov Model assists MSPs in choosing rational bidding strategies according to historical bids, and the double auction mechanism determines the winners and price of FL-NFT. Finally, the numerical results based on theoretical analysis and simulations demonstrate that the proposed PMS-AM can increase the quality of FL-NFT and achieve the economic properties of incentive mechanisms such as individual rationality and incentive compatibility.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",auction mechanism; blockchain; federated learning; HMM; Metaverse; NFT; Stackelberg game,Blockchain; Economic and social effects; Machine learning; Privacy-preserving techniques; Quality control; Auctions mechanisms; Block-chain; Federated learning; HMM; Metaverses; NFT; Privacy preserving; Sealed-bid auctions; Service provider; Stackelberg Games; Hidden Markov models
MultiMatch: Multi-task Learning for Semi-supervised Domain Generalization,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189466918&doi=10.1145%2f3648680&partnerID=40&md5=97cd9373801bf55430a3a397e6f21336,"Domain generalization (DG) aims at learning a model on source domains to well generalize on the unseen target domain. Although it has achieved great success, most of the existing methods require the label information for all training samples in source domains, which is time-consuming and expensive in the real-world application. In this article, we resort to solving the semi-supervised domain generalization (SSDG) task, where there are a few label information in each source domain. To address the task, we first analyze the theory of multi-domain learning, which highlights that (1) mitigating the impact of domain gap and (2) exploiting all samples to train the model can effectively reduce the generalization error in each source domain so as to improve the quality of pseudo-labels. According to the analysis, we propose MultiMatch, i.e., extending FixMatch to the multi-task learning framework, producing the high-quality pseudo-label for SSDG. To be specific, we consider each training domain as a single task (i.e., local task) and combine all training domains together (i.e., global task) to train an extra task for the unseen test domain. In the multi-task framework, we utilize the independent batch normalization and classifier for each task, which can effectively alleviate the interference from different domains during pseudo-labeling. Also, most of the parameters in the framework are shared, which can be trained by all training samples sufficiently. Moreover, to further boost the pseudo-label accuracy and the model's generalization, we fuse the predictions from the global task and local task during training and testing, respectively. A series of experiments validate the effectiveness of the proposed method, and it outperforms the existing semi-supervised methods and the SSDG method on several benchmark DG datasets.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesMulti-task learning; domain generalization; semi-supervised learning,Learning systems; Supervised learning; Additional key word and phrasesmulti-task learning; Domain generalization; Generalisation; Key words; Label information; Multitask learning; Semi-supervised; Semi-supervised learning; Task learning; Training sample; Sampling
Detection of Adversarial Facial Accessory Presentation Attacks Using Local Face Differential,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193681266&doi=10.1145%2f3643831&partnerID=40&md5=ed2c25f960889ac124ebf06c56135819,"To counter adversarial facial accessory presentation attacks (PAs), a detection method based on local face differential is proposed in this article. It extracts the local face differential features from a suspected face image and a reference face image, and then adaptively fuses the differential features of different local face regions to detect adversarial facial accessory PAs. Meanwhile, the principle of the proposed method is explained by theoretically investigating the local facial differences between a bona fide presentation and an adversarial facial accessory PA when they are compared with a reference face image. To evaluate the proposed method, this article builds a database with different adversarial examples (AEs), presentation attack instruments (PAIs), illumination conditions, and cameras. The experimental results show that it can effectively distinguish between adversarial facial accessory PAs and bona fide presentations, and it has good generalization ability to unseen AEs, PAIs, illumination conditions, and cameras. Moreover, it outperforms the existing AE detection and presentation attack detection methods in detecting adversarial facial accessory PAs.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adversarial examples; differential; Face verification; presentation attack detection; presentation attacks,Face recognition; Adversarial example; Attack detection; Detection methods; Differential; Face images; Face regions; Face Verification; Illumination conditions; Presentation attack; Presentation attack detection; Cameras
QuickCSGModeling: Quick CSG Operations Based on Fusing Signed Distance Fields for VR Modeling,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193733071&doi=10.1145%2f3599729&partnerID=40&md5=4975aa72633e527dee26f0f6b941b6ec,"The latest advancements in Virtual Reality (VR) enable the creation of 3D models within a holographic immersive simulation environment. In this article, we create QuickCSGModeling, a user-friendly mid-air interactive modeling system. We first prepare a dataset consisting of diverse components and precompute the discrete signed distance function (SDF) for each component. During the modeling phase, users can freely design complicated shapes with a pair of VR controllers. Based on the discrete SDF representation, any CSG-like operation (union, intersection, and subtraction) can be performed voxel-wisely. Also, we maintain a single dynamic SDF for the whole scene, whose zero-level set surface exactly encodes the most recent constructed shape. Both SDF fusion and surface extraction are implemented via GPU for a smooth user experience. A total of 34 volunteers were asked to create their favorite models using QuickCSGModeling. With a simple training, most of them can create a fascinating shape or even a descriptive scene quickly. We also discuss how to extend our system to create articulated models with hinges, where an adaptive cube subdivision has to be enforced to improve the reconstruction accuracy around the hinge part, followed by a Dual Contouring-based surface extraction.1 © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",boolean operation; modeling; SDF fusion; Virtual reality,Boolean functions; Extraction; 3D models; 3d-modeling; Boolean operations; CSG operations; Modeling; Signed distance fields; Signed distance function; Signed distance function fusion; Surface extraction; Virtual reality modeling; Virtual reality
HCNCT: A Cross-chain Interaction Scheme for the Blockchain-based Metaverse,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193756409&doi=10.1145%2f3594542&partnerID=40&md5=ca42d0ff36bef0265a0970fc1e23d844,"As a new type of digital living space that blends virtual and reality, Metaverse combines many emerging technologies. It provides an immersive experience based on VR technology and stores and protects users' digital content and digital assets through blockchain technology. However, different virtual environments are often highly heterogeneous in terms of underlying architecture and software implementation technology, which leads to many challenges in scalability and interoperability for blockchains serving the Metaverse. Cross-chain technology is an essential technology to realize the scalability and interoperability of blockchain. However, the current cross-chain technologies all have their own merits and demerits, and there is no cross-chain solution that can be fully applied to any scenario. To this end, in the blockchain-based Metaverse, this article proposes a cross-chain transaction scheme based on improved hash timelock, HCNCT. By combining the notary mechanism, this scheme uses a group of notaries to supervise and participate in cross-chain transactions, effectively solving the problem that malicious users create a large number of time-out transactions to block the transaction channel, which exists in the traditional hash timelock method. Besides, this article uses the verifiable secret sharing method in the notary group, which can effectively prevent the centralization problem of the notary mechanism. Moreover, this article discusses the process of key processing, cross-chain transaction and transaction verification of the scheme, and designs the user credibility evaluation mechanism, which can effectively reduce the occurrence of malicious default of users. Compared with existing solutions, our solution has the advantage of effectively addressing time-out transaction attacks and centralization issues while guaranteeing security. The experiments also verify the effectiveness of the proposed scheme. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",blockchain; blockchain interoperability; cross-chain; Hash timelock contract; Metaverse; notary mechanism,Blockchain; Scalability; Virtual reality; Block-chain; Blockchain interoperability; Centralisation; Chain interactions; Cross-chain; Hash timelock contract; Metaverses; Notary mechanism; Time-outs; Interoperability
Jointly Harnessing Prior Structures and Temporal Consistency for Sign Language Video Generation,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189454373&doi=10.1145%2f3648368&partnerID=40&md5=c11e1d6c4f06715094f585b38d25572a,"Sign language provides a way for differently-abled individuals to express their feelings and emotions. However, learning sign language can be challenging and time consuming. An alternative approach is to animate user photos using sign language videos of specific words, which can be achieved using existing image animation methods. However, the finger motions in the generated videos are often not ideal. To address this issue, we propose the Structure-aware Temporal Consistency Network (STCNet), which jointly optimizes the prior structure of humans with temporal consistency to produce sign language videos. We use a fine-grained skeleton detector to acquire knowledge of body structure and introduce both short- and long-term cycle loss to ensure the continuity of the generated video. The two losses and keypoint detector network are optimized in an end-to-end manner. Quantitative and qualitative evaluations on three widely used datasets, namely LSA64, Phoenix-2014T, and WLASL-2000, demonstrate the effectiveness of the proposed method. It is our hope that this work can contribute to future studies on sign language production.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSign language; jointly training; motion transfer; video generation,Additional key word and phrasessign language; Fine grained; Finger motion; Jointly training; Key words; Motion transfer; Sign language; Structure-aware; Temporal consistency; Video generation
Region-Focused Network for Dense Captioning,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189484428&doi=10.1145%2f3648370&partnerID=40&md5=1f9169cc6981576cb603552c924085e0,"Dense captioning is a very critical but under-explored task, which aims to densely detect localized regions-of-interest (RoIs) and describe them with natural language in a given image. Although recent studies tried to fuse multi-scale features from different visual instances to generate more accurate descriptions, their methods still suffer from the lack of exploration of relation semantic information in images, leading to less informative descriptions. Furthermore, indiscriminately fusing all visual instance features will introduce redundant information, resulting in poor matching between descriptions and corresponding regions. In this work, we propose a Region-Focused Network (RFN) to address these issues. Specifically, to fully comprehend the images, we first extract the object-level features, and encode the interaction and position relations between objects to enhance the object representations. Then, to decrease the interference from redundant information about the target region, we extract the most relevant information to the region. Finally, a region-based Transformer is employed to compose and align the previous mined information and generate the corresponding descriptions. Extensive experiments on Visual Genome V1.0 and V1.2 datasets show that our RFN model outperforms the state-of-the-art methods, thus verifying its effectiveness. Our code is available at https://github.com/VILAN-Lab/DesCap.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesDense captioning; interaction relation; region-focus; transformer,Semantics; Additional key word and phrasesdense captioning; Interaction relation; Key words; Localised; Multi-scale features; Natural languages; Region-focus; Region-of-interest; Regions of interest; Transformer; Image enhancement
Tensorial Evolutionary Optimization for Natural Image Matting,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193719671&doi=10.1145%2f3649138&partnerID=40&md5=c52c7041c02003a478e0c77f90789c2e,"Natural image matting has garnered increasing attention in various computer vision applications. The matting problem aims to find the optimal foreground/background (F/B) color pair for each unknown pixel and thus obtain an alpha matte indicating the opacity of the foreground object. This problem is typically modeled as a large-scale pixel pair combinatorial optimization (PPCO) problem. Heuristic optimization is widely employed to tackle the PPCO problem owing to its gradient-free property and promising search ability. However, traditional heuristic methods often encode F/B solutions to a one-dimensional (1D) representation and then evolve the solutions in a 1D manner. This 1D representation destroys the intrinsic two-dimensional (2D) structure of images, where the significant spatial correlations among pixels are ignored. Moreover, the 1D representation also brings operation inefficiency. To address the above issues, this article develops a spatial-aware tensorial evolutionary image matting (TEIM) method. Specifically, the matting problem is modeled as a 2D Spatial-PPCO (S-PPCO) problem, and a global tensorial evolutionary optimizer is proposed to tackle the S-PPCO problem. The entire population is represented as a whole by a third-order tensor, in which individuals are classified into two types: F and B individuals for denoting the 2D F/B solutions, respectively. The evolution process, consisting of three tensorial evolutionary operators, is implemented based on pure tensor computation for efficiently seeking F/B solutions. The local spatial smoothness of images is also integrated into the evaluation process for obtaining a high-quality alpha matte. Experimental results compared with state-of-the-art methods validate the effectiveness of TEIM. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",heuristic optimization; Natural image matting; tensorial evolutionary algorithm,Combinatorial optimization; Evolutionary algorithms; Heuristic methods; Tensors; Alpha mattes; Background solutions; Combinatorial optimization problems; Computer vision applications; Evolutionary optimizations; Foreground/background; Heuristic optimization; Image matting; Natural image matting; Tensorial evolutionary algorithm; Pixels
Joint Distortion Restoration and Quality Feature Learning for No-reference Image Quality Assessment,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193719033&doi=10.1145%2f3649899&partnerID=40&md5=4ddac8b90ec411998ab986f06d0d02ca,"No-reference image quality assessment (NR-IQA) methods, inspired by the free energy principle, improve the accuracy of image quality prediction by simulating the human brain's repair process for distorted images. However, existing methods use separate optimization schemes for distortion restoration and quality prediction, which undermines the accurate mapping of feature representations to quality scores. To address this issue, we propose a joint restoration and quality feature learning NR-IQA (RQFL-IQA) method to jointly tackle distortion image restoration and quality prediction within a unified framework. To accurately establish the quality reconstruction relationship between distorted and restored images, a hybrid loss function based on pixel-wise and structure-wise representations is used to improve the restoration capability of the image restoration network. The proposed RQFL-IQA exploits rich labels, including restored images and quality scores, to enable the model to learn more discriminative features and establish a more accurate mapping from feature representation to quality scores. In addition, to avoid the impact of poor restoration on quality prediction, we propose a module with a cleaning function to reweight the fusion of restored and primitive features to achieve more perceptual consistency in feature fusion. Experimental results on public IQA datasets show that the proposed RQFL-IQA is superior over existing methods. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",hybrid loss; Image quality assessment; joint optimization; multi-task learning; no-reference,Forecasting; Free energy; Image enhancement; Image quality; Image reconstruction; Machine learning; Mapping; Quality control; Accurate mapping; Feature learning; Hybrid loss; Image quality assessment; Joint optimization; Multitask learning; No-reference; No-reference images; Quality features; Quality prediction; Restoration
Semantics and Non-fungible Tokens for Copyright Management on the Metaverse and Beyond,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193758040&doi=10.1145%2f3585387&partnerID=40&md5=1eceb9ada6af4bcf7bdbf113c4fa3027,"Recent initiatives related to the Metaverse focus on better visualization, like augmented or virtual reality, but also persistent digital objects. To guarantee real ownership of these digital objects, open systems based on public blockchains and Non-Fungible Tokens (NFTs) are emerging together with a nascent decentralized and open creator economy. To manage this emerging economy in a more organized way, and fight the so common NFT plagiarism, we propose CopyrightLY, a decentralized application for authorship and copyright management. It provides means to claim content authorship, including supporting evidence. Content and metadata are stored in decentralized storage and registered on the blockchain. A token is used to curate these claims, and potential complaints, by staking it on them. Staking is incentivized by the fact that the token is minted using a bonding curve. The tokenomics include the resolution of complaints and enabling the monetization of curated claims. Monetization is achieved through licensing NFTs with metadata enhanced by semantic technologies. Semantic data makes explicit the reuse conditions transferred with the token while keeping the connection to the underlying copyright claims to improve the trustability of the NFTs. Moreover, the semantic metadata is flexible enough to enable licensing not just in the real world. Licenses can refer to reuses in specific locations in a metaverse, thus facilitating the emergence of creative economies in them. © 2024 Copyright held by the owner/author(s).",blockchain; copyright; Metaverse; Non-Fungible Token; ontology; social media,Blockchain; Digital storage; Metadata; Semantics; Social networking (online); Virtual reality; Block-chain; Decentralised; Digital Objects; Emerging economies; Metaverses; Non-fungible token; Ontology's; Reuse; Semantic technologies; Social media; Copyrights
Introduction to the Special Issue on Integrity of Multimedia and Multimodal Data in Internet of Things,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189443736&doi=10.1145%2f3643040&partnerID=40&md5=a69e8ded3db1fd4d9bfa1c2a439b55a3,[No abstract available],,
NSDIE: Noise Suppressing Dark Image Enhancement Using Multiscale Retinex and Low-Rank Minimization,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189433371&doi=10.1145%2f3638772&partnerID=40&md5=1237112772297d3f158d878e3db9ef23,"It is inevitable for dark images to have crucial information obscured by low-light conditions, which are worsened by the presence of noise in these images. This work introduces a groundbreaking solution, Noise-Suppressing Dark Image Enhancement for Web Apps (NSDIE), to address the challenging task of enhancing low-light images marred by noise. The proposed work utilizes a low-rank model with simultaneous enhancement of reflectance and illumination components to improve the nighttime scenes while also eradicating the present noise of the image. The reflectance component is further processed using a multiscale retinex model to compensate for the possible color distortions while the illumination component is enhanced using the camera response model to ensure the genuineness of the scene. The proposed work is also tested for a standalone application and is presented to the user through a web portal to aid the concerns of dark image enhancement in the daily life of the user. Rigorous quantitative and qualitative analyses assert NSDIE's superiority over existing techniques, establishing its pivotal role in addressing the critical concern of dark image enhancement.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesImage enhancement; camera response function; multiscale retinex model; nighttime scenes,Cameras; Portals; Reflection; Additional key word and phrasesimage enhancement; Camera response functions; Dark image; Key words; Multi-scale Retinex; Multiscale retinex model; Nighttime scene; Noise suppressing; Reflectance components; Web App; Image enhancement
Iterative Temporal-spatial Transformer-based Cardiac T1 Mapping MRI Reconstruction,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189322192&doi=10.1145%2f3643640&partnerID=40&md5=3bcb9284cbd86c0df7650377114a1a23,"The precise reconstruction of accelerated magnetic resonance imaging (MRI) brings about notable advantages, such as enhanced diagnostic precision and decreased examination costs. In contrast, traditional cardiac MRI necessitates repetitive acquisitions across multiple heartbeats, resulting in prolonged acquisition times. Significant strides have been made in accelerating MRI through deep learning-based reconstruction methods. However, these existing methods encounter certain limitations: (1) The intricate nature of heart reconstruction involving multiple complex time-series data poses a challenge in exploring nonlinear dependencies between temporal contexts. (2) Existing research often overlooks weight sharing in iterative frameworks, impeding the effective capturing of non-local information and, consequently, limiting improvements in model performance. In order to improve cardiac MRI reconstruction, we propose a novel temporal-spatial transformer with a strategy in this study. Based on the multi-level encoder and decoder transformer architecture, we conduct multi-level spatiotemporal information feature aggregation over several adjacent views, that create nonlinear dependencies among features and efficiently learn important information among adjacent cardiac temporal frames. Additionally, in order to improve contextual awareness between neighboring views, we add cross-view attention for temporal information fusion. Furthermore, we introduce an iterative strategy for training weights during the reconstruction process, which improves feature fusion in critical locations and reduces the number of computations required to calculate global feature dependencies. Extensive experiments have demonstrated the substantial superiority of this procedure over the most advanced techniques, suggesting that it has broad potential for clinical use.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesCardiac MRI reconstruction; multi-level; T1 mapping; temporal information; transformer,Deep learning; Heart; Iterative methods; Magnetic resonance imaging; Acquisition time; Additional key word and phrasescardiac magnetic resonance imaging reconstruction; Cardiac magnetic resonance imaging; Imaging reconstruction; Key words; Multilevels; Nonlinear dependencies; T1 mapping; Temporal information; Transformer; Mapping
GMS-3DQA: Projection-Based Grid Mini-patch Sampling for 3D Model Quality Assessment,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189239602&doi=10.1145%2f3643817&partnerID=40&md5=5a646508c38f39b5a92740cf179124eb,"Nowadays, most three-dimensional model quality assessment (3DQA) methods have been aimed at improving accuracy. However, little attention has been paid to the computational cost and inference time required for practical applications. Model-based 3DQA methods extract features directly from the 3D models, which are characterized by their high degree of complexity. As a result, many researchers are inclined towards utilizing projection-based 3DQA methods. Nevertheless, previous projection-based 3DQA methods directly extract features from multi-projections to ensure quality prediction accuracy, which calls for more resource consumption and inevitably leads to inefficiency. Thus, in this article, we address this challenge by proposing a no-reference (NR) projection-based Grid Mini-patch Sampling 3D Model Quality Assessment (GMS-3DQA) method. The projection images are rendered from six perpendicular viewpoints of the 3D model to cover sufficient quality information. To reduce redundancy and inference resources, we propose a multi-projection grid mini-patch sampling strategy (MP-GMS), which samples grid mini-patches from the multi-projections and forms the sampled grid mini-patches into one quality mini-patch map (QMM). The Swin-Transformer tiny backbone is then used to extract quality-aware features from the QMMs. The experimental results show that the proposed GMS-3DQA outperforms existing state-of-the-art NR-3DQA methods on the point cloud quality assessment databases for both accuracy and efficiency. The efficiency analysis reveals that the proposed GMS-3DQA requires far less computational resources and inference time than other 3DQA competitors. The code is available at https://github.com/zzc-1998/GMS-3DQA.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and Phrases3D model quality assessment; efficient; mini-patch; no-reference; projection-based,3D modeling; Three dimensional computer graphics; 3D models; 3d-modeling; Additional key word and phrases3d model quality assessment; Computational inferences; Efficient; Key words; Mini-patch; Model quality assessments; No-reference; Projection-based; Efficiency
Semantic-Consistency-guided Learning on Deep Features for Unsupervised Salient Object Detection,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189350439&doi=10.1145%2f3640816&partnerID=40&md5=eb0f5d2fb8f64241864633b4efb24063,"Unsupervised salient object detection is an important task in many real-world scenarios where pixel-wise label information is of scarce availability. Despite its significance, this problem remains rarely explored, with a few works that consider unsupervised salient object detection methods based on the fused graph from the sum fusion of multiple deep feature similarity matrices. However, these methods ignore the interrelation of the low-level feature similarity matrices and the high-level semantic similarity matrice, which degrades the quality of the fused graph. In this article, we propose a semantic-consistency-guided multi-graph fusion learning algorithm for unsupervised saliency detection, where the consistency and inconsistency between multiple low-level feature similarity matrices and the high-level semantic similarity matrice are explored to promote the robustness and quality of the fused graph. In the first stage, a semantic-consistency-guided multi-graph fusion learning method is proposed to exploit consistency and inconsistency of multiple low-level deep features and the high-level semantic feature. The semantic-consistency-guided similarity matrices are computed for preliminary saliency ranking. In the following saliency refinement stage, the semantic-enhanced similarity matrices are built by the cross diffusion to fuse the multiple low-level deep features and the high semantic deep feature. Based on the semantic-enhanced similarity matrices, the refinement saliency maps are calculated in a semantic-enhanced cellular automata manner. Furthermore, the final ensemble stage of the large margin semi-supervised classification views the preliminary ranking results and refinement results as features, adopts the large margin graphs for saliency ensemble. Extensive evaluations over four benchmark datasets show that the proposed unsupervised method performs favorably against the state-of-the-art approaches and is competitive with some supervised deep learning-based methods.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesMulti-graph fusion learning; consistency similarity matrics; large margin semi-supervised classification; semantic-enhanced cellular automata; semantic-guided consistent graphs,Cellular automata; Deep learning; Feature extraction; Learning algorithms; Learning systems; Object detection; Object recognition; Supervised learning; Additional key word and phrasesmulti-graph fusion learning; Cellular automatons; Consistency similarity matric; Key words; Large margin semi-supervised classification; Large margins; Matrics; Semantic-enhanced cellular automaton; Semantic-guided consistent graph; Semisupervised classification (SSC); Semantics
Detecting Post Editing of Multimedia Images using Transfer Learning and Fine Tuning,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189455382&doi=10.1145%2f3633284&partnerID=40&md5=9e5fcd5df9d51030ed0b11a41f758b6f,"In the domain of general image forgery detection, a myriad of different classification solutions have been developed to distinguish a ""tampered""image from a ""pristine""image. In this work, we aim to develop a new method to tackle the problem of binary image forgery detection. Our approach builds upon the extensive training that state-of-the-art image classification models have undergone on regular images from the ImageNet dataset, and transfers that knowledge to the image forgery detection space. By leveraging transfer learning and fine tuning, we can fit state-of-the-art image classification models to the forgery detection task. We train the models on a diverse and evenly distributed image forgery dataset. With five models - EfficientNetB0, VGG16, Xception, ResNet50V2, and NASNet-Large - we transferred and adapted pre-trained knowledge from ImageNet to the forgery detection task. Each model was fitted, fine-tuned, and evaluated according to a set of performance metrics. Our evaluation demonstrated the efficacy of large-scale image classification models - paired with transfer learning and fine tuning - at detecting image forgeries. When pitted against a previously unseen dataset, the best-performing model of EfficientNetB0 could achieve an accuracy rate of nearly 89.7%. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesMultimedia data integrity; fake news; fine tuning; image forgery; post editing; transfer learning,Binary images; Classification (of information); Fake detection; Learning systems; Additional key word and phrasesmultimedium data integrity; Data integrity; Fake news; Fine tuning; Image forgery; Image forgery detections; Images classification; Key words; Post-editing; Transfer learning; Image classification
Cross-Modal Attention Preservation with Self-Contrastive Learning for Composed Query-Based Image Retrieval,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189474381&doi=10.1145%2f3639469&partnerID=40&md5=0786d09b60aefb88337b93d36a9b99be,"In this article, we study the challenging cross-modal image retrieval task, Composed Query-Based Image Retrieval (CQBIR), in which the query is not a single text query but a composed query, i.e., a reference image, and a modification text. Compared with the conventional cross-modal image-text retrieval task, the CQBIR is more challenging as it requires properly preserving and modifying the specific image region according to the multi-level semantic information learned from the multi-modal query. Most recent works focus on extracting preserved and modified information and compositing it into a unified representation. However, we observe that the preserved regions learned by the existing methods contain redundant modified information, inevitably degrading the overall retrieval performance. To this end, we propose a novel method termed Cross-Modal Attention Preservation (CMAP). Specifically, we first leverage the cross-level interaction to fully account for multi-granular semantic information, which aims to supplement the high-level semantics for effective image retrieval. Furthermore, different from conventional contrastive learning, our method introduces self-contrastive learning into learning preserved information, to prevent the model from confusing the attention for the preserved part with the modified part. Extensive experiments on three widely used CQBIR datasets, i.e., FashionIQ, Shoes, and Fashion200k, demonstrate that our proposed CMAP method significantly outperforms the current state-of-the-art methods on all the datasets. The anonymous implementation code of our CMAP method is available at https://github.com/CFM-MSG/Code_CMAP.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesComposed query-based image retrieval; cross-level interaction; cross-modal retrieval; preserved and modified attentions,Learning systems; Semantics; Additional key word and phrasescomposed query-based image retrieval; Cross levels; Cross-level interaction; Cross-modal; Cross-modal retrieval; Key words; Preservation methods; Preserved and modified attention; Semantics Information; Text query; Image retrieval
SPIRIT: Style-guided Patch Interaction for Fashion Image Retrieval with Text Feedback,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189488947&doi=10.1145%2f3640345&partnerID=40&md5=09537474ca32b4974fea48d09298a8eb,"Fashion image retrieval with text feedback aims to find the target image according to the reference image and the modification from the user. This is a challenging task, as it requires not only the synergistic understanding of both visual and textual modalities but also the ability to model a wide variety of styles that fashion images contain. Hence, the crucial aspect of addressing this problem lies in exploiting the abundant semantic information inherent in fashion images and correlating it with the textual description of style. Recognizing that style is generally situated at the local level, we explicitly define style as the commonalities and differences between local areas of fashion images. Building upon this, we propose a Style-guided Patch InteRaction approach for fashion Image retrieval with Text feedback (SPIRIT), which focuses on the decisive influence of local details of fashion images on their style. Three corresponding networks are designed pertinently. The Patch-level Style Commonality network is introduced to fully leverage the semantic information among patches and compute their average as the style commonality. Subsequently, the Patch-level Style Difference network employs a graph reasoning network to model the patch-level difference and filter out insignificant patches. By considering the above two networks, mutual information about style is obtained from the interaction between patches. Finally, the Visual Textual Fusion network is utilized to integrate visual features with rich semantic information and textual features. Experimental results on four benchmark datasets demonstrate that our proposed SPIRIT achieves state-of-the-art performance. Source code is available at https://github.com/PKU-ICST-MIPL/SPIRIT_TOMM2024.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesFashion image retrieval with text feedback; multimodal fusion; style modeling,Benchmarking; Semantics; Additional key word and phrasesfashion image retrieval with text feedback; Key words; Level difference; Local areas; Multi-modal fusion; Reference image; Semantics Information; Style modeling; Target images; Textual description; Image retrieval
Incomplete Multiview Clustering via Semidiscrete Optimal Transport for Multimedia Data Mining in IoT,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189488826&doi=10.1145%2f3625548&partnerID=40&md5=c04ca3d7871c12c0ce10d93fb9d54cac,"With the wide deployment of the Internet of Things (IoT), large volumes of incomplete multiview data that violates data integrity is generated by various applications, which inevitably produces negative impacts on the quality of service of IoT systems. Incomplete multiview clustering (IMC), as an essential technique of data processing, has the potential for mining patterns of incomplete IoT data. However, previous methods utilize notion-strong distances that can only measure differences between distributions at the overlap of data manifolds in fusing complementary information of data for pattern mining. They may suffer from biased estimation and information loss in capturing intrinsic structures of incomplete multiview data. To address these challenges, a semidiscrete multiview optimal transport (SD-MOT) is defined for IMC, which utilizes distances with weak notions to capture intrinsic structures of incomplete multiview data. Specifically, IMC is recast as an equivalent optimal transport between continuous incomplete multiview data and discrete clustering centroids, to avoid the strict assumption on overlap between manifolds in pattern mining. Then, SD-MOT is instantiated as a deep incomplete contrastive clustering network to remedy biased estimation and information loss on intrinsic structures of incomplete multiview data. Afterwards, a variational solution to SD-MOT is derived to effectively train the network parameters for pattern mining. Finally, extensive experiments on four representative incomplete multiview datasets verify the superiority of SD-MOT in comparison with nine baseline methods. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesIncomplete multiview data; data integrity; internet of things; optimal transport,Data handling; Data mining; Quality of service; Structural optimization; Additional key word and phrasesincomplete multiview data; Biased information; Data integrity; Intrinsic structures; Key words; Multi-view clustering; Multi-view datum; Multi-views; Optimal transport; Pattern mining; Internet of things
Discriminative Action Snippet Propagation Network for Weakly Supervised Temporal Action Localization,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189239558&doi=10.1145%2f3643815&partnerID=40&md5=c6ed25c26ee6f40cbbdb037bcd333e50,"Weakly supervised temporal action localization (WTAL) aims to classify and localize actions in untrimmed videos with only video-level labels. Recent studies have attempted to obtain more accurate temporal boundaries by exploiting latent action instances in ambiguous snippets or propagating representative action features. However, empirically handcrafted ambiguous snippet extraction and the imprecise alignment of representative snippet propagation lead to challenges in modeling the completeness of actions for these methods. In this article, we propose a Discriminative Action Snippet Propagation Network (DASP-Net) to accurately discover ambiguous snippets in videos and propagate discriminative instance-level features throughout the video for improving action completeness. Specifically, we introduce a novel discriminative feature propagation module for capturing the global contextual attention and propagating the action concept across the whole video by perceiving the discriminative action snippets with instance information from the same video. Simultaneously, we incorporate denoised pseudo-labels as supervision, where we correct the controversial prediction based on the feature space distribution during training, thereby alleviating false detection caused by noise background features. Furthermore, we design an ambiguous feature mining module, which maximizes the feature affinity information of action and background in ambiguous snippets to generate more accurate latent action and background snippets and learns more precise action instance boundaries through contrastive learning of action and background snippets. Extensive experiments show that DASP-Net achieves state-of-the-art results on THUMOS14 and ActivityNet1.2 datasets.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesTemporal action localization; contrastive learning; cross attention; feature propagation; pseudo labels; weakly supervised,Additional key word and phrasestemporal action localization; Contrastive learning; Cross attention; Discriminative features; Feature propagation; Key words; Localisation; Propagation modules; Pseudo label; Weakly supervised
Immersive Multimedia Service Caching in Edge Cloud with Renewable Energy,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189494147&doi=10.1145%2f3643818&partnerID=40&md5=fda216a4ca2621d7a0668135cbff3c1e,"Immersive service caching, based on the intelligent edge cloud, can meet delay-sensitive service requirements. Although numerous service caching solutions for edge clouds have been designed, they have not been well explored. Moreover, to the best of our knowledge, there is no work to consider the immersive service caching scheme under the supply of renewable energy. In this article, we investigate the service caching problem under the renewable energy supply to minimize service latency while making full use of renewable energy. Specifically, we formulate the service caching and renewable energy harvesting problem, which considers the dynamic renewable energy, unknown service requests, and limited capacity of the edge cloud. To solve this problem, we propose an effective algorithm, called OSCRE. Our algorithm first uses Lyapunov optimization to convert the time-average problem into time-independence optimization and thus realizes optimal renewable energy harvesting. Then, it realizes the service caching scheme using data-driven combinatorial multi-armed bandit learning. The simulation results show that the OSCRE scheme can save service latency while making sufficient use of renewable energy.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesEdge cloud; intelligent scheduling; renewable energy; multimedia service caching,Delay-sensitive applications; Energy harvesting; Additional key word and phrasesedge cloud; Caching scheme; Edge clouds; Immersive; Intelligent scheduling; Intelligent scheduling;; Key words; Multimedium service caching; Renewable energies; Service latency; Multimedia services
Privacy and Integrity Protection for IoT Multimodal Data Using Machine Learning and Blockchain,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188548556&doi=10.1145%2f3638769&partnerID=40&md5=c84b9deb687dc823838d36bb060a10d4,"With the wide application of Internet of Things (IoT) technology, large volumes of multimodal data are collected and analyzed for various diagnoses, analyses, and predictions to help in decision-making and management. However, the research on protecting data integrity and privacy is quite limited, while the lack of proper protection for sensitive data may have significant impacts on the benefits and gains of data owners. In this research, we propose a protection solution for data integrity and privacy. Specifically, our system protects data integrity through distributed systems and blockchain technology. Meanwhile, our system guarantees data privacy using differential privacy and Machine Learning (ML) techniques. Our system aims to maintain the usability of the data for further data analytical tasks of data users, while encrypting the data according to the requirements of data owners. We implement our solution with smart contracts, distributed file systems, and ML models. The experimental results show that our proposed solution can effectively encrypt source IoT data according to the requirements of data users while data integrity can be protected under the blockchain.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesBlockchain; integrity; Internet of Things; machine learning; multimodal data; privacy,Cryptography; Decision making; File organization; Internet of things; Machine learning; Network security; Sensitive data; Smart contract; Additional key word and phrasesblockchain; Block-chain; Data integrity; Data users; Integrity; Key words; Machine-learning; Multi-modal data; Privacy; Privacy protection; Blockchain
Exploiting Spatial-Temporal Context for Interacting Hand Reconstruction on Monocular RGB Video,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189238971&doi=10.1145%2f3639707&partnerID=40&md5=170e4526d00a7ca42ef3ac51409d5794,"Reconstructing interacting hands from monocular RGB data is a challenging task, as it involves many interfering factors, e.g., self- and mutual occlusion and similar textures. Previous works only leverage information from a single RGB image without modeling their physically plausible relation, which leads to inferior reconstruction results. In this work, we are dedicated to explicitly exploiting spatial-temporal information to achieve better interacting hand reconstruction. On the one hand, we leverage temporal context to complement insufficient information provided by the single frame and design a novel temporal framework with a temporal constraint for interacting hand motion smoothness. On the other hand, we further propose an interpenetration detection module to produce kinetically plausible interacting hands without physical collisions. Extensive experiments are performed to validate the effectiveness of our proposed framework, which achieves new state-of-the-art performance on public benchmarks.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesInteracting hand; model-based 3D hand reconstrcution; temporal context,3D modeling; Benchmarking; Image reconstruction; Three dimensional computer graphics; Additional key word and phrasesinteracting hand; Hand reconstruction; Key words; Model-based 3d hand reconstrcution; Model-based OPC; RGB images; Single frames; Spatial temporals; Temporal context; Temporal information; Textures
Improving Continuous Sign Language Recognition with Consistency Constraints and Signer Removal,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189456973&doi=10.1145%2f3640815&partnerID=40&md5=84bd888eb72cc5c731f514d4a7a4aac2,"Deep-learning-based continuous sign language recognition (CSLR) models typically consist of a visual module, a sequential module, and an alignment module. However, the effectiveness of training such CSLR backbones is hindered by limited training samples, rendering the use of a single connectionist temporal classification loss insufficient. To address this limitation, we propose three auxiliary tasks to enhance CSLR backbones. First, we enhance the visual module, which is particularly sensitive to the challenges posed by limited training samples, from the perspective of consistency. Specifically, since sign languages primarily rely on signers' facial expressions and hand movements to convey information, we develop a keypoint-guided spatial attention module that directs the visual module to focus on informative regions, thereby ensuring spatial attention consistency. Furthermore, recognizing that the output features of both the visual and sequential modules represent the same sentence, we leverage this prior knowledge to better exploit the power of the backbone. We impose a sentence embedding consistency constraint between the visual and sequential modules, enhancing the representation power of both features. The resulting CSLR model, referred to as consistency-enhanced CSLR, demonstrates superior performance on signer-dependent datasets, where all signers appear during both training and testing. To enhance its robustness for the signer-independent setting, we propose a signer removal module based on feature disentanglement, effectively eliminating signer-specific information from the backbone. To validate the effectiveness of the proposed auxiliary tasks, we conduct extensive ablation studies. Notably, utilizing a transformer-based backbone, our model achieves state-of-the-art or competitive performance on five benchmarks, including PHOENIX-2014, PHOENIX-2014-T, PHOENIX-2014-SI, CSL, and CSL-Daily. Code and models are available at https://github.com/2000ZRL/LCSA_C2SLR_SRM.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesContinuous sign language recognition; auxiliary learning; feature disentanglement.; signer-independent,Benchmarking; Knowledge management; Sampling; Visual languages; Additional key word and phrasescontinuous sign language recognition; Auxiliary learning; Consistency constraints; Feature disentanglement.; Key words; Recognition models; Sign Language recognition; Signer-independent; Spatial attention; Training sample; Deep learning
Diverse Visual Question Generation Based on Multiple Objects Selection,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189491737&doi=10.1145%2f3640014&partnerID=40&md5=6db38cf86a4dcba5eac38ba6496b8f6d,"Visual question generation task aims at generating high-quality questions about a given image. To make this tak applicable to various scenarios, e.g., the growing demand for exams, it is important to generate diverse questions. The existing methods for this task control diverse question generation based on different question types, e.g., ""what""and ""when.""Although different question types lead to description diversity, they cannot guarantee semantic diversity when asking the same objects. Research in the field of psychology shows that humans pay attention to different objects in an image based on their preferences, which is beneficial to constructing semantically diverse questions. According to the research, we propose a multi-selector visual question generation (MS-VQG) model that aims to focus on different objects to generate diverse questions. Specifically, our MS-VQG model employs multiple selectors to imitate different humans to select different objects in a given image. Based on these different selected objects, our MS-VQG model can generate diverse questions corresponding to each selector. Extensive experiments on two datasets show that our proposed model outperforms the baselines in generating diverse questions.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesMultimodal; mixture of experts; visual question generation,Additional key word and phrasesmultimodal; Growing demand; High quality; Key words; Mixture of experts; Multiple objects; Object selection; Question type; Task control; Visual question generation; Semantics
Multimodal Attentive Representation Learning for Micro-video Multi-label Classification,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189245701&doi=10.1145%2f3643888&partnerID=40&md5=407d0e6a922169dbd8630156f340d071,"As one of the representative types of user-generated contents (UGCs) in social platforms, micro-videos have been becoming popular in our daily life. Although micro-videos naturally exhibit multimodal features that are rich enough to support representation learning, the complex correlations across modalities render valuable information difficult to integrate. In this paper, we introduced a multimodal attentive representation network (MARNET) to learn complete and robust representations to benefit micro-video multi-label classification. To address the commonly missing modality issue, we presented a multimodal information aggregation mechanism module to integrate multimodal information, where latent common representations are obtained by modeling the complementarity and consistency in terms of visual-centered modality groupings instead of single modalities. For the label correlation issue, we designed an attentive graph neural network module to adaptively learn the correlation matrix and representations of labels for better compatibility with training data. In addition, a cross-modal multi-head attention module is developed to make the learned common representations label-aware for multi-label classification. Experiments conducted on two micro-video datasets demonstrate the superior performance of MARNET compared with state-of-the-art methods.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesMicro-video; graph network; multi-label; multimodal representations,Classification (of information); Additional key word and phrasesmicro-video; Graph networks; Key words; Learn+; Multi-label classifications; Multi-labels; Multi-modal; Multi-modal information; Multimodal representation; User-generated; Graph neural networks
MF2ShrT: Multimodal Feature Fusion Using Shared Layered Transformer for Face Anti-spoofing,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189433563&doi=10.1145%2f3640817&partnerID=40&md5=4eaaf5b0eed836d65d68ff45db0ca547,"In recent times, Face Anti-spoofing (FAS) has gained significant attention in both academic and industrial domains. Although various convolutional neural network (CNN)-based solutions have emerged, multimodal approaches incorporating RGB, depth, and information retrieval (IR) have exhibited better performance than unimodal classifiers. The increasing veracity of modern presentation attack instruments results in a persistent need to enhance the performance of such models. Recently, self-attention-based vision transformers (ViT) have become a popular choice in this field. Their fundamental aspects for multimodal FAS have not been thoroughly explored yet. Therefore, we propose a novel framework for FAS called MF2ShrT, which is based on a pretrained vision transformer. The proposed framework uses overlap patches and parameter sharing in the ViT network, allowing it to utilize multiple modalities in a computationally efficient manner. Furthermore, to effectively fuse intermediate features from different encoders of each ViT, we explore a T-encoder-based hybrid feature block enabling the system to identify correlations and dependencies across different modalities. MF2ShrT outperforms conventional vision transformers and achieves state-of-the-art performance on benchmarks CASIA-SURF and WMCA, demonstrating the efficiency of transformer-based models for presentation attack detection PAD).  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesFace anti-spoofing; multimodal; presentation attack detection; vision transformer,Classification (of information); Convolutional neural networks; Signal encoding; Additional key word and phrasesface anti-spoofing; Antispoofing; Attack detection; Convolutional neural network; Key words; Multi-modal; Multimodal feature fusions; Performance; Presentation attack detection; Vision transformer; Benchmarking
Head3D: Complete 3D Head Generation via Tri-plane Feature Distillation,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189458061&doi=10.1145%2f3635717&partnerID=40&md5=7154ab4f087881bba37720e95283f6d3,"Head generation with diverse identities is an important task in computer vision and computer graphics, widely used in multimedia applications. However, current full-head generation methods require a large number of three-dimensional (3D) scans or multi-view images to train the model, resulting in expensive data acquisition costs. To address this issue, we propose Head3D, a method to generate full 3D heads with limited multi-view images. Specifically, our approach first extracts facial priors represented by tri-planes learned in EG3D, a 3D-aware generative model, and then proposes feature distillation to deliver the 3D frontal faces within complete heads without compromising head integrity. To mitigate the domain gap between the face and head models, we present a dual-discriminator to guide the frontal and back head generation. Our model achieves cost-efficient and diverse complete head generation with photo-realistic renderings and high-quality geometry representations. Extensive experiments demonstrate the effectiveness of our proposed Head3D, both qualitatively and quantitatively.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesHead generation; adversarial generative network; limited data; neural radiance field,Data acquisition; Generative adversarial networks; Interactive computer graphics; 'current; 3D head; Additional key word and phraseshead generation; Generation method; Key words; Limited data; Multi-view image; Multimedia applications; Neural radiance field; Three-dimensional (3D) scans; Distillation
SWRM: Similarity Window Reweighting and Margin for Long-Tailed Recognition,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189337597&doi=10.1145%2f3643816&partnerID=40&md5=420f639264980060ca2e35ca1cb92960,"Real-world data usually obeys a long-tailed distribution, where a few classes have higher number of samples compared to the other classes. Recent studies have been proposed to alleviate the extreme data imbalance from different perspectives. In this article, we experimentally find that due to the easily confusing visual features between some head- and tail classes, the cross-entropy model is prone to misclassify tail samples to similar head classes. Therefore, to alleviate the influence of the confusion on model performance and improve the classification of tail classes, we propose a Similarity Window Reweighting and Margin (SWRM) algorithm, where the SWRM consists of Similarity Window Reweighting (SWR) and Similarity Window Margin (SWM) algorithms. For the confusable head- and tail classes, SWR assigns larger weights to tail classes and smaller weights to head classes. Therefore, the model can enlarge the importance of tail classes and effectively improve their classification. Moreover, SWR considers the difference in label frequency and the impact of category similarity simultaneously, so that the weight coefficients are more reasonable and efficacious. SWM generates adaptive margins that are proportional to the ratio of the classifier's weight norm, thus promoting the learning of tail classifier with small weight norm. Our SWRM effectively eliminates the confusion between head- and tail classes and alleviates the misclassification issues. Extensive experiments on three long-tailed datasets, i.e., CIFAR100-LT, ImageNet-LT, and Places-LT, verify our proposed method's effectiveness and superiority over comparative methods.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesLong-tailed recognition; class re-balancing; logit adjustment; reweighting,Additional key word and phraseslong-tailed recognition; Class re-balancing; Data imbalance; Key words; Logit adjustment; Long-tailed distributions; Number of samples; Re-weighting; Real-world; Visual feature
A Reconfigurable Framework for Neural Network Based Video In-Loop Filtering,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189484913&doi=10.1145%2f3640467&partnerID=40&md5=f32c4f652a55b7327f5b847cd5162ac3,"This article proposes a reconfigurable framework for neural network based video in-loop filtering to guide large-scale models for content-aware processing. Specifically, the backbone neural model is decomposed into several convolutional groups and the encoder systematically traverses all candidate configurations combined by these groups to find the best one. The selected configuration index is then encapsulated as side information and passed to the decoder, enabling dynamic model reconfiguration during the decoding stage. The preceding reconfiguration process is only deployed in the inference stage on top of a pre-trained backbone model. Furthermore, we devise WMSPFormer, a wavelet multi-scale Poolformer, as the backbone network structure. WMSPFormer utilizes a wavelet-based multi-scale structure to losslessly decompose the input into multiple scales for spatial-spectral features aggregation. Moreover, it uses multi-scale pooling operations (MSPoolformer) instead of complicated matrix calculations to substitute the attention process. We also extend MSPoolformer to a large-scale version using more parameters, referred to as MSPoolformerExt. Extensive experiments demonstrate that the proposed WMSPFormer+Reconfig. and WMSPFormerExt+Reconfig. achieve a remarkable 7.13% and 7.92% BD-Rate reduction over the anchor H.266/VVC, outperforming most existing methods evaluated under the same training and testing conditions. In addition, the low-complexity nature of the WMSPFormer series makes it attractive for practical applications.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesIn-loop filter; H.266/VVC; neural model; reconfigurable; Transformer,Video signal processing; Additional key word and phrasesin-loop filter; H.266/VVC; Key words; Loop filter; Loop filtering; Network-based; Neural modelling; Neural-networks; Reconfigurable; Transformer; Decoding
IoT-enabled Biometric Security: Enhancing Smart Car Safety with Depth-based Head Pose Estimation,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189483665&doi=10.1145%2f3639367&partnerID=40&md5=7527f445c621a041ffea46ed7f0006f3,"Advanced Driver Assistance Systems (ADAS) are experiencing higher levels of automation, facilitated by the synergy among various sensors integrated within vehicles, thereby forming an Internet of Things (IoT) framework. Among these sensors, cameras have emerged as valuable tools for detecting driver fatigue and distraction. This study introduces HYDE-F, a Head Pose Estimation (HPE) system exclusively utilizing depth cameras. HYDE-F adeptly identifies critical driver head poses associated with risky conditions, thus enhancing the safety of IoT-enabled ADAS. The core of HYDE-F's innovation lies in its dual-process approach: it employs a fractal encoding technique and keypoint intensity analysis in parallel. These two processes are then fused using an optimization algorithm, enabling HYDE-F to blend the strengths of both methods for enhanced accuracy. Evaluations conducted on a specialized driving dataset, Pandora, demonstrate HYDE-F's competitive performance compared to existing methods, surpassing current techniques in terms of average Mean Absolute Error (MAE) by nearly 1°. Moreover, case studies highlight the successful integration of HYDE-F with vehicle sensors. Additionally, HYDE-F exhibits robust generalization capabilities, as evidenced by experiments conducted on standard laboratory-based HPE datasets, i.e., Biwi and ICT-3DHP databases, achieving an average MAE of 4.9° and 5°, respectively. © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesHead pose estimation; data fusion; depth images; driver attention; fractal encoding; HPE; landmark prediction,Automobile drivers; Cameras; Encoding (symbols); Fractals; Image coding; Image enhancement; Internet of things; Safety engineering; Security systems; Signal encoding; Vehicle safety; Additional key word and phraseshead pose estimation; Biometrics security; Depth image; Driver attention; Fractal encoding; Head Pose Estimation; Key words; Landmark prediction; Mean absolute error; Pose-estimation; Advanced driver assistance systems
Deep Neighborhood-aware Proxy Hashing with Uniform Distribution Constraint for Cross-modal Retrieval,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189348808&doi=10.1145%2f3643639&partnerID=40&md5=b5b4faa1c12c546cffbf5fe767a0c524,"Cross-modal retrieval methods based on hashing have gained significant attention in both academic and industrial research. Deep learning techniques have played a crucial role in advancing supervised cross-modal hashing methods, leading to significant practical improvements. Despite these achievements, current deep cross-modal hashing still encounters some underexplored limitations. Specifically, most of the available deep hashing usually utilizes pair-wise or triplet-wise strategies to promote the separation of the inter-classes by calculating the relative similarities between samples, weakening the compactness of intra-class data from different modalities, which could generate ambiguous neighborhoods. In this article, the Deep Neighborhood-aware Proxy Hashing (DNPH) framework is proposed to learn a discriminative embedding space with the original neighborhood relation preserved. By introducing learnable shared category proxies, the neighborhood-aware proxy loss is proposed to project the heterogeneous data into a unified common embedding, in which the sample is pulled closer to the corresponding category proxy and is pushed away from other proxies, capturing small within-class scatter and big between-class scatter. To enhance the quality of the obtained binary codes, the uniform distribution constraint is developed to make each hash bit independently obey the discrete uniform distribution. In addition, the discrimination loss is designed to preserve modality-specific semantic information of samples. Extensive experiments are performed on three benchmark datasets to prove that our proposed DNPH framework achieves comparable or even better performance compared with the state-of-the-art cross-modal retrieval applications. The corresponding code implementation of our DNPH framework is as follows: https://github.com/QinLab-WFU/OUR-DNPH.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesCross-modal retrieval; deep hashing; discrimination loss; neighborhood-aware proxy loss; uniform distribution constraint,Benchmarking; Codes (symbols); Deep learning; Industrial research; Semantics; Additional key word and phrasescross-modal retrieval; Cross-modal; Deep hashing; Discrimination loss; Embeddings; Key words; Neighborhood-aware proxy loss; Neighbourhood; Uniform distribution; Uniform distribution constraint; Embeddings
Human Selective Matting,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189484666&doi=10.1145%2f3640017&partnerID=40&md5=f504f394e79716fb35d648df99cb1e1d,"Existing human matting methods are incapable of accurately estimating the alpha mattes of arbitrarily selected humans from a group photo. An alternative solution is to apply them to the corresponding cropped image patches. However, this option obtains an inaccurate alpha estimation due to the interference of the body parts of the neighboring humans. In addition, these methods are only trained on finely annotated synthetic data, which causes poor performance in real-world scenarios due to the domain shift. To address these problems, we propose human selective matting (HSMatt), which performs matting for arbitrarily selected humans from a group photo given only a simple bounding box as guidance. Specifically, we design a global-local context network to extract both local and global semantic context features. A human-aware trimap network is then proposed to generate human-aware trimaps for the selected humans, which adopts stacked bidirectional inference modules with intermediate supervision to progressively refine the estimated trimap. Finally, a partially supervised matting network is introduced to estimate the alpha matte, which uses a sample-varying loss to train the network on both the finely annotated synthetic data and coarsely annotated real-world data, resulting in high accuracy and good generalization. To evaluate the proposed HSMatt, we construct the first human selective matting dataset, named HSM-200K, which contains over 200,000 human images with instance-level alpha matte annotations. Experimental results demonstrate that the proposed HSMatt outperforms state-of-the-art methods.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesImage matting; human matting; semantic segmentation,Semantics; Additional key word and phrasesimage matting; Alpha estimations; Alpha mattes; Alternative solutions; Human matting; Human-aware; Image patches; Key words; Semantic segmentation; Synthetic data; Semantic Segmentation
Multi-Content Interaction Network for Few-Shot Segmentation,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189446232&doi=10.1145%2f3643850&partnerID=40&md5=85c68aff7606f95cc6175372fa8ad108,"Few-Shot Segmentation (FSS) poses significant challenges due to limited support images and large intra-class appearance discrepancies. Most existing approaches focus on aligning the support-query correlations from the same layer of the frozen backbone while neglecting the bias between different tasks and different layers. In this article, we propose a Multi-Content Interaction Network (MCINet) to remedy these issues by fully exploiting and interacting with the different contextual information contained in distinct branches. Specifically, MCINet improves FSS from three perspectives: (1) boosting the query representations through incorporating the independent information from another learnable branch into the features from the frozen backbone, (2) enhancing the support-query correlations by exploiting both the same-layer and adjacent-layer features, and (3) refining the predicted results with a multi-scale mask prediction strategy. Experiments on three benchmarks demonstrate that our approach reaches state-of-the-art performances and outperforms the best competitors with many desirable advantages, especially on the challenging COCO dataset. Code will be released on GitHub (https://github.com/chenhao-zju/mcinet).  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesFew-shot semantic segmentation; adjacent-layer similarity; multi-content interaction,Semantics; Additional key word and phrasesfew-shot semantic segmentation; Adjacent layers; Adjacent-layer similarity; Different layers; Interaction networks; Intra class; Key words; Multi-content interaction; Semantic segmentation; Shot segmentation; Semantic Segmentation
Multiply Complementary Priors for Image Compressive Sensing Reconstruction in Impulsive Noise,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189283434&doi=10.1145%2f3643032&partnerID=40&md5=ca9e0125e12db66f9cfb3c00292eaadd,"Impulsive noise is always present in real-world image Compressive Sensing (CS) acquisition systems, where existing CS reconstruction performance may seriously deteriorate. In this article, we propose a robust CS formulation for image reconstruction to suppress outliers in the presence of impulsive noise. To address this issue, we consider a novel truncated-Cauchy loss function as the metric of residual error to elevate the reconstruction robustness. Specifically, we design a complementary priors model to incorporate nonconvex nonlocal low-rank prior and deep denoiser prior for high-accuracy image reconstruction. By means of the half-quadratic optimization theory and generalized soft-thresholding technique, we also develop an alternative optimization algorithm for solving the induced nonconvex optimization problem. Numerical simulations demonstrate the robustness and accuracy of the proposed robust CS method compared to some recent CS methods for image reconstruction in impulsive noise.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesTruncated-cauchy; alternatively; compressive sensing; impulsive noise; nonlocal and deep priors,Computation theory; Constrained optimization; Image compression; Image reconstruction; Impulse noise; Numerical methods; Quadratic programming; Site selection; Acquisition systems; Additional key word and phrasestruncated-cauchy; Alternatively; Compressive sensing; Images reconstruction; Key words; Nonlocal; Nonlocal and deep prior; Performance; Real-world image; Compressed sensing
A Bitcoin-based Secure Outsourcing Scheme for Optimization Problem in Multimedia Internet of Things,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189238706&doi=10.1145%2f3637489&partnerID=40&md5=02baca603349f03d63e5574d6227e5bf,"With the development of the Internet of Things (IoT) and cloud computing, various multimedia data such as audio, video, and images have experienced explosive growth, ushering in the era of big data. Large-scale computing tasks in the Multimedia Internet of Things (M-IoT), such as mathematical optimization problems, have begun to be outsourced from IoT devices with limited computing power to cloud servers for execution. However, outsourcing computation brings security concerns, because the behaviors of clouds are invisible to users. The leakage of privacy data in outsourced optimization problems leads to immeasurable losses. The mutual distrust between clouds and users causes that the correctness of the optimal decisions and the fairness of the payment activities are not guaranteed. Blockchain technology has the characteristic of immutability and has become a new security paradigm for eliminating multi-party trust concerns. In this article, we propose a Bitcoin-based secure outsourcing scheme to address the aforementioned security concerns. To prevent confidential data leakage, the proposed scheme designs a computable privacy-preserving method for the outsourced optimization problems. To judge the correctness of the optimal decision and reduce verification costs, the proposed scheme designs a low-cost two-layer verification mechanism based on dual theory and blockchain technology. Blockchain nodes reach a consensus on the problem solutions and trigger an automatic fair payment protocol-based Bitcoin. Security analysis and experimental results demonstrate that our scheme guarantees privacy, fairness, and computational efficiency.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSecure outsourcing computation; Bitcoin; multimedia internet of things; optimization problem,Bitcoin; Blockchain; Computation theory; Computational efficiency; Computing power; Data privacy; Decision theory; Edge computing; Optimization; Outsourcing; Additional key word and phrasessecure outsourcing computation; Block-chain; Cloud-computing; Key words; Multimedia data; Multimedia internet; Multimedium internet of thing; Optimal decisions; Optimization problems; Scheme design; Internet of things
PADVG: A Simple Baseline of Active Protection for Audio-Driven Video Generation,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189343944&doi=10.1145%2f3638556&partnerID=40&md5=80c35a557fedf1b5fb0e1acf959322eb,"Over the past few years, deep generative models have significantly evolved, enabling the synthesis of realistic content and also bringing security concerns of illegal misuse. Therefore, active protection for generative models has been proposed recently, aiming to generate samples with hidden messages for future identification while preserving the original generating performance. However, existing active protection methods are specifically designed for generative adversarial networks (GANs), restricted to handling unconditional image generation. We observe that they get limited identification performance and visual quality when handling audio-driven video generation conditioned on target audio and source input to drive video generation with consistent context, e.g., identity and movement, between frame sequences. To address this issue, we introduce a simple yet effective active Protection framework for Audio-Driven Video Generation, named PADVG. To be specific, we present a novel frame-shared embedding module in which messages to hide are first transformed into frame-shared message coefficients. Then, these coefficients are assembled with the intermediate feature maps of video generators at multiple feature levels to generate the embedded video frames. Besides, PADVG further considers two visual consistent losses: (i) intra-frame loss is utilized to keep the visual consistency with different hidden messages; (ii) inter-frame loss is used to preserve the visual consistency across different video frames. Moreover, we also propose an auxiliary denoising training strategy through perturbing the assembled features by learnable pixel-level noise to improve identification performance, while enhancing robustness against real-world disturbances. Extensive experiments demonstrate that our proposed PADVG for audio-driven video generation can effectively identify the generated videos and achieve high visual quality.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesActive protection; audio-driven video generative models,Electric equipment protection; Active protection; Additional key word and phrasesactive protection; Audio-driven video generative model; Generative model; Hidden messages; Key words; Performance; Simple++; Video generation; Visual qualities; Generative adversarial networks
Vocoder Detection of Spoofing Speech Based on GAN Fingerprints and Domain Generalization,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189439269&doi=10.1145%2f3630751&partnerID=40&md5=9acc0161e9c3c73dc932f12e63bad2a0,"As an important part of the text-to-speech (TTS) system, vocoders convert acoustic features into speech waveforms. The difference in vocoders is key to producing different types of forged speech in the TTS system. With the rapid development of general adversarial networks (GANs), an increasing number of GAN vocoders have been proposed. Detectors often encounter vocoders of unknown types, which leads to a decline in the generalization performance of models. However, existing studies lack research on detection generalization based on GAN vocoders. To solve this problem, this study proposes vocoder detection of spoofed speech based on GAN fingerprints and domain generalization. The framework can widen the distance between real speech and forged speech in feature space, improving the detection model's performance. Specifically, we utilize a fingerprint extractor based on an autoencoder to extract GAN fingerprints from vocoders. We then weight them to the forged speech for subsequent classification to learn the forged speech features with high differentiation. Subsequently, domain generalization is used to further improve the generalization ability of the model for unseen forgery types. We achieve domain generalization using domain-adversarial learning and asymmetric triplet loss to learn a better generalized feature space in which real speech is compact and forged speech synthesized by different vocoders is dispersed. Finally, to optimize the training process, curriculum learning is used to dynamically adjust the contributions of the samples with different difficulties in the training process. Experimental results show that the proposed method achieves the most advanced detection results among four GAN vocoders. The code is available at https://github.com/multimedia-infomation-security/GAN-Vocoder-detection. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSpeech forgery; curriculum learning; domain generalization; GAN fingerprint; vocoder,Feature extraction; Learning systems; Palmprint recognition; Speech recognition; Vocoders; Additional key word and phrasesspeech forgery; Adversarial networks; Curriculum learning; Domain generalization; Feature space; General adversarial network fingerprint; Generalisation; Key words; Text-to-speech system; Vocod; Curricula
Trustworthy and Efficient Digital Twins in Post-Quantum Era with Hybrid Hardware-Assisted Signatures,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189443900&doi=10.1145%2f3638250&partnerID=40&md5=b33fe2d9cf61f19566031972c9b9e451,"Digital Twins (DT) virtually model cyber-physical objects via sensory inputs by simulating or monitoring their behavior. Therefore, DTs usually harbor vast quantities of Internet of Things (IoT) components (e.g., sensors) that gather, process, and offload sensitive information (e.g., healthcare) to the cloud. It is imperative to ensure the trustworthiness of such sensitive information with long-term and compromise-resilient security guarantees. Digital signatures provide scalable authentication and integrity with non-repudiation and are vital tools for DTs. Post-quantum cryptography (PQC) and forward-secure signatures are two fundamental tools to offer long-term security and breach resiliency. However, NIST-PQC signature standards are exorbitantly costly for embedded DT components and are infeasible when forward-security is also considered. Moreover, NIST-PQC signatures do not admit aggregation, which is a highly desirable feature to mitigate the heavy storage and transmission burden in DTs. Finally, NIST recommends hybrid PQ solutions to enable cryptographic agility and transitional security. Yet, there is a significant gap in the state of the art in the achievement of all these advanced features simultaneously. Therefore, there is a significant need for lightweight digital signatures that offer compromise resiliency and compactness while permitting transitional security into the PQ era for DTs.We create a series of highly lightweight digital signatures called Hardware-ASisted Efficient Signature (HASES) that meets the above requirements. The core of HASES is a hardware-assisted cryptographic commitment construct oracle (CCO) that permits verifiers to obtain expensive commitments without signer interaction. We created three HASES schemes: PQ-HASES is a forward-secure PQ signature, LA-HASES is an efficient aggregate Elliptic-Curve signature, and HY-HASES is a novel hybrid scheme that combines PQ-HASES and LA-HASES with novel strong nesting and sequential aggregation. HASES does not require a secure-hardware on the signer. We prove that HASES schemes are secure and implemented them on commodity hardware and and 8-bit AVR ATmega2560. Our experiments confirm that PQ-HASES and LA-HASES are two magnitudes of times more signer efficient than their PQ and conventional-secure counterparts, respectively. HY-HASES outperforms NIST PQC and conventional signature combinations, offering a standard-compliant transitional solution for emerging DTs. We open-source HASES schemes for public-testing and adaptation.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesMultimedia authentication; cyber-security; digital twins; post-quantum security,Cybersecurity; Internet of things; Network security; Quantum cryptography; Quantum efficiency; Additional key word and phrasesmultimedium authentication; CryptoGraphics; Cyber security; Hardware-assisted; Key words; Post quantum; Post quantum cryptography; Post-quantum securities; Sensitive informations; Signature Scheme; Authentication
MCFNet: Multi-Attentional Class Feature Augmentation Network for Real-Time Scene Parsing,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189469313&doi=10.1145%2f3639053&partnerID=40&md5=d3b37389a347ca299bf83c29eeccff8d,"For real-time scene parsing tasks, capturing multi-scale semantic features and performing effective feature fusion is crucial. However, many existing solutions ignore stripe-shaped things like poles, traffic lights and are so computationally expensive that cannot meet the high real-time requirements. This article presents a novel model, the Multi-Attention Class Feature Augmentation Network (MCFNet) to address this challenge. MCFNet is designed to capture long-range dependencies across different scales with low computational cost and to perform a weighted fusion of feature maps. It features the BAM (Strip Matrix Based Attention Module) for extracting strip objects in images. The BAM module replaces the conventional self-attention method using square matrices with strip matrices, which allows it to focus more on strip objects while reducing computation. Additionally, MCFNet has a parallel branch that focuses on global information based on self-attention to avoid wasting computation. The two branches are merged to enhance the performance of traditional self-attention modules. Experimental results on two mainstream datasets demonstrate the effectiveness of MCFNet. On the Camvid and Cityscapes test sets, MCFNet achieved 207.5 FPS/73.5% mIoU and 136.1 FPS/71.63% mIoU, respectively. The experiments show that MCFNet outperforms other models on the Camvid dataset and can significantly improve the performance of real-time scene parsing tasks. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesComputer vision; attention mechanism; CNN; real-time semantic segmentation,Computer vision; Semantic Segmentation; Semantics; Additional key word and phrasescomputer vision; Attention mechanisms; Key words; matrix; Multi-scales; Performance; Real- time; Real-time semantic segmentation; Real-time semantics; Semantic segmentation; Matrix algebra
Perceptual Quality Assessment of Omnidirectional Images: A Benchmark and Computational Model,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189472285&doi=10.1145%2f3640344&partnerID=40&md5=f2fe08cd153724e67e1f540c0957ceeb,"Compared with traditional 2D images, omnidirectional images (also referred to as 360 images) have more complicated perceptual characteristics due to the particularities of imaging and display. How humans perceive omnidirectional images in an immersive environment and form the immersive quality of experience are important problems. Thus, it is crucial to measure the quality of omnidirectional images under different viewing conditions, which suffer from realistic distortions. In this article, we build a large-scale subjective assessment database for omnidirectional images and carry out a comprehensive psychophysical experiment to study the relationships between different factors (viewing conditions and viewing behaviors) and the perceptual quality of omnidirectional images. In addition, we collect both subjective ratings and head movement data. A thorough analysis of the collected subjective data is also provided, where we make several interesting findings. Moreover, with the proposed database, we propose a novel transformer-based omnidirectional image quality assessment model. To be consistent with the human viewing process, viewing conditions and behaviors are naturally incorporated into the proposed model. Specifically, the proposed model mainly consists of three parts: viewport sequence generation, multi-scale feature extraction, and perceptual quality prediction. Extensive experimental results conducted on the proposed database demonstrate the effectiveness of the proposed method over existing image quality assessment methods.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesOmnidirectional image; image quality assessment; non-uniform distortion; viewing condition,Benchmarking; Image quality; Quality of service; Additional key word and phrasesomnidirectional image; Image quality assessment; Key words; Non-uniform; Non-uniform distortion; Omnidirectional image; Perceptual quality; Quality assessment; Uniform distortions; Viewing conditions; Database systems
Pedestrian Attribute Recognition via Spatio-temporal Relationship Learning for Visual Surveillance,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189432461&doi=10.1145%2f3632624&partnerID=40&md5=5f6c6befdbf40689ea8cd5538fd8d36c,"Pedestrian attribute recognition (PAR) aims at predicting the visual attributes of a pedestrian image. PAR has been used as soft biometrics for visual surveillance and IoT security. Most of the current PAR methods are developed based on discrete images. However, it is challenging for the image-based method to handle the occlusion and action-related attributes in real-world applications. Recently, video-based PAR has attracted much attention in order to exploit the temporal cues in the video sequences for better PAR. Unfortunately, existing methods usually ignore the correlations among different attributes and the relations between attributes and spatio regions. To address this problem, we propose a novel method for video-based PAR by exploring the relationships among different attributes in both the spatio and temporal domains. More specifically, a spatio-temporal saliency module (STSM) is introduced to capture the key visual patterns from the video sequences, and a module for spatio-temporal attribute relationship learning (STARL) is proposed to mine the correlations among these patterns. Meanwhile, a large-scale benchmark for video-based PAR, RAP-Video, is built by extending the image-based dataset RAP-2, which contains 83,216 tracklets with 25 scenes. To the best of our knowledge, this is the largest dataset for video-based PAR. Extensive experiments are performed on the proposed benchmark as well as on MARS Attribute and DukeMTMC-Video Attribute. The superior performance demonstrates the effectiveness of the proposed method.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesVideo-based pedestrian attribute recognition; IoT security; spatio-temporal relationship learning,Internet of things; Security systems; Video recording; Additional key word and phrasesvideo-based pedestrian attribute recognition; Attribute recognition; IoT security; Key words; Spatio-temporal; Spatio-temporal relationship learning; Spatio-temporal relationships; Video sequences; Visual attributes; Visual surveillance; Large datasets
Bi-directional Block Encoding for Reversible Data Hiding over Encrypted Images,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185222135&doi=10.1145%2f3638771&partnerID=40&md5=6d7c47c4447711607b049b28ead6a746,"Reversible data hiding over encrypted images (RDH-EI) technology is a viable solution for privacy-preserving cloud storage, as it enables the reversible embedding of additional data into images while maintaining image confidentiality. Since the data hiders, e.g., cloud servers, are willing to embed as much data as possible for storage, management, or other processing purposes, a large embedding capacity is desirable in an RDH-EI scheme. In this article, we introduce a novel bi-directional block encoding (BDBE) method, which, for the first time, encodes the distances of values in a binary sequence from both ends. This approach allows for encoding images with smaller sizes compared to traditional and state-of-the-art encoding methods. Leveraging the BDBE technique, we propose a high-capacity RDH-EI scheme. In this scheme, the content owner initially predicts the image pixels and then employs BDBE to encode the prediction errors, creating space for data embedding. The resulting encoded data are subsequently encrypted using a secure stream cipher, such as the Advanced Encryption Standard, before being transmitted to a data hider. The data hider can embed confidential information within the encrypted image for the purposes of storage, management, or other processing. Upon receiving the data, an authorized receiver can accurately recover the original image and the embedded data without any loss. Experimental results demonstrate that our RDH-EI scheme achieves a significantly larger embedding capacity compared to several state-of-the-art schemes. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesImage encoding; encrypted image; reversible data hiding,Cloud storage; Cryptography; Data privacy; Embeddings; Encoding (symbols); Image coding; Information management; Signal encoding; Steganography; Storage management; Additional key word and phrasesimage encoding; Bi-directional; Block encoding; Embedding capacity; Encoding methods; Encodings; Encrypted images; Image technology; Key words; Reversible data hiding; Binary sequences
Privacy-preserving Multi-source Cross-domain Recommendation Based on Knowledge Graph,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185398587&doi=10.1145%2f3639706&partnerID=40&md5=b233e32ce5a31ba77f48460738261543,"The cross-domain recommender systems aim to alleviate the data sparsity problem in the target domain by transferring knowledge from the auxiliary domain. However, existing works ignore the fact that the data sparsity problem may also exist in the single auxiliary domain, and sharing user behavior data is restricted by the privacy policy. In addition, their cross-domain models lack interpretability. To address these concerns, we propose a novel multi-source cross-domain model based on knowledge graph. Specifically, to avoid the insufficiency of single auxiliary domain, we construct a knowledge graph comprehensively leveraging items from multiple auxiliary domains. To avoid the leakage of user privacy when user information is transferred to multiple domains, we construct graph for information transfer between items to effectively avoid the propagation of users' private information between different domains. We implicitly integrate the user-item interaction by transferring the learned item embeddings. To improve the interpretability of cross-domain knowledge transfer, we propose a knowledge graph-based retrieval and fusion method to transfer knowledge derived from multiple auxiliary domains. An attention-based fusion network is designed to enhance the representation of the targeted user and items with the transferred item embedding. We perform extensive experiments on three real-world datasets, demonstrating that our model outperforms the states of the art. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesCross-domain recommendation; collaborative filtering; implicit feedback; knowledge graph; privacy preserving,Behavioral research; Domain Knowledge; Embeddings; Graphic methods; Knowledge graph; Knowledge management; Privacy-preserving techniques; User profile; Additional key word and phrasescross-domain recommendation; Cross-domain; Cross-domain modelling; Data sparsity problems; Implicit feedback; Interpretability; Key words; Knowledge graphs; Multi-Sources; Privacy preserving; Collaborative filtering
Modality-collaborative Transformer with Hybrid Feature Reconstruction for Robust Emotion Recognition,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185218612&doi=10.1145%2f3640343&partnerID=40&md5=d0d32a923b6630c6d09e43a9216e1f13,"As a vital aspect of affective computing, Multimodal Emotion Recognition has been an active research area in the multimedia community. Despite recent progress, this field still confronts two major challenges in real-world applications: (1) improving the efficiency of constructing joint representations from unaligned multimodal features and (2) relieving the performance decline caused by random modality feature missing. In this article, we propose a unified framework, Modality-Collaborative Transformer with Hybrid Feature Reconstruction (MCT-HFR), to address these issues. The crucial component of MCT is a novel attention-based encoder that concurrently extracts and dynamically balances the intra- and inter-modality relations for all associated modalities. With additional modality-wise parameter sharing, a more compact representation can be encoded with less time and space complexity. To improve the robustness of MCT, we further introduce HFR, which consists of two modules: Local Feature Imagination (LFI) and Global Feature Alignment (GFA). During model training, LFI leverages complete features as supervisory signals to recover local missing features, while GFA is designed to reduce the global semantic gap between pairwise-complete and -incomplete representations. Experimental evaluations on two popular benchmark datasets demonstrate that our proposed method consistently outperforms advanced baselines in both complete and incomplete data scenarios. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesMultimodal emotion recognition; hybrid feature reconstruction; sequential data missing; transformer,Semantics; Speech recognition; Additional key word and phrasesmultimodal emotion recognition; Data missing; Emotion recognition; Feature reconstruction; Hybrid feature reconstruction; Hybrid features; Key words; Sequential data; Sequential data missing; Transformer; Emotion Recognition
Omniscient Video Super-Resolution with Explicit-Implicit Alignment,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185216435&doi=10.1145%2f3640346&partnerID=40&md5=6426235264960a101ed24658f917e218,"When considering the temporal relationships, most previous video super-resolution (VSR) methods follow the iterative or recurrent framework. The iterative framework adopts neighboring low-resolution (LR) frames from a sliding window, while the recurrent framework utilizes the output generated in the previous SR procedure. The hybrid framework combines them but still cannot fully leverage the temporal relationships. Meanwhile, the existing methods are limited in the receptive field of the optical flow or lack semantic constrains on motion information. In this work, we propose an omniscient framework to fully explore the temporal relationships in the video, which encompasses both LR frames and SR outputs from the past, present, and future. The omniscient framework is more generic because the iterative, recurrent, and hybrid frameworks can be regarded as its special cases. Besides, when addressing the motion information, most previous VSR methods adopt the explicit motion estimation and compensation, while many recent methods turn to implicit alignment. In implicit alignment methods, because basic non-local means suffers from heavy computational costs, we improve it by capturing the non-local correlations in a relatively local manner to reduce the complexity. Moreover, we integrate the explicit and implicit methods into an explicit-implicit alignment module to better utilize motion information. We have conducted extensive experiments on public datasets, which show that our method is superior over the state-of-the-art methods in objective metrics, subjective visual quality, and complexity. In particular, on datasets of Vid4 and UDM10, our method improves PSNR by 0.19 dB, 0.49 dB against the most advanced method BasicVSR++, respectively. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesConvolutional Neural Network; Explicit-Implicit Alignment; Omniscient Framework; Video Super-Resolution,Complex networks; Iterative methods; Optical resolving power; Semantics; Additional key word and phrasesconvolutional neural network; Explicit-implicit alignment; Iterative framework; Key words; Motion information; Neural-networks; Omniscient framework; Superresolution methods; Temporal relationships; Video super-resolution; Motion estimation
An Underwater Organism Image Dataset and a Lightweight Module Designed for Object Detection Networks,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185225335&doi=10.1145%2f3640465&partnerID=40&md5=932600052960fda34396305d43f64471,"Long-term monitoring and recognition of underwater organism objects are of great significance in marine ecology, fisheries science and many other disciplines. Traditional techniques in this field, including manual fishing-based ones and sonar-based ones, are usually flawed. Specifically, the method based on manual fishing is time-consuming and unsuitable for scientific researches, while the sonar-based one, has the defects of low acoustic image accuracy and large echo errors. In recent years, the rapid development of deep learning and its excellent performance in computer vision tasks make vision-based solutions feasible. However, the researches in this area are still relatively insufficient in mainly two aspects. First, to our knowledge, there is still a lack of large-scale datasets of underwater organism images with accurate annotations. Second, in consideration of the limitation on hardware resources of underwater devices, an underwater organism detection algorithm that is both accurate and lightweight enough to be able to infer in real time is still lacking. As an attempt to fill in the aforementioned research gaps to some extent, we established the Multiple Kinds of Underwater Organisms (MKUO) dataset with accurate bounding box annotations of taxonomic information, which consists of 10,043 annotated images, covering eighty-four underwater organism categories. Based on our benchmark dataset, we evaluated a series of existing object detection algorithms to obtain their accuracy and complexity indicators as the baseline for future reference. In addition, we also propose a novel lightweight module, namely Sparse Ghost Module, designed especially for object detection networks. By substituting the standard convolution with our proposed one, the network complexity can be significantly reduced and the inference speed can be greatly improved without obvious detection accuracy loss. To make our results reproducible, the dataset and the source code are available online at https://cslinzhang.github.io/MKUO-and-Sparse-Ghost-Module/. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesBenchmark dataset; lightweight module.; object detection,Complex networks; Deep learning; Fisheries; Large datasets; Object recognition; Oceanography; Signal detection; Sonar; Underwater acoustics; Additional key word and phrasesbenchmark dataset; Detection networks; Image datasets; Key words; Lightweight module.; Long term monitoring; Marine ecology; Objects detection; Scientific researches; Traditional techniques; Object detection
Two-stream Multi-level Dynamic Point Transformer for Two-person Interaction Recognition,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185398144&doi=10.1145%2f3639470&partnerID=40&md5=42309de5fc4ee135a6872e60bb363818,"As a fundamental aspect of human life, two-person interactions contain meaningful information about people's activities, relationships, and social settings. Human action recognition serves as the foundation for many smart applications, with a strong focus on personal privacy. However, recognizing two-person interactions poses more challenges due to increased body occlusion and overlap compared to single-person actions. In this article, we propose a point cloud-based network named Two-stream Multi-level Dynamic Point Transformer for two-person interaction recognition. Our model addresses the challenge of recognizing two-person interactions by incorporating local-region spatial information, appearance information, and motion information. To achieve this, we introduce a designed frame selection method named Interval Frame Sampling (IFS), which efficiently samples frames from videos, capturing more discriminative information in a relatively short processing time. Subsequently, a frame features learning module and a two-stream multi-level feature aggregation module extract global and partial features from the sampled frames, effectively representing the local-region spatial information, appearance information, and motion information related to the interactions. Finally, we apply a transformer to perform self-attention on the learned features for the final classification. Extensive experiments are conducted on two large-scale datasets, the interaction subsets of NTU RGB+D 60 and NTU RGB+D 120. The results show that our network outperforms state-of-the-art approaches in most standard evaluation settings. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesTwo-person interaction recognition; frame sampling; point cloud-based method; transformer; two-stream multi-level feature aggregation,Additional key word and phrasestwo-person interaction recognition; Cloud-based; Feature aggregation; Frame sampling; Key words; Multilevels; Point cloud-based method; Point-clouds; Transformer; Two-stream; Two-stream multi-level feature aggregation; Large datasets
HARR: Learning Discriminative and High-Quality Hash Codes for Image Retrieval,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185395924&doi=10.1145%2f3627162&partnerID=40&md5=ced3d9c3ea42167444741c1823b1c821,"This article studies deep unsupervised hashing, which has attracted increasing attention in large-scale image retrieval. The majority of recent approaches usually reconstruct semantic similarity information, which then guides the hash code learning. However, they still fail to achieve satisfactory performance in reality for two reasons. On the one hand, without accurate supervised information, these methods usually fail to produce independent and robust hash codes with semantics information well preserved, which may hinder effective image retrieval. On the other hand, due to discrete constraints, how to effectively optimize the hashing network in an end-to-end manner with small quantization errors remains a problem. To address these difficulties, we propose a novel unsupervised hashing method called HARR to learn discriminative and high-quality hash codes. To comprehensively explore semantic similarity structure, HARR adopts the Winner-Take-All hash to model the similarity structure. Then similarity-preserving hash codes are learned under the reliable guidance of the reconstructed similarity structure. Additionally, we improve the quality of hash codes by a bit correlation reduction module, which forces the cross-correlation matrix between a batch of hash codes under different augmentations to approach the identity matrix. In this way, the generated hash bits are expected to be invariant to disturbances with minimal redundancy, which can be further interpreted as an instantiation of the information bottleneck principle. Finally, for effective hashing network training, we minimize the cosine distances between real-value network outputs and their binary codes for small quantization errors. Extensive experiments demonstrate the effectiveness of our proposed HARR. © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesDeep unsupervised hashing; large-scale image retrieval; similarity learning,Hash functions; Semantics; Additional key word and phrasesdeep unsupervised hashing; High quality; Key words; Large-scale image retrieval; Large-scales; Performance; Quantization errors; Semantic similarity; Similarity informations; Similarity learning; Image retrieval
Exploring Visual Relationships via Transformer-based Graphs for Enhanced Image Captioning,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185221963&doi=10.1145%2f3638558&partnerID=40&md5=4481bd2fd875f150fe233966f90f8b91,"Image captioning (IC), bringing vision to language, has drawn extensive attention. A crucial aspect of IC is the accurate depiction of visual relations among image objects. Visual relations encompass two primary facets: content relations and structural relations. Content relations, which comprise geometric positions content (i.e., distances and sizes) and semantic interactions content (i.e., actions and possessives), unveil the mutual correlations between objects. In contrast, structural relations pertain to the topological connectivity of object regions. Existing Transformer-based methods typically resort to geometric positions to enhance the visual relations, yet only using the shallow geometric content is unable to precisely cover actional content correlations and structural connection relations. In this article, we adopt a comprehensive perspective to examine the correlations between objects, incorporating both content relations (i.e., geometric and semantic relations) and structural relations, with the aim of generating plausible captions. To achieve this, first, we construct a geometric graph from bounding box features and a semantic graph from the scene graph parser to model the content relations. Innovatively, we construct a topology graph that amalgamates the sparsity characteristics of the geometric and semantic graphs, enabling the representation of image structural relations. Second, we propose a novel unified approach to enrich image relation representations by integrating semantic, geometric, and structural relations into self-attention. Finally, in the language decoding stage, we further leverage the semantic relation as prior knowledge to generate accurate words. Extensive experiments on MS-COCO dataset demonstrate the effectiveness of our model, with improvements of CIDEr from 128.6% to 136.6%. Codes have been released at https://github.com/CrossmodalGroup/ER-SAN/tree/main/VG-Cap.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesImage captioning; scene graph; topology graph; transformer,Geometry; Graphic methods; Integrated circuits; Knowledge management; Semantics; Topology; Additional key word and phrasesimage captioning; Geometric graphs; Geometric position; Image captioning; Key words; Scene-graphs; Semantic graphs; Semantic relations; Topology graphs; Transformer; Image enhancement
DPDFormer: A Coarse-to-Fine Model for Monocular Depth Estimation,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185220479&doi=10.1145%2f3638559&partnerID=40&md5=36ad11afedbeb399776c6422a7e5d762,"Monocular depth estimation attracts great attention from computer vision researchers for its convenience in acquiring environment depth information. Recently classification-based MDE methods show its promising performance and begin to act as an essential role in many multi-view applications such as reconstruction and 3D object detection. However, existed classification-based MDE models usually apply fixed depth range discretization strategy across a whole scene. This fixed depth range discretization leads to the imbalance of discretization scale among different depth ranges, resulting in the inexact depth range localization. In this article, to alleviate the imbalanced depth range discretization problem in classification-based monocular depth estimation (MDE) method we follow the coarse-to-fine principle and propose a novel depth range discretization method called depth post-discretization (DPD). Based on a coarse depth anchor roughly indicating the depth range, the DPD generates the depth range discretization adaptively for every position. The depth range discretization with DPD is more fine-grained around the actual depth, which is beneficial for locating the depth range more precisely for each scene position. Besides, to better manage the prediction of the coarse depth anchor and depth probability distribution for calculating the final depth, we design a dual-decoder transformer-based network, i.e., DPDFormer, which is more compatible with our proposed DPD method. We evaluate DPDFormer on popular depth datasets NYU Depth V2 and KITTI. The experimental results prove the superior performance of our proposed method.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesMonocular depth estimation; coarse-to-fine; depth post-discretization; DPDformer,Discrete event simulation; Probability distributions; Additional key word and phrasesmonocular depth estimation; Coarse to fine; Depth Estimation; Depth post-discretization; Depth range; Discretizations; Dpdformer; Estimation methods; Key words; Performance; Object detection
Weighted Guided Optional Fusion Network for RGB-T Salient Object Detection,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185335598&doi=10.1145%2f3624984&partnerID=40&md5=4f0e359679a0f929201ceaf018d4ed81,"There is no doubt that the rational and effective use of visible and thermal infrared image data information to achieve cross-modal complementary fusion is the key to improving the performance of RGB-T salient object detection (SOD). A meticulous analysis of the RGB-T SOD data reveals that it mainly consists of three scenarios in which both modalities (RGB and T) have a significant foreground and only a single modality (RGB or T) is disturbed. However, existing methods are obsessed with pursuing more effective cross-modal fusion based on treating both modalities equally. Obviously, the subjective use of equivalence has two significant limitations. Firstly, it does not allow for practical discrimination of which modality makes the dominant contribution to performance. While both modalities may have visually significant foregrounds, differences in their imaging properties will result in distinct performance contributions. Secondly, in a specific acquisition scenario, a pair of images with two modalities will contribute differently to the final detection performance due to their varying sensitivity to the same background interference. Intelligibly, for the RGB-T saliency detection task, it would be more reasonable to generate exclusive weights for the two modalities and select specific fusion mechanisms based on different weight configurations to perform cross-modal complementary integration. Consequently, we propose a weighted guided optional fusion network (WGOFNet) for RGB-T SOD. Specifically, a feature refinement module is first used to perform an initial refinement of the extracted multilevel features. Subsequently, a weight generation module (WGM) will generate exclusive network performance contribution weights for each of the two modalities, and an optional fusion module (OFM) will rely on this weight to perform particular integration of cross-modal information. Simple cross-level fusion is finally utilized to obtain the final saliency prediction map. Comprehensive experiments on three publicly available benchmark datasets demonstrate the proposed WGOFNet achieves superior performance compared with the state-of-the-art RGB-T SOD methods. The source code is available at: https://github.com/WJ-CV/WGOFNet. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSalient object detection; cross-modal fusion; modality contribution weights; RGB-T; transformer,Benchmarking; Computer vision; Image enhancement; Infrared imaging; Object recognition; Additional key word and phrasessalient object detection; Cross-modal; Cross-modal fusion; Key words; Modality contribution weight; Objects detection; Performance; RGB-T; Salient object detection; Transformer; Object detection
Learning Offset Probability Distribution for Accurate Object Detection,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185226357&doi=10.1145%2f3637214&partnerID=40&md5=d0adc9979cb7232672ca5975438d6504,"Object detection combines object classification and object localization problems. Current object detection methods heavily depend on regression networks to locate objects, which are optimized with various regression loss functions to predict offsets between candidate boxes and objects. However, these regression losses are difficult to assign the appropriate penalties for samples with large offset errors, resulting in suboptimal regression networks and inaccurate object offsets. In this article, we consider object location as offset bin classification problem, and propose a distance-aware offset bin classification network optimized with multiple binary cross entropy losses to learn various offset probability distribution, including single label distribution and distance-aware label distribution. On one hand, it provides gradient contributions for different samples based on the bounded probability instead of previous incalculable offset error. On the other hand, it explores the distance correlations between discrete offset bins to facilitate network learning. Specifically, we discretize the continuous offset into a number of bins, and predict the probability of each offset bin, in which the probability should be higher for the offset bin closer to the target offsets, and vice versa. Furthermore, we propose an expectation-based offset prediction and a hierarchical focusing method to improve the precision of prediction. We conduct extensive experiments to evaluate the effectiveness of our method. In addition, our method can be conveniently and flexibly inserted into existing object detection methods, which consistently achieves a large gain based on popular anchor-based and anchor-free methods on the PASCAL VOC, MS-COCO, KITTI, and CrowdHuman datasets. Code will be released at: https://github.com/QiuHeqian/DBC. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesObject detection; distance-aware offset bin classification; expectation-based offset prediction; hierarchical focusing method; offset probability distribution,Large datasets; Object detection; Object recognition; Probability distributions; Regression analysis; Additional key word and phrasesobject detection; Distance-aware offset bin classification; Expectation-based offset prediction; Hierarchical focusing method; Key words; Object detection method; Objects detection; Offset errors; Offset probability distribution; Probability: distributions; Forecasting
Two-Stage Perceptual Quality Oriented Rate Control Algorithm for HEVC,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185391824&doi=10.1145%2f3636510&partnerID=40&md5=7a167e6414d2396a444b0fda553823ff,"As a practical technique in mainstream video coding applications, rate control dominates important to ensure compression quality with limited bitrates constraints. However, most rate control methods mainly focus on objective quality while ignoring the perceptual quality improvement for human eyes. In this paper, we propose a two-stage rate control algorithm to optimize the perceptual quality at the frame encoding stage and the coding tree unit (CTU) encoding stage for high efficiency video coding (HEVC), respectively. Firstly, for the frame encoding stage, with inter-frame distortion dependency consideration, a frame-level rate control method is presented by adjusting the frame-level Lagrange multiplier adaptively with a preprocessing method. Secondly, for the CTU encoding stage, we propose a saliency-based CTU-level perceptual quality rate control algorithm, which employs CTU-level saliency weight to adjust the perceptual rate-distortion (R-D) model. We conduct the CTU-level rate control by an optimized Lagrange multiplier and quantization parameter (QP) to achieve perceptual quality optimization. Extensive experimental results reveal that, compared with state-of-the-art rate control methods on HEVC, our algorithm achieves significant perceptual coding performance with improved subjective visual quality. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesRate control; distortion dependency; HEVC; Lagrange multiplier; perceptual distortion model,Electric distortion; Encoding (symbols); Image coding; Quality control; Signal distortion; Signal encoding; Trees (mathematics); Video signal processing; Additional key word and phrasesrate control; Distortion dependency; Distortion model; Encodings; High-efficiency video coding; Key words; Perceptual distortion; Perceptual distortion model; Perceptual quality; Rate controls; Lagrange multipliers
Viewpoint Disentangling and Generation for Unsupervised Object Re-ID,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185403491&doi=10.1145%2f3632959&partnerID=40&md5=8f65ee046466d363248052a14beb2a28,"Unsupervised object Re-ID aims to learn discriminative identity features from a fully unlabeled dataset to solve the open-class re-identification problem. Satisfying results have been achieved in existing unsupervised Re-ID methods, primarily trained with pseudo-labels created by feature clustering. However, the viewpoint variation of objects is the key challenge, introducing noisy labels in the clustering process. To address this problem, a novel viewpoint disentangling and generation framework (VDG) is proposed to learn viewpoint-invariant ID features, including a disentangling and generation module, as well as a contrastive learning module. First, we design an ID encoder to map the viewpoint and identity features into the latent space. Second, a generator is used to disentangle view features and synthesize images with different orientations. Especially, the well-trained encoder serves as a pre-trained feature extractor in the contrastive learning module. Third, a viewpoint-aware loss and a class-level loss are integrated to facilitate contrastive learning between original and novel views. The generation of novel view images and the application of viewpoint-aware contrastive loss mutually assist model learning viewpoint-invariant ID features. Extensive experiments on Market-1501, DukeMTMC, MSMT17, and VeRi-776 demonstrate the effectiveness of the proposed VDG framework, as well as its superiority over the existing state-of-the-art approaches. The VDG model also demonstrates high quality in the image generation tasks. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesPerson re-identification; disentanglement; generation; unsupervised; viewpoint,Computer vision; Learning systems; Signal encoding; Additional key word and phrasesperson re-identification; Disentanglement; Generation; Key words; Learn+; Learning modules; Re identifications; Unsupervised; Viewpoint; Viewpoint invariant; Clustering algorithms
Controlling Media Player with Hands: A Transformer Approach and a Quality of Experience Assessment,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185390842&doi=10.1145%2f3638560&partnerID=40&md5=8071627085e4d8484ac608dc7ab88f96,"In this article, we propose a Hand Gesture Recognition (HGR) system based on a novel deep transformer (DT) neural network for media player control. The extracted hand skeleton features are processed by separate transformers for each finger in isolation to better identify the finger characteristics to drive the following classification. The achieved HGR accuracy (0.853) outperforms state-of-the-art HGR approaches when tested on the popular NVIDIA dataset. Moreover, we conducted a subjective assessment involving 30 people to evaluate the Quality of Experience (QoE) provided by the proposed DT-HGR for controlling a media player application compared with two traditional input devices, i.e., mouse and keyboard. The assessment participants were asked to evaluate objective (accuracy) and subjective (physical fatigue, usability, pragmatic quality, and hedonic quality) measurements. We found that (i) the accuracy of DT-HGR is very high (91.67%), only slightly lower than that of traditional alternative interaction modalities; and that (ii) the perceived quality for DT-HGR in terms of satisfaction, comfort, and interactivity is very high, with an average Mean Opinion Score (MOS) value as high as 4.4, whereas the alternative approaches did not reach 3.8, which encourages a more pervasive adoption of the natural gesture interaction. © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesHuman-computer interface; hand gesture recognition; media player; quality of experience; transformer neural network,Gesture recognition; Mammals; Palmprint recognition; Quality of service; Additional key word and phraseshuman-computer interface; Gesture recognition system; Hand-gesture recognition; Key words; Media players; Neural-networks; Player control; Quality of experience; Recognition accuracy; Transformer neural network; Quality control
QoE Estimation of WebRTC-based Audio-visual Conversations from Facial and Speech Features,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185218758&doi=10.1145%2f3638251&partnerID=40&md5=c027e01286950410d63e977f4ac01f02,"The utilization of user's facial- and speech-related features for the estimation of the Quality of Experience (QoE) of multimedia services is still underinvestigated despite its potential. Currently, only the use of either facial or speech features individually has been proposed, and relevant limited experiments have been performed. To advance in this respect, in this study, we focused on WebRTC-based videoconferencing, where it is often possible to capture both the facial expressions and vocal speech characteristics of the users. First, we performed thorough statistical analysis to identify the most significant facial- and speech-related features for QoE estimation, which we extracted from the participants' audio-video data collected during a subjective assessment. Second, we trained individual QoE estimation machine learning-based models on the separated facial and speech datasets. Finally, we employed data fusion techniques to combine the facial and speech datasets into a single dataset to enhance the QoE estimation performance due to the integrated knowledge provided by the fusion of facial and speech features. The obtained results demonstrate that the data fusion technique based on the Improved Centered Kernel Alignment (ICKA) allows for reaching a mean QoE estimation accuracy of 0.93, whereas the values of 0.78 and 0.86 are reached when using only facial or speech features, respectively.  © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesQuality of Experience; Data Fusion.; Facial Expressions; Machine Learning; Speech; WebRTC,Data fusion; Multimedia services; Quality of service; Speech recognition; Video conferencing; Additional key word and phrasesquality of experience; Audio-visual; Data fusion technique; Data fusion.; Facial Expressions; Facial feature; Key words; Machine-learning; Speech features; WebRTC; Machine learning
Joint Audio-Visual Attention with Contrastive Learning for More General Deepfake Detection,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185287258&doi=10.1145%2f3625100&partnerID=40&md5=baaab7adb8a78237b035fd1f14bf7847,"With the continuous advancement of deepfake technology, there has been a surge in the creation of realistic fake videos. Unfortunately, the malicious utilization of deepfake poses a significant threat to societal morality and political security. Therefore, numerous researchers have proposed various deepfake detection methods. However, traditional deepfake approaches tend to focus on specific forgery features, such as artifacts or inconsistent actions, which can be vulnerable to specialized countermeasures. Recent studies show an intrinsic correlation between facial and audio cues, which can be exploited for deepfake detection. To address these challenges and enhance the robustness and generalization of deepfake detection algorithms, we propose a novel joint audio-visual deepfake detection model named AVA-CL, which is capable of detecting deepfakes in both audio and visual domains. Furthermore, exploiting the inherent correlation and consistency between audio and visual enhances the effectiveness of deepfake detection significantly. Through extensive experiments, we demonstrate that our proposed AVA-CL model outperforms many state-of-the-art (SOTA) methods with superior robustness and generalization capabilities. This research presents a promising approach for deepfake detection and reducing the harm caused by malicious use. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesDeepfake detection; audio-visual attention; contrastive learning,Deep learning; Additional key word and phrasesdeepfake detection; Audio cues; Audio-visual; Audio-visual attention; Contrastive learning; Detection algorithm; Detection methods; Generalisation; Key words; Visual Attention; Behavioral research
Knowledge-integrated Multi-modal Movie Turning Point Identification,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185221655&doi=10.1145%2f3638557&partnerID=40&md5=3f0d46af3f716c3ca402618b2b435f27,"The rapid development of artificial intelligence provides rich technologies and tools for the automated understanding of literary works. As a comprehensive carrier of storylines, movies are natural multimodal data sources that provide sufficient data foundations, and how to fully leverage the benefits of data remains a sustainable research hotspot. In addition, the efficient representation of multi-source data also poses new challenges for information fusion technology. Therefore, we propose a knowledge-enhanced turning points identification (KTPi) method for multimodal scene recognition. First, the BiLSTM method is used to encode scene text and integrate contextual information into scene representations to complete text sequence modeling. Then, the graph structure is used to model all scenes, which strengthens long-range semantic dependencies between scenes and enhances scene representations using graph convolution network. After, the self-supervised method is used to obtain the optimal number of neighboring nodes in sparse graph. Next, actor and verb knowledge involved in the scene text are added to the multimodal data to enhance the diversity of scene feature expressions. Finally, the teacher-student network strategy is used to train the KTPi model. Experimental results show that KTPi outperforms baseline methods in scene role recognition tasks, and ablation experiments show that incorporating knowledge into multimodal model can improve its performance.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesKnowledge enhance; multimodal representation; text tagging,Graph theory; Additional key word and phrasesknowledge enhance; Key words; Knowledge integrated; Multi-modal; Multi-modal data; Multimodal representation; Scene representation; Scene Text; Text tagging; Turning-points; Semantics
CrowdGraph: Weakly supervised Crowd Counting via Pure Graph Neural Network,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185407917&doi=10.1145%2f3638774&partnerID=40&md5=cdebfc7c6b0ba195d4d0496918e5ad94,"Most existing weakly supervised crowd counting methods utilize Convolutional Neural Networks (CNN) or Transformer to estimate the total number of individuals in an image. However, both CNN-based (grid-to-count paradigm) and Transformer-based (sequence-to-count paradigm) methods take images as inputs in a regular form. This approach treats all pixels equally but cannot address the uneven distribution problem within human crowds. This challenge would lead to a decline in the counting performance of the model. Compared with grid and sequence, the graph structure could better explore the relationship among features. In this article, we propose a new graph-based crowd counting method named CrowdGraph, which reinterprets the weakly supervised crowd counting problem from a graph-to-count perspective. In the proposed CrowdGraph, each image is constructed as a graph, and a graph-based network is designed to extract features at the graph level. CrowdGraph comprises three main components: a dynamic graph convolutional backbone, a multi-scale dilated graph convolution module, and a regression head. To the best of our knowledge, CrowdGraph is the first method that is completely formulated based on the Graph Neural Network (GNN) for the crowd counting task. Extensive experiments demonstrate that the proposed CrowdGraph outperforms pure CNN-based and pure Transformer-based weakly supervised methods comprehensively and achieves highly competitive counting performance. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesCrowd counting; graph neural network; uneven distribution of crowds; weakly supervised learning,Convolution; Convolutional neural networks; Graphic methods; Supervised learning; Additional key word and phrasescrowd counting; Convolutional neural network; Distribution problem; Graph neural networks; Graph-based; Key words; Network-based; Performance; Uneven distribution of crowd; Weakly supervised learning; Graph neural networks
TinyPredNet: A Lightweight Framework for Satellite Image Sequence Prediction,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185227297&doi=10.1145%2f3638773&partnerID=40&md5=e927e6d4a3fccbd31cf2ff0a3cb20452,"Satellite image sequence prediction aims to precisely infer future satellite image frames with historical observations, which is a significant and challenging dense prediction task. Though existing deep learning models deliver promising performance for satellite image sequence prediction, the methods suffer from quite expensive training costs, especially in training time and GPU memory demand, due to the inefficiently modeling for temporal variations. This issue seriously limits the lightweight application in satellites such as space-borne forecast models. In this article, we propose a lightweight prediction framework TinyPredNet for satellite image sequence prediction, in which a spatial encoder and decoder model the intra-frame appearance features and a temporal translator captures inter-frame motion patterns. To efficiently model the temporal evolution of satellite image sequences, we carefully design a multi-scale temporal-cascaded structure and a channel attention-gated structure in the temporal translator. Comprehensive experiments are conducted on FengYun-4A (FY-4A) satellite dataset, which show that the proposed framework achieves very competitive performance with much lower computation cost compared to state-of-the-art methods. In addition, corresponding interpretability experiments are conducted to show how our designed structures work. We believe the proposed method can serve as a solid lightweight baseline for satellite image sequence prediction. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSatellite image sequence prediction; deep learning; interpretability; lightweight,Deep learning; Satellites; Additional key word and phrasessatellite image sequence prediction; Deep learning; Image sequence; Interpretability; Key words; Lightweight; Lightweight frameworks; Satellite image sequences; Satellite images; Sequence prediction; Forecasting
Context-detail-aware United Network for Single Image Deraining,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185396749&doi=10.1145%2f3639407&partnerID=40&md5=8f643a7f4ffe4ea80fa7f0b1d21c9418,"Images captured outdoors are often affected by rainy days, resulting in a severe deterioration in the visual quality of the captured images and a decrease in the performance of related applications. Therefore, single image deraining has attracted attention as a challenging research topic. Nowadays, there are two common deraining architectures in single image deraining. The first one is to restore the rain-free image by deducting rain streaks learned by the model from the rain image, but the background structure is easily mistaken for rain streaks and subtracted. The other one is to directly learn the clean background structure through the model using rain images, but it is difficult to completely remove the rain streaks due to the complexity of the information in images with rain. Therefore, current methods cannot balance rain streak removal and rain-free image background restoration in a single architecture and achieve good results. To address this issue, we propose a novel framework, namely, Context-Detail-Aware United Network (CDaUNet), which combines the above two architectures in this study. More specifically, we divide the restoration of the background structure of rain-free images and the learning of rain streaks into two independent sub-networks. The proposed Structure-Aware Rain Removal Network (SaRRN) is to learn the background structure in images to reconstruct clean rain-free images, whereas Detail-Aware Rain Streak Learning Network (DaRLN) is proposed to learn the details of rain streaks in images. Finally, we fuse the results generated by the two sub-networks through our designed Dual Architecture Fusion Network (DAFN) to reconstruct original rain images to effectively fuse the results of the two sub-networks. The experimental results show that CDaUNet achieves satisfactory performance in comparison with the state-of-the-art approaches included in rain streak removal and rain-free image structure restoration architectures on both synthetic and real image datasets, confirming the effectiveness of our method. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSingle image deraining; context awareness; detail awareness; united network,Deterioration; Image reconstruction; Network architecture; Restoration; Additional key word and phrasessingle image deraining; Context- awareness; Detail awareness; Key words; Learn+; Performance; Rainy days; Single images; Subnetworks; United network; Rain
RAST: Restorable Arbitrary Style Transfer,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185223534&doi=10.1145%2f3638770&partnerID=40&md5=e0e35852cb837b93e5f65f66fa61a67c,"The objective of arbitrary style transfer is to apply a given artistic or photo-realistic style to a target image. Although current methods have shown some success in transferring style, arbitrary style transfer still has several issues, including content leakage. Embedding an artistic style can result in unintended changes to the image content. This article proposes an iterative framework called Restorable Arbitrary Style Transfer (RAST) to effectively ensure content preservation and mitigate potential alterations to the content information. RAST can transmit both content and style information through multi-restorations and balance the content-style tradeoff in stylized images using the image restoration accuracy. To ensure RAST's effectiveness, we introduce two novel loss functions: multi-restoration loss and style difference loss. We also propose a new quantitative evaluation method to assess content preservation and style embedding performance. Experimental results show that RAST outperforms state-of-the-art methods in generating stylized images that preserve content and embed style accurately. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesNeural style transfer; multi-restorations; style difference,Embeddings; Image reconstruction; Iterative methods; 'current; Additional key word and phrasesneural style transfer; Content leakages; Embeddings; Image content; Key words; Multi-restoration; Photo-realistic; Style difference; Target images; Restoration
Graph Based Cross-Channel Transform for Color Image Compression,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182606365&doi=10.1145%2f3631710&partnerID=40&md5=1cf7cbf6e4bf24f758851d7e15e91676,"Adaptive transform coding is gaining more and more attention for better mining of image content over fixed transforms such as discrete cosine transform (DCT). As a special case, graph transform learning establishes a novel paradigm for the graph-based transforms. However, there still exists a challenge for graph transform learning-based image codecs design on natural image compression, and graph representation cannot describe regular image samples well over graph-structured data. Therefore, in this article, we propose a cross-channel graph-based transform (CCGBT) for natural color image compression. We observe that neighboring pixels having similar intensities should have similar values in the chroma channels, which means that the prominent structure of the luminance channel is related to the contours of the chrominance channels. A collaborative design of the learned graphs and their corresponding distinctive transforms lies in the assumption that a sufficiently small block can be considered smooth, meanwhile, guaranteeing the compression of the luma and chroma signals at the cost of a small overhead for coding the description of the designed luma graph. In addition, a color image compression framework based on the CCGBT is designed for comparing DCT on the classic JPEG codec. The proposed method benefits from its flexible transform block design on arbitrary sizes to exploit image content better than the fixed transform. The experimental results show that the unified graph-based transform outperforms conventional DCT, while close to discrete wavelet transform on JPEG2000 at high bit-rates. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",discrete cosine transform; dual graph; Graph fourier transform (GFT); graph learning; graph-based transform (GBT); image compression,Color; Cosine transforms; Discrete wavelet transforms; Graphic methods; Image coding; Image compression; Adaptive transform coding; Color image compression; Dual graphs; Graph fourier transform; Graph Fourier transforms; Graph learning; Graph-based; Graph-based transform; Image content; Images compression; Discrete cosine transforms
Scene Text Recognition via Dual-path Network with Shape-driven Attention Alignment,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182598099&doi=10.1145%2f3633517&partnerID=40&md5=f47ac88d4e0ea43ec9c52feefd130c06,"Scene text recognition (STR), one typical sequence-to-sequence problem, has drawn much attention recently in multimedia applications. To guarantee good performance, it is essential for STR to obtain aligned character-wise features from the whole-image feature maps. While most present works adopt fully data-driven attention-based alignment, such practice ignores specific character geometric information. In this article, built upon a group of learnable geometric points, we propose a novel shape-driven attention alignment method that is able to obtain character-wise features. Concretely, we first design a corner detector to generate a shape map to guide the attention alignments explicitly, where a series of points can be learned to represent character-wise features flexibly. We then propose a dual-path network with a mutual learning and cooperating strategy that successfully combines CNN with a ViT-based model, leading to further accuracy improvement. We conduct extensive experiments to evaluate the proposed method on various scene text benchmarks, including six popular regular and irregular datasets, two more challenging datasets (i.e., WordArt and OST), and three Chinese datasets. Experimental results indicate that our method can achieve superior performance with a comparable model size against many state-of-the-art models.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesOCR; attention alignment; deformable attention; dual path network; scene text recognition,Edge detection; Additional key word and phrasesocr; Attention alignment; Deformable attention; Dual path; Dual path network; Key words; Performance; Scene Text; Scene text recognition; Text recognition; Character recognition
Self-Adaptive Representation Learning Model for Multi-Modal Sentiment and Sarcasm Joint Analysis,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185453978&doi=10.1145%2f3635311&partnerID=40&md5=f906039fe48f2be8814ab510e604ba2d,"Sentiment and sarcasm are intimate and complex, as sarcasm often deliberately elicits an emotional response in order to achieve its specific purpose. Current challenges in multi-modal sentiment and sarcasm joint detection mainly include multi-modal representation fusion and the modeling of the intrinsic relationship between sentiment and sarcasm. To address these challenges, we propose a single-input stream self-adaptive representation learning model (SRLM) for sentiment and sarcasm joint recognition. Specifically, we divide the image into blocks to learn its serialized features and fuse textual feature as input to the target model. Then, we introduce an adaptive representation learning network using a gated network approach for sarcasm and sentiment classification. In this framework, each task is equipped with its dedicated expert network responsible for learning task-specific information, while the shared expert knowledge is acquired and weighted through the gating network. Finally, comprehensive experiments conducted on two publicly available datasets, namely Memotion and MUStARD, demonstrate the effectiveness of the proposed model when compared to state-of-the-art baselines. The results reveal a notable improvement on the performance of sentiment and sarcasm tasks. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesMulti-modal sentiment analysis; multi-task learning; representation learning; sarcasm detection,Data mining; Emotion Recognition; Modal analysis; Adaptive representations; Additional key word and phrasesmulti-modal sentiment analyse; Joint analysis; Key words; Learning models; Multi-modal; Multitask learning; Representation learning; Sarcasm detection; Sentiment analysis; Learning systems
Dynamic weighted adversarial learning for semi-supervised classification under intersectional class mismatch,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183753850&doi=10.1145%2f3635310&partnerID=40&md5=5ec05d1979fcd341ad08c2c2f332957f,"Nowadays, class-mismatch problem has drawn intensive attention in Semi-Supervised Learning (SSL), where the classes of labeled data are assumed to be only a subset of the classes of unlabeled data. However, in a more realistic scenario, the labeled data and unlabeled data often share some common classes while they also have their individual classes, which leads to an “intersectional class-mismatch” problem. As a result, existing SSL methods are often confused by these individual classes and suffer from performance degradation. To address this problem, we propose a novel Dynamic Weighted Adversarial Learning (DWAL) framework to properly utilize unlabeled data for boosting the SSL performance. Specifically, to handle the influence of the individual classes in unlabeled data (i.e., Out-Of-Distribution classes), we propose an enhanced adversarial domain adaptation to dynamically assign weight for each unlabeled example from the perspectives of domain adaptation and a class-wise weighting mechanism, which consists of transferability score and prediction confidence value. Besides, to handle the influence of the individual classes in labeled data (i.e., private classes), we propose a dissimilarity maximization strategy to suppress the inaccurate correlations caused by the examples of individual classes within labeled data. Therefore, our DWAL can properly make use of unlabeled data to acquire an accurate SSL classifier under intersectional class-mismatch setting, and extensive experimental results on five public datasets demonstrate the effectiveness of the proposed model over other state-of-the-art SSL methods. © 2024 Association for Computing Machinery. All rights reserved.",CCS Concepts: • Computing methodologies → Semi-supervised learning settings; Image representations; Neural networks,Deep learning; Image classification; Image representation; Supervised learning; Adversarial learning; CCS concept: • computing methodology → semi-supervised learning setting; Computing methodologies; Image representations; Labeled data; Learning settings; Mismatch problems; Neural-networks; Semi-supervised learning; Unlabeled data; Classification (of information)
Visual-linguistic-stylistic Triple Reward for Cross-lingual Image Captioning,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182591857&doi=10.1145%2f3634917&partnerID=40&md5=71e496400b0e527e0790a1bd8fc92e41,"Generating image captions in different languages is worth exploring and essential for non-native speakers. Nevertheless, collecting paired annotation for every language is time-consuming and impractical, particularly for minor languages. To this end, the cross-lingual image captioning task is proposed, which leverages existing image-source caption annotation data and wild unrelated target corpus to generate satisfactory caption in the target language. Current methods perform a two-step translation process of image-to-pivot (source) and pivot-to-target. The distinct two-step process comes with certain caption issues, such as the weak semantic alignment between the image and the generated caption and the generated caption's non-target language style. To address these issues, we propose an end-to-end reinforce learning framework with Visual-linguistic-stylistic Triple Reward named TriR. In TriR, we jointly consider the visual, linguistic, and stylistic alignments to generate factual, fluent, and natural caption in the target language. To be specific, the image-source caption annotation provides factual semantic guidance, whereas the unrelated target corpus guides the language style of generated caption. To achieve this, we construct a visual reward module to measure the cross-modal semantic embedding of image and target caption, a linguistic reward module to measure the cross-linguistic embedding of source and target captions, and a stylistic reward module to imitate the presentation style of target corpus. The TriR can be implemented with either classical CNN-LSTM or prevalent Transformer architecture. Extensive experiments are conducted with four cross-lingual settings, i.e., Chinese-to-English, English-to-Chinese, English-to-German, and English-to-French. Experimental results demonstrate the remarkable superiority of our method, and sufficient ablation experiments validate the beneficial impact of every reward.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesCross-lingual; image captioning; semantic matching; triple reward,Embeddings; Long short-term memory; Visual languages; Additional key word and phrasescross-lingual; Cross-lingual; Image caption; Image captioning; Image source; Key words; Non-native speakers; Semantic matching; Target language; Triple reward; Semantics
DoubleAUG: Single-domain Generalized Object Detector in Urban via Color Perturbation and Dual-style Memory,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185394652&doi=10.1145%2f3634683&partnerID=40&md5=e71c72a09a536960559114dd706cf80b,"Object detection in urban scenarios is crucial for autonomous driving in intelligent traffic systems. However, unlike conventional object detection tasks, urban-scene images vary greatly in style. For example, images taken on sunny days differ significantly from those taken on rainy days. Therefore, models trained on sunny-day images may not generalize well to rainy-day images. In this article, we aim to solve the single-domain generalizable object detection task in urban scenarios, meaning that a model trained on images from one weather condition should be able to perform well on images from any other weather conditions. To address this challenge, we propose a novel Double AUGmentation (DoubleAUG) method that includes image- and feature-level augmentation schemes. In the image-level augmentation, we consider the variation in color information across different weather conditions and propose a Color Perturbation (CP) method that randomly exchanges the RGB channels to generate various images. In the feature-level augmentation, we propose to utilize a Dual-Style Memory (DSM) to explore the diverse style information on the entire dataset, further enhancing the model's generalization capability. Extensive experiments demonstrate that our proposed method outperforms state-of-the-art methods. Furthermore, ablation studies confirm the effectiveness of each module in our proposed method. Moreover, our method is plug-and-play and can be integrated into existing methods to further improve model performance. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSingle-domain generalization; DoubleAUG; Object detection,Color; Meteorology; Object recognition; Additional key word and phrasessingle-domain generalization; Condition; Detection tasks; Double augmentation; Generalisation; Key words; Objects detection; Single domains; Sunny days; Urban scenarios; Object detection
Graph Pooling Inference Network for Text-based VQA,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182596374&doi=10.1145%2f3634918&partnerID=40&md5=de4ed0a4513e9c0633cbcfe647ad787a,"Effectively leveraging objects and optical character recognition (OCR) tokens to reason out pivotal scene text is critical for the challenging Text-based Visual Question Answering (TextVQA) task. Graph-based models can effectively capture the semantic relationship among visual entities (objects and tokens) and report remarkable performance in TextVQA. However, previous efforts usually leverage all visual entities and ignore the negative effect of superfluous entities. This article presents a Graph Pooling Inference Network (GPIN), which is an evolutionary graph learning method to purify the visual entities and capture the core semantics. It is observed that the dense distribution of reduplicative objects and the crowd of semantically dependent OCR tokens usually co-exist in the image. Motivated by this, GPIN adopts an adaptive node dropping strategy to dynamically downscale semantically closed nodes for graph evolution and update. To deepen the comprehension of scene text, GPIN is a dual-path hierarchical graph architecture that progressively aggregates the evolved object graph and the evolved token graph semantics into a graph vector that serves as visual cues to facilitate the answer reasoning. It can effectively eliminate object redundancy and enhance the association of semantically continuous tokens. Experiments conducted on TextVQA and ST-VQA datasets show that GPIN achieves promising performance compared with state-of-the-art methods.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesText-based visual question answering; graph inference; graph pooling,Graph theory; Graphic methods; Learning systems; Semantics; Additional key word and phrasestext-based visual question answering; Graph inference; Graph pooling; Inference network; Key words; Performance; Question Answering; Question Answering Task; Scene Text; Visual entities; Optical character recognition
Multi-object Tracking with Spatial-Temporal Tracklet Association,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185216121&doi=10.1145%2f3635155&partnerID=40&md5=2dac68e2bf539af806ba97e87a67ab02,"Recently, the tracking-by-detection methods have achieved excellent performance in Multi-Object Tracking (MOT), which focuses on obtaining a robust feature for each object and generating tracklets based on feature similarity. However, they are confronted with two issues: (1) unstable features in short-term occlusion and (2) insufficient matching in long-term occlusion. Specifically, the unstable feature is caused by the appearance variation under occlusion, and the association with the current unstable feature will lead to insufficient matching in long-term occlusion. To address the above issues, we propose a two-stage tracklet-level association method, Spatial-Temporal Tracklet Association (STTA), to effectively combine spatial-temporal context between feature extraction and data association. In the first stage, we propose the Tracklet-guided Spatial-Temporal Attention network (TSTA) to generate robust and stable features. Specifically, TSTA captures spatial-temporal context to obtain the most salient regions between the current and previous clips. In the second stage, we design the Bi-Tracklet Spatial-Temporal association (BTST) module to fully exploit the spatial-temporal context in data association. Specifically, we leverage BTST to merge different tracklets into long-term trajectories by jointly learning visual feature and spatial-temporal context and designing a bidirectional interpolation to recover the missed objects between matched tracklets. Extensive experiments of public and private detections on four benchmarks demonstrate the robustness of STTA. Furthermore, the proposed method is a model-agnostic method, which can be plugged and played with existing methods to boost their performance, e.g., obtain 11.0%, 10.1%, 2.9%, 3.2%, and 7.8% improvement on IDF1 in the MOT16 validation dataset for Tracktor, CenterTrack, Deepsort, JDE, and CTracker, respectively.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesMulti-object tracking; bi-tracklet spatial-temporal association; spatial-temporal tracklet association; tracklet-guided spatial-temporal attention network,Additional key word and phrasesmulti-object tracking; Bi-tracklet spatial-temporal association; Key words; Object Tracking; Spatial temporals; Spatial-temporal tracklet association; Temporal association; Tracklet associations; Tracklet-guided spatial-temporal attention network; Tracklets; Feature extraction
JPEG-compatible Joint Image Compression and Encryption Algorithm with File Size Preservation,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182598204&doi=10.1145%2f3633459&partnerID=40&md5=eb371fde249a3dd86ac8f60d31e56c68,"Joint image compression and encryption algorithms are intensively investigated due to their powerful capability of simultaneous image data compression and sensitive information protection. Unfortunately, most of the existing algorithms suffered from either poor compression efficiency or weak encryption strength, making them vulnerable to cryptanalysis. To address these limitations, we propose a chaos-based JPEG-compatible joint image compression and encryption algorithm. We separate the luminance and chrominance coefficients to preserve file size and encrypt the discrete cosine transform (DCT) coefficients in parallel. The proposed inter-block DC encryption strategy achieves high encryption intensity based on the permutation-substitution structure. In addition, we apply both inter- and intra-block permutations to AC coefficients and strengthen the encryption using an inter-block substitution for non-zero AC coefficients. The results of security and performance analyses demonstrate that the proposed algorithm offers robust encryption of image data while maintaining compression efficiency for real-time transmission.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesImage encryption; Chaotic system; File size preservation; JPEG-compatible; Parallel encryption,Cryptography; Discrete cosine transforms; Efficiency; Image compression; Sensitive data; AC coefficients; Additional key word and phrasesimage encryption; Compression efficiency; File size preservation; Image compression algorithms; Image encryption algorithm; Joint image compression and encryptions; JPEG-compatible; Key words; Parallel encryption; Chaotic systems
Deep Modular Co-Attention Shifting Network for Multimodal Sentiment Analysis,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182608097&doi=10.1145%2f3634706&partnerID=40&md5=7c2b464e581fb3342d5bd6ee65bedf63,"Human Multimodal Sentiment Analysis (MSA) is an attractive research that studies sentiment expressed from multiple heterogeneous modalities. While transformer-based methods have achieved great success, designing an effective ""co-attention""model to associate text modality with nonverbal modalities remains challenging. There are two main problems: 1) the dominant role of the text in modalities is underutilization, and 2) the interaction between modalities is not sufficiently explored. This paper proposes a deep modular Co-Attention Shifting Network (CoASN) for MSA. A Cross-modal Modulation Module based on Co-attention (CMMC) and an Advanced Modality-mixing Adaptation Gate (AMAG) are constructed. The CMMC consists of the Text-guided Co-Attention (TCA) and Interior Transformer Encoder (ITE) units to capture inter-modal features and intra-modal features. With text modality as the core, the CMMC module aims to guide and promote the expression of emotion in nonverbal modalities, and the nonverbal modalities increase the richness of the text-based multimodal sentiment information. In addition, the AMAG module is introduced to explore the dynamical correlations among all modalities. Particularly, this efficient module first captures the nonverbal shifted representations and then combines them to calculate the shifted word embedding representations for the final MSA tasks. Extensive experiments on two commonly used datasets, CMU-MOSI and CMU-MOSEI, demonstrate that our proposed method is superior to the state-of-the-art performance.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesMultimodal sentiment analysis; co-attention; cross-modal modulation; fine-tuning; modality-mixing adaptation gate; shifted representations,Mixing; Modal analysis; Additional key word and phrasesmultimodal sentiment analyse; Co-attention; Cross-modal; Cross-modal modulation; Fine tuning; Key words; Modality-mixing adaptation gate; Multi-modal; Sentiment analysis; Shifted representation; Sentiment analysis
"One-Bit Supervision for Image Classification: Problem, Solution, and beyond",2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182607779&doi=10.1145%2f3633779&partnerID=40&md5=783e94881e7ab38d6da00d6fd6b3d596,"This article presents one-bit supervision, a novel setting of learning with fewer labels, for image classification. Instead of the training model using the accurate label of each sample, our setting requires the model to interact with the system by predicting the class label of each sample and learn from the answer whether the guess is correct, which provides one bit (yes or no) of information. An intriguing property of the setting is that the burden of annotation largely is alleviated in comparison to offering the accurate label. There are two keys to one-bit supervision: (i) improving the guess accuracy and (ii) making good use of the incorrect guesses. To achieve these goals, we propose a multi-stage training paradigm and incorporate negative label suppression into an off-the-shelf semi-supervised learning algorithm. Theoretical analysis shows that one-bit annotation is more efficient than full-bit annotation in most cases and gives the conditions of combining our approach with active learning. Inspired by this, we further integrate the one-bit supervision framework into the self-supervised learning algorithm, which yields an even more efficient training schedule. Different from training from scratch, when self-supervised learning is used for initialization, both hard example mining and class balance are verified to be effective in boosting the learning performance. However, these two frameworks still need full-bit labels in the initial stage. To cast off this burden, we utilize unsupervised domain adaptation to train the initial model and conduct pure one-bit annotations on the target dataset. In multiple benchmarks, the learning efficiency of the proposed approach surpasses that using full-bit, semi-supervised supervision.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",active learning; Additional Key Words and PhrasesOne-bit supervision; self-supervised learning; semi-supervised learning; unsupervised domain adaptation,Image classification; Learning algorithms; Learning systems; Active Learning; Additional key word and phrasesone-bit supervision; Domain adaptation; Images classification; Key words; Problem solutions; Self-supervised learning; Semi-supervised learning; Training model; Unsupervised domain adaptation; Supervised learning
Auxiliary Information Guided Self-Attention for Image Quality Assessment,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182602523&doi=10.1145%2f3635716&partnerID=40&md5=d3b2ab529c3f57bef7761db24dd416f0,"Image quality assessment (IQA) is an important problem in computer vision with many applications. We propose a transformer-based multi-Task learning framework for the IQA task. Two subtasks: constructing an auxiliary information error map and completing image quality prediction, are jointly optimized using a shared feature extractor. We use visual transformers (ViT) as a feature extractor for feature extraction and guide ViT to focus on image quality-related features by building auxiliary information error map subtask. In particular, we propose a fusion network that includes a channel focus module. Unlike the fusion methods commonly used in previous IQA methods, we use the fusion network, including the channel attention module, to fuse the auxiliary information error map features with the image features, which facilitates the model to mine the image quality features for more accurate image quality assessment. And by jointly optimizing the two subtasks, ViT focuses more on extracting image quality features and building a more precise mapping from feature representation to quality score. With slight adjustments to the model, our approach can be used in both no-reference (NR) and full-reference (FR) IQA environments. We evaluate the proposed method in multiple IQA databases, showing better performance than state-of-The-Art FR and NR IQA methods.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",full-reference; image quality assessment; multi-Task learning; no-reference; Transformer,Errors; Image fusion; Image representation; Learning systems; Auxiliary information; Error map; Feature extractor; Full references; Image quality assessment; Multitask learning; No-reference; Quality features; Subtask; Transformer; Image quality
Toward Egocentric Compositional Action Anticipation with Adaptive Semantic Debiasing,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185528064&doi=10.1145%2f3633333&partnerID=40&md5=f1ba7f0cc50ee359a8c83a055cf26273,"Predicting the unknown from the first-person perspective is expected as a necessary step toward machine intelligence, which is essential for practical applications including autonomous driving and robotics. As a human-level task, egocentric action anticipation aims at predicting an unknown action seconds before it is performed from the first-person viewpoint. Egocentric actions are usually provided as verb-noun pairs; however, predicting the unknown action may be trapped in insufficient training data for all possible combinations. Therefore, it is crucial for intelligent systems to use limited known verb-noun pairs to predict new combinations of actions that have never appeared, which is known as compositional generalization. In this article, we are the first to explore the egocentric compositional action anticipation problem, which is more in line with real-world settings but neglected by existing studies. Whereas prediction results are prone to suffer from semantic bias considering the distinct difference between training and test distributions, we further introduce a general and flexible adaptive semantic debiasing framework that is compatible with different deep neural networks. To capture and mitigate semantic bias, we can imagine one counterfactual situation where no visual representations have been observed and only semantic patterns of observation are used to predict the next action. Instead of the traditional counterfactual analysis scheme that reduces semantic bias in a mindless way, we devise a novel counterfactual analysis scheme to adaptively amplify or penalize the effect of semantic experience by considering the discrepancy both among categories and among examples. We also demonstrate that the traditional counterfactual analysis scheme is a special case of the devised adaptive counterfactual analysis scheme. We conduct experiments on three large-scale egocentric video datasets. Experimental results verify the superiority and effectiveness of our proposed solution. © 2024 Copyright held by the owner/author(s).",adaptive counterfactual analysis; Additional Key Words and PhrasesEgocentric video understanding; compositional action anticipation; semantic bias,Computer vision; Deep neural networks; Forecasting; Intelligent systems; Large datasets; Action anticipations; Adaptive counterfactual analyse; Additional key word and phrasesegocentric video understanding; Compositional action anticipation; Counterfactuals; De-biasing; First-person perspectives; Key words; Semantic bias; Video understanding; Semantics
Arbitrary Virtual Try-on Network: Characteristics Preservation and Tradeoff between Body and Clothing,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185406038&doi=10.1145%2f3636426&partnerID=40&md5=2be423926b108d098f980c6dbd64bbff,"Deep learning based virtual try-on system has achieved some encouraging progress recently, but there still remain several big challenges that need to be solved, such as trying on arbitrary clothes of all types, trying on the clothes from one category to another and generating image-realistic results with few artifacts. To handle this issue, we in this article first collect a new dataset with all types of clothes, i.e., tops, bottoms, and whole clothes, each one has multiple categories with rich information of clothing characteristics such as patterns, logos, and other details. Based on this dataset, we then propose the Arbitrary Virtual Try-On Network (AVTON) that is utilized for all-type clothes, which can synthesize realistic try-on images by preserving and trading off characteristics of the target clothes and the reference person. Our approach includes three modules: (1) Limbs Prediction Module, which is utilized for predicting the human body parts by preserving the characteristics of the reference person. This is especially good for handling cross-category try-on task (e.g., long sleeves ĝ†""short sleeves or long pants ĝ†""skirts), where the exposed arms or legs with the skin colors and details can be reasonably predicted; (2) Improved Geometric Matching Module, which is designed to warp clothes according to the geometry of the target person. We improve the TPS based warping method with a compactly supported radial function (Wendland's ψ-function); (3) Trade-Off Fusion Module, which is to tradeoff the characteristics of the warped clothes and the reference person. This module is to make the generated try-on images look more natural and realistic based on a fine-tune symmetry of the network structure. Extensive simulations are conducted and our approach can achieve better performance compared with the state-of-the-art virtual try-on methods. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesDeep learning; artificial intelligence in fashion; generative adversarial networks; virtual try-on,Deep learning; E-learning; Economic and social effects; Hosiery manufacture; Additional key word and phrasesdeep learning; Artificial intelligence in fashion; Body parts; Geometric matching; Human bodies; Key words; Network characteristics; Reference persons; Skin colour; Virtual try-on; Generative adversarial networks
Efficient Video Transformers via Spatial-temporal Token Merging for Action Recognition,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182607420&doi=10.1145%2f3633781&partnerID=40&md5=7f1ef2bb31ffacd6aeab5d1f688e55e7,"Transformer has exhibited promising performance in various video recognition tasks but brings a huge computational cost in modeling spatial-temporal cues. This work aims to boost the efficiency of existing video transformers for action recognition through eliminating redundancies in their tokens and efficiently learning motion cues of moving objects. We propose a lightweight and plug-and-play module, namely Spatial-temporal Token Merger (STTM), to merge the tokens belonging to the same object into a more compact representation. STTM first adaptively identifies crucial object clues underlying the video as meta tokens. Similarity scores between input tokens and meta tokens are hence computed and used to guide the fusion of similar tokens in both spatial and temporal domains, respectively. To compensate for motion cues lost in the merging procedure, we compute the linear aggregation of spatial-temporal positions of tokens as motion features. STTM hence outputs a compact set of tokens fusing both appearance and motion features of moving objects. This procedure substantially decreases the number of tokens that need to be processed by each Transformer block and boosts the efficiency. As a general module, STTM can be applied to different layers of various video Transformers. Extensive experiments on the action recognition datasets Kinectics-400 and SSv2 demonstrate its promising performance. For example, it reduces the computation complexity of ViT by 38% while maintaining a similar performance on Kinectics-400. It also brings 1.7% gains of top-1 accuracy on SSv2 under the same computational cost.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesEfficient video recognition; deep learning; spatial-temporal information; transformer,Deep learning; Merging; Action recognition; Additional key word and phrasesefficient video recognition; Deep learning; Key words; Performance; Spatial temporals; Spatial-temporal information; Temporal information; Transformer; Video recognition; Efficiency
Exploring Neighbor Correspondence Matching for Multiple-hypotheses Video Frame Synthesis,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182603067&doi=10.1145%2f3633780&partnerID=40&md5=810856fc6f341cb1dd68c03375e9cb8a,"Video frame synthesis, which consists of interpolation and extrapolation, is an essential video processing technique that can be applied to various scenarios. However, most existing methods cannot handle small objects or large motion well, especially in high-resolution videos such as 4K videos. To eliminate such limitations, we introduce a neighbor correspondence matching (NCM) algorithm for flow-based frame synthesis. Since the current frame is not available in video frame synthesis, NCM is performed in a current-frame-agnostic fashion to establish multi-scale correspondences in the spatial-temporal neighborhoods of each pixel. Based on the powerful motion representation capability of NCM, we propose a heterogeneous coarse-to-fine scheme for intermediate flow estimation. The coarse-scale and fine-scale modules are trained progressively, making NCM computationally efficient and robust to large motions. We further explore the mechanism of NCM and find that neighbor correspondence is powerful, since it provides multiple-hypotheses motion information for synthesis. Based on this analysis, we introduce a multiple-hypotheses estimation process for video frame extrapolation, resulting in a more robust framework, NCM-MH. Experimental results show that NCM and NCM-MH achieve 31.63 and 28.08 dB for interpolation and extrapolation on the most challenging X4K1000FPS benchmark, outperforming all the other state-of-the-art methods that use two reference frames as input.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesVideo frame synthesis; correspondence matching,Interpolation; Video signal processing; Additional key word and phrasesvideo frame synthesis; Correspondence matching; Current frame; Key words; Matchings; Multiple hypothesis; Processing technique; Small objects; Video frame; Video processing; Extrapolation
Transform-Equivariant Consistency Learning for Temporal Sentence Grounding,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182593969&doi=10.1145%2f3634749&partnerID=40&md5=560a8f39159a053d3925c097d218270c,"This paper addresses the temporal sentence grounding (TSG). Although existing methods have made decent achievements in this task, they not only severely rely on abundant video-query paired data for training, but also easily fail into the dataset distribution bias. To alleviate these limitations, we introduce a novel Equivariant Consistency Regulation Learning (ECRL) framework to learn more discriminative query-related frame-wise representations for each video, in a self-supervised manner. Our motivation comes from that the temporal boundary of the query-guided activity should be consistently predicted under various video-level transformations. Concretely, we first design a series of spatio-temporal augmentations on both foreground and background video segments to generate a set of synthetic video samples. In particular, we devise a self-refine module to enhance the completeness and smoothness of the augmented video. Then, we present a novel self-supervised consistency loss (SSCL) applied on the original and augmented videos to capture their invariant query-related semantic by minimizing the KL-divergence between the sequence similarity of two videos and a prior Gaussian distribution of timestamp distance. At last, a shared grounding head is introduced to predict the transform-equivariant query-guided segment boundaries for both the original and augmented videos. Extensive experiments on three challenging datasets (ActivityNet, TACoS, and Charades-STA) demonstrate both effectiveness and efficiency of our proposed ECRL framework.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesTemporal sentence grounding; consistency learning; equivariant; transformation,Additional key word and phrasestemporal sentence grounding; Consistency learning; Equivariant; First designs; Key words; Learn+; Learning frameworks; Level transformation; Spatio-temporal; Transformation; Semantics
Secure Low-complexity Compressive Sensing with Preconditioning Prior Regularization Reconstruction,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182587696&doi=10.1145%2f3635308&partnerID=40&md5=f644d9575d311297e2f2ae590724169b,"Compressive sensing (CS), a breakthrough technology in image processing, provides a privacy-preserving layer and image reconstruction while performing sensing and recovery processes, respectively. Unfortunately, it still faces high-complexity, low-security, and low-quality reconstruction challenges during image processing. Therefore, this article presents a secure low-complexity CS scheme with preconditioning prior regularization reconstruction. More specifically, the original image is compressed by a low-complexity LFSR-based sparse circulant matrix to obtain measurements. It is worth noting that measurements achieve preliminary distribution equalization through the Tanh sequence to acquire processed measurements. Furthermore, the privacy-preserving edge processing for processed measurements can achieve high security. Finally, preconditioning prior regularization CS reconstruction is designed to improve reconstruction performance. Simulation results and analyses demonstrate that the proposed scheme can achieve low-complexity sampling, high security, and superior reconstruction performance.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesCompressive sensing; joint quantization and diffusion; preconditioning prior regularization reconstruction; sparse circulant matrix,Image reconstruction; Privacy-preserving techniques; Additional key word and phrasescompressive sensing; Compressive sensing; Images processing; Joint quantization and diffusion; Key words; Lower complexity; Preconditioning prior regularization reconstruction; Quantisation; Regularization reconstruction; Sparse circulant matrices; Compressed sensing
Divide-and-conquer-based RDO-free CU Partitioning for 8K Video Compression,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182594810&doi=10.1145%2f3634705&partnerID=40&md5=c0b4d0851eb0c6da37276de06fad6c9d,"8K (7689×4320) ultra-high definition (UHD) videos are growing popular with the improvement of human visual experience demand. Therefore, the compression of 8K UHD videos has become a top priority in the third-generation audio video coding standard (AVS3). However, as an important part of the coding standard promotion, the real-time hardware implementation for AVS3-based 8K UHD video intra coding is severely hindered, especially in the coding unit (CU) partition stage. To break through the limitation, this article proposes a divide-and-conquer-based rate-distortion-optimization-free (RDO-free) CU partitioning algorithm for efficient hardware implementation. Aimed at the complex CU partition in AVS3, we separately design a lightweight optimization for original partitioning rules to improve division efficiency and a decision tree-based RDO-free CU decision framework to eliminate the latency caused by the waiting for rate-distortion cost calculation in RDO strategy. Afterward, a divide-and-conquer-based hardware-friendly gradient difference calculating approach is devised to accelerate the learning feature extracting speed. To ensure that the proposed algorithm is sufficient to support the real-time CU partition for 8K videos, we also develop a hardware architecture based on FPGA. Experimental results illustrate that the software coding performance of our algorithm is significantly ahead of the efficient implementation uAVS3e for AVS3 and the reference software HM-16.20 for High Efficiency Video Coding (HEVC), even though there is 9.96% loss on BD-Rate Y. Considering its importance for the hardware implementation of AVS3-based 8K real-time encoder, the coding loss is acceptable. Moreover, the hardware simulation results on VU440 FPGA with Vivado 2019 show that our algorithm can support 61.12 frames per second (fps) CU partition for 8K UHD videos with only 0.00%, 0.00%, 1.01%, and 7.78% consumption of BRAM_18K, DSP48E, FF, and LUT, respectively. Additionally, with dual-path parallelism, 122.24 fps also can be implemented with controllable resource utilization, which achieves the state-of-the-art performance.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",8K UHD; Additional Key Words and PhrasesAVS3; CU decision; hardware implementation; RDO-free,Efficiency; Electric distortion; Field programmable gate arrays (FPGA); Image coding; Image compression; Optimization; Signal distortion; Video signal processing; 8k ultra-high definition; Additional key word and phrasesavs3; Coding unit decision; Hardware implementations; High definition; High-definition videos; Key words; Rate-distortion optimization; Rate-distortion-optimization-free; Ultra-high; Decision trees
Multiple Pseudo-Siamese Network with Supervised Contrast Learning for Medical Multi-modal Retrieval,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185405192&doi=10.1145%2f3637441&partnerID=40&md5=fa71052dc1d2e897b7aba638d927b7ad,"Medical multi-modal retrieval aims to provide doctors with similar medical images from different modalities, which can greatly promote the efficiency and accuracy of clinical diagnosis. However, most existing medical retrieval methods hardly support the retrieval of multi-modal medical images, i.e., the number of modalities is greater than 2, and just convert retrieval to classification or clustering. It futilely breaks the gap between the visual information and the semantic information in different medical image modalities. To solve the problem, a Supervised Contrast Learning method based on a Multiple Pseudo-Siamese network (SCL-MPS) is proposed for multi-modal medical image retrieval. In order to make the samples with semantic similarity close neighbors on Riemann manifold, the multiple constraints based on semantic consistency and modal invariance are designed in different forward stages of SCL-MPS. We theoretically demonstrate the feasibility of the designed constraints. Finally, experiments on four benchmark datasets (ADNI1, ADNI2, ADNI3, and OASIS3) show that SCL-MPS achieves state-of-the-art performance compared to 15 retrieval methods. Especially, SCL-MPS achieves a 100% mAP score in medical cross-modal retrieval on ADNI1. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesPseudo-siamese network; medical image retrieval; multi-modal; semantic consistency; supervised contrast learning,Diagnosis; Image retrieval; Learning systems; Medical imaging; Semantic Web; Semantics; Additional key word and phrasespseudo-siamese network; Clinical diagnosis; Clusterings; Key words; Medical image retrieval; Medical retrieval; Multi-modal; Retrieval methods; Semantic consistency; Supervised contrast learning; Benchmarking
CMAF: Cross-Modal Augmentation via Fusion for Underwater Acoustic Image Recognition,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185395701&doi=10.1145%2f3636427&partnerID=40&md5=638802595f9d514c4465a959112c28e9,"Underwater image recognition is crucial for underwater detection applications. Fish classification has been one of the emerging research areas in recent years. Existing image classification models usually classify data collected from terrestrial environments. However, existing image classification models trained with terrestrial data are unsuitable for underwater images, as identifying underwater data is challenging due to their incomplete and noisy features. To address this, we propose a cross-modal augmentation via fusion (CMAF) framework for acoustic-based fish image classification. Our approach involves separating the process into two branches: visual modality and sonar signal modality, where the latter provides a complementary character feature. We augment the visual modality, design an attention-based fusion module, and adopt a masking-based training strategy with a mask-based focal loss to improve the learning of local features and address the class imbalance problem. Our proposed method outperforms the state-of-the-art methods. Our source code is available at https://github.com/WilkinsYang/CMAF. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesNeural networks; class imbalance; multi-modal fusion; sonar image,Classification (of information); Fish; Image fusion; Image recognition; Sonar; Underwater acoustics; Additional key word and phrasesneural network; Class imbalance; Classification models; Cross-modal; Images classification; Key words; Multi-modal fusion; Sonar image; Underwater acoustic image; Visual modalities; Image classification
Principal Component Approximation Network for Image Compression,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185215532&doi=10.1145%2f3637490&partnerID=40&md5=87d4451e6932a8a81dd6d536f991498e,"In this work, we propose a novel principal component approximation network (PCANet) for image compression. The proposed network is based on the assumption that a set of images can be decomposed into several shared feature matrices, and an image can be reconstructed by the weighted sum of these matrices. The proposed PCANet is specifically devised to learn and approximate these feature matrices and weight vectors, which are used to encode images for compression. Unlike previous deep learning-based methods, a distinctive aspect of our approach is its consideration of network size in the bit-rate computation. Despite this inclusion, our proposed method yields promising results. Through extensive experiments conducted on standard datasets, we demonstrate the effectiveness of our approach in comparison to state-of-the-art techniques. To the best of our knowledge, this is the first machine learning approach that includes the size of networks during bitrate computation in image compression. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesImage compression; decomposition; neural network,Deep learning; Matrix algebra; Additional key word and phrasesimage compression; Approximation networks; Bit rates; Feature matrices; Images compression; Key words; matrix; Neural-networks; Principal Components; Weighted Sum; Image compression
P2ANet: A Large-Scale Benchmark for Dense Action Detection from Table Tennis Match Broadcasting Videos,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182602474&doi=10.1145%2f3633516&partnerID=40&md5=059b1457bd291d31862df3e9c76e0091,"While deep learning has been widely used for video analytics, such as video classification and action detection, dense action detection with fast-moving subjects from sports videos is still challenging. In this work, we release yet another sports video benchmark P2ANet for Ping Pong-Action detection, which consists of 2,721 video clips collected from the broadcasting videos of professional table tennis matches in World Table Tennis Championships and Olympiads. We work with a crew of table tennis professionals and referees on a specially designed annotation toolbox to obtain fine-grained action labels (in 14 classes) for every ping-pong action that appeared in the dataset, and formulate two sets of action detection problems - action localization and action recognition. We evaluate a number of commonly seen action recognition (e.g., TSM, TSN, Video SwinTransformer, and Slowfast) and action localization models (e.g., BSN, BSN++, BMN, TCANet), using P2ANet for both problems, under various settings. These models can only achieve 48% area under the AR-AN curve for localization and 82% top-one accuracy for recognition since the ping-pong actions are dense with fast-moving subjects but broadcasting videos are with only 25 FPS. The results confirm that P2ANet is still a challenging task and can be used as a special benchmark for dense action detection from videos. We invite readers to examine our dataset by visiting the following link: https://github.com/Fred1991/P2ANET.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",action recognition and localization; Additional Key Words and PhrasesDatasets; annotation toolbox; table tennis; video analysis,Deep learning; Sports; Action recognition; Action recognition and localization; Additional key word and phrasesdataset; Annotation toolbox; Key words; Localisation; Moving subject; Ping-pongs; Table-tennis; Video analysis; Broadcasting
Synthetic data and hierarchical object detection in overhead imagery,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183752568&doi=10.1145%2f3635309&partnerID=40&md5=be4c5039856a34393d9b2ddf6299ad39,"The performance of neural network models is often limited by the availability of big datasets. To treat this problem, we survey and develop novel synthetic data generation and augmentation techniques for enhancing low/zero-sample learning in satellite imagery. In addition to extending synthetic data generation approaches, we propose a hierarchical detection architecture to improve the utility of synthetic training samples. We consider existing techniques for producing synthetic imagery–3D models and neural style transfer–as well as introducing our own adversarially trained reskinning network, the GAN-Reskinner, to blend 3D models. Additionally, we test the value of synthetic data in a two-stage, hierarchical detection/classification model of our own construction. To test the effectiveness of synthetic imagery, we employ it in the training of detection models and our two stage model, and evaluate the resulting models on real satellite images. All modalities of synthetic data are tested extensively on practical, geospatial analysis problems. Our experiments show that synthetic data developed using our approach can often enhance detection performance, particularly when combined with some real training images. When the only source of data is synthetic, our GAN-Reskinner often boosts performance over conventionally rendered 3D models and in all cases, the hierarchical model outperforms the baseline end-to-end detection architecture. © 2024 Association for Computing Machinery. All rights reserved.",Boosting; CCS Concepts: • Computing methodologies → Neural networks,Deep learning; Hierarchical systems; Image enhancement; Network architecture; Satellite imagery; 3D models; 3d-modeling; Boosting; CCS concept: • computing methodology → neural network; Computing methodologies; Neural-networks; Performance; Synthetic data; Synthetic data generations; Synthetic imagery; Object detection
Incomplete Cross-Modal Retrieval with Deep Correlation Transfer,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185225560&doi=10.1145%2f3637442&partnerID=40&md5=5f9bc328a765cc687406d13fb30f037c,"Most cross-modal retrieval methods assume the multi-modal training data is complete and has a one-to-one correspondence. However, in the real world, multi-modal data generally suffers from missing modality information due to the uncertainty of data collection and storage processes, which limits the practical application of existing cross-modal retrieval methods. Although some solutions have been proposed to generate the missing modality data using a single pseudo sample, this may lead to incomplete semantic restoration and sub-optimal retrieval results due to the limited semantic information it provides. To address this challenge, this article proposes an Incomplete Cross-Modal Retrieval with Deep Correlation Transfer (ICMR-DCT) method that can robustly model incomplete multi-modal data and dynamically capture the adjacency semantic correlation for cross-modal retrieval. Specifically, we construct intra-modal graph attention-based auto-encoder to learn modality-invariant representations by performing semantic reconstruction through intra-modality adjacency correlation mining. Then, we design dual cross-modal alignment constraints to project multi-modal representations into a common semantic space, thus bridging the heterogeneous modality gap and enhancing the discriminability of the common representation. We further introduce semantic preservation to enhance adjacency semantic information and achieve cross-modal semantic correlation. Moreover, we propose a nearest-neighbor weighting integration strategy with cross-modal correlation transfer to generate the missing modality data according to inter-modality mapping relations and adjacency correlations between each sample and its neighbors, which improves the robustness of our method against incomplete multi-modal training data. Extensive experiments on three widely tested benchmark datasets demonstrate the superior performance of our method in cross-modal retrieval tasks under both complete and incomplete retrieval scenarios. Our used datasets and source codes are available at https://github.com/shidan0122/DCT.git.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesIncomplete cross-modal retrieval; adjacency semantic correlation; graph attention; robustness,Benchmarking; Digital storage; Information retrieval; Modal analysis; Additional key word and phrasesincomplete cross-modal retrieval; Adjacency semantic correlation; Cross-modal; Graph attention; Key words; Multi-modal; Multi-modal data; Retrieval methods; Robustness; Training data; Semantics
Supervised Hierarchical Online Hashing for Cross-modal Retrieval,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182590111&doi=10.1145%2f3632527&partnerID=40&md5=6ba34b79ebce97aebadbd154506b911c,"Online cross-modal hashing has gained attention for its adaptability in processing streaming data. However, existing methods only define the hard similarity between data using labels. This results in poor retrieval performance, as they fail to exploit the semantic structure information of labels and miss the high-quality hash codes guided by the hierarchical relevance between labels. In addition, they ignore the bit-flipping problem, which leads to sub-optimal cross-modal retrieval performance. To address these issues, we propose Supervised Hierarchical Online Hashing (SHOH) for cross-modal retrieval. Our approach acquires hierarchical similarity via cross-layer affiliation of labels and explores its application to online hashing. We design a hierarchical similarity learning method in the online learning framework, which includes virtual center learning and hierarchical similarity embedding. Labels with soft similarity bridge the label hierarchy and cross-modal hash embedding. Furthermore, we propose a Weighted Retrieval Strategy (WRS) to mitigate the impact caused by bit-flipping errors. Extensive experiments and verification on hierarchical and non-hierarchical datasets demonstrate that SHOH preserves accurate inter-class distances and achieves performance improvements compared to state-of-the-art methods. The source code is available at https://github.com/HUST-IDSM-AI/SHOH.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesOnline hashing; cross-modal retrieval; label hierarchy,Codes (symbols); Data handling; Hash functions; Information retrieval; Semantics; Additional key word and phrasesonline hashing; Bit flipping; Cross-modal; Cross-modal retrieval; Key words; Label hierarchy; Retrieval performance; Semantic structures; Streaming data; Structure information; Embeddings
Sentiment-Oriented Transformer-Based Variational Autoencoder Network for Live Video Commenting,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182587534&doi=10.1145%2f3633334&partnerID=40&md5=1e35eee65275b0fd46fe9d320a18608c,"Automatic live video commenting is getting increasing attention due to its significance in narration generation, topic explanation, etc. However, the diverse sentiment consideration of the generated comments is missing from current methods. Sentimental factors are critical in interactive commenting, and there has been lack of research so far. Thus, in this article, we propose a Sentiment-oriented Transformer-based Variational Autoencoder (So-TVAE) network, which consists of a sentiment-oriented diversity encoder module and a batch attention module, to achieve diverse video commenting with multiple sentiments and multiple semantics. Specifically, our sentiment-oriented diversity encoder elegantly combines a VAE and random mask mechanism to achieve semantic diversity under sentiment guidance, which is then fused with cross-modal features to generate live video comments. A batch attention module is also proposed in this article to alleviate the problem of missing sentimental samples, caused by the data imbalance that is common in live videos as the popularity of videos varies. Extensive experiments on Livebot and VideoIC datasets demonstrate that the proposed So-TVAE outperforms the state-of-the-art methods in terms of the quality and diversity of generated comments. Related code is available at https://github.com/fufy1024/So-TVAE.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesAutomatic live video commenting; batch attention mechanism; multi-modal learning; variational autoencoder,Learning systems; Signal encoding; 'current; Additional key word and phrasesautomatic live video commenting; Attention mechanisms; Auto encoders; Batch attention mechanism; Cross-modal; Key words; Live video; Multi-modal learning; Variational autoencoder; Semantics
Nonlocal Hybrid Network for Long-tailed Image Classification,2024,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182587015&doi=10.1145%2f3630256&partnerID=40&md5=01e22fbde62f7e3a59cf86db59a984d6,"It is a significant issue to deal with long-tailed data when classifying images. A nonlocal hybrid network (NHN) that takes into account both feature learning and classifier learning is proposed. The NHN can capture the existence of dependencies between two locations that are far away from each other as well as alleviate the impact of long-tailed data on the model to some extent. The dependency relationship between distant pixels is obtained first through a nonlocal module to extract richer feature representations. Then, a learnable soft class center is proposed to balance the supervised contrastive loss and reduce the impact of long-tailed data on feature learning. For efficiency, a logit adjustment strategy is adopted to correct the bias caused by the different label distributions between the training and test sets and obtain a classifier that is more suitable for long-tailed data. Finally, extensive experiments are conducted on two benchmark datasets, the long-tailed CIFAR and the large-scale real-world iNaturalist 2018, both of which have imbalanced label distributions. The experimental results show that the proposed NHN model is efficient and promising. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",balanced contrastive loss; logits adjustment; long-tailed classification; Nonlocal module,Classification (of information); Large dataset; Machine learning; Balanced contrastive loss; Feature classifiers; Feature learning; Hybrid network; Images classification; Label distribution; Logit adjustment; Long-tailed classification; Nonlocal; Nonlocal module; Image classification
Characters Link Shots: Character Attention Network for Movie Scene Segmentation,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182605272&doi=10.1145%2f3630257&partnerID=40&md5=2882eb71e04a460213ba10ef891edd21,"Movie scene segmentation aims to automatically segment a movie into multiple story units, i.e., scenes, each of which is a series of semantically coherent and time-continual shots. Previous methods have continued efforts on shot semantic association, but few take into account the impact of different semantics on foreground characters and background scenes in movie shots. In particular, the background scene in the shot can adversely affect scene boundary classification. Motivated by the fact that it is the characters who drive the plot development of a movie scene, we build a Character Attention Network (CANet) to detect movie scene boundaries in a character-centric fashion. To eliminate the background clutter, we extract multi-view character semantics for each shot in terms of human bodies and faces. Furthermore, we equip our CANet with two stages of character attention. The first is Masked Shot Attention (MSA) through selective self-attention over similar temporal contexts from multi-view character semantics to yield an enhanced omni-view shot representation, by which the CANet can better handle the variations of characters in pose and appearance. The second is Key Character Attention (KCA) through temporal-aware attention on character reappearances for Bidirectional Long Short-Term Memory (Bi-LSTM) feature association so that linking shots can be focused on those with recurring key characters. We encourage the proposed CANet in learning boundary-discriminative shot features. Specifically, we formulate a Boundary-Aware circle Loss (BAL) to push far apart CANet-features between adjacent scenes, which is also coupled with the cross-entropy loss to drive CANet-features sensitive to scene boundaries. Experimental results on the MovieNet-SSeg and OVSD datasets show that our method achieves superior performance in temporal scene segmentation compared with state-of-the-art methods. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",masked attention; Movie scene detection; video structuring; video understanding,Long short-term memory; Motion pictures; Background scenes; Key characters; Masked attention; Movie scene detection; Movie scenes; Multi-views; Scene detection; Scene segmentation; Video structuring; Video understanding; Semantics
Generating Robust Adversarial Examples against Online Social Networks (OSNs),2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182606099&doi=10.1145%2f3632528&partnerID=40&md5=1d67b550d39c954483c99425034467a0,"Online Social Networks (OSNs) have blossomed into prevailing transmission channels for images in the modern era. Adversarial examples (AEs) deliberately designed to mislead deep neural networks (DNNs) are found to be fragile against the inevitable lossy operations conducted by OSNs. As a result, the AEs would lose their attack capabilities after being transmitted over OSNs. In this work, we aim to design a new framework for generating robust AEs that can survive the OSN transmission; namely, the AEs before and after the OSN transmission both possess strong attack capabilities. To this end, we first propose a differentiable network termed SImulated OSN (SIO) to simulate the various operations conducted by an OSN. Specifically, the SIO network consists of two modules: (1) a differentiable JPEG layer for approximating the ubiquitous JPEG compression and (2) an encoder-decoder subnetwork for mimicking the remaining operations. Based upon the SIO network, we then formulate an optimization framework to generate robust AEs by enforcing model outputs with and without passing through the SIO to be both misled. Extensive experiments conducted over Facebook, WeChat and QQ demonstrate that our attack methods produce more robust AEs than existing approaches, especially under small distortion constraints; the performance gain in terms of Attack Success Rate (ASR) could be more than 60%. Furthermore, we build a public dataset containing more than 10,000 pairs of AEs processed by Facebook, WeChat or QQ, facilitating future research in the robust AEs generation. The dataset and code are available at https://github.com/csjunjun/RobustOSNAttack.git.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesAdversarial examples; adversarial images; deep neural networks; online social networks; robustness,Deep neural networks; Image compression; Additional key word and phrasesadversarial example; Adversarial image; Attack capability; Encoder-decoder; Facebook; JPEG compression; Key words; Network transmission; Robustness; Transmission channels; Social networking (online)
Complex Scenario Image Retrieval via Deep Similarity-aware Hashing,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182608499&doi=10.1145%2f3624016&partnerID=40&md5=a89430a2928ad88850118670777a5232,"When performing hashing-based image retrieval, it is difficult to learn discriminative hash codes especially for the multi-label, zero-shot and fine-grained settings. This is due to the fact that the similarities vary, even within the same category, under the conditions of complex scenario settings. To address this problem, this study develops a deep similarity-aware hashing method for complex scenario image retrieval (DEPISH). DEPISH more focuses on the samples that are difficult to distinguish from other images (i.e., “difficult samples”), such as images that contain multiple semantics. It dynamically divides attention among samples according to their difficulty levels with a margin weighting strategy. Furthermore, by adding special terms in the model, DEPISH is capable of avoiding the inconsistency between the hash code representation and true similarity among negative samples. In addition, unlike the existing methods that use a pre-defined similarity matrix with fixed values, the DEPISH adopts an adaptive similarity matrix, which accurately captures the various similarities among all samples. The results of our experiment on multiple benchmark datasets containing complex scenarios (i.e., multi-label, zero-shot, and fine-grained datasets) verify the effectiveness of this method. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",complex scenarios; difficult samples; Hashing; similarity-aware hashing,Hash functions; Image retrieval; Zero-shot learning; Complex scenario; Condition; Difficult sample; Fine grained; Hashing; Hashing method; Learn+; Multi-labels; Similarity matrix; Similarity-aware hashing; Semantics
Cloth Interactive Transformer for Virtual Try-On,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182608583&doi=10.1145%2f3617374&partnerID=40&md5=2163a9a74648a929228dd669933b6ebf,"The 2D image-based virtual try-on has aroused increased interest from the multimedia and computer vision fields due to its enormous commercial value. Nevertheless, most existing image-based virtual try-on approaches directly combine the person-identity representation and the in-shop clothing items without taking their mutual correlations into consideration. Moreover, these methods are commonly established on pure convolutional neural networks (CNNs) architectures which are not simple to capture the long-range correlations among the input pixels. As a result, it generally results in inconsistent results. To alleviate these issues, in this article, we propose a novel two-stage cloth interactive transformer (CIT) method for the virtual try-on task. During the first stage, we design a CIT matching block, aiming at precisely capturing the long-range correlations between the cloth-agnostic person information and the in-shop cloth information. Consequently, it makes the warped in-shop clothing items look more natural in appearance. In the second stage, we put forth a CIT reasoning block for establishing global mutual interactive dependencies among person representation, the warped clothing item, and the corresponding warped cloth mask. The empirical results, based on mutual dependencies, demonstrate that the final try-on results are more realistic. Substantial empirical results on a public fashion dataset illustrate that the suggested CIT attains competitive virtual try-on performance. © 2023 Copyright held by the owner/author(s).",cross attention; garment transfer; transformer; Virtual try-on,2D images; Convolutional neural network; Cross attention; Garment transfer; Image-based; Long range correlations; Mutual correlations; Neural network architecture; Transformer; Virtual try-on; Convolutional neural networks
Subjective and Objective Quality Assessment for in-the-Wild Computer Graphics Images,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182607142&doi=10.1145%2f3631357&partnerID=40&md5=d8b24f2b92c9540e81862f8f2e1ef02c,"Computer graphics images (CGIs) are artificially generated by means of computer programs and are widely perceived under various scenarios, such as games, streaming media, etc. In practice, the quality of CGIs consistently suffers from poor rendering during production, inevitable compression artifacts during the transmission of multimedia applications, and low aesthetic quality resulting from poor composition and design. However, few works have been dedicated to dealing with the challenge of computer graphics image quality assessment (CGIQA). Most image quality assessment (IQA) metrics are developed for natural scene images (NSIs) and validated on databases consisting of NSIs with synthetic distortions, which are not suitable for in-the-wild CGIs. To bridge the gap between evaluating the quality of NSIs and CGIs, we construct a large-scale in-the-wild CGIQA database consisting of 6,000 CGIs (CGIQA-6k) and carry out the subjective experiment in a well-controlled laboratory environment to obtain the accurate perceptual ratings of the CGIs. Then, we propose an effective deep learning–based no-reference (NR) IQA model by utilizing both distortion and aesthetic quality representation. Experimental results show that the proposed method outperforms all other state-of-the-art NR IQA methods on the constructed CGIQA-6k database and other CGIQA-related databases. The database is released at https://github.com/zzc-1998/CGIQA6K. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Computer graphics images; image quality assessment; in-the-wild distortions; no-reference,Application programs; Computer games; Deep learning; Image quality; Multimedia systems; Aesthetic qualities; Compression artifacts; Computer graphic images; Image quality assessment; In-the-wild distortion; Natural scene images; No-reference; No-reference images; Streaming medium; Subjective and objective quality assessments; Database systems
Cross-modal Semantically Augmented Network for Image-text Matching,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182594049&doi=10.1145%2f3631356&partnerID=40&md5=ea3b1fe17dc07ecbb958426fe515a5ba,"Image-text matching plays an important role in solving the problem of cross-modal information processing. Since there are nonnegligible semantic differences between heterogenous pairwise data, a crucial challenge is how to learn a unified representation. Existing methods mainly rely on the alignment between regional image features and corresponding entity words. However, the regional features in the image are often more concerned with the foreground entity information, and the attribute information of the entities and the relational information are ignored. How to effectively integrate entity-attribute alignment and relationship alignment has not been fully studied. Therefore, we propose a Cross-Modal Semantically Augmented Network for Image-Text Matching (CMSAN), which combines the relationships between entities in the image with the semantics of relational words in the text. CMSAN (1) proposes an adaptive word-type prediction model that classifies the words into four types, i.e., entity word, attribute word, relation word, and unnecessary word. It can align different image features at multiple levels. CMSAN (2) designs a sophisticated relationship alignment module and an entity-attribute alignment module that maximizes the exploitation of the semantic information, which enables the model to have more discriminative power and further improves the matching accuracy. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adaptive word-type prediction model; cross-modal semantically augmented; Image-text matching; relationship alignment,Adaptive word-type prediction model; Cross-modal; Cross-modal semantically augmented; Image features; Image texts; Image-text matching; Prediction modelling; Relationship alignment; Text-matching; Type predictions; Semantics
Sparsity-guided Discriminative Feature Encoding for Robust Keypoint Detection,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181451597&doi=10.1145%2f3628432&partnerID=40&md5=751e99ef2cd784a36b9faf854f0599be,"Existing handcrafted keypoint detectors typically focus on designing specific local structures manually while ignoring whether they have enough flexibility to explore diverse visual patterns in an image. Despite the advancement of learning-based approaches in the past few years, most of them still rely on the availability of the outputs of handcrafted detectors as a part of training. In fact, such dependence limits their ability to discover various visual information. Recently, semi-handcrafted methods based on sparse coding have emerged as a promising paradigm to alleviate the above issue. However, the visual relationships between feature points have not been considered in the encoding stage, which may weaken the discriminative capability of feature representations for keypoint recognition. To tackle this problem, we propose a novel sparsity-guided discriminative feature representation (SDFR) method that attempts to explore the intrinsic correlations of keypoint candidates, thus ensuring the validity of characterizing distinctive and diverse structural information. Specifically, we first incorporate an affinity constraint into the feature representation objective, which jointly encodes all the patches in an image while highlighting the similarities and differences between them. Meanwhile, a smoother sparsity regularization with the Frobenius norm is leveraged to further preserve the similarity relationships of patch representations. Due to the differentiable property of this sparsity, SDFR is computationally feasible and effective for representing dense patches. Finally, we treat the SDFR model as multiple optimization sub-problems and introduce an iterative solver. During comprehensive evaluations on five challenging benchmarks, the proposed method achieves favorable performances compared with the state of the art in the literature. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",feature representation; Keypoint; sparse coding; visual dictionary,Encoding (symbols); Image representation; Signal encoding; Discriminative features; Encodings; Feature representation; Key-point detectors; Keypoint detection; Keypoints; Local structure; Sparse coding; Visual dictionaries; Visual pattern; Iterative methods
2BiVQA: Double Bi-LSTM-based Video Quality Assessment of UGC Videos,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182601405&doi=10.1145%2f3632178&partnerID=40&md5=af822e648670b4d182e5fbd0e21f38f6,"Recently, with the growing popularity of mobile devices as well as video sharing platforms (e.g., YouTube, Facebook, TikTok, and Twitch), User-Generated Content (UGC) videos have become increasingly common and now account for a large portion of multimedia traffic on the internet. Unlike professionally generated videos produced by filmmakers and videographers, typically, UGC videos contain multiple authentic distortions, generally introduced during capture and processing by naive users. Quality prediction of UGC videos is of paramount importance to optimize and monitor their processing in hosting platforms, such as their coding, transcoding, and streaming. However, blind quality prediction of UGC is quite challenging, because the degradations of UGC videos are unknown and very diverse, in addition to the unavailability of pristine reference. Therefore, in this article, we propose an accurate and efficient Blind Video Quality Assessment (BVQA) model for UGC videos, which we name 2BiVQA for double Bi-LSTM Video Quality Assessment. 2BiVQA metric consists of three main blocks, including a pre-trained Convolutional Neural Network to extract discriminative features from image patches, which are then fed into two Recurrent Neural Networks for spatial and temporal pooling. Specifically, we use two Bi-directional Long Short-term Memory networks, the first is used to capture short-range dependencies between image patches, while the second allows capturing long-range dependencies between frames to account for the temporal memory effect. Experimental results on recent large-scale UGC VQA datasets show that 2BiVQA achieves high performance at lower computational cost than most state-of-the-art VQA models. The source code of our 2BiVQA metric is made publicly available at https://github.com/atelili/2BiVQA. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Bi-LSTM; Blind video quality assessment; deep learning; spatial pooling; temporal pooling; user-generated content,Convolutional neural networks; HTTP; Large dataset; Motion pictures; Video signal processing; Bi-LSTM; Blind video quality assessment; Deep learning; Quality assessment; Quality prediction; Spatial pooling; Temporal pooling; User-generated; User-generated content; Video quality; Long short-term memory
Rethinking Person Re-Identification via Semantic-based Pretraining,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181460588&doi=10.1145%2f3628452&partnerID=40&md5=1357cf89ecd278b01b7d5e22d2107d96,"Pretraining is a dominant paradigm in computer vision. Generally, supervised ImageNet pretraining is commonly used to initialize the backbones of person re-identification (Re-ID) models. However, recent works show a surprising result that CNN-based pretraining on ImageNet has limited impacts on Re-ID system due to the large domain gap between ImageNet and person Re-ID data. To seek an alternative to traditional pretraining, here we investigate semantic-based pretraining as another method to utilize additional textual data against ImageNet pretraining. Specifically, we manually construct a diversified FineGPR-C caption dataset for the first time on person Re-ID events. Based on it, a pure semantic-based pretraining approach named VTBR is proposed to adopt dense captions to learn visual representations with fewer images. We train convolutional neural networks from scratch on the captions of FineGPR-C dataset, and then transfer them to downstream Re-ID tasks. Comprehensive experiments conducted on benchmark datasets show that our VTBR can achieve competitive performance compared with ImageNet pretraining - despite using up to 1.4× fewer images, revealing its potential in Re-ID pretraining. Our source code is also publicly available at https://github.com/JeremyXSC/VTBR. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",efficient training; Person re-identification; synthetic data,Benchmarking; Computer vision; Convolutional neural networks; Efficient training; Identification data; Identification modeling; Large domain; Learn+; Person re identifications; Pre-training; Re identifications; Synthetic data; Textual data; Semantics
BiC-Net: Learning Efficient Spatio-temporal Relation for Text-Video Retrieval,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181518548&doi=10.1145%2f3627103&partnerID=40&md5=ea31a96e98539a6eb1bcb768db7c3289,"The task of text-video retrieval aims to understand the correspondence between language and vision and has gained increasing attention in recent years. Recent works have demonstrated the superiority of local spatio-temporal relation learning with graph-based models. However, most existing graph-based models are handcrafted and depend heavily on expert knowledge and empirical feedback, which may be unable to mine the high-level fine-grained visual relations effectively. These limitations result in their inability to distinguish videos with the same visual components but different relations. To solve this problem, we propose a novel cross-modal retrieval framework, Bi-Branch Complementary Network (BiC-Net), which modifies Transformer architecture to effectively bridge text-video modalities in a complementary manner via combining local spatio-temporal relation and global temporal information. Specifically, local video representations are encoded using multiple Transformer blocks and additional residual blocks to learn fine-grained spatio-temporal relations and long-term temporal dependency, calling the module a Fine-grained Spatio-temporal Transformer (FST). Global video representations are encoded using a multi-layer Transformer block to learn global temporal features. Finally, we align the spatio-temporal relation and global temporal features with the text feature on two embedding spaces for cross-modal text-video retrieval. Extensive experiments are conducted on MSR-VTT, MSVD, and YouCook2 datasets. The results demonstrate the effectiveness of our proposed model. Our code is public at https://github.com/lionel-hing/BiC-Net. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bi-branch complementary network; spatio-temporal relation; Text-video retrieval,Graphic methods; Bi-branch complementary network; Cross-modal; Fine grained; Graph-based models; Learn+; Spatio-temporal relations; Temporal features; Text-video retrieval; Video representations; Video retrieval; Video recording
Asymmetric Dual-Decoder U-Net for Joint Rain and Haze Removal,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181498619&doi=10.1145%2f3628451&partnerID=40&md5=075fd5a09501b6fb904655fc6c0734a8,"This work studies the multi-weather restoration problem. In real-life scenarios, rain and haze, two often co-occurring common weather phenomena, can greatly degrade the clarity and quality of the scene images, leading to a performance drop in the visual applications, such as autonomous driving. However, jointly removing the rain and haze in scene images is ill-posed and challenging, where the existence of haze and rain and the change of atmosphere light, can both degrade the scene information. Current methods focus on the contamination removal part, thus ignoring the restoration of the scene information affected by the change of atmospheric light. We propose a novel deep neural network, named Asymmetric Dual-decoder U-Net (ADU-Net), to address the aforementioned challenge. The ADU-Net produces both the contamination residual and the scene residual to efficiently remove the contamination while preserving the fidelity of the scene information. Extensive experiments show our work outperforms the existing state-of-the-art methods by a considerable margin in both synthetic data and real-world data benchmarks, including RainCityscapes, BID Rain, and SPA-Data. For instance, we improve the state-of-the-art PSNR value by 2.26/4.57 on the RainCityscapes/SPA-Data, respectively. Codes will be made available freely to the research community. © 2023 Copyright held by the owner/author(s).",asymmetric dual-decoder U-Net (ADU-Net); contamination residual; Joint rain and haze removal; scene residual,Contamination; Decoding; Deep neural networks; Restoration; Asymmetric dual-decoder U-net; Contamination residual; Haze removal; Joint rain and haze removal; Rain removals; Restoration problems; Scene image; Scene residual; Weather phenomena; Work study; Rain
Robust RGB-T Tracking via Adaptive Modality Weight Correlation Filters and Cross-modality Learning,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182596770&doi=10.1145%2f3630100&partnerID=40&md5=7d372d5c051eded3af49108ffa255981,"RGBT tracking is gaining popularity due to its ability to provide effective tracking results in a variety of weather conditions. However, feature specificity and complementarity have not been fully used in existing models that directly fuse the correlation filtering response, which leads to poor tracker performance. In this article, we propose correlation filters with adaptive modality weight and cross-modality learning (AWCM) ability to solve multimodality tracking tasks. First, we use weighted activation to fuse thermal infrared and visible modalities, and the fusion modality is used as an auxiliary modality to suppress noise and increase the learning ability of shared modal features. Second, we design modal weights through average peak-to-correlation energy coefficients to improve model reliability. Third, we propose consistency in using the fusion modality as an intermediate variable for joint learning consistency, thereby increasing tracker robustness via interactive cross-modal learning. Finally, we use the alternating direction method of multipliers algorithm to produce a closed solution and conduct extensive experiments on the RGBT234, VOT-TIR2019, and GTOT tracking benchmark datasets to demonstrate the superior performance of the proposed AWCM against compared to existing tracking algorithms. The code developed in this study is available at the following website.1 © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adaptive modality weight; Correlation filters; cross-modality learning; RGBT tracking,Adaptive filters; Benchmarking; Adaptive modality weight; Condition; Correlation filtering; Correlation filters; Cross modality; Cross-modality learning; Learning abilities; Multi-modality; RGBT tracking; Tracker performance; Adaptive filtering
Contrastive Learning of View-invariant Representations for Facial Expressions Recognition,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182601211&doi=10.1145%2f3632960&partnerID=40&md5=aa7a21f601cd174924d06f4276a3d065,"Although there has been much progress in the area of facial expression recognition (FER), most existing methods suffer when presented with images that have been captured from viewing angles that are non-frontal and substantially different from those used in the training process. In this article, we propose ViewFX, a novel view-invariant FER framework based on contrastive learning, capable of accurately classifying facial expressions regardless of the input viewing angles during inference. ViewFX learns view-invariant features of expression using a proposed self-supervised contrastive loss, which brings together different views of the same subject with a particular expression in the embedding space. We also introduce a supervised contrastive loss to push the learned view-invariant features of each expression away from other expressions. Since facial expressions are often distinguished with very subtle differences in the learned feature space, we incorporate the Barlow twins loss to reduce the redundancy and correlations of the representations in the learned representations. The proposed method is a substantial extension of our previously proposed CL-MEx, which only had a self-supervised loss. We test the proposed framework on two public multi-view facial expression recognition datasets, KDEF and DDCF. The experiments demonstrate that our approach outperforms previous works in the area and sets a new state-of-the-art for both datasets while showing considerably less sensitivity to challenging angles and the number of output labels used for training. We also perform detailed sensitivity and ablation experiments to evaluate the impact of different components of our model as well as its sensitivity to different parameters.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesAffective computing; contrastive learning; expression recognition,Additional key word and phrasesaffective computing; Contrastive learning; Expression recognition; Facial expression recognition; Facial Expressions; Invariant features; Invariant representation; Key words; View invariants; Viewing angle; Face recognition
Hierarchical Synergy-Enhanced Multimodal Relational Network for Video Question Answering,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182589281&doi=10.1145%2f3630101&partnerID=40&md5=bd95b88d6861bd49b164c05b1694111d,"Video question answering (VideoQA) is challenging as it requires reasoning about natural language and multimodal interactive relations. Most existing methods apply attention mechanisms to extract interactions between the question and the video or to extract effective spatio-temporal relational representations. However, these methods neglect the implication of relations between intra- and inter-modal interactions for multimodal learning, and they fail to fully exploit the synergistic effect of multiscale semantics in answer reasoning. In this article, we propose a novel hierarchical synergy-enhanced multimodal relational network (HMRNet) to address these issues. Specifically, we devise (i) a compact and unified relation-oriented interaction module that explores the relation between intra- and inter-modal interactions to enable effective multimodal learning; and (ii) a hierarchical synergistic memory unit that leverages a memory-based interaction scheme to complement and fuse multimodal semantics at multiple scales to achieve synergistic enhancement of answer reasoning. With careful design of each component, our HMRNet has fewer parameters and is computationally efficient. Extensive experiments and qualitative analyses demonstrate that the HMRNet is superior to previous state-of-the-art methods on eight benchmark datasets. We also demonstrate the effectiveness of the different components of our method. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",attention mechanisms; multimodal learning; multiscale semantics; Video question answering,Semantic Web; Attention mechanisms; Interactive relation; Modal interactions; Multi-modal; Multi-modal learning; Multiscale semantic; Natural languages; Question Answering; Relational network; Video question answering; Semantics
Web3 Metaverse: State-of-the-Art and Vision,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182604816&doi=10.1145%2f3630258&partnerID=40&md5=253924086d941b15749497a6c4e2b6e3,"The metaverse, as a rapidly evolving socio-technical phenomenon, exhibits significant potential across diverse domains by leveraging Web3 (a.k.a. Web 3.0) technologies such as blockchain, smart contracts, and non-fungible tokens (NFTs). This survey aims to provide a comprehensive overview of the Web3 metaverse from a human-centered perspective. We (i) systematically review the development of the metaverse over the past 30 years, highlighting the balanced contributions from its core components: Web3, immersive convergence, and crowd intelligence communities, (ii) define the metaverse that integrates the Web3 community as the Web3 metaverse and propose an analysis framework from the community, society, and human layers to describe the features, missions, and relationships for each community and their overlapping sections, (iii) survey the state-of-the-art of the Web3 metaverse from a human-centered perspective, namely, the identity, field, and behavior aspects, and (iv) provide supplementary technical reviews. To the best of our knowledge, this work represents the first systematic, interdisciplinary survey on the Web3 metaverse. Specifically, we commence by discussing the potential for establishing decentralized identities (DID) utilizing mechanisms such as profile picture (PFP) NFTs, domain name NFTs, and soulbound tokens (SBTs). Subsequently, we examine land, utility, and equipment NFTs within the Web3 metaverse, highlighting interoperable and full on-chain solutions for existing centralization challenges. Lastly, we spotlight current research and practices about individual, intragroup, and inter-group behaviors within the Web3 metaverse, such as Creative Commons Zero license (CC0) NFTs, decentralized education, decentralized science (DeSci), and decentralized autonomous organizations (DAO). Furthermore, we share our insights into several promising directions, encompassing three key socio-technical facets of Web3 metaverse development. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",definition and framework; human-centered; Metaverse; survey; Web3,Aka web 3.0; Block-chain; Decentralised; Definition and framework; Diverse domains; Human-centered; Metaverses; Sociotechnical; State of the art; Web 3.0; Smart contract
Hierarchical Learning and Dummy Triplet Loss for Efficient Deepfake Detection,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181700568&doi=10.1145%2f3626101&partnerID=40&md5=51ccf2ca0f2afb1cc014c5c2fa734834,"The advancement of generative models has made it easier to create highly realistic Deepfake videos. This accessibility has led to a surge in research on Deepfake detection to mitigate potential misuse. Typically, Deepfake detection models utilize binary backbones, even though the training dataset contains additional exploitable information, such as the Deepfake generation method employed for each video. However, recent findings suggest that inferring a binary class from a multi-class backbone yields superior performance compared to directly employing a binary backbone. Building upon this research, our article introduces two novel methods to infer a binary class from a multi-class backbone. The first method, named root dummies, leverages the dummy triplet loss, which employs fixed vectors (i.e., dummies) instead of mined positives and negatives in the triplet loss. By training the multi-class backbone with these dummies, we can easily infer a binary class during testing by adjusting the number of dummies (from six during training to two during inference). Through this approach, we achieve an accuracy improvement of 0.23% compared to the existing inference method, without requiring additional training. The second proposed method is transfer learning. It involves training a classifier, such as a support vector machine, to predict binary classes based on the image embeddings generated by the multi-class backbone. Although this method necessitates additional training, it further enhances the model's performance, resulting in an accuracy increase of 1.79%. In summary, our proposed methods improve the accuracy of Deepfake detection by simply modifying the number of classes during training, making them suitable for integration into a variety of existing Deepfake training pipelines. Additionally, to foster reproducible research, we have made the source code of our solution publicly available at https://github.com/beuve/DmyT. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deepfake forensics; metric learning; neural networks,Digital forensics; Learning systems; Deepfake forensic; Detection models; Generation method; Generative model; Hierarchical learning; Metric learning; Neural-networks; Novel methods; Performance; Training dataset; Support vector machines
Geometric and Learning-Based Mesh Denoising: A Comprehensive Survey,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181457695&doi=10.1145%2f3625098&partnerID=40&md5=07a3b11bc77da4dad7c61d48d602de0b,"Mesh denoising is a fundamental problem in digital geometry processing. It seeks to remove surface noise while preserving surface intrinsic signals as accurately as possible. While traditional wisdom has been built upon specialized priors to smooth surfaces, learning-based approaches are making their debut with great success in generalization and automation. In this work, we provide a comprehensive review of the advances in mesh denoising, containing both traditional geometric approaches and recent learning-based methods. First, to familiarize readers with the denoising tasks, we summarize four common issues in mesh denoising. We then provide two categorizations of the existing denoising methods. Furthermore, three important categories, including optimization-, filter-, and data-driven-based techniques, are introduced and analyzed in detail, respectively. Both qualitative and quantitative comparisons are illustrated, to demonstrate the effectiveness of the state-of-the-art denoising methods. Finally, potential directions of future work are pointed out to solve the common problems of these approaches. A mesh denoising benchmark is also built in this work, and future researchers will easily and conveniently evaluate their methods with state-of-the-art approaches. To aid reproducibility, we release our datasets and used results at https://github.com/chenhonghua/Mesh-Denoiser. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep learning; low-rank recovery; Mesh denoising; optimization method,Deep learning; Learning systems; Deep learning; Denoising methods; Digital geometry processing; Intrinsic signal; Low-rank recoveries; Mesh denoising; Optimization method; Smooth surface; Surface learning; Surface noise; Mesh generation
A Visual Sensitivity Aware ABR Algorithm for DASH via Deep Reinforcement Learning,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181477723&doi=10.1145%2f3591108&partnerID=40&md5=9d2dd092b3041589014a92981285d490,"In order to cope with the fluctuation of network bandwidth and provide smooth video services, adaptive video streaming technology is proposed. In particular, the adaptive bitrate (ABR) algorithm is widely used in dynamic adaptive streaming over HTTP (DASH) to improve quality of experience (QoE). However, existing ABR algorithms still ignore the inherent visual sensitivity of human visual system (HVS). As the final receiver of video, HVS has different sensitivity to the quality distortion of different video content, and video content with high visual sensitivity needs to allocate more bitrate resources. Therefore, existing ABR algorithms still have limitations in reasonably allocating bitrate and maximizing QoE. To solve this problem, this paper designs an adaptive bitrate strategy from the perspective of user vision, studies the modeling of visual sensitivity, and proposes a visual sensitivity aware ABR algorithm. We extract a set of content features and attribute features from the video, and consider the simulation of HVS to establish a total masking effect model that reflects the visual sensitivity more accurately. Further, the network status, buffer occupancy, and visual sensitivity are comprehensively considered under a deep reinforcement learning framework to select the appropriate bitrate for maximizing QoE. We implement the proposed algorithm over a realistic trace-driven evaluation and compare its performance with several latest algorithms. Experimental results show that our algorithm can align ABR strategy with visual sensitivity to achieve better QoE in high visual sensitivity content, and improves the average perceptual video quality and overall user QoE by 18.3% and 22.8%, respectively. Additionally, we prove the feasibility of our algorithm through subjective evaluation in the real environment.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",ABR; DASH; deep reinforcement learning; QoE; visual sensitivity,Deep learning; Quality of service; Video recording; Adaptive bitrate; Bit rates; Deep reinforcement learning; Dynamic Adaptive Streaming over HTTP; Human Visual System; Network bandwidth; Quality of experience; Reinforcement learnings; Video contents; Visual sensitivity; Reinforcement learning
Voice-Face Homogeneity Tells Deepfake,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181501368&doi=10.1145%2f3625231&partnerID=40&md5=5bc9ca125e69e5a8cd50634f59895d2f,"Detecting forgery videos is highly desirable due to the abuse of deepfake. Existing detection approaches contribute to exploring the specific artifacts in deepfake videos and fit well on certain data. However, the growing technique on these artifacts keeps challenging the robustness of traditional deepfake detectors. As a result, the development of these approaches has reached a blockage. In this article, we propose to perform deepfake detection from an unexplored voice-face matching view. Our approach is founded on two supporting points: first, there is a high degree of homogeneity between the voice and face of an individual (i.e., they are highly correlated), and second, deepfake videos often involve mismatched identities between the voice and face due to face-swapping techniques. To this end, we develop a voice-face matching method that measures the matching degree between these two modalities to identify deepfake videos. Nevertheless, training on specific deepfake datasets makes the model overfit certain traits of deepfake algorithms. We instead advocate a method that quickly adapts to untapped forgery, with a pre-training then fine-tuning paradigm. Specifically, we first pre-train the model on a generic audio-visual dataset, followed by the fine-tuning on downstream deepfake data. We conduct extensive experiments over three widely exploited deepfake datasets: DFDC, FakeAVCeleb, and DeepfakeTIMIT. Our method obtains significant performance gains as compared to other state-of-the-art competitors. For instance, our method outperforms the baselines by nearly 2%, achieving an AUC of 86.11% on FakeAVCeleb. It is also worth noting that our method already achieves competitive results when fine-tuned on limited deepfake data. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cross-modal matching; Deepfake detection; face; voice,Speech recognition; Cross-modal; Cross-modal matching; Deepfake detection; Detection approach; Face; Face matching; Face swapping; Fine tuning; Highly-correlated; Modal matching; Face recognition
An Image Arbitrary-Scale Super-Resolution Network Using Frequency-domain Information,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181464305&doi=10.1145%2f3616376&partnerID=40&md5=5324ae30f52d1fdc199538edf6609a65,"Image super-resolution (SR) is a technique to recover lost high-frequency information in low-resolution (LR) images. Since spatial-domain information has been widely exploited, there is a new trend to involve frequency-domain information in SR tasks. Besides, image SR is typically application-oriented and various computer vision tasks call for image arbitrary magnification. Therefore, in this article, we study image features in the frequency domain to design a novel image arbitrary-scale SR network. First, we statistically analyze LR-HR image pairs of several datasets under different scale factors and find that the high-frequency spectra of different images under different scale factors suffer from different degrees of degradation, but the valid low-frequency spectra tend to be retained within a certain distribution range. Then, based on this finding, we devise an adaptive scale-aware feature division mechanism using deep reinforcement learning, which can accurately and adaptively divide the frequency spectrum into the low-frequency part to be retained and the high-frequency one to be recovered. Finally, we design a scale-aware feature recovery module to capture and fuse multi-level features for reconstructing the high-frequency spectrum at arbitrary scale factors. Extensive experiments on public datasets show the superiority of our method compared with state-of-the-art methods.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",arbitrary magnification; deep reinforcement learning; image frequency domain; Super-resolution,Deep learning; Frequency domain analysis; Optical resolving power; Recovery; Arbitrary magnification; Deep reinforcement learning; Domain informations; Frequency domains; Image frequency; Image frequency domain; Image super resolutions; Reinforcement learnings; Scale Factor; Superresolution; Reinforcement learning
Boosting Diversity in Visual Search with Pareto Non-Dominated Re-Ranking,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181436346&doi=10.1145%2f3625296&partnerID=40&md5=802bf90e02ac778cce69de929316e999,"The field of visual search has gained significant attention recently, particularly in the context of web search engines and e-commerce product search platforms. However, the abundance of web images presents a challenge for modern image retrieval systems, as they need to find both relevant and diverse images that maximize users' satisfaction. In response to this challenge, we propose a non-dominated visual diversity re-ranking (NDVDR) method based on the concept of Pareto optimality. To begin with, we employ a fast binary hashing method as a coarse-grained retrieval procedure. This allows us to efficiently obtain a subset of candidate images for subsequent re-ranking. Fed with this initial retrieved image results, the NDVDR performs a fine-grained re-ranking procedure for boosting both relevance and visual diversity among the top-ranked images. Recognizing the inherent conflict nature between the objectives of relevance and diversity, the re-ranking procedure is simulated as the analytical stage of a multi-criteria decision-making process, seeking the optimal tradeoff between the two conflicting objectives within the initial retrieved images. In particular, a non-dominated sorting mechanism is devised that produces Pareto non-dominated hierarchies among images based on the Pareto dominance relation. Additionally, two novel measures are introduced for the effective characterization of the relevance and diversity scores among different images. We conduct experiments on three popular real-world image datasets and compare our re-ranking method with several state-of-the-art image search re-ranking methods. The experimental results validate that our re-ranking approach guarantees retrieval accuracy while simultaneously boosting diversity among the top-ranked images. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",image diversity; Pareto optimality; re-ranking; Visual image search,Decision making; Image retrieval; Pareto principle; Image diversity; Image search; Pareto-optimality; Ranking methods; Ranking procedures; Re-ranking; Retrieved images; Visual image; Visual image search; Visual search; Search engines
An Iterative Semi-supervised Approach with Pixel-wise Contrastive Loss for Road Extraction in Aerial Images,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181722628&doi=10.1145%2f3606374&partnerID=40&md5=21f077d151ac69133fb3c9ecf20985f3,"Extracting roads in aerial images has numerous applications in artificial intelligence and multimedia computing, including traffic pattern analysis and parking space planning. Learning deep neural networks, though very successful, demand vast amounts of high-quality annotations, of which acquisition is time-consuming and expensive. In this work, we propose a semi-supervised approach for image-based road extraction in which only a small set of labeled images are available for training to address this challenge. We design a pixel-wise contrastive loss to self-supervise the network training to utilize the large corpus of unlabeled images. The key idea is to identify pairs of overlapping image regions (positive) or non-overlapping image regions (negative) and encourage the network to make similar outputs for positive pairs or dissimilar outputs for negative pairs. We also develop a negative sampling strategy to filter false-negative samples during the process. An iterative procedure is introduced to apply the network over raw images to generate pseudo-labels, filter and select high-quality labels with the proposed contrastive loss, and retrain the network with the enlarged training dataset. We repeat these iterative steps until convergence. We validate the effectiveness of the proposed methods by performing extensive experiments on the public SpaceNet3 and DeepGlobe Road datasets. Results show that our proposed method achieves state-of-the-art results on public image segmentation benchmarks and significantly outperforms other semi-supervised methods.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",contrastive loss; Deep learning; iterative labeling; road extraction; semi-supervised learning,Antennas; Deep neural networks; Extraction; Iterative methods; Learning algorithms; Pixels; Roads and streets; Aerial images; Contrastive loss; Deep learning; High quality; Iterative labeling; Labelings; Overlapping images; Road extraction; Semi-supervised; Semi-supervised learning; Image segmentation
Task Recommendation via Heterogeneous Multi-modal Features and Decision Fusion in Mobile Crowdsensing,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181654429&doi=10.1145%2f3626239&partnerID=40&md5=2c7f89bfbe10179f1dec1048a8e66d84,"In the decision-making process of the behavior of mobile crowdsensing, using a single view to learn a user's preference will lead to a mismatch between the user's wishes and the final task recommendation list, resulting in the low efficiency of the model recommendation. Aiming at the lack of perceptual representation and cognitive fusion of multimodal coupled information, a task recommendation method based on heterogeneous multimodal features and decision fusion is proposed. According to the content characteristics of multi-source data in the user's historical task set, several task-task similarity matrices are constructed to align feature dimensions and feature semantics. Using the improved similarity network fusion algorithm, networks composed of multiple content similarity matrices are effectively fused into a similarity network. Considering the influence of the time factor, the tasks that have had interest drift are filtered out from the set of tasks that the user has participated in. Finally, the updated similarity network is clustered to predict the current preference of the user for new tasks. Experimental results based on simulation and real datasets show that the proposed method can effectively improve the accuracy and efficiency of task assignments while improving user satisfaction. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Mobile crowd sensing; multi-modal information fusion; similarity network; task recommendation; user preference,Behavioral research; Decision making; Matrix algebra; Recommender systems; Semantics; Decision-making process; Decisions fusion; Features fusions; Mobile crowd sensing; Multi-modal; Multimodal information fusion; Similarity matrix; Similarity network; Task recommendation; User's preferences; Efficiency
Toward Effective Semi-supervised Node Classification with Hybrid Curriculum Pseudo-labeling,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181497791&doi=10.1145%2f3626528&partnerID=40&md5=4941c702cc54d3e9b0728ba415b11d9a,"Semi-supervised node classification is a crucial challenge in relational data mining and has attracted increasing interest in research on graph neural networks (GNNs). However, previous approaches merely utilize labeled nodes to supervise the overall optimization, but fail to sufficiently explore the information of their underlying label distribution. Even worse, they often overlook the robustness of models, which may cause instability of network outputs to random perturbations. To address the aforementioned shortcomings, we develop a novel framework termed Hybrid Curriculum Pseudo-Labeling (HCPL) for efficient semi-supervised node classification. Technically, HCPL iteratively annotates unlabeled nodes by training a GNN model on the labeled samples and any previously pseudo-labeled samples, and repeatedly conducts this process. To improve the model robustness, we introduce a hybrid pseudo-labeling strategy that incorporates both prediction confidence and uncertainty under random perturbations, therefore mitigating the influence of erroneous pseudo-labels. Finally, we leverage the idea of curriculum learning to start from annotating easy samples, and gradually explore hard samples as the iteration grows. Extensive experiments on a number of benchmarks demonstrate that our HCPL beats various state-of-the-art baselines in diverse settings.  © 2023 Copyright held by the owner/author(s).",curriculum learning; Graph neural network; semi-supervised learning,Data mining; Graph neural networks; Graph theory; Random processes; Supervised learning; Curriculum learning; Graph neural networks; Hybrid curricula; Label distribution; Labelings; Optimisations; Random perturbations; Relational data mining; Semi-supervised; Semi-supervised learning; Curricula
Boosting Few-shot Object Detection with Discriminative Representation and Class Margin,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181461756&doi=10.1145%2f3608478&partnerID=40&md5=415c56cdaffd7cebb57711292a788811,"Classifying and accurately locating a visual category with few annotated training samples in computer vision has motivated the few-shot object detection technique, which exploits transfering the source-domain detection model to the target domain. Under this paradigm, however, such transferred source-domain detection model usually encounters difficulty in the classification of the target domain because of the low data diversity of novel training samples. To combat this, we present a simple yet effective few-shot detector, Transferable RCNN. To transfer general knowledge learned from data-abundant base classes to data-scarce novel classes, we propose a weight transfer strategy to promote model transferability and an attention-based feature enhancement mechanism to learn more robust object proposal feature representations. Further, we ensure strong discrimination by optimizing the contrastive objectives of feature maps via a supervised spatial contrastive loss. Meanwhile, we introduce an angle-guided additive margin classifier to augment instance-level inter-class difference and intra-class compactness, which is beneficial for improving the discriminative power of the few-shot classification head under a few supervisions. Our proposed framework outperforms the current works in various settings of PASCAL VOC and MSCOCO datasets; this demonstrates the effectiveness and generalization ability.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep learning; few-shot object detection; transfer learning,Adaptive boosting; Deep learning; Object recognition; Sampling; Deep learning; Detection models; Domain detections; Few-shot object detection; General knowledge; Objects detection; Simple++; Target domain; Training sample; Transfer learning; Object detection
Feature Disentanglement Network: Multi-Object Tracking Needs More Differentiated Features,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181512184&doi=10.1145%2f3626825&partnerID=40&md5=6fa37071b3077de8c21ccf275db131a1,"To reduce computational redundancies, a common approach is to integrate detection and re-identification (Re-ID) into a single network in multi-object tracking (MOT), referred to as ""tracking by detection.""Most of the previous research has focused on resolving the conflict between the detection and Re-ID branches, considering it a simple coupling. In our work, we uncover that the entangled state between the detection and Re-ID tasks is much more complex than previous idea, resulting in a form of competition that degrades performance. To address the preceding issue, we propose a feature disentanglement network that deeply disentangles the intricately interwoven latent space of features and provides differentiated feature maps for each individual task. Furthermore, considering the demand for shallow semantic features in the feature re-ID branch, we also introduce a feature re-globalization module to enrich the shallow semantics. By integrating two distinct networks into a one-shot online MOT method, we develop a robust MOT tracker (named HDGTrack). We conduct extensive experiments on a number of benchmarks, and our experimental results demonstrate that our method significantly outperforms state-of-the-art MOT methods. Besides, HDGTrack is efficient and can run at 13.9 (MOT17) and 8.7 (MOT20) frames per second. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Feature disentanglement network; feature enhancement; Multiple object tracking; one-shot tracking,Object detection; Quantum entanglement; Tracking (position); Detection/identification; Feature disentanglement network; Feature enhancement; Multi-object tracking; Multiple object tracking; One-shot tracking; Re identifications; Single-networks; Tracking by detections; Tracking method; Semantics
Deep Learning Based Occluded Person Re-Identification: A Survey,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177099286&doi=10.1145%2f3610534&partnerID=40&md5=431176dacccdd2a395e6f01c0041ef8d,"Occluded person re-identification (Re-ID) focuses on addressing the occlusion problem when retrieving the person of interest across non-overlapping cameras. With the increasing demand for intelligent video surveillance and the application of person Re-ID technology, the real-world occlusion problem draws considerable interest from researchers. Although a large number of occluded person Re-ID methods have been proposed, there are few surveys that focus on occlusion. To fill this gap and help boost future research, this article provides a systematic survey of occluded person Re-ID. In this work, we review recent deep learning based occluded person Re-ID research. First, we summarize the main issues caused by occlusion as four groups: position misalignment, scale misalignment, noisy information, and missing information. Second, we categorize existing methods into six solution groups: matching, image transformation, multi-scale features, attention mechanism, auxiliary information, and contextual recovery. We also discuss the characteristics of each approach, as well as the issues they address. Furthermore, we present the performance comparison of recent occluded person Re-ID methods on four public datasets: Partial-ReID, Partial-iLIDS, Occluded-ReID, and Occluded-DukeMTMC. We conclude the study with thoughts on promising future research directions.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",deep learning; literature survey; Occluded person re-identification; partial person re-identification,Deep learning; Security systems; Deep learning; Identification method; Literature survey; Non-overlapping cameras; Occluded person re-identification; Occlusion problems; Partial person re-identification; Person re identifications; Persons of interests; Alignment
VirtualLoc: Large-scale Visual Localization Using Virtual Images,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180786265&doi=10.1145%2f3622788&partnerID=40&md5=41ce2ee59a79cbb271cc8d6602d6f31e,"Robust and accurate camera pose estimation is fundamental in computer vision. Learning-based regression approaches acquire six-degree-of-freedom camera parameters accurately from visual cues of an input image. However, most are trained on street-view and landmark datasets. These approaches can hardly be generalized to overlooking use cases, such as the calibration of the surveillance camera and unmanned aerial vehicle. Besides, reference images captured from the real world are rare and expensive, and their diversity is not guaranteed. In this article, we address the problem of using alternative virtual images for visual localization training. This work has the following principle contributions: First, we present a new challenging localization dataset containing six reconstructed large-scale three-dimensional scenes, 10,594 calibrated photographs with condition changes, and 300k virtual images with pixelwise labeled depth, relative surface normal, and semantic segmentation. Second, we present a flexible multi-feature fusion network trained on virtual image datasets for robust image retrieval. Third, we propose an end-to-end confidence map prediction network for feature filtering and pose estimation. We demonstrate that large-scale rendered virtual images are beneficial to visual localization. Using virtual images can solve the diversity problem of real images and leverage labeled multi-feature data for deep learning. Experimental results show that our method achieves remarkable performance surpassing state-of-the-art approaches. To foster research on improvement for visual localization using synthetic images, we release our benchmark at https://github.com/YuanXiong/contributions.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",image retrieval; rendering; virtual reality; Visual localization,Antennas; Cameras; Deep learning; Degrees of freedom (mechanics); Image enhancement; Image retrieval; Large dataset; Security systems; Semantic Segmentation; Semantics; Camera parameter; Camera pose estimation; Input image; Landmark datasets; Large-scales; Rendering; Six degrees of freedom; Virtual images; Visual cues; Visual localization; Virtual reality
Explaining Cross-domain Recognition with Interpretable Deep Classifier,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181702767&doi=10.1145%2f3623399&partnerID=40&md5=dbe7699963dd23dadafab0bfd7ab3346,"The recent advances in deep learning predominantly construct models in their internal representations, and it is opaque to explain the rationale behind and decisions to human users. Such explainability is especially essential for domain adaptation, whose challenges require developing more adaptive models across different domains. In this article, we ask the question: How much does each sample in the source domain contribute to the network's prediction on the samples from the target domain? To address this, we devise a novel Interpretable Deep Classifier (IDC) that learns the nearest source samples of a target sample as evidence upon which the classifier makes the decision. Technically, IDC maintains a differentiable memory bank for each category, and the memory slot derives a form of key-value pair. The key records the features of discriminative source samples, and the value stores the corresponding properties, e.g., representative scores of the features for describing the category. IDC computes the loss between the output of IDC and the labels of source samples to back-propagate to adjust the representative scores and update the memory banks. Extensive experiments on Office-Home and VisDA-2017 datasets demonstrate that our IDC leads to a more explainable model with almost no accuracy degradation and effectively calibrates classification for optimum reject options. More remarkably, when taking IDC as a prior interpreter, capitalizing on 0.1% source training data selected by IDC still yields superior results than that uses full training set on VisDA-2017 for unsupervised domain adaptation. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Explainable; memory matching; unsupervised domain adaptation,Classification (of information); Construct models; Cross-domain; Domain adaptation; Explainable; Internal it; Internal representation; Matchings; Memory banks; Memory matching; Unsupervised domain adaptation; Deep learning
Attention-guided Multi-modality Interaction Network for RGB-D Salient Object Detection,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181455317&doi=10.1145%2f3624747&partnerID=40&md5=4499b4b80fa8b5f1c7bd78c7cb69c075,"The past decade has witnessed great progress in RGB-D salient object detection (SOD). However, there are two bottlenecks that limit its further development. The first one is low-quality depth maps. Most existing methods directly use raw depth maps to perform detection, but low-quality depth images can bring negative impacts to the detection performance. Hence, it is not desirable to utilize depth maps indiscriminately. The other one is how to effectively predict salient maps with clear boundary and complete salient region. To address these problems, an Attention-Guided Multi-Modality Interaction Network (AMINet) is proposed. First, we propose a new quality enhancement strategy for unreliable depth images, named Depth Enhancement Module (DEM). With respect to the second issue, we propose Cross-Modality Attention Module (CMAM) to rapidly locate salient region. The Boundary-Aware Module (BAM) is designed to utilize high-level feature to guide the low-level feature generation in a top-down way to make up for the dilution of the boundary. To further improve the accuracy, we propose Atrous Refined Block (ARB) to adaptively compensate for the shortcoming of atrous convolution. By integrating these interactive modules, features from depth and RGB streams can be refined efficiently, which consequently boosts the detection performance. Experimental results demonstrate the proposed AMINet exceeds state-of-the-art (SOTA) methods on several public RGB-D datasets. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",boundary aware; depth enhancement; multi-modality; Salient object detection,Image enhancement; Object detection; Boundary aware; Depth enhancement; Depth image; Depthmap; Detection performance; Interaction networks; Low qualities; Multi-modality; Salient object detection; Salient regions; Object recognition
GJFusion: A Channel-Level Correlation Construction Method for Multimodal Physiological Signal Fusion,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176764471&doi=10.1145%2f3617503&partnerID=40&md5=09bb1e02e9fcb87791f0d7dfbf1b3a5c,"Physiological signal based ubiquitous computing has garnered significant attention. However, the heterogeneity among multimodal physiological signals poses a critical challenge to practical applications. To traverse this heterogeneity gap, recent studies have focused on establishing inter-modality correlations. Early works only consider coarse-level correlations between the embeddings of each modality. More recent graph-based approaches incorporate prior knowledge-based correlations, although they may not be entirely accurate. In this article, we propose the Graph Joint Fusion (GJFusion) network, which leverages channel-level inter-modality correlations based on a graph joint to mitigate the heterogeneous gap. Our proposed GJFusion first represents each modality as a graph, with each vertex corresponding to a signal channel, and the edges denoting their functional connectivity. We then join each modality by constructing inter-modality correlations for each salient channel using a sampling-based matching method. Discarded channels are transformed into a virtual vertex through a lightweight pooling operation. Subsequently, the fusion network integrates intra- and inter-modality features, enabling multimodal physiological signal fusion. To validate the effectiveness of our method, we select emotional state recognition as the downstream task and conduct comprehensive experiments on two benchmark datasets. The results demonstrate that our proposed GJFusion network surpasses the latest state-of-the-art methods, achieving relative accuracy improvements of 1.22% and 0.81% on the DEAP and MAHNOB-HCI datasets, respectively. Furthermore, visualization experiments of the salient brain regions reveal the presence of interpretable knowledge within the proposed GJFusion model.  © 2023 Copyright held by the owner/author(s).",emotion state recognition; graph neural network; Multimodal; physiological signal; ubiquitous computing,Deep learning; Emotion Recognition; Graph neural networks; Graphic methods; Knowledge based systems; State estimation; Ubiquitous computing; Channel-level; Construction method; Emotion state recognition; Graph neural networks; Intermodality; Level correlation; Multi-modal; Physiological signals; Signal fusions; State recognition; Brain
Black-box Attack against Self-supervised Video Object Segmentation Models with Contrastive Loss,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176739098&doi=10.1145%2f3617502&partnerID=40&md5=df450168ee2d4a57d4e42862da41fc2b,"Deep learning models have been proven to be susceptible to malicious adversarial attacks, which manipulate input images to deceive the model into making erroneous decisions. Consequently, the threat posed to these models serves as a poignant reminder of the necessity to focus on the model security of object segmentation algorithms based on deep learning. However, the current landscape of research on adversarial attacks primarily centers around static images, resulting in a dearth of studies on adversarial attacks targeting Video Object Segmentation (VOS) models. Given that a majority of self-supervised VOS models rely on affinity matrices to learn feature representations of video sequences and achieve robust pixel correspondence, our investigation has delved into the impact of adversarial attacks on self-supervised VOS models. In response, we propose an innovative black-box attack method incorporating contrastive loss. This method induces segmentation errors in the model through perturbations in the feature space and the application of a pixel-level loss function. Diverging from conventional gradient-based attack techniques, we adopt an iterative black-box attack strategy that incorporates contrastive loss across the current frame, any two consecutive frames, and multiple frames. Through extensive experimentation conducted on the DAVIS 2016 and DAVIS 2017 datasets using three self-supervised VOS models and one unsupervised VOS model, we unequivocally demonstrate the potent attack efficiency of the black-box approach. Remarkably, the J&F metric value experiences a significant decline of up to 50.08% post-attack.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Black-box adversarial attack; contrastive loss; feature loss; pixel-level loss; self-supervised video object segmentation,Deep learning; Image segmentation; Learning systems; Motion compensation; Pixels; Black boxes; Black-box adversarial attack; Contrastive loss; Feature loss; Learning models; Pixel level; Pixel-level loss; Segmentation models; Self-supervised video object segmentation; Video objects segmentations; Iterative methods
A Feature Map is Worth a Video Frame: Rethinking Convolutional Features for Visible-Infrared Person Re-identification,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176812011&doi=10.1145%2f3617375&partnerID=40&md5=8b91ae48095070347a2a826469ef2637,"Visible-Infrared Person Re-identification (VI-ReID) aims to search for the identity of the same person across different spectra. The feature maps obtained from the convolutional layers are generally used for loss calculation in the later stages of the model in VI-ReID, but their role in the early and middle stages of the model remains unexplored. In this article, we propose a novel Rethinking Convolutional Features (ReCF) approach for VI-ReID. ReCF consists of two modules: Middle Feature Generation (MFG), which utilizes the feature maps in the early stage to reduce significant modality gap, and Temporal Feature Aggregation (TFA), which uses the feature maps in the middle stage to aggregate multi-level features for enlarging the receptive field. MFG generates middle modality features in the form of a learnable convolution layer as a bridge between RGB and IR modalities, which is more flexible than using fixed-parameter grayscale images and yields a better middle modality to further reduce the modality gap. TFA first treats the convolution process as a video sequence, and the feature map of each convolution layer can be considered a worthwhile video frame. Based on this, we can obtain a multi-level receptive field and a temporal refinement. In addition, we introduce a color-unrelated loss and a modality-unrelated loss to constrain the modality features for providing a common feature representation space. Experimental results on the challenging VI-ReID datasets demonstrate that our proposed method achieves state-of-the-art performance.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",middle modality; modality gap; multi-level feature aggregation; Visible-infrared person re-identification,Feature aggregation; Feature generation; Feature map; Middle modality; Modality gap; Multi-level feature aggregation; Multilevels; Person re identifications; Video frame; Visible-infrared person re-identification; Convolution
Adaptive Adversarial Logits Pairing,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176444265&doi=10.1145%2f3616375&partnerID=40&md5=b5a828736adf81b8694743daa4a932a3,"Adversarial examples provide an opportunity as well as impose a challenge for understanding image classification systems. Based on the analysis of the adversarial training solution - Adversarial Logits Pairing (ALP), we observed in this work that: (1) The inference of adversarially robust model tends to rely on fewer high-contribution features compared with vulnerable ones. (2) The training target of ALP does not fit well to a noticeable part of samples, where the logits pairing loss is overemphasized and obstructs minimizing the classification loss. Motivated by these observations, we design an Adaptive Adversarial Logits Pairing (AALP) solution by modifying the training process and training target of ALP. Specifically, AALP consists of an adaptive feature optimization module with Guided Dropout to systematically pursue fewer high-contribution features, and an adaptive sample weighting module by setting sample-specific training weights to balance between logits pairing loss and classification loss. The proposed AALP solution demonstrates superior defense performance on multiple datasets with extensive experiments. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adaptive; Adversarial defense; dropout,Adaptive; Adaptive features; Adversarial defense; Dropout; Features optimizations; Image classification systems; Optimization module; Robust modeling; Training process; Training solutions; Network security
Contrastive Attention-guided Multi-level Feature Registration for Reference-based Super-resolution,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176743452&doi=10.1145%2f3616495&partnerID=40&md5=9f0efebe9243605ff20ce73cdacc2764,"Given low-quality input and assisted by referential images, reference-based super-resolution (RefSR) strives to enlarge the spatial size with the guarantee of realistic textures, for which sophisticated feature-matching strategies are naturally demanded. However, the miserable transformation gap between inputs and references, e.g., texture rotation and scaling within patches, often yields distorted textures and terrible ghosting artifacts, which seriously hampers the visual senses and their further investigation. To circumvent this challenge, we propose a contrastive attention-guided multi-level feature registration for RefSR, explicitly tapping the potential of interacting between inputs and references. Specifically, we develop a multi-level feature warping scheme, involving patch-level coarse feature swapping and pixel-level deformable alignment, to model generalized spatial transformation correspondences steered by contrastive attention. Notably, a spatial registration module is embedded for further calibration against the potential misalignment issue and inter-feature distribution difference. In addition, aiming at suppressing the impacts of irrelevant or superfluous information on cross-scale features, we incorporate a multi-residual feature fusion module to strive for visually plausible textures. Experimental results on four publicly available datasets demonstrate that our method outperforms most state-of-the-art approaches in terms of both efficiency and perceptual effectiveness.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",contrastive attention; feature warping; Reference-based super-resolution; spatial registration,Interactive computer graphics; Optical resolving power; Contrastive attention; Feature warping; Features matching; Low qualities; Multilevels; Reference-based super-resolution; Scalings; Spatial registrations; Spatial size; Superresolution; Textures
"Multimodality Representation Learning: A Survey on Evolution, Pretraining and Its Applications",2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181439905&doi=10.1145%2f3617833&partnerID=40&md5=3280f082ec5b7c9a0ca9136fdadf974e,"Multimodality Representation Learning, as a technique of learning to embed information from different modalities and their correlations, has achieved remarkable success on a variety of applications, such as Visual Question Answering (VQA), Natural Language for Visual Reasoning (NLVR), and Vision Language Retrieval (VLR). Among these applications, cross-modal interaction and complementary information from different modalities are crucial for advanced models to perform any multimodal task, e.g., understand, recognize, retrieve, or generate optimally. Researchers have proposed diverse methods to address these tasks. The different variants of transformer-based architectures performed extraordinarily on multiple modalities. This survey presents the comprehensive literature on the evolution and enhancement of deep learning multimodal architectures to deal with textual, visual and audio features for diverse cross-modal and modern multimodal tasks. This study summarizes the (i) recent task-specific deep learning methodologies, (ii) the pretraining types and multimodal pretraining objectives, (iii) from state-of-the-art pretrained multimodal approaches to unifying architectures, and (iv) multimodal task categories and possible future improvements that can be devised for better multimodal learning. Moreover, we prepare a dataset section for new researchers that covers most of the benchmarks for pretraining and finetuning. Finally, major challenges, gaps, and potential research topics are explored. A constantly-updated paperlist related to our survey is maintained at https://github.com/marslanm/multimodality-representation-learning.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",multimodal applications; multimodal methods; Multimodality; pretrained models; representation learning,Deep learning; Learning systems; Natural language processing systems; Visual languages; ITS applications; Multi-modal; Multi-modality; Multimodal application; Multimodal method; Natural languages; Pre-training; Pretrained model; Question Answering; Representation learning; Architecture
Underwater Image Quality Assessment from Synthetic to Real-world: Dataset and Objective Method,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181166585&doi=10.1145%2f3624983&partnerID=40&md5=b2616737cf4292aced36fc74d3acc3d2,"The complicated underwater environment and lighting conditions lead to severe influence on the quality of underwater imaging, which tends to impair underwater exploration and research. To effectively evaluate the quality of underwater images, an underwater image quality assessment dataset is constructed from synthetic to real-world, and then a new objective underwater image assessment method based on the characteristics of the underwater imaging is proposed (UICQA). Specifically, to address the lack of a publicly available datasets and more accurately quantify the quality of underwater images, a subjective underwater image quality assessment dataset from synthetic to real-world underwater images, named USRD, is constructed. Considering that the transmission map can effectively reflect the characteristics of the underwater imaging, statistical features are effectively extracted from the transmission map for distinguishing underwater images of different quality. Further, considering that the transmission map negatively correlates with scene depth, a local-to-global transmission map weighted contrast feature is constructed. Additionally, the color features of human perception and texture features based on fractal dimensions are proposed. Finally, the experimental results show that the proposed UICQA method exhibits the highest correlation with ground truth scores compared to state-of-the-art UIQA methods. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",transmission map; Underwater image quality assessment; underwater imaging,Fractal dimension; Image quality; Textures; Environment conditions; Image quality assessment; Lighting conditions; Objective methods; Real-world; Real-world datasets; Transmission map; Underwater environments; Underwater exploration; Underwater image quality assessment; Underwater imaging
Self-Adaptive Clothing Mapping Based Virtual Try-on,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181675091&doi=10.1145%2f3613453&partnerID=40&md5=7d410a09b1628d02e910f421418ac5ea,"VTON (Virtual Try-ON), as an innovative visual application in e-commerce scenarios with great commercial value, has been widely studied in recent years. Due to its better robustness and realistic effect, deformation-synthesize-based VTON has become the dominant approach in this field. Existing clothing deformation techniques optimize the mapping relations between the original clothing image and the ground truth (GT) image of the worn clothing. However, there are color differences between the original and GT clothing images caused by lighting, warping, and occlusion. The color differences may lead to misaligned clothing mapping by only minimizing the cost of pixel value difference. Another drawback is that taking the parsing prediction as GT will bring alignment remnant, rooting in the processing order of parsing and deformation. Aiming above two drawbacks, we put forward SAME-VTON (Self-Adaptive clothing Mapping basEd Virtual Try-ON) for achieving realistic virtual try-on results. The core of SAME-VTON is the self-adaptive clothing mapping technique, composed of two parts: a color-adaptive clothing mapping module and a parsing-adaptive prediction process. In the color-adaptive clothing mapping module, we map each pixel of the target clothing with a combination of multiple pixel values from the original clothing image, which considers both the position and color changes. Furthermore, different combination weights are learned to increase the diversity of color mapping. In the parsing-adaptive prediction process, the color-adaptive clothing mapping module is adopted to deform clothing first, then the human parsing result is predicted under the reference of the deformed clothing, which can avoid alignment remnant. Extensive experiments demonstrate that the proposed SAME-VTON with the self-adaptive clothing mapping technique can achieve optimal mapping in the case of large color differences and obtain superior results compared with existing deformation-synthesize-based VTON.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",clothing mapping; color difference; self-adaptive; Virtual try-on,Color; Colorimetry; Forecasting; Pixels; Adaptive predictions; Clothing mapping; Color difference; Ground truth; Mapping modules; Mapping techniques; Pixel values; Prediction process; Self-adaptive; Virtual try-on; Mapping
Composed Image Retrieval using Contrastive Learning and Task-oriented CLIP-based Features,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181504063&doi=10.1145%2f3617597&partnerID=40&md5=a8d76345ba34ebea01e383654df5a77e,"Given a query composed of a reference image and a relative caption, the Composed Image Retrieval goal is to retrieve images visually similar to the reference one that integrates the modifications expressed by the caption. Given that recent research has demonstrated the efficacy of large-scale vision and language pre-trained (VLP) models in various tasks, we rely on features from the OpenAI CLIP model to tackle the considered task. We initially perform a task-oriented fine-tuning of both CLIP encoders using the element-wise sum of visual and textual features. Then, in the second stage, we train a Combiner network that learns to combine the image-text features integrating the bimodal information and providing combined features used to perform the retrieval. We use contrastive learning in both stages of training. Starting from the bare CLIP features as a baseline, experimental results show that the task-oriented fine-tuning and the carefully crafted Combiner network are highly effective and outperform more complex state-of-the-art approaches on FashionIQ and CIRR, two popular and challenging datasets for composed image retrieval. Code and pre-trained models are available at https://github.com/ABaldrati/CLIP4Cir.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",combiner networks; Multimodal retrieval; vision language model,Combiner network; Fine tuning; Language model; Large-scales; Multi-modal; Multimodal retrieval; Recent researches; Reference image; Task-oriented; Vision language model; Image retrieval
Learning from the Past: Fast NAS for Tasks and Datasets,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181489464&doi=10.1145%2f3618000&partnerID=40&md5=6e3b58b00e0ed7ada55eadcdf47cfa1c,"Nowadays, with the advancement of technology, many retail companies require in-house data scientist teams to build machine learning tasks, such as user segmentation and item price prediction. These teams typically use a trial-and-error process to obtain a good model for a given dataset and machine learning task, which is time-consuming and requires expertise. However, the team may have built models for other tasks on different datasets. This article proposes a framework to obtain a model architecture using the previous solved machine learning tasks and datasets. By analyzing real datasets with over 70,000 images from 11 online retail e-commerce websites, it is demonstrated that the performance of a model is related to the similarity among datasets, models, and machine learning tasks. A framework is hence proposed to obtain the model using the similarities among them. It was proven that the model was 26.6% better in accuracy, and using only 20% of the runtime while comparing to an auto network architecture search library, Auto-Keras, in predicting the attributes of fashion images. To the best of our knowledge, this is the first article to obtain the best model based on the similarity among machine learning tasks, models, and datasets.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Auto-ML; e-commerce; machine learning,Electronic commerce; Network architecture; Auto-ML; E- commerces; Learning dataset; Learning from the pasts; Learning tasks; Machine-learning; Modeling architecture; Price prediction; Trial-and-error process; User segmentation; Machine learning
PLACE Dropout: A Progressive Layer-wise and Channel-wise Dropout for Domain Generalization,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181729771&doi=10.1145%2f3624015&partnerID=40&md5=ed7e0ab63449abeb5e786a4057a816a1,"Domain generalization (DG) aims to learn a generic model from multiple observed source domains that generalizes well to arbitrary unseen target domains without further training. The major challenge in DG is that the model inevitably faces a severe overfitting issue due to the domain gap between source and target domains. To mitigate this problem, some dropout-based methods have been proposed to resist overfitting by discarding part of the representation of the intermediate layers. However, we observe that most of these methods only conduct the dropout operation in some specific layers, leading to an insufficient regularization effect on the model. We argue that applying dropout at multiple layers can produce stronger regularization effects, which could alleviate the overfitting problem on source domains more adequately than previous layer-specific dropout methods. In this article, we develop a novel layer-wise and channel-wise dropout for DG, which randomly selects one layer and then randomly selects its channels to conduct dropout. Particularly, the proposed method can generate a variety of data variants to better deal with the overfitting issue. We also provide theoretical analysis for our dropout method and prove that it can effectively reduce the generalization error bound. Besides, we leverage the progressive scheme to increase the dropout ratio with the training progress, which can gradually boost the difficulty of training the model to enhance its robustness. Extensive experiments on three standard benchmark datasets have demonstrated that our method outperforms several state-of-the-art DG methods. Our code is available at https://github.com/lingeringlight/PLACEdropout. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",distribution shift; Domain generalization; dropout regularization; overfitting problem,Distribution shift; Domain generalization; Dropout regularization; Generalisation; Layer-wise; Learn+; Over fitting problem; Overfitting; Regularisation; Target domain
Boosting Scene Graph Generation with Contextual Information,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176759372&doi=10.1145%2f3615868&partnerID=40&md5=621cac1626167c1eb621b539a2ce1cb2,"Scene graph generation (SGG) has been developed to detect objects and their relationships from the visual data and has attracted increasing attention in recent years. Existing works have focused on extracting object context for SGG. However, very few works have attempted to exploit implicit contextual correlations among relationships of the objects. Furthermore, most existing SGG schemes rely on high-level features to predict the predicates while overlooking the potential inherent association of low-level features with the object relationships. We present in this article a novel scheme to capture enhanced contextual information for both objects and relationships. We design a Dual-branch Context Analysis Transformer (DCAT) architecture to extract both object context and relationship context from the visual data with dual transformer branches and then effectively fuse both high-level and low-level features by an adaptive approach to facilitate relationship prediction. Specifically, we first conduct feature representation learning to enrich relation representations by the visual, spatial, and linguistic feature extractors. Next, two transformer branches are designed to leverage the modeling of global associative interaction and mine the hidden association among objects and relationships. Then, we devise a novel feature disentangling method to decouple contextualized high-level features with guidance from the visual semantics. Finally, we develop a refined attention module to perform low-level feature recalibration for the refinement of the final predicate prediction. Experiments on Visual Genome and Action Genome datasets demonstrate the effectiveness of DCAT for both image and video SGG settings. Moreover, we also test the quality of the generated image scene graphs to verify the generalizability on downstream tasks like sentence-to-graph retrieval and image retrieval.  © 2023 Copyright held by the owner/author(s).",Scene graph generation; Visual relationship detection,Forecasting; Image retrieval; Object detection; Semantics; Context analysis; Contextual information; Graph generation; High-level features; Image scene; Low-level features; Scene graph generation; Scene-graphs; Visual data; Visual relationship detection; Association reactions
Cross-modality Multiple Relations Learning for Knowledge-based Visual Question Answering,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181675113&doi=10.1145%2f3618301&partnerID=40&md5=0d820d24c9c8efc09b448920149000ff,"Knowledge-based visual question answering not only needs to answer the questions based on images but also incorporates external knowledge to study reasoning in the joint space of vision and language. To bridge the gap between visual content and semantic cues, it is important to capture the question-related and semantics-rich vision-language connections. Most existing solutions model simple intra-modality relation or represent cross-modality relation using a single vector, which makes it difficult to effectively model complex connections between visual features and question features. Thus, we propose a cross-modality multiple relations learning model, aiming to better enrich cross-modality representations and construct advanced multi-modality knowledge triplets. First, we design a simple yet effective method to generate multiple relations that represent the rich cross-modality relations. The various cross-modality relations link the textual question to the related visual objects. These multi-modality triplets efficiently align the visual objects and corresponding textual answers. Second, to encourage multiple relations to better align with different semantic relations, we further formulate a novel global-local loss. The global loss enables the visual objects and corresponding textual answers close to each other through cross-modality relations in the vision-language space, and the local loss better preserves semantic diversity among multiple relations. Experimental results on the Outside Knowledge VQA and Knowledge-Routed Visual Question Reasoning datasets demonstrate that our model outperforms the state-of-the-art methods.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Cross-modality relation; external knowledge; visual question answering,Knowledge based systems; Visual languages; Cross modality; Cross-modality relation; External knowledge; Knowledge based; Local loss; Multi-modality; Question Answering; Simple++; Visual objects; Visual question answering; Semantics
Deep Learning for Logo Detection: A Survey,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181496006&doi=10.1145%2f3611309&partnerID=40&md5=afdecf99e43c616addbc388ed09108a6,"Logo detection has gradually become a research hotspot in the field of computer vision and multimedia for its various applications, such as social media monitoring, intelligent transportation, and video advertising recommendation. Recent advances in this area are dominated by deep learning-based solutions, where many datasets, learning strategies, network architectures, and loss functions have been employed. This article reviews the advance in applying deep learning techniques to logo detection. First, we discuss a comprehensive account of public datasets designed to facilitate performance evaluation of logo detection algorithms, which tend to be more diverse, more challenging, and more reflective of real life. Next, we perform an in-depth analysis of the existing logo detection strategies and their strengths and weaknesses of each learning strategy. Subsequently, we summarize the applications of logo detection in various fields, from intelligent transportation and brand monitoring to copyright and trademark compliance. Finally, we analyze the potential challenges and present the future directions for the development of logo detection. This study aims better to inform readers about the current state of logo detection and encourage more researchers to get involved in logo detection.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",computer vision; datasets; deep learning; Logo detection,Deep learning; Learning systems; Network architecture; Dataset; Deep learning; Hotspots; Intelligent transportation; Intelligent video; Learning strategy; Logo detections; Network loss; Social media monitoring; Video advertisings; Computer vision
How Will You Pod? Implications of Creators' Perspectives for Designing Innovative Podcasting Tools,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177858803&doi=10.1145%2f3625099&partnerID=40&md5=12275e8a3a2093bace9880188101af8a,"While centred on the medium of audio, podcasts are often a multimedia concern, and one that has become hugely popular in recent years, though relatively little is known about the perspectives of podcast creators and their visions of innovation. This article details the results of an exploratory study conducted to enhance our understanding of potential innovation in the field of podcasting. Sixteen podcast creators were interviewed about their work and what they wanted from next-generation podcasts, in order to understand the requirements and expectations of tools that could be built to create new forms of audio-based programming. Through a combination of qualitative and quantitative analysis, we reveal novel findings, such as the duality between ""listener-centric""and ""creator-centric""innovations, to improve listener experience but also to unleash new creative possibilities in a streamlined production workflow. We shed light on what podcast creators envision as ""next-generation podcasting,""the archetypal podcast production workflow, and creators' expectations of podcasting tools. Combining these findings, we identify how the workflow could be modified to include new steps that will help to realise podcast creators' visions. This study crystalises on important information about podcasters - their behavior and perspectives on the future of the medium - which will allow further research, design, and development in the field to be founded upon empirical observations. © 2023 Copyright held by the owner/author(s).",audio production; broadcasting; new media; Podcast; production workflow,Audio production; Audio-based; Creatives; Exploratory studies; New forms; New media; Podcasting; Podcasts; Production workflows; Qualitative and quantitative analysis; Computer programming
Relation with Free Objects for Action Recognition,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176737534&doi=10.1145%2f3617596&partnerID=40&md5=2e1b6f9e71479129d30420d87389dc3f,"Relevant objects are widely used for aiding human action recognition in still images. Such objects are founded by a dedicated and pre-trained object detector in all previous methods. Such methods have two drawbacks. First, training an object detector requires intensive data annotation. This is costly and sometimes unaffordable in practice. Second, the relation between objects and humans are not fully taken into account in training.This work proposes a systematic approach to address the two problems. We propose two novel network modules. The first is an object extraction module that automatically finds relevant objects for action recognition, without requiring annotations. Thus, it is free. The second is a human-object relation module that models the pairwise relation between humans and objects, and enhances their features. Both modules are trained in the action recognition network, end-to-end.Comprehensive experiments and ablation studies on three datasets for action recognition in still images demonstrate the effectiveness of the proposed approach. Our method yields state-of-the-art results. Specifically, on the HICO dataset, it achieves 44.9% mAP, which is 12% relative improvement over the previous best result. In addition, this work makes an observational contribution that it is no longer necessary to rely on a pre-trained object detector for this task. Relevant objects can be found via end-to-end learning with only action labels. This is encouraging for action recognition in the wild. Models and code will be released. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",object detection; PhrasesAction recognition; relation,Object recognition; Action recognition; Action Recognition in still images; Data annotation; End to end; Human-action recognition; Object detectors; Object extraction; Objects detection; Phrasesaction recognition; Relation; Object detection
Image Defogging Based on Regional Gradient Constrained Prior,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181475692&doi=10.1145%2f3617834&partnerID=40&md5=fe0b39b530388bd61aeb778adecf99bd,"Foggy days limit the functionality of outdoor surveillance systems. However, it is still a challenge for existing methods to maintain the uniformity of defogging between image regions with a similar depth of field and large differences in appearance. To address above problem, this article proposes a regional gradient constrained prior (RGCP) for defogging that uses the piecewise smoothing characteristic of the scene structure to achieve accurate estimation and reliable constraint of the transmission. RGCP first derives that when adjacent similar pixels in the fog image are aggregated and spatially divided into regions, clusters of region pixels in RGB space conform to a chi-square distribution. The offset of the confidence boundary of the clusters can be regarded as the initial transmission of each region. RGCP further uses a gradient distribution to distinguish different regional appearances and formulate an interregional constraint function to constrain the overestimation of the transmission in the flat region, thereby maintaining the consistency between the estimated transmission map and the depth map. The experimental results demonstrate that the proposed method can achieve natural defogging performance in terms of various foggy conditions.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Image defogging; regional gradient constraint function; transmission estimation prior,Probability distributions; Transmissions; Constraint functions; Depth of field; Gradient constrained; Image defogging; Image regions; Piece-wise; Regional gradient constraint function; Regional gradients; Surveillance systems; Transmission estimation prior; Pixels
An Applied Image Cryptosystem on Moore's Automaton Operating on δ(qk)/F2,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176766119&doi=10.1145%2f3614433&partnerID=40&md5=8ca4eb46667d21bd6354293da678c040,"The volume of multimedia-based image data or video frames in Web 3.0 is constantly increasing, owing to the advancement of real-time data transmission. However, security vulnerabilities frequently impair the performance of real-time applications. Many researchers have recently proposed image encryption schemes based on a high-dimensional chaotic system due to properties such as ergodicity and initial state sensitivity. Nonetheless, most schemes have suffered from excessive computational complexity, low security, and the generation of cryptographically secure random numbers. To overcome these challenges, an efficient and highly secure cryptosystem is necessary for safe multimedia transmission in Web 3.0. This article proposes a novel work on the image cryptosystem based on the Escalation function with a one-time key-oriented Moore's Automaton over a finite field 2. The Escalation function is a nonlinear scrambling technique for plaintext images that goes through the confusion phase and plays an essential role in row-column permutation. To make the algorithm more secure and robust in the diffusion phase, the proposed Moore's Automaton produced ciphertext images through a highly random key stream generated by the combination of a logistic map and cyclic group. Specifically, the proposed Moore's Automaton operates on δ(qk)/2 to render random binary bits into unpredictable sequences to construct ciphertext images. Our new finding quickens the speed and provides adequate key space, and pixel distributions are more uniform, have high entropy value, and are secure against differential and statistical attacks.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cyclic group; decryption; diffusion; encryption; escalation; Image encryption; Moore's Automaton; security,Chaotic systems; Image processing; Random number generation; Security of data; Ciphertexts; Cyclic group; Data frames; Decryption; Escalation; Image data; Images encryptions; Moore automaton; Security; Web 3.0; Cryptography
Incremental Audio-Visual Fusion for Person Recognition in Earthquake Scene,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176422355&doi=10.1145%2f3614434&partnerID=40&md5=e7c16d57346db51f9fdab47bb43b1e25,"Earthquakes have a profound impact on social harmony and property, resulting in damage to buildings and infrastructure. Effective earthquake rescue efforts require rapid and accurate determination of whether any survivors are trapped in the rubble of collapsed buildings. While deep learning algorithms can enhance the speed of rescue operations using single-modal data (either visual or audio), they are confronted with two primary challenges: insufficient information provided by single-modal data and catastrophic forgetting. In particular, the complexity of earthquake scenes means that single-modal features may not provide adequate information. Additionally, catastrophic forgetting occurs when the model loses the information learned in a previous task after training on subsequent tasks, due to non-stationary data distributions in changing earthquake scenes. To address these challenges, we propose an innovative approach that utilizes an incremental audio-visual fusion model for person recognition in earthquake rescue scenarios. Firstly, we leverage a cross-modal hybrid attention network to capture discriminative temporal context embedding, which uses self-attention and cross-modal attention mechanisms to combine multi-modality information, enhancing the accuracy and reliability of person recognition. Secondly, an incremental learning model is proposed to overcome catastrophic forgetting, which includes elastic weight consolidation and feature replay modules. Specifically, the elastic weight consolidation module slows down learning on certain weights based on their importance to previously learned tasks. The feature replay module reviews the learned knowledge by reusing the features conserved from the previous task, thus preventing catastrophic forgetting in dynamic environments. To validate the proposed algorithm, we collected the Audio-Visual Earthquake Person Recognition (AVEPR) dataset from earthquake films and real scenes. Furthermore, the proposed method gets 85.41% accuracy while learning the 10th new task, which demonstrates the effectiveness of the proposed method and highlights its potential to significantly improve earthquake rescue efforts. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Cross-modal audio-visual fusion; elastic weight consolidation; feature replay; incremental learning; person recognition,Deep learning; Learning algorithms; Learning systems; Modal analysis; Audio-visual fusion; Catastrophic forgetting; Cross-modal; Cross-modal audio-visual fusion; Elastic weight consolidation; Feature replay; Incremental learning; Modal data; Person recognition; Single-modal; Earthquakes
E-detector: Asynchronous Spatio-temporal for Event-based Object Detection in Intelligent Transportation System,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173984372&doi=10.1145%2f3584361&partnerID=40&md5=4e8cc130dea9e3b4470770fc859fbfc0,"In intelligent transportation systems, various sensors, including radar and conventional frame cameras, are used to improve system robustness in various challenging scenarios. An event camera is a novel bio-inspired sensor that has attracted the interest of several researchers. It provides a form of neuromorphic vision to capture motion information asynchronously at high speeds. Thus, it possesses advantages for intelligent transportation systems that conventional frame cameras cannot match, such as high temporal resolution, high dynamic range, as well as sparse and minimal motion blur. Therefore, this study proposes an E-detector based on event cameras that asynchronously detect moving objects. The main innovation of our framework is that the spatiotemporal domain of the event camera can be adjusted according to different velocities and scenarios. It overcomes the inherent challenges that traditional cameras face when detecting moving objects in complex environments, such as high speed, complex lighting, and motion blur. Moreover, our approach adopts filter models and transfer learning to improve the performance of event-based object detection. Experiments have shown that our method can detect high-speed moving objects better than conventional cameras using state-of-the-art detection algorithms. Thus, our proposed approach is extremely competitive and extensible, as it can be extended to other scenarios concerning high-speed moving objects. The study findings are expected to unlock the potential of event cameras in intelligent transportation system applications.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adaptable framework; Bio-inspired sensor; event-based vision; moving object detection,Biomimetics; Image enhancement; Intelligent systems; Intelligent vehicle highway systems; Object detection; Object recognition; Adaptable framework; Bioinspired sensors; Event-based; Event-based vision; High Speed; Intelligent transportation systems; Motion blur; Moving objects; Moving-object detection; Objects detection; Cameras
Automatic Assessment of Depression and Anxiety through Encoding Pupil-wave from HCI in VR Scenes,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176778457&doi=10.1145%2f3513263&partnerID=40&md5=15b524e61d2becb5b3c6afd97cc7cf8a,"At present, there have been many studies on the methods of using the deep learning regression model to assess depression level based on behavioral signals (facial expression, speech, and language); however, the research on the assessment method of anxiety level using deep learning is absent. In this article, pupil-wave, a physiological signal collected by Human Computer Interaction (HCI) that can directly represent the emotional state, is developed to assess the level of depression and anxiety for the first time. In order to distinguish between different depression and anxiety levels, we use the HCI method to induce the participants' emotional experience through three virtual reality (VR) emotional scenes of joyful, sad, and calm, and construct two differential pupil-waves of joyful and sad with the calm pupil-wave as the baseline. Correspondingly, a dual-channel fusion depression and anxiety level assessment model is constructed using the improved multi-scale convolution module and our proposed width-channel attention module for one-dimensional signal processing. The test results show that the MAE/RMSE of the depression and anxiety level assessment method proposed in this article is 3.05/4.11 and 2.49/1.85, respectively, which has better assessment performance than other related research methods. This study provides an automatic assessment technique based on human computer interaction and virtual reality for mental health physical examination.  © 2023 Association for Computing Machinery.",Deep learning; Human computer interaction (HCI); pupil-wave; Virtual reality (VR); width-channel attention module,Deep learning; E-learning; Human computer interaction; Learning systems; Regression analysis; Signal encoding; Anxiety levels; Automatic assessment; Deep learning; Encodings; Facial Expressions; Human computer interaction; Pupil-wave; Regression modelling; Virtual reality; Width-channel attention module; Virtual reality
Editorial to Special Issue on Multimedia Cognitive Computing for Intelligent Transportation System,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176792358&doi=10.1145%2f3604938&partnerID=40&md5=5ac6fd07fa2b9124b2a5c8c4f61165bd,[No abstract available],,
Monocular Vision-aided Depth Measurement from RGB Images for Autonomous UAV Navigation,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168314974&doi=10.1145%2f3550485&partnerID=40&md5=998eb1851b7561fe21e6b895fd69cc35,"Monocular vision-based 3D scene understanding has been an integral part of many machine vision applications. Always, the objective is to measure the depth using a single RGB camera, which is at par with the depth cameras. In this regard, monocular vision-guided autonomous navigation of robots is rapidly gaining popularity among the research community. We propose an effective monocular vision-assisted method to measure the depth of an Unmanned Aerial Vehicle (UAV) from an impending frontal obstacle. This is followed by collision-free navigation in unknown GPS-denied environments. Our approach deals upon the fundamental principle of perspective vision that the size of an object relative to its field of view (FoV) increases as the center of projection moves closer towards the object. Our contribution involves modeling the depth followed by its realization through scale-invariant SURF features. Noisy depth measurements arising due to external wind, or the turbulence in the UAV, are rectified by employing a constant velocity-based Kalman filter model. Necessary control commands are then designed based on the rectified depth value to avoid the obstacle before collision. Rigorous experiments with SURF scale-invariant features reveal an overall accuracy of 88.6% with varying obstacles, in both indoor and outdoor environments.  © 2023 Association for Computing Machinery.",autonomous navigation; depth measurement; Monocular vision; obstacle avoidance; RGB-D vision; scene understanding; SURF; UAV,Antennas; Cameras; Robots; Unmanned aerial vehicles (UAV); Vision; Aerial vehicle; Autonomous navigation; D-vision; Depth measurements; Monocular vision; Obstacles avoidance; RGB-D vision; Scene understanding; SURF; Unmanned aerial vehicle; Navigation
Introduction to the Special Issue on DNA-centric Modeling and Practice for Next-generation Computing and Communication Systems,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176748235&doi=10.1145%2f3578364&partnerID=40&md5=310a8a0cef79685c51bb869210a69386,[No abstract available],,
Meta-MMFNet: Meta-learning-based Multi-model Fusion Network for Micro-expression Recognition,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176752893&doi=10.1145%2f3539576&partnerID=40&md5=25dcff1bfc48e57900e9af60fd8e35c5,"Despite its wide applications in criminal investigations and clinical communications with patients suffering from autism, automatic micro-expression recognition remains a challenging problem because of the lack of training data and imbalanced classes problems. In this study, we proposed a meta-learning-based multi-model fusion network (Meta-MMFNet) to solve the existing problems. The proposed method is based on the metric-based meta-learning pipeline, which is specifically designed for few-shot learning and is suitable for model-level fusion. The frame difference and optical flow features were fused, deep features were extracted from the fused feature, and finally in the meta-learning-based framework, weighted sum model fusion method was applied for micro-expression classification. Meta-MMFNet achieved better results than state-of-the-art methods on four datasets. The code is available at https://github.com/wenjgong/meta-fusion-based-method.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Feature fusion; meta-learning; micro-expression recognition; model fusion,Deep learning; Clinical communications; Criminal investigation; Expression recognition; Features fusions; Metalearning; Micro-expression recognition; Micro-expressions; Model fusion; Multi-model fusion; Patient's suffering; Learning systems
An Efficient and Accurate GPU-based Deep Learning Model for Multimedia Recommendation,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176787087&doi=10.1145%2f3524022&partnerID=40&md5=6d5a567c0df445fb5870a8bebd1d6591,"This article proposes the use of deep learning in human-computer interaction and presents a new explainable hybrid framework for recommending relevant hashtags on a set of orpheline tweets, which are tweets with hashtags. The approach starts by determining the set of batches used in the convolution neural network based on frequent pattern mining solutions. The convolutional neural network is then applied to the set of batches of tweets to learn the hashtags of the tweets. An optimization strategy has been proposed to accurately perform the learning process by reducing the number of frequent patterns. Moreover, eXplainable AI is introduced for hashtag recommendations by analyzing the user preferences and understanding the different weights of the deep learning model used in the learning process. This is performed by learning the hyper-parameters of the deep architecture using the genetic algorithm. GPU computing is also investigated to achieve high speed and enable the execution of the overall framework in real time. Extensive experimental analysis has been performed to show that our methodology is useful on different collections of tweets. The experimental results clearly show the efficiency of our proposed approach compared to baseline approaches in terms of both runtime and accuracy. Thus, the proposed solution achieves an accuracy of 90% when analyzing complex Wikipedia data while the other algorithms did not achieve 85% when processing the same amount of data.  © 2023 Association for Computing Machinery.",deep learning; GPU; Human computer interaction; multimedia data; pattern recommendation; XAI,Convolution; Deep learning; Graphics processing unit; Human computer interaction; Learning systems; Recommender systems; User interfaces; Convolution neural network; Deep learning; Hashtags; Hybrid framework; Learning models; Learning process; Multimedia data; Network-based; Pattern recommendation; XAI; Genetic algorithms
PAINT: Photo-realistic Fashion Design Synthesis,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176806036&doi=10.1145%2f3545610&partnerID=40&md5=97dad3d558a49fefc55db3d8d7e16d6c,"In this article, we investigate a new problem of generating a variety of multi-view fashion designs conditioned on a human pose and texture examples of arbitrary sizes, which can replace the repetitive and low-level design work for fashion designers. To solve this challenging multi-modal image translation problem, we propose a novel Photo-reAlistic fashIon desigN synThesis (PAINT) framework, which decomposes the framework into three manageable stages. In the first stage, we employ a Layout Generative Network (LGN) to transform an input human pose into a series of person semantic layouts. In the second stage, we propose a Texture Synthesis Network (TSN) to synthesize textures on all transformed semantic layouts. Specifically, we design a novel attentive texture transfer mechanism for precisely expanding texture patches to the irregular clothing regions of the target fashion designs. In the third stage, we leverage an Appearance Flow Network (AFN) to generate the fashion design images of other viewpoints from a single-view observation by learning 2D multi-scale appearance flow fields. Experimental results demonstrate that our method is capable of generating diverse photo-realistic multi-view fashion design images with fine-grained appearance details conditioned on the provided multiple inputs. The source code and trained models are available at https://github.com/gxl-groups/PAINT.  © 2023 Association for Computing Machinery.",AI-assisted fashion design; fashion image synthesis; Generative adversarial network,Generative adversarial networks; Paint; Semantics; AI-assisted fashion design; Design synthesis; Design work; Fashion design; Fashion image synthesis; Human pose; Images synthesis; Low-level designs; Multi-views; Photo-realistic; Textures
A Deep Graph Network with Multiple Similarity for User Clustering in Human-Computer Interaction,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176808962&doi=10.1145%2f3549954&partnerID=40&md5=21a8c788a69d97abba5b0fab193ef8f7,"User counterparts, such as user attributes in social networks or user interests, are the keys to more natural Human-Computer Interaction (HCI). In addition, users' attributes and social structures help us understand the complex interactions in HCI. Most previous studies have been based on supervised learning to improve the performance of HCI. However, in the real world, owing to signal malfunctions in user devices, large amounts of abnormal information, unlabeled data, and unsupervised approaches (e.g., the clustering method) based on mining user attributes are particularly crucial. This paper focuses on improving the clustering performance of users' attributes in HCI and proposes a deep graph embedding network with feature and structure similarity (called DGENFS) to cluster users' attributes in HCI applications based on feature and structure similarity. The DGENFS model consists of a Feature Graph Autoencoder (FGA) module, a Structure Graph Attention Network (SGAT) module, and a Dual Self-supervision (DSS) module. First, we design an attributed graph clustering method to divide users into clusters by making full use of their attributes. To take full advantage of the information of human feature space, a k-neighbor graph is generated as a feature graph based on the similarity between human features. Then, the FGA and SGAT modules are utilized to extract the representations of human features and topological space, respectively. Next, an attention mechanism is further developed to learn the importance weights of different representations to effectively integrate human features and social structures. Finally, to learn cluster-friendly features, the DSS module unifies and integrates the features learned from the FGA and SGAT modules. DSS explores the high-confidence cluster assignment as a soft label to guide the optimization of the entire network. Extensive experiments are conducted on five real-world data sets on user attribute clustering. The experimental results demonstrate that the proposed DGENFS model achieves the most advanced performance compared with nine competitive baselines.  © 2023 Association for Computing Machinery.",Attributed graph clustering; cluster-friendly features; deep graph embedding; self-supervision module,Cluster analysis; Graph embeddings; Human computer interaction; Nearest neighbor search; Network embeddings; Attributed graph clustering; Auto encoders; Cluster-friendly feature; Clustering methods; Deep graph embedding; Graph embeddings; Performance; Real-world; Self-supervision module; Social structure; Graphic methods
Special Issue on Deep Learning for Intelligent Human Computer Interaction,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176783611&doi=10.1145%2f3605151&partnerID=40&md5=511118d2dc59d092b6eede357b15caf9,[No abstract available],,
A Study of Human-AI Symbiosis for Creative Work: Recent Developments and Future Directions in Deep Learning,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149729136&doi=10.1145%2f3542698&partnerID=40&md5=6062b4587d0905fa63ea555df76560ef,"Recent advances in Artificial Intelligence (AI), particularly deep learning, are having an enormous impact on our society today. Record numbers of jobs previously held by people have been automated, from manufacturing to transportation to customer services. The concerns of AI replacing humans by taking over people's jobs need to be urgently addressed. This article investigates some promising different directions of AI development: Instead of using AI to replace people, we should use AI to team up with people so that both can work better and smarter. Human-AI symbiosis refers to people and AI working together to jointly solve problems and perform specific tasks. The recent developments in deep learning models and frameworks have significantly improved the efficiency and performance of human and AI collaborations. In this article, some research work on human-AI collaborative environments has been extensively studied and analyzed to reveal the progress in this field. Although the teaming of humans and machines includes many complex tasks, the development has been very promising. One of the main goals in this field is to develop additional capabilities in machines capable of being successful teammates with a human partner. The correctness of the outcomes is often determined by the underlying technology and how performance and human satisfaction are measured through the collaborative nature of the system. We conclude that the teaming of humans and AI, particularly deep learning, has the advantage of combining the power of AI with the human domain expertise to improve performance and create value. Human-AI symbiosis could be a promising future direction for AI's continuing integration into the world. © 2023 Association for Computing Machinery.",artificial intelligence; collaborative concept development; Human-AI collaboration; human-AI teaming,Collaborative concept development; Concept development; Creative work; Customer-service; Efficiency and performance; Human-artificial intelligence collaboration; Human-artificial intelligence teaming; Learning frameworks; Learning models; Specific tasks; Deep learning
Explanation-Driven HCI Model to Examine the Mini-Mental State for Alzheimer's Disease,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176787251&doi=10.1145%2f3527174&partnerID=40&md5=86d639457ae759859db367a79a70fae2,"Directing research on Alzheimer's disease toward only early prediction and accuracy cannot be considered a feasible approach toward tackling a ubiquitous degenerative disease today. Applying deep learning (DL), Explainable artificial intelligence, and advancing toward the human-computer interface (HCI) model can be a leap forward in medical research. This research aims to propose a robust explainable HCI model using SHAPley additive explanation, local interpretable model-agnostic explanations, and DL algorithms. The use of DL algorithms - logistic regression (80.87%), support vector machine (85.8%), k-nearest neighbor (87.24%), multilayer perceptron (91.94%), and decision tree (100%) - and explainability can help in exploring untapped avenues for research in medical sciences that can mold the future of HCI models. The presented model's results show improved prediction accuracy by incorporating a user-friendly computer interface into decision-making, implying a high significance level in the context of biomedical and clinical research.  © 2023 Association for Computing Machinery.",Alzheimer's prediction; deep learning; Explainable AI; Human computer interface; LIME; machine learning; SHAP,Clinical research; Decision trees; Forecasting; Learning systems; Long short-term memory; Nearest neighbor search; Neurodegenerative diseases; Support vector machines; Alzheimer; Alzheimer prediction; Alzheimers disease; Deep learning; Explainable AI; Human computer interfaces; Interface modeling; Machine-learning; Mental state; SHAP; Lime
Hierarchical Multi-Attention Transfer for Knowledge Distillation,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176762097&doi=10.1145%2f3568679&partnerID=40&md5=e1ccfbcff33cdb5aa06c854db261e5b5,"Knowledge distillation (KD) is a powerful and widely applicable technique for the compression of deep learning models. The main idea of knowledge distillation is to transfer knowledge from a large teacher model to a small student model, where the attention mechanism has been intensively explored in regard to its great flexibility for managing different teacher-student architectures. However, existing attention-based methods usually transfer similar attention knowledge from the intermediate layers of deep neural networks, leaving the hierarchical structure of deep representation learning poorly investigated for knowledge distillation. In this paper, we propose a hierarchical multi-attention transfer framework (HMAT), where different types of attention are utilized to transfer the knowledge at different levels of deep representation learning for knowledge distillation. Specifically, position-based and channel-based attention knowledge characterize the knowledge from low-level and high-level feature representations, respectively, and activation-based attention knowledge characterize the knowledge from both mid-level and high-level feature representations. Extensive experiments on three popular visual recognition tasks, image classification, image retrieval, and object detection, demonstrate that the proposed hierarchical multi-attention transfer or HMAT significantly outperforms recent state-of-the-art KD methods.  © 2023 Association for Computing Machinery.",hierarchical attention transfer; knowledge distillation; Model compression,Deep neural networks; Image representation; Image retrieval; Multilayer neural networks; Object detection; Attention mechanisms; Feature representation; Hierarchical attention transfer; High-level features; Knowledge distillation; Learning models; Model compression; Student Modeling; Teacher models; Teachers'; Distillation
Spontaneous Facial Behavior Analysis Using Deep Transformer-based Framework for Child-computer Interaction,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176753141&doi=10.1145%2f3539577&partnerID=40&md5=b6278286bce9446c435a504405f92d53,"A fascinating challenge in robotics-human interaction is imitating the emotion recognition capability of humans to robots with the aim to make human-robotics interaction natural, genuine and intuitive. To achieve the natural interaction in affective robots, human-machine interfaces, and autonomous vehicles, understanding our attitudes and opinions is very important, and it provides a practical and feasible path to realize the connection between machine and human. Multimodal interface that includes voice along with facial expression can manifest a large range of nuanced emotions compared to purely textual interfaces and provide a great value to improve the intelligence level of effective communication. Interfaces that fail to manifest or ignore user emotions may significantly impact the performance and risk being perceived as cold, socially inept, untrustworthy, and incompetent. To equip a child well for life, we need to help our children identify their feelings, manage them well, and express their needs in healthy, respectful, and direct ways. Early identification of emotional deficits can help to prevent low social functioning in children. In this work, we analyzed the child's spontaneous behavior using multimodal facial expression and voice signal presenting multimodal transformer-based last feature fusion for facial behavior analysis in children to extract contextualized representations from RGB video sequence and Hematoxylin and eosin video sequence and then using these representations followed by pairwise concatenations of contextualized representations using cross-feature fusion technique to predict users emotions. To validate the performance of the proposed framework, we have performed experiments with the different pairwise concatenations of contextualized representations that showed significantly better performance than state-of-the-art method. Besides, we perform t-distributed stochastic neighbor embedding visualization to visualize the discriminative feature in lower dimension space and probability density estimation to visualize the prediction capability of our proposed model. © 2023 Association for Computing Machinery.",gaze detection; neural networks; PhrasesDatasets; text tagging,Behavioral research; Human robot interaction; Modal analysis; Stochastic models; Stochastic systems; User interfaces; Video recording; Behavior analysis; Facial Expressions; Features fusions; Gaze detection; Multi-modal; Neural-networks; Performance; Phrasesdataset; Text tagging; User emotions; Emotion Recognition
Tensor-Empowered LSTM for Communication-Efficient and Privacy-Enhanced Cognitive Federated Learning in Intelligent Transportation Systems,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176790290&doi=10.1145%2f3575661&partnerID=40&md5=d6c02f15c3ad3b750adad998c01c914b,"Multimedia cognitive computing as a revolutionary emerging concept of artificial intelligence emulating the reasoning process like human brains can facilitate the evolution of intelligent transportation systems (ITS) to be smarter, safer, and more efficient. Massive multimedia traffic big data is an important prerequisite for the success of cognitive computing in ITS. However, traditional data-centralized artificial intelligence approaches often face the problems of data islands and data famine due to concerns about data privacy and security. To this end, we propose the concept of cognitive federated learning leveraging federated learning as the learning paradigm for cognitive computing, which solves the preceding concerns by sharing updated models rather than raw data. Nevertheless, the exchange of numerous model parameters not only generates significant communication overhead but also suffers from the risk of privacy leakage due to inference attacks. This article aims to design a novel lightweight and privacy-enhanced cognitive federated learning architecture to facilitate the development of ITS. First, a privacy-enhanced model protection scheme with homomorphic encryption as the underlying technology is proposed to simultaneously defend against the inference attacks launched by external malicious attackers, honest-but-curious cognitive platforms, and internal participants. Furthermore, a novel tensor ring-block decomposition and its corresponding deep computation model converting the weight tensor into a set of matrices and third-order core tensors are proposed, which could reduce the communication overhead and storage requirements without compromising model performance. Experimental results on real-world datasets show that the proposed approach performs well.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",communication efficiency; Federated learning; homomorphic encryption; tensor ring-block decomposition,Brain; Cognitive systems; Cryptography; Data privacy; Digital storage; Food supply; Intelligent systems; Intelligent vehicle highway systems; Long short-term memory; Tensors; Block decomposition; Cognitive Computing; Communication efficiency; Communication overheads; Federated learning; Ho-momorphic encryptions; Homomorphic-encryptions; Inference attacks; Intelligent transportation systems; Tensor ring-block decomposition; Learning systems
HCMS: Hierarchical and Conditional Modality Selection for Efficient Video Recognition,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176423709&doi=10.1145%2f3572776&partnerID=40&md5=9198f7f0d5ca9f32796b0e71d0bfdb9a,"Videos are multimodal in nature. Conventional video recognition pipelines typically fuse multimodal features for improved performance. However, this is not only computationally expensive but also neglects the fact that different videos rely on different modalities for predictions. This article introduces Hierarchical and Conditional Modality Selection (HCMS), a simple yet efficient multimodal learning framework for efficient video recognition. HCMS operates on a low-cost modality, i.e., audio clues, by default, and dynamically decides on-the-fly whether to use computationally expensive modalities, including appearance and motion clues, on a per-input basis. This is achieved by the collaboration of three LSTMs that are organized in a hierarchical manner. In particular, LSTMs that operate on high-cost modalities contain a gating module, which takes as inputs lower-level features and historical information to adaptively determine whether to activate its corresponding modality; otherwise, it simply reuses historical information. We conduct extensive experiments on two large-scale video benchmarks, FCVID and ActivityNet, and the results demonstrate the proposed approach can effectively explore multimodal information for improved classification performance while requiring much less computation. © 2023 Association for Computing Machinery.",efficient inference; Multimodal analysis; video recognition,Benchmarking; Costs; Modal analysis; Efficient inference; Historical information; Learning frameworks; Multi-modal; Multi-modal learning; Multimodal analysis; Multimodal features; Performance; Simple++; Video recognition; Classification (of information)
Unsupervised Domain Adaptation by Causal Learning for Biometric Signal-based HCI,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176403018&doi=10.1145%2f3583885&partnerID=40&md5=c8ddebc6cd44eee8bd28296471670863,"Biometric signal based human-computer interface (HCI) has attracted increasing attention due to its wide application in healthcare, entertainment, neurocomputing, and so on. In recent years, deep learning-based approaches have made great progress on biometric signal processing. However, the state-of-the-art (SOTA) approaches still suffer from model degradation across subjects or sessions. In this work, we propose a novel unsupervised domain adaptation approach for biometric signal-based HCI via causal representation learning. Specifically, three kinds of interventions on biometric signals (i.e., subjects, sessions, and trials) can be selected to generalize deep models across the selected intervention. In the proposed approach, a generative model is trained for producing intervened features that are subsequently used for learning transferable and causal relations with three modes. Experiments on the EEG-based emotion recognition task and sEMG-based gesture recognition task are conducted to confirm the superiority of our approach. An improvement of +0.21% on the task of inter-subject EEG-based emotion recognition is achieved using our approach. Besides, on the task of inter-session sEMG-based gesture recognition, our approach achieves improvements of +1.47%, +3.36%, +1.71%, and +1.01% on sEMG datasets including CSL-HDEMG, CapgMyo DB-b, 3DC, and Ninapro DB6, respectively. The proposed approach also works on the task of inter-trial sEMG-based gesture recognition and an average improvement of +0.66% on Ninapro databases is achieved. These experimental results show the superiority of the proposed approach compared with the SOTA unsupervised domain adaptation methods on HCIs based on biometric signal. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",causal learning; Deep learning; domain adaptation; human-computer interface,Biomedical signal processing; Biometrics; Deep learning; Gesture recognition; Human computer interaction; Speech recognition; Biometric signal processing; Causal learning; Deep learning; Domain adaptation; Emotion recognition; Gestures recognition; Human computer interfaces; Learning-based approach; Neurocomputing; State-of-the-art approach; Emotion Recognition
Self-Supervised Learning of Depth and Ego-Motion for 3D Perception in Human Computer Interaction,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176752588&doi=10.1145%2f3588571&partnerID=40&md5=d143b527f6a2689f645eaff63c86079e,"3D perception of depth and ego-motion is of vital importance in intelligent agent and Human Computer Interaction (HCI) tasks, such as robotics and autonomous driving. There are different kinds of sensors that can directly obtain 3D depth information. However, the commonly used Lidar sensor is expensive, and the effective range of RGB-D cameras is limited. In the field of computer vision, researchers have done a lot of work on 3D perception. While traditional geometric algorithms require a lot of manual features for depth estimation, Deep Learning methods have achieved great success in this field. In this work, we proposed a novel self-supervised method based on Vision Transformer (ViT) with Convolutional Neural Network (CNN) architecture, which is referred to as ViT-Depth. The image reconstruction losses computed by the estimated depth and motion between adjacent frames are treated as supervision signal to establish a self-supervised learning pipeline. This is an effective solution for tasks that need accurate and low-cost 3D perception, such as autonomous driving, robotic navigation, 3D reconstruction, and so on. Our method could leverage both the ability of CNN and Transformer to extract deep features and capture global contextual information. In addition, we propose a cross-frame loss that could constrain photometric error and scale consistency among multi-frames, which lead the training process to be more stable and improve the performance. Extensive experimental results on autonomous driving dataset demonstrate the proposed approach is competitive with the state-of-the-art depth and motion estimation methods. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",3D perception; monocular depth and motion estimation; PhrasesAutonomous driving; Self-supervised Learning; visual SLAM,Autonomous vehicles; Convolutional neural networks; Human computer interaction; Image reconstruction; Motion estimation; Optical radar; Supervised learning; 3D perception; Autonomous driving; Convolutional neural network; Depth information; Ego-motion; LIDAR sensors; Monocular depth and motion estimation; Phrasesautonomous driving; Self-supervised learning; Visual SLAM; Deep learning
Realtime Recognition of Dynamic Hand Gestures in Practical Applications,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176779920&doi=10.1145%2f3561822&partnerID=40&md5=1d54496453137cddeee10536fa14bd89,"Dynamic hand gesture acting as a semaphoric gesture is a practical and intuitive mid-air gesture interface. Nowadays benefiting from the development of deep convolutional networks, the gesture recognition has already achieved a high accuracy, however, when performing a dynamic hand gesture such as gestures of direction commands, some unintentional actions are easily misrecognized due to the similarity of the hand poses. This hinders the application of dynamic hand gestures and cannot be solved by just improving the accuracy of the applied algorithm on public datasets, thus it is necessary to study such problems from the perspective of human-computer interaction. In this article, two methods are proposed to avoid misrecognition by introducing activation delay and using asymmetric gesture design. First the temporal process of a dynamic hand gesture is decomposed and redefined, then a realtime dynamic hand gesture recognition system is built through a two-dimensional convolutional neural network. In order to investigate the influence of activation delay and asymmetric gesture design on system performance, a user study is conducted and experimental results show that the two proposed methods can effectively avoid misrecognition. The two methods proposed in this article can provide valuable guidance for researchers when designing realtime recognition system in practical applications.  © 2023 Association for Computing Machinery.",activation delay; asymmetric gesture design; convolutional neural network; Dynamic gesture recognition; human-computer interaction,Chemical activation; Convolution; Convolutional neural networks; Gesture recognition; Palmprint recognition; Activation delay; Asymmetric gesture design; Convolutional networks; Convolutional neural network; Dynamic gesture recognition; Gesture interfaces; Gestures recognition; Hand gesture; High-accuracy; Real time recognition; Human computer interaction
Full-body Human Motion Reconstruction with Sparse Joint Tracking Using Flexible Sensors,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176729482&doi=10.1145%2f3564700&partnerID=40&md5=43e77302f4874d925eb43bcb34bea0ba,"Human motion tracking is a fundamental building block for various applications including computer animation, human-computer interaction, healthcare, and so on. To reduce the burden of wearing multiple sensors, human motion prediction from sparse sensor inputs has become a hot topic in human motion tracking. However, such predictions are non-trivial as (i) the widely adopted data-driven approaches can easily collapse to average poses, and (ii) the predicted motions contain unnatural jitters. In this work, we address the aforementioned issues by proposing a novel framework which can accurately predict the human joint moving angles from the signals of only four flexible sensors, thereby achieving the tracking of human joints in multi-degrees of freedom. Specifically, we mitigate the collapse to average poses by implementing the model with a Bi-LSTM neural network that makes full use of short-time sequence information; we reduce jitters by adding a median pooling layer to the network, which smooths consecutive motions. Although being bio-compatible and ideal for improving the wearing experience, the flexible sensors are prone to aging which increases prediction errors. Observing that the aging of flexible sensors usually results in drifts of their resistance ranges, we further propose a novel dynamic calibration technique to rescale sensor ranges, which further improves the prediction accuracy. Experimental results show that our method achieves a low and stable tracking error of 4.51 degrees across different motion types with only four sensors. © 2023 Association for Computing Machinery.",median pooling; PhrasesFlexible sensors; sparse signal processing; temporal convolutional network,Forecasting; Human computer interaction; Jitter; Long short-term memory; Medical computing; Motion estimation; Network layers; Wear of materials; Convolutional networks; Flexible sensor; Full body; Human joints; Human motion reconstruction; Human motion tracking; Median pooling; Phrasesflexible sensor; Sparse signal processing; Temporal convolutional network; Degrees of freedom (mechanics)
Robust Searching-Based Gradient Collaborative Management in Intelligent Transportation System,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176383819&doi=10.1145%2f3549939&partnerID=40&md5=d1d8dcea6b81b8ec2f49618ee99d7945,"With the rapid development of big data and the Internet of Things (IoT), traffic data from an Intelligent Transportation System (ITS) is becoming more and more accessible. To understand and simulate the traffic patterns from the traffic data, Multimedia Cognitive Computing (MCC) is an efficient and practical approach. Distributed Machine Learning (DML) has been the trend to provide sufficient computing resources and efficiency for MCC tasks to handle massive data and complex models. DML can speed up computation with those computing resources but introduces communication overhead. Gradient collaborative management or gradient aggregation in DML for MCC tasks is a critical task. An efficient managing algorithm of the communication schedules for gradient aggregation in ITS can improve the performance of MCC tasks. However, existing communication schedules typically rely on specific physical connection matrices, which have low robustness when a malfunction occurs. In this article, we propose Robust Searching-based Gradient Collaborative Management (RSGCM) in Intelligent Transportation System, a practical ring-based gradient managing algorithm for communication schedules across devices to deal with ITS malfunction. RSGCM provides solutions of communication schedules to various kinds of connection matrices with an acceptable amount of training time. Our experimental results have shown that RSGCM can deal with more varieties of connection matrices than existing state-of-the-art communication schedules. RSGCM also increases the robustness of ITS since it can restore the system's functionality in an acceptable time when device or connection breakdown happens. © 2023 Association for Computing Machinery.",All-reduce; collaborative management; communication scheduling; gradient aggregation; robustness,Intelligent systems; Intelligent vehicle highway systems; Internet of things; All-reduce; Cognitive Computing; Collaborative management; Communication scheduling; Computing-task; Connection matrix; Distributed machine learning; Gradient aggregation; Intelligent transportation systems; Robustness; Matrix algebra
Semi-supervised Video Object Segmentation Via an Edge Attention Gated Graph Convolutional Network,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173281720&doi=10.1145%2f3611389&partnerID=40&md5=e4b6933746008a07087522ba30992145,"Video object segmentation (VOS) exhibits heavy occlusions, large deformation, and severe motion blur. While many remarkable convolutional neural networks are devoted to the VOS task, they often mis-identify background noise as the target or output coarse object boundaries, due to the failure of mining detail information and high-order correlations of pixels within the whole video. In this work, we propose an edge attention gated graph convolutional network (GCN) for VOS. The seed point initialization and graph construction stages construct a spatio-temporal graph of the video by exploring the spatial intra-frame correlation and the temporal inter-frame correlation of superpixels. The node classification stage identifies foreground superpixels by using an edge attention gated GCN which mines higher-order correlations between superpixels and propagates features among different nodes. The segmentation optimization stage optimizes the classification of foreground superpixels and reduces segmentation errors by using a global appearance model which captures the long-term stable feature of objects. In summary, the key contribution of our framework is twofold: (a) the spatio-temporal graph representation can propagate the seed points of the first frame to subsequent frames and facilitate our framework for the semi-supervised VOS task; and (b) the edge attention gated GCN can learn the importance of each node with respect to both the neighboring nodes and the whole task with a small number of layers. Experiments on Davis 2016 and Davis 2017 datasets show that our framework achieves the excellent performance with only small training samples (45 video sequences). © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",graph convolutional network; semi-supervised video object segmentation; spatio-temporal graph model; superpixel,Convolution; Convolutional neural networks; Graph theory; Image segmentation; Motion compensation; Convolutional networks; Graph convolutional network; Graph model; Higher order correlation; Semi-supervised; Semi-supervised video object segmentation; Spatio-temporal graph model; Spatio-temporal graphs; Super pixels; Video objects segmentations; Superpixels
Self-Supervised Consistency Based on Joint Learning for Unsupervised Person Re-identification,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173246556&doi=10.1145%2f3612926&partnerID=40&md5=ddb7b931fc997cee10c027fc9f6c0646,"Recently, unsupervised domain adaptive person re-identification (Re-ID) methods have been extensively studied thanks to not requiring annotations, and they have achieved excellent performance. Most of the existing methods aim to train the Re-ID model for learning a discriminative feature representation. However, they usually only consider training the model to learn a global feature of a pedestrian image, but neglecting the local feature, which restricts further improvement of model performance. To address this problem, two local branches are added to the networks, aiming to allow the model to focus on the local feature containing identity information. Furthermore, we propose a self-supervised consistency constraint to further improve robustness of the model. Specifically, the self-supervised consistency constraint uses the basic data augmentation operations without other auxiliary networks, which can improve performance of the model effectively. Then, a learnable memory matrix is designed to store the mapping vectors that maps person features into probability distributions. Finally, extensive experiments are conducted on multiple commonly used person Re-ID datasets to verify the effectiveness of the proposed generative adversarial networks fusing global and local features. Experimental results reveal that our method achieves results comparable to state-of-the-art methods.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",joint learning; Person re-identification; self-supervised; unsupervised domain adaptive,Generative adversarial networks; Image enhancement; Consistency constraints; Global feature; Identification method; Joint learning; Local feature; Performance; Person re identifications; Re identifications; Self-supervised; Unsupervised domain adaptive; Probability distributions
On Content-Aware Post-Processing: Adapting Statistically Learned Models to Dynamic Content,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173245101&doi=10.1145%2f3612925&partnerID=40&md5=8879dfc18a7c3cdc023ace4e4af9942f,"Learning-based post-processing methods generally produce neural models that are statistically optimal on their training datasets. These models, however, neglect intrinsic variations of local video content and may fail to process unseen content. To address this issue, this article proposes a content-aware approach for the post-processing of compressed videos. We develop a backbone network, called BackboneFormer, where a Fast Transformer using Separable Self-Attention, Spatial Attention, and Channel Attention is devised to support underlying feature embedding and aggregation. Furthermore, we introduce Meta-learning to strengthen BackboneFormer for better performance. Specifically, we propose Meta Post-Processing (Meta-PP) which leverages the Meta-learning framework to drive BackboneFormer to capture and analyze input video variations for spontaneous updating. Since the original frame is unavailable to the decoder, we devise a Compression Degradation Estimation model where a low-complexity neural model and classic operators are used collaboratively to estimate the compression distortion. The estimated distortion is then utilized to guide the BackboneFormer model for dynamic updating of weighting parameters. Experimental results demonstrate that the proposed BackboneFormer itself gains about 3.61% Bjøntegaard delta bit-rate reduction over Versatile Video Coding in the post-processing task and ""BackboneFormer + Meta-PP""attains 4.32%, costing only 50K and 61K parameters, respectively. The computational complexity of MACs is 49k/pixel and 50k/pixel, which represents only about 16% of state-of-the-art methods having similar coding gains.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",in-loop filtering; Meta-learning; post-processing; transformer; VVC,Complex networks; Image coding; Learning systems; Content-aware; Dynamic content; In-loop filtering; Loop filtering; Metalearning; Neural modelling; Post-processing; Postprocessing methods; Transformer; VVC; Video signal processing
S2CL-Leaf Net: Recognizing Leaf Images Like Human Botanists,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173584627&doi=10.1145%2f3615659&partnerID=40&md5=b71b9903bedee5d8eb6c536dbf3615f6,"Automatically classifying plant leaves is a challenging fine-grained classification task because of the diversity in leaf morphology, including size, texture, shape, and venation. Although powerful deep learning-based methods have achieved great improvement in leaf classification, these methods still require a large number of well-labeled samples for supervised training, which is difficult to get. In contrast, relying on the specific coarse-to-fine classification strategy, human botanists only require a small number of samples for accurate leaf recognition. Inspired by the classification strategy of human botanists, we propose a novel S2CL-Leaf Net, which exploits multi-granularity clues with a hierarchical attention mechanism and boosts the learning ability with the supervised sampling contrastive learning with limited training samples to classify plant leaves as human botanists do. Specifically, to fully explore and exploit the subtle details of the leaves, a novel sampling transformation mechanism is combined with the supervised contrastive learning to enhance the network's perception of details by amplifying the discriminative regions with a weighted sampling of different regions. Furthermore, we construct the hierarchical attention mechanism to produce attention maps of different granularity, which helps to discover details in leaves that are important for classification. Experiments are conducted on the open-access leaf datasets, including Flavia, Swedish, and LeafSnap, which prove the effectiveness of the proposed S2CL-Leaf Net. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",few-shot learning; Fine-grained image classification; leaf recognition,Deep learning; Plants (botany); Textures; Attention mechanisms; Classification tasks; Few-shot learning; Fine grained; Fine-grained image classification; Images classification; Leaf images; Leaf morphology; Leaf recognition; Plant leaves; Image classification
Enhancing Adversarial Embedding based Image Steganography via Clustering Modification Directions,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173249455&doi=10.1145%2f3603377&partnerID=40&md5=cf3992b33af39ddf2db9c744a18d03e6,"Image steganography is a technique used to conceal secret information within cover images without being detected. However, the advent of convolutional neural networks (CNNs) has threatened the security of image steganography. Due to the inherent properties of adversarial examples, adding perturbations to stego images can mislead the CNN-based image steganalysis, but it also easily leads to some errors when extracting secret information. Recently, some adversarial embedding methods have been proposed for improving image steganography security. In this work, we aim at furthering enhance the security of adversarial embedding-based image steganography by exploiting the strong correlation between adjacent pixels. Specifically, we divide the cover image into four non-overlapping parts for four-stage information embedding. During the adversarial embedding process, we cluster the modification directions of adjacent pixels and select only those with relatively larger amplitudes of gradients and smaller embedding costs to update their original embedding costs. Experimental results demonstrate that our proposed method can effectively fool targeted steganalyzers and outperform state-of-the-art techniques under different scenarios.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adversarial embedding; convolutional neural network; Image steganography; steganalysis,Convolution; Convolutional neural networks; Image enhancement; Pixels; Steganography; Adjacent pixels; Adversarial embedding; Clusterings; Convolutional neural network; Cover-image; Embeddings; Image steganography; Property; Secret information; Steganalysis; Embeddings
Collocated Clothing Synthesis with GANs Aided by Textual Information: A Multi-Modal Framework,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173236533&doi=10.1145%2f3614097&partnerID=40&md5=6c2948872289a033c316acc6935dac4c,"Synthesizing realistic images of fashion items which are compatible with given clothing images, as well as conditioning on multiple modalities, brings novel and exciting applications together with enormous economic potential. In this work, we propose a multi-modal collocation framework based on generative adversarial network (GAN) for synthesizing compatible clothing images. Given an input clothing item that consists of an image and a text description, our model works on synthesizing a clothing image which is compatible with the input clothing, as well as being guided by a given text description from the target domain. Specifically, a generator aims to synthesize realistic and collocated clothing images relying on image- and text-based latent representations learned from the source domain. An auxiliary text representation from the target domain is added for supervising the generation results. In addition, a multi-discriminator framework is carried out to determine compatibility between the generated clothing images and the input clothing images, as well as visual-semantic matching between the generated clothing images and the targeted textual information. Extensive quantitative and qualitative results demonstrate that our model substantially outperforms state-of-the-art methods in terms of authenticity, diversity, and visual-semantic similarity between image and text.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",clothes collocation; fashion data; generative adversarial networks; image translation; Multi-modal,Image processing; Semantics; Clothes collocation; Economic potentials; Fashion data; Image translation; Multi-modal; Multiple modalities; Realistic images; Target domain; Textual information; Visual semantics; Generative adversarial networks
Usefulness of QoS in Multidimensional QoE Prediction for Haptic-Audiovisual Communications,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173282869&doi=10.1145%2f3613246&partnerID=40&md5=628e27160a5c240eed38f48778580702,"This article investigates prediction of Quality of Experience (QoE) by comparing borrowing-from-neighbor situations and isolated ones. We demonstrate that joint utilization of multiple QoE measures enhances the accuracy of QoE prediction compared to that by a collection of individual QoE measures each regressed on Quality of Service (QoS) parameters, while the accuracy improvement with additional usage of QoS information in the former is limited. As an example of system that needs multidimensional QoE representation, the article gives haptic audiovisual interactive communications. We employ QoE and QoS data taken previously in an experiment, where 13 QoE measures (a five-point score each) and 12 QoS parameters (nonnegative continuous values each) are available at three average rates of load traffic. We build two kinds of Bayesian models for QoE prediction; one is a logistic regression model of a single QoE measure as the response variable and QoS parameters as predictors, which is a typical traditional method of discrete QoE prediction and isolated in a sense. The other is a structural equation model (SEM) with latent constructs (i.e., factors) of audiovisual quality, haptic quality, and user experience quality; the original SEM, which contains only QoE indicators of three constructs (audiovisual quality (AVQ), haptic quality (HQ), and user experience quality (UXQ)), was proposed in one of the author's previous studies. This article extends the SEM to accommodate QoS parameters. We develop two kinds of new SEMs with QoS parameters: One has three extended constructs referred to as eAVQ, eHQ, and eUXQ, each of which has both QoE and QoS indicators, and the other has separate constructs for QoE and QoS, which lead to totally six constructs (AVqoe, Hqoe, UXqoe, AVqos, Hqos, and UXqos). We performed Markov chain Monte Carlo simulation of the Bayesian models with the JAGS software in an R environment. For comparison of QoE prediction accuracy, we adopt the 10-fold cross validation method and in part widely applicable information criterion. We then found that the three-construct models outperform the logistic regression models with respect to all subjective QoE measures and that the two kinds of the models are comparable as for the objective measure. The six-construct model exhibits almost the same accuracy as that of the three-construct one unless the number of QoE measures (nqoe) in the model is small. When the number nqoe is small, single-construct models may be a better choice. We have thus learned that multiple QoE measures should be utilized jointly (i.e., borrowing from neighbor) in QoE prediction rather than resorting to QoS information only.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Bayesian modeling; construct; haptic-audiovisual interactive communications; JAGS; latent variables; logistic regression; MCMC; MIMIC; multidimensional QoE; QoE prediction; Quality of Experience (QoE); Quality of Service (QoS); R; SEM,Bayesian networks; Computer software; Intelligent systems; Logistic regression; Monte Carlo methods; Quality of service; Bayesian modelling; Construct; Haptic-audiovisual interactive communication; Haptics; Interactive communications; JAGS; Latent variable; Logistics regressions; MCMC; MIMIC; Multidimensional quality of experience; Quality of experience; Quality of experience prediction; Quality of service; Quality-of-service; R; Structural equation models; Forecasting
Visual Security Index Combining CNN and Filter for Perceptually Encrypted Light Field Images,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173235731&doi=10.1145%2f3612924&partnerID=40&md5=d9e5954f25282def854d6e04f6c4961b,"Visual security index (VSI) represents a quantitative index for the visual security evaluation of perceptually encrypted images. Recently, the research on visual security of encrypted light field (LF) images faces two challenges. One is that the existing perceptually encrypted image databases are often too small, which is easy to cause overfitting in convolutional neural network (CNN). The other is that existing VSI models did not take a full account the intrinsic characteristics of the LF images and highly relied on handcrafted feature extraction. In this article, we construct a new database of perceptually encrypted LF images, called the PE-SLF, which is 2.6 times as big as the existing largest perceptual encrypted image database. Moreover, a novel visual security index (VSI) model is proposed by taking into full consideration the intrinsic spatial-angular characteristics of the LF images and the outstanding capabilities of CNN in feature extraction. First, we exploit CNN to detect the texture and structure features of encrypted sub-aperture images in the spatial domain. Second, we apply the Gabor filter to detect the Gabor feature over the epi-polar plane images in angular domain. Last, the spatial and angular similarity measurements are subsequently calculated for jointly yielding the final visual security score. Experimental results on the constructed PE-SLF demonstrate that the proposed VSI model is closer to the perception of HVS in visual security evaluation of encrypted LF images compared to other classical and state-of-the-art models.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",convolutional neural networks; epi-polar plane image; gabor filter; Light field image,Convolution; Convolutional neural networks; Cryptography; Database systems; Extraction; Feature extraction; Network security; Textures; Convolutional neural network; Encrypted images; Epi-polar plane image; Field images; Image database; Index models; Light field image; Light fields; Security evaluation; Security indices; Gabor filters
Enhancement of Information Carrying and Decoding for Visual Cryptography with Error Correction,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173586572&doi=10.1145%2f3612927&partnerID=40&md5=62e26b016c45604668171210aa17b4cf,"Recently, three visual cryptography schemes with t-error-correcting capability (VCSs-tEC) were introduced for preventing the shadows carrying additional information from being corrupted by noise interference. However, the concerns on VCS-tEC, such as the average amount of carrying information, decoding of information from shadows, and the demonstration way of a secret, should be considered and improved. In this article, two schemes, namely the (k, n) probabilistic VCS-tEC (PVCS-tEC) and the (2, n) deterministic VCS-tEC (DVCS-tEC), are proposed. The concept of probabilistic VCS is combined with the Bose-Chaudhuri-Hocquenghem code (BCH code) for designing the (k, n) PVCS-tEC. Furthermore, some constant-weight BCH codewords are adopted to build the (2, n)-DVCS-tEC. Comprehensive results and experiments are demonstrated to clarify the enhancement of information-carrying and decoding by the two proposed methods. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",carrying information; decoding; error correction; Secret sharing; visual cryptography,Cryptography; Decoding; Carrying information; Decoding; Deterministics; Error-correcting; Errors correction; Noise interference; Probabilistics; Secret-sharing; Visual cryptography; Visual cryptography schemes; Error correction
Double High-Order Correlation Preserved Robust Multi-View Ensemble Clustering,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173279390&doi=10.1145%2f3612923&partnerID=40&md5=2cf3967e595f0fae532fbc37aee6e85e,"Ensemble clustering (EC), utilizing multiple basic partitions (BPs) to yield a robust consensus clustering, has shown promising clustering performance. Nevertheless, most current algorithms suffer from two challenging hurdles: (1) a surge of EC-based methods only focus on pair-wise sample correlation while fully ignoring the high-order correlations of diverse views. (2) they deal directly with the co-association (CA) matrices generated from BPs, which are inevitably corrupted by noise and thus degrade the clustering performance. To address these issues, we propose a novel Double High-Order Correlation Preserved Robust Multi-View Ensemble Clustering (DC-RMEC) method, which preserves the high-order inter-view correlation and the high-order correlation of original data simultaneously. Specifically, DC-RMEC constructs a hypergraph from BPs to fuse high-level complementary information from different algorithms and incorporates multiple CA-based representations into a low-rank tensor to discover the high-order relevance underlying CA matrices, such that double high-order correlation of multi-view features could be dexterously uncovered. Moreover, a marginalized denoiser is invoked to gain robust view-specific CA matrices. Furthermore, we develop a unified framework to jointly optimize the representation tensor and the result matrix. An effective iterative optimization algorithm is designed to optimize our DC-RMEC model by resorting to the alternating direction method of multipliers. Extensive experiments on seven real-world multi-view datasets have demonstrated the superiority of DC-RMEC compared with several state-of-the-art multi-view ensemble clustering methods.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Ensemble clustering; high-order correlation; hypergraph learning; tensor representation,Cluster analysis; Clustering algorithms; Data mining; Iterative methods; Clusterings; Co-association matrix; Ensemble clustering; High order correlation; Higher order correlation; Hyper graph; Hypergraph learning; Multi-views; Performance; Tensor representation; Tensors
Diverse Image Captioning via Conditional Variational Autoencoder and Dual Contrastive Learning,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173581182&doi=10.1145%2f3614435&partnerID=40&md5=06a407ec368652cc10dfa04bedc3713c,"Diverse image captioning has achieved substantial progress in recent years. However, the discriminability of generative models and the limitation of cross entropy loss are generally overlooked in the traditional diverse image captioning models, which seriously hurts both the diversity and accuracy of image captioning. In this article, aiming to improve diversity and accuracy simultaneously, we propose a novel Conditional Variational Autoencoder (DCL-CVAE) framework for diverse image captioning by seamlessly integrating sequential variational autoencoder with contrastive learning. In the encoding stage, we first build conditional variational autoencoders to separately learn the sequential latent spaces for a pair of captions. Then, we introduce contrastive learning in the sequential latent spaces to enhance the discriminability of latent representations for both image-caption pairs and mismatched pairs. In the decoding stage, we leverage the captions sampled from the pre-trained Long Short-Term Memory (LSTM), LSTM decoder as the negative examples and perform contrastive learning with the greedily sampled positive examples, which can restrain the generation of common words and phrases induced by the cross entropy loss. By virtue of dual constrastive learning, DCL-CVAE is capable of encouraging the discriminability and facilitating the diversity, while promoting the accuracy of the generated captions. Extensive experiments are conducted on the challenging MSCOCO dataset, showing that our proposed methods can achieve a better balance between accuracy and diversity compared to the state-of-the-art diverse image captioning models.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",contrastive learning; Diverse image captioning; sequential latent space; variational autoencoder,Decoding; Entropy; Image enhancement; Learning systems; Auto encoders; Contrastive learning; Cross entropy; Discriminability; Diverse image captioning; Entropy loss; Generative model; Image captioning; Sequential latent space; Variational autoencoder; Long short-term memory
Dynamic Message Propagation Network for RGB-D and Video Salient Object Detection,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173225044&doi=10.1145%2f3597612&partnerID=40&md5=7a28444506155b3e267b40916e783371,"Exploiting long-range semantic contexts and geometric information is crucial to infer salient objects from RGB and depth features. However, existing methods mainly focus on excavating local features within fixed regions by continuously feeding forward networks. In this article, we introduce Dynamic Message Propagation (DMP) to dynamically learn context information within more flexible regions. We integrate DMP into a Siamese-based network to process the RGB image and depth map separately and design a multi-level feature fusion module to explore cross-level information between refined RGB and depth features. Extensive experiments show clear improvements of our method over 17 methods on six benchmark datasets for RGB-D salient object detection (SOD). Additionally, our method outperforms its competitors for the video SOD task. Code is available at https://github.com/chenbaian-cs/DMPNet.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cross-modal learning; depth feature propagation; dynamic message propagation; RGB-D salient object detection,Object recognition; Semantics; Context information; Cross-modal; Cross-modal learning; Depth feature propagation; Depth features; Dynamic message propagation; Message propagation; RGB-D salient object detection; Salient object detection; Semantic context; Object detection
Self-supervised Multi-view Learning via Auto-encoding 3D Transformations,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173237380&doi=10.1145%2f3597613&partnerID=40&md5=f2ee3078e569455d60933a4884b5f47f,"3D object representation learning is a fundamental challenge in computer vision to infer about the 3D world. Recent advances in deep learning have shown their efficiency in 3D object recognition, among which view-based methods have performed best so far. However, feature learning of multiple views in existing methods is mostly performed in a supervised fashion, which often requires a large amount of data labels with high costs. In contrast, self-supervised learning aims to learn multi-view feature representations without involving labeled data. To this end, we propose a novel self-supervised framework to learn Multi-View Transformation Equivariant Representations (MV-TER), exploring the equivariant transformations of a 3D object and its projected multiple views that we derive. Specifically, we perform a 3D transformation on a 3D object and obtain multiple views before and after the transformation via projection. Then, we train a representation encoding module to capture the intrinsic 3D object representation by decoding 3D transformation parameters from the fused feature representations of multiple views before and after the transformation. Experimental results demonstrate that the proposed MV-TER significantly outperforms the state-of-the-art view-based approaches in 3D object classification and retrieval tasks and show the generalization to real-world datasets. The code is available at https://github.com/gyshgx868/mvter.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",multi-view learning; Self-supervised learning; transformation equivariant representation,Classification (of information); Computer vision; Deep learning; Encoding (symbols); Learning systems; Object recognition; Signal encoding; 3D object representation; 3D transformations; Feature representation; Learn+; Multi-view learning; Multi-views; Multiple views; Self-supervised learning; Transformation equivariant representation; View transformations; Supervised learning
Cross-User Similarities in Viewing Behavior for 360° Video and Caching Implications,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163683443&doi=10.1145%2f3507917&partnerID=40&md5=d890f3ec2f1b9383a26ee3aa7d920561,"The demand and usage of 360° video services are expected to increase. However, despite these services being highly bandwidth intensive, not much is known about the potential value that basic bandwidth saving techniques such as server or edge-network on-demand caching (e.g., in a CDN) could have when used for delivery of such services. This problem is both important and complicated as client-side solutions have been developed that split the full 360° view into multiple tiles, and adapt the quality of the downloaded tiles based on the user’s expected viewing direction and bandwidth conditions. This article presents new trace-based analysis methods that incorporate users’ viewports (the area of the full 360° view the user actually sees), a first characterization of the cross-user similarities of the users’ viewports, and a trace-based analysis of the potential bandwidth savings that caching-based techniques may offer under different conditions. Our analysis takes into account differences in the time granularity over which viewport overlaps can be beneficial for resource saving techniques, compares and contrasts differences between video categories, and accounts for uncertainties in the network conditions and the prediction of the future viewing direction when prefetching. The results provide substantial insight into the conditions under which overlap can be considerable and caching effective, and inform the design of new caching system policies tailored for 360° video. © 2023 Association for Computing Machinery.",360° streaming; caching; tiled video caching; viewport overlap,Behavioral research; Trace analysis; Uncertainty analysis; Video on demand; Video streaming; 360° streaming; Bandwidth savings; Caching; Condition; Tiled video caching; Users similarities; Video caching; Video services; Viewing directions; Viewport overlap; Bandwidth
Transformer-Based Relational Inference Network for Complex Visual Relational Reasoning,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173273385&doi=10.1145%2f3605781&partnerID=40&md5=c81c49133fd839a3da0e7ba7eb5cc795,"Visual Relational Reasoning is the basis of many vision-and-language based tasks (e.g., visual question answering and referring expression comprehension). In this article, we regard the complex referring expression comprehension (c-REF) task as the reasoning basis, in which c-REF seeks to localise a target object in an image guided by a complex query. Such queries often contain complex logic and thus impose two critical challenges for reasoning: (i) Comprehending the complex queries is difficult since these queries usually refer to multiple objects and their relationships; (ii) Reasoning among multiple objects guided by the queries and then localising the target correctly are non-trivial. To address the above challenges, we propose a Transformer-based Relational Inference Network (Trans-RINet). Specifically, to comprehend the queries, we mimic the language-comprehending mechanism of humans, and devise a language decomposition module to decompose the queries into four types, i.e., basic attributes, absolute location, visual relationship and relative location. We further devise four modules to address the corresponding information. In each module, we consider the intra-(i.e., between the objects) and inter-modality relationships(i.e., between the queries and objects) to improve the reasoning ability. Moreover, we construct a relational graph to represent the objects and their relationships, and devise a multi-step reasoning method to progressively understand the complex logic. Since each type of the queries is closely related, we let each module interact with each other before making a decision. Extensive experiments on the CLEVR-Ref+, Ref-Reasoning, and CLEVR-CoGenT datasets demonstrate the superior reasoning performance of our Trans-RINet.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",complex referring expression comprehension; Gated Graph Neural Network; Visual Relational Reasoning,Complex networks; Computer circuits; Graph neural networks; Complex queries; Complex referring expression comprehension; Gated graph neural network; Graph neural networks; Inference network; Multiple objects; Referring expressions; Relational inferences; Relational reasoning; Visual relational reasoning; Visual languages
Double-Layer Search and Adaptive Pooling Fusion for Reference-Based Image Super-Resolution,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173215538&doi=10.1145%2f3604937&partnerID=40&md5=11d73a245aba8b5a39e49cd83a8ed2cc,"Reference-based image super-resolution (RefSR) aims to reconstruct high-resolution (HR) images from low-resolution (LR) images by introducing HR reference images. The key step of RefSR is to transfer reference features to LR features. However, existing methods still lack an efficient transfer mechanism, resulting in blurry details in the generated image. In this article, we propose a double-layer search module and an adaptive pooling fusion module group for reference-based image super-resolution, called DLASR. Based on the re-search strategy, the double-layer search module can produce an accurate index map and score map. These two maps are used to filter out accurate reference features, which greatly increases the efficiency of feature transfer in the later stage. Through two continuous feature-enhancement steps, the adaptive pooling fusion module group can transfer more valuable reference features to the corresponding LR features. In addition, a structure reconstruction module is proposed to recover the geometric information of the images, which further improves the visual quality of the generated image. We conduct comparative experiments on a variety of datasets, and the results prove that DLASR achieves significant improvements over other state-of-the-art methods, in terms of quantitative accuracy and qualitative visual effect. The code is available at https://github.com/clttyou/DLASR.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adaptive pooling fusion; double-layer search; Reference-based super-resolution; structure reconstruction,Image enhancement; Image fusion; Image reconstruction; Optical resolving power; Adaptive pooling fusion; Double layers; Double-layer search; Fusion modules; High-resolution images; Image super resolutions; Lower resolution; Reference-based super-resolution; Structure reconstruction; Superresolution; Maps
Zero-shot Scene Graph Generation via Triplet Calibration and Reduction,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173580891&doi=10.1145%2f3604284&partnerID=40&md5=e24d794eb927e5f8466970ffa37b296e,"Scene Graph Generation (SGG) plays a pivotal role in downstream vision-language tasks. Existing SGG methods typically suffer from poor compositional generalizations on unseen triplets. They are generally trained on incompletely annotated scene graphs that contain dominant triplets and tend to bias toward these seen triplets during inference. To address this issue, we propose a Triplet Calibration and Reduction (T-CAR) framework in this article. In our framework, a triplet calibration loss is first presented to regularize the representations of diverse triplets and to simultaneously excavate the unseen triplets in incompletely annotated training scene graphs. Moreover, the unseen space of scene graphs is usually several times larger than the seen space, since it contains a huge number of unrealistic compositions. Thus, we propose an unseen space reduction loss to shift the attention of excavation to reasonable unseen compositions to facilitate the model training. Finally, we propose a contextual encoder to improve the compositional generalizations of unseen triplets by explicitly modeling the relative spatial relations between subjects and objects. Extensive experiments show that our approach achieves consistent improvements for zero-shot SGG over state-of-the-art methods. The code is available at https://github.com/jkli1998/T-CAR. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",compositional zero-shot learning; Scene analysis and understanding; scene graph generation,Computer vision; Zero-shot learning; % reductions; Compositional zero-shot learning; Down-stream; Generalisation; Generation method; Graph generation; Scene analysis; Scene graph generation; Scene understanding; Scene-graphs; Calibration
Towards Food Image Retrieval via Generalization-Oriented Sampling and Loss Function Design,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168648024&doi=10.1145%2f3600095&partnerID=40&md5=4a957c599e2062f8e0a6e32aeeb58b12,"Food computing has increasingly received widespread attention in the multimedia field. As a basic task of food computing, food image retrieval has wide applications, that is, food image retrieval can help users to find the desired food from a large number of food images. Besides, the retrieved information can be applied to establish a richer database for the subsequent food content-related recommendation. Food image retrieval aims to achieve better performance on novel categories. Thus, it is worth studying to transfer the embedding ability from the training set to the unseen test set, that is, the generalization of the model. Food is influenced by various factors, such as culture and geography, leading to great differences between domains, such as Asian food and western food. Therefore, it is challenging to study the generalization of the model in food image retrieval. In this article, we improve the classical metric learning framework and propose a generalization-oriented sampling strategy, which boosts the generalization of the model by maximizing the intra-class distance from a proportion of positive pairs to avoid the excessive distance compression in the embedding space. Considering that the existing optimization process is in an opposite direction to our proposed sampling strategy, we further propose an adaptive gradient assignment policy named gradient-adaptive optimization, which can alleviate the intra-class distance compression during optimization by assigning different gradients to different samples. Extensive evaluation on three popular food image datasets demonstrates the effectiveness of the proposed method. We also experiment on three popular general datasets to prove that solving the problem from the generalization can also improve the performance of general image retrieval. Code is available at https://github.com/Jiajun-ISIA/Generalization-oriented-Sampling-and-Loss. © 2023 Copyright held by the owner/author(s).",deep learning; Food computing; image retrieval,Deep learning; Embeddings; Image retrieval; Class-distance; Deep learning; Embeddings; Food computing; Food image; Generalisation; Intra class; Performance; Sampling functions; Sampling strategies; Image enhancement
Scale-Semantic Joint Decoupling Network for Image-Text Retrieval in Remote Sensing,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173578916&doi=10.1145%2f3603628&partnerID=40&md5=5fcbc10e8890b6aaada9d5323d485eb5,"Image-text retrieval in remote sensing aims to provide flexible information for data analysis and application. In recent years, state-of-the-art methods are dedicated to ""scale decoupling""and ""semantic decoupling""strategies to further enhance the capability of representation. However, these previous approaches focus on either the disentangling scale or semantics but ignore merging these two ideas in a union model, which extremely limits the performance of cross-modal retrieval models. To address these issues, we propose a novel Scale-Semantic Joint Decoupling Network (SSJDN) for remote sensing image-text retrieval. Specifically, we design the Bidirectional Scale Decoupling (BSD) module, which exploits Salience Extraction Map (SEM) and Salience Suppression Map (SSM) units to adaptively extract potential features and suppress cumbersome features at other scales in a bidirectional pattern to yield different scale clues. Besides, we design the Label-supervised Semantic Decoupling (LSD) module by leveraging the category semantic labels as prior knowledge to supervise images and texts probing significant semantic-related information. Finally, we design a Semantic-guided Triple Loss (STL), which adaptively generates a constant to adjust the loss function to improve the probability of matching the same semantic image and text and shorten the convergence time of the retrieval model. Our proposed SSJDN outperforms state-of-the-art approaches in numerical experiments conducted on four benchmark remote sensing datasets. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",image-text retrieval; Remote sensing; scale-semantic joint decoupling,Image enhancement; Information retrieval; Knowledge management; Semantic Web; Semantics; Decoupling network; Decouplings; Image texts; Image-text retrieval; Remote-sensing; Retrieval models; Scale-semantic joint decoupling; Semantic decoupling; State-of-the-art methods; Text retrieval; Remote sensing
Advanced Predictive Tile Selection Using Dynamic Tiling for Prioritized 360° Video VR Streaming,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173576458&doi=10.1145%2f3603146&partnerID=40&md5=caf71d59d8ca202603b9833061dcfcca,"The widespread availability of smart computing and display devices such as mobile phones, gaming consoles, laptops, and tethered/untethered head-mounted displays has fueled an increase in demand for omnidirectional (360°) videos. 360° video applications enable users to change their viewing angles while interacting with the video during playback. This allows users to have a more personalized and interactive viewing experience. Unfortunately, these applications require substantial network and computational resources that the conventional infrastructure and end devices cannot support. Recently proposed viewport adaptive fixed tiling solutions stream only relevant video tiles based on user interaction with the virtual reality (VR) space to use existing transmission resources more efficiently. However, achieving real-time accurate viewport extraction and transmission in response to both head movements and bandwidth dynamics can be challenging, which can impact the user's Quality of Experience (QoE). This article proposes innovative dynamic tiling-based adaptive 360° video streaming solutions in order to achieve high viewer QoE. First, novel and easy-to-scale tiling layout selection methods are introduced, and the best tiling layouts are employed in each adaptation interval based on the prediction-assisted visual quality metric and the observed viewport divergence. Second, a novel proactive tile selection approach is presented, which adaptively extracts tiles for each selected tiling layout based on two low-complex viewport prediction mechanisms. Finally, a practical dynamic tile priority-oriented bitrate adaptation scheme is introduced, which uniformly distributes the bitrate budget among different tiles during 360° video streaming. Extensive trace-driven experiments are conducted to evaluate the proposed solutions using head motion traces from 48 VR users for five 360° videos with tiling layouts of 4 × 3, 6 × 4, and 8 × 6 and segment durations of 1s, 1.5s, and 2s. The experimental evaluations show that the dynamic video tiling solutions achieve up to 11.2% more viewport matches and an average improvement in QoE of 9.7% to 18% compared to state-of-the-art 360° streaming approaches. © 2023 Copyright held by the owner/author(s).",360° Video streaming; bitrate adaptation; dynamic tiling; QoE; tiles selection,Budget control; Helmet mounted displays; Quality of service; Virtual reality; 360° video streaming; Bit rates; Bitrate adaptation; Dynamic tiling; Gaming consoles; Head-mounted-displays; Quality of experience; Smart computing; Tile selection; Video-streaming; Video streaming
A Universal Optimization Framework for Learning-based Image Codec,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173217346&doi=10.1145%2f3580499&partnerID=40&md5=432086c71907ba3dff5041a0c3d25e80,"Recently, machine learning-based image compression has attracted increasing interests and is approaching the state-of-the-art compression ratio. But unlike traditional codec, it lacks a universal optimization method to seek efficient representation for different images. In this paper, we develop a plug-and-play optimization framework for seeking higher compression ratio, which can be flexibly applied to existing and potential future compression networks. To make the latent representation more efficient, we propose a novel latent optimization algorithm to adaptively remove the redundancy for each image. Additionally, inspired by the potential of side information for traditional codecs, we introduce side information into our framework, and integrate side information optimization with latent optimization to further enhance the compression ratio. In particular, with the joint side information and latent optimization, we can achieve fine rate control using only single model instead of training different models for different rate-distortion trade-offs, which significantly reduces the training and storage cost to support multiple bit rates. Experimental results demonstrate that our proposed framework can remarkably boost the machine learning-based compression ratio, achieving more than 10% additional bit rate saving on three different representative network structures. With the proposed optimization framework, we can achieve 7.6% bit rate saving against the latest traditional coding standard VVC on Kodak dataset, yielding the state-of-the-art compression ratio.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Image compression; machine learning; rate distortion optimization; universal optimization framework,Economic and social effects; Electric distortion; Image coding; Image compression; Signal distortion; Bit-rate savings; Image codecs; Images compression; Machine-learning; Optimisations; Optimization framework; Rate-distortion optimization; Side information; State of the art; Universal optimization framework; Machine learning
Syncretic Space Learning Network for NIR-VIS Face Recognition,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173247381&doi=10.1145%2f3607143&partnerID=40&md5=5234e1f3f6c2bf51fe72d14a67c4ab65,"To overcome the technical bottleneck of face recognition in low-light scenarios, Near-InfraRed and VISible (NIR-VIS) heterogeneous face recognition is proposed for matching well-lit VIS faces with poorly lit NIR faces. Current cross-modal synthesis methods visually convert the NIR modality to the VIS modality and then perform face matching in the VIS modality. However, using a heavyweight GAN network on unpaired NIR-VIS faces may lead to high synthesis difficulty, low inference efficiency, and other problems. To alleviate the above problems, we simultaneously synthesize NIR and VIS images into modality-independent syncretic images and propose a novel syncretic space learning (SSL) model to eliminate the modal gap. First, Syncretic Modality Generator (SMG) synthesizes NIR and VIS images into syncretic images using channel-level convolution with a shallow CNN. In particular, the discriminative structural information is well preserved and the face quality can be further improved with small modal variations in a self-supervised learning manner. Second, Modality-adversarial Syncretic space Learning (MSL) projects NIR and VIS images into the syncretic space by a syncretic-modality adversarial learning strategy with syncretic pattern guided objective, so the modal gap of NIR-VIS faces can be effectively reduced. Finally, the Syncretic Distribution Consistency (SDC) constructed by NIR-syncretic, syncretic-syncretic, and VIS-syncretic consistency can enhance the intra-class compactness and learn discriminative representations. Extensive experiments on three challenging datasets demonstrate the effectiveness of the SSL method.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adversarial learning; NIR-VIS face recognition; syncretic space learning,Computer vision; Deep learning; Infrared devices; Learning systems; Modal analysis; 'current; Adversarial learning; Cross-modal; Learning network; Low light; Matchings; Near Infrared; Near-infrared; Near-infrared and visible face recognition; Syncretic space learning; Face recognition
S3Mix: Same Category Same Semantics Mixing for Augmenting Fine-grained Images,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173581535&doi=10.1145%2f3605892&partnerID=40&md5=efef74052ec95718eb557aa86a931422,"Data augmentation is a common technique to improve the generalization performance of models for image classification. Although methods such as Mixup and CutMix that mix images randomly are indeed instrumental in general image classification, randomly swapping or masking regions is not friendly to fine-grained images, since the key to fine-grained image classification precisely lies in discriminative and informative regions, and it is unreasonable to generate labels solely consistent with the proportion of synthesis. Some erasing methods like Cutout even endanger fine-grained image classification because of erasing the discriminative regions by chance. In this article, we propose the Same Category Same Semantics Mixing method (S3Mix) corresponding to the characteristics of fine-grained images. Specifically, we limit the mixture to regions of the same category and semantics. The core of the method is two constraints. The exchange with the semantic region ensures the discrimination and semantics integrity of the generated image, and the exchange in the same class avoids the problem of unreasonable label generation. At the same time, we propose a homology loss to promote the semantic relationship between the generated positive image pairs. Experiments have been conducted on four fine-grained datasets, and the results show the proposed method is superior to the traditional image augmentation methods as well as some fine-grained data augmentation methods. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Data augmentation; fine-grained images; homology loss; Vision Transformer,Image enhancement; Mixing; Semantics; Augmentation methods; Data augmentation; Fine grained; Fine-grained image; Generalization performance; Homology loss; Images classification; Masking regions; Mixing method; Vision transformer; Image classification
Attentional Composition Networks for Long-Tailed Human Action Recognition,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173584255&doi=10.1145%2f3603253&partnerID=40&md5=bd420cd2675ea2d1f76c6de715cdd064,"The problem of long-tailed visual recognition has been receiving increasing research attention. However, the long-tailed distribution problem remains underexplored for video-based visual recognition. To address this issue, in this article we propose a compositional learning based solution for video-based human action recognition. Our method, named Attentional Composition Networks (ACN), first learns verb-like and preposition-like components, then shuffles these components to generate samples for the tail classes in the feature space to augment the data for the tail classes. Specifically, during training, we represent each action video by a graph that captures the spatial-temporal relations (edges) among detected human/object instances (nodes). Then, ACN utilizes the position information to decompose each action into a set of verb and preposition representations using the edge features in the graph. After that, the verb and preposition features from different videos are combined via an attention structure to synthesize feature representations for tail classes. This way, we can enrich the data for the tail classes and consequently improve the action recognition for these classes. To evaluate the compositional human action recognition, we further contribute a new human action recognition dataset, namely NEU-Interaction (NEU-I). Experimental results on both Something-Something V2 and the proposed NEU-I demonstrate the effectiveness of the proposed method for long-tailed, few-shot, and zero-shot problems in human action recognition. Source code and the NEU-I dataset are available at https://github.com/YajieW99/ACN. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",action recognition; Compositional learning; few-shot; long tail; zero-shot,Action recognition; Compositional learning; Distribution problem; Few-shot; Human-action recognition; Learn+; Long tail; Long-tailed distributions; Visual recognition; Zero-shot; Zero-shot learning
DiRaC-I: Identifying Diverse and Rare Training Classes for Zero-Shot Learning,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173270985&doi=10.1145%2f3603147&partnerID=40&md5=4edba039fa00d71c948ac9528e988063,"Zero-Shot Learning (ZSL) is an extreme form of transfer learning that aims at learning from a few ""seen classes""to have an understanding about the ""unseen classes""in the wild. Given a dataset in ZSL research, most existing works use a predetermined, disjoint set of seen-unseen classes to evaluate their methods. These seen (training) classes might be sub-optimal for ZSL methods to appreciate the diversity and rarity of an object domain. Inspired by strategies like active learning, it is intuitive that intelligently selecting the training classes can improve ZSL performance. In this work, we propose a framework called Diverse and Rare Class Identifier (DiRaC-I) which, given an attribute-based dataset, can intelligently yield the most suitable ""seen classes""for training ZSL models. DiRaC-I has two main goals - constructing a diversified set of seed classes, and using them to initialize a visual-semantic mining algorithm for acquiring the classes capturing both diversity and rarity in the object domain adequately. These classes can then be used as ""seen classes""to train ZSL models for image classification. We simulate a real-world scenario where visual samples of novel object classes in the wild are available to neither DiRaC-I nor the ZSL models during training and conducted extensive experiments on two benchmark data sets for zero-shot image classification - CUB and SUN. Our results demonstrate DiRaC-I helps ZSL models to achieve significant classification accuracy improvements - specifically, up to 8% for CUB and up to 5% for SUN dataset. Additionally, while recognizing classes exhibiting rare attributes we also observe a performance boost for ZSL models, which is up to 10% and 7% for CUB and SUN datasets, respectively.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",active learning; domain rarity; image classification; Zero-shot learning,Classification (of information); Semantics; Zero-shot learning; Active Learning; Disjoint sets; Domain rarity; Images classification; Learning methods; Learning models; Object domains; Rare class; Training class; Transfer learning; Image classification
Instance-Based Continual Learning: A Real-World Dataset and Baseline for Fresh Recognition,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173264999&doi=10.1145%2f3591209&partnerID=40&md5=3556fb59f41b375234a9b0c313a7492a,"Real-time learning on real-world data streams with temporal relations is essential for intelligent agents. However, current online Continual Learning (CL) benchmarks adopt the mini-batch setting and are composed of temporally unrelated and disjoint tasks as well as pre-set class boundaries. In this paper, we delve into a real-world CL scenario for fresh recognition where algorithms are required to recognize a huge variety of products to facilitate the checkout speed. Products mainly consists of packaged cereals, seasonal fruits, and vegetables from local farms or shipped from overseas. Since algorithms process instance streams consisting of sequential images, we name this real-world CL problem as Instance-Based Continual Learning (IBCL). Different from the current online CL setting, algorithms are required to perform instant testing and learning upon each incoming instance. Moreover, IBCL has no task boundaries or class boundaries and allows the evolution and the forgetting of old samples within each class. To promote the researches on real CL challenges, we propose the first real-world CL dataset coined the Continual Fresh Recognition (CFR) dataset, which consists of fresh recognition data streams (766 K labelled images in total) collected from 30 supermarkets. Based on the CFR dataset, we extensively evaluate the performance of current online CL methods under various settings and find that current prominent online CL methods operate at high latency and demand significant memory consumption to cache old samples for replaying. Therefore, we make the first attempt to design an efficient and effective Instant Training-Free Learning (ITFL) framework for IBCL. ITFL consists of feature extractors trained in the metric learning manner and reformulates CL as a temporal classification problem among several most similar classes. Unlike current online CL methods that cache image samples (150 KB per image) and rely on training to learn new knowledge, our framework only caches features (2 KB per image) and is free of training in deployment. Extensive evaluations across three datasets demonstrate that our method achieves comparable recognition accuracy to current methods with lower latency and less resource consumption. Our codes and datasets will be publicly available at https://github.com/detectRecog/IBCL.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",fresh recognition; Online continual learning,HTTP; Intelligent agents; Learning algorithms; 'current; Class boundary; Continual learning; Data stream; Fresh recognition; Learning methods; Online continual learning; Real-time learning; Real-world; Real-world datasets; E-learning
Robust Hashing via Global and Local Invariant Features for Image Copy Detection,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173253198&doi=10.1145%2f3600234&partnerID=40&md5=770b82a8136e02657d3127f9541d4531,"Robust hashing is a powerful technique for processing large-scale images. Currently, many reported image hashing schemes do not perform well in balancing the performances of discrimination and robustness, and thus they cannot efficiently detect image copies, especially the image copies with multiple distortions. To address this, we exploit global and local invariant features to develop a novel robust hashing for image copy detection. A critical contribution is the global feature calculation by gray level co-occurrence moment learned from the saliency map determined by the phase spectrum of quaternion Fourier transform, which can significantly enhance discrimination without reducing robustness. Another essential contribution is the local invariant feature computation via Kernel Principal Component Analysis (KPCA) and vector distances. As KPCA can maintain the geometric relationships within image, the local invariant features learned with KPCA and vector distances can guarantee discrimination and compactness. Moreover, the global and local invariant features are encrypted to ensure security. Finally, the hash is produced via the ordinal measures of the encrypted features for making a short length of hash. Numerous experiments are conducted to show efficiency of our scheme. Compared with some well-known hashing schemes, our scheme demonstrates a preferable classification performance of discrimination and robustness. The experiments of detecting image copies with multiple distortions are tested and the results illustrate the effectiveness of our scheme.  © 2023 Copyright held by the owner/author(s).",dimension reduction; image copy detection; invariant features; Robust hashing; saliency map,Cryptography; Feature extraction; Dimension reduction; Image copy detection; Image hashing; Invariant features; Kernel principal component analyses (KPCA); Large-scales; Local invariant features; Robust hashing; Saliency map; Vector distance; Principal component analysis
Dynamic Weighted Gradient Reversal Network for Visible-infrared Person Re-identification,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173220259&doi=10.1145%2f3607535&partnerID=40&md5=d60173d7682e5d819e7f9b553ed50cf2,"Due to intra-modality variations and cross-modality discrepancy, visible-infrared person re-identification (VI Re-ID) is an important and challenging task in intelligent video surveillance. The cross-modality discrepancy is mainly caused by the differences between visible images and infrared images, the inherent essence of which is heterogeneous. To alleviate this discrepancy, we propose a Dynamic Weighted Gradient Reversal Network (DGRNet) to enhance the learning of discriminative common representations by confusing the modality discrimination. In the proposed DGRNet, we design the gradient reversal model guiding adversarial training between identity classifier and modality discriminator to reduce the modality discrepancy of the same person in different modalities. Furthermore, we propose an optimization training method, that is, designing dynamic weight of gradient reversal to achieve optimal adversarial training, and dynamic weight has the ability to dynamically and adaptively evaluate the significance of target loss term, without involving hyper-parameter tuning. Extensive experiments were conducted on two public VI Re-ID datasets, SYSU-MM01 and RegDB. The experimental results show that the proposed DGRNet outperforms state-of-the-art methods and demonstrate the effectiveness of the DGRNet to learn more discriminative common representations for VI Re-ID.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adversarial training; Dynamic weight; gradient reversal; VI Re-ID,Security systems; Adversarial training; Cross modality; Dynamic weight; Gradient reversal; Intelligent video surveillance; Optimisations; Person re identifications; Training methods; Visible image; Visible-infrared person re-identification; Infrared imaging
1DIEN: Cross-session Electrocardiogram Authentication Using 1D Integrated EfficientNet,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173216844&doi=10.1145%2f3609800&partnerID=40&md5=6d27b2603d5d637ac7c2177bc017e554,"The potential of using electrocardiogram (ECG), an important physiological signal for humans, as a new biometric trait has been demonstrated, and ongoing efforts have focused on utilizing deep learning (e.g., 2D neural networks) to improve authentication accuracy (with some efficiency tradeoffs). In most of the existing ECG-based authentication approaches, the ECG recordings for enrollment and testing are collected within short intervals (e.g., within an hour). However, since ECG biometrics change over time, this design may decrease authentication accuracy when ECG recordings are collected weeks or even months prior. In this article, we propose 1D Integrated EfficientNet (1DIEN) to achieve cross-session ECG authentication. We adopt 1D neural networks as a lightweight alternative to 2D neural networks, and a voting scheme is designed to reduce variance and improve general authentication performance. We use three public ECG databases (i.e., an inter-session database, a mixed-session database, and an intra-session database) to evaluate our proposed 1DIEN under different authentication scenarios. The experimental results show that our approach achieves satisfactory performance for ECG authentication at a 3-month interval and is suitable for practical applications.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",1D Efficientnet; authentication; ECG,Authentication; Biometrics; Database systems; Deep learning; 1d efficientnet; Biometric traits; Change-over time; Electrocardiogram recordings; Neural-networks; Performance; Physiological signals; Short-interval; Voting schemes; Electrocardiograms
Cascading Blend Network for Image Inpainting,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173273158&doi=10.1145%2f3608952&partnerID=40&md5=ceeab62ac2a1b620d1a630712bc73212,"Image inpainting refers to filling in unknown regions with known knowledge, which is in full flourish accompanied by the popularity and prosperity of deep convolutional networks. Current inpainting methods have excelled in completing small-sized corruption or specifically masked images. However, for large-proportion corrupted images, most attention-based and structure-based approaches, though reported with state-of-the-art performance, fail to reconstruct high-quality results due to the short consideration of semantic relevance. To relieve the above problem, in this paper, we propose a novel image inpainting approach, namely cascading blend network (CBNet), to strengthen the capacity of feature representation. As a whole, we introduce an adjacent transfer attention (ATA) module in the decoder, which preserves contour structure reasonably from the deep layer and blends structure-texture information from the shadow layer. In a coarse to delicate manner, a multi-scale contextual blend (MCB) block is further designed to felicitously assemble the multi-stage feature information. In addition, to ensure a high qualified hybrid of the feature information, extra deep supervision is applied to the intermediate features through a cascaded loss. Qualitative and quantitative experiments on the Paris StreetView, CelebA, and Places2 datasets demonstrate the superior performance of our approach compared with most state-of-the-art algorithms.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",attention-based; deep convolutional networks; Image inpainting; large-proportion corrupted images; multi-scale context blend,Image processing; Semantics; Textures; Attention-based; Convolutional networks; Corrupted images; Deep convolutional network; Feature information; Filling in; Image Inpainting; Large-proportion corrupted image; Multi-scale context blend; Multi-scales; Convolution
Language-guided Residual Graph Attention Network and Data Augmentation for Visual Grounding,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173285136&doi=10.1145%2f3604557&partnerID=40&md5=206aa91b587d4471afa6a8d8f565e141,"Visual grounding is an essential task in understanding the semantic relationship between the given text description and the target object in an image. Due to the innate complexity of language and the rich semantic context of the image, it is still a challenging problem to infer the underlying relationship and to perform reasoning between the objects in an image and the given expression. Although existing visual grounding methods have achieved promising progress, cross-modal mapping across different domains for the task is still not well handled, especially when the expressions are complex and long. To address the issue, we propose a language-guided residual graph attention network for visual grounding (LRGAT-VG), which enables us to apply deeper graph convolution layers with the assistance of residual connections between them. This allows us to better handle long and complex expressions than other graph-based methods. Furthermore, we perform a Language-guided Data Augmentation (LGDA), which is based on copy-paste operations on pairs of source and target images to increase the diversity of training data while maintaining the relationship between the objects in the image and the expression. With extensive experiments on three visual grounding benchmarks, including RefCOCO, RefCOCO+, and RefCOCOg, LRGAT-VG with LGDA achieves competitive performance with other state-of-the-art graph network-based referring expression approaches and demonstrates its effectiveness.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",data augmentation; Residual graph attention network; visual grounding,Benchmarking; Complex networks; Graphic methods; Visual languages; Cross-modal; Data augmentation; Different domains; Grounding methods; Network augmentation; Residual graph attention network; Semantic context; Semantic relationships; Target object; Visual grounding; Semantics
SSR-Net: A Spatial Structural Relation Network for Vehicle Re-identification,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168234630&doi=10.1145%2f3578578&partnerID=40&md5=f21fe94217068a7ce2b2290318ae2589,"Vehicle re-identification (Re-ID) represents the task aiming to identify the same vehicle from images captured by different cameras. Recent years have seen various feature learning-based approaches merely focusing on feature representations including global features or local features to obtain more subtle details to identify highly similar vehicles. However, few such methods consider the spatial geometrical structure relationship among local regions or between the global and local regions. By contrast, in this study, we propose a Spatial Structural Relation Network (SSR-Net) that explores the above-mentioned two kinds of relations simultaneously to learn more discriminative features by modeling the spatial structure information and global context information. In this article, we propose to adopt a Graph Convolution Network (GCN), for modeling spatial structural relationships among characteristic features. The GCN model aggregating the local and global features is shown to be more discriminative and robust to several car image transformations. To improve the performance of our proposed network, we jointly combine the classification loss with metric learning loss. Extensive experiments conducted on the public VehicleID and VeRi-776 datasets validate the effectiveness of our approach in comparison with recent works. © 2023 Association for Computing Machinery.",attention mechanism; deep learning; Graph Convolution Network; Vehicle re-identification,Convolution; Deep learning; Attention mechanisms; Deep learning; Feature learning; Global feature; Graph convolution network; Learning-based approach; Local feature; Local region; Re identifications; Vehicle re-identification; Vehicles
Text Image Super-Resolution Guided by Text Structure and Embedding Priors,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167803681&doi=10.1145%2f3595924&partnerID=40&md5=6a9a9bf08e35ecaf388e0efa7621defe,"We aim to super-resolve text images from unrecognizable low-resolution inputs. Existing super-resolution methods mainly learn a direct mapping from low-resolution to high-resolution images by exploring low-level features, which usually generate blurry outputs and suffer from severe structure distortion for text parts, especially when the resolution is quite low. Both the visual quality and the readability will suffer. To tackle these issues, we propose a new text super-resolution paradigm by recovering with understanding. Specifically, we extract a text-embedding prior and a text-structure prior from the upsampled image by learning to understand the text. The two priors with rich structure information and text-embedding information are then used as auxiliary information to recover the clear text structure. In addition, we introduce a text-feature loss to guide the training for better text recognizability. Extensive evaluations on both screen and scene text image datasets show that our method largely outperforms the state-of-the-art in both visual quality and recognition accuracy.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Text image super-resolution; text-embedding prior; text-structure prior,Character recognition; Optical resolving power; Embeddings; Image super resolutions; Lower resolution; Superresolution methods; Text image super-resolution; Text images; Text structure; Text-embedding prior; Text-structure prior; Visual qualities; Embeddings
Hypergraph Association Weakly Supervised Crowd Counting,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168244833&doi=10.1145%2f3594670&partnerID=40&md5=7365d68b57e3ad684dac1f137b82bde9,"Weakly supervised crowd counting involves the regression of the number of individuals present in an image, using only the total number as the label. However, this task is plagued by two primary challenges: the large variation of head size and uneven distribution of crowd density. To address these issues, we propose a novel Hypergraph Association Crowd Counting (HACC) framework. Our approach consists of a new multi-scale dilated pyramid module that can efficiently handle the large variation of head size. Further, we propose a novel hypergraph association module to solve the problem of uneven distribution of crowd density by encoding higher-order associations among features, which opens a new direction to solve this problem. Experimental results on multiple datasets demonstrate that our HACC model achieves new state-of-the-art results. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Crowd counting; hypergraph association; hypergraph neural network; uneven distribution of crowd density,Crowd counting; Crowd density; Head size; Hyper graph; Hypergraph association; Hypergraph neural network; Multi-scales; Neural-networks; Uneven distribution of crowd density
Learning Semantic Representation on Visual Attribute Graph for Person Re-identification and beyond,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166259625&doi=10.1145%2f3487044&partnerID=40&md5=f5c08c650e7840859120dbe26e62196a,"Person re-identification (re-ID) aims to match pedestrian pairs captured from different cameras. Recently, various attribute-based models have been proposed to combine the pedestrian attribute as an auxiliary semantic information to learn a more discriminative pedestrian representation. However, these methods usually directly concatenate the visual branch and attribute branch embeddings as the final pedestrian representation, which ignores the semantic relation between the pedestrian revealed by attribute similarity. To capture and explore such semantic relation, we propose a unified pedestrian representation framework, called Visual Attribute Graph Embedding Network (VAGEN), to simultaneously learn attribute and visual representation. We unify the visual embedding and attribute similarity into a Visual Attribute Graph, where pedestrian is considered as a node and attribute similarity as an edge. Then, we learn graph node embedding to generate pedestrian representation through Graph Neural Network. Except for this unified representation for visual and attribute embeddings, VAGEN also conducts implicitly hard example mining for visual similar false-positive results, which has not been explored yet among existing attribute-based methods. We conduct extensive empirical studies on several person re-ID datasets to evaluate our proposed algorithm from different aspects. The results show that our proposed method outperforms state-of-the-art techniques with considerable margins.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",attribute-based image retrieval; Graph Neural Network; Person re-identification; representation learning,Graph embeddings; Graph theory; Image retrieval; Network embeddings; Semantic Web; Semantics; Attribute graphs; Attribute similarity; Attribute-based; Attribute-based image retrieval; Embeddings; Graph neural networks; Learn+; Person re identifications; Representation learning; Visual attributes; Graph neural networks
Sim2Word: Explaining Similarity with Representative Attribute Words via Counterfactual Explanations,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166385003&doi=10.1145%2f3563039&partnerID=40&md5=551208949092c5a9ac1a9ec0b25c28f8,"Recently, we have witnessed substantial success using the deep neural network in many tasks. Although there still exist concerns about the explainability of decision making, it is beneficial for users to discern the defects in the deployed deep models. Existing explainable models either provide the image-level visualization of attention weights or generate textual descriptions as post hoc justifications. Different from existing models, in this article we propose a new interpretation method that explains the image similarity models by salience maps and attribute words. Our interpretation model contains visual salience maps generation and the counterfactual explanation generation. The former has two branches: global identity relevant region discovery and multi-attribute semantic region discovery. The first branch aims to capture the visual evidence supporting the similarity score, which is achieved by computing counterfactual feature maps. The second branch aims to discover semantic regions supporting different attributes, which helps to understand which attributes in an image might change the similarity score. Then, by fusing visual evidence from two branches, we can obtain the salience maps indicating important response evidence. The latter will generate the attribute words that best explain the similarity using the proposed erasing model. The effectiveness of our model is evaluated on the classical face verification task. Experiments conducted on two benchmarks - VGGFace2 and Celeb-A - demonstrate that our model can provide convincing interpretable explanations for the similarity. Moreover, our algorithm can be applied to evidential learning cases, such as finding the most characteristic attributes in a set of face images, and we verify its effectiveness on the VGGFace2 dataset.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesExplainable AI; counterfactual explanation; similarity explanation,Decision making; Deep neural networks; Additional key word and phrasesexplainable AI; Counterfactual explanation; Counterfactuals; Decisions makings; Key words; Salience map; Similarity explanation; Similarity scores; Textual description; Visual evidence; Semantics
A2SC: Adversarial Attacks on Subspace Clustering,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168243237&doi=10.1145%2f3587097&partnerID=40&md5=ef48811cad7431d67071dd3a5ba0ccd7,"Many studies demonstrate that supervised learning techniques are vulnerable to adversarial examples. However, adversarial threats in unsupervised learning have not drawn sufficient scholarly attention. In this article, we formally address the unexplored adversarial attacks in the equally important unsupervised clustering field and propose the concept of the adversarial set and adversarial set attack for clustering. To illustrate the basic idea, we design a novel adversarial space-mapping attack algorithm to confuse subspace clustering, one of the mainstream branches of unsupervised clustering. It maps a sample into one wrong class by moving it towards the closest point on the linear subspace of the target class, that is, along the normal of the closest point. This simple single-step algorithm has the power to craft the adversarial set where the image samples can be wrongly clustered, even into the targeted labels. Empirical results on different image datasets verify the effectiveness and superiority of our algorithm. We further show that deep supervised learning algorithms (such as VGG and ResNet) are also vulnerable to our crafted adversarial set, which illustrates the good cross-task transferability of the adversarial set. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",adversarial attack; adversarial set; black-box attack; Subspace clustering; unsupervised learning,Clustering algorithms; Deep learning; Learning algorithms; Learning systems; Supervised learning; Adversarial attack; Adversarial set; Black boxes; Black-box attack; Clusterings; Learning techniques; Linear subspace; Space-mapping; Subspace clustering; Unsupervised clustering; Unsupervised learning
Temporal Dynamic Concept Modeling Network for Explainable Video Event Recognition,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166394028&doi=10.1145%2f3568312&partnerID=40&md5=30110d325ac7c39920964929959dbb2b,"Recently, with the vigorous development of deep learning and multimedia technology, intelligent urban computing has received more and more extensive attention from academia and industry. Unfortunately, most of the related technologies are black-box paradigms that lack interpretability. Among them, video event recognition is a basic technology. Event contains multiple concepts and their rich interactions, which can assist us to construct explainable event recognition methods. However, the crucial concepts needed to recognize events have various temporal existing patterns, and the relationship between events and the temporal characteristics of concepts has not been fully exploited. This brings great challenges for concept-based event categorization. To address the above issues, we introduce the temporal concept receptive field, which is the length of the temporal window size required to capture key concepts for concept-based event recognition methods. Accordingly, we introduce the temporal dynamic convolution (TDC) to model the temporal concept receptive field dynamically according to different events. Its core idea is to combine the results of multiple convolution layers with the learned coefficients from two complementary perspectives. These convolution layers contain a variety of kernel sizes, which can provide temporal concept receptive fields of different lengths. Similarly, we also propose the cross-domain temporal dynamic convolution (CrTDC) with the help of the rich relationship between different concepts. Different coefficients can help us to capture suitable temporal concept receptive field sizes and highlight crucial concepts to obtain accurate and complete concept representations for event analysis. Based on the TDC and CrTDC, we introduce the temporal dynamic concept modeling network (TDCMN) for explainable video event recognition. We evaluate TDCMN on large-scale and challenging datasets FCVID, ActivityNet, and CCV. Experimental results show that TDCMN significantly improves the event recognition performance of concept-based methods, and the explainability of our method inspires us to construct more explainable models from the perspective of the temporal concept receptive field.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesEvent recognition; dynamic convolution; temporal concept receptive field,Deep learning; Large dataset; Multimedia systems; Additional key word and phrasesevent recognition; Concept model; Dynamic concepts; Dynamic convolution; Event recognition; Key words; Receptive fields; Temporal concept receptive field; Temporal concepts; Temporal dynamics; Convolution
Complementary Coarse-to-Fine Matching for Video Object Segmentation,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168305065&doi=10.1145%2f3596496&partnerID=40&md5=afb04f795bb44a0a74b996416dca6652,"Semi-supervised Video Object Segmentation (VOS) needs to establish pixel-level correspondences between a video frame and preceding segmented frames to leverage their segmentation clues. Most works rely on features at a single scale to establish those correspondences, e.g., perform dense matching with Convolutional Neural Network (CNN) features from a deep layer. Differently, this work explores complementary features at different scales to pursue more robust feature matching. A coarse feature from a deep layer is first adopted to get coarse pixel-level correspondences. We hence evaluate the quality of those correspondences, and select pixels with low-quality correspondences for fine-scale feature matching. Segmentation clues of previous frames are propagated by both coarse and fine-scale correspondences, which are fused with appearance features for object segmentation. Compared with previous works, this coarse-to-fine matching scheme is more robust to distractions by similar objects and better preserves object details. The sparse fine-scale matching also ensures a fast inference speed. On popular VOS datasets including DAVIS and YouTube-VOS, the proposed method shows promising performance compared with recent works.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",coarse-to-fine matching; label propagation; Video object segmentation,Backpropagation; Convolutional neural networks; Image segmentation; Multilayer neural networks; Pixels; Coarse to fine; Coarse-to-fine matching; Deep layer; Features matching; Fine-scale; Label propagation; Matchings; Pixel level; Semi-supervised; Video objects segmentations; Motion compensation
Context-Based Novel Histogram Bin Stretching Algorithm for Automatic Contrast Enhancement,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168311629&doi=10.1145%2f3597303&partnerID=40&md5=e78dff01369999acbb0dd3c4e04a4a2e,"This article presents CHBS, a novel context-based histogram bin stretching method that enhances the contrast by increasing the range of gray levels and randomness among the gray levels. It comprises image spatial contextual information and discrete cosine transform (DCT). It constitutes the global enhancement with the context-based histogram bin stretching and local details with the DCT. First, it uses the spatial similarities among surrounding pixels to generate random numbers. Unlike the other methods, the similarity map is generated based on the neighboring pixels' mutual relationship. Intensity values are distributed among the available dynamic range to generate a global contrast-enhanced image. Second, the DCT is further applied to the previous contrast-enhanced image to adjust its local details automatically. Several experiments are conducted on the different levels of contrast degraded images. Both subjective and objective assessment outcomes validate that the projected approach is better or comparable with several state-of-the-art approaches in terms of brightness preservation, richer details, and natural appearance.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",contrast enhancement; discrete cosine transform; Histogram bin stretching; spatial contextual information,Graphic methods; Image compression; Image enhancement; Pixels; Random number generation; Bin stretching; Context-based; Contextual information; Contrast Enhancement; Contrast-enhanced images; Global enhancement; Gray-level; Histogram bin stretching; Spatial contextual information; Spatial similarity; Discrete cosine transforms
Introduction to the Special Issue on Trustworthy Multimedia Computing and Applications in Urban Scenes,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166404946&doi=10.1145%2f3603534&partnerID=40&md5=1ea5facdd1c8d61c17baba8ea7cfd188,[No abstract available],,
VISCOUNTH: A Large-scale Multilingual Visual Question Answering Dataset for Cultural Heritage,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166385185&doi=10.1145%2f3590773&partnerID=40&md5=86e02c6bf52ad96aa09d3ff87920c216,"Visual question answering has recently been settled as a fundamental multi-modal reasoning task of artificial intelligence that allows users to get information about visual content by asking questions in natural language. In the cultural heritage domain, this task can contribute to assisting visitors in museums and cultural sites, thus increasing engagement. However, the development of visual question answering models for cultural heritage is prevented by the lack of suitable large-scale datasets. To meet this demand, we built a large-scale heterogeneous and multilingual (Italian and English) dataset for cultural heritage that comprises approximately 500K Italian cultural assets and 6.5M question-answer pairs. We propose a novel formulation of the task that requires reasoning over both the visual content and an associated natural language description, and present baselines for this task. Results show that the current state of the art is reasonably effective but still far from satisfactory; therefore, further research in this area is recommended. Nonetheless, we also present a holistic baseline to address visual and contextual questions and foster future research on the topic.  © 2023 Copyright held by the owner/author(s).",cultural heritage; Visual question answering,Visual languages; Cultural heritages; Large-scale datasets; Large-scales; Modal reasoning; Multi-modal; Natural languages; Question Answering; Reasoning tasks; Visual content; Visual question answering; Large dataset
Compressed Screen Content Image Super Resolution,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168236409&doi=10.1145%2f3589963&partnerID=40&md5=c84aa8d9931ae65bdc6b75999bfc6974,"Screen content has become one of the prominent mediums in the increasingly connected world. With the prevalence of remote collaboration and communication such as virtual conferences and online education, recent years have witnessed a dramatic increase in the data volume of the screen content. Screen content compression serves as the fundamental technology in fostering the storage, transmission, and exhibition of screen content. In this article, we target the super-resolution of the compressed screen content images, intending to tackle the real-world challenge problems. A dataset is proposed for the super-resolution of the screen contents contaminated with different compression distortion levels. Subsequently, we introduce the principle of the multi-hypothesis into the super resolution and propose a new paradigm for the restoration of the compressed screen content images. The luminance and sharpness similarity metric is adopted in the network learning to better adapt to the screen content characteristic and ensure perceptual fidelity. Experimental results verify the superiority and effectiveness of the proposed method. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",convolutional neural network; deep learning; Screen content; super resolution; versatile video coding,Convolutional neural networks; Deep learning; Digital storage; E-learning; Image compression; Optical resolving power; Personnel training; Video signal processing; Convolutional neural network; Deep learning; Image super resolutions; On-line education; Remote collaboration; Remote communication; Screen content; Superresolution; Versatile video coding; Virtual conferences; Image coding
Shot Boundary Detection Using Color Clustering and Attention Mechanism,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167812627&doi=10.1145%2f3595923&partnerID=40&md5=5160ff070b9bb213296155c27282f0e7,"Shot boundary detection (SBD) is widely used in scene segmentation, semantic analysis, and video retrieval. However, existing SBD algorithms have certain applications in video processing, but they have the following three problems. First, these algorithms cannot effectively handle shot boundaries caused by sudden lighting changes. Second, when there are dimly lighting frames in the video, these algorithms cannot perform boundary detection well. Third, when there is object or camera motion in the video, these algorithms also fail to work. To resolve these issues, we propose an SBD algorithm with color clustering changes in small regions (CCSR) to detect the shot transitions, which are abrupt changes and gradual transitions (dissolve and fade). The main idea behind the CCSR algorithm is to compute the distance of color features and to preserve the spatio-temporal information as much as possible. This model has relatively less dependence on the threshold parameters and sliding windows. Unlike other SBD algorithms, the clustering results of CCSR weaken factors such as object motion and illumination changes between adjacent frames in the video, which is helpful for reducing false detections. Furthermore, we utilize an attention mechanism in the gradual transitions to improve detection efficiency and accuracy. Finally, we evaluated the SBD algorithm, which was tested on a standard TRECVID dataset. The experimental results suggest that our algorithm yields significant improvements in precision and recall compared to the current techniques, with an average improvement of 10.35% and 8.85%, respectively. Moreover, compared with state-of-the-art algorithms, the results prove that the proposed method improves the F-score by more than 2.64% and the computation time efficiency by over 10%.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesShot boundary detection; attention mechanism; color clustering,Clustering algorithms; Color; Computational efficiency; Image segmentation; Lighting; Video signal processing; Additional key word and phrasesshot boundary detection; Attention mechanisms; Boundary detection; Boundary detection algorithms; Color clustering; Gradual transition; Key words; Object motion; Shot boundary detection; Small region; Semantics
Color-Unrelated Head-Shoulder Networks for Fine-Grained Person Re-identification,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168310366&doi=10.1145%2f3599730&partnerID=40&md5=4d4c5afb6669789ad07fb15a31e082a8,"Person re-identification (re-id) attempts to match pedestrian images with the same identity across non-overlapping cameras. Existing methods usually study person re-id by learning discriminative features based on the clothing attributes (e.g., color, texture). However, the clothing appearance is not sufficient to distinguish different persons especially when they are in similar clothes, which is known as the fine-grained (FG) person re-id problem. By contrast, this paper proposes to exploit the color-unrelated feature along with the head-shoulder feature for FG person re-id. Specifically, a color-unrelated head-shoulder network (CUHS) is developed, which is featured in three aspects: (1) It consists of a lightweight head-shoulder segmentation layer for localizing the head-shoulder region and learning the corresponding feature. (2) It exploits instance normalization (IN) for learning color-unrelated features. (3) As IN inevitably reduces inter-class differences, we propose to explore richer visual cues for IN by an attention exploration mechanism to ensure high discrimination. We evaluate our model on the FG-reID, Market1501, and DukeMTMC-reID datasets, and the results show that CUHS surpasses previous methods on both the FG and conventional person re-id problems.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",fine-grained matching; Person re-identification; visual surveillance,Color matching; Textures; Color textures; Discriminative features; Feature-based; Fine grained; Fine-grained matching; Identification problem; Non-overlapping cameras; Normalisation; Person re identifications; Visual surveillance; Color
A Siamese Inverted Residuals Network Image Steganalysis Scheme based on Deep Learning,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168237826&doi=10.1145%2f3579166&partnerID=40&md5=bd903a18641e9de289fc316269e6aa2f,"With the rapid proliferation of urbanization, massive data in social networks are collected and aggregated in real time, making it possible for criminals to use images as a cover to spread secret information on the Internet. How to determine whether these images contain secret information is a huge challenge for multimedia computing security. The steganalysis method based on deep learning can effectively judge whether the pictures transmitted on the Internet in urban scenes contain secret information, which is of great significance to safeguarding national and social security. Image steganalysis based on deep learning has powerful learning ability and classification ability, and its detection accuracy of steganography images has surpassed that of traditional steganalysis based on manual feature extraction. In recent years, it has become a hot topic of the information hiding technology. However, the detection accuracy of existing deep learning based steganalysis methods still needs to be improved, especially when detecting arbitrary-size and multi-source images, their detection efficientness is easily affected by cover mismatch. In this manuscript, we propose a steganalysis method based on Inverse Residuals structured Siamese network (abbreviated as SiaIRNet method, Siamese-Inverted-Residuals-Network Based method). The SiaIRNet method uses a siamese convolutional neural network (CNN) to obtain the residual features of subgraphs, including three stages of preprocessing, feature extraction, and classification. Firstly, a preprocessing layer with high-pass filters combined with depth-wise separable convolution is designed to more accurately capture the correlation of residuals between feature channels, which can help capture rich and effective residual features. Then, a feature extraction layer based on the Inverse Residuals structure is proposed, which improves the ability of the model to obtain residual features by expanding channels and reusing features. Finally, a fully connected layer is used to classify the cover image and the stego image features. Utilizing three general datasets, BossBase-1.01, BOWS2, and ALASKA#2, as cover images, a large number of experiments are conducted comparing with the state-of-the-art steganalysis methods. The experimental results show that compared with the classical SID method and the latest SiaStegNet method, the detection accuracy of the proposed method for 15 arbitrary-size images is improved by 15.96% and 5.86% on average, respectively, which verifies the higher detection accuracy and better adaptability of the proposed method to multi-source and arbitrary-size images in urban scenes. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Inverted Residuals; multimedia computing; siamese network; steganalysis; Urban scenes,Classification (of information); Convolution; Convolutional neural networks; Deep learning; Extraction; High pass filters; Image classification; Image enhancement; Inverse problems; Learning systems; Steganography; Cover-image; Detection accuracy; Features extraction; Image steganalysis; Inverted residual; Multimedia computing; Secret information; Siamese network; Steganalysis; Urban scenes; Feature extraction
LFR-GAN: Local Feature Refinement based Generative Adversarial Network for Text-to-Image Generation,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168241807&doi=10.1145%2f3589002&partnerID=40&md5=0ebf2ef3570204dc37da6e982cf9caab,"Text-to-image generation aims to generate images from text descriptions. Its main challenge lies in two aspects: (1) Semantic consistency, i.e., the generated images should be semantically consistent with the input text; and (2) Visual reality, i.e., the generated images should look like real images. To ensure text-image consistency, existing works mainly learn to establish the cross-modal representations via a text encoder and image encoder. However, due to the limited representation capability of the fixed-length embeddings and the flexibility of the free-form text descriptions, the learned text-to-image model is incapable of maintaining the semantic consistency between image local regions and fine-grained descriptions. As a result, the generated images sometimes miss some fine-grained attributes of the generated object, such as the color or shape of a part of the object. To address this issue, this paper proposes a Local Feature Refinement Based Generative Adversarial Network (LFR-GAN), which first divides the text into some independent fine-grained attributes and generates an initial image, then refines the image details based on these attributes. The main contributions are three-fold: (1) An attribute modeling approach is proposed to model the fine-grained text descriptions by mapping them into representations of independent attributes, which provides more fine-grained details for image generation. (2) A local feature refinement approach is proposed to enable the generated image to form a complete reflection of the fine-grained attributes contained in the text description. (3) A multi-stage generation approach is proposed to realize the fine-grained manipulation of complex images progressively, which aims to improve the performance of the refinement and generate photo-realistic images. Extensive experiments on the CUB and Oxford102 datasets show the effectiveness of our LFR-GAN approach in both text-to-image generation and text-guided image manipulation tasks. Our LFR-GAN approach shows superior performance compared to the state-of-the-art methods. The codes will be released at https://github.com/PKU-ICST-MIPL/LFR-GAN_TOMM2023. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",generative adversarial network; Local feature refinement; text-to-image generation,Image enhancement; Semantics; Signal encoding; Feature refinement; Fine grained; Image generations; Local feature; Local feature refinement; Performance; Real images; Semantic consistency; Text-to-image generation; Visual realities; Generative adversarial networks
Identity Feature Disentanglement for Visible-Infrared Person Re-Identification,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168234162&doi=10.1145%2f3595183&partnerID=40&md5=bf878d5ad58d46059d885ea442e46be0,"Visible-infrared person re-identification (VI-ReID) task aims to retrieve persons from different spectrum cameras (i.e., visible and infrared images). The biggest challenge of VI-ReID is the huge cross-modal discrepancy caused by different imaging mechanisms. Many VI-ReID methods have been proposed by embedding different modal person images into a shared feature space to narrow the cross-modal discrepancy. However, these methods ignore the purification of identity features, which results in identity features containing different modal information and failing to align well. In this article, an identity feature disentanglement method is proposed to disentangle the identity features from identity-irrelevant information, such as pose and modality. Specifically, images of different modalities are first processed to extract shared features that reduce the cross-modal discrepancy preliminarily. Then the extracted feature of each image is disentangled into a latent identity variable and an identity-irrelevant variable. In order to enforce the latent identity variable to contain as much identity information as possible and as little identity-irrelevant information, an ID-discriminative loss and an ID-swapping reconstruction process are additionally designed. Extensive quantitative and qualitative experiments on two popular public VI-ReID datasets, RegDB and SYSU-MM01, demonstrate the efficacy and superiority of the proposed method. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cross-modal; deep learning; feature disentanglement; Visible-infrared person re-identification,Infrared imaging; Cross-modal; Deep learning; Embeddings; Feature disentanglement; Feature space; Identification method; Imaging mechanism; Person re identifications; Spectra's; Visible-infrared person re-identification; Deep learning
TCSD: Triple Complementary Streams Detector for Comprehensive Deepfake Detection,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166218175&doi=10.1145%2f3558004&partnerID=40&md5=aa56e8bb402bd48d8154992bbd24bb39,"Advancements in computer vision and deep learning have made it difficult to distinguish deepfake visual media. While existing detection frameworks have achieved significant performance on challenging deepfake datasets, these approaches consider only a single perspective. More importantly, in urban scenes, neither complex scenarios can be covered by a single view nor can the correlation between multiple datasets of information be well utilized. In this article, to mine the new view for deepfake detection and utilize the correlation of multi-view information contained in images, we propose a novel triple complementary streams detector (TCSD). First, a novel depth estimator is designed to extract depth information (DI), which has not been used in previous methods. Then, to supplement depth information for obtaining comprehensive forgery clues, we consider the incoherence between image foreground and background information (FBI) and the inconsistency between local and global information (LGI). In addition, we designed an attention-based multi-scale feature extraction (MsFE) module to extract more complementary features from DI, FBI, and LGI. Finally, two attention-based feature fusion modules are proposed to adaptively fuse information. Extensive experiment results show that the proposed approach achieves state-of-the-art performance on detecting deepfakes.  © 2023 Association for Computing Machinery.",complementary information mining; Deepfake; depth information; generalization ability,Computer hardware description languages; Background information; Complementary information mining; Deepfake; Depth information; Foreground information; Generalization ability; Global informations; Information mining; Local information; Visual media; Deep learning
PANet: An End-to-end Network Based on Relative Motion for Online Multi-object Tracking,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168250238&doi=10.1145%2f3595379&partnerID=40&md5=3dd0fe1974b9904d0f56d3d589885828,"The popular tracking-by-detection paradigm of multi-object tracking (MOT) takes detections of each frame as the input and associates detections from one frame to another. Existing association methods based on the relative motion have attracted attention, because they restrain the effect of noisy detections and improve the performance of MOT. However, these methods depend only on the immediately previous frame, which may easily lead to inaccurate matches and even large accumulated errors. Furthermore, multiple objects involved in occlusions are not fully exploited in these existing methods, which leads to the aggravation of inaccurate matches. Motivated by these issues, we design the pivot to represent each object and propose a novel pivot association network (PANet) for the MOT task. Specifically, pivots are learned from spatial semantic and historical contextual clues, which alleviates the dependency on the immediately previous frame. Our online tracker PANet employs pivots and a lightweight associator to localize tracklets of objects, which can inhibit noise detections and improve the accuracy of tracklet prediction by learning the correlation responses between pivots and spatial search areas. Extensive experiments conducted on two-dimensional MOT15, MOT16, MOT17, and MOT20 demonstrate the effectiveness of the proposed method against numerous state-of-the-art MOT trackers. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",association; Multi-object tracking; pivot; tracking-by-detection,Semantics; Tracking (position); Accumulated errors; Association methods; End-to-end network; Multi-object tracking; Network-based; Performance; Pivot; Relative motion; Tracking by detections; Tracklets; Object detection
Context-Aware 3D Points of Interest Detection via Spatial Attention Mechanism,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168313617&doi=10.1145%2f3597026&partnerID=40&md5=62d918f79d7285972c49a46b40d7898d,"Detecting points of interest is a fundamental problem in 3D shape analysis and can be beneficial to various tasks in multimedia processing. Traditional learning-based detection methods usually rely on each vertex's geometric features to discriminate points of interest from other vertices. Observing that points of interest are related to not only geometric features on themselves but also the geometric features of surrounding vertices, we propose a novel context-aware 3D points of interest detection algorithm by adopting the spatial attention mechanism in this article. By designing a context attention module, our approach presents a novel deep neural network to simultaneously pay attention to the geometric features of vertices and their local contexts during extracting points of interest. To obtain satisfactory extraction results, our method adaptively assigns different weights to those features in a data-driven way. Extensive experimental results on SHREC 2007, SHREC 2011, and SHREC 2014 datasets show that our algorithm achieves superior performance over existing methods.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",3D point of interest; attention mechanism; deep learning,Feature extraction; Geometry; 3d point of interest; 3D shape analysis; Attention mechanisms; Context-Aware; Deep learning; Detecting point; Geometric feature; Interest detection; Multimedia processing; Spatial attention; Deep neural networks
Recurrent Multi-scale Approximation-Guided Network for Single Image Super-Resolution,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168237005&doi=10.1145%2f3592613&partnerID=40&md5=86c0a551e791d82ad5f6549b20aabe9c,"Single-image super-resolution (SISR) is an essential topic in computer vision applications. However, most CNN-based SISR approaches directly learn the relationship between low- and high-resolution images while ignoring the contextual texture and detail fidelity to explore super-resolution; thus, they hinder the representational power of CNNs and lead to the unrealistic, distorted reconstruction of edges and textures in the images. In this study, we propose a novel recurrent structure preservation mechanism with the integration and innovative use of multi-scale wavelet transform, Recurrent Multiscale Approximation-guided Network (RMANet), to recursively process the low-frequency and high-frequency sub-networks at each level separately. Unlike traditional wavelet transform, we propose a novel Approximation Level Preservation (ALP) architecture to import and learn the low-frequency sub-networks at each level. Through proposed Approximation level fusion (ALF) and inverse wavelet transform, rich image structures of low frequency at each level can be recursively restored and greatly preserved with the combination of ALP at each level. In addition, a novel low-frequency to high-frequency detail enhancement (DE) mechanism is also proposed to solve the problem of detail distortion in high-frequency networks by transmitting low-frequency information to the high-frequency network. Finally, a joint loss function is used to balance low-frequency and high-frequency information with different degrees of fusion. In addition to correct restoration, image details are further enhanced by tuning different hyperparameters during training. Compared with the state-of-the-art approaches, the experimental results on synthetic and real datasets demonstrate that the proposed RMANet achieves better performance in visual presentation, especially in image edges and texture details. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",approximation-guided network; multi-scale wavelet; recurrent structure preservation; Single image super-resolution,Computer vision; Image compression; Image enhancement; Image reconstruction; Image texture; Inverse problems; Optical resolving power; Restoration; Textures; Approximation-guided network; High frequency HF; Image super resolutions; Low-high; Lower frequencies; Multi-scale wavelet; Recurrent structure preservation; Single image super-resolution; Single images; Structure preservation; Wavelet transforms
Modeling Long-range Dependencies and Epipolar Geometry for Multi-view Stereo,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168307165&doi=10.1145%2f3596445&partnerID=40&md5=407195a375e6aab7fc773738180f7d58,"This article proposes a network, referred to as Multi-View Stereo TRansformer (MVSTR) for depth estimation from multi-view images. By modeling long-range dependencies and epipolar geometry, the proposed MVSTR is capable of extracting dense features with global context and 3D consistency, which are crucial for reliable matching in multi-view stereo (MVS). Specifically, to tackle the problem of the limited receptive field of existing CNN-based MVS methods, a global-context Transformer module is designed to establish intra-view long-range dependencies so that global contextual features of each view are obtained. In addition, to further enable features of each view to be 3D consistent, a 3D-consistency Transformer module with an epipolar feature sampler is built, where epipolar geometry is modeled to effectively facilitate cross-view interaction. Experimental results show that the proposed MVSTR achieves the best overall performance on the DTU dataset and demonstrates strong generalization on the Tanks & Temples benchmark dataset.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",3D consistency; epipolar geometry; feature matching; global context; long-range dependency; Multi-view stereo; Transformer,Benchmarking; Stereo image processing; 3d consistency; Depth Estimation; Epipolar geometry; Features matching; Global context; Long-range dependencies; Matchings; Multi-view image; Multi-view stereo; Transformer; Geometry
Dilated Convolution-based Feature Refinement Network for Crowd Localization,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168252123&doi=10.1145%2f3571134&partnerID=40&md5=2974001bae6e2af2fb85c2c6170272ad,"As an emerging computer vision task, crowd localization has received increasing attention due to its ability to produce more accurate spatially predictions. However, continuous scale variations in complex crowd scenes lead to tiny individuals at the edges, so that existing methods cannot achieve precise crowd localization. Aiming at alleviating the above problems, we propose a novel Dilated Convolution-based Feature Refinement Network (DFRNet) to enhance the representation learning capability. Specifically, the DFRNet is built with three branches that can capture the information of each individual in crowd scenes more precisely. More specifically, we introduce a Feature Perception Module to model long-range contextual information at different scales by adopting multiple dilated convolutions, thus providing sufficient feature information to perceive tiny individuals at the edge of images. Afterwards, a Feature Refinement Module is deployed at multiple stages of the three branches to facilitate the mutual refinement of feature information at different scales, thus further improving the expression capability of multi-scale contextual information. By incorporating the above modules, DFRNet can locate individuals in complex scenes more precisely. Extensive experiments on multiple datasets demonstrate that the proposed method has more advanced performance compared to existing methods and can be more accurately adapted to complex crowd scenes. © 2023 Association for Computing Machinery.",contextual information; crowd localization; Dilated convolution; Feature Refinement,Complex networks; Contextual information; Continuous scale; Crowd localization; Dilated convolution; Feature information; Feature refinement; Learning capabilities; Localisation; Multiple stages; Three-branch; Convolution
Improving Face Anti-spoofing via Advanced Multi-perspective Feature Learning,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168254891&doi=10.1145%2f3575660&partnerID=40&md5=841400d8a84edd260e031849c77a1052,"Face anti-spoofing (FAS) plays a vital role in securing face recognition systems. Previous approaches usually learn spoofing features from a single perspective, in which only universal cues shared by all attack types are explored. However, such single-perspective-based approaches ignore the differences among various attacks and commonness between certain attacks and bona fides, thus tending to neglect some non-universal cues that contain strong discernibility against certain types. As a result, when dealing with multiple types of attacks, the above approaches may suffer from the uncomprehensive representation of bona fides and spoof faces. In this work, we propose a novel Advanced Multi-Perspective Feature Learning network (AMPFL), in which multiple perspectives are adopted to learn discriminative features, to improve the performance of FAS. Specifically, the proposed network first learns universal cues and several perspective-specific cues from multiple perspectives, then aggregates the above features and further enhances them to perform face anti-spoofing. In this way, AMPFL obtains features that are difficult to be captured by single-perspective-based methods and provides more comprehensive information on bona fides and spoof faces, thus achieving better performance for FAS. Experimental results show that our AMPFL achieves promising results in public databases, and it effectively solves the issues of single-perspective-based approaches. © 2023 Association for Computing Machinery.",Face anti-spoofing; multi-perspective; universal cues,Machine learning; Antispoofing; Face anti-spoofing; Face recognition systems; Feature learning; Learn+; Learning network; Multi-perspective; Multiple perspectives; Performance; Universal cue; Face recognition
Unifying Dual-Attention and Siamese Transformer Network for Full-Reference Image Quality Assessment,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168308786&doi=10.1145%2f3597434&partnerID=40&md5=a1213bfee036782565db267afd02a07c,"Image Quality Assessment (IQA) is a critical task of computer vision. Most Full-Reference (FR) IQA methods have limitation in the accurate prediction of perceptual qualities of the traditional distorted images and the Generative Adversarial Networks (GANs) based distorted images. To address this issue, we propose a novel method by Unifying Dual-Attention and Siamese Transformer Network (UniDASTN) for FR-IQA. An important contribution is the spatial attention module composed of a Siamese Transformer Network and a feature fusion block. It can focus on significant regions and effectively maps the perceptual differences between the reference and distorted images to a latent distance for distortion evaluation. Another contribution is the dual-attention strategy that exploits channel attention and spatial attention to aggregate features for enhancing distortion sensitivity. In addition, a novel loss function is designed by jointly exploiting Mean Square Error (MSE), bidirectional Kullback-Leibler divergence, and rank order of quality scores. The designed loss function can offer stable training and thus enables the proposed UniDASTN to effectively learn visual perceptual image quality. Extensive experiments on standard IQA databases are conducted to validate the effectiveness of the proposed UniDASTN. The IQA results demonstrate that the proposed UniDASTN outperforms some state-of-the-art FR-IQA methods on the LIVE, CSIQ, TID2013, and PIPAL databases.  © 2023 Copyright held by the owner/author(s).",dual-attention; image quality assessment (IQA); siamese network; Transformer,Generative adversarial networks; Image quality; Distorted images; Dual-attention; Full references; Image quality assessment; Loss functions; Reference image; Siamese network; Spatial attention; Transformer; Mean square error
Weakly Supervised Hashing with Reconstructive Cross-modal Attention,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168235476&doi=10.1145%2f3589185&partnerID=40&md5=cc001fd106a77708ddba8403980de764,"On many popular social websites, images are usually associated with some meta-data such as textual tags, which involve semantic information relevant to the image and can be used to supervise the representation learning for image retrieval. However, these user-provided tags are usually polluted by noise, therefore the main challenge lies in mining the potential useful information from those noisy tags. Many previous works simply treat different tags equally to generate supervision, which will inevitably distract the network learning. To this end, we propose a new framework, termed as Weakly Supervised Hashing with Reconstructive Cross-modal Attention (WSHRCA), to learn compact visual-semantic representation with more reliable supervision for retrieval task. Specifically, for each image-tag pair, the weak supervision from tags is refined by cross-modal attention, which takes image feature as query to aggregate the most content-relevant tags. Therefore, tags with relevant content will be more prominent while noisy tags will be suppressed, which provides more accurate supervisory information. To improve the effectiveness of hash learning, the image embedding in WSHRCA is reconstructed from hash code, which is further optimized by cross-modal constraint and explicitly improves hash learning. The experiments on two widely-used datasets demonstrate the effectiveness of our proposed method for weakly-supervised image retrieval. The code is available at https://github.com/duyc168/weakly-supervised-hashing. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",attention; Weakly supervised hashing,Codes (symbols); Hash functions; Image enhancement; Semantics; Attention; Cross-modal; Image embedding; Image features; Learn+; Networks learning; Semantic representation; Semantics Information; Visual semantics; Weakly supervised hashing; Image retrieval
3V3D: Three-View Contextual Cross-slice Difference Three-dimensional Medical Image Segmentation Adversarial Network,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167364314&doi=10.1145%2f3592614&partnerID=40&md5=87b68f84492e2017a11c832d125983b8,"In three-dimensional (3D) medical image segmentation, it is still a great challenge to obtain the multidimensional feature information contained in voxel images using a single view for smaller segmentation targets, and the robustness of models obtained by relying solely on segmentation networks needs to be enhanced. In this article, we propose a three-view contextual cross-slice difference 3D segmentation adversarial network, in which three-view contextual cross-slice difference decoding blocks are introduced to improve the segmentation decoder's ability to perceive edge feature information. Meanwhile, dense skip connections are used to alleviate the problem that a large amount of shallow feature information is lost in encoding and insufficient information provided by a single long skip connection during image reconstruction. The adversarial network improves the performance of the segmentation network by distinguishing true or false for each patch of the predicted image. Further, the robustness of the segmentation model is improved in the form of adversarial training. We evaluate our model on the publicly available brain tumor BraTS2019 dataset as well as the ADNI1 dataset and achieve optimal results compared to recent excellent models.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",3D Medical image segmentation; cross-slice difference; generative adversarial networks; multi-view,Decoding; Generative adversarial networks; Image enhancement; Image segmentation; Medical image processing; 3D medical image; 3d medical image segmentation; Adversarial networks; Cross-slice difference; Feature information; Medical image segmentation; Multi-views; Three dimensional (3D) medical images; Three views; Voxel images; Image reconstruction
"A Closer Look at Debiased Temporal Sentence Grounding in Videos: Dataset, Metric, and Approach",2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166122022&doi=10.1145%2f3565573&partnerID=40&md5=fe91dc0c30f8b66042792dea111bdaed,"Temporal Sentence Grounding in Videos (TSGV), which aims to ground a natural language sentence that indicates complex human activities in an untrimmed video, has drawn widespread attention over the past few years. However, recent studies have found that current benchmark datasets may have obvious moment annotation biases, enabling several simple baselines even without training to achieve state-of-the-art (SOTA) performance. In this paper, we take a closer look at existing evaluation protocols for TSGV, and find that both the prevailing dataset splits and evaluation metrics are the devils that lead to untrustworthy benchmarking. Therefore, we propose to re-organize the two widely-used datasets, making the ground-truth moment distributions different in the training and test splits, i.e., out-of-distribution (OOD) test. Meanwhile, we introduce a new evaluation metric ""dR@n,IoU=m""that discounts the basic recall scores especially with small IoU thresholds, so as to alleviate the inflating evaluation caused by biased datasets with a large proportion of long ground-truth moments. New benchmarking results indicate that our proposed evaluation protocols can better monitor the research progress in TSGV. Furthermore, we propose a novel causality-based Multi-branch Deconfounding Debiasing (MDD) framework for unbiased moment prediction. Specifically, we design a multi-branch deconfounder to eliminate the effects caused by multiple confounders with causal intervention. In order to help the model better align the semantics between sentence queries and video moments, we enhance the representations during feature encoding. Specifically, for textual information, the query is parsed into several verb-centered phrases to obtain a more fine-grained textual feature. For visual information, the positional information has been decomposed from the moment features to enhance the representations of moments with diverse locations. Extensive experiments demonstrate that our proposed approach can achieve competitive results among existing SOTA approaches and outperform the base model with great gains. © 2023 Copyright held by the owner/author(s).",dataset bias; dataset re-splitting; evaluation metric; out-of-distribution test; Temporal sentence grounding in videos,Large dataset; Semantics; Statistical tests; Dataset bias; Dataset re-splitting; Evaluation metrics; Evaluation protocol; Ground truth; Natural languages; Out-of-distribution test; Splittings; Temporal sentence grounding in video; Video dataset; Benchmarking
Multi-Agent Semi-Siamese Training for Long-Tail and Shallow Face Learning,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168241195&doi=10.1145%2f3594669&partnerID=40&md5=14ec0af74e072a73ed7aa1e46302a1f0,"With the recent development of deep convolutional neural networks and large-scale datasets, deep face recognition has made remarkable progress and been widely used in various applications. However, unlike the existing public face datasets, in many real-world scenarios of face recognition, the depth of the training dataset is shallow, which means that only two face images are available for each ID. With the non-uniform increase of samples, such issue is converted to a more general case, known as long-tail face learning, which suffers from data imbalance and intra-class diversity dearth simultaneously. These adverse conditions damage the training and result in the decline of model performance. Based on Semi-Siamese Training, we introduce an advanced solution, named Multi-Agent Semi-Siamese Training (MASST), to address these problems. MASST includes a probe network and multiple gallery agents - the former aims to encode the probe features, and the latter constitutes a stack of networks that encode the prototypes (gallery features). For each training iteration, the gallery network, which is sequentially rotated from the stack, and the probe network form a pair of Semi-Siamese networks. We give the theoretical and empirical analysis that, given the long-tail (or shallow) data and training loss, MASST smooths the loss landscape and satisfies the Lipschitz continuity with the help of multiple agents and the updating gallery queue. The proposed method is out of extra-dependency, and thus can be easily integrated with the existing loss functions and network architectures. It is worth noting that although multiple gallery agents are employed for training, only the probe network is needed for inference, without increasing the inference cost. Extensive experiments and comparisons demonstrate the advantages of MASST for long-tail and shallow face learning. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Face recognition; long-tail face learning; shallow face learning,Convolution; Deep neural networks; Iterative methods; Large dataset; Multi agent systems; Network architecture; Probes; Convolutional neural network; Face learning; Large-scale datasets; Long tail; Long-tail face learning; Multi agent; Network scale; Real-world scenario; Shallow face learning; Face recognition
Meta-learning Advisor Networks for Long-tail and Noisy Labels in Social Image Classification,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164284481&doi=10.1145%2f3584360&partnerID=40&md5=173b8be8bf84faffa829df5706be5b6e,"Deep neural networks (DNNs) for social image classification are prone to performance reduction and overfitting when trained on datasets plagued by noisy or imbalanced labels. Weight loss methods tend to ignore the influence of noisy or frequent category examples during the training, resulting in a reduction of final accuracy and, in the presence of extreme noise, even a failure of the learning process. A new advisor network is introduced to address both imbalance and noise problems, and is able to pilot learning of a main network by adjusting the visual features and the gradient with a meta-learning strategy. In a curriculum learning fashion, the impact of redundant data is reduced while recognizable noisy label images are downplayed or redirected. Meta Feature Re-Weighting (MFRW) and Meta Equalization Softmax (MES) methods are introduced to let the main network focus only on the information in an image deemed relevant by the advisor network and to adjust the training gradient to reduce the adverse effects of frequent or noisy categories. The proposed method is first tested on synthetic versions of CIFAR10 and CIFAR100, and then on the more realistic ImageNet-LT, Places-LT, and Clothing1M datasets, reporting state-of-the-art results.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",long-tail; Meta-learning; neural networks; noisy labels,Classification (of information); Deep neural networks; Learning systems; % reductions; Images classification; Long tail; Metalearning; Neural-networks; Noisy labels; Overfitting; Performance; Social images; Weight loss method; Image classification
Design Principles for Content Creation in Location-Based Games,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164284087&doi=10.1145%2f3583689&partnerID=40&md5=13602a26c6db77cf6dea659dbc522e73,"Location-based games have been around since 2000 across various fields, including education, health, and entertainment. The main challenge facing such games is content generation. In contrast to normal games, content in location-based games is inherently dependent on location. The biggest challenge is the availability of the content globally. Other challenges include player engagement, enjoyable interactions with the real-world environment, safety, and customizability based on player performance and preference. While crowdsourcing has often been adopted as a tool for content creation, this approach requires quality control. Designing high-quality content requires detailed guidelines. In this paper, we introduce design principles for the creation of high-quality content that can survive for long periods of time. These principles are derived from ten years of experience running our in-house orienteering treasure-hunt game called O-Mopsi, which represents a case study in this paper. O-Mopsi allows players to visit pre-defined locations. The design principles are expected to be generalizable to other location-based games as well as to the creation of sightseeing tours more generally.  © 2023 Copyright held by the owner/author(s).",Content creation; location-based game (LBG); O-Mopsi; orienteering; quality evaluation; treasure hunting; user-generated content (UGC); user-generated media (UGM),Location; Content creation; Location-based game; Location-based Games; O-mopsi; Orienteering; Quality evaluation; Treasure hunting; User-generated; User-generated content; User-generated medium; Quality control
VL-NMS: Breaking Proposal Bottlenecks in Two-stage Visual-language Matching,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164277821&doi=10.1145%2f3579095&partnerID=40&md5=ec8f40a9975554a5b030cf14c424aa62,"The prevailing framework for matching multimodal inputs is based on a two-stage process: (1) detecting proposals with an object detector and (2) matching text queries with proposals. Existing two-stage solutions mostly focus on the matching step. In this article, we argue that these methods overlook an obvious mismatch between the roles of proposals in the two stages: they generate proposals solely based on the detection confidence (i.e., query-agnostic), hoping that the proposals contain all instances mentioned in the text query (i.e., query-aware). Due to this mismatch, chances are that proposals relevant to the text query are suppressed during the filtering process, which in turn bounds the matching performance. To this end, we propose VL-NMS, which is the first method to yield query-aware proposals at the first stage. VL-NMS regards all mentioned instances as critical objects and introduces a lightweight module to predict a score for aligning each proposal with a critical object. These scores can guide the NMS operation to filter out proposals irrelevant to the text query, increasing the recall of critical objects, and resulting in a significantly improved matching performance. Since VL-NMS is agnostic to the matching step, it can be easily integrated into any state-of-the-art two-stage matching method. We validate the effectiveness of VL-NMS on three multimodal matching tasks, namely referring expression grounding, phrase grounding, and image-text matching. Extensive ablation studies on several baselines and benchmarks consistently demonstrate the superiority of VL-NMS.  © 2023 Association for Computing Machinery.",image-text matching; non-maximum suppression; Text-guided region proposal generation; visual grounding,Object detection; Breakings; Image texts; Image-text matching; Matching performance; Matchings; Non-maximum suppression; Text query; Text-guided region proposal generation; Text-matching; Visual grounding; Visual languages
Neural Network Assisted Depth Map Packing for Compression Using Standard Hardware Video Codecs,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164276337&doi=10.1145%2f3588440&partnerID=40&md5=20544e57c007e9c6507d9c8575ef4b95,"Depth maps are needed by various graphics rendering and processing operations. Depth map streaming is often necessary when such operations are performed in a distributed system and it requires in most cases fast performing compression, which is why video codecs are often used. Hardware implementations of standard video codecs enable relatively high resolution and frame rate combinations, even on resource constrained devices, but unfortunately those implementations do not currently support RGB+depth extensions. However, they can be used for depth compression by first packing the depth maps into RGB or YUV frames. We investigate depth map compression using a combination of depth map packing followed by encoding with a standard video codec. We show that the precision at which depth maps are packed has a large and nontrivial impact on the resulting error caused by the combination of the packing scheme and lossy compression when the bitrate is constrained. Consequently, we propose a variable precision packing scheme assisted by a neural network model that predicts the optimal precision for each depth map given a bitrate constraint. We demonstrate that the model yields near optimal predictions and that it can be integrated into a game engine with very low overhead using modern hardware.  © 2023 Copyright held by the owner/author(s).",Depth map; game engine; neural network; video encoding,Encoding (symbols); Image compression; Interactive computer graphics; Neural networks; Video signal processing; Depthmap; Distributed systems; Game Engine; Graphics processing; Graphics rendering; Neural-networks; Processing operations; Standard hardware; Video codecs; Video encodings; Network coding
3D Object Watermarking from Data Hiding in the Homomorphic Encrypted Domain,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164296401&doi=10.1145%2f3588573&partnerID=40&md5=e3c08585b3c348c65c9883424fa99151,"For over a decade, 3D objects are an increasingly popular form of media. It has become necessary and urgent to secure them during their transmission or archiving. In this article, we propose a new method to obtain a watermarked 3D object from high-capacity data hiding in the encrypted domain. Based on the homomorphic properties of the Paillier cryptosystem, our proposed method allows us to embed several secret messages in the encrypted domain with a high-capacity. These messages can be extracted in the plain-text domain after the 3D object decryption. To the best of our knowledge, we are the first to propose a data hiding method in the encrypted domain where the high-capacity watermark is conserved in the plain-text domain after the 3D object is decrypted. The encryption and the data hiding in the encrypted domain are format compliant and without size expansion, despite the use of the Paillier cryptosystem. Each time a new message is embedded in the encrypted domain, flags are added in order to indicate which blocks are still available for the embedding of additional messages. After the decryption of a watermarked encrypted 3D object, our method produces a watermarked 3D object which is visually very similar to the original 3D object. From the decrypted watermarked 3D object, we can then extract all the embedded messages directly in the plain-text domain, without the need for an auxiliary file. Moreover, large keys are used, rending our method secure for real-life applications.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",3D object security; format compliant; high-capacity data hiding; Multimedia security; Paillier homomorphic encryption; signal processing in the encrypted domain,Signal processing; Steganography; Watermarking; 3D object; 3d object security; Data-hiding; Format-compliant; High capacity data hiding; Ho-momorphic encryptions; Homomorphic-encryptions; Multimedia security; Paillier homomorphic encryption; Signal processing in the encrypted domains; Cryptography
Less Is More: Learning from Synthetic Data with Fine-Grained Attributes for Person Re-Identification,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164292329&doi=10.1145%2f3588441&partnerID=40&md5=9154b6bf0917d519285e5308f6852178,"Person re-identification (ReID) plays an important role in applications such as public security and video surveillance. Recently, learning from synthetic data [9], which benefits from the popularity of the synthetic data engine, has attracted great attention from the public. However, existing datasets are limited in quantity, diversity, and realisticity, and cannot be efficiently used for the ReID problem. To address this challenge, we manually construct a large-scale person dataset named FineGPR with fine-grained attribute annotations. Moreover, aiming to fully exploit the potential of FineGPR and promote the efficient training from millions of synthetic data, we propose an attribute analysis pipeline called AOST based on the traditional machine learning algorithm, which dynamically learns attribute distribution in a real domain, then eliminates the gap between synthetic and real-world data and thus is freely deployed to new scenarios. Experiments conducted on benchmarks demonstrate that FineGPR with AOST outperforms (or is on par with) existing real and synthetic datasets, which suggests its feasibility for the ReID task and proves the proverbial less-is-more principle. Our synthetic FineGPR dataset is publicly available at https://github.com/JeremyXSC/FineGPR.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",efficient training; Person re-identification; synthetic data,Large dataset; Machine learning; Security systems; Data engines; Efficient training; Fine grained; Less is mores; Person re identifications; Public security; Re identifications; Security surveillance; Synthetic data; Video surveillance; Learning algorithms
Local Bidirection Recurrent Network for Efficient Video Deblurring with the Fused Temporal Merge Module,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164289401&doi=10.1145%2f3587468&partnerID=40&md5=e311ded80bd02739297a6f69d95c2ae5,"Video deblurring methods exploit the correlation between consecutive blurry inputs to generate sharp frames. However, designing an effective and efficient method is a challenging problem for video deblurring. To guarantee the effectiveness and further improve the deblurring performance, we adopt the recurrent-based method as the baseline and reconsider the recurrent mechanism as well as the temporal feature alignment in the state-of-the-art methods. For the recurrent mechanism, we add the local backward connection to the global forward recurrent backbone to effectively exploit accurate future information. For the temporal alignment, we adopt a fused temporal merge module that exploits the superiority of flow-based and kernel-based methods with progressive correlation volumes estimation. In addition, we evaluate our method with both synthetic datasets (GoPro, DVD) and a realistic dataset (BSD). The experimental results demonstrate that our method achieves significant performance improvement with a slight computational cost increase against the state-of-the-art video deblurring methods. The extended ablation studies verify the effectiveness of our model.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",fused temporal merge; local bidirection; Video deblurring,Mergers and acquisitions; Bi-direction; Deblurring; Feature alignment; Fused temporal merge; Local bidirection; Performance; Recurrent networks; State-of-the-art methods; Temporal features; Video deblurring; Recurrent neural networks
Multimodal Presentation of Interactive Audio-Tactile Graphics Supporting the Perception of Visual Information by Blind People,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164285282&doi=10.1145%2f3586076&partnerID=40&md5=53d0f3bd7938718ac3f059686c8a9e4a,"Due to the limitations in the perception of graphical information by blind people and the need to substitute the sense of sight with other senses, the correct use of multimedia in the presentation of graphics is particularly important. The aim of the authors was to correctly present visual information in the tactile and audio form and to provide contextually selected information to reduce the existing cognitive barriers. In this article, the authors decided to research the method of exploring a tactile picture by a blind person and providing contextual and semantic information about the touched image elements using the developed tool for multimodal presentation of interactive audio-tactile graphics supporting the perception of blind people. The use of multimedia should improve the perception of the conveyed content. Therefore, the effectiveness of interpreting the information contained in the tactile image is verified by analyzing the tactile, audio, and contextual perceptions during the experiments. The following issues were considered in detail from the point of view of the blind and visually impaired: the recognition of shapes of image elements depending on their size and properties, the optimal width of elements displayed on the tablet screen, the time intervals between taps on an image element, and the acceptable length of graphic element audio description. The results from this study suggest the need to select the image presentation parameters in various perception channels to the user's needs. The findings of our research on tactile shape perception of geometric figures indicate potential problems in recognizing the shapes of figures in the case of wrong preparation of tactile pictures. In the case of a small difference in the proportions of the length of the sides of the figure or a slight difference in the measures of the angles of the figure sides, we have noticed that most blind people fail to recognize its shape. Considerable progress in improving such perception can be achieved by increasing the proportions in the lengths of the sides and the measures of the angles between the sides of the figure. Further steps concern introducing an alternative audio description of the properties of the figure so as to improve the interpretability of the figure shape. For most users, the width of a virtual line in a digital image displayed on a tablet should be around 5 mm for a corresponding 1 mm tactile line. The configuration of the time intervals defining the user's gestures (two-taps, three-taps) should be about 340 ms. Applying audio descriptions to tactile picture elements improves the understanding and interpretation of the information presented on it. Most participants in the test group accepted 5 to 10 seconds of voice prompts. Longer messages were incomprehensible, and details were hard to remember. In the proposed solution, the audio description can be divided into two or three different descriptions available to the user after tapping two or three times on an image element. It allows the user to decide on the amount of contextual information needed about the touched tactile picture element. The research attempted to show the typical values of various image presentation parameters and their ranges of values and indicated effective methods of their selection for a specific system user.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Accessibility; audio-tactile graphics; multimodal interfaces; perception; touch interface; visually impaired people,Taps; User interfaces; Accessibility; Audio description; Audio-tactile graphic; Blind people; Image elements; Interactive audio; Multi-modal; Multi-modal interfaces; Touch interfaces; Visually impaired people; Semantics
Self-contained Entity Discovery from Captioned Videos,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164295065&doi=10.1145%2f3583138&partnerID=40&md5=e4f6b2b33931eb01d0614f5033f0e01c,"This article introduces the task of visual named entity discovery in videos without the need for task-specific supervision or task-specific external knowledge sources. Assigning specific names to entities (e.g., faces, scenes, or objects) in video frames is a long-standing challenge. Commonly, this problem is addressed as a supervised learning objective by manually annotating entities with labels. To bypass the annotation burden of this setup, several works have investigated the problem by utilizing external knowledge sources such as movie databases. While effective, such approaches do not work when task-specific knowledge sources are not provided and can only be applied to movies and TV series. In this work, we take the problem a step further and propose to discover entities in videos from videos and corresponding captions or subtitles. We introduce a three-stage method where we (i) create bipartite entity-name graphs from frame-caption pairs, (ii) find visual entity agreements, and (iii) refine the entity assignment through entity-level prototype construction. To tackle this new problem, we outline two new benchmarks, SC-Friends and SC-BBT, based on the Friends and Big Bang Theory TV series. Experiments on the benchmarks demonstrate the ability of our approach to discover which named entity belongs to which face or scene, with an accuracy close to a supervised oracle, just from the multimodal information present in videos. Additionally, our qualitative examples show the potential challenges of self-contained discovery of any visual entity for future work. The code and the data are available on GitHub.1  © 2023 Copyright held by the owner/author(s).",Entity discovery; multimodal video understanding; self-contained video recognition,Entity discovery; External knowledge; Knowledge sources; Multi-modal; Multimodal video understanding; Named entities; Self-contained video recognition; Video recognition; Video understanding; Visual entities
Video Captioning by Learning from Global Sentence and Looking Ahead,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164289064&doi=10.1145%2f3587252&partnerID=40&md5=fb666bf35b6a5e13414f0819211917ba,"Video captioning aims to automatically generate natural language sentences describing the content of a video. Although encoder-decoder-based models have achieved promising progress, it is still very challenging to effectively model the linguistic behavior of humans in generating video captions. In this paper, we propose a novel video captioning model by learning from gLobal sEntence and looking AheaD, LEAD for short. Specifically, LEAD consists of two modules: a Vision Module (VM) and a Language Module (LM). Thereinto, VM is a novel attention network, which can map visual features to high-level language space and model entire sentences explicitly. LM can not only effectively make use of the information of the previous sequence when generating the current word, but also have a look at the future word. Therefore, based on VM and LM, LEAD can obtain global sentence information and future word information to make video captioning more like a fill-in-the-blank task than a word-by-word sentence generation. In addition, we also propose an autonomous strategy and a multi-stage training scheme to optimize the model, which can mitigate the problem of information leakage. Extensive experiments show that LEAD outperforms some state-of-the-art methods on MSR-VTT, MSVD, and VATEX, demonstrating the effectiveness of the proposed approach in video captioning. In addition, we release the code of our proposed model to be publicly available.1  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",future word; global sentence features; Video captioning,Natural language processing systems; Encoder-decoder; Future word; Global sentence feature; Higher-level languages; Natural languages; Sentence features; Video captioning; Video captions; Vision modules; Visual feature; Visual languages
A Geometrical Approach to Evaluate the Adversarial Robustness of Deep Neural Networks,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162816436&doi=10.1145%2f3587936&partnerID=40&md5=f718ac5dc96dd8dacfb6c7c726c52923,"Deep neural networks (DNNs) are widely used for computer vision tasks. However, it has been shown that deep models are vulnerable to adversarial attacks - that is, their performances drop when imperceptible perturbations are made to the original inputs, which may further degrade the following visual tasks or introduce new problems such as data and privacy security. Hence, metrics for evaluating the robustness of deep models against adversarial attacks are desired. However, previous metrics are mainly proposed for evaluating the adversarial robustness of shallow networks on the small-scale datasets. Although the Cross Lipschitz Extreme Value for nEtwork Robustness (CLEVER) metric has been proposed for large-scale datasets (e.g., the ImageNet dataset), it is computationally expensive and its performance relies on a tractable number of samples. In this article, we propose the Adversarial Converging Time Score (ACTS), an attack-dependent metric that quantifies the adversarial robustness of a DNN on a specific input. Our key observation is that local neighborhoods on a DNN's output surface would have different shapes given different inputs. Hence, given different inputs, it requires different time for converging to an adversarial sample. Based on this geometry meaning, the ACTS measures the converging time as an adversarial robustness metric. We validate the effectiveness and generalization of the proposed ACTS metric against different adversarial attacks on the large-scale ImageNet dataset using state-of-the-art deep networks. Extensive experiments show that our ACTS metric is an efficient and effective adversarial metric over the previous CLEVER metric. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Adversarial robustness; deep neural network (DNN); image classification,Computer vision; Image classification; Large dataset; Vision; Adversarial robustness; Deep neural network; Extreme value; Geometrical approaches; Images classification; Lipschitz; Network robustness; Performance; Robustness metrics; Visual tasks; Deep neural networks
TEVL: Trilinear Encoder for Video-language Representation Learning,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164289947&doi=10.1145%2f3585388&partnerID=40&md5=43137b4000898864cf0f2dc35e6e1e97,"Pre-training model on large-scale unlabeled web videos followed by task-specific fine-tuning is a canonical approach to learning video and language representations. However, the accompanying Automatic Speech Recognition (ASR) transcripts in these videos are directly transcribed from audio, which may be inconsistent with visual information and would impair the language modeling ability of the model. Meanwhile, previous V-L models fuse visual and language modality features using single- or dual-stream architectures, which are not suitable for the current situation. Besides, traditional V-L research focuses mainly on the interaction between vision and language modalities and leaves the modeling of relationships within modalities untouched. To address these issues and maintain a small manual labor cost, we add automatically extracted dense captions as a supplementary text and propose a new trilinear video-language interaction framework TEVL (Trilinear Encoder for Video-Language representation learning). TEVL contains three unimodal encoders, a TRIlinear encOder (TRIO) block, and a temporal Transformer. TRIO is specially designed to support effective text-vision-text interaction, which encourages inter-modal cooperation while maintaining intra-modal dependencies. We pre-train TEVL on the HowTo100M and TV datasets with four task objectives. Experimental results demonstrate that TEVL can learn powerful video-text representation and achieve competitive performance on three downstream tasks, including multimodal video captioning, video Question Answering (QA), as well as video and language inference. Implementation code is available at https://github.com/Gufrannn/TEVL.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",pre-training techniques; Self-supervised learning; trilinear encoder; vision and language (V-L) representation learning,Computational linguistics; Computer hardware description languages; Machine learning; Modeling languages; Signal encoding; Speech recognition; Visual languages; Wages; Language model; Large-scales; Pre-training; Pre-training technique; Self-supervised learning; Training model; Training techniques; Trilinear encoder; Vision and language  representation learning; Web video; Learning systems
Counterfactual Scenario-relevant Knowledge-enriched Multi-modal Emotion Reasoning,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164279199&doi=10.1145%2f3583690&partnerID=40&md5=768f06386da45d6d9197021e5e300a84,"Multi-modal video emotion reasoning (MERV) has recently attracted increasing attention due to its potential application in human-computer interaction. This task needs to not only recognize utterance-level emotions for conspicuous speakers, but also perceive the emotions of non-speakers in videos. Existing methods focus on modeling multi-modal multi-level contexts to capture emotion-relevant clues from the complex scenarios in videos. However, the context information is far from enough to infer the emotion labels of non-speakers due to the large gap between the scenario situation and emotions labels. Inspired by the observation that humans can find solutions to complex problems with the leverage of experience and knowledge, we propose SK-MER, a Scenario-relevant Knowledge-enhanced Multi-modal Emotion Reasoning framework for MERV task, which can leverage external knowledge to enhance the video scenario understanding and emotion reasoning. Specifically, we use scenario concepts extracted from videos to build knowledge subgraphs from external knowledge bases. The knowledge subgraphs are then utilized to obtain scenario-relevant knowledge representations through dynamic knowledge graph attention. Next, we incorporate the knowledge representations into context modeling to enhance emotion reasoning with external scenario-relevant knowledge. In addition, we propose a counterfactual knowledge representation learning approach to obtain more effective scenario-relevant knowledge representations. Extensive experimental results on MEmoR dataset show that the proposed SK-MER framework achieves new state-of-the-art results.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",counterfactual; emotion reasoning; knowledge enhancement; Neural networks,Complex networks; Human computer interaction; Knowledge graph; Knowledge management; Counterfactuals; Emotion reasoning; External knowledge; Knowledge enhancement; Knowledge-representation; Multi-level contexts; Multi-modal; Neural-networks; Subgraphs; Task-needs; Behavioral research
AMC: Adaptive Multi-expert Collaborative Network for Text-guided Image Retrieval,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167562519&doi=10.1145%2f3584703&partnerID=40&md5=1d64c5066dc116f22cfb69768ed82082,"Text-guided image retrieval integrates reference image and text feedback as a multimodal query to search the image corresponding to user intention. Recent approaches employ multi-level matching, multiple accesses, or multiple subnetworks for better performance regardless of the heavy burden of storage and computation in the deployment. Additionally, these models not only rely on expert knowledge to handcraft image-text composing modules but also do inference by the static computational graph. It limits the representation capability and generalization ability of networks in the face of challenges from complex and varied combinations of reference image and text feedback. To break the shackles of the static network concept, we introduce the dynamic router mechanism to achieve data-dependent expert activation and flexible collaboration of multiple experts to explore more implicit multimodal fusion patterns. Specifically, we construct AMC, our Adaptive Multi-expert Collaborative network, by using the proposed router to activate the different experts with different levels of image-text interaction. Since routers can dynamically adjust the activation of experts for the current samples, AMC can achieve the adaptive fusion mode for the different reference image and text combinations and generate dynamic computational graphs according to varied multimodal queries. Extensive experiments on two benchmark datasets demonstrate that due to benefits from the image-text composing representation produced by an adaptive multi-expert collaboration mechanism, AMC has better retrieval performance and zero-shot generalization ability than the state-of-the-art method while keeping the lightweight model and fast retrieval speed. Moreover, we analyze the visualization of path activation, attention map, and retrieval results to further understand the routing decisions and semantic localization ability of AMC. The codes and pretrained models are available at https://github.com/KevinLight831/AMC. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",mixture-of-experts; multimodal fusion; Text-guided image retrieval,Benchmarking; Chemical activation; Digital storage; Dynamics; Image fusion; Image retrieval; Zero-shot learning; Collaborative network; Computational graph; Guided images; Image texts; Mixture of experts; Multi-expert; Multi-modal fusion; Multi-modal queries; Reference image; Text-guided image retrieval; Semantics
Visual Paraphrase Generation with Key Information Retained,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159172681&doi=10.1145%2f3585010&partnerID=40&md5=7a8e61f6228ae3ff42a67498ef103d27,"Visual paraphrase generation task aims to rewrite a given image-related original sentence into a new paraphrase, where the paraphrase needs to have the same expressed meaning as the original sentence but have a difference in expression form. Existing studies mainly extract two semantic vectors to represent the entire image and the entire original sentence, respectively, for paraphrase generation. However, these semantic vectors for an image or a sentence may lead to the model failing to focus on some key objects in the original sentence, which may generate semantically inconsistent sentences by changing key object information. In this article, we propose an object-level paraphrase generation model, which generates paraphrases by adjusting the permutation of key objects and modifying their associated descriptions. To adjust the permutation of key objects, an object-sorting module aims to obtain new object sequences based on the key object information and original sentences. Then, a sequence generation module sequentially generates paraphrases based on the permutation of the newly object sequences. Each generation step focuses on different image features associated with different key objects to generate descriptions with differences. Furthermore, we use a semantic discriminator module to promote the generated paraphrase to be semantically close to the original sentence. Specifically, the loss function of the discriminator penalizes the excessive distance between the paraphrase and the original sentence. Extensive experiments on the MS COCO dataset show that the proposed model outperforms the baselines. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesMultimodal; visual paraphrase generation; VisualBERT,Additional key word and phrasesmultimodal; Image features; Key object; Key words; Object information; Object sorting; Semantic vectors; Sequence generation; Visual paraphrase generation; Visualbert; Semantics
Affective Feedback Synthesis Towards Multimodal Text and Image Data,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168249855&doi=10.1145%2f3589186&partnerID=40&md5=9bf17b8d13c2aa624f6e75a8fde1edbe,"In this article, we have defined a novel task of affective feedback synthesis that generates feedback for input text and corresponding images in a way similar to humans responding to multimodal data. A feedback synthesis system has been proposed and trained using ground-truth human comments along with image-text input. We have also constructed a large-scale dataset consisting of images, text, Twitter user comments, and the number of likes for the comments by crawling news articles through Twitter feeds. The proposed system extracts textual features using a transformer-based textual encoder. The visual features have been extracted using a Faster region-based convolutional neural networks model. The textual and visual features have been concatenated to construct multimodal features that the decoder uses to synthesize the feedback. We have compared the results of the proposed system with baseline models using quantitative and qualitative measures. The synthesized feedbacks have been analyzed using automatic and human evaluation. They have been found to be semantically similar to the ground-truth comments and relevant to the given text-image input. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Affective computing; context vector; dataset construction; feedback synthesis; multimodal input,Convolutional neural networks; Image processing; Large dataset; Affective Computing; Context vector; Dataset construction; Feedback synthesis; Ground truth; Image texts; Multi-modal; Multimodal inputs; Textual features; Visual feature; Social networking (online)
Transformer-Based Visual Grounding with Cross-Modality Interaction,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167562650&doi=10.1145%2f3587251&partnerID=40&md5=fdc847961c264ba9bcc43c96a424e664,"This article tackles the challenging yet important task of Visual Grounding (VG), which aims to localize a visual region in the given image referred by a natural language query. Existing efforts on the VG task are twofold: (1) two-stage methods first extract region proposals and then rank them according to their similarities with the referring expression, which usually leads to suboptimal results due to the quality of region proposals; (2) one-stage methods usually predict all the possible coordinates of the target region online by leveraging modern object detection architectures, which pay little attention to cross-modality correlations and have limited generalization ability. To better address the task, we present an effective transformer-based end-to-end visual grounding approach, which focuses on capturing the cross-modality correlations between the referring expression and visual regions for accurately reasoning the location of the target region. Specifically, our model consists of a feature encoder, a cross-modality interactor, and a modality-agnostic decoder. The feature encoder is employed to capture the intra-modality correlation, which models the linguistic context in query and the spatial dependency in image respectively. The cross-modality interactor endows the model with the capability of highlighting the localization-relevant visual and textual cues by mutual verification of vision and language, which plays a key role in our model. The decoder learns a consolidated token representation enriched by multi-modal contexts and further directly predicts the box coordinates. Extensive experiments on five public benchmark datasets with quantitative and qualitative analysis clearly demonstrate the effectiveness and rationale of our proposed method. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cross-modality interaction; referring expression; Visual Grounding,Decoding; Linguistics; Natural language processing systems; Object detection; Signal encoding; Cross modality; Cross-modality interaction; Interactors; Natural language queries; Objects detection; One-stage method; Referring expressions; Target regions; Two-stage methods; Visual grounding; Visual languages
Novel View Synthesis from a Single Unposed Image via Unsupervised Learning,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168254297&doi=10.1145%2f3587467&partnerID=40&md5=e1d507b35a29cb372958e16232bbe433,"Novel view synthesis aims to generate novel views from one or more given source views. Although existing methods have achieved promising performance, they usually require paired views with different poses to learn a pixel transformation. This article proposes an unsupervised network to learn such a pixel transformation from a single source image. In particular, the network consists of a token transformation module that facilities the transformation of the features extracted from a source image into an intrinsic representation with respect to a pre-defined reference pose and a view generation module that synthesizes an arbitrary view from the representation. The learned transformation allows us to synthesize a novel view from any single source image of an unknown pose. Experiments on the widely used view synthesis datasets have demonstrated that the proposed network is able to produce comparable results to the state-of-the-art methods despite the fact that learning is unsupervised and only a single source image is required for generating a novel view. The code will be available upon the acceptance of the article. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",3D display; Multimedia applications; token transformation module; unsupervised single-view synthesis; view generation module,Learning systems; Pixels; Unsupervised learning; 3-D displays; 3D-displays; Multimedia applications; Source images; Token transformation module; Transformation modules; Unsupervised single-view synthesis; View generation; View generation module; View synthesis; Three dimensional displays
Learning Disentangled Features for Person Re-identification under Clothes Changing,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168243851&doi=10.1145%2f3584359&partnerID=40&md5=250a52a62f69e0db5beed25f6c2f7b27,"Clothes changing is one of the challenges in person re-identification (ReID), since clothes provide remarkable and reliable information for decision, especially when the resolution of an image is low. Variation of clothes significantly downgrades standard ReID models, since the clothes information dominates the decisions. The performance of the existing methods considering clothes changing is still not satisfying, since they fail to extract sufficient identity information that excludes clothes information. This study aims to disentangle identity, clothes, and unrelated features with a Generative Adversarial Network (GAN). A GAN model with three encoders, one generator, and three discriminators, and its training procedure are proposed to learn these kinds of features separately and exclusively. Experimental results indicate that our model generally achieves the best performance among state-of-the-art methods in both ReID tasks with and without clothes changing, which confirms that the identity, clothes, and unrelated features are extracted by our model more precisely and effectively. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",clothes changing; feature disentanglement; Person re-identification,Clothes changing; Feature disentanglement; Identification modeling; Identity information; Learn+; Network models; Performance; Person re identifications; Re identifications; Training procedures; Generative adversarial networks
Unsupervised Discovery and Manipulation of Continuous Disentangled Factors of Variation,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167588066&doi=10.1145%2f3591358&partnerID=40&md5=43910c975200a47670be13b831fec47a,"Learning a disentangled representation of a distribution in a completely unsupervised way is a challenging task that has drawn attention recently. In particular, much focus has been put in separating factors of variation (i.e., attributes) within the latent code of a Generative Adversarial Network (GAN). Achieving that permits control of the presence or absence of those factors in the generated samples by simply editing a small portion of the latent code. Nevertheless, existing methods that perform very well in a noise-to-image setting often fail when dealing with a real data distribution, i.e., when the discovered attributes need to be applied to real images. However, some methods are able to extract and apply a style to a sample but struggle to maintain its content and identity, while others are not able to locally apply attributes and end up achieving only a global manipulation of the original image. In this article, we propose a completely (i.e., truly) unsupervised method that is able to extract a disentangled set of attributes from a data distribution and apply them to new samples from the same distribution by preserving their content. This is achieved by using an image-to-image GAN that maps an image and a random set of continuous attributes to a new image that includes those attributes. Indeed, these attributes are initially unknown and they are discovered during training by maximizing the mutual information between the generated samples and the attributes' vector. Finally, the obtained disentangled set of continuous attributes can be used to freely manipulate the input samples. We prove the effectiveness of our method over a series of datasets and show its application on various tasks, such as attribute editing, data augmentation, and style transfer. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep learning; image manipulation; mutual information maximization; unsupervised learning,Codes (symbols); Data mining; Deep learning; Unsupervised learning; Continuous attribute; Data distribution; Deep learning; Image manipulation; Mutual information maximization; Mutual informations; Original images; Random set; Real images; Unsupervised method; Generative adversarial networks
Cross-scale Graph Interaction Network for Semantic Segmentation of Remote Sensing Images,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166386541&doi=10.1145%2f3558770&partnerID=40&md5=1a868b7dbd00d5a1165a0de8ca61aecb,"Semantic segmentation of remote sensing (RS) images plays a vital role in a variety of fields, including urban planning, natural disaster monitoring, and land resource management. Due to the complexity and low resolution of RS images, many approaches have been proposed to handle the related task. However, these previously developed approaches dedicate to contextual interaction but ignore the cross-scale semantic correlation and multi-scale boundary information. Therefore, we propose a Cross-scale Graph Interaction Network (CGIN) to address semantic segmentation problems of RS images, which consists of a semantic branch and a boundary branch. In the semantic branch, we first apply atrous convolution to extract multi-scale semantic features of RS images. Particularly, based on the multi-scale semantic features, a Cross-scale Graph Interaction (CGI) module is introduced, which establishes cross-scale graph structures and performs adaptive graph reasoning to capture the cross-scale semantic correlation of RS objects. In the boundary branch, we propose a Multi-scale Boundary Feature Extraction (MBFE) module that utilizes atrous convolutions with different dilation rates to extract multi-scale boundary features. Finally, to address the problem of sparse boundary pixels in the fusion process of the two branches, we propose a Multi-scale Similarity-guided Aggregation (MSA) module by calculating the similarity of semantic features and boundary features at the corresponding scale, which can emphasize the boundary information in semantic features. Our proposed CGIN outperforms state-of-the-art approaches in numerical experiments conducted on two benchmark remote sensing datasets.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesRemote sensing; boundary; cross-scale; graph convolutional network; semantic segmentation,Convolution; Disasters; Remote sensing; Semantic Segmentation; Semantic Web; Additional key word and phrasesremote sensing; Boundary; Convolutional networks; Cross-scale; Graph convolutional network; Key words; Multi-scales; Remote sensing images; Semantic features; Semantic segmentation; Semantics
Semantic Enhanced Video Captioning with Multi-feature Fusion,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167567052&doi=10.1145%2f3588572&partnerID=40&md5=936f83306e7e9bb0bdb787bfcb886b56,"Video captioning aims to automatically describe a video clip with informative sentences. At present, deep learning-based models have become the mainstream for this task and achieved competitive results on public datasets. Usually, these methods leverage different types of features to generate sentences, e.g., semantic information, 2D or 3D features. However, some methods only treat semantic information as a complement of visual representations and cannot fully exploit it; some of them ignore the relationship between different types of features. In addition, most of them select multiple frames of a video with an equally spaced sampling scheme, resulting in much redundant information. To address these issues, we present a novel video-captioning framework, Semantic Enhanced video captioning with Multi-feature Fusion, SEMF for short. It optimizes the use of different types of features from three aspects. First, a semantic encoder is designed to enhance meaningful semantic features through a semantic dictionary to boost performance. Second, a discrete selection module pays attention to important features and obtains different contexts at different steps to reduce feature redundancy. Finally, a multi-feature fusion module uses a novel relation-aware attention mechanism to separate the common and complementary components of different features to provide more effective visual features for the next step. Moreover, the entire framework can be trained in an end-to-end manner. Extensive experiments are conducted on Microsoft Research Video Description Corpus (MSVD) and MSR-Video to Text (MSR-VTT) datasets. The results demonstrate that SEMF is able to achieve state-of-the-art results. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",discrete selection; multi-feature fusion; semantic encoder; Video captioning,Deep learning; Feature extraction; Signal encoding; Discrete selection; Learning Based Models; Multi-feature fusion; Multiple-frame; Public dataset; Semantic encoder; Semantics Information; Video captioning; Video-clips; Visual representations; Semantics
Deep Convolutional Pooling Transformer for Deepfake Detection,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168242453&doi=10.1145%2f3588574&partnerID=40&md5=60303bda8784b4fe6f3cab60af955412,"Recently, Deepfake has drawn considerable public attention due to security and privacy concerns in social media digital forensics. As the wildly spreading Deepfake videos on the Internet become more realistic, traditional detection techniques have failed in distinguishing between real and fake. Most existing deep learning methods mainly focus on local features and relations within the face image using convolutional neural networks as a backbone. However, local features and relations are insufficient for model training to learn enough general information for Deepfake detection. Therefore, the existing Deepfake detection methods have reached a bottleneck to further improve the detection performance. To address this issue, we propose a deep convolutional Transformer to incorporate the decisive image features both locally and globally. Specifically, we apply convolutional pooling and re-attention to enrich the extracted features and enhance efficacy. Moreover, we employ the barely discussed image keyframes in model training for performance improvement and visualize the feature quantity gap between the key and normal image frames caused by video compression. We finally illustrate the transferability with extensive experiments on several Deepfake benchmark datasets. The proposed solution consistently outperforms several state-of-the-art baselines on both within- and cross-dataset experiments. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deepfake detection; image keyframes; transformer,Convolution; Convolutional neural networks; Deep learning; Fake detection; Image compression; Image enhancement; Learning systems; Deepfake detection; Image keyframe; Key-frames; Learning methods; Local feature; Model training; Privacy concerns; Security and privacy; Social media; Transformer; Digital forensics
Complementary Feature Pyramid Network for Object Detection,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168247120&doi=10.1145%2f3584362&partnerID=40&md5=8eecac7ccfd95d4cd955df0f1bf69da7,"The way of constructing a robust feature pyramid is crucial for object detection. However, existing feature pyramid methods, which aggregate multi-level features by using element-wise sum or concatenation, are inefficient to construct a robust feature pyramid. The reason is that these methods cannot be effective in discriminating the relevant semantics of objects. In this article, we propose a Complementary Feature Pyramid Network (CFPN) to aggregate multi-level features selectively and efficiently by exploring complementary information between multi-level features. Specifically, a Spatial Complementary Module (SCM) and a Channel Complementary Module (CCM) are designed and embedded in CFPN to enhance useful information and suppress irrelevant information during feature fusions along spatial and channel dimensions, respectively. CFPN is a generic feature extractor, as evidenced by its seamless integration into single-stage, two-stage, and end-to-end object detectors. Experiments conducted on the COCO and Pascal VOC datasets demonstrate that integrating our CFPN into RetinaNet, Faster RCNN, Cascade RCNN, and Sparse RCNN obtains consistent performance improvements with negligible overheads. Code and models are available at: https://github.com/VIPLab-CQU/CFPN. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Feature pyramid networks; Multi-scale object detection; Object detection,Aggregates; Feature extraction; Object recognition; Semantics; Complementary features; Feature pyramid; Feature pyramid network; Features fusions; Multi-scale object detection; Multi-scales; Multilevels; Objects detection; Pyramid network; Object detection
Low-light Image Enhancement via a Frequency-based Model with Structure and Texture Decomposition,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150543922&doi=10.1145%2f3590965&partnerID=40&md5=9a22ec450df274eeb9bbe5ade61aaf38,"This article proposes a frequency-based structure and texture decomposition model in a Retinex-based framework for low-light image enhancement and noise suppression. First, we utilize the total variation-based noise estimation to decompose the observed image into low-frequency and high-frequency components. Second, we use a Gaussian kernel for noise suppression in the high-frequency layer. Third, we propose a frequency-based structure and texture decomposition method to achieve low-light enhancement. We extract texture and structure priors by using the high-frequency layer and a low-frequency layer, respectively. We present an optimization problem and solve it with the augmented Lagrange multiplier to generate a balance between structure and texture in the reflectance map. Our experimental results reveal that the proposed method can achieve superior performance in naturalness preservation and detail retention compared with state-of-the-art algorithms for low-light image enhancement. Our code is available on the following website1. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",denosing; low-light image enhancement; Retinex theory,Frequency estimation; Image denoising; Image texture; Lagrange multipliers; Spurious signal noise; Textures; De-nosing; Decomposition model; Enhancement/suppression; High frequency HF; Low-light image enhancement; Low-light images; Lower frequencies; Noise suppression; Retinex; Retinex theory; Image enhancement
CD2: Fine-grained 3D Mesh Reconstruction with Twice Chamfer Distance,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166388005&doi=10.1145%2f3582694&partnerID=40&md5=05eb0380466b10350da9364f68300ba7,"Monocular 3D reconstruction is to reconstruct the shape of object and its other information from a single RGB image. In 3D reconstruction, polygon mesh, with detailed surface information and low computational cost, is the most prevalent expression form obtained from deep learning models. However, the state-of-the-art schemes fail to directly generate well-structured meshes, and we identify that most meshes have severe Vertices Clustering (VC) and Illegal Twist (IT) problems. By analyzing the mesh deformation process, we pinpoint that the inappropriate usage of Chamfer Distance (CD) loss is a root cause of VC and IT problems in deep learning model. In this article, we initially demonstrate these two problems induced by CD loss with visual examples and quantitative analyses. Then, we propose a fine-grained reconstruction method CD2 by employing Chamfer distance twice to perform a plausible and adaptive deformation. Extensive experiments on two 3D datasets and comparisons with five latest schemes demonstrate that our CD2 directly generates a well-structured mesh and outperforms others in terms of several quantitative metrics.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",3D reconstruction; chamfer distance; machine learning; mesh deformation,Image reconstruction; Learning systems; Mesh generation; Three dimensional computer graphics; 3D meshes; 3D reconstruction; Chamfer distance; Fine grained; Learning models; Machine-learning; Mesh deformation; Mesh reconstruction; Structured mesh; Vertex-clustering; Deep learning
Self-supervised Image-based 3D Model Retrieval,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154066644&doi=10.1145%2f3548690&partnerID=40&md5=ea7878fed803b2b0479fa8ebcd3e5382,"Image-based 3D model retrieval aims at organizing unlabeled 3D models according to the relevance to the labeled 2D images. With easy accessibility of 2D images and wide applications of 3D models, image-based 3D model retrieval attracts more and more attentions. However, it is still a challenging problem due to the modality gap between 2D images and 3D models. In spite of the remarkable progress brought by domain adaptation techniques for this research topic, which usually propose to align the global distribution statistics of two domains, these methods are limited in learning discriminative features for target samples due to the lack of label information in target domain. In this article, besides utilizing the label information of 2D image domain and the adversarial domain alignment, we additionally incorporate self-supervision to address cross-domain 3D model retrieval problem. Specifically, we simultaneously optimize the adversarial adaptation for both domains based on visual features and the contrastive learning for unlabeled 3D model domain to help the feature extractor to learn discriminative feature representations. The contrastive learning is used to map view representations of the identical model nearby while view representations of different models far apart. To guarantee adequate and high-quality negative samples for contrastive learning, we design a memory bank to store and update representative view for each 3D model based on entropy minimization principle. Comprehensive experimental results on the public image-based 3D model retrieval datasets, i.e., MI3DOR and MI3DOR-2, have demonstrated the effectiveness of the proposed method.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",contrastive learning; discriminative feature representation; domain adaptation; Image-based 3D model retrieval; self-supervised learning,3D modeling; Learning systems; Self organizing maps; Three dimensional computer graphics; 3D model retrieval; 3d-modeling; Contrastive learning; Discriminative feature representation; Discriminative features; Domain adaptation; Feature representation; Image-based; Image-based 3d model retrieval; Self-supervised learning; Entropy
Frequency-Aware Camouflaged Object Detection,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163834552&doi=10.1145%2f3545609&partnerID=40&md5=f8150eb43cb62e5fd90c60d8990b4b62,"Camouflaged object detection (COD) is important as it has various potential applications. Unlike salient object detection (SOD), which tries to identify visually salient objects, COD tries to detect objects that are visually very similar to the surrounding background. We observe that recent COD methods try to fuse features from different levels using some context aggregation strategies originally developed for SOD. Such an approach, however, may not be appropriate for COD as these existing context aggregation strategies are good at detecting distinctive objects while weakening the features from less discriminative objects. To address this problem, we propose in this article to exploit frequency learning to suppress the confusing high-frequency texture information, to help separate camouflaged objects from their surrounding background, and a frequency-based method, called FBNet, for camouflaged object detection. Specifically, we design a frequency-Aware context aggregation (FACA) module to suppress high-frequency information and aggregate multi-scale features from a frequency perspective, an adaptive frequency attention (AFA) module to enhance the features of the learned important frequency components, and a gradient-weighted loss function to guide the proposed method to pay more attention to contour details. Experimental results show that our model outperforms relevant state-of-The-Art methods.  © 2023 Association for Computing Machinery.",Camouflaged object detection; frequency learning,Object recognition; Textures; Aggregation strategy; Camouflaged object detection; Frequency learning; High frequency HF; High-frequency informations; Object detection method; Objects detection; Salient object detection; Salient objects; Texture information; Object detection
Multi-Source Knowledge Reasoning Graph Network for Multi-Modal Commonsense Inference,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164274383&doi=10.1145%2f3573201&partnerID=40&md5=a61524aab5e47dc4593de8058bd2db99,"As a crucial part of natural language processing, event-centered commonsense inference task has attracted increasing attention. With a given observed event, the intention and reaction of the people involved in the event are required to be inferred with artificial intelligent algorithms. To solve this problem, sequence-to-sequence methods are widely studied, where the event is first encoded into a specific representation and then decoded to generate the results. However, all the existing methods learn the event representation only with the textual information, while the visual information is ignored, which is actually helpful for the commonsense reference. In this article, we first define a new task of multi-modal commonsense reference with both textual and visual information. A new event-centered multi-modal dataset is also provided. Then we propose a multi-source knowledge reasoning graph network to solve this task, where three kinds of relational knowledge are considered. Multi-modal correlations are learned to get the event's multi-modal representation from a global perspective. Intra-event object relations are explored to capture the fine-grained event feature with an object graph. Inter-event semantic relations are also explored through the external knowledge to understand the semantic associations among events with an event graph. We conduct extensive experiments on the new dataset, and the results show the effectiveness of our method.  © 2023 Association for Computing Machinery.",graph neural network; Knowledge reasoning; multi-modal commonsense inference,Natural language processing systems; Semantics; Graph networks; Graph neural networks; Knowledge reasoning; Language processing; Multi-modal; Multi-modal commonsense inference; Multi-Sources; Natural languages; Textual information; Visual information; Graph neural networks
EMES: Efficient Multi-encoding Schemes for HEVC-based Adaptive Bitrate Streaming,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160328738&doi=10.1145%2f3575659&partnerID=40&md5=c33fd4822d6a747097a48df1bce84d03,"In HTTP Adaptive Streaming (HAS), videos are encoded at multiple bitrates and spatial resolutions (i.e., representations) to adapt to the heterogeneity of network conditions, device attributes, and end-user preferences. Encoding the same video segment at multiple representations increases costs for content providers. State-of-the-art multi-encoding schemes improve the encoding process by utilizing encoder analysis information from already encoded representation(s) to reduce the encoding time of the remaining representations. These schemes typically use the highest bitrate representation as the reference to accelerate the encoding of the remaining representations. Nowadays, most streaming services utilize cloud-based encoding techniques, enabling a fully parallel encoding process to reduce the overall encoding time. The highest bitrate representation has a higher encoding time than the other representations. Thus, utilizing it as the reference encoding is unfavorable in a parallel encoding setup as the overall encoding time is bound by its encoding time. This article provides a comprehensive study of various multi-rate and multi-encoding schemes in both serial and parallel encoding scenarios. Furthermore, it introduces novel heuristics to limit the Rate Distortion Optimization (RDO) process across various representations. Based on these heuristics, three multi-encoding schemes are proposed, which rely on encoder analysis sharing across different representations: (i) optimized for the highest compression efficiency, (ii) optimized for the best compression efficiency-encoding time savings tradeoff, and (iii) optimized for the best encoding time savings. Experimental results demonstrate that the proposed multi-encoding schemes (i), (ii), and (iii) reduce the overall serial encoding time by 34.71%, 45.27%, and 68.76% with a 2.3%, 3.1%, and 4.5% bitrate increase to maintain the same VMAF, respectively, compared to stand-alone encodings. The overall parallel encoding time is reduced by 22.03%, 20.72%, and 76.82% compared to stand-alone encodings for schemes (i), (ii), and (iii), respectively. © 2023 Copyright held by the owner/author(s).",HEVC; HTTP Adaptive Streaming; multi-encoding; multi-rate encoding,Efficiency; Electric distortion; Encoding (symbols); Image coding; Optimization; Signal distortion; Signal encoding; Adaptive streaming; Bit rates; Encoding schemes; Encoding time; Encodings; HEVC; HTTP adaptive streaming; Multi rate; Multi-encoding; Multi-rate encoding; HTTP
Distilled Meta-learning for Multi-Class Incremental Learning,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177975253&doi=10.1145%2f3576045&partnerID=40&md5=82c2bcc2277963b4b9f611864277ffa7,"Meta-learning approaches have recently achieved promising performance in multi-class incremental learning. However, meta-learners still suffer from catastrophic forgetting, i.e., they tend to forget the learned knowledge from the old tasks when they focus on rapidly adapting to the new classes of the current task. To solve this problem, we propose a novel distilled meta-learning (DML) framework for multi-class incremental learning that integrates seamlessly meta-learning with knowledge distillation in each incremental stage. Specifically, during inner-loop training, knowledge distillation is incorporated into the DML to overcome catastrophic forgetting. During outer-loop training, a meta-update rule is designed for the meta-learner to learn across tasks and quickly adapt to new tasks. By virtue of the bilevel optimization, our model is encouraged to reach a balance between the retention of old knowledge and the learning of new knowledge. Experimental results on four benchmark datasets demonstrate the effectiveness of our proposal and show that our method significantly outperforms other state-of-the-art incremental learning methods.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",catastrophic forgetting; Incremental learning; knowledge distillation; meta-learning; stability-plasticity dilemma,Knowledge management; Learning systems; 'current; Catastrophic forgetting; Incremental learning; Knowledge distillation; Meta-learner; Meta-learning approach; Meta-learning frameworks; Metalearning; Performance; Stability-plasticity dilemma; Distillation
Aesthetic Attribute Assessment of Images Numerically on Mixed Multi-attribute Datasets,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151851677&doi=10.1145%2f3547144&partnerID=40&md5=b6946389d83fb552bc57ee184170b1a8,"With the continuous development of social software and multimedia technology, images have become a kind of important carrier for spreading information and socializing. How to evaluate an image comprehensively has become the focus of recent researches. The traditional image aesthetic assessment methods often adopt single numerical overall assessment scores, which has certain subjectivity and can no longer meet the higher aesthetic requirements. In this article, we construct an new image attribute dataset called aesthetic mixed dataset with attributes (AMD-A) and design external attribute features for fusion. Besides, we propose an efficient method for image aesthetic attribute assessment on mixed multi-attribute dataset and construct a multitasking network architecture by using the EfficientNet-B0 as the backbone network. Our model can achieve aesthetic classification, overall scoring, and attribute scoring. In each sub-network, we improve the feature extraction through ECA channel attention module. As for the final overall scoring, we adopt the idea of the teacher-student network and use the classification sub-network to guide the aesthetic overall fine-grain regression. Experimental results, using the MindSpore, show that our proposed method can effectively improve the performance of the aesthetic overall and attribute assessment.  © 2022 held by the owner/author(s). Publication rights licensed to ACM.",Aesthetic mixed dataset with attributes; ECA channel attention; external attribute features; multitasking,Multimedia systems; Multitasking; Numerical methods; Aesthetic attributes; Continuous development; ECA channel attention; Esthetic mixed dataset with attribute; External attribute feature; Image Aesthetics; Multi-attributes; Multimedia technologies; Social software technologies; Subnetworks; Network architecture
Feedback Chain Network for Hippocampus Segmentation,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169435632&doi=10.1145%2f3571744&partnerID=40&md5=61474cb9044f9f682462c520ed5d5384,"The hippocampus plays a vital role in the diagnosis and treatment of many neurological disorders. Recent years, deep learning technology has made great progress in the field of medical image segmentation, and the performance of related tasks has been constantly refreshed. In this article, we focus on the hippocampus segmentation task and propose a novel hierarchical feedback chain network. The feedback chain structure unit learns deeper and wider feature representation of each encoder layer through the hierarchical feature aggregation feedback chains and achieves feature selection and feedback through the feature handover attention module. Then, we embed a global pyramid attention unit between the feature encoder and the decoder to further modify the encoder features, including the pairwise pyramid attention module for achieving adjacent attention interaction and the global context modeling module for capturing the long-range knowledge. The proposed approach achieves state-of-the-art performance on three publicly available datasets compared with existing hippocampus segmentation approaches. The code and results can be found from the link of https://github.com/easymoneysniper183/sematic_seg. © 2023 Association for Computing Machinery.",Deep learning; feedback chain; hippocampus segmentation; multi-level feature fusion,Diagnosis; Image segmentation; Medical imaging; Signal encoding; Chain networks; Deep learning; Features fusions; Feedback chain; Hippocampus segmentation; Learning technology; Medical image segmentation; Multi-level feature fusion; Multilevels; Neurological disorders; Deep learning
D3T-GAN: Data-Dependent Domain Transfer GANs for Image Generation with Limited Data,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180994674&doi=10.1145%2f3576858&partnerID=40&md5=aa4e55b7d2b25a40ea2500ffe3d35b1a,"As an important and challenging problem, image generation with limited data aims at generating realistic images through training a GAN model given few samples. A typical solution is to transfer a well-trained GAN model from a data-rich source domain to the data-deficient target domain. In this paper, we propose a novel self-supervised transfer scheme termed D3T-GAN, addressing the cross-domain GANs transfer in limited image generation. Specifically, we design two individual strategies to transfer knowledge between generators and discriminators, respectively. To transfer knowledge between generators, we conduct a data-dependent transformation, which projects target samples into the latent space of source generator and reconstructs them back. Then, we perform knowledge transfer from transformed samples to generated samples. To transfer knowledge between discriminators, we design a multi-level discriminant knowledge distillation from the source discriminator to the target discriminator on both the real and fake samples. Extensive experiments show that our method improves the quality of generated images and achieves the state-of-the-art FID scores on commonly used datasets.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",data-dependent knowledge transfer; Limited data; projection and reconstruction,Distillation; Image reconstruction; Knowledge management; Metadata; Data dependent; Data-dependent knowledge transfer; Domain transfers; Image generations; Knowledge transfer; Limited data; Projection and reconstruction; Realistic images; Target domain; Transfer scheme; Image enhancement
Dual Scene Graph Convolutional Network for Motivation Prediction,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164289422&doi=10.1145%2f3572914&partnerID=40&md5=d2b5b11f062ff38f41764606de9e6255,"Humans can easily infer the motivations behind human actions from only visual data by comprehensively analyzing the complex context information and utilizing abundant life experiences. Inspired by humans' reasoning ability, existing motivation prediction methods have improved image-based deep classification models using the commonsense knowledge learned by pre-trained language models. However, the knowledge learned from public text corpora is probably incompatible with the task-specific data of the motivation prediction, which may impact the model performance. To address this problem, this paper proposes a dual scene graph convolutional network (dual-SGCN) to comprehensively explore the complex visual information and semantic context prior from the image data for motivation prediction. The proposed dual-SGCN has a visual branch and a semantic branch. For the visual branch, we build a visual graph based on scene graph where object nodes and relation edges are represented by visual features. For the semantic branch, we build a semantic graph where nodes and edges are directly represented by the word embeddings of the object and relation labels. In each branch, node-oriented and edge-oriented message passing is adopted to propagate interaction information between different nodes and edges. Besides, a multi-modal interactive attention mechanism is adopted to cooperatively attend and fuse the visual and semantic information. The proposed dual-SGCN is learned in an end-to-end form by a multi-task co-training scheme. In the inference stage, Total Direct Effect is adopted to alleviate the bias caused by the semantic context prior. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance. © 2023 Association for Computing Machinery.",graph convolutional network; Motivation prediction; multi-modalities; scene graph,Complex networks; Convolution; Deep learning; Forecasting; Graph theory; Graphic methods; Image enhancement; Message passing; Semantics; Context information; Convolutional networks; Graph convolutional network; Human actions; Motivation prediction; Multi-modality; Scene-graphs; Semantic context; Visual data; Visual information; Motivation
NumCap: A Number-controlled Multi-caption Image Captioning Network,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177995469&doi=10.1145%2f3576927&partnerID=40&md5=3ea421dc8e02ed37503c27e52e88bc6a,"Image captioning is a promising task that attracted researchers in the last few years. Existing image captioning models are primarily trained to generate one caption per image. However, an image may contain rich contents, and one caption cannot express its full details. A better solution is to describe an image with multiple captions, with each caption focusing on a specific aspect of the image. In this regard, we introduce a new number-based image captioning model that describes an image with multiple sentences. An image is annotated with multiple ground-truth captions; thus, we assign an external number to each caption to distinguish its order. Given an image-number pair as input, we could achieve different captions for the same image under different numbers. First, a number is attached to the image features to form an image-number vector (INV). Then, this vector and the corresponding caption are embedded using the order-embedding approach. Afterward, the INV's embedding is fed to a language model to generate the caption. To show the efficiency of the numbers incorporation strategy, we conduct extensive experiments using MS-COCO, Flickr30K, and Flickr8K datasets. The proposed model attains 24.1 in METEOR on MS-COCO. The achieved results demonstrate that our method is competitive with a range of state-of-the-art models and validate its ability to produce different descriptions under different given numbers.  © 2023 Association for Computing Machinery.",encoder-decoder framework; image captioning; Numbers incorporation strategy; order-embedding,Network coding; Embeddings; Encoder-decoder; Encoder-decoder framework; Ground truth; Image captioning; Image features; Language model; Multiple grounds; Number incorporation strategy; Order embedding; Embeddings
Semantic Completion and Filtration for Image-Text Retrieval,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174474976&doi=10.1145%2f3572844&partnerID=40&md5=c2ceecdd4a68abdb228acd0d7cd7d898,"Image-text retrieval is a vital task in computer vision and has received growing attention, since it connects cross-modality data. It comes with the critical challenges of learning unified representations and eliminating the large gap between visual and textual domains. Over the past few decades, although many works have made significant progress in image-text retrieval, they are still confronted with the challenge of incomplete text descriptions of images, i.e., how to fully learn the correlations between relevant region-word pairs with semantic diversity. In this article, we propose a novel semantic completion and filtration (SCAF) method to alleviate the above issue. Specifically, the text semantic completion module is presented to generate a complete semantic description of an image using multi-view text descriptions, guiding the model to explore the correlations of relevant region-word pairs fully. Meanwhile, the adaptive structural semantic matching module is presented to filter irrelevant region-word pairs by considering the relevance score of each region-word pair, which facilitates the model to focus on learning the relevance of matching pairs. Extensive experiments show that our SCAF outperforms the existing methods on Flickr30K and MSCOCO datasets, which demonstrates the superiority of our proposed method.  © 2023 Association for Computing Machinery.",Image-text retrieval; multimodal; semantic completion; semantic filtration,Information retrieval; Critical challenges; Cross modality; Image texts; Image-text retrieval; Learn+; Multi-modal; Semantic completion; Semantic filtration; Text retrieval; Word-pairs; Semantics
UID2021: An Underwater Image Dataset for Evaluation of No-Reference Quality Assessment Metrics,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148326730&doi=10.1145%2f3578584&partnerID=40&md5=7c50868c6b792ef23d56b61182f84eb6,"Achieving subjective and objective quality assessment of underwater images is of high significance in underwater visual perception and image/video processing. However, the development of underwater image quality assessment (UIQA) is limited for the lack of publicly available underwater image datasets with human subjective scores and reliable objective UIQA metrics. To address this issue, we establish a large-scale underwater image dataset, dubbed UID2021, for evaluating no-reference (NR) UIQA metrics. The constructed dataset contains 60 multiply degraded underwater images collected from various sources, covering six common underwater scenes (i.e., bluish scene, blue-green scene, greenish scene, hazy scene, low-light scene, and turbid scene), and their corresponding 900 quality improved versions are generated by employing 15 state-of-the-art underwater image enhancement and restoration algorithms. Mean opinion scores with 52 observers for each image of UID2021 are also obtained by using the pairwise comparison sorting method. Both in-air and underwater-specific NR IQA algorithms are tested on our constructed dataset to fairly compare their performance and analyze their strengths and weaknesses. Our proposed UID2021 dataset enables ones to evaluate NR UIQA algorithms comprehensively and paves the way for further research on UIQA. The dataset is available at https://github.com/Hou-Guojia/UID2021.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",benchmark dataset; image enhancement and restoration; image quality assessment; mean opinion score; Underwater image,Benchmarking; Image quality; Image reconstruction; Large dataset; Quality control; Restoration; Assessment metric; Benchmark datasets; Image datasets; Image quality assessment; Mean opinion scores; No-reference; Quality assessment; Subjective and objective quality assessments; Underwater image; Visual perception; Image enhancement
DDIFN: A Dual-discriminator Multi-modal Medical Image Fusion Network,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177989124&doi=10.1145%2f3574136&partnerID=40&md5=86ef292ff63f54cc37dd1a147d12a6ee,"Multi-modal medical image fusion is a long-standing important research topic that can obtain informative medical images and assist doctors diagnose and treat diseases more efficiently. However, most fusion methods extract and fuse features by subjectively defining constraints, which easily distorts the unique information of source images. In this work, we present a novel end-to-end unsupervised network to fuse multi-modal medical images. It is composed of a generator and two symmetrical discriminators. The former aims to generate a ""real-like""fused image based on a specifically designed content and structure loss, while the latter are devoted to distinguishing the differences between the fused image and the source ones. They are trained alternately until discriminators cannot distinguish the fused image from the source ones. In addition, the symmetrical discriminator scheme is conducive to maintaining the feature consistency among different modalities. More importantly, to enhance the retention degree of texture details, U-Net is adopted as the generator heuristically, where the up-sampling method is modified to bilinear interpolation for avoiding checkerboard artifacts. As for the optimization, we define the content loss function, which preserves the gradient information and pixel activity of source images. Both visual analysis and quantitative evaluation of experimental results show the superiority of our method as compared to the cutting-edge baselines.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",dual discriminator; generative adversarial networks; medical image fusion; Multi-modal fusion; U-Net,Discriminators; Image fusion; Medical imaging; Textures; Dual discriminator; End to end; Fused images; Fusion methods; Medical image fusion; Multi-modal; Multi-modal fusion; Research topics; Source images; U-net; Generative adversarial networks
Graph Attention Transformer Network for Multi-label Image Classification,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163710742&doi=10.1145%2f3578518&partnerID=40&md5=4b911384a901ed5014a0e07e4e7ef54c,"Multi-label classification aims to recognize multiple objects or attributes from images. The key to solving this issue relies on effectively characterizing the inter-label correlations or dependencies, which bring the prevailing graph neural network. However, current methods often use the co-occurrence probability of labels based on the training set as the adjacency matrix to model this correlation, which is greatly limited by the dataset and affects the model's generalization ability. This article proposes a Graph Attention Transformer Network, a general framework for multi-label image classification by mining rich and effective label correlation. First, we use the cosine similarity value of the pre-trained label word embedding as the initial correlation matrix, which can represent richer semantic information than the co-occurrence one. Subsequently, we propose the graph attention transformer layer to transfer this adjacency matrix to adapt to the current domain. Our extensive experiments have demonstrated that our proposed methods can achieve highly competitive performance on three datasets.  © 2023 Association for Computing Machinery.",attention mechanism; Graph neural network; multi-label classification; transformer,Classification (of information); Graph neural networks; Semantics; 'current; Adjacency matrix; Attention mechanisms; Graph neural networks; Images classification; Label correlations; Label images; Multi-label classifications; Multi-labels; Transformer; Image classification
Semi-supervised Learning for Mars Imagery Classification and Segmentation,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144633324&doi=10.1145%2f3572916&partnerID=40&md5=ae92e58332eddd7ed515ed1ca4c1a54a,"With the progress of Mars exploration, numerous Mars image data are being collected and need to be analyzed. However, due to the severe train-test gap and quality distortion of Martian data, the performance of existing computer vision models is unsatisfactory. In this article, we introduce a semi-supervised framework for machine vision on Mars and try to resolve two specific tasks: classification and segmentation. Contrastive learning is a powerful representation learning technique. However, there is too much information overlap between Martian data samples, leading to a contradiction between contrastive learning and Martian data. Our key idea is to reconcile this contradiction with the help of annotations and further take advantage of unlabeled data to improve performance. For classification, we propose to ignore inner-class pairs on labeled data as well as neglect negative pairs on unlabeled data, forming supervised inter-class contrastive learning and unsupervised similarity learning. For segmentation, we extend supervised inter-class contrastive learning into an element-wise mode and use online pseudo labels for supervision on unlabeled areas. Experimental results show that our learning strategies can improve the classification and segmentation models by a large margin and outperform state-of-the-art approaches.  © 2023 Association for Computing Machinery.",image classification; image segmentation; Mars vision tasks; representation learning; unsupervised learning,Computer vision; Image classification; Learning systems; Martian surface analysis; Supervised learning; Unsupervised learning; Image data; Images classification; Images segmentations; Inter class; Mars exploration; Mars vision; Mars vision task; Representation learning; Semi-supervised learning; Unlabeled data; Image segmentation
"Attention, Please! Adversarial Defense via Activation Rectification and Preservation",2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177983796&doi=10.1145%2f3572843&partnerID=40&md5=79409e83fd8b6323edfec1380e96eb46,"This study provides a new understanding of the adversarial attack problem by examining the correlation between adversarial attack and visual attention change. In particular, we observed that: (1) images with incomplete attention regions are more vulnerable to adversarial attacks; and (2) successful adversarial attacks lead to deviated and scattered activation map. Therefore, we use the mask method to design an attention-preserving loss and a contrast method to design a loss that makes the model's attention rectification. Accordingly, an attention-based adversarial defense framework is designed, under which better adversarial training or stronger adversarial attacks can be performed through the above constraints. We hope the attention-related data analysis and defense solution in this study will shed some light on the mechanism behind the adversarial attack and also facilitate future adversarial defense/attack model design.  © 2023 Association for Computing Machinery.",activation map; Adversarial defense; preservation; rectification,Behavioral research; Network security; Activation maps; Adversarial defense; Analysis solution; Attack modeling; Defense solutions; MASK method; Modeling designs; Preservation; Rectification; Visual Attention; Chemical activation
A Novel Lightweight Audio-visual Saliency Model for Videos,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163523513&doi=10.1145%2f3576857&partnerID=40&md5=a1c04e83648452b23202e87e72196805,"Audio information has not been considered an important factor in visual attention models regardless of many psychological studies that have shown the importance of audio information in the human visual perception system. Since existing visual attention models only utilize visual information, their performance is limited but also requires high-computational complexity due to the limited information available. To overcome these problems, we propose a lightweight audio-visual saliency (LAVS) model for video sequences. To the best of our knowledge, this article is the first trial to utilize audio cues for an efficient deep-learning model for the video saliency estimation. First, spatial-temporal visual features are extracted by the lightweight receptive field block (RFB) with the bidirectional ConvLSTM units. Then, audio features are extracted by using an improved lightweight environment sound classification model. Subsequently, deep canonical correlation analysis (DCCA) aims at capturing the correspondence between audio and spatial-temporal visual features, thus obtaining a spatial-temporal auditory saliency. Lastly, the spatial-temporal visual and auditory saliency are fused to obtain the audio-visual saliency map. Extensive comparative experiments and ablation studies validate the performance of the LAVS model in terms of effectiveness and complexity.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",audio-visual saliency prediction; deep canonical correlation analysis; feature fusion; Lightweight model; sound source localization,Audio acoustics; Behavioral research; Correlation methods; Visualization; Audio-visual; Audio-visual saliency prediction; Canonical correlations analysis; Deep canonical correlation analyse; Features fusions; Lightweight model; Sound source localization; Spatial temporals; Visual saliency; Visual saliency model; Deep learning
Context Sensing Attention Network for Video-based Person Re-identification,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176792736&doi=10.1145%2f3573203&partnerID=40&md5=37c0506811cf26a9fa9f4426f82a643e,"Video-based person re-identification (ReID) is challenging due to the presence of various interferences in video frames. Recent approaches handle this problem using temporal aggregation strategies. In this work, we propose a novel Context Sensing Attention Network (CSA-Net), which improves both the frame feature extraction and temporal aggregation steps. First, we introduce the Context Sensing Channel Attention (CSCA) module, which emphasizes responses from informative channels for each frame. These informative channels are identified with reference not only to each individual frame, but also to the content of the entire sequence. Therefore, CSCA explores both the individuality of each frame and the global context of the sequence. Second, we propose the Contrastive Feature Aggregation (CFA) module, which predicts frame weights for temporal aggregation. Here, the weight for each frame is determined in a contrastive manner: i.e., not only by the quality of each individual frame, but also by the average quality of the other frames in a sequence. Therefore, it effectively promotes the contribution of relatively good frames. Extensive experimental results on four datasets show that CSA-Net consistently achieves state-of-the-art performance.  © 2023 Association for Computing Machinery.",channel attention; feature aggregation; Video-based person re-identification,Aggregation strategy; Channel attention; Context sensing; Feature aggregation; Frame features; Person re identifications; Sensing channels; Temporal aggregation; Video frame; Video-based person re-identification
Fake and Dishonest Participant Immune Secret Image Sharing,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176140777&doi=10.1145%2f3572842&partnerID=40&md5=acb412421da8f3ee756f4cf09d3e9fc2,"Secret image sharing (SIS) has received increased attention from the research community because of its usefulness in multiparty secure computing, access control, blockchain distributive storage and other security-oriented applications. Prevention of fake and dishonest participants is a key issue that has spurred interest in practical applications of SIS. Unfortunately, most previous SIS schemes failed to detect and locate fake or dishonest participants. In this article, an SIS for a (k,n)-threshold without pixel expansion is presented, which can detect and locate both fake and dishonest participants. Using a screening operation, the proposed approach fuses the benefits of polynomial-based SIS, visual cryptographic scheme (VCS), and hash functions to authenticate separate participants both with and without a dealer. In addition, the proposed approach achieves lossless rebuilding of the secret image. Analyses and experiments are conducted in this study to establish the effectiveness of the presented approach.  © 2023 Association for Computing Machinery.",hash function; no pixel expansion; participant authentication; Secret image sharing,Access control; Expansion; Fake detection; Pixels; Block-chain; Cryptographic schemes; Key Issues; No pixel expansion; Participant authentication; Pixel expansion; Research communities; Screening operations; Secret image sharing; Sharing schemes; Hash functions
Towards Intelligent Attack Detection Using DNA Computing,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168553909&doi=10.1145%2f3561057&partnerID=40&md5=92b3a2bcaa51a9fa3edd85119e6f29a7,"In recent years, frequent network attacks have seriously threatened the interests and security of humankind. To address this threat, many detection methods have been studied, some of which have achieved good results. However, with the development of network interconnection technology, massive amounts of network data have been produced, and considerable redundant information has been generated. At the same time, the frequently changing types of cyberattacks result in great difficulty collecting samples, resulting in a serious imbalance in the sample size of each attack type in the dataset. These two problems seriously reduce the robustness of existing detection methods, and existing research methods do not provide a good solution. To address these two problems, we define an unbalanced index and an optimal feature index to directly reflect the performance of a detection method in terms of overall accuracy, feature subset optimization, and detection balance. Inspired by DNA computing, we propose intelligent attack detection based on DNA computing (ADDC). First, we design a set of regular encoding and decoding features based on DNA sequences and obtain a better subset of features through biochemical reactions. Second, nondominated ranking based on reference points is used to select individuals to form a new population to optimize the detection balance. Finally, a large number of experiments are carried out on four datasets to reflect real-world cyberattack situations. Experimental results show that compared with the most recent detection methods, our method can improve the overall accuracy of multiclass classification by up to 10%; the imbalance index decreased by 0.5, and 1.5 more attack types were detected on average; and the optimal index of the feature subset increased by 83.8%.  © 2023 Association for Computing Machinery.",attack detection; DNA computing; Imbalance; multiclassification; nondominated ranking,Classification (of information); DNA; Feature extraction; Interconnection networks (circuit switching); Large dataset; Attack detection; Cyber-attacks; Detection methods; DNA-computing; Feature subset; Imbalance; Multi-classification; Network attack; Nondominated ranking; Overall accuracies; DNA sequences
A DNA Based Colour Image Encryption Scheme Using A Convolutional Autoencoder,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168552427&doi=10.1145%2f3570165&partnerID=40&md5=d3f25de5b6ce761319cda74845aadc46,"With the advancement in technology, digital images can easily be transmitted and stored over the Internet. Encryption is used to avoid illegal interception of digital images. Encrypting large-sized colour images in their original dimension generally results in low encryption/decryption speed along with exerting a burden on the limited bandwidth of the transmission channel. To address the aforementioned issues, a new encryption scheme for colour images employing convolutional autoencoder, DNA and chaos is presented in this paper. The proposed scheme has two main modules, the dimensionality conversion module using the proposed convolutional autoencoder, and the encryption/decryption module using DNA and chaos. The dimension of the input colour image is first reduced from N × M × 3 to P × Q gray-scale image using the encoder. Encryption and decryption are then performed in the reduced dimension space. The decrypted gray-scale image is upsampled to obtain the original colour image having dimension N × M × 3. The training and validation accuracy of the proposed autoencoder is 97% and 95%, respectively. Once the autoencoder is trained, it can be used to reduce and subsequently increase the dimension of any arbitrary input colour image. The efficacy of the designed autoencoder has been demonstrated by the successful reconstruction of the compressed image into the original colour image with negligible perceptual distortion. The second major contribution presented in this paper is an image encryption scheme using DNA along with multiple chaotic sequences and substitution boxes. The security of the proposed image encryption algorithm has been gauged using several evaluation parameters, such as histogram of the cipher image, entropy, NPCR, UACI, key sensitivity, contrast, and so on. The experimental results of the proposed scheme demonstrate its effectiveness to perform colour image encryption.  © 2023 Association for Computing Machinery.",Autoencoder; chaos; colour image encryption; deep learning; dimensionality reduction; DNA coding,Color; Convolution; Cryptography; Deep learning; DNA sequences; Gene encoding; Image coding; Learning systems; Signal encoding; Auto encoders; Color image encryptions; Colour image; Deep learning; Digital image; Dimensionality reduction; DNA coding; Encryption/decryption; Grey scale images; Image encryption scheme; DNA
Learning the User's Deeper Preferences for Multi-modal Recommendation Systems,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163515304&doi=10.1145%2f3573010&partnerID=40&md5=f53f9554c548724bb5dfa21c32acda8e,"Recommendation system plays an important role in the rapid development of micro-video sharing platform. Micro-video has rich modal features, such as visual, audio, and text. It is of great significance to carry out personalized recommendation by integrating multi-modal features. However, most of the current multi-modal recommendation systems can only enrich the feature representation on the item side, while it leads to poor learning of user preferences. To solve this problem, we propose a novel module named Learning the User's Deeper Preferences (LUDP), which constructs the item-item modal similarity graph and user preference graph in each modality to explore the learning of item and user representation. Specifically, we construct item-item similar modalities graph using multi-modal features, the item ID embedding is propagated and aggregated on the graph to learn the latent structural information of items; The user preference graph is constructed through the historical interaction between the user and item, on which the multi-modal features are aggregated as the user's preference for the modal. Finally, combining the two parts as auxiliary information enhances the user and item representation learned from the collaborative signals to learn deeper user preferences. Through a large number of experiments on two public datasets (TikTok, Movielens), our model is proved to be superior to the most advanced multi-modal recommendation methods. © 2023 Association for Computing Machinery.",Graph convolutional networks; multimodal recommendation,Large dataset; Learning systems; Convolutional networks; Graph convolutional network; Learn+; Multi-modal; Multimodal recommendation; Personalized recommendation; Preference graph; Sharing platforms; User's preferences; Video sharing; Recommender systems
Joint Augmented and Compressed Dictionaries for Robust Image Classification,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168548638&doi=10.1145%2f3572910&partnerID=40&md5=37b75da78e05f07e6d35d9198397611b,"Dictionary-based Classification (DC) has been a promising learning theory in multimedia computing. Previous studies focused on learning a discriminative dictionary as well as the sparsest representation based on the dictionary, to cope with the complex conditions in real-world applications. However, robustness by learning only one single dictionary is far from the optimal level. What is worse, it cannot take advantage of the available techniques proven in modern machine learning, like data augmentation, to mitigate the same problem. In this work, we propose a novel method that utilizes joint Augmented and Compressed Dictionaries for Robust Dictionary-based Classification (ACD-RDC). For optimization under the noise model introduced by real-world conditions, the objective function of ACD-RDC incorporates only two simple, but well-designed constraints, including one enhanced sparsity constraint by the general data augmentation, which requires less case-by-case and sophisticated tuning, and another discriminative constraint solved by a jointly learned dictionary. The optimization of the objective function is then deduced theoretically to an approximate linear problem. The sparsity and discrimination enhanced by data augmentation guarantees the robustness for image classification under various conditions, which constructs the first positive case using data augmentation to obtain robust dictionary-based classification. Numerous experiments have been conducted on popular facial and object image datasets. The results demonstrate that ACD-RDC obtains more promising classification on diversely collected images than the current dictionary-based classification methods. ACD-RDC is also confirmed to be a state-of-the-art classification method when using deep features as inputs. © 2023 Association for Computing Machinery.",Classification and regression; inference algorithms; machine learning; supervised learning,Classification (of information); Image classification; Image enhancement; Inference engines; Classification and regression; Classification methods; Condition; Data augmentation; Images classification; Inference algorithm; Machine-learning; Objective functions; Optimisations; Real-world; Machine learning
Continual Recognition with Adaptive Memory Update,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151395566&doi=10.1145%2f3573202&partnerID=40&md5=fd2f1d1e79001297fff8e74e462051ff,"Class incremental continual learning aims to improve the ability of modern classification models to continually recognize new classes without forgetting the previous ones. Prior art in the field has largely considered using a replay buffer. In this article, we start from an observation that the existing replay-based method would fail when the stored exemplars are not hard enough to get a good decision boundary between a previously learned class and a new class. To prevent this situation, we propose a method from the perspective of remedy after forgetting for the first time. In the proposed method, a set of exemplars is preserved as a working memory, which helps to recognize new classes. When the working memory is insufficient to distinguish between new classes, more discriminating samples would be swapped from a long-term memory, which is built up during the early training process, in an adaptive way. Our continual recognition model with adaptive memory update is capable of overcoming the problem of catastrophic forgetting with various new classes coming in sequence, especially for similar but different classes. Extensive experiments on different real-world datasets demonstrate that the proposed model is superior to existing state-of-the-art algorithms. Moreover, our model can be used as a general plugin for any replay-based continual learning algorithm to further improve their performance. © 2023 Association for Computing Machinery.",Continual learning; incremental; lifelong; replay,Learning algorithms; Adaptive memory; Classification models; Continual learning; Decision boundary; Incremental; Lifelong; Long term memory; Prior arts; Replay; Working memory; Learning systems
Structure-aware Video Style Transfer with Map Art,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168560294&doi=10.1145%2f3572030&partnerID=40&md5=39d38d1d5a8fc0b5628469754a8d2a14,"Changing the style of an image/video while preserving its content is a crucial criterion to access a new neural style transfer algorithm. However, it is very challenging to transfer a new map art style to a certain video in which ""content""comprises a map background and animation objects. In this article, we present a novel comprehensive system that solves the problems in transferring map art style in such video. Our system takes as input an arbitrary video, a map image, and an off-the-shelf map art image. It then generates an artistic video without damaging the functionality of the map and the consistency in details. To solve this challenge, we propose a novel network, Map Art Video Network (MAViNet), the tailored objective functions, and a rich training set with rich animation contents and different map structures. We have evaluated our method on various challenging cases and many comparisons with those of the related works. Our method substantially outperforms state-of-the-art methods in terms of visual quality and meets the mentioned criteria in this research domain. © 2023 Association for Computing Machinery.",CNN; coherence; map art; MAViNet; Style transfer video,Arts computing; Arts image; Comprehensive system; Map art; Map art video network; Map image; Network maps; Objective functions; Structure-aware; Style transfer video; Video networks; Animation
DNA Computing-Based Multi-Source Data Storage Model in Digital Twins,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168545595&doi=10.1145%2f3561823&partnerID=40&md5=95e08ed7128ac82e4b7a1e30e078051e,"The work aims to study the application of Deoxyribonucleic Acid (DNA) multi-source data storage in Digital Twins (DT). Through the investigation of the research status of DT and DNA computing, the work puts forward the concept of DNA multi-source data storage for DT. Raptor code is improved from the design direction of degree distribution function, and six degree function distribution schemes are proposed in turn in the process of describing the research method. Additionally, a quaternary dynamic Huffman coding method is applied in DNA data storage, combined with the improved concatenated code as the error correction code. Considering the content of cytosine deoxynucleotide (C) and guanine deoxynucleotide Guanine (G) and the distribution of homopolymer in DNA storage, the work proposes and verifies an improved concatenated code algorithm Deoxyribonucleic Acid-Improved Concatenated code (DNA-ICC). The results show that while the Signal-to-Noise Ratio (SNR) increases, the Bit Error Rate (BER) decreases gradually and the trend is similar. But the anti-interference ability of the degree distribution function optimized by the probability transfer method is better. The BER of DNA-ICC scheme decreases with the decrease of error probability, which is stronger than other error correction codes. Compared with the original concatenated code, it saves at least 1.65 s, and has a good control effect on homopolymer. When the size of homopolymer exceeds 4 nt, the probability of homopolymer is only 0.44%. The proposed Quaternary dynamic Huffman code and concatenated error correction code have excellent performance. © 2023 Copyright held by the owner/author(s).",Digital Twins; DNA computing; error correction code; Huffman coding; multi-source data storage,Bioinformatics; Biomolecules; Bit error rate; C (programming language); Concatenated codes; Digital storage; Distribution functions; DNA sequences; Forward error correction; Gene encoding; Organic acids; Positive ions; Signal to noise ratio; Bit-error rate; Data storage; Degree distributions; Deoxyribonucleic acid computing; Distribution-functions; Error correction codes; Guanine; Huffman coding; Multi-source data storage; Multisource data; DNA
Variational Autoencoder with CCA for Audio-Visual Cross-modal Retrieval,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168541091&doi=10.1145%2f3575658&partnerID=40&md5=5d7406a2d9a9459dceb8d36cb5658e51,"Cross-modal retrieval is to utilize one modality as a query to retrieve data from another modality, which has become a popular topic in information retrieval, machine learning, and databases. Finding a method to effectively measure the similarity between different modality data is the major challenge of cross-modal retrieval. Although several research works have calculated the correlation between different modality data via learning a common subspace representation, the encoder's ability to extract features from multi-modal information is not satisfactory. In this article, we present a novel variational autoencoder architecture for audio-visual cross-modal retrieval by learning paired audio-visual correlation embedding and category correlation embedding as constraints to reinforce the mutuality of audio-visual information. On the one hand, audio encoder and visual encoder separately encode audio data and visual data into two different latent spaces. Further, two mutual latent spaces are respectively constructed by canonical correlation analysis. On the other hand, probabilistic modeling methods are used to deal with possible noise and missing information in the data. Additionally, in this way, the cross-modal discrepancies from intra-modal and inter-modal information are simultaneously eliminated in the joint embedding subspace. We conduct extensive experiments over two benchmark datasets. The experimental results confirm that the proposed architecture is effective in learning audio-visual correlation and is appreciably better than the existing cross-modal retrieval methods. © 2023 Association for Computing Machinery.",audio-visual correlation learning; Cross-modal retrieval,Information retrieval; Signal encoding; Audio-visual; Audio-visual correlation learning; Audio-visual correlations; Auto encoders; Cross-modal; Cross-modal retrieval; Embeddings; Machine-learning; Multi-modal information; Subspace representation; Embeddings
PEDM: A Multi-task Learning Model for Persona-aware Emoji-embedded Dialogue Generation,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168541484&doi=10.1145%2f3571819&partnerID=40&md5=e03ec824ec70021d61dd4d698a9ef5cd,"As a vivid and linguistic symbol, Emojis have become a prevailing medium interspersed in text-based communication (e.g., social media and chit-chat) to express emotions, attitudes, and situations. Generally speaking, a social-oriented chatbot that can generate appropriate Emoji-embedded responses would be much more competitive, making communications more fun, engaging, and human-like. However, the current Emoji-related research is still in its infancy, leading to an awkward situation of data deficiency. How to develop an Emoji-embedded dialogue system while addressing the lack of data will be interesting and meaningful for the application of future AI. To bridge this gap, we propose a multi-task learning method for persona-aware Emoji-embedded dialogue generation in this article. Specifically, as the benchmark of model training and evaluation, which includes 1.2 million Emoji-embedded tweets and 1.1 million post-response pairs, we first construct a dataset named EmojiTweet to handle the data deficiency problem. Then, a Seq2Seq-based model with multi-task learning is designed to simultaneously learn response generation and Emoji embedding from the constructed non-Emoji dialogue and Emoji-embedded monologue data. Afterward, we incorporate persona factors into our model by adopting persona fusion and personalized bias methods to deliver personalized dialogues with more accurately selected Emojis. Finally, we conduct extensive experiments, where the experimental results and evaluations demonstrate that our model has three key benefits: improved dialogue quality, higher user engagement, and not relying on large-scale Emoji-embedded dialogue data representing specific personas. EmojiTweet will be published publicly via https://mea-lab-421.github.io/EmojiTweet/. © 2023 Association for Computing Machinery.",dialogue generation; Emoji embedding; multi-task learning; personalized conversation,Quality control; Speech processing; Dialogue generations; Embeddings; Emoji embedding; Express emotions; Learning models; Linguistic symbols; Multitask learning; Personalized conversation; Social media; Text-based communication; Embeddings
AMSA: Adaptive Multimodal Learning for Sentiment Analysis,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168549804&doi=10.1145%2f3572915&partnerID=40&md5=dded361dc3754990baee84bf45abde76,"Efficient recognition of emotions has attracted extensive research interest, which makes new applications in many fields possible, such as human-computer interaction, disease diagnosis, service robots, and so forth. Although existing work on sentiment analysis relying on sensors or unimodal methods performs well for simple contexts like business recommendation and facial expression recognition, it does far below expectations for complex scenes, such as sarcasm, disdain, and metaphors. In this article, we propose a novel two-stage multimodal learning framework, called AMSA, to adaptively learn correlation and complementarity between modalities for dynamic fusion, achieving more stable and precise sentiment analysis results. Specifically, a multiscale attention model with a slice positioning scheme is proposed to get stable quintuplets of sentiment in images, texts, and speeches in the first stage. Then a Transformer-based self-adaptive network is proposed to assign weights flexibly for multimodal fusion in the second stage and update the parameters of the loss function through compensation iteration. To quickly locate key areas for efficient affective computing, a patch-based selection scheme is proposed to iteratively remove redundant information through a novel loss function before fusion. Extensive experiments have been conducted on both machine weakly labeled and manually annotated datasets of self-made Video-SA, CMU-MOSEI, and CMU-MOSI. The results demonstrate the superiority of our approach through comparison with baselines. © 2023 Association for Computing Machinery.",multimodal fusion; patch-based selection; self-adaptive mechanism; Sentiment analysis; Transformer,Diagnosis; Emotion Recognition; Human computer interaction; Human robot interaction; Interactive computer systems; Modal analysis; Loss functions; Multi-modal fusion; Multi-modal learning; Patch based; Patch-based selection; Recognition of emotion; Research interests; Self-adaptive mechanisms; Sentiment analysis; Transformer; Iterative methods
A Low Distortion and Steganalysis-resistant Reversible Data Hiding for 2D Engineering Graphics,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163833451&doi=10.1145%2f3539661&partnerID=40&md5=7d9d53dacaf67b4fbf7a8c0d545eab25,"To reduce the distortion resulting from the large number of crossing quantization cells and resist steganalysis, a reversible data hiding scheme for 2D engineering graphics is put forward based on reversible dual-direction quantization index modulation (RDQIM). The quantization cell index of the host data is first computed, and its distances to the embedding cells in both the left and the right directions are calculated. After that, the data hiding is performed by modifying the data to the nearest embedding cell. To guarantee the reversibility, each quantization cell is further subdivided into three sub-cells, and the source quantization interval of the host data is marked by the index of the located sub-cell. The data extraction is accomplished by calculating the index of the quantization cell where the stego data is in. Meanwhile, the lossless recovery of the stego data is realized by combining the index of the located sub-cell and the relative distance within the sub-cell. Besides, different embedding strategies are adopted for different types of entities to achieve steganalysis-resistant ability. Experimental results and analysis show that the proposed scheme can strike a good balance among imperceptibility, semi-fragility, and steganalysis-resistant ability. Moreover, under the same conditions, the average imperceptibility and the average capacity are, respectively, improved by at least 7.487% and 41.045% compared with the existing methods. © 2023 Association for Computing Machinery.",2D engineering graphics; Additional Key Words and PhrasesSteganalysis-resistant data hiding; quantization index modulation; reversible data hiding,Cell engineering; Cytology; Embeddings; Molecular biology; Steganography; 2d engineering graphic; Additional key word and phrasessteganalyse-resistant data hiding; Data-hiding; Engineering graphics; Key words; Quantization cells; Quantization index modulation; Reversible data hiding; Steganalysis; Sub-cells; Cells
A Bayesian Quality-of-Experience Model for Adaptive Streaming Videos,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151892361&doi=10.1145%2f3491432&partnerID=40&md5=f45882461e840ac14cb95a9802fcc15b,"The fundamental conflict between the enormous space of adaptive streaming videos and the limited capacity for subjective experiment casts significant challenges to objective Quality-of-Experience (QoE) prediction. Existing objective QoE models either employ pre-defined parametrization or exhibit complex functional form, achieving limited generalization capability in diverse streaming environments. In this study, we propose an objective QoE model, namely, the Bayesian streaming quality index (BSQI), to integrate prior knowledge on the human visual system and human annotated data in a principled way. By analyzing the subjective characteristics towards streaming videos from a corpus of subjective studies, we show that a family of QoE functions lies in a convex set. Using a variant of projected gradient descent, we optimize the objective QoE model over a database of training videos. The proposed BSQI demonstrates strong prediction accuracy in a broad range of streaming conditions, evident by state-of-the-art performance on four publicly available benchmark datasets and a novel analysis-by-synthesis visual experiment.  © 2022 Association for Computing Machinery.",adaptive video streaming; quadratic programming; Quality-of-experience assessment,Benchmarking; Gradient methods; Quality of service; Set theory; Video streaming; Adaptive streaming; Adaptive video streaming; Bayesian; Limited capacity; Objective qualities; Parametrizations; Quality indices; Quality-of-experience assessment; Streaming videos; Subjective experiments; Quadratic programming
PRNU-based Image Forgery Localization with Deep Multi-scale Fusion,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163844737&doi=10.1145%2f3548689&partnerID=40&md5=9c8cfac7136baf122d7af8ae21707254,"Photo-response non-uniformity (PRNU), as a class of device fingerprint, plays a key role in the forgery detection/localization for visual media. The state-of-The-Art PRNU-based forensics methods generally rely on the multi-scale trace analysis and result fusion, with Markov random field model. However, such hand-crafted strategies are difficult to provide satisfactory multi-scale decision, exhibiting a high false-positive rate. Motivated by this, we propose an end-To-end multi-scale decision fusion strategy, where a mapping from multi-scale forgery probabilities to binary decision is achieved by a supervised deep fully connected neural network. As the first time, the deep learning technology is employed in PRNU-based forensics for more flexible and reliable integration of multi-scale information. The benchmark experiments exhibit the state-of-The-Art accuracy performance of our method in both pixel-level and image-level, especially for false positives. Additional robustness experiments also demonstrate the benefits of the proposed method in resisting noise and compression attacks.  © 2023 Association for Computing Machinery.",deep learning; Image forgery localization; multi-scale analysis; photo-response non-uniformity,Benchmarking; Deep learning; Digital forensics; Deep learning; Image forgery; Image forgery localization; Localisation; Multi scale analysis; Multi-scales; Nonuniformity; Photo-response non-uniformity; Photoresponses; State of the art; Markov processes
Millimeter Wave and Free-space-optics for Future Dual-connectivity 6DOF Mobile Multi-user VR Streaming,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163837938&doi=10.1145%2f3544494&partnerID=40&md5=5c6dc0a101de492e98a9fc903f7ec4a0,"Dual-connectivity streaming is a key enabler of next-generation six Degrees Of Freedom (6DOF) Virtual Reality (VR) scene immersion. Indeed, using conventional sub-6 GHz WiFi only allows to reliably stream a low-quality baseline representation of the VR content, while emerging high-frequency communication technologies allow to stream in parallel a high-quality user viewport-specific enhancement representation that synergistically integrates with the baseline representation to deliver high-quality VR immersion. We investigate holistically as part of an entire future VR streaming system two such candidate emerging technologies, Free Space Optics (FSO) and millimeter-Wave (mmWave), that benefit from a large available spectrum to deliver unprecedented data rates. We analytically characterize the key components of the envisioned dual-connectivity 6DOF VR streaming system that integrates in addition edge computing and scalable 360° video tiling, and we formulate an optimization problem to maximize the immersion fidelity delivered by the system, given the WiFi and mmWave/FSO link rates, and the computing capabilities of the edge server and the users' VR headsets. This optimization problem is mixed integer programming of high complexity and we formulate a geometric programming framework to compute the optimal solution at low complexity. We carry out simulation experiments to assess the performance of the proposed system using actual 6DOF navigation traces from multiple mobile VR users that we collected. Our results demonstrate that our system considerably advances the traditional state of the art and enables streaming of 8K-120 frames-per-second (fps) 6DOF content at high fidelity.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",6DOF virtual reality; MMWAVE communication; scalable 360 degree video tiling; visible light communication; WiFi-VLC dual connectivity wireless streaming,Degrees of freedom (mechanics); Integer programming; Millimeter waves; Visible light communication; Wireless local area networks (WLAN); 6DOF virtual reality; Freespace optics; High quality; MMWAVE communication; Multiusers; Optimization problems; Scalable 360 degree video tiling; Six degrees of freedom; Streaming systems; Wifi-VLC dual connectivity wireless streaming; Virtual reality
Image Super-Resolution via Lightweight Attention-Directed Feature Aggregation Network,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159312887&doi=10.1145%2f3546076&partnerID=40&md5=7676c0d36690a4d3ee6f9f10de61e553,"The advent of convolutional neural networks (CNNs) has brought substantial progress in image super-resolution (SR) reconstruction. However, most SR methods pursue deep architectures to boost performance, and the resulting large model sizes are impractical for real-world applications. Furthermore, they insufficiently explore the internal structural information of image features, disadvantaging the restoration of fine texture details. To solve these challenges, we propose a lightweight architecture based on a CNN named attention-directed feature aggregation network (AFAN), consisting of chained stacking multi-Aware attention modules (MAAMs) and a simple channel attention module (SCAM), for image SR. Specifically, in each MAAM, we construct a space-Aware attention block (SAAB) and a dimension-Aware attention block (DAAB) that individually yield unique three-dimensional modulation coefficients to adaptively recalibrate structural information from an asymmetric convolution residual block (ACRB). The synergistic strategy captures multiple content features that are both space-Aware and dimension-Aware to preserve more fine-grained details. In addition, to further enhance the accuracy and robustness of the network, SCAM is embedded in the last MAAM to highlight channels with high activated values at low computational load. Comprehensive experiments verify that our proposed network attains high qualitative accuracy while employing fewer parameters and moderate computational requirements, exceeding most state-of-The-Art lightweight approaches.  © 2023 Association for Computing Machinery.",asymmetric convolution; attention mechanism; lightweight; spatial information; Super-resolution,Image reconstruction; Network architecture; Optical resolving power; Textures; Aggregation network; Asymmetric convolution; Attention mechanisms; Convolutional neural network; Feature aggregation; Image super resolutions; Lightweight; Spatial informations; Structural information; Superresolution; Convolution
Resolution Identification of Encrypted Video Streaming Based on HTTP/2 Features,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163830874&doi=10.1145%2f3551891&partnerID=40&md5=dce62192a60eab41908916c2558d5995,"With the inevitable dominance of video traffic on the Internet, Internet service providers (ISP) are striving to deliver video streaming with high quality. Video resolution, as a direct reflection of video quality, is a key factor of the video quality of experience (QoE). Since the displayed information of video cannot be observed by ISPs, ISPs can only measure the video resolution from traffic. However, with HTTP/2 being gradually adopted in video services, the multiplexing feature of HTTP/2 allows audio and video chunks to be mixed during transmission, making existing monitoring approaches unusable. In this article, we propose a method called H2CI to monitor resolution for adaptive encrypted video traffic under HTTP/2. We consider the size of the mixed data for identification. Specifically, H2CI consists of a length restoration method to extract restored fingerprints and a fingerprint-matching method for fine-grained resolution identification. The experimental results show that H2CI can achieve more than 98% accuracy for fine-grained resolution identification. Our method can be effectively applied to infer the adaptation behavior of encrypted video streaming and monitor the QoE of video services under HTTP/2.  © 2023 Association for Computing Machinery.",DASH; encrypted video streaming; HTTP/2; resolution identification,Behavioral research; Cryptography; Internet service providers; Pattern matching; Quality of service; Restoration; Video streaming; DASH; Encrypted video streaming; Fine grained; HTTP/2; Resolution identification; Video quality; Video resolutions; Video services; Video traffic; Video-streaming; HTTP
Introduction to the Special Issue on 6G Enabled Interactive Multimedia Communication Systems,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151883309&doi=10.1145%2f3567835&partnerID=40&md5=f8722247de99b4f7c19b7a3d51f07dcb,[No abstract available],,
An Interaction-process-guided Framework for Small-group Performance Prediction,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163890975&doi=10.1145%2f3558768&partnerID=40&md5=9d2c09d2147db2da90768226d8cf2d58,"A small group is a fundamental interaction unit for achieving a shared goal. Group performance can be automatically predicted using computational methods to analyze members' verbal behavior in task-oriented interactions, as has been proven in several recent works. Most of the prior works focus on lower-level verbal behaviors, such as acoustics and turn-taking patterns, using either hand-crafted features or even advanced end-to-end methods. However, higher-level group-based communicative functions used between group members during conversations have not yet been considered. In this work, we propose a two-stage training framework that effectively integrates the communication function, as defined using Bales's interaction process analysis (IPA) coding system, with the embedding learned from the low-level features in order to improve the group performance prediction. Our result shows a significant improvement compared to the state-of-the-art methods (4.241 MSE and 0.341 Pearson's correlation on NTUBA-task1 and 3.794 MSE and 0.291 Pearson's correlation on NTUBA-task2) on the National Taiwan University Business Administration (NTUBA) small-group interaction database. Furthermore, based on the design of IPA, our computational framework can provide a time-grained analysis of the group communication process and interpret the beneficial communicative behaviors for achieving better group performance.  © 2023 Association for Computing Machinery.",communicative functions; multimodal behaviors; Small group interaction; Supervised Auto-encoder,Correlation methods; Auto encoders; Business administration; Communicative functions; Group interaction; Group performance; Interaction process; Multimodal behaviour; National Taiwan University; Small group interaction; Supervised auto-encoder; Signal encoding
No-reference Quality Assessment for Contrast-distorted Images Based on Gray and Color-gray-difference Space,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163843703&doi=10.1145%2f3555355&partnerID=40&md5=bf813aa1e249397642ed36cf50ca565c,"No-reference image quality assessment is a basic and challenging problem in the field of image processing. Among them, contrast distortion has a great impact on the perception of image quality. However, there are relatively few studies on no-reference quality assessment of contrast-distorted images. This article proposes a no-reference quality assessment algorithm for contrast-distorted images based on gray and color-gray-difference (CGD) space. In terms of gray space, we consider the local and global aspects, and use the distribution characteristics of the grayscale histogram to represent global features, while local features are described by the fusion of Local Binary Pattern (LBP) operator and gradient. In terms of CGD space, we first randomly extract patches from the entire image and then extract appropriate quality perception features in the patch's CGD histogram. Finally, the AdaBoosting back propagation (BP) neural network is used to train the prediction model to predict the quality of the contrast-distorted image. Extensive analysis and cross-validation are carried out on five contrast-related image databases, and the experimental results have proved the superiority of this method compared with recent related algorithms.  © 2023 Association for Computing Machinery.",Contrast distortion; image quality assessment (IQA); no-reference,Backpropagation; Image quality; Local binary pattern; Contrast distortion; Distorted images; Gray spaces; Image quality assessment; Image-based; Images processing; No-reference; No-reference images; Quality assessment; Graphic methods
Opportunistic Transmission for Video Streaming over Wild Internet,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151850810&doi=10.1145%2f3488722&partnerID=40&md5=8036c0c6cf1abf5540cf4ce242c220d4,"The video streaming system employs adaptive bitrate (ABR) algorithms to optimize a user's quality of experience. However, it is hard for ABR algorithms to choose the right bitrate consistently under highly dynamic bandwidth fluctuations in wild Internet. In this article, we propose a building block on the client side named Opportunistic Chunk Replacement Mechanism (OCRM) to help existing ABR algorithms make full use of the available bandwidth to improve the network utilization and viewing experience of users. Specifically, the servers take advantages of the spare bandwidth to opportunistically transmit high-quality chunks (called opportunistic chunks) with low priority to the client, without incurring any extra delay. Then, the client player replaces the low-quality chunks with the opportunistic ones that have high quality. We compare OCRM with state-of-the-art ABR algorithms by using trace-driven experiments spanning a wide variety of quality of experience metrics and network conditions. The test results show that OCRM effectively achieves high network utilization and improves the user's viewing experience by up to 35%.  © 2022 Association for Computing Machinery.",ABR; DASH; video streaming,Bandwidth; Quality of service; Adaptive bitrate; Bandwidth fluctuations; Bit rates; DASH; Dynamic bandwidth; High quality; Net work utilization; Opportunistic transmission; Streaming systems; Video-streaming; Video streaming
Quality Enhancement of Compressed 360-Degree Videos Using Viewport-based Deep Neural Networks,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163867639&doi=10.1145%2f3551641&partnerID=40&md5=ec5302a582b9b344af7c3e0879ae6d5c,"360-degree video provides omnidirectional views by a bounding sphere, thus also called omnidirectional video. For omnidirectional video, people can only see specific content in the viewport through head movement, i.e., only a small portion of the 360-degree content is exposed at a given time. Therefore, the viewport quality is of particular importance for 360-degree videos. In this article, we propose a quality enhancement of compressed 360-degree videos using viewport-based deep neural networks, named V-DNN. V-DNN is mainly composed of two modules: viewport prediction network (VPN) and viewport quality enhancement network (VQEN). VPN based on spherical convolution and 2D convolution generates potential viewports for omnidirectional video. VQEN takes the current viewport and its reference viewports as the input and enhances residual for the current viewport based on bidirectional offset prediction and Spatio-temporal deformable convolutions. Compared with HM16.16 baseline at QP = 37 under the Low Delay P (LDP) configuration, experimental results show that V-DNN achieves an average 0.605 dB and 0.0139 gains in viewport-based ΔPSNR and ΔMS-SSIM, respectively, and is 0.379 dB (59.63%) and 0.0073 (110.61%) higher than the multi-frame quality enhancement (MFQE-2.0) scheme at QP = 37, respectively. Moreover, V-DNN consistently outperforms MFQE-1.0, MFQE-2.0, and HM16.16 baseline at the other QPs in terms of ΔPSNR, ΔWS-PSNR, and ΔMS-SSIM.  © 2023 Association for Computing Machinery.",360-degree video; offset field; residual enhancement; video compression; viewport prediction; viewport quality enhancement,Convolution; Deep neural networks; Image compression; Virtual private networks; 'current; 360-degree video; Head movements; Network-based; Offset field; Omni-directional view; Quality enhancement; Residual enhancement; Viewport prediction; Viewport quality enhancement; Forecasting
Deep Saliency Mapping for 3D Meshes and Applications,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163797210&doi=10.1145%2f3550073&partnerID=40&md5=4d59ffe5102d8edee1eb13e88a28ef13,"Nowadays, three-dimensional (3D) meshes are widely used in various applications in different areas (e.g., industry, education, entertainment and safety). The 3D models are captured with multiple RGB-D sensors, and the sampled geometric manifolds are processed, compressed, simplified, stored, and transmitted to be reconstructed in a virtual space. These low-level processing applications require the accurate representation of the 3D models that can be achieved through saliency estimation mechanisms that identify specific areas of the 3D model representing surface patches of importance. Therefore, saliency maps guide the selection of feature locations facilitating the prioritization of 3D manifold segments and attributing to vertices more bits during compression or lower decimation probability during simplification, since compression and simplification are counterparts of the same process. In this work, we present a novel deep saliency mapping approach applied to 3D meshes, emphasizing decreasing the execution time of the saliency map estimation, especially when compared with the corresponding time by other relevant approaches. Our method utilizes baseline 3D importance maps to train convolutional neural networks. Furthermore, we present applications that utilize the extracted saliency, namely feature-aware multiscale compression and simplification frameworks.  © 2023 Copyright held by the owner/author(s).",compression and simplification; Saliency mapping estimation,3D modeling; Accident prevention; Convolutional neural networks; Interactive computer graphics; Three dimensional computer graphics; 3D application; 3D meshes; 3D models; 3d-modeling; Compression and simplification; Processing applications; Saliency map; Saliency mapping estimation; Three dimensional (3D) meshes; Virtual spaces; Mapping
Semantic Embedding Guided Attention with Explicit Visual Feature Fusion for Video Captioning,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163804937&doi=10.1145%2f3550276&partnerID=40&md5=b92e1a2603c409445ac305d80b025b24,"Video captioning, which bridges vision and language, is a fundamental yet challenging task in computer vision. To generate accurate and comprehensive sentences, both visual and semantic information is quite important. However, most existing methods simply concatenate different types of features and ignore the interactions between them. In addition, there is a large semantic gap between visual feature space and semantic embedding space, making the task very challenging. To address these issues, we propose a framework named semantic embedding guided attention with Explicit visual Feature Fusion for vidEo CapTioning, EFFECT for short, in which we design an explicit visual-feature fusion (EVF) scheme to capture the pairwise interactions between multiple visual modalities and fuse multimodal visual features of videos in an explicit way. Furthermore, we propose a novel attention mechanism called semantic embedding guided attention (SEGA), which cooperates with the temporal attention to generate a joint attention map. Specifically, in SEGA, the semantic word embedding information is leveraged to guide the model to pay more attention to the most correlated visual features at each decoding stage. In this way, the semantic gap between visual and semantic space is alleviated to some extent. To evaluate the proposed model, we conduct extensive experiments on two widely used datasets, i.e., MSVD and MSR-VTT. The experimental results demonstrate that our approach achieves state-of-the-art results in terms of four evaluation metrics.  © 2023 Association for Computing Machinery.",explicit visual feature fusion; semantic embedding guided attention; Video captioning,Embeddings; Explicit visual feature fusion; Feature space; Features fusions; Semantic embedding; Semantic embedding guided attention; Semantic gap; Semantics Information; Video captioning; Visual feature; Visual information; Semantics
Egocentric Early Action Prediction via Adversarial Knowledge Distillation,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163801280&doi=10.1145%2f3544493&partnerID=40&md5=06d62d2f843b58779e5d133a3511868d,"Egocentric early action prediction aims to recognize actions from the first-person view by only observing a partial video segment, which is challenging due to the limited context information of the partial video. In this article, to tackle the egocentric early action prediction problem, we propose a novel multi-modal adversarial knowledge distillation framework. In particular, our approach involves a teacher network to learn the enhanced representation of the partial video by considering the future unobserved video segment, and a student network to mimic the teacher network to produce the powerful representation of the partial video and based on that predicting the action label. To promote the knowledge distillation between the teacher and the student network, we seamlessly integrate adversarial learning with latent and discriminative knowledge regularizations encouraging the learned representations of the partial video to be more informative and discriminative toward the action prediction. Finally, we devise a multi-modal fusion module toward comprehensively predicting the action label. Extensive experiments on two public egocentric datasets validate the superiority of our method over the state-of-The-Art methods. We have released the codes and involved parameters to benefit other researchers.1  © 2023 Association for Computing Machinery.",Early action prediction; egocentric video understanding; generative adversarial networks; teacher-student knowledge distillation,Distillation; Forecasting; Image segmentation; Students; Action prediction; Early action prediction; Egocentric video understanding; First person; Student knowledge; Student network; Teacher-student knowledge distillation; Teachers'; Video segments; Video understanding; Generative adversarial networks
Progressive Localization Networks for Language-Based Moment Localization,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163848211&doi=10.1145%2f3543857&partnerID=40&md5=efc8d1bee1dda354f6fa3413eed6b311,"This article targets the task of language-based video moment localization. The language-based setting of this task allows for an open set of target activities, resulting in a large variation of the temporal lengths of video moments. Most existing methods prefer to first sample sufficient candidate moments with various temporal lengths, then match them with the given query to determine the target moment. However, candidate moments generated with a fixed temporal granularity may be suboptimal to handle the large variation in moment lengths. To this end, we propose a novel multi-stage Progressive Localization Network (PLN) that progressively localizes the target moment in a coarse-To-fine manner. Specifically, each stage of PLN has a localization branch and focuses on candidate moments that are generated with a specific temporal granularity. The temporal granularities of candidate moments are different across the stages. Moreover, we devise a conditional feature manipulation module and an upsampling connection to bridge the multiple localization branches. In this fashion, the later stages are able to absorb the previously learned information, thus facilitating the more fine-grained localization. Extensive experiments on three public datasets demonstrate the effectiveness of our proposed PLN for language-based moment localization, especially for localizing short moments in long videos.  © 2023 Association for Computing Machinery.",coarse-To-fine manner; Moment localization; multi-stage model; progressive learning,Coarse to fine; Coarse-to-fine manner; Localisation; Moment localization; Multi stage modeling; Multi-stages; Progressive learning; Target activity; Temporal granularity; Upsampling
Synergy between Semantic Segmentation and Image Denoising via Alternate Boosting,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163879962&doi=10.1145%2f3548459&partnerID=40&md5=624187d0a59f7cd8a4acebe64e0832ff,"The capability of image semantic segmentation may be deteriorated due to the noisy input image, where image denoising prior to segmentation may help. Both image denoising and semantic segmentation have been developed significantly with the advance of deep learning. In this work, we are interested in the synergy between these two tasks by using a holistic deep model. We observe that not only denoising helps combat the drop of segmentation accuracy due to the noisy input, but also pixel-wise semantic information boosts the capability of denoising. We then propose a boosting network to perform denoising and segmentation alternately. The proposed network is composed of multiple segmentation and denoising blocks (SDBs), each of which estimates a semantic map and then uses the map to regularize denoising. Experimental results show that the denoised image quality is improved substantially and the segmentation accuracy is improved to close to that on clean images, and segmentation and denoising are both boosted as the number of SDBs increases. On the Cityscapes dataset, using three SDBs improves the denoising quality to 34.42 dB in PSNR, and the segmentation accuracy to 66.5 in mIoU, when the additive white Gaussian noise level is 50.  © 2023 Association for Computing Machinery.",Alternate boosting; deep learning; image denoising; semantic segmentation,Computer vision; Deep learning; Gaussian noise (electronic); Image enhancement; Semantic Segmentation; Semantics; White noise; Alternate boosting; De-noising; Deep learning; Image semantics; Input image; Multiple segmentation; Segmentation accuracy; Semantic images; Semantic segmentation; Semantics Information; Image denoising
Learning Video-Text Aligned Representations for Video Captioning,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163887397&doi=10.1145%2f3546828&partnerID=40&md5=2f59f7285dbb5ce25e327ad276e67529,"Video captioning requires that the model has the abilities of video understanding, video-Text alignment, and text generation. Due to the semantic gap between vision and language, conducting video-Text alignment is a crucial step to reduce the semantic gap, which maps the representations from the visual to the language domain. However, the existing methods often overlook this step, so the decoder has to directly take the visual representations as input, which increases the decoder's workload and limits its ability to generate semantically correct captions. In this paper, we propose a video-Text alignment module with a retrieval unit and an alignment unit to learn video-Text aligned representations for video captioning. Specifically, we firstly propose a retrieval unit to retrieve sentences as additional input which is used as the semantic anchor between visual scene and language description. Then, we employ an alignment unit with the input of the video and retrieved sentences to conduct the video-Text alignment. The representations of two modal inputs are aligned in a shared semantic space. The obtained video-Text aligned representations are used to generate semantically correct captions. Moreover, retrieved sentences provide rich semantic concepts which are helpful for generating distinctive captions. Experiments on two public benchmarks, i.e., VATEX and MSR-VTT, demonstrate that our method outperforms state-of-The-Art performances by a large margin. The qualitative analysis shows that our method generates correct and distinctive captions.  © 2023 Association for Computing Machinery.",aligned representation; Video captioning; video-Text alignment,Decoding; Semantics; Visual languages; Aligned representation; Learn+; Semantic gap; Text alignments; Text generations; Video captioning; Video understanding; Video-text alignment; Visual representations; Visual scene; Alignment
Local Correlation Ensemble with GCN Based on Attention Features for Cross-domain Person Re-ID,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163823834&doi=10.1145%2f3542820&partnerID=40&md5=6447c4cdaa5eeda617cd12e2a8496acb,"Person re-identification (Re-ID) has achieved great success in single-domain. However, it remains a challenging task to adapt a Re-ID model trained on one dataset to another one. Unsupervised domain adaption (UDA) was proposed to migrate a model from a labeled source domain to an unlabeled target domain. The main difference in the cross-domain is different background styles. Although the style transfer approach effectively reduces inter-domain gaps, it ignores the reduction of intra-class differences. Clustering-based pipelines maintain state-of-The-Art performance for UDA by learning domain-independent features; however, most existing models do not sufficiently exploit the rich unlabeled samples in target domains due to unsatisfactory clustering. Thus, we propose a novel local correlation ensemble model that focuses on the diversity of intra-class information and the reliability of class centers. Specifically, a pedestrian attention module is proposed to enable the encoder to pay more attention to the person's features to relieve interference caused by the shared background style. Furthermore, we propose a priority-distance graph convolutional network (PDGCN) module that employs a graph convolutional network network to predict the priority of a node as a class center and then calculates the distance between nodes with high priority values to screen out the class center nodes. Finally, the encoder features (local) and PDGCN features (context-Aware) are combined to perform person Re-ID. The results of experiments on the large-scale public Re-ID datasets verified the effectiveness of the proposed method.  © 2023 Association for Computing Machinery.",attention; cluster; Cross-domain; GCN; person Re-ID,Convolution; Graph theory; Signal encoding; Attention; Class Centers; Cluster; Convolutional networks; Cross-domain; Domain adaptions; GCN; Local correlations; Person re identifications; Re identifications; Large dataset
Aligning Image Semantics and Label Concepts for Image Multi-Label Classification,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142726107&doi=10.1145%2f3550278&partnerID=40&md5=15d98c385ab948cede3d18f9e1f74154,"Image multi-label classification task is mainly to correctly predict multiple object categories in the images. To capture the correlation between labels, graph convolution network based methods have to manually count the label co-occurrence probability from training data to construct a pre-defined graph as the input of graph network, which is inflexible and may degrade model generalizability. Moreover, most of the current methods cannot effectively align the learned salient object features with the label concepts, so that the predicted results of model may not be consistent with the image content. Therefore, how to learn the salient semantic features of images and capture the correlation between labels, and then effectively align them is one of the key to improve the performance of image multi-label classification task. To this end, we propose a novel image multi-label classification framework which aims to align Image Semantics with Label Concepts (ISLC). Specifically, we propose a residual encoder to learn salient object features in the images, and exploit the self-attention layer in aligned decoder to automatically capture the correlation between labels. Then, we leverage the cross-attention layers in aligned decoder to align image semantic features with label concepts, so as to make the labels predicted by model more consistent with image content. Finally, the output features of the last layer of residual encoder and aligned decoder are fused to obtain the final output feature for classification. The proposed ISLC model achieves good performance on various prevalent multi-label image datasets such as MS-COCO 2014, PASCAL VOC 2007, VG-500, and NUS-WIDE with 87.2%, 96.9%, 39.4%, and 64.2%, respectively.  © 2023 Association for Computing Machinery.",label correlation; Multi-label classification; salient features; self-attention; transformer; visual analysis,Classification (of information); Computer vision; Decoding; Image enhancement; Signal encoding; Classification tasks; Image content; Image semantics; Label correlations; Multi-label classifications; Salient features; Salient objects; Self-attention; Transformer; Visual analysis; Semantics
Hierarchical and Progressive Image Matting,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163798703&doi=10.1145%2f3540201&partnerID=40&md5=49003562a46d2ffd068f71405dee1417,"Most matting research resorts to advanced semantics to achieve high-quality alpha mattes, and a direct low-level features combination is usually explored to complement alpha details. However, we argue that appearance-Agnostic integration can only provide biased foreground (FG) details and that alpha mattes require different-level feature aggregation for better pixel-wise opacity perception. In this article, we propose an end-To-end hierarchical and progressive attention matting network (HAttMatting++), which can better predict the opacity of the FG from single RGB images without additional input. Specifically, we utilize channel-wise attention (CA) to distill pyramidal features and employ spatial attention (SA) at different levels to filter appearance cues. This progressive attention mechanism can estimate alpha mattes from adaptive semantics and semantics-indicated boundaries. We also introduce a hybrid loss function fusing structural similarity, mean square error, adversarial loss, and sentry supervision to guide the network to further improve the overall FG structure. In addition, we construct a large-scale and challenging image matting dataset comprised of 59,000 training images and 1,000 test images (a total of 646 distinct FG alpha mattes), which can further improve the robustness of our hierarchical and progressive aggregation model. Extensive experiments demonstrate that the proposed HAttMatting++ can capture sophisticated FG structures and achieve state-of-The-Art performance with single RGB images as input.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",alpha matte; attention; hierarchical; Image matting; progressive,Image enhancement; Large dataset; Mean square error; Opacity; Statistical tests; Alpha mattes; Attention; Feature combination; Hierarchical; High quality; Image matting; Low-level features; Progressive; Progressive images; RGB images; Semantics
A Survey on Temporal Sentence Grounding in Videos,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161010597&doi=10.1145%2f3532626&partnerID=40&md5=9e26e1831ebca224e6fbbb3cbffeebba,"Temporal sentence grounding in videos (TSGV), which aims at localizing one target segment from an untrimmed video with respect to a given sentence query, has drawn increasing attentions in the research community over the past few years. Different from the task of temporal action localization, TSGV is more flexible since it can locate complicated activities via natural languages, without restrictions from predefined action categories. Meanwhile, TSGV is more challenging since it requires both textual and visual understanding for semantic alignment between two modalities (i.e., text and video). In this survey, we give a comprehensive overview for TSGV, which (i) summarizes the taxonomy of existing methods, (ii) provides a detailed description of the evaluation protocols (i.e., datasets and metrics) to be used in TSGV, and (iii) in-depth discusses potential problems of current benchmarking designs and research directions for further investigations. To the best of our knowledge, this is the first systematic survey on temporal sentence grounding. More specifically, we first discuss existing TSGV approaches by grouping them into four categories, i.e., two-stage methods, single-stage methods, reinforcement learning-based methods, and weakly supervised methods. Then we present the benchmark datasets and evaluation metrics to assess current research progress. Finally, we discuss some limitations in TSGV through pointing out potential problems improperly resolved in the current evaluation protocols, which may push forwards more cutting-edge research in TSGV. Besides, we also share our insights on several promising directions, including four typical tasks with new and practical settings based on TSGV. © 2023 Copyright held by the owner/author(s).",cross-modal video retrieval; multi-modality; Video understanding; vision and language,Computer vision; Reinforcement learning; 'current; Cross-modal; Cross-modal video retrieval; Evaluation protocol; Multi-modality; Potential problems; Research communities; Video retrieval; Video understanding; Vision and language; Semantics
Multimodal Graph for Unaligned Multimodal Sequence Analysis via Graph Convolution and Graph Pooling,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163894124&doi=10.1145%2f3542927&partnerID=40&md5=438bede06823f7b981564c7d940e7b39,"Multimodal sequence analysis aims to draw inferences from visual, language, and acoustic sequences. A majority of existing works focus on the aligned fusion of three modalities to explore inter-modal interactions, which is impractical in real-world scenarios. To overcome this issue, we seek to focus on analyzing unaligned sequences, which is still relatively underexplored and also more challenging. We propose Multimodal Graph, whose novelty mainly lies in transforming the sequential learning problem into graph learning problem. The graph-based structure enables parallel computation in time dimension (as opposed to recurrent neural network) and can effectively learn longer intra-and inter-modal temporal dependency in unaligned sequences. First, we propose multiple ways to construct the adjacency matrix for sequence to perform sequence to graph transformation. To learn intra-modal dynamics, a graph convolution network is employed for each modality based on the defined adjacency matrix. To learn inter-modal dynamics, given that the unimodal sequences are unaligned, the commonly considered word-level fusion does not pertain. To this end, we innovatively devise graph pooling algorithms to automatically explore the associations between various time slices from different modalities and learn high-level graph representation hierarchically. Multimodal Graph outperforms state-of-The-Art models on three datasets under the same experimental setting.  © 2023 Association for Computing Machinery.",Graph pooling; multimodal graph; multimodal sequence analysis; sentiment analysis,Convolution; Graphic methods; Linear transformations; Modal analysis; Modeling languages; Recurrent neural networks; Visual languages; Adjacency matrix; Graph pooling; Learn+; Learning problem; Multi-modal; Multimodal graph; Multimodal sequence analyse; Sentiment analysis; Sequence analysis; Sentiment analysis
Referring Expression Comprehension Via Enhanced Cross-modal Graph Attention Networks,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163807580&doi=10.1145%2f3548688&partnerID=40&md5=ba9ee43b3239264750985fe3428d5d2e,"Referring expression comprehension aims to localize a specific object in an image according to a given language description. It is still challenging to comprehend and mitigate the gap between various types of information in the visual and textual domains. Generally, it needs to extract the salient features from a given expression and match the features of expression to an image. One challenge in referring expression comprehension is the number of region proposals generated by object detection methods is far more than the number of entities in the corresponding language description. Remarkably, the candidate regions without described by the expression will bring a severe impact on referring expression comprehension. To tackle this problem, we first propose a novel Enhanced Cross-modal Graph Attention Networks (ECMGANs) that boosts the matching between the expression and the entity position of an image. Then, an effective strategy named Graph Node Erase (GNE) is proposed to assist ECMGANs in eliminating the effect of irrelevant objects on the target object. Experiments on three public referring expression comprehension datasets show unambiguously that our ECMGANs framework achieves better performance than other state-of-The-Art methods. Moreover, GNE is able to obtain higher accuracies of visual-expression matching effectively.  © 2023 Association for Computing Machinery.",Enhanced Cross-modal Graph Attention Networks; Graph Node Erase; object detection; Referring expression comprehension,Graph theory; Image enhancement; Object recognition; Cross-modal; Enhanced cross-modal graph attention network; Graph node erase; Language description; Matchings; Object detection method; Objects detection; Referring expression comprehension; Referring expressions; Salient features; Object detection
Hyper-node Relational Graph Attention Network for Multi-modal Knowledge Graph Completion,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163830207&doi=10.1145%2f3545573&partnerID=40&md5=82a941b398bf1004c20298ea77ad7313,"Knowledge graphs often suffer from incompleteness, and knowledge graph completion (KGC) aims at inferring the missing triplets through knowledge graph embedding from known factual triplets. However, most existing knowledge graph embedding methods only use the relational information of knowledge graph and treat the entities and relations as IDs with simple embedding layer, ignoring the multi-modal information among triplets, such as text descriptions, images, etc. In this work, we propose a novel network to incorporate different modal information with graph structure information for more precise representation of multi-modal knowledge graph, termed as hyper-node relational graph attention (HRGAT) network. In HRGAT, we use low-rank multi-modal fusion to model the intra-modality and inter-modality dynamics, which transforms the original knowledge graph to a hyper-node graph. Then, relational graph attention (RGAT) network is used, which contains relation-specific attention and entity-relation fusion operation to capture the graph structure information. Finally, we aggregate the updated multi-modal information and graph structure information to generate the final embeddings of knowledge graph to achieve KGC. By exploring multi-modal information and graph structure information, HRGAT embraces faster convergence speed and achieves the state-of-The-Art for KGC on the standard datasets. Implementation code is available at https://github.com/broliang/HRGAT.  © 2023 Association for Computing Machinery.",knowledge graph completion; low-rank multi-modal fusion; Multi-modal knowledge graph; relational graph attention network,Graph embeddings; Graph theory; Graphic methods; Graph structures; Knowledge graph completion; Knowledge graphs; Low-rank multi-modal fusion; Multi-modal; Multi-modal fusion; Multi-modal knowledge graph; Relational graph; Relational graph attention network; Knowledge graph
Introduction to the Special Issue on Affective Services based on Representation Learning,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151878206&doi=10.1145%2f3567836&partnerID=40&md5=6ac6df014eba3d51dcef06c77d0b5ead,[No abstract available],,
L2BEC2: Local Lightweight Bidirectional Encoding and Channel Attention Cascade for Video Frame Interpolation,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163832474&doi=10.1145%2f3547660&partnerID=40&md5=1bd5c495d8bb5e50cf65525b4d4b14b0,"Video frame interpolation (VFI) is of great importance for many video applications, yet it is still challenging even in the era of deep learning. Some existing VFI models directly exploit existing lightweight network frameworks, thus making synthesized in-between frames blurry and creating artifacts due to imprecise motion representation. The other existing VFI models typically depend on heavy model architectures with a large number of parameters, preventing them from being deployed on small terminals. To address these issues, we propose a local lightweight VFI network (L2BEC2) that leverages bidirectional encoding structure with channel attention cascade. Specifically, we improve visual quality by introducing a forward and backward encoding structure with channel attention cascade to better characterize motion information. Furthermore, we introduce a local lightweight strategy into the state-of-The-Art Adaptive Collaboration of Flows (AdaCoF) model to simplify its model parameters. Compared with the original AdaCoF model, the proposed L2BEC2 obtains performance gain at the cost of only one-Third of the number of parameters and performs favorably against the state-of-The-Art works on public datasets. Our source code is available at https://github.com/Pumpkin123709/LBEC.git.  © 2023 Association for Computing Machinery.",bidirectional encoding; channel attention cascade; lightweight network; Video frame interpolation,Deep learning; Encoding (symbols); Network coding; Video signal processing; Adaptive collaboration; Bidirectional encoding; Channel attention cascade; Encoding structure; Encodings; Frame interpolation; Lightweight network; State of the art; Video frame; Video frame interpolation; Interpolation
Toward A No-reference Omnidirectional Image Quality Evaluation by Using Multi-perceptual Features,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162137314&doi=10.1145%2f3549544&partnerID=40&md5=d38844d10faddc730e760592bd1a57e1,"Compared to ordinary images, omnidirectional image (OI) usually has a broader view and a higher resolution, and image quality assessment (IQA) can help people to understand and improve their visual experience. However, the current IQA works cannot achieve good performance. To address this, we proposed a novel visual perception-based no-reference/blind omnidirectional image quality assessment (NR/B-OIQA) model. The gradient-based global structural features and gray-level co-occurrence matrix-based local structural features are combined together to highlight the rich quality-aware structural information. And a novel steganalysis real model-based color descriptor is extracted to reflect the color information that ignored in most IQA models. With a multi-scale visual perception, we take image entropy and the natural scene statistics features to convey the high-level semantics and quantify the unnaturalness of omnidirectional images. Finally, we apply support vector regression to predict the objective quality value based on the subjective scores and extracted all features. Experiments are conducted on OIQA and CVIQD2018 Databases, and the results illustrate that our model has more reliable performance and stronger competitiveness and receives better conformity with the subjective values.  © 2023 Association for Computing Machinery.",blind quality assessment; multi-scale visual perception; Omnidirectional images,Color; Image enhancement; Image quality; Steganography; Support vector machines; Vision; Blind quality assessments; Image quality assessment; Image quality evaluation; Multi-scale visual perception; Multi-scales; No-reference; Omnidirectional image; Perceptual feature; Quality assessment model; Visual perception; Semantics
Disentangling Features for Fashion Recommendation,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148039635&doi=10.1145%2f3531017&partnerID=40&md5=b3566880e5abac9fdcba9b37bfa4692d,"Online stores have become fundamental for the fashion industry, revolving around recommendation systems to suggest appropriate items to customers. Such recommendations often suffer from a lack of diversity and propose items that are similar to previous purchases of a user. Recently, a novel kind of approach based on Memory Augmented Neural Networks (MANNs) has been proposed, aimed at recommending a variety of garments to create an outfit by complementing a given fashion item. In this article we address the task of compatible garment recommendation developing a MANN architecture by taking into account the co-occurrence of clothing attributes, such as shape and color, to compose an outfit. To this end we obtain disentangled representations of fashion items and store them in external memory modules, used to guide recommendations at inference time. We show that our disentangled representations are able to achieve significantly better performance compared to the state of the art and also provide interpretable latent spaces, giving a qualitative explanation of the recommendations. © 2023 Association for Computing Machinery.",Garment recommendation; memory augmented neural networks; recommendation systems,Memory architecture; Online systems; Purchasing; Co-occurrence; External memory; Fashion industry; Garment recommendation; Memory augmented neural network; Memory modules; Neural network architecture; Neural-networks; Online store; Performance; Recommender systems
JDAN: Joint Detection and Association Network for Real-Time Online Multi-Object Tracking,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148014460&doi=10.1145%2f3533253&partnerID=40&md5=c517dac99222b93f307baf74bcc86f8d,"In the last few years, enormous strides have been made for object detection and data association, which are vital subtasks for one-stage online multi-object tracking (MOT). However, the two separated submodules involved in the whole MOT pipeline are processed or optimized separately, resulting in a complex method design and requiring manual settings. In addition, few works integrate the two subtasks into a single end-to-end network to optimize the overall task. In this study, we propose an end-to-end MOT network called joint detection and association network (JDAN) that is trained and inferred in a single network. All layers in JDAN are differentiable, and can be optimized jointly to detect targets and output an association matrix for robust multi-object tracking. What's more, we generate suitable pseudo-labels to address the data inconsistency between object detection and association. The detection and association submodules could be optimized by the composite loss function that is derived from the detection results and the generated pseudo association labels, respectively. The proposed approach is evaluated on two MOT challenge datasets, and achieves promising performance compared with classic and latest methods. © 2023 Association for Computing Machinery.",end-to-end model; object detection and data association; Online multi-object tracking,Object recognition; Data association; End-to-end models; Joint-detection; Multi-object tracking; Object data; Object detection and data association; Objects detection; Online multi-object tracking; Submodules; Subtask; Object detection
CAQoE: A Novel No-Reference Context-aware Speech Quality Prediction Metric,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148002200&doi=10.1145%2f3529394&partnerID=40&md5=60f678c94c24632684cfb6e6569177ed,"The quality of speech degrades while communicating over Voice over Internet Protocol applications, for example, Google Meet, Microsoft Skype, and Apple FaceTime, due to different types of background noise present in the surroundings. It reduces human perceived Quality of Experience (QoE). Along this line, this article proposes a novel speech quality prediction metric that can meet human's desired QoE level. Our motivation is driven by the lack of evidence showing speech quality metrics that can distinguish different noise degradations before predicting the quality of speech. The quality of speech in noisy environments is improved by speech enhancement algorithms, and for measuring and monitoring the quality of speech, objective speech quality metrics are used. With the integration of these components, a novel no-reference context-aware QoE prediction metric (CAQoE) is proposed in this article, which initially identifies the context or noise type or degradation type of the input noisy speech signal and then predicts context-specific speech quality for that input speech signal. It will have of great importance in deciding the speech enhancement algorithms if the types of degradations causing poor speech quality are known along with the quality metric. Results demonstrate that the proposed CAQoE metric outperforms in different contexts as compared to the metric where contexts are not identified before predicting the quality of speech, even in the presence of limited size speech corpus having different contexts available from the NOIZEUS speech database. © 2023 Association for Computing Machinery.",Classifier; deep neural network; no-reference; quality of experience (QoE); speech enhancement; speech quality; voice activity detector; VoIP,Deep neural networks; Internet telephony; Quality of service; Speech communication; Speech enhancement; Voice/data communication systems; Context-Aware; No-reference; Noise degradations; Quality metrices; Quality of experience; Quality of speech; Quality prediction; Speech quality; Voice-activity detector; VoIP; Forecasting
Image Quality Assessment-driven Reinforcement Learning for Mixed Distorted Image Restoration,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148044279&doi=10.1145%2f3532625&partnerID=40&md5=8feb1c286cd32bf5a0e98f94cb448580,"Due to the diversity of the degradation process that is difficult to model, the recovery of mixed distorted images is still a challenging problem. The deep learning model trained under certain degradation declines significantly in other degradation situations. In this article, we explore ways to use a combination of tools to deal with the mixed distortion. First, we illustrate the limitations of a single deep network in dealing with multiple distortion types and then introduce a hierarchical toolkit with distinguished powerful tools. Second, we investigate how an efficient representation of images combined with a reinforcement learning (RL) paradigm helps to deal with tool noise in continuous restoration. The proposed method can accurately capture the distortion preferences for selecting the optimal recovery tools by RL agent. Finally, to fully utilize random tools for unknown distortion combinations, we adopt the exploration scheme with various quality evaluation methods to achieve more quality improvements. Experimental results demonstrate that the peak signal-to-noise ratio of the proposed method is 3.30 dB higher than other state-of-the-art RL-based methods on the CSIQ single distortion dataset and 0.95 dB higher on the DIV2K mixed distortion dataset. © 2023 Association for Computing Machinery.",deep learning; Image restoration; reinforcement learning,Deep learning; Image quality; Image representation; Reinforcement learning; Restoration; Signal to noise ratio; Deep learning; Degradation process; Distorted images; Image quality assessment; Learning models; Learning paradigms; Optimal recovery; Quality evaluation method; Reinforcement learning agent; Reinforcement learnings; Image reconstruction
Learning Pixel Affinity Pyramid for Arbitrary-Shaped Text Detection,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148033672&doi=10.1145%2f3524617&partnerID=40&md5=c0ac8f5920a833cd12bc28d5991e983f,"Arbitrary-shaped text detection in natural images is a challenging task due to the complexity of the background and the diversity of text properties. The difficulty lies in two aspects: accurate separation of adjacent texts and sufficient text feature representation. To handle these problems, we consider text detection as instance segmentation and propose a novel text detection framework, which jointly learns semantic segmentation and a pixel affinity pyramid in a unified fully convolutional network. Specifically, the pixel affinity pyramid is proposed to encode multi-scale instance affiliation relationships of pixels, which is not only robust to varying shapes of text but also provides an accurate boundary description for separating closely located texts. In the inference phase, a simple but effective post-processing is presented to reconstruct text instances from the semantic segmentation results under the guidance of the learned pixel affinity pyramid, achieving good accuracy and efficiency. Furthermore, to enhance the representation of text features in the neural network, two modules - the Region Enhancement Module (REM) and Attentional Fusion Module (AFM) - are proposed. The REM models the semantic correlations of regional features to enhance the features from the text area, which effectively suppresses false-positive detection. The AFM adaptively fuses multi-scale textual information through an attention mechanism to obtain abundant text semantic features, which benefits multi-sized text detection. Extensive ablation experiments are conducted demonstrating the effectiveness of the REM and AFM. Evaluation results on standard benchmarks, including Total-Text, ICDAR2015, SCUT-CTW1500, and MSRA-TD500, show that our method surpasses most existing text detectors and achieves state-of-the-art performance, denoting its superior capability in detecting arbitrary-shaped texts. © 2023 Association for Computing Machinery.",deep learning; instance segmentation; pixel affinity; Scene text detection,Benchmarking; Deep learning; Feature extraction; Pixels; Semantics; Deep learning; Fusion modules; Instance segmentation; Multi-scales; Pixel affinities; Scene Text; Scene text detection; Semantic segmentation; Text detection; Text feature; Semantic Segmentation
LCSNet: End-to-end Lipreading with Channel-aware Feature Selection,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148043989&doi=10.1145%2f3524620&partnerID=40&md5=73689b5fefcc85ff1b0c51f33ffb4e0a,"Lipreading is a task of decoding the movement of the speaker's lip region into text. In recent years, lipreading methods based on deep neural network have attracted widespread attention, and the accuracy has far surpassed that of experienced human lipreaders. The visual differences in some phonemes are extremely subtle and pose a great challenge to lipreading. Most of the lipreading existing methods do not process the extracted visual features, which mainly suffer from two problems. First, the extracted features contain lot of useless information such as noise caused by differences in speech speed and lip shape, for example. In addition, the extracted features are not abstract enough to distinguish phonemes with similar pronunciation. These problems have a bad effect on the performance of lipreading. To extract features from the lip regions that are more distinguishable and more relevant to the speech content, this article proposes an end-to-end deep neural network-based lipreading model (LCSNet). The proposed model extracts the short-term spatio-temporal features and the motion trajectory features from the lip region in the video clips. The extracted features are filtered by the channel attention module to eliminate the useless features and then used as input to the proposed Selective Feature Fusion Module (SFFM) to extract the high-level abstract features. Afterwards, these features are used as input to the bidirectional GRU network in time order for temporal modeling to obtain the long-term spatio-temporal features. Finally, a Connectionist Temporal Classification (CTC) decoder is used to generate the output text. The experimental results show that the proposed model achieves a 1.0% CER and 2.3% WER on the GRID corpus database, which, respectively, represents an improvement of 52% and 47% compared to LipNet. © 2023 Association for Computing Machinery.",channel attention mechanism; deep neural network; Lipreading; selective feature fusion module,Abstracting; Decoding; Feature extraction; Text processing; Attention mechanisms; Channel attention mechanism; Channel aware; End to end; Features fusions; Features selection; Fusion modules; Lipreading; Selective feature fusion module; Spatiotemporal feature; Deep neural networks
Self-supervised Calorie-aware Heterogeneous Graph Networks for Food Recommendation,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146429075&doi=10.1145%2f3524618&partnerID=40&md5=f39e5712af6c221f8a0f410c5a4d5ff2,"With the rapid development of online recipe sharing platforms, food recommendation is emerging as an important application. Although recent studies have made great progress on food recommendation, they have two shortcomings that are likely to affect the recommendation performance. (1) The relations between ingredients are not considered, which may lead to sub-optimal representations of recipes and further result in the neglect of the user's personalized ingredient combination preference. (2) Existing methods do not consider the impact of users' preferences on calories in users' food decision-making process. In this article, we propose a Self-supervised Calorie-aware Heterogeneous Graph Network (SCHGN) to model the relations between ingredients and incorporate calories of food simultaneously. Specifically, we first incorporate users, recipes, ingredients, and calories into a heterogeneous graph and explicitly present the complex relations among them with directed edges. Then, we explore the co-occurrence relation of ingredients in different recipes via self-supervised ingredient prediction. To capture users' dynamic preferences on calories of food, we learn calorie-aware user representations by hierarchical message passing and compute a comprehensive user-guided recipe representation by attention mechanism. The final food recommendation is accomplished based on the similarity between a user's calorie-aware representation and the user-guided representation of a recipe. Extensive experiment results on benchmark datasets demonstrate the effectiveness of the proposed method. © 2023 Association for Computing Machinery.",Food recommendation; heterogeneous graph; recipe calories; self-supervised learning,Decision making; Graph algorithms; Graph neural networks; Machine learning; Message passing; Decision-making process; Food recommendation; Graph networks; Heterogeneous graph; Recipe calory; Recommendation performance; Self-supervised learning; Sharing platforms; User's preferences; User-guided; Directed graphs
BMIF: Privacy-preserving Blockchain-based Medical Image Fusion,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148042688&doi=10.1145%2f3531016&partnerID=40&md5=8f9da8d471ba8a1a57d1085f5b0569db,"Medical image fusion generates a fused image containing multiple features extracted from different source images, and it is of great help in clinical analysis and diagnosis. However, training a deep learning model for image fusion usually requires enormous computing power, especially for large volumes of medical data. Meanwhile, the privacy of images is also a critical issue. In this article, we propose a privacy-preserving blockchain-based medical image fusion (BMIF) framework. First, to ensure fusion performance, we design a new medical image fusion model based on convolutional neural network and Inception network and integrate the proposed model into the consensus process of blockchain. Next, to save computing power of blockchain, we design a consensus mechanism by requesting consensus nodes to train the fusion model instead of calculating useless hash values in traditional blockchain. Then, to protect data privacy, we further present an efficient homomorphic encryption to realize the training of fusion model on encrypted medical data. Finally, we conduct theoretical analysis and extensive experiments on public datasets to evaluate the feasibility and the performance of our proposed BMIF. The results exhibit that BMIF is efficient and secure, and our medical image fusion network performs better than state-of-the-art approaches. © 2023 Association for Computing Machinery.",blockchain; consensus mechanism; fully homomorphic encryption; Medical image fusion; privacy protection,Convolutional neural networks; Deep learning; Diagnosis; Image fusion; Medical imaging; Privacy-preserving techniques; Block-chain; Computing power; Consensus mechanism; Fully homomorphic encryption; Fused images; Fusion model; Medical data; Medical image fusion; Privacy preserving; Privacy protection; Blockchain
Learning Streamed Attention Network from Descriptor Images for Cross-Resolution 3D Face Recognition,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148038627&doi=10.1145%2f3527158&partnerID=40&md5=f601ae5666cd0c1327e865b601e56540,"In this article, we propose a hybrid framework for cross-resolution 3D face recognition which utilizes a Streamed Attention Network (SAN) that combines handcrafted features with Convolutional Neural Networks (CNNs). It consists of two main stages: first, we process the depth images to extract low-level surface descriptors and derive the corresponding Descriptor Images (DIs), represented as four-channel images. To build the DIs, we propose a variation of the 3D Local Binary Pattern (3DLBP) operator that encodes depth differences using a sigmoid function. Then, we design a CNN that learns from these DIs. The peculiarity of our solution consists in processing each channel of the input image separately, and fusing the contribution of each channel by means of both self- and cross-attention mechanisms. This strategy showed two main advantages over the direct application of Deep-CNN to depth images of the face; on the one hand, the DIs can reduce the diversity between high- and low-resolution data by encoding surface properties that are robust to resolution differences. On the other, it allows a better exploitation of the richer information provided by low-level features, resulting in improved recognition. We evaluated the proposed architecture in a challenging cross-dataset, cross-resolution scenario. To this aim, we first train the network on scanner-resolution 3D data. Next, we utilize the pre-trained network as feature extractor on low-resolution data, where the output of the last fully connected layer is used as face descriptor. Other than standard benchmarks, we also perform experiments on a newly collected dataset of paired high- and low-resolution 3D faces. We use the high-resolution data as gallery, while low-resolution faces are used as probe, allowing us to assess the real gap existing between these two types of data. Extensive experiments on low-resolution 3D face benchmarks show promising results with respect to state-of-the-art methods. © 2023 Association for Computing Machinery.",3D face recognition; convolutional neural networks; feature descriptors; self- and cross-attention,Convolution; Convolutional neural networks; Encoding (symbols); Local binary pattern; 3D face recognition; 3D faces; Convolutional neural network; Depth image; Descriptors; Feature descriptors; High resolution; High-low; Lower resolution; Self- and cross-attention; Face recognition
A Novel Reversible Data Hiding Scheme Based on Pixel-Residual Histogram,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148030056&doi=10.1145%2f3534565&partnerID=40&md5=51e9d2c2987e1b2bf00d298519279852,"Prediction-error expansion (PEE) is the most popular reversible data hiding (RDH) technique due to its efficient capacity-distortion tradeoff. With the generated prediction-error histogram (PEH) and adaptively selected expansion bins, the image redundancy is well exploited by PEE. However, for the most widely used rhombus predictor, the rounding operation which groups different prediction-errors into one value is completely unnecessary. The embedding can be extended to a general case by removing the rounding operation, and more histogram bins can be derived for expansion with a new mapping mechanism. Therefore, in this article, instead of pixel prediction-error, we propose to compute the pixel residuals without the rounding operation, and a new embedding mechanism based on pixel-residual histogram (PRH) modification is devised. In PRH, four bins correspond to one bin in PEH. Then, different from the one-to-one mapping between the prediction-error and pixel modification, a four-to-one mapping between the pixel-residual and pixel modification is established, and the performance is optimized by adaptively selecting four expansion bin pairs for embedding. Since more modification selections are considered, better performance can be obtained. Moreover, the proposed scheme is extended to the two-dimensional (2D) histogram and multiple histograms based embedding, and the performance is further enhanced. The superiority of the proposed method is experimentally verified by comparing it with some state-of-the-art works. © 2023 Association for Computing Machinery.",adaptive embedding; pixel-residual histogram; prediction-error expansion; Reversible data hiding,Embeddings; Errors; Forecasting; Mapping; Steganography; Adaptive embedding; Error histograms; Performance; Pixel modification; Pixel-residual histogram; Prediction errors; Prediction-error expansions; Reversible data hiding; Rounding operation; Pixels
Adaptive Text Denoising Network for Image Caption Editing,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148016046&doi=10.1145%2f3532627&partnerID=40&md5=31734d3f42fd6f4e1fc5ef4e20252a36,"Image caption editing, which aims at editing the inaccurate descriptions of the images, is an interdisciplinary task of computer vision and natural language processing. As the task requires encoding the image and its corresponding inaccurate caption simultaneously and decoding to generate an accurate image caption, the encoder-decoder framework is widely adopted for image caption editing. However, existing methods mostly focus on the decoder, yet ignore a big challenge on the encoder: the semantic inconsistency between image and caption. To this end, we propose a novel Adaptive Text Denoising Network (ATD-Net) to filter out noises at the word level and improve the model's robustness at sentence level. Specifically, at the word level, we design a cross-attention mechanism called Textual Attention Mechanism (TAM), to differentiate the misdescriptive words. The TAM is designed to encode the inaccurate caption word by word based on the content of both image and caption. At the sentence level, in order to minimize the influence of misdescriptive words on the semantic of an entire caption, we introduce a Bidirectional Encoder to extract the correct semantic representation from the raw caption. The Bidirectional Encoder is able to model the global semantics of the raw caption, which enhances the robustness of the framework. We extensively evaluate our proposals on the MS-COCO image captioning dataset and prove the effectiveness of our method when compared with the state-of-the-arts. © 2023 Association for Computing Machinery.",cross-modal semantic matching; Image caption editing; sequence editing,Arts computing; Decoding; Encoding (symbols); Image coding; Natural language processing systems; Semantic Web; Signal encoding; Attention mechanisms; Cross-modal; Cross-modal semantic matching; De-noising; Image caption; Image caption editing; Modal semantics; Semantic matching; Sequence editing; Word level; Semantics
Sequential Hierarchical Learning with Distribution Transformation for Image Super-Resolution,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148012897&doi=10.1145%2f3532864&partnerID=40&md5=e2d7bf2842d4fd32712a099d3cb1de46,"Multi-scale design has been considered in recent image super-resolution (SR) works to explore the hierarchical feature information. Existing multi-scale networks aim at building elaborate blocks or progressive architecture for restoration. In general, larger scale features concentrate more on structural and high-level information, while smaller scale features contain plentiful details and textured information. In this point of view, information from larger scale features can be derived from smaller ones. Based on the observation, in this article, we build a sequential hierarchical learning super-resolution network (SHSR) for effective image SR. Specially, we consider the inter-scale correlations of features, and devise a sequential multi-scale block (SMB) to progressively explore the hierarchical information. SMB is designed in a recursive way based on the linearity of convolution with restricted parameters. Besides the sequential hierarchical learning, we also investigate the correlations among the feature maps and devise a distribution transformation block (DTB). Different from attention-based methods, DTB regards the transformation in a normalization manner, and jointly considers the spatial and channel-wise correlations with scaling and bias factors. Experiment results show SHSR achieves superior quantitative performance and visual quality to state-of-the-art methods with near 34% parameters and 50% MACs off when scaling factor is × 4. To boost the performance without further training, the extension model SHSR+ with self-ensemble achieves competitive performance than larger networks with near 92% parameters and 42% MACs off with scaling factor ×4. © 2023 Association for Computing Machinery.",distribution transformation; Image super-resolution; multi-scale; neural network,Optical resolving power; Distribution transformation; Hierarchical learning; Image super resolutions; Large-scales; Multi-scale design; Multi-scales; Neural-networks; Scale blocks; Scaling factors; Superresolution; Textures
On Teaching Mode of MTI Translation Workshop Based on IPT Corpus for Tibetan Areas of China,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148005586&doi=10.1145%2f3527173&partnerID=40&md5=6a7cb4e57fdcf9c5e012b09f3a6b729b,"With the technological turn of applied research in translation, increasing attention has been paid to the teaching of translation technology. This article addresses two important questions in this regard: how to independently develop Master of Translation and Interpreting (MTI) translation teaching resources with ethnic minority characteristics and how to use information technology to carry out Tibet-related computer-assisted translation (CAT) teaching. This article discusses the background, structure, and functions of the International Publicity Translation Corpus (IPT Corpus) for Tibetan Areas of China through empirical research, combining theory with practice, and validates the translation teaching mode through case study to better train translators and interpreters working on content related to Tibetan culture. Through teaching practice since 2017, the MTI translation workshop based on the IPT Corpus has proven to be an effective teaching mode that is worthy of further improvement and extension. © 2023 Association for Computing Machinery.",IPT Corpus; MTI translation teaching; Translation technology; workshop,Computer aided instruction; Applied research; Ethnic minorities; International publicity translation corpus; Master of translation and interpreting translation teaching; Teaching modes; Teaching resources; Tibetans; Translation teachings; Translation technology; Workshop; E-learning
DIPS: A Dyadic Impression Prediction System for Group Interaction Videos,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148042978&doi=10.1145%2f3532865&partnerID=40&md5=8f5da856fddbe58d24713f1c8455dab8,"We consider the problem of predicting the impression that one subject has of another in a video clip showing a group of interacting people. Our novel Dyadic Impression Prediction System (DIPS) contains two major innovations. First, we develop a novel method to align the facial expressions of subjects pi and pj as well as account for the temporal delay that might be involved in pi reacting to pj's facial expressions. Second, we propose the concept of a multilayered stochastic network for impression prediction on top of which we build a novel Temporal Delayed Network graph neural network architecture. Our overall DIPS architecture predicts six dependent variables relating to the impression pi has of pj. Our experiments show that DIPS beats eight baselines from the literature, yielding statistically significant improvements of 19.9% to 30.8% in AUC and 12.6% to 47.2% in F1-score. We further conduct ablation studies showing that our novel features contribute to the overall quality of the predictions made by DIPS. © 2023 Association for Computing Machinery.",computational psychology; graph neural networks; Impression prediction; multi-layer networks; video analysis,Graph neural networks; Network architecture; Network layers; Stochastic systems; Computational psychology; Facial Expressions; Graph neural networks; Group interaction; Impression prediction; Multi-layer network; Novel methods; Prediction systems; Video analysis; Video-clips; Forecasting
Distance and Direction Based Deep Discriminant Metric Learning for Kinship Verification,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144822091&doi=10.1145%2f3531014&partnerID=40&md5=16f70e6b8a3d3c5ff9a3fffd7d6fe05b,"Image-based kinship verification is an important task in computer vision and has many applications in practice, such as missing children search and family album construction, among others. Due to the differences in age, gender, expression and appearance, there usually exists a large discrepancy between the facial images of parent and child. This makes kinship verification a challenging task. In this article, we propose a Distance and Direction Based Deep Discriminant Metric Learning (D4ML) approach for kinship verification. The basic idea of D4ML is to make full use of the discriminant information contained in the facial images of parent and child such that the network can learn more a discriminating distance metric. Specifically, D4ML learns the metric by utilizing the discriminant information from two perspectives: distance-based perspective and direction-based perspective. From the distance-based perspective, the designed loss function is used to minimize the distance between images having kinship and maximize the distance between images without kinship. In practice, the gender difference and large age gap may significantly increase the distance between facial images of parent and child. Therefore, learning the metric only from a distance-based perspective is insufficient. Considering that two vectors with a large distance may appear with high similarity in direction, D4ML also employs the direction-based loss function in the training process. Both kinds of loss function work together to improve the discriminability of the learned metric. Experimental results on four small size publicly available datasets demonstrate the effectiveness of our approach. Source code of our approach can be found at https://github.com/lclhenu/D4ML. © 2023 Association for Computing Machinery.",distance-based and direction-based deep discriminant metric learning (D<sup>4</sup>ML); Image-based kinship verification,Discriminant informations; Distance-based; Distance-based and direction-based deep discriminant metric learning (D4ML); Facial images; Image-based; Image-based kinship verification; Learn+; Loss functions; Metric learning; Missing children; Deep learning
Modified 2D-Ghost-Free Stereoscopic Display with Depth-of-Field Effects,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147988740&doi=10.1145%2f3534964&partnerID=40&md5=9f8649a9ab18e2662795653d951b8523,"Backward-compatible stereoscopic display, a novel display technique that can simultaneously present satisfying 3D effects to viewers with stereo glasses and clear 2D contents to viewers without, aims at helping the people who are unsuitable for watching 3D movies for a long time. In this article, we introduce two versions of backward-compatible stereoscopic display: the simpler version is far simpler than Hidden Stereo (the state-of-the-art method) while preserving competitive 2D-3D effects; the advanced version, which we call 2D-Ghost-Free Stereoscopic Display, overcomes the limitation that Hidden Stereo and the simpler version are both confined to small absolute disparity. 2D-Ghost-Free Stereoscopic Display improves tolerable disparity range by adding depth-of-field in the regions with large disparity, so that it can be applied to more scenes of 3D movies. User experiments and theoretical analysis both demonstrate the superiority of the 2D-Ghost-Free Stereoscopic Display over the state-of-the-art method and our simpler version. In addition, to make the user experiments double-blind and automatic, we developed a user study system that can automatically present 3D images and videos in NVIDIA 3D Vision 2 and collect corresponding votes of subjects on stimuli, whereas the previous researchers did not state that their user experiments were double-blind. © 2023 Association for Computing Machinery.",Backward-compatible stereoscopic display; perception; stereoscopy,Three dimensional displays; 3D effects; Backward compatible; Backward-compatible stereoscopic display; Depth of field; Field-effect; Simple++; State-of-the-art methods; Stereo glass; Stereoscopic display; User experiments; Stereo image processing
Deep Inter Prediction with Error-Corrected Auto-Regressive Network for Video Coding,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141032342&doi=10.1145%2f3528173&partnerID=40&md5=2e6f5605ba34067c9168d4752199ce7f,"Modern codecs remove temporal redundancy of a video via inter prediction, i.e., searching previously coded frames for similar blocks and storing motion vectors to save bit-rates. However, existing codecs adopt block-level motion estimation, where a block is regressed by reference blocks linearly and is doomed to fail to deal with non-linear motions. In this article, we generate virtual reference frames (VRFs) with previously reconstructed frames via deep networks to offer an additional candidate, which is not constrained to linear motion structure and further significantly improves coding efficiency. More specifically, we propose a novel deep Auto-Regressive Moving-Average (ARMA) model, Error-Corrected Auto-Regressive Network (ECAR-Net), equipped with the powers of the conventional statistic ARMA models and deep networks jointly for reference frame prediction. Similar to conventional ARMA models, the ECAR-Net consists of two stages: Auto-Regression (AR) stage and Error-Correction (EC) stage, where the first part predicts the signal at the current time-step based on previously reconstructed frames, while the second one compensates for the output of the AR stage to obtain finer details. Different from the statistic AR models only focusing on short-term temporal dependency, the AR model of our ECAR-Net is further injected with the long-term dynamics mechanism, where long temporal information is utilized to help predict motions more accurately. Furthermore, ECAR-Net works in a configuration-adaptive way, i.e., using different dynamics and error definitions for the Low Delay B and Random Access configurations, which helps improve the adaptivity and generality in diverse coding scenarios. With the well-designed network, our method surpasses HEVC on average 5.0% and 6.6% BD-rate saving for the luma component under the Low Delay B and Random Access configurations and also obtains on average 1.54% BD-rate saving over VVC. Furthermore, ECAR-Net works in a configuration-adaptive way, i.e., using different dynamics and error definitions for the Low Delay B and Random Access configurations, which helps improve the adaptivity and generality in diverse coding scenarios. © 2023 Association for Computing Machinery.",deep learning; Error-Corrected Auto-Regressive Network; High Efficient Video Coding (HEVC); inter prediction; Versatile Video Coding (VVC); virtual reference frame,Deep learning; Dynamics; Error correction; Forecasting; Motion estimation; Video signal processing; Auto-regressive; Deep learning; Error-corrected auto-regressive network; High efficient; High efficient video coding; Inter prediction; Reference frame; Versatile video coding; Virtual reference; Virtual reference frame; Image coding
Optimizing Performance of Federated Person Re-identification: Benchmarking and Analysis,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147988053&doi=10.1145%2f3531013&partnerID=40&md5=b49d8494e230e3954be27db14788292c,"Increasingly stringent data privacy regulations limit the development of person re-identification (ReID) because person ReID training requires centralizing an enormous amount of data that contains sensitive personal information. To address this problem, we introduce federated person re-identification (FedReID) - implementing federated learning, an emerging distributed training method, to person ReID. FedReID preserves data privacy by aggregating model updates, instead of raw data, from clients to a central server. Furthermore, we optimize the performance of FedReID under statistical heterogeneity via benchmark analysis. We first construct a benchmark with an enhanced algorithm, two architectures, and nine person ReID datasets with large variances to simulate the real-world statistical heterogeneity. The benchmark results present insights and bottlenecks of FedReID under statistical heterogeneity, including challenges in convergence and poor performance on datasets with large volumes. Based on these insights, we propose three optimization approaches: (1) we adopt knowledge distillation to facilitate the convergence of FedReID by better transferring knowledge from clients to the server, (2) we introduce client clustering to improve the performance of large datasets by aggregating clients with similar data distributions, and (3) we propose cosine distance weight to elevate performance by dynamically updating the weights for aggregation depending on how well models are trained in clients. Extensive experiments demonstrate that these approaches achieve satisfying convergence with much better performance on all datasets. We believe that FedReID will shed light on implementing and optimizing federated learning on more computer vision applications. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Federated learning; person re-identification,Benchmarking; Distillation; Sensitive data; Federated learning; Model updates; Optimizing performance; Performance; Person re identifications; Personal information; Privacy regulation; Statistical heterogeneities; Stringents; Training methods; Large dataset
iDAM: Iteratively Trained Deep In-loop Filter with Adaptive Model Selection,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148002660&doi=10.1145%2f3529107&partnerID=40&md5=c88f5b34be2f9f748ed3d7a14502f62b,"As a rapid development of neural-network-based machine learning algorithms, deep learning methods are being tentatively used in a much wider range than well-known artificial intelligence applications such as face recognition or auto-driving. Recently, deep learning models are investigated intensively to improve the compression efficiency for video coding, especially at the in-loop filtering stage. Although deep learning-based in-loop filtering methods in prior arts have already shown a remarkable potential capability in video coding, content propagation issue is still not well recognized and addressed yet. Content propagation is the fact that contents of reference frames are propagated to frames referring to them, which typically leads to over-filtering issues. In this article, we develop an iteratively trained deep in-loop filter with adaptive model selection (iDAM) to address the content propagation issue. First, we propose an iterative training scheme, which enables the network to gradually take into account the impacts of content propagation. Second, we propose a filter selection mechanism, i.e., allowing a block to select from a set of candidate filters with different filtering strengths. Besides, we propose a novel approach to design a conditional in-loop filtering method that can deal with multiple quality levels with a single model and serve the functionality of filter selection by modifying the input parameters. Extensive experiments on top of the latest video coding standard (Versatile Video Coding, VVC) have been conducted to evaluate the proposed techniques. Compared with VTM-11.0, our scheme achieves a new state-of-the-art, leading to {7.91%, 20.25%, 20.44%}, {11.64%, 26.40%, 26.50%}, and {10.97%, 26.63%, 26.77%} BD-rate reductions on average for {Y, Cb, Cr} under all-intra, random-access, and low-delay configurations, respectively. As far as we know, our proposed iDAM scheme provides the highest coding performance compared to all existing solutions. In addition, the syntax elements of the proposed scheme were adopted at the 76th meeting of Audio Video coding Standard (AVS) held this year. © 2023 Association for Computing Machinery.",Adaptive model selection; conditional in-loop filter; convolutional neural networks; deep in-loop filtering; iterative training; Versatile Video Coding,Adaptive filtering; Adaptive filters; Backpropagation; Convolutional neural networks; Deep neural networks; Face recognition; Image coding; Learning systems; Video signal processing; Adaptive model selection; Conditional in-loop filter; Convolutional neural network; Deep in-loop filtering; Filter selection; Filtering method; In-loop filters; Iterative training; Loop filtering; Versatile video coding; Iterative methods
Exploiting Manifold Feature Representation for Efficient Classification of 3D Point Clouds,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147989550&doi=10.1145%2f3539611&partnerID=40&md5=0de43a67fba9f3f589d41757f23d42f6,"In this paper, we propose an efficient point cloud classification method via manifold learning based feature representation. Different from conventional methods, we use manifold learning algorithms to embed point cloud features for better considering the geometric continuity on the surface. Then, the nature of point cloud can be acquired in low dimensional space, and after being concatenated with features in the original three-dimensional (3D) space, both the capability of feature representation and the classification network performance can be improved. We explore three traditional manifold algorithms (i.e., Isomap, Locally-Linear Embedding, and Laplacian eigenmaps) in detail, and finally, we select the Locally-Linear Embedding (LLE) algorithm due to its low complexity and locality consistency preservation. Furthermore, we propose a neural network based manifold learning (NNML) method to implement manifold learning based non-linear projection. Experiments demonstrate that the proposed two manifold learning methods can obtain better performances than the state-of-the-art methods, and the obtained mean class accuracy (mA) and overall accuracy (oA) can reach 91.4% and 94.4%, respectively. Moreover, because of the improved feature learning capability, the proposed NNML method can also have better classification accuracy on models with prominent geometric shapes. To further demonstrate the advantages of PointManifold, we extend it as a plug and play method for point cloud classification task, which can be directly used with existing methods and gain a significant improvement. © 2023 Association for Computing Machinery.",3D vision; deep neural network; feature representation; manifold learning; Point cloud classification,Classification (of information); Computer vision; Embeddings; 3-D vision; 3D point cloud; Cloud classification; Feature representation; Learning methods; Manifold learning; Network-based; Neural-networks; Point cloud classification; Point-clouds; Deep neural networks
Using Four Hypothesis Probability Estimators for CABAC in Versatile Video Coding,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147994441&doi=10.1145%2f3531015&partnerID=40&md5=74f50d89ac35de8d17ba3930dea223cf,"This article introduces the key technologies involved in four hypothetical probability estimators for Context-based Adaptive Binary Arithmetic Coding (CABAC). The focus is on the selected adaptation rate performed in these estimators, which are selected based on coding efficiency and memory considerations, and also the relationship with the current size of the coding block. The proposed scheme can linearly realize the quantitative representation of probabilistic prediction and describes the scalability potential for higher accuracy. Besides a description of the design concept, this work also discusses motivation and implementation aspects, which are based on simple operations such as bitwise operations and single subsampling for subinterval updates. The experimental results verify the effectiveness of the proposed CABAC method specified in Versatile Video Coding (VVC). © 2023 Association for Computing Machinery.",adaptation rate; CABAC; entropy coding; probability estimator; video compression; VVC,Binary codes; Digital arithmetic; Image coding; Video signal processing; 'current; Adaptation rate; Coding blocks; Coding efficiency; Context-based adaptive binary arithmetic coding; Entropy coding; Key technologies; Memory considerations; Probability estimator; Versatile video coding; Image compression
Retrieval Augmented Convolutional Encoder-decoder Networks for Video Captioning,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148002894&doi=10.1145%2f3539225&partnerID=40&md5=9c1e767f219e8e8a7e7f4480f3d357f1,"Video captioning has been an emerging research topic in computer vision, which aims to generate a natural sentence to correctly reflect the visual content of a video. The well-established way of doing so is to rely on encoder-decoder paradigm by learning to encode the input video and decode the variable-length output sentence in a sequence-to-sequence manner. Nevertheless, these approaches often fail to produce complex and descriptive sentences as natural as those from human being, since the models are incapable of memorizing all visual contents and syntactic structures in the human-annotated video-sentence pairs. In this article, we uniquely introduce a Retrieval Augmentation Mechanism (RAM) that enables the explicit reference to existing video-sentence pairs within any encoder-decoder captioning model. Specifically, for each query video, a video-sentence retrieval model is first utilized to fetch semantically relevant sentences from the training sentence pool, coupled with the corresponding training videos. RAM then writes the relevant video-sentence pairs into memory and reads the memorized visual contents/syntactic structures in video-sentence pairs from memory to facilitate the word prediction at each timestep. Furthermore, we present Retrieval Augmented Convolutional Encoder-Decoder Network (R-ConvED), which novelly integrates RAM into convolutional encoder-decoder structure to boost video captioning. Extensive experiments on MSVD, MSR-VTT, Activity Net Captions, and VATEX datasets validate the superiority of our proposals and demonstrate quantitatively compelling results. © 2023 Association for Computing Machinery.",deep convolutional neural networks; Video captioning,Convolution; Convolutional neural networks; Deep neural networks; Network coding; Random access storage; Convolutional encoders; Convolutional neural network; Deep convolutional neural network; Encoder-decoder; Input videos; Research topics; Syntactic structure; Variable length; Video captioning; Visual content; Decoding
Cyclic Self-attention for Point Cloud Recognition,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147990359&doi=10.1145%2f3538648&partnerID=40&md5=12b77d8c48c20cf15b3a615ac55e1753,"Point clouds provide a flexible geometric representation for computer vision research. However, the harsh demands for the number of input points and computer hardware are still significant challenges, which hinder their deployment in real applications. To address these challenges, we design a simple and effective module named cyclic self-attention module (CSAM). Specifically, three attention maps of the same input are obtained by cyclically pairing the feature maps, thus exploring the features sufficiently of the attention space of the original input. CSAM can adequately explore the correlation between points to obtain sufficient feature information despite the multiplicative decrease in inputs. Meanwhile, it can direct the computational power to the more essential features, relieving the burden on the computer hardware. We build a point cloud classification network by simply stacking CSAM called cyclic self-attention network (CSAN). We also propose a novel framework for point cloud semantic segmentation called full cyclic self-attention network (FCSAN). By adaptively fusing the original mapping features and the CSAM extracted features, it can better capture the context information of point clouds. Extensive experiments on several benchmark datasets show that our methods can achieve competitive performance in classification and segmentation tasks. © 2023 Association for Computing Machinery.",adaptive fuse; cyclic pairing; Point cloud; self-attention,Benchmarking; Classification (of information); Computer vision; Semantic Segmentation; Semantics; Adaptive fuse; Cloud recognition; Cyclic pairing; Feature map; Geometric representation; Point-clouds; Real applications; Self-attention; Simple++; Vision research; Computer hardware
(Compress and Restore)N: A Robust Defense Against Adversarial Attacks on Image Classification,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147988822&doi=10.1145%2f3524619&partnerID=40&md5=e079c149995e80fe9fc2395c092ee0ce,"Modern image classification approaches often rely on deep neural networks, which have shown pronounced weakness to adversarial examples: images corrupted with specifically designed yet imperceptible noise that causes the network to misclassify. In this article, we propose a conceptually simple yet robust solution to tackle adversarial attacks on image classification. Our defense works by first applying a JPEG compression with a random quality factor; compression artifacts are subsequently removed by means of a generative model Artifact Restoration GAN. The process can be iterated ensuring the image is not degraded and hence the classification not compromised. We train different AR-GANs for different compression factors, so that we can change its parameters dynamically at each iteration depending on the current compression, making the gradient approximation difficult. We experiment with our defense against three white-box and two black-box attacks, with a particular focus on the state-of-the-art BPDA attack. Our method does not require any adversarial training, and is independent of both the classifier and the attack. Experiments demonstrate that dynamically changing the AR-GAN parameters is of fundamental importance to obtain significant robustness. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Adversarial attacks; image restoration,Deep neural networks; Image classification; Image compression; Iterative methods; Network security; Restoration; Adversarial attack; Classification approach; Compression artifacts; Compression factor; Generative model; Images classification; JPEG compression; Quality factors; Robust solutions; Simple++; Image reconstruction
MFGAN: Multi-modal Feature-fusion for CT Metal Artifact Reduction Using GANs,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147994144&doi=10.1145%2f3528172&partnerID=40&md5=1c119af89893fe1252d6414a3bda42cc,"Due to the existence of metallic implants in certain patients, the Computed Tomography (CT) images from these patients are often corrupted by undesirable metal artifacts, which causes severe problem of metal artifact. Although many methods have been proposed to reduce metal artifact, reduction is still challenging and inadequate. Some reduced results are suffering from symptom variance, second artifact, and poor subjective evaluation. To address these, we propose a novel method based on generative adversarial nets (GANs) to reduce metal artifacts. Specifically, we firstly encode interactive information (text) and imaging CT (image) to yield multi-modal feature-fusion representation, which overcomes representative ability limitation of single-modal CT images. The incorporation of interaction information constrains feature generation, which ensures symptom consistency between corrected and target CT. Then, we design an enhancement network to avoid second artifact and enhance edge as well as suppress noise. Besides, three radiology physicians are invited to evaluate the corrected CT image. Experiments show that our method gains significant improvement over other methods. Objectively, ours achieves an average increment of 7.44% PSNR and 6.12% SSIM on two medical image datasets. Subjectively, ours outperforms others in comparison in term of sharpness, resolution, invariance, and acceptability. © 2023 Association for Computing Machinery.",edge enhancement; Feature fusion; generative adversarial nets; metal artifact reduction; second artifact,Computerized tomography; Medical imaging; Computed tomography images; Edge enhancements; Features fusions; Metal artifact reduction; Metal artifacts; Metallic implants; Multi-modal; Novel methods; Second artifact; Subjective evaluations; Generative adversarial networks
"Introduction to the Special Section on Learning Representations, Similarity, and Associations in Dynamic Multimedia Environments",2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146462961&doi=10.1145%2f3569952&partnerID=40&md5=660e75f6e0cf8ec5b433b99b4a2c6d61,[No abstract available],,
Boosting Hyperspectral Image Classification with Dual Hierarchical Learning,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147264754&doi=10.1145%2f3522713&partnerID=40&md5=75c140c0cbe66c5049a632a73a9b1746,"Hyperspectral image (HSI) classification aims at predicting the pixel-wise labels in an image, where there are only a few labeled pixel samples (hard labels) for training. It is a challenging task since the classification process is susceptible to over-fitting under training with limited samples. To relieve this problem, we propose a method based on dual hierarchical learning. First, we employ a connectionist hyperspectral convolution (HC) network to capture the representations of the pixels from different receptive fields. Specifically, an HC is designed to learn the correlation among adjacent pixels and is further extended to a connectionist hierarchical structure. These operations use the correlation to enhance one-pixel learning from multiple receptive fields. Second, we analyze the properties in the hyperspectral image and introduce a hierarchical pseudo label generation algorithm to enrich the supervision of the label information. Finally, we design a dual hierarchical learning strategy to help all HC layers learn from both the hard labels and the hierarchical pseudo labels. In other words, it addresses the HSI classification problem from different views. For inference, we employ two fusion strategies to find a better prediction. The experimental results on four popular HSI benchmarks, i.e., Salinas-A, IndianPines, PaviaU, and PaviaC, demonstrate the effectiveness of the proposed method. Our code is publicly available on GitHub: https://github.com/ShuoWangCS/HSI-DHL. © 2023 Association for Computing Machinery.",hierarchical learning; Hyperspectral image classification; pseudo label,Adaptive boosting; Image classification; Learning systems; Adjacent pixels; Classification process; Hierarchical learning; HyperSpectral; Hyperspectral image classification; Labeled pixels; Learn+; Overfitting; Pseudo label; Receptive fields; Pixels
Toward High-quality Face-Mask Occluded Restoration,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148046564&doi=10.1145%2f3524137&partnerID=40&md5=1899ebe4fd4140391b78c82b221e266d,"Face-mask occluded restoration aims at restoring the masked region of a human face, which has attracted increasing attention in the context of the COVID-19 pandemic. One major challenge of this task is the large visual variance of masks in the real world. To solve it we first construct a large-scale Face-mask Occluded Restoration (FMOR) dataset, which contains 5,500 unmasked images and 5,500 face-mask occluded images with various illuminations, and involves 1,100 subjects of different races, face orientations, and mask types. Moreover, we propose a Face-Mask Occluded Detection and Restoration (FMODR) framework, which can detect face-mask regions with large visual variations and restore them to realistic human faces. In particular, our FMODR contains a self-adaptive contextual attention module specifically designed for this task, which is able to exploit the contextual information and correlations of adjacent pixels for achieving high realism of the restored faces, which are however often neglected in existing contextual attention models. Our framework achieves state-of-the-art results of face restoration on three datasets, including CelebA, AR, and our FMOR datasets. Moreover, experimental results on AR and FMOR datasets demonstrate that our framework can significantly improve masked face recognition and verification performance. © 2023 Association for Computing Machinery.",face restoration; Face-mask occluded dataset; masked face recognition and verification; self-adaptive contextual attention,Face recognition; Large dataset; Face masks; Face orientation; Face restoration; Face-mask occluded dataset; High quality; Human faces; Large-scales; Masked face recognition and verification; Real-world; Self-adaptive contextual attention; Restoration
Multi-view Shape Generation for a 3D Human-like Body,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147939127&doi=10.1145%2f3514248&partnerID=40&md5=5eec616953d5618785a3daaa9046602c,"Three-dimensional (3D) human-like body reconstruction via a single RGB image has attracted significant research attention recently. Most of the existing methods rely on the Skinned Multi-Person Linear model and thus can only predict unified human bodies. Moreover, meshes reconstructed by current methods sometimes perform well from a canonical view but not from other views, as the reconstruction process is commonly supervised by only a single view. To address these limitations, this article proposes a multi-view shape generation network for a 3D human-like body. Particularly, we propose a coarse-to-fine learning model that gradually deforms a template body toward the ground truth body. Our model utilizes the information of multi-view renderings and corresponding 3D vertex transformation as supervision. Such supervision will help to generate 3D bodies well aligned to all views. To accurately operate mesh deformation, a graph convolutional network structure is introduced to support the shape generation from 3D vertex representation. Additionally, a graph up-pooling operation is designed over the intermediate representations of the graph convolutional network, and thus our model can generate 3D shapes with higher resolution. Novel loss functions are employed to help optimize the whole multi-view generation model, resulting in smoother surfaces. In addition, two multi-view human body datasets are produced and contributed to the community. Extensive experiments conducted on the benchmark datasets demonstrate the efficacy of our model over the competitors.  © 2023 Association for Computing Machinery.",3D reconstruction; human body reconstruction; multi-view stereo,3D modeling; Image reconstruction; Stereo image processing; Three dimensional computer graphics; 3D reconstruction; Body reconstruction; Convolutional networks; Human bodies; Human body reconstruction; Human like; Multi-view stereo; Multi-views; RGB images; Shape generations; Convolution
A Deep Multi-level Attentive Network for Multimodal Sentiment Analysis,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148038496&doi=10.1145%2f3517139&partnerID=40&md5=b93ae899f26c0cec5b207f8014265883,"Multimodal sentiment analysis has attracted increasing attention with broad application prospects. Most of the existing methods have focused on a single modality, which fails to handle social media data due to its multiple modalities. Moreover, in multimodal learning, most of the works have focused on simply combining the two modalities without exploring the complicated correlations between them. This resulted in dissatisfying performance for multimodal sentiment classification. Motivated by the status quo, we propose a Deep Multi-level Attentive network (DMLANet), which exploits the correlation between image and text modalities to improve multimodal learning. Specifically, we generate the bi-attentive visual map along the spatial and channel dimensions to magnify Convolutional neural network representation power. Then, we model the correlation between the image regions and semantics of the word by extracting the textual features related to the bi-attentive visual features by applying semantic attention. Finally, self-attention is employed to fetch the sentiment-rich multimodal features for the classification automatically. We conduct extensive evaluations on four real-world datasets, namely, MVSA-Single, MVSA-Multiple, Flickr, and Getty Images, which verify our method's superiority. © 2023 Association for Computing Machinery.",Attention; deep learning; multimodal analysis; sentiment analysis,Convolutional neural networks; Deep learning; Image enhancement; Modal analysis; Semantics; Application prospect; Attention; Broad application; Deep learning; Multi-modal; Multi-modal learning; Multilevels; Multimodal analysis; Sentiment analysis; Social media datum; Sentiment analysis
A Large-Scale Synthetic Gait Dataset Towards in-the-Wild Simulation and Comparison Study,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148044842&doi=10.1145%2f3517199&partnerID=40&md5=bef281dc3b3d3ab15a7bd44121e2a759,"Gait recognition has a rapid development in recent years. However, current gait recognition focuses primarily on ideal laboratory scenes, leaving the gait in the wild unexplored. One of the main reasons is the difficulty of collecting in-the-wild gait datasets, which must ensure diversity of both intrinsic and extrinsic human gait factors. To remedy this problem, we propose to construct a large-scale gait dataset with the help of controllable computer simulation. In detail, to diversify the intrinsic factors of gait, we generate numerous characters with diverse attributes and associate them with various types of walking styles. To diversify the extrinsic factors of gait, we build a complicated scene with a dense camera layout. Then we design an automatic generation toolkit under Unity3D for simulating the walking scenarios and capturing the gait data. As a result, we obtain a dataset simulating towards the in-the-wild scenario, called VersatileGait, which has more than one million silhouette sequences of 10,000 subjects with diverse scenarios. VersatileGait possesses several nice properties, including huge dataset size, diverse pedestrian attributes, complicated camera layout, high-quality annotations, small domain gap with the real one, good scalability for new demands, and no privacy issues. By conducting a series of experiments, we first explore the effects of different factors on gait recognition. We further illustrate the effectiveness of using our dataset to pre-train models, which obtain considerable performance gain on CASIA-B, OU-MVLP, and CASIA-E. Besides, we show the great potential of the fine-grained labels other than the ID label in improving the efficiency and effectiveness of models. Our dataset and its corresponding generation toolkit are available at https://github.com/peterzpy/VersatileGait. © 2023 Association for Computing Machinery.",fine-grained attributes; Gait recognition; in the wild scenarios; synthetic dataset; Unity3D,Gait analysis; Large dataset; Pattern recognition; 'current; Comparison study; Fine grained; Fine-grained attribute; Gait recognition; In the wild scenario; Large-scales; Simulation studies; Synthetic datasets; Unity3d; Cameras
Social Network Analytic-Based Online Counterfeit Seller Detection using User Shared Images,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148046861&doi=10.1145%2f3524135&partnerID=40&md5=ff3cb775376ebf6dc7172bff0b524ac7,"Selling counterfeit online has become a serious problem, especially with the advancement of social media and mobile technology. Instead of investigating the products directly, one can only check the images, tags annotated by the sellers on the images, or the price to decide if a seller sells counterfeits. One of the ways to detect counterfeit sellers is to investigate their social graphs, in which counterfeit sellers show different behaviour in network measurements, such as those in centrality and EgoNet. However, social graphs are not easily accessible. They may be kept private by the operators, or there are no connections at all. This article proposes a framework to detect counterfeit sellers using their connection graphs discovered from their shared images. Based on 153 K shared images from Taobao, it is proven that counterfeit sellers have different network behaviours. It is observed that the network measurements follow Beta function well. Those distributions are formulated to detect counterfeit sellers by the proposed framework, which is 60% better than approaches using classification. © 2023 Association for Computing Machinery.",Counterfeit seller detection; deep learning; social network analytic,Crime; Decision theory; E-learning; Electronic commerce; Social networking (online); Connection graphs; Counterfeit seller detection; Deep learning; In networks; Media technology; Mobile Technology; Network measurement; Social graphs; Social media; Social network analytic; Deep learning
Wavelength-based Attributed Deep Neural Network for Underwater Image Restoration,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147951781&doi=10.1145%2f3511021&partnerID=40&md5=8d9f97e3abeaf77223970f5bce8bd718,"Background: Underwater images, in general, suffer from low contrast and high color distortions due to the non-uniform attenuation of the light as it propagates through the water. In addition, the degree of attenuation varies with the wavelength, resulting in the asymmetric traversing of colors. Despite the prolific works for underwater image restoration (UIR) using deep learning, the above asymmetricity has not been addressed in the respective network engineering.Contributions: As the first novelty, this article shows that attributing the right receptive field size (context) based on the traversing range of the color channel may lead to a substantial performance gain for the task of UIR. Further, it is important to suppress the irrelevant multi-contextual features and increase the representational power of the model. Therefore, as a second novelty, we have incorporated an attentive skip mechanism to adaptively refine the learned multi-contextual features. The proposed framework, called Deep WaveNet, is optimized using the traditional pixel-wise and feature-based cost functions. An extensive set of experiments have been carried out to show the efficacy of the proposed scheme over existing best-published literature on benchmark datasets. More importantly, we have demonstrated a comprehensive validation of enhanced images across various high-level vision tasks, e.g., underwater image semantic segmentation and diver's 2D pose estimation. A sample video to exhibit our real-world performance is available at https://tinyurl.com/yzcrup9n. Also, we have open-sourced our framework at https://github.com/pksvision/Deep-WaveNet-Underwater-Image-Restoration.  © 2023 Association for Computing Machinery.",deep learning; enhancement; Image restoration; super-resolution; underwater vision,Color; Cost functions; Deep neural networks; HTTP; Image enhancement; Restoration; Semantic Segmentation; Semantics; Color distortions; Contextual feature; Deep learning; Enhancement; Low contrast; Low-high; Network engineering; Non-uniform; Superresolution; Underwater vision; Image reconstruction
Spherical Convolution Empowered Viewport Prediction in 360 Video Multicast with Limited FoV Feedback,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147938569&doi=10.1145%2f3511603&partnerID=40&md5=803236282b5538657d14dcf87275d6c4,"Field of view (FoV) prediction is critical in 360-degree video multicast, which is a key component of the emerging virtual reality and augmented reality applications. Most of the current prediction methods combining saliency detection and FoV information neither take into account that the distortion of projected 360-degree videos can invalidate the weight sharing of traditional convolutional networks nor do they adequately consider the difficulty of obtaining complete multi-user FoV information, which degrades the prediction performance. This article proposes a spherical convolution-empowered FoV prediction method, which is a multi-source prediction framework combining salient features extracted from 360-degree video with limited FoV feedback information. A spherical convolutional neural network is used instead of a traditional two-dimensional convolutional neural network to eliminate the problem of weight sharing failure caused by video projection distortion. Specifically, salient spatial-temporal features are extracted through a spherical convolution-based saliency detection model, after which the limited feedback FoV information is represented as a time-series model based on a spherical convolution-empowered gated recurrent unit network. Finally, the extracted salient video features are combined to predict future user FoVs. The experimental results show that the performance of the proposed method is better than other prediction methods.  © 2023 Association for Computing Machinery.",360-degree video; FoV prediction; saliency detection; spherical convolution; video multicast,Augmented reality; Forecasting; Multicasting; Spheres; Virtual reality; 360-degree video; Augmented reality applications; Convolutional neural network; Field of view prediction; Field of views; Prediction methods; Saliency detection; Spherical convolution; Video multicast; View predictions; Convolution
CL2R: Compatible Lifelong Learning Representations,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146419094&doi=10.1145%2f3564786&partnerID=40&md5=822ebe54d9877c102fea027372fe1f8f,"In this article, we propose a method to partially mimic natural intelligence for the problem of lifelong learning representations that are compatible. We take the perspective of a learning agent that is interested in recognizing object instances in an open dynamic universe in a way in which any update to its internal feature representation does not render the features in the gallery unusable for visual search. We refer to this learning problem as Compatible Lifelong Learning Representations (CL2R), as it considers compatible representation learning within the lifelong learning paradigm. We identify stationarity as the property that the feature representation is required to hold to achieve compatibility and propose a novel training procedure that encourages local and global stationarity on the learned representation. Due to stationarity, the statistical properties of the learned features do not change over time, making them interoperable with previously learned features. Extensive experiments on standard benchmark datasets show that our CL2R training procedure outperforms alternative baselines and state-of-the-art methods. We also provide novel metrics to specifically evaluate compatible representation learning under catastrophic forgetting in various sequential learning tasks. © 2022 Association for Computing Machinery.",compatible learning; Deep learning; fixed classifier; lifelong learning; representation learning,Compatible learning; Deep learning; Feature representation; Fixed classifier; Learning agents; Life long learning; Natural intelligence; Representation learning; Stationarity; Training procedures; Deep learning
Weakly Supervised Text-based Actor-Action Video Segmentation by Clip-level Multi-instance Learning,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148012225&doi=10.1145%2f3514250&partnerID=40&md5=fe04bfc3c61540544719ebcb4118a302,"In real-world scenarios, it is common that a video contains multiple actors and their activities. Selectively localizing one specific actor and its action spatially and temporally via a language query becomes a vital and challenging task. Existing fully supervised methods require extensive elaborately annotated data and are sensitive to the class labels, which cannot satisfy real-world applications' needs. Thus, we introduce the task of weakly supervised actor-action video segmentation from a sentence query (AAVSS) in this work, where only the video-sentence pairs are provided. To the best of our knowledge, our work is the first to perform AAVSS under weakly supervised situations. However, this task is extremely challenging not only because the task aims to learn the complex interactions between two heterogeneous modalities but also because the task needs to learn fine-grained analysis of video content without pixel-level annotations. To overcome the challenges, we propose a two-stage network. The network first follows the sentence guidance to localize the candidate region and then performs segmentation to achieve selective segmentation. Specifically, a novel tracker-based clip-level multiple instance learning paradigm is proposed in this article to learn the matches between regions and sentences, which makes our two-stage network robust to the region proposal network. Furthermore, two intrinsic characteristics of the video, temporal consistency and motion information, are utilized in companion with the weak supervision to facilitate the region-query matching. Through extensive experiments, the proposed method achieves comparable performance to state-of-the-art fully supervised approaches on two large-scale benchmarks, including A2D Sentences and J-HMDB Sentences. © 2023 Association for Computing Machinery.",cross-modal learning; Multiple instance learning; video actor-action segmentation; weakly supervised learning,Image segmentation; Learning systems; Supervised learning; Action segmentation; Clip level; Cross-modal; Cross-modal learning; Learn+; Multi-instance learning; Multiple-instance learning; Video actor-action segmentation; Video segmentation; Weakly supervised learning; Benchmarking
Dual Projective Zero-Shot Learning Using Text Descriptions,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147009087&doi=10.1145%2f3514247&partnerID=40&md5=fda2b06f2fd7027f028fbc19df93eebf,"Zero-shot learning (ZSL) aims to recognize image instances of unseen classes solely based on the semantic descriptions of the unseen classes. In this field, Generalized Zero-Shot Learning (GZSL) is a challenging problem in which the images of both seen and unseen classes are mixed in the testing phase of learning. Existing methods formulate GZSL as a semantic-visual correspondence problem and apply generative models such as Generative Adversarial Networks and Variational Autoencoders to solve the problem. However, these methods suffer from the bias problem since the images of unseen classes are often misclassified into seen classes. In this work, a novel model named the Dual Projective model for Zero-Shot Learning (DPZSL) is proposed using text descriptions. In order to alleviate the bias problem, we leverage two autoencoders to project the visual and semantic features into a latent space and evaluate the embeddings by a visual-semantic correspondence loss function. An additional novel classifier is also introduced to ensure the discriminability of the embedded features. Our method focuses on a more challenging inductive ZSL setting in which only the labeled data from seen classes are used in the training phase. The experimental results, obtained from two popular datasets - Caltech-UCSD Birds-200-2011 (CUB) and North America Birds (NAB) - show that the proposed DPZSL model significantly outperforms both the inductive ZSL and GZSL settings. Particularly in the GZSL setting, our model yields an improvement up to 15.2% in comparison with state-of-the-art CANZSL on datasets CUB and NAB with two splittings.  © 2023 Association for Computing Machinery.",autoencoder; generalized zero-shot learning; inductive zero-shot learning; Zero-shot learning,Birds; Learning systems; Semantics; Auto encoders; Bias problems; Caltech; Correspondence problems; Generalized zero-shot learning; Generative model; Inductive zero-shot learning; Learning settings; Semantic descriptions; Testing phase; Zero-shot learning
Towards Accurate Oriented Object Detection in Aerial Images with Adaptive Multi-level Feature Fusion,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145970349&doi=10.1145%2f3513133&partnerID=40&md5=64e79942a03a47ae47b54c06889ddbd2,"Detecting objects in aerial images is a long-standing and challenging problem since the objects in aerial images vary dramatically in size and orientation. Most existing neural network based methods are not robust enough to provide accurate oriented object detection results in aerial images since they do not consider the correlations between different levels and scales of features. In this paper, we propose a novel two-stage network-based detector with adaptive feature fusion towards highly accurate oriented object detection in aerial images, named AFF-Det. First, a multi-scale feature fusion module (MSFF) is built on the top layer of the extracted feature pyramids to mitigate the semantic information loss in the small-scale features. We also propose a cascaded oriented bounding box regression method to transform the horizontal proposals into oriented ones. Then the transformed proposals are assigned to all feature pyramid network (FPN) levels and aggregated by the weighted RoI feature aggregation (WRFA) module. The above modules can adaptively enhance the feature representations in different stages of the network based on the attention mechanism. Finally, a rotated decoupled-RCNN head is introduced to obtain the classification and localization results. Extensive experiments are conducted on the DOTA and HRSC2016 datasets to demonstrate the advantages of our proposed AFF-Det. The best detection results can achieve 80.73% mAP and 90.48% mAP, respectively, on these two datasets, outperforming recent state-of-the-art methods.  © 2023 Association for Computing Machinery.",aerial images; convolutional neural network; oriented object detection; Remote sensing images,Antennas; Convolutional neural networks; Feature extraction; Image fusion; Object recognition; Regression analysis; Remote sensing; Semantics; Aerial images; Convolutional neural network; Detecting objects; Feature pyramid; Features fusions; Multilevels; Network-based; Objects detection; Oriented object detection; Remote sensing images; Object detection
Quantum Fourier Convolutional Network,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147914540&doi=10.1145%2f3514249&partnerID=40&md5=4215bcc03d15cc28a65ae623bf1dbca3,"The neural network and quantum computing are both significant and appealing fields, with their interactive disciplines promising for large-scale computing tasks that are untackled by conventional computers. However, both developments are restricted by the scope of the hardware development. Nevertheless, many neural network algorithms had been proposed before GPUs became powerful enough for running very deep models. Similarly, quantum algorithms can also be proposed as knowledge reserve before real quantum computers are easily accessible. Specifically, taking advantage of both the neural networks and quantum computation and designing quantum deep neural networks (QDNNs) for acceleration on the Noisy Intermediate-Scale Quantum (NISQ) processors is also an important research problem. As one of the most widely used neural network architectures, convolutional neural network (CNN) remains to be accelerated by quantum mechanisms, with only a few attempts having been demonstrated. In this article, we propose a new hybrid quantum-classical circuit, namely, Quantum Fourier Convolutional Network (QFCN). Our model achieves exponential speedup compared with classical CNN theoretically and improves over the existing best result of quantum CNN. We demonstrate the potential of this architecture by applying it on different deep learning tasks, including traffic prediction and image classification.  © 2023 Association for Computing Machinery.",convolutional neural network; hybrid quantum-classical circuit; Quantum machine learning,Convolutional neural networks; Deep neural networks; Learning systems; Network architecture; Quantum computers; Timing circuits; Traffic control; Convolutional networks; Convolutional neural network; Fourier; Hybrid quantum-classical circuit; Machine-learning; Network computing; Quantum machine learning; Quantum machines; Quantum-classical; Convolution
Deep Uncoupled Discrete Hashing via Similarity Matrix Decomposition,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148009297&doi=10.1145%2f3524021&partnerID=40&md5=d25229f2b5c66bbe57a1f0f4e0cc569d,"Hashing has been drawing increasing attention in the task of large-scale image retrieval owing to its storage and computation efficiency, especially the recent asymmetric deep hashing methods. These approaches treat the query and database in an asymmetric way and can take full advantage of the whole training data. Though it has achieved state-of-the-art performance, asymmetric deep hashing methods still suffer from the large quantization error and efficiency problem on large-scale datasets due to the tight coupling between the query and database. In this article, we propose a novel asymmetric hashing method, called Deep Uncoupled Discrete Hashing (DUDH), for large-scale approximate nearest neighbor search. Instead of directly preserving the similarity between the query and database, DUDH first exploits a small similarity-transfer image set to transfer the underlying semantic structures from the database to the query and implicitly keep the desired similarity. As a result, the large similarity matrix is decomposed into two relatively small ones and the query is decoupled from the database. Then both database codes and similarity-transfer codes are directly learned during optimization. The quantization error of DUDH only exists in the process of preserving similarity between the query and similarity-transfer set. By uncoupling the query from the database, the training cost of optimizing the CNN model for the query is no longer related to the size of the database. Besides, to further accelerate the training process, we propose to optimize the similarity-transfer codes with a constant-approximation solution. In doing so, the training cost of optimizing similarity-transfer codes can be almost ignored. Extensive experiments on four widely used image retrieval benchmarks demonstrate that DUDH can achieve state-of-the-art retrieval performance with remarkable training cost reduction (30× - 50× relative). © 2023 Association for Computing Machinery.",Deep hashing; large-scale image retrieval; similarity-transfer,Benchmarking; Codes (symbols); Computational efficiency; Digital storage; Image retrieval; Large dataset; Matrix algebra; Nearest neighbor search; Query processing; Semantics; Deep hashing; Hashing method; Large-scale image retrieval; Large-scales; Matrix decomposition; Quantization errors; Similarity matrix; Similarity-transfer; Storage efficiency; Training costs; Database systems
Guided Graph Attention Learning for Video-Text Matching,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146423773&doi=10.1145%2f3538533&partnerID=40&md5=97c43e3063b59f8de8c9da81128b5dbb,"As a bridge between videos and natural languages, video-text matching has been a hot multimedia research topic in recent years. Such cross-modal retrieval is usually achieved by learning a common embedding space where videos and text captions are directly comparable. It is still challenging because existing visual representations cannot exploit semantic correlations within videos well, resulting in a mismatch with semantic concepts that are contained in the corresponding text descriptions. In this article, we propose a new Guided Graph Attention Learning (GGAL) model to enhance video embedding learning by capturing important region-level semantic concepts within the spatiotemporal space. Our model builds connections between object regions and performs hierarchical graph reasoning on both frame-level and whole video-level region graphs. During this process, global context is used to guide attention learning on this hierarchical graph topology so that the learned overall video embedding can focus on essential semantic concepts and can be better aligned with text captions. Experiments on commonly used benchmarks validate that GGAL outperforms many recent video-text retrieval methods with a clear margin. As multimedia data in dynamic environments becomes critically important, we also validate GGAL learned video-text representations that can be generalized well to unseen out-of-domain data via cross-dataset evaluations. To further investigate the interpretability of our model, we visualize attention weights learned by GGAL models. We find that GGAL successfully focuses on key semantic concepts in the video and has complementary attention on the context parts based on different ways of building region graphs.  © 2022 Association for Computing Machinery.",Cross-modal retrieval; graph neural networks; multimedia applications; video-text embedding,Graph neural networks; Information retrieval; Semantics; Topology; Cross-modal; Cross-modal retrieval; Embeddings; Graph neural networks; Hierarchical graphs; Learning models; Multimedia applications; Semantic concept; Text-matching; Video-text embedding; Embeddings
Generative Metric Learning for Adversarially Robust Open-world Person Re-Identification,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148016451&doi=10.1145%2f3522714&partnerID=40&md5=113f0759d829020206162d51bbff3a0e,"The vulnerability of re-identification (re-ID) models under adversarial attacks is of significant concern as criminals may use adversarial perturbations to evade surveillance systems. Unlike a closed-world re-ID setting (i.e., a fixed number of training categories), a reliable re-ID system in the open world raises the concern of training a robust yet discriminative classifier, which still shows robustness in the context of unknown examples of an identity. In this work, we improve the robustness of open-world re-ID models by proposing a generative metric learning approach to generate adversarial examples that are regularized to produce robust distance metric. The proposed approach leverages the expressive capability of generative adversarial networks to defend the re-ID models against feature disturbance attacks. By generating the target people variants and sampling the triplet units for metric learning, our learned distance metrics are regulated to produce accurate predictions in the feature metric space. Experimental results on the three re-ID datasets, i.e., Market-1501, DukeMTMC-reID, and MSMT17 demonstrate the robustness of our method. © 2023 Association for Computing Machinery.",Adversarial attack; generative metric learning; open-world person re-identification; robust models,Generative adversarial networks; Adversarial attack; Distance metrics; Generative metric learning; Identification modeling; Metric learning; Open world; Open-world person re-identification; Person re identifications; Re identifications; Robust modeling; Learning systems
Double Attention Based on Graph Attention Network for Image Multi-Label Classification,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148017547&doi=10.1145%2f3519030&partnerID=40&md5=eebef4ce52fa23bb7eb07d9dc61223af,"The task of image multi-label classification is to accurately recognize multiple objects in an input image. Most of the recent works need to leverage the label co-occurrence matrix counted from training data to construct the graph structure, which are inflexible and may degrade model generalizability. In addition, these methods fail to capture the semantic correlation between the channel feature maps to further improve model performance. To address these issues, we propose DA-GAT (a Double Attention framework based on the Graph Attention neTwork) to effectively learn the correlation between labels from training data. First, we devise a new channel attention mechanism to enhance the semantic correlation between channel feature maps, so as to implicitly capture the correlation between labels. Second, we propose a new label attention mechanism to avoid the adverse impact of a manually constructed label co-occurrence matrix. It only needs to leverage the label embedding as the input of network, then automatically constructs the label relation matrix to explicitly establish the correlation between labels. Finally, we effectively fuse the output of these two attention mechanisms to further improve model performance. Extensive experiments are conducted on three public multi-label classification benchmarks. Our DA-GAT model achieves mean average precision of 87.1%, 96.6%, and 64.3% on MS-COCO 2014, PASCAL VOC 2007, and NUS-WIDE, respectively, and obviously outperforms other existing state-of-the-art methods. In addition, visual analysis experiments demonstrate that each attention mechanism can capture the correlation between labels well and significantly promote the model performance. © 2023 Association for Computing Machinery.",channel attention mechanism; graph attention network; label correlation; Multi-label classification; visual analysis,Classification (of information); Image classification; Matrix algebra; Attention mechanisms; Channel attention mechanism; Cooccurrence matrixes (COM); Feature map; Graph attention network; Label correlations; Modeling performance; Multi-label classifications; Training data; Visual analysis; Semantics
SLAM for Indoor Parking: A Comprehensive Benchmark Dataset and a Tightly Coupled Semantic Framework,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147950130&doi=10.1145%2f3510856&partnerID=40&md5=c0d5667a9a172056b2adb492eb334316,"For the task of autonomous indoor parking, various Visual-Inertial Simultaneous Localization And Mapping (SLAM) systems are expected to achieve comparable results with the benefit of complementary effects of visual cameras and the Inertial Measurement Units. To compare these competing SLAM systems, it is necessary to have publicly available datasets, offering an objective way to demonstrate the pros/cons of each SLAM system. However, the availability of such high-quality datasets is surprisingly limited due to the profound challenge of the groundtruth trajectory acquisition in the Global Positioning Satellite denied indoor parking environments. In this article, we establish BeVIS, a large-scale Benchmark dataset with Visual (front-view), Inertial and Surround-view sensors for evaluating the performance of SLAM systems developed for autonomous indoor parking, which is the first of its kind where both the raw data and the groundtruth trajectories are available. In BeVIS, the groundtruth trajectories are obtained by tracking artificial landmarks scattered in the indoor parking environments, whose coordinates are recorded in a surveying manner with a high-precision Electronic Total Station. Moreover, the groundtruth trajectories are comprehensively evaluated in terms of two respects, the reprojection error and the pose volatility, respectively. Apart from BeVIS, we propose a novel tightly coupled semantic SLAM framework, namely VISSLAM-2, leveraging Visual (front-view), Inertial, and Surround-view sensor modalities, specially for the task of autonomous indoor parking. It is the first work attempting to provide a general form to model various semantic objects on the ground. Experiments on BeVIS demonstrate the effectiveness of the proposed VISSLAM-2. Our benchmark dataset BeVIS is publicly available at https://shaoxuan92.github.io/BeVIS.  © 2023 Association for Computing Machinery.",Autonomous indoor parking; benchmark dataset; Electronic Total Station; groundtruth trajectory acquisition; semantic SLAM,Benchmarking; Indoor positioning systems; Large dataset; Robotics; Trajectories; Autonomous indoor parking; Benchmark datasets; Electronic total station; Groundtruth trajectory acquisition; Localisation Systems; Mapping systems; Semantic simultaneous localization and mapping; Simultaneous localization and mapping; Tightly-coupled; Total station; Semantics
Disentangle Saliency Detection into Cascaded Detail Modeling and Body Filling,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147935415&doi=10.1145%2f3513134&partnerID=40&md5=aade680d9acc1ca68906da516048ee8d,"Salient object detection has been long studied to identify the most visually attractive objects in images/videos. Recently, a growing amount of approaches have been proposed, all of which rely on the contour/edge information to improve detection performance. The edge labels are either put into the loss directly or used as extra supervision. The edge and body can also be learned separately and then fused afterward. Both methods either lead to high prediction errors near the edge or cannot be trained in an end-to-end manner. Another problem is that existing methods may fail to detect objects of various sizes due to the lack of efficient and effective feature fusion mechanisms. In this work, we propose to decompose the saliency detection task into two cascaded sub-tasks, i.e., detail modeling and body filling. Specifically, detail modeling focuses on capturing the object edges by supervision of explicitly decomposed detail label that consists of the pixels that are nested on the edge and near the edge. Then the body filling learns the body part that will be filled into the detail map to generate more accurate saliency map. To effectively fuse the features and handle objects at different scales, we have also proposed two novel multi-scale detail attention and body attention blocks for precise detail and body modeling. Experimental results show that our method achieves state-of-the-art performances on six public datasets.  © 2023 Association for Computing Machinery.",foreground segmentation; Salient object detection; visual saliency,Object detection; Object recognition; Contour edges; Detection performance; Edge information; Edge labels; End to end; Foreground segmentation; Prediction errors; Saliency detection; Salient object detection; Visual saliency; Filling
Boolean-based Two-in-One Secret Image Sharing by Adaptive Pixel Grouping,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147992647&doi=10.1145%2f3517140&partnerID=40&md5=13c04f25d27f43e1773e91e8427328f8,"The two-in-one secret image sharing (TiOSIS) technique is a hybrid scheme that protects a secret image by combining visual cryptography (VCS) and polynomial-based secret image sharing (PSIS). There are two decoding methods available in TiOSIS: stacking-to-see decryption and lossless image recovery. However, the majority of current TiOSIS methods use Lagrange interpolation to precisely reconstruct the secret, which would result in intense computations. In this article, an efficient TiOSIS scheme using Boolean XOR operation for lossless image recovery is proposed. The proposed scheme consists of three building blocks: shared data generation, shadow construction, and image decryption. In shared data generation, the grayscale secret image is processed by a Boolean-based SIS to derive the shared bits. In shadow construction, an adaptive pixel grouping (APG) strategy is utilized to determine a grouping pattern. The halftone image adjustment algorithm is adopted to generate a suitable halftone image. With the grouping pattern and halftone image, we construct the shadows via the group-pixel embedding and sharing approach. In image decryption, we can reveal the secret image by stacking-to-see decoding or Boolean-based lossless image recovery. Extensive experiments and comparisons are illustrated to show the effectiveness and benefits of the proposed scheme. © 2023 Association for Computing Machinery.",Boolean operation; secret image sharing; Secret sharing; visual cryptography,Cryptography; Decoding; Recovery; Boolean operations; Halftone images; Image recovery; Lossless; Secret image sharing; Secret images; Secret-sharing; Stackings; Two in ones; Visual cryptography; Pixels
Boosting Scene Graph Generation with Visual Relation Saliency,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147912211&doi=10.1145%2f3514041&partnerID=40&md5=00aa2b0f456339c72267bd5494f3e4cc,"The scene graph is a symbolic data structure that comprehensively describes the objects and visual relations in a visual scene, while ignoring the inherent perceptual saliency of each visual relation (i.e., relation saliency). However, humans often quickly allocate attention to important/salient visual relations in a scene. To align with such human perception of a scene, we explicitly model the perceptual saliency of visual relation in scene graph by upgrading each graph edge (i.e., visual relation) with an attribute of relation saliency. We present a new design, named as Saliency-guided Message Passing (SMP), that boosts the generation of such scene graph structure with the guidance from the visual relation saliency. Technically, an object interaction encoder is first utilized to strengthen object relation representations by jointly exploiting the appearance, semantic, and spatial relations in between. A branch is further leveraged to estimate the relation saliency of each visual relation by ordinal regression. Next, conditioned on the object and relation features (coupled with the estimated relation saliency), our SMP enhances scene graph generation by performing message passing over the objects and the most salient relations. Extensive experiments on VG-KR and VG150 datasets demonstrate the superiority of SMP for the scene graph generation. Moreover, we empirically validate the compelling generalizability of the learned scene graphs via SMP on downstream tasks like cross-model retrieval and image captioning.  © 2023 Association for Computing Machinery.",relation saliency; Scene graph generation,Graphic methods; Semantics; Graph edges; Graph generation; Human perception; Message-passing; Relation saliency; Scene graph generation; Scene graph structures; Scene-graphs; Symbolic data; Visual scene; Message passing
Boosting Vision-and-Language Navigation with Direction Guiding and Backtracing,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148047062&doi=10.1145%2f3526024&partnerID=40&md5=d40c263204fad0b8f115d71fa68ed3e5,"Vision-and-Language Navigation (VLN) has been an emerging and fast-developing research topic, where an embodied agent is required to navigate in a real-world environment based on natural language instructions. In this article, we present a Direction-guided Navigator Agent (DNA) that novelly integrates direction clues derived from instructions into the essential encoder-decoder navigation framework. Particularly, DNA couples the standard instruction encoder with an additional direction branch which sequentially encodes the direction clues in the instructions to boost navigation. Furthermore, an Instruction Flipping mechanism is uniquely devised to enable fast data augmentation as well as a follow-up backtracing for navigating the agent in a backward direction. Such a way naturally amplifies the grounding of instruction in the local visual scenes along both forward and backward directions, and thus strengthens the alignment between instruction and action sequence. Extensive experiments conducted on Room to Room (R2R) dataset validate our proposal and demonstrate quantitatively compelling results. © 2023 Association for Computing Machinery.",cross-modal matching; Vision-and-language navigation,Signal encoding; Backtracing; Cross-modal; Cross-modal matching; Embodied agent; Encoder-decoder; Modal matching; Natural languages; Real world environments; Research topics; Vision-and-language navigation; Navigation
Animating Still Natural Images Using Warping,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147937606&doi=10.1145%2f3511894&partnerID=40&md5=e7e6b20addfd393285f3d107f3ca354c,"From a single still image, a looping video could be generated by imparting subtle motion to objects in the image. The results are a hybrid of photography and video. They contain gentle motion in some objects, while the rest of the image remains still. Existing techniques are successful in animating such images. However, there are still some drawbacks that need to be investigated, such as too-large computation time necessary to retrieve the matched videos or the challenges of controlling the desired motion not only in terms of a single region but also in terms of consistency in regions. In this work, we address these issues by proposing an interactive system with a novel warping method. The key idea of our approach is to utilize user's annotations to impart motion to certain objects. With two proposed phases in terms of preserve-curve-warping and cycle warping, a looping video is generated. We demonstrate the effectiveness of our method via various experimental challenging results and evaluations. We show that with a simple and lightweight method, our system is able to deal with animating a still image's problems and results in realistic motion and appealing videos. In addition, using our proposed system, it is easy to create plausible animation using simple user annotations without referencing the video database or machine learning models and allows ordinary users with minimal expertise to produce compelling results.  © 2023 Association for Computing Machinery.",Animation; cycle warping; preserve-curve-warping; still images,Interactive computer graphics; Computation time; Cycle warping; Natural images; Preserve-curve-warping; Simple++; Still-images; Subtle motions; User annotations; Warpings; Animation
Category-Stitch Learning for Union Domain Generalization,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148002598&doi=10.1145%2f3524136&partnerID=40&md5=3916ccb897d0ba6d33b6869bfd30b9df,"Domain generalization aims at generalizing the network trained on multiple domains to unknown but related domains. Under the assumption that different domains share the same classes, previous works can build relationships across domains. However, in realistic scenarios, the change of domains is always followed by the change of categories, which raises a difficulty for collecting sufficient aligned categories across domains. Bearing this in mind, this article introduces union domain generalization (UDG) as a new domain generalization scenario, in which the label space varies across domains, and the categories in unknown domains belong to the union of all given domain categories. The absence of categories in given domains is the main obstacle to aligning different domain distributions and obtaining domain-invariant information. To address this problem, we propose category-stitch learning (CSL), which aims at jointly learning the domain-invariant information and completing missing categories in all domains through an improved variational autoencoder and generators. The domain-invariant information extraction and sample generation cross-promote each other to better generalizability. Additionally, we decouple category and domain information and propose explicitly regularizing the semantic information by the classification loss with transferred samples. Thus our method can breakthrough the category limit and generate samples of missing categories in each domain. Extensive experiments and visualizations are conducted on MNIST, VLCS, PACS, Office-Home, and DomainNet datasets to demonstrate the effectiveness of our proposed method. © 2023 Association for Computing Machinery.",Domain generalization; generator; variational autoencoder,Learning systems; Semantics; Auto encoders; Different domains; Domain distribution; Domain generalization; Generalisation; Generator; Label space; Multiple domains; Realistic scenario; Variational autoencoder; Classification (of information)
A Novel GAPG Approach to Automatic Property Generation for Formal Verification: The GAN Perspective,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148001146&doi=10.1145%2f3517154&partnerID=40&md5=a8276730a27b8f6430fe0dc4bcdb2548,"Formal methods have been widely used to support software testing to guarantee correctness and reliability. For example, model checking technology attempts to ensure that the verification property of a specific formal model is satisfactory for discovering bugs or abnormal behavior from the perspective of temporal logic. However, because automatic approaches are lacking, a software developer/tester must manually specify verification properties. A generative adversarial network (GAN) learns features from input training data and outputs new data with similar or coincident features. GANs have been successfully used in the image processing and text processing fields and achieved interesting and automatic results. Inspired by the power of GANs, in this article, we propose a GAN-based automatic property generation (GAPG) approach to generate verification properties supporting model checking. First, the verification properties in the form of computational tree logic (CTL) are encoded and used as input to the GAN. Second, we introduce regular expressions as grammar rules to check the correctness of the generated properties. These rules work to detect and filter meaningless properties that occur because the GAN learning process is uncontrollable and may generate unsuitable properties in real applications. Third, the learning network is further trained by using labeled information associated with the input properties. These are intended to guide the training process to generate additional new properties, particularly those that map to corresponding formal models. Finally, a series of comprehensive experiments demonstrate that the proposed GAPG method can obtain new verification properties from two aspects: (1) using only CTL formulas and (2) using CTL formulas combined with Kripke structures. © 2023 Association for Computing Machinery.",automatic property generation; computational tree logic; correctness and reliability; generative adversarial network (GAN); Model checking; verification property,Computer circuits; Formal verification; Generative adversarial networks; Image processing; Learning systems; Software reliability; Software testing; Temporal logic; Automatic property generation; Computational tree logic; Correctness and reliability; Formal modeling; Generative adversarial network; Logic formulas; Models checking; Property; Software testings; Verification properties; Model checking
RDH-DES: Reversible Data Hiding over Distributed Encrypted-Image Servers Based on Secret Sharing,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139951651&doi=10.1145%2f3512797&partnerID=40&md5=cd4df803e6a97d35c01d4a871e4e0c0d,"Reversible Data Hiding in Encrypted Image (RDHEI) schemes may redistribute the data hiding procedure to other parties and can preserve privacy of the cover image. Recently, cloud computing technology has led to the rapid growth of networked media, and many multimedia rights are owned by multiple parties, such as a film's producer and multiple distributors. Thus, the data hiding task could be distributed to multiple distributed servers. Multi-party data hiding has become an important demand for networked media. In addition, it is essential to preserve multi-server and multi-message privacy and data integrity. However, most of the RDHEI schemes involve only one data hider. That inspired us to design the secure multi-party embedding over distributed encrypted-image servers as a solution for multi-party RDHEI applications. In this article, we propose a novel Reversible Data Hiding over Distributed Encrypted-Image Servers (RDH-DES) based on secret sharing. The Chinese remainder theorem, secret sharing, and block-level scrambling are developed as a lightweight cryptography to generate the encrypted image shares. These shares are distributed to different image servers and are used to embed secret data in the proposed framework. The marked encrypted image can be constructed through the marked encrypted shares from different parties, and the decryption and extraction can be completed by the receiver. The experimental results and theoretical analysis have demonstrated that the proposed scheme is secure and effective.  © 2023 Association for Computing Machinery.",encrypted image; lightweight cryptography; multi-server and multi-message privacy; Reversible data hiding; secret sharing,Computation theory; Cryptography; Data privacy; Image processing; Encrypted images; Image servers; Light-weight cryptography; Message privacy; Multi-server and multi-message privacy; Multiservers; Reversible data hiding; Secret-sharing; Server-based; Steganography
AABLSTM: A Novel Multi-task Based CNN-RNN Deep Model for Fashion Analysis,2023,"ACM Transactions on Multimedia Computing, Communications and Applications",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148038360&doi=10.1145%2f3519029&partnerID=40&md5=09f22ef6a53e02ec05eba6a585a455e0,"With the rapid growth of online commerce and fashion-related applications, visual clothing analysis and recognition has become a hotspot in computer vision. In this paper, we propose a novel AABLSTM network, which is based on deep CNN-RNN, to solve the visual fashion analysis of clothing category classification, attribute detection, and landmark localization. The designed fashion model is leveraged with the multi-task driven mechanism as follows: firstly, a bidirectional LSTM (Bi-LSTM) branch is proposed for efficiently mining the semantic association between related attributes so as to improve the precision of clothing category classification and attribute detection; then, an imitated hourglass sub-network of ""down-up sampling""is constructed for boosting the accuracy of fashion landmark localization; and finally, a specially designed multi-loss function is constructed to better optimize the network training. Extensive experimental results on large-scale fashion datasets demonstrate the superior performance of our approach. © 2023 Association for Computing Machinery.",Attribute detection; CNN-RNN deep architecture; landmark localization; multi-task mechanism,Large dataset; Long short-term memory; Attribute detections; Category Classification; CNN-RNN deep architecture; Deep architectures; Landmark localization; Multi tasks; Multi-task mechanism; On-line fashion; Rapid growth; Task-based; Semantics
