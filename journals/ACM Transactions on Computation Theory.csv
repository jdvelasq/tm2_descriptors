Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
Input-oblivious proof systems and a uniform complexity perspective on P/poly,2015,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941011954&doi=10.1145%2f2799645&partnerID=40&md5=88b7c4efb5807541b903c78f048d9ad1,"An input-oblivious proof system is a proof system in which the proof does not depend on the claim being proved. Input-oblivious versions of NP and MA were introduced in passing by Fortnow, Santhanam, and Williams, who also showed that those classes are related to questions on circuit complexity. In this article, we wish to highlight the notion of input-oblivious proof systems and initiate a more systematic study of them. We begin by describing in detail the results of Fortnow et al. and discussing their connection to circuit complexity. We then extend the study to input-oblivious versions of IP, PCP, and Z K, and present few preliminary results regarding those versions. © 2015 ACM.",BPP; E; EXP; F.1.2 [Theory of Computation]: Computation by Abstract Devices; IP; MA; Modes of computation; NE; NEXP; NP; P/poly; PCP; RP; Theory; ZK,Computation theory; Computational methods; Computer science; Einsteinium; Neon; Neptunium; Abstract devices; Circuit complexity; NEXP; Proof system; Systematic study; Theory; Uniform complexity; Williams; Abstracting
Exploring the subexponential complexity of completion problems,2015,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941000787&doi=10.1145%2f2799640&partnerID=40&md5=d18f3a1bcf7e1820b0c76753e5726909,"Let F be a family of graphs. In the F-COMPLETION problem, we are given an n-vertex graph G and an integer k as input, and asked whether at most k edges can be added to G so that the resulting graph does not contain a graph from F as an induced subgraph. It was shown recently that two special cases of F-COMPLETION, namely, (i) the problem of completing into a chordal graph known as MINIMUM FILL-IN (SIAM J. Comput. 2013), which corresponds to the case of F = {C4, C5, C6, . . .}, and (ii) the problem of completing into a split graph (Algorithmica 2015), that is, the case of F = {C4, 2K2, C5}, are solvable in parameterized subexponential time 2O(√k log K)nO(1). The exploration of this phenomenon is the main motivation for our research on F-COMPLETION. In this article, we prove that completions into several well-studied classes of graphs without long induced cycles and paths also admit parameterized subexponential time algorithms by showing that: - The problem TRIVIALLY PERFECT COMPLETION, which is F-COMPLETION for F = {C4, P4}, a cycle and a path on four vertices, is solvable in parameterized subexponential time 2O(√k log k)nO(1). - The problems known in the literature as PSEUDOSPLIT COMPLETION, the case in which F = {2K2, C4}, and THRESHOLD COMPLETION, in which F = {2K2 , P4, C4}, are also solvable in time 2O(√ k log k)nO(1). We complement our algorithms for F-COMPLETION with the following lower bounds: - For F = {2K2}, F = {C4}, F = {P4}, and F = {2K2, P4}, F-COMPLETION cannot be solved in time 2o(k)nO(1) unless the Exponential Time Hypothesis (ETH) fails. Our upper and lower bounds provide a complete picture of the subexponential parameterized complexity of F-COMPLETION problems for any F ⊆ {2K2, C4, P4}. 2015 Copyright is held by the owner/author(s).",Algorithms; Completion; Edge completion; G.2.2 [Graph Theory]: Graph Algorithms; Modification; Parameterized; Subexponential; Subexponential parameterized complexity,Algorithms; Graph algorithms; Parameter estimation; Parameterization; Well completion; Completion problem; Exponential time hypothesis; Modification; Parameterized; Parameterized complexity; Subexponential; Subexponential time; Upper and lower bounds; Graph theory
Quantum XOR games,2015,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940984217&doi=10.1145%2f2799560&partnerID=40&md5=2e6dc9f8021c0c7a133c898a53fb72ed,"We introduce quantum XOR games, a model of two-player, one-round games that extends the model of XOR games by allowing the referee's questions to the players to be quantum states. We give examples showing that quantum XOR games exhibit a wide range of behaviors that are known not to exist for standard XOR games, such as cases in which the use of entanglement leads to an arbitrarily large advantage over the use of no entanglement. By invoking two deep extensions of Grothendieck's inequality, we present an efficient algorithm that gives a constant-factor approximation to the best performance that players can obtain in a given game, both in the case that they have no shared entanglement and that they share unlimited entanglement. As a byproduct of the algorithm, we prove some additional interesting properties of quantum XOR games, such as the fact that sharing a maximally entangled state of arbitrary dimension gives only a small advantage over having no entanglement at all. © 2015 ACM.",Entangled games; F.1.2. [modes of computation]: quantum computing; Grothendieck inequality; Theory; XOR games,Algorithms; Approximation algorithms; Computation theory; Computer games; Quantum computers; Quantum optics; Quantum theory; Entangled games; Grothendieck inequality; Quantum Computing; Theory; XOR games; Quantum entanglement
On approximate decidability of minimal programs,2015,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940973882&doi=10.1145%2f2799561&partnerID=40&md5=5616ec522fd485f74a7f641447fad800,"An index e in a numbering of partial-recursive functions is called minimal if every lesser index computes a different function from e. Since the 1960s, it has been known that, in any reasonable programming language, no effective procedure determines whether or not a given index is minimal. We investigate whether the task of determining minimal indices can be solved in an approximate sense. Our first question, regarding the set of minimal indices, is whether there exists an algorithm that can correctly label 1 out of k indices as either minimal or nonminimal. Our second question, regarding the function that computes minimal indices, is whether one can compute a short list of candidate indices that includes a minimal index for a given program. We give negative answers to both questions for the important case of numberings with linearly bounded translators. © 2015 ACM.",Frequency computation; Kolmogorov complexity; List approximations; Minimal indices; Numberings; Theory of computation → Computability,Computability and decidability; Computational complexity; Frequency computations; Kolmogorov complexity; List approximations; Minimal indices; Numberings; Theory of computation; Recursive functions
Mutual dimension,2015,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937880303&doi=10.1145%2f2786566&partnerID=40&md5=0b4da8f4fd660f7e9eb3df8ed2df8889,"We define the lower and upper mutual dimensions mdim(x : y) and Mdim(x : y) between any two points x and y in Euclidean space. Intuitively, these are the lower and upper densities of the algorithmic information shared by x and y. We show that these quantities satisfy the main desiderata for a satisfactory measure of mutual algorithmic information. Our main theorem, the data processing inequality for mutual dimension, says that if f : Rm→Rn is computable and Lipschitz, then the inequalities mdim( f (x) : y) ≠ mdim(x : y) and Mdim( f (x) : y) ≠ Mdim(x : y) hold for all x ∈ Rm and y ∈ Rt. We use this inequality and related inequalities that we prove in like fashion to establish conditions under which various classes of computable functions on Euclidean space preserve or otherwise transform mutual dimensions between points. © 2015 ACM.",Computable analysis; Data processing inequality; Effective fractal dimensions; Kolmogorov complexity; Mutual information,Fractal dimension; Geometry; Algorithmic informations; Computable analysis; Computable functions; Euclidean spaces; Kolmogorov complexity; Lipschitz; Mutual informations; Two-point; Data handling
Some hard families of parameterized counting problems,2015,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937920227&doi=10.1145%2f2786017&partnerID=40&md5=40bc85007024b6851b51b9f2d0850981,"We consider parameterized subgraph counting problems of the following form: given a graph G, how many k-tuples of its vertices induce a subgraph with a given property? A number of such problems are known to be #W[1]-complete; here, we substantially generalize some of these existing results by proving hardness for two large families of such problems. We demonstrate that it is #W[1]-hard to count the number of k-vertex subgraphs having any property where the number of distinct edge densities of labeled subgraphs that satisfy the property is o(k2). In the special case in which the property in question depends only on the number of edges in the subgraph, we give a strengthening of this result, which leads to our second family of hard problems. © 2015 ACM.",Counting complexity; Parameterized complexity; Ramsey theory,Parameterization; Counting complexity; Counting problems; Edge densities; Hard problems; Parameterized; Parameterized complexity; Ramsey theory; Subgraphs; Graph theory
Lower bounds on the deterministic and quantum communication complexity of hamming-distance problems,2015,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934757096&doi=10.1145%2f2698587&partnerID=40&md5=6cc68ca9d48417fabe144d38e910d182,"Alice and Bob want to know if two strings of length n are almost equal. That is, do the strings differ on at most a bits? Let 0 ≤ a ≤ n-1. We show (1) any deterministic protocol-as well as any error-free quantum protocol (C∗ version)-for this problem requires at least n-2 bits of communication, and (2) a lower bound of n/2-1 for error-free Q∗ quantum protocols. We also show the same results for determining if two strings differ in exactly a bits. Our results are obtained by lower-bounding the ranks of the appropriate matrices. © 2015 ACM.",Communication complexity; Hamming distance,Computational complexity; Quantum communication; Communication complexity; Deterministic protocols; Lower bounds; Quantum communication complexity; Quantum protocols; Hamming distance
Using parametric transformations toward polynomial kernels for packing problems allowing overlaps,2015,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934754557&doi=10.1145%2f2786015&partnerID=40&md5=553185b93b4547909051c7e2c798f235,"We consider the problem of discovering overlapping communities in networks that we model as generalizations of the Set and Graph Packing problems with overlap. As usual for Set Packing problems, we seek a collection S′⊆ S consisting of at least κ sets subject to certain disjointness restrictions. In the r-Set Packing with t-Membership, each element of U belongs to at most t sets of S′, while in r-Set Packing with t-Overlap, each pair of sets in S′ overlaps in at most t elements. For both problems, each set of S has at most r elements Similarly, both of our Graph Packing problems seek a collection Κ of at least κ subgraphs in a graph G, each isomorphic to a graph H ∈ H. In H-Packing with t-Membership, each vertex of G belongs to at most t subgraphs of κ, while in H-Packing with t-Overlap, each pair of subgraphs in κ overlaps in at most t vertices For both problems, each member of H has at most r vertices and medges, where t, r, and mare constants. Here, we show NP-completeness results for all of our packing problems. Furthermore, we give a dichotomy result for the H-Packing with t-Membership problem analogous to the Kirkpatrick and Hell dichotomy [Kirkpatrick and Hell 1978]. Using polynomial parameter transformations, we reduce the r-Set Packing with t-Membership to a problem kernel with O((r + 1)rκr) elements and the H-Packing with t-Membership and its edge version to problem kernels with O((r + 1)rκr) and O((m+ 1)mκm) vertices, respectively. On the other hand, by generalizing [Fellows et al. 2008; Moser 2009], we achieve a kernel with O(rrκr-t-1)elements for the r-Set Packing with t-Overlap and kernels with O(rrκr-t-1) and O(mmκm-t-1) vertices for the H-Packing with t-Overlap and its edge version, respectively. In all cases, κ is the input parameter, while t, r, and mare constants. © 2015 ACM.",Graph packing; Kernels; Overlap; Polynomial parametric transformations; Set packing,Polynomial approximation; Graph packing; Kernels; Overlap; Parametric transformation; Set packing; Graph theory
Exponential lower bounds for AC0-Frege imply superpolynomial Frege lower bounds,2015,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930165488&doi=10.1145%2f2656209&partnerID=40&md5=a5ca6c5cd820b67a3f7d2c6835980553,"We give a general transformation that turns polynomial-size Frege proofs into subexponential-size AC0-Frege proofs. This indicates that proving truly exponential lower bounds for AC0-Frege is hard, as it is a long-standing open problem to prove superpolynomial lower bounds for Frege. Our construction is optimal for proofs of formulas of unbounded depth. As a consequence of our main result, we are able to shed some light on the question of automatizability for bounded-depth Frege systems. First, we present a simpler proof of the results of Bonet et al. showing that under cryptographic assumptions, bounded-depth Frege proofs are not automatizable. Second, we show that because our proof is more general, under the right cryptographic assumptions, it could resolve the automatizability question for lower-depth Frege systems. © 2015 ACM 1942-3454/2015/05-ART5 15.00.",Proof complexity,Computation theory; Computational methods; Computer science; Automatizability; Cryptographic assumptions; Frege proofs; Lower bounds; Polynomial size; Proof complexity; Cryptography
"Pebbling, entropy, and branching program size lower bounds",2015,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930160999&doi=10.1145%2f2751320&partnerID=40&md5=002de2c02fb199af7a5da87911d54a8b,"We contribute to the program of proving lower bounds on the size of branching programs solving the Tree Evaluation Problem introduced by Cook et al. [2012]. Proving a superpolynomial lower bound for the size of nondeterministic thrifty branching programs would be an important step toward separating NL from P using the tree evaluation problem. First, we show that Read-Once Nondeterministic Thrifty BPs are equivalent to whole black-white pebbling algorithms, thus showing a tight lower bound (ignoring polynomial factors) for this model. We then introduce a weaker restriction of nondeterministic thrifty branching programs called Bitwise Independence. The best known [Cook et al. 2012] nondeterministic thrifty branching programs (of size O(kh/2+)) for the tree evaluation problem are Bitwise Independent. As our main result, we show that any Bitwise Independent Nondeterministic Thrifty Branching Program solving BT2(h, k) must have at least (k/2)h/2 states. Prior to this work, lower bounds were known for nondeterministic thrifty branching programs only for fixed heights h = 2, 3, 4 [Cook et al. 2012]. We prove our results by associating a fractional blackwhite pebbling strategy with any bitwise independent nondeterministic thrifty branching program solving the Tree Evaluation Problem. Such a connection was not known previously, even for fixed heights. Our main technique is the entropy method introduced by Jukna and Zák [2001] originally in the context of proving lower bounds for read-once branching programs. We also show that the previous lower bounds known [Cook et al. 2012] for deterministic branching programs for the Tree Evaluation Problem can be obtained using this approach. Using this method, we also show tight lower bounds for any k-way deterministic branching program solving the Tree Evaluation Problem when the instances are restricted to have the same group operation in all internal nodes. © 2015 ACM 1942-3454/2015/05-ART8 15.00.",Branching programs; Entropy; Lower bounds; Pebbling games,Algorithms; Computer Programs; Entropy; Problem Solving; Computation theory; Computational methods; Computer science; Entropy; Branching programs; Entropy methods; Group operations; Lower bounds; Pebbling games; Polynomial factor; Read-once branching programs; Tree evaluations; Forestry
The fine classification of conjunctive queries and parameterized logarithmic space,2015,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930150381&doi=10.1145%2f2751316&partnerID=40&md5=5db67ed6b5247f7728957f1ff3352d91,"We perform a fundamental investigation of the complexity of conjunctive query evaluation from the perspective of parameterized complexity. We classify sets of Boolean conjunctive queries according to the complexity of this problem. Previous work showed that a set of conjunctive queries is fixed-parameter tractable precisely when the set is equivalent to a set of queries having bounded treewidth. We present a fine classification of query sets up to parameterized logarithmic space reduction. We show that, in the bounded treewidth regime, there are three complexity degrees and that the properties that determine the degree of a query set are bounded pathwidth and bounded tree depth. We also engage in a study of the two higher degrees via logarithmic space machine characterizations and complete problems. Our work yields a significantly richer perspective on the complexity of conjunctive queries and, at the same time, suggests new avenues of research in parameterized complexity. © 2015 ACM 1942-3454/2015/05-ART7 15.00.",Conjunctive queries; Graph decompositions; Homomorphism problems; Parameterized logarithmic space,Computation theory; Computational methods; Computer science; Bounded treewidth; Complete problems; Conjunctive queries; Graph decompositions; Homomorphism problems; Machine characterization; Parameterized; Parameterized complexity; Parameterization
Smoothed complexity theory,2015,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930171067&doi=10.1145%2f2656210&partnerID=40&md5=b90915e72fdd9537a17ab3d6c3f9c133,"Smoothed analysis is a new way of analyzing algorithms introduced by Spielman and Teng. Classical methods like worst-case or average-case analysis have accompanying complexity classes, such as P and Avg-P, respectively. Whereas worst-case or average-case analysis give us a means to talk about the running time of a particular algorithm, complexity classes allow us to talk about the inherent difficulty of problems. Smoothed analysis is a hybrid of worst-case and average-case analysis and compensates some of their drawbacks. Despite its success for the analysis of single algorithms and problems, there is no embedding of smoothed analysis into computational complexity theory, which is necessary to classify problems according to their intrinsic difficulty. We propose a framework for smoothed complexity theory, define the relevant classes, and prove some first hardness results (of bounded halting and tiling) and tractability results (binary optimization problems, graph coloring, satisfiability) within this framework. © 2015 ACM 1942-3454/2015/05-ART6 15.00.",Average-case complexity; Computational complexity; Smoothed analysis,Algorithms; Graph theory; Optimization; Average-case analysis; Average-case complexity; Binary optimization; Classical methods; Complexity theory; Computational complexity theory; Particular algorithms; Smoothed analysis; Computational complexity
"Hardness of Max-2Lin and Max-3Lin over integers, reals, and large cyclic groups",2015,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930168274&doi=10.1145%2f2751322&partnerID=40&md5=45c12168f561222c2f13db730725993e,"In 1997, Håstad showed NP-hardness of (1 - ε, 1/q + δ)-approximating Max-3Lin(Zq); however, it was not until 2007 that Guruswami and Raghavendra were able to show NP-hardness of (1 - ε, δ)-approximating Max-3Lin(Z). In 2004, Khot-Kindler-Mossel-O'Donnell showed UG-hardness of (1 - ε, δ)-approximating Max-2Lin(Zq) for q = q(ε,δ) a sufficiently large constant; however, achieving the same hardness for Max-2Lin(Z) was given as an open problem in Raghavendra's 2009 thesis. In this work, we show that fairly simple modifications to the proofs of the Max-3Lin(Zq) and Max-2Lin(Zq) results yield optimal hardness results over Z. In fact, we show a kind of ""bicriteria"" hardness: Even when there is a (1 - ε)-good solution over Z, it is hard for an algorithm to find a δ-good solution over Z, R, or Zm for any m ≥ q(ε, δ) of the algorithm's choosing. © 2015 ACM 1942-3454/2015/05-ART9 15.00.",Hardness of approximation; Large alphabet sets; Max-2Lin; Max-3Lin; The unique games conjecture,NP-hard; Hardness of approximation; Large alphabets; Max-2Lin; Max-3Lin; Unique games conjecture; Hardness
Advice lower bounds for the dense model theorem,2014,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921479348&doi=10.1145%2f2676659&partnerID=40&md5=76442f99e273a2a505ff1dab14bf0c54,"We prove a lower bound on the amount of nonuniform advice needed by black-box reductions for the Dense Model Theorem of Green, Tao, and Ziegler, and of Reingold, Trevisan, Tulsiani, and Vadhan. The latter theorem roughly says that for every distribution D that is δ-dense in a distribution that is ε -indistinguishable from uniform, there exists a ""dense model"" for D, that is, a distribution that is δ-dense in the uniform distribution and is ε′-indistinguishable from D. This ε -indistinguishability is with respect to an arbitrary small class of functions F. For the natural case where ε ≥ Δ (ε δ) and ε ≥ δO(1), our lower bound implies that Δ( √ (1/ε) log(1/δ) log |F| ) advice bits are necessary for a certain type of reduction that establishes a stronger form of the Dense Model Theorem (and which encompasses all known proofs of the Dense Model Theorem in the literature). There is only a polynomial gap between our lower bound and the best upper bound for this case (due to Zhang), which is O ((1/ε2) log(1/δ) log |F|). Our lower bound can be viewed as an analogue of list size lower bounds for list-decoding of error-correcting codes, but for ""dense model decoding"" instead. © 2014 ACM.",,Computation theory; Computational methods; Computer science; Black-box reductions; Error correcting code; Indistinguishability; List decoding; Lower bounds; Uniform distribution; Upper Bound; Decoding
Limitations of local filters of lipschitz and monotone functions,2014,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921511330&doi=10.1145%2f2692372.2692373&partnerID=40&md5=16175a52d853824f4c7996652c6aa9ab,"We study local filters for two properties of functions of the form f : {0, 1}d → R: the Lipschitz property and monotonicity. A local filter with additive error a is a randomized algorithm that is given black-box access to a function f and a query point x in the domain of f. It outputs a value F(x) such that (i) the reconstructed function F(x) satisfies the property (in our case, is Lipschitz or monotone) and (ii) if the input function f satisfies the property, then for every point x in the domain (with high constant probability) the reconstructed value F(x) differs from f (x) by at most a. Local filters were introduced by Saks and Seshadhri [2010]. The relaxed definition we study is due to Bhattacharyya et al. [2012], except that we further relax it by allowing additive error. Local filters for Lipschitz and monotone functions have applications to areas such as data privacy. We show that every local filter for Lipschitz or monotone functions runs in time exponential in the dimension d, even when the filter is allowed significant additive error. Prior lower bounds (for local filters with no additive error, that is, with a = 0) applied only to a more restrictive class of filters, for example, nonadaptive filters. To prove our lower bounds, we construct families of hard functions and show that lookups of a local filter on these functions are captured by a combinatorial object that we call a c-connector. Then we present a lower bound on the maximum outdegree of a c-connector and show that it implies the desired bounds on the running time of local filters. Our lower bounds, in particular, imply the same bound on the running time for a class of privacy mechanisms. © 2014 ACM.",,Additives; Data privacy; Errors; Additive errors; Combinatorial objects; Input functions; Lipschitz property; Monotone functions; Privacy mechanisms; Properties of Functions; Randomized Algorithms; C (programming language)
Kernel lower bounds using co-nondeterminism: Finding induced hereditary subgraphs,2014,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921497651&doi=10.1145%2f2691321&partnerID=40&md5=d95a8e4ad7812a835ea7948c4975e628,"This work further explores the applications of co-nondeterminism for showing kernelization lower bounds. The only known example prior to this work excludes polynomial kernelizations for the so-called RAMSEY problem of finding an independent set or a clique of at least κ vertices in a given graph [Kratsch 2012]. We study the more general problem of finding induced subgraphs on κ vertices fulfilling some hereditary property Π called Π-INDUCED SUBGRAPH. The problem is NP-hard for all nontrivial choices of Π by a classic result of Lewis and Yannakakis [1980]. The parameterized complexity of this problem was classified by Khot and Raman [2002] depending on the choice of Π The interesting cases for kernelization are for Π containing all independent sets and all cliques, since the problem is trivially polynomial time solvable or W[1]-hard otherwise. Our results are twofold. Regarding Π -INDUCED SUBGRAPH, we show that for a large choice of natural graph properties Π including chordal, perfect, cluster, and cograph, there is no polynomial kernel with respect to κ. This is established by two theorems, each one capturing different (but not necessarily exclusive) sets of properties: one using a co-nondeterministic variant of OR-cross-composition and one by a polynomial parameter transformation from RAMSEY. Additionally, we show how to use improvement versions of NP-hard problems as source problems for lower bounds, without requiring their NP-hardness. For example, for Π -INDUCED SUBGRAPH our compositions may assume existing solutions of size κ-1. This follows from the more general fact that source problems for OR-(cross-)compositions need only be NP-hard under co-nondeterministic reductions. We believe this to be useful for further lower-bound proofs, for example, since improvement versions simplify the construction of a disjunction (OR) of instances required in compositions. This adds a second way of using co-nondeterminism for lower bounds. © 2014 ACM.",,NP-hard; Polynomial approximation; Graph properties; Hereditary property; Induced subgraphs; Lower-bound proofs; Parameterized complexity; Polynomial kernels; Polynomial parameter transformations; Polynomial-time; Graph theory
The complexity of the nucleolus in compact games,2014,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921480275&doi=10.1145%2f2692372.2692374&partnerID=40&md5=884cc50e1ef435681618c2052564a0f5,"The nucleolus is a well-known solution concept for coalitional games to fairly distribute the total available worth among the players. The nucleolus is known to be NP-hard to compute over compact coalitional games, that is, over games whose functions specifying the worth associated with each coalition are encoded in terms of polynomially computable functions over combinatorial structures. In particular, hardness results have been exhibited over minimum spanning tree games, threshold games, and flow games. However, due to its intricate definition involving reasoning over exponentially many coalitions, a nontrivial upper bound on its complexity was missing in the literature and looked for. This article faces this question and precisely characterizes the complexity of the nucleolus, by exhibiting an upper bound that holds on any class of compact games, and by showing that this bound is tight even on the (structurally simple) class of graph games. The upper bound is established by proposing a variant of the standard linear-programming based algorithm for nucleolus computation and by studying a framework for reasoning about succinctly specified linear programs, which are contributions of interest in their own. The hardness result is based on an elaborate combinatorial reduction, which is conceptually relevant for it provides a ""measure"" of the computational cost to be paid for guaranteeing voluntary participation to the distribution process. In fact, the pre-nucleolus is known to be efficiently computable over graph games, with this solution concept being defined as the nucleolus but without guaranteeing that each player is granted with it at least the worth she can get alone, that is, without collaborating with the other players. Finally, this article identifies relevant tractable classes of coalitional games, based on the notion of type of a player. Indeed, in most applications where many players are involved, it is often the case that such players do belong in fact to a limited number of classes, which is known in advance and may be exploited for computing the nucleolus in a fast way. © 2014 ACM.",,Hardness; Coalitional game; Combinatorial structures; Computable functions; Computational costs; Distribution process; Minimum spanning trees; Solution concepts; Voluntary participation; Linear programming
Learning hurdles for sleeping experts,2014,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906858570&doi=10.1145%2f2505983&partnerID=40&md5=f40a780ec5428126c1371a862c6c5ea9,"We study the online decision problem in which the set of available actions varies over time, also called the sleeping experts problem.We consider the setting in which the performance comparison is made with respect to the best ordering of actions in hindsight. In this article, both the payoff function and the availability of actions are adversarial. Kleinberg et al. [2010] gave a computationally efficient no-regret algorithm in the setting in which payoffs are stochastic. Kanade et al. [2009] gave an efficient no-regret algorithm in the setting in which action availability is stochastic. However, the question of whether there exists a computationally efficient no-regret algorithm in the adversarial setting was posed as an open problem by Kleinberg et al. [2010]. We show that such an algorithm would imply an algorithm for PAC learning DNF, a long-standing important open problem. We also consider the setting in which the number of available actions is restricted and study its relation to agnostic-learning monotone disjunctions over examples with bounded Hamming weight.© 2014 ACM.",Lower bounds; Online learning; Sleeping experts,Computational efficiency; Sleep research; Stochastic systems; Computationally efficient; Lower bounds; Monotone disjunction; Online decision problem; Online learning; Ordering of actions; Performance comparison; Sleeping experts; Algorithms
Introduction to the special issue on innovations in theoretical computer science 2012 - Part II,2014,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906887056&doi=10.1145%2f2638595&partnerID=40&md5=5e96a263df06f7ec32a5f080e846f83e,[No abstract available],,
(Leveled) fully homomorphic encryption without bootstrapping,2014,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906857634&doi=10.1145%2f2633600&partnerID=40&md5=1e852423e3ee8fc4d443a2b8ea1dce96,"We present a novel approach to fully homomorphic encryption (FHE) that dramatically improves performance and bases security on weaker assumptions. A central conceptual contribution in our work is a new way of constructing leveled, fully homomorphic encryption schemes (capable of evaluating arbitrary polynomial-size circuits of a-priori bounded depth), without Gentry's bootstrapping procedure. Specifically, we offer a choice of FHE schemes based on the learning with error (LWE) or Ring LWE (RLWE) problems that have 2 λ security against known attacks. We construct the following. (1) A leveled FHE scheme that can evaluate depth-L arithmetic circuits (composed of fan-in 2 gates) using O ̃ (λ · L3) per-gate computation, quasilinear in the security parameter. Security is based on RLWE for an approximation factor exponential in L. This construction does not use the bootstrapping procedure. (2) A leveled FHE scheme that can evaluate depth-L arithmetic circuits (composed of fan-in 2 gates) using ̃O (λ2) per-gate computation, which is independent of L. Security is based on RLWE for quasipolynomial factors. This construction uses bootstrapping as an optimization. We obtain similar results for LWE, but with worse performance. All previous (leveled) FHE schemes required a per-gate computation of ̃ Ω (λ3.5), and all of them relied on subexponential hardness assumptions. We introduce a number of further optimizations to our scheme based on the Ring LWE assumption. As an example, for circuits of large width (e.g., where a constant fraction of levels have width Ω (λ)), we can reduce the per-gate computation of the bootstrapped version to O(λ), independent of L, by batching the bootstrapping operation. At the core of our construction is a new approach for managing the noise in latticebased ciphertexts, significantly extending the techniques of Brakerski and Vaikuntanathan [2011b]. © 2014 ACM.",Fully homomorphic encryption; Lattices; Learning with errors,Crystal lattices; Integrating circuits; Logic circuits; Approximation factor; Arithmetic circuit; Fully homomorphic encryption; Fully homomorphic encryption schemes; Learning with Errors; New approaches; Quasi-poly-nomial; Security parameters; Cryptography
"FPT is characterized by useful obstruction sets: Connecting algorithms, kernels, and quasi-orders",2014,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906513775&doi=10.1145%2f2635820&partnerID=40&md5=ef4dc299cbe8845b9537bfdbd37ef7d3,"Many graph problems were first shown to be fixed-parameter tractable using the results of Robertson and Seymour on graph minors. We show that the combination of finite, computable obstruction sets and efficient order tests is not just one way of obtaining strongly uniform FPT algorithms, but that all of FPT may be captured in this way. Our new characterization of FPT has a strong connection to the theory of kernelization, as we prove that problems with polynomial kernels can be characterized by obstruction sets whose elements have polynomial size. Consequently we investigate the interplay between the sizes of problem kernels and the sizes of the elements of such obstruction sets, obtaining several examples of how results in one area yield new insights in the other. We show how exponential-size minor-minimal obstructions for pathwidth k form the crucial ingredient in a novel OR-cross-composition for k-PATHWIDTH, complementing the trivial AND-composition that is known for this problem. In the other direction, we show that OR-cross-compositions into a parameterized problem can be used to rule out the existence of efficiently generated quasi-orders on its instances that characterize the NO-instances by polynomial-size obstructions. © 2014 ACM.",Kernelization; Parameterized complexity; Pathwidth; Well-quasiordering,Algorithms; FPT algorithms; Kernelization; Parameterized complexity; Parameterized problems; Pathwidth; Polynomial kernels; Polynomial size; Well-quasiordering; Polynomials
The complexity of counting homomorphisms to cactus graphs modulo 2,2014,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906489860&doi=10.1145%2f2635825&partnerID=40&md5=2ebbdf4b5817d37d36bee9e278499349,"A homomorphism from a graph G to a graph H is a function from V(G) to V(H) that preserves edges. Many combinatorial structures that arise in mathematics and in computer science can be represented naturally as graph homomorphisms and as weighted sums of graph homomorphisms. In this article, we study the complexity of counting homomorphisms modulo 2. The complexity of modular counting was introduced by Papadimitriou and Zachos and it has been pioneered by Valiant who famously introduced a problem for which counting modulo 7 is easy but counting modulo 2 is intractable. Modular counting provides a rich setting in which to study the structure of homomorphism problems. In this case, the structure of the graph H has a big influence on the complexity of the problem. Thus, our approach is graph-theoretic. We give a complete solution for the class of cactus graphs, which are connected graphs in which every edge belongs to at most one cycle. Cactus graphs arise in many applications such as the modelling of wireless sensor networks and the comparison of genomes. We show that, for some cactus graphs H, counting homomorphisms to H modulo 2 can be done in polynomial time. For every other fixed cactus graph H, the problem is complete in the complexity class ⊕P, which is a wide complexity class to which every problem in the polynomial hierarchy can be reduced (using randomised reductions). Determining which H lead to tractable problems can be done in polynomial time. Our result builds upon the work of Faben and Jerrum, who gave a dichotomy for the case in which H is a tree. © 2014 ACM.",Cactus graph; Counting modulo 2; Graph homomorphism; Parity complexity dichotomy,Computational complexity; Graphic methods; Polynomial approximation; Wireless sensor networks; Cactus graphs; Combinatorial structures; Complete solutions; Complexity dichotomies; Graph homomorphisms; Modular counting; Modulo 2; Polynomial hierarchies; Graph theory
The complexity of the comparator circuit value problem,2014,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906484678&doi=10.1145%2f2635822&partnerID=40&md5=ec7fa67b765328fa68b3fc7dd10c6d2b,"In 1990, Subramanian [1990] defined the complexity class CC as the set of problems log-space reducible to the comparator circuit value problem (CCV). He and Mayr showed that NL ⊆ CC ⊆ P, and proved that in addition to CCV several other problems are complete for CC, including the stable marriage problem, and finding the lexicographically first maximal matching in a bipartite graph. Although the class has not received much attention since then, we are interested in CC because we conjecture that it is incomparable with the parallel class NC which also satisfies NL ⊆ NC ⊆ P, and note that this conjecture implies that none of the CC-complete problems has an efficient polylog time parallel algorithm. We provide evidence for our conjecture by giving oracle settings in which relativized CC and relativized NC are incomparable. We give several alternative definitions of CC, including (among others) the class of problems computed by uniform polynomial-size families of comparator circuits supplied with copies of the input and its negation, the class of problems AC0-reducible to CCV, and the class of problems computed by uniform AC0 circuits with CCV gates. We also give a machine model for CC, which corresponds to its characterization as logspace uniform polynomial-size families of comparator circuits. These various characterizations show that CC is a robust class. Our techniques also show that the corresponding function class FCC is closed under composition. The main technical tool we employ is universal comparator circuits. Other results include a simpler proof of NL ⊆ CC, a more careful analysis showing the lexicographically first maximal matching problem and its variants are CC-complete under AC0 many-one reductions, and an explanation of the relation between the Gale-Shapley algorithm and Subramanian's algorithm for stable marriage. This article continues the previous work of Cook et al. [2011], which focused on Cook-Nguyen style uniform proof complexity, answering several open questions raised in that article. © 2014 ACM.",Comparator circuits; P-completeness,Computation theory; Computational methods; Computer science; Bipartite graphs; Circuit value problem; Complexity class; Gale-shapley algorithms; Maximal matchings; P-completeness; Stable marriage problem; Stable marriages; Comparator circuits
On the one-way function candidate proposed by Goldreich,2014,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906879507&doi=10.1145%2f2633602&partnerID=40&md5=e4e37abf6e6e5c916dbf33f839d251ff,"Goldreich [2000] proposed a candidate one-way function based on a bipartite graph of small right-degree d, where the vertices on the left (resp. right) represent input (resp. output) bits of the function. Each output bit is computed by evaluating a fixed d-ary binary predicate on the input bits adjacent to that output bit. We study this function when the predicate is random or depends linearly on many of its input bits. We assume that the graph is a random balanced bipartite graph with right-degree d. Inverting this function as a one-way function by definition means finding an element in the preimage of output of this function for a random input. We bound the expected size of this preimage. Next, using the preceding bound, we prove that two restricted types of backtracking algorithms called myopic and drunk backtracking algorithms with high probability take exponential time to invert the function, even if we allow the algorithms to use DPLL elimination rules. (For drunk algorithms, a similar result was proved by Itsykson [2010].) We also ran a SAT solver on the satisfiability problem equivalent to the problem of inverting the function, and experimentally observed an exponential increase in running time as a function of the input length. © 2014 ACM.",Average preimage size; DPLL elimination rules; One-way function; Proof complexity; Random bipartite graph; Restricted backtracking algorithm,Constraint theory; Formal logic; Gold; Graph theory; Backtracking algorithm; Bipartite graphs; Elimination rules; One-way functions; Pre images; Proof complexity; Algorithms
Evolvability of real functions,2014,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906847227&doi=10.1145%2f2633598&partnerID=40&md5=b826260b38b273cd67372f722cfdc252,"We formulate a notion of evolvability for functions with domain and range that are real-valued vectors, a compelling way of expressing many natural biological processes. We show that linear and fixed-degree polynomial functions are evolvable in the following dually-robust sense: There is a single evolution algorithm that, for all convex loss functions, converges for all distributions. It is possible that such dually-robust results can be achieved by simpler and more-natural evolution algorithms. Towards this end, we introduce a simple and natural algorithm that we call wide-scale random noise and prove a corresponding result for the L2 metric. We conjecture that the algorithm works for a more general class of metrics. © 2014 ACM.",Evolvability; Optimization,Computation theory; Computational methods; Computer science; Optimization; Biological process; Evolution algorithms; Evolvability; General class; Loss functions; Polynomial functions; Random noise; Real functions; Evolutionary algorithms
Clique cover and graph separation: New incompressibility results,2014,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902511377&doi=10.1145%2f2594439&partnerID=40&md5=598c2bb9b8e291e36f428e07f79248eb,"The field of kernelization studies polynomial-time preprocessing routines for hard problems in the framework of parameterized complexity. In this article, we show that, unless the polynomial hierarchy collapses to its third level, the following parameterized problems do not admit a polynomial-time preprocessing algorithm that reduces the size of an instance to polynomial in the parameter: - Edge Clique Cover, parameterized by the number of cliques, - Directed Edge/Vertex Multiway Cut, parameterized by the size of the cutset, even in the case of two terminals, - Edge/Vertex Multicut, parameterized by the size of the cutset, and - k-Way Cut, parameterized by the size of the cutset. © 2014 ACM.",Cut problems; Edge clique cover; Kernelization; Parameterized complexity,Polynomial approximation; Clique cover; Cut problems; Directed edges; Kernelization; Parameterized complexity; Parameterized problems; Polynomial hierarchies; Polynomial-time preprocessings; Parameterization
Hard instances of algorithms and proof systems,2014,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902517550&doi=10.1145%2f2601336&partnerID=40&md5=36b0a230d6b618865de496715f0013a6,"If the class TAUT of tautologies of propositional logic has no almost optimal algorithm, then every algorithm A deciding TAUT has a hard sequence, that is, a polynomial time computable sequence witnessing that A is not almost optimal. We show that this result extends to every Πt p-complete problem with t ≥ 1; however, assuming the Measure Hypothesis, there is a problem which has no almost optimal algorithm but is decided by an algorithm without hard sequences. For problems Q with an almost optimal algorithm, we analyze whether every algorithm deciding Q, which is not almost optimal, has a hard sequence. © 2014 ACM.",Hard sequences; Measure hypothesis; Optimal algorithms; Optimal proof systems,Formal logic; Polynomial approximation; Complete problems; Computable sequences; Hard sequences; Measure hypothesis; Optimal algorithm; Polynomial-time; Proof system; Propositional logic; Algorithms
The complexity of approximately counting tree homomorphisms,2014,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902483339&doi=10.1145%2f2600917&partnerID=40&md5=daee610702338baaa3ddeb1b883f749d,"We study two computational problems, parameterised by a fixed tree H. #HOMSTO(H) is the problem of counting homomorphisms from an input graph G to H. #WHOMSTO(H) is the problem of counting weighted homomorphisms to H, given an input graph G and a weight function for each vertex v of G. Even though H is a tree, these problems turn out to be sufficiently rich to capture all of the known approximation behaviour in #P. We give a complete trichotomy for #WHOMSTO(H). If H is a star, then #WHOMSTO(H) is in FP. If H is not a star but it does not contain a certain induced subgraph J3, then #WHOMSTO(H) is equivalent under approximation-preserving (AP) reductions to #BIS, the problem of counting independent sets in a bipartite graph. This problem is complete for the class #RHΠ1 under AP-reductions. Finally, if H contains an induced J3, then #WHOMSTO(H) is equivalent under AP-reductions to #SAT, the problem of counting satisfying assignments to a CNF Boolean formula. Thus, #WHOMSTO(H) is complete for #P under AP-reductions. The results are similar for #HOMSTO(H) except that a rich structure emerges if H contains an induced J3. We show that there are trees H for which #HOMSTO(H) is #SAT-equivalent (disproving a plausible conjecture of Kelk). However, it is still not known whether #HOMSTO(H) is #SAT-hard for every tree H which contains an induced J3. It turns out that there is an interesting connection between these homomorphism-counting problems and the problem of approximating the partition function of the ferromagnetic Potts model. In particular, we show that for a family of graphs Jq, parameterised by a positive integer q, the problem #HOMSTO(Jq) is AP-interreducible with the problem of approximating the partition function of the q-state Potts model. It was not previously known that the Potts model had a homomorphism-counting interpretation. We use this connection to obtain some additional upper bounds for the approximation complexity of #HOMSTO(J q). © 2014 ACM.",Complexity classes; Counting problems; Graph homomorphisms; Reducibility,Forestry; Mathematics; Problem Solving; Trees; Boolean algebra; Forestry; Potts model; Stars; Approximation complexity; Complexity class; Computational problem; Counting independent sets; Counting problems; Graph homomorphisms; Reducibility; Satisfying assignments; Trees (mathematics)
"A note on the complexity of comparing succinctly represented integers, with an application to maximum probability parsing",2014,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902507297&doi=10.1145%2f2601327&partnerID=40&md5=0bed2153b9f63d92d377a8b1b2e58223,"The following two decision problems capture the complexity of comparing integers or rationals that are succinctly represented in product-of-exponentials notation, or equivalently, via arithmetic circuits using only multiplication and division gates, and integer inputs. Input instance: Four lists of positive integers: a1, ... , an ∈ ℕ+ n; b1, ... , bn ∈ ℕ+ n; c1, ... , cm ∈ ℕ+ m; d1, ... , dm ∈ ℕ+ m; where each of the integers is represented in binary. Problem 1 (equality testing): Decide whether a1b1a2 b2 ... anbn = c1d1c 2d2 ... cmdm. Problem 2 (inequality testing): Decide whether a1b1a2b2 ... anbn ≥ c1d1c2 d2 ... cmdm. Problem 1 is easily decidable in polynomial time using a simple iterative algorithm. Problem 2 is much harder. We observe that the complexity of Problem 2 is intimately connected to deep conjectures and results in number theory. In particular, if a refined form of the ABC conjecture formulated by Baker in 1998 holds, or if the older Lang-Waldschmidt conjecture (formulated in 1978) on linear forms in logarithms holds, then Problem 2 is decidable in P-time (in the standard Turing model of computation). Moreover, it follows from the best available quantitative bounds on linear forms in logarithms, namely, by Baker and Wüstholz [1993] or Matveev [2000], that if m and n are fixed universal constants then Problem 2 is decidable in P-time (without relying on any conjectures). This latter fact was observed earlier by Shub [1993]. We describe one application: P-time maximum probability parsing for arbitrary stochastic context-free grammars (where ε-rules are allowed). © 2014 ACM.",ABC conjecture; Arithmetic circuits; Lang-Waldschmidt conjecture; Logarithmic forms; Probabilistic parsing; Stochastic context-free grammars; Succinct representation of numbers,Algorithms; Context free grammars; Logic circuits; Number theory; Polynomial approximation; ABC conjecture; Arithmetic circuit; Lang-Waldschmidt conjecture; Probabilistic parsing; Stochastic context free grammar; Succinct representation; Integrating circuits
New NP-hardness results for 3-coloring and 2-to-1 Label Cover,2014,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897521983&doi=10.1145%2f2537800&partnerID=40&md5=1abb11169d547382f89b5cd10996fc71,"We show that given a 3-colorable graph, it is NP-hard to find a 3-coloring with (16/17 + ∈) of the edges bichromatic. In a related result, we show that given a satisfiable instance of the 2-to-1 Label Cover problem, it is NP-hard to find a (23/24 + ∈)-satisfying assignment. © 2014 ACM.",Constraint satisfaction problems; Graph coloring; Label cover; NP-hardness,Computation theory; Computational methods; Computer science; Constraint satisfaction problems; 3-coloring; Cover problem; Graph colorings; NP-hard; NP-hardness; Np-hardness results; Satisfying assignments; Hardness
Optimal lower bounds for locality-sensitive hashing (except when q is tiny),2014,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897475487&doi=10.1145%2f2578221&partnerID=40&md5=0295545572454e07aede6640baee01b5,"We study lower bounds for Locality-Sensitive Hashing (LSH) in the strongest setting: point sets in {0,1}d under the Hamming distance. Recall that H is said to be an (r,cr,p, q)-sensitive hash family if all pairs x,y ∈ {0,1}d with dist(x,y) ≤ r have probability at least p of collision under a randomly chosen h ∈ H, whereas all pairs x,y ∈ {0,1}d with dist(x,y) ≥ cr have probability at most q of collision. Typically, one considers d → ∞, with c > 1 fixed and q bounded away from 0. For its applications to approximate nearest-neighbor search in high dimensions, the quality of an LSH family H is governed by how small its ρ parameter ρ = ln(1/p)/ln(1/q) is as a function of the parameter c. The seminal paper of Indyk and Motwani [1998] showed that for each c ≥ 1, the extremely simple family H = {x → xi: i ∈ [d]} achieves ρ ≤ 1/c. The only known lower bound, due to Motwani et al. [2007], is that ρ must be at least (e1/c - 1)/(e1/c + 1) ≥.46/c (minus od(1)). The contribution of this article is twofold. (1) We show the ""optimal"" lower bound for ρ: it must be at least 1/c (minus od(1)). Our proof is very simple, following almost immediately from the observation that the noise stability of a boolean function at time t is a log-convex function of t. (2) We raise and discuss the following issue: neither the application of LSH to nearest-neighbor search nor the known LSH lower bounds hold as stated if the q parameter is tiny. Here, ""tiny"" means q = 2-Θ(d), a parameter range we believe is natural. © 2014 ACM.",Fourier analysis of boolean functions; Locality-Sensitive Hashing; LSH; Noise sensitivity; Noise stability,Fourier analysis; Hamming distance; Optimization; Approximate nearest-neighbor searches; Locality sensitive hashing; Log-convex functions; LSH; Nearest Neighbor search; Noise sensitivity; Noise stability; Optimal lower bound; Boolean functions
The hardness of being private,2014,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897525414&doi=10.1145%2f2567671&partnerID=40&md5=685ed16cd521f1c2b8b6e4077dbb6d0a,"Kushilevitz [1989] initiated the study of information-theoretic privacy within the context of communication complexity. Unfortunately, it has been shown that most interesting functions are not privately computable [Kushilevitz 1989, Brandt and Sandholm 2008]. The unattainability of perfect privacy for many functions motivated the study of approximate privacy. Feigenbaum et al. [2010a, 2010b] define notions of worst-case as well as average-case approximate privacy and present several interesting upper bounds as well as some open problems for further study. In this article, we obtain asymptotically tight bounds on the trade-offs between both the worst-case and average-case approximate privacy of protocols and their communication cost for Vickrey auctions. Further, we relate the notion of average-case approximate privacy to other measures based on information cost of protocols. This enables us to prove exponential lower bounds on the subjective approximate privacy of protocols for computing the Intersection function, independent of its communication cost. This proves a conjecture of Feigenbaum et al. [2010a]. © 2014 ACM.",Approximate privacy; Communication complexity; Information theory; Privacy; Privacy trade-off; Vickrey auction,Costs; Data privacy; Economic and social effects; Information theory; Average-case; Communication complexity; Communication cost; Information costs; Lower bounds; Tight bound; Upper Bound; Vickrey auction; Communication
On effective convergence of numerical solutions for differential equations,2014,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897543831&doi=10.1145%2f2578219&partnerID=40&md5=034ae7371e4e6ed07e6cc254d1c67d30,"This article studies the effective convergence of numerical solutions of initial value problems (IVPs) for ordinary differential equations (ODEs). A convergent sequence {Ym} of numerical solutions is said to be effectively convergent to the exact solution if there is an algorithm that computes an N ∈ ℕ, given an arbitrary n ∈ ℕ as input, such that the error between Ym and the exact solution is less than 2 -n for all m ≥ N. It is proved that there are convergent numerical solutions generated from Euler's method which are not effectively convergent. It is also shown that a theoretically-proved-computable solution using Picard's iteration method might not be computable by classical numerical methods, which suggests that sometimes there is a gap between theoretical computability and practical numerical computations concerning solutions of ODEs. Moreover, it is noted that the main theorem (Theorem 4.1) provides an example of an IVP with a nonuniform Lipschitz function for which the numerical solutions generated by Euler's method are still convergent. © 2014 ACM.",Effective convergence; Numerical solutions of ODEs,Initial value problems; Numerical methods; Ordinary differential equations; Convergent sequences; Effective convergence; Euler's method; Exact solution; Iteration method; Lipschitz functions; Numerical computations; Numerical solution; Iterative methods
Unions of disjoint NP-complete sets,2014,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897497573&doi=10.1145%2f2591508&partnerID=40&md5=b9a50848401ba9e3eb7874b8a03f82fc,"We study the following question: if A and B are disjoint NP-complete sets, then is A ∪ B NP-complete? We provide necessary and sufficient conditions under which the union of disjoint NP-complete sets remain complete. © 2014 ACM.",,Computation theory; Computational methods; Computer science; NP Complete; Polynomials
"Exact learning algorithms, betting games, and circuit lower bounds",2013,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890603314&doi=10.1145%2f2539126.2539130&partnerID=40&md5=f1412fe76e2040da5702c1ce20a43615,"This article extends and improves the work of Fortnow and Klivans [2009], who showed that if a circuit class C has an efficient learning algorithm in Angluin's model of exact learning via equivalence and membership queries [Angluin 1988], then we have the lower bound EXPNP not C. We use entirely different techniques involving betting games [Buhrman et al. 2001] to remove the NP oracle and improve the lower bound to EXP not C. This shows that it is even more difficult to design a learning algorithm for C than the results of Fortnow and Klivans [2009] indicated. We also investigate the connection between betting games and natural proofs, and as a corollary the existence of strong pseudorandom generators. Our results also yield further evidence that the class of Boolean circuits has no efficient exact learning algorithm. This is because our separation is strong in that it yields a natural proof [Razborov and Rudich 1997] against the class. From this we conclude that an exact learning algorithm for Boolean circuits would imply that strong pseudorandom generators do not exist, which contradicts widely believed conjectures from cryptography. As a corollary we obtain that if strong pseudorandom generators exist, then there is no exact learning algorithm for Boolean circuits. © 2013.",,Computation theory; Computational methods; Computer science; Boolean circuit; Circuit lower bounds; Efficient learning; Exact learning; Lower bounds; Membership query; Natural proofs; Pseudorandom generators; Learning algorithms
Graph isomorphism is not ac°-reducible to group isomorphism,2013,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890424759&doi=10.1145%2f2540088&partnerID=40&md5=e2cad91a2b0ba9902ea1b0a010cb7c71,"We give a new upper bound for the Group and Quasigroup Isomorphism problems when the input structures are given explicitly by multiplication tables. We show that these problems can be computed by polynomial size nondeterministic circuits of unbounded fan-in with O(log log n) depth and O(log2 n) nondeterministic bits, where n is the number of group elements. This improves the existing upper bound for the problems. In the previous bound the circuits have bounded fan-in but depth O(log2 n) and also O(log2 n) nondeterministic bits.We then prove that the kind of circuits from our upper bound cannot compute the Parity function. Since Parity is AC0- reducible to Graph Isomorphism, this implies that Graph Isomorphism is strictly harder than Group or Quasigroup Isomorphism under the ordering defined by AC0 reductions. We extend this result to the stronger ACC 0[ p] reduction and its randomized version. © 2013.",Algorithms; Complexity; Isomorphism problems; Limited nondeterminism,Algorithms; Computation theory; Computational methods; Computer science; Complexity; Graph isomorphism; Isomorphism problems; Limited nondeterminism; Parity functions; Polynomial size; Quasigroup; Upper Bound; Set theory
Explicit optimal hardness via gaussian stability results,2013,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890350920&doi=10.1145%2f2505766&partnerID=40&md5=dc9bc8afa1829f3e9e110ee617786e46,"The results of Raghavendra [2008] show that assuming Khot's Unique Games Conjecture [2002], for every constraint satisfaction problem there exists a generic semidefinite program that achieves the optimal approximation factor. This result is existential as it does not provide an explicit optimal rounding procedure nor does it allow to calculate exactly the Unique Games hardness of the problem. Obtaining an explicit optimal approximation scheme and the corresponding approximation factor is a difficult challenge for each specific approximation problem. Khot et al. [2004] established a general approach for determining the exact approximation factor and the corresponding optimal rounding algorithm for any given constraint satisfaction problem. However, this approach crucially relies on results explicitly proving optimal partitions in the Gaussian space. Until recently, Borell's result [1985] was the only nontrivial Gaussian partition result known. In this article we derive the first explicit optimal approximation algorithm and the corresponding approximation factor using a new result on Gaussian partitions due to Isaksson and Mossel [2012]. This Gaussian result allows us to determine the exact Unique Games Hardness of MAX-3-EQUAL. In particular, our results show that Zwick's algorithm for this problem achieves the optimal approximation factor and prove that the approximation achieved by the algorithm is ≈ 0.796 as conjectured by Zwick [1998]. We further use the previously known optimal Gaussian partitions results to obtain a new Unique Games Hardness factor for MAX-k-CSP: Using the well-known fact that jointly normal pairwise independent random variables are fully independent, we show that the UGC hardness of Max-k-CSP is [equation presented] , improving on results of Austrin and Mossel [2009]. Categories and Subject Descriptors: F.2.2 [Analysis of Algorithms and Problem Complexity]: Nonnumerical Algorithms and Problems General Terms: Theory, Algorithms. © 2013.",Dictatorship test; Gaussian stability; Unique games conjecture,Constraint satisfaction problems; Gaussian distribution; Hardness; Optimization; Analysis of algorithms; Approximation problems; Dictatorship tests; Gaussians; Independent random variables; Nonnumerical algorithms and problems; Semidefinite programs; Unique games conjecture; Approximation algorithms
Real advantage,2013,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890737933&doi=10.1145%2f2540089&partnerID=40&md5=327e73fec8b67c9f4b4ac30b3dcd3c93,"We highlight the challenge of proving correlation bounds between boolean functions and real-valued polynomials, where any non-boolean output counts against correlation. We prove that real-valued polynomials of degree 1/2 lg2 lg2 n have correlation with parity at most zero. Such a result is false for modular and threshold polynomials. Its proof is based on a variant of an anticoncentration result by Costello et al. [2006]. © 2013.",Correlation bounds; Lower bounds; Parity; Polynomials,Boolean functions; Correlation bounds; Lower bounds; Parity; Polynomials
Distortion is fixed parameter tractable,2013,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890349923&doi=10.1145%2f2489789&partnerID=40&md5=d75ab58d4e4882b158887ec85716b5df,"We study low-distortion embedding of metric spaces into the line, and more generally, into the shortest path metric of trees, from the parameterized complexity perspective. Let M = M(G) be the shortest path metric of an edge-weighted graph G, with the vertex set V(G) and the edge set E(G), on n vertices. We give the first fixed parameter tractable algorithm that for an unweighted graph metric M and integer d either constructs an embedding of M into the line with distortion at most d, or concludes that no such embedding exists. Our algorithm requires O(nd4(2d + 1)2d) time which is a significant improvement over the best previous algorithm that runs in time O(n4d+2dO(1)). Because of its apparent similarity to the notoriously hard BANDWIDTH MINIMIZATION problem, we find it surprising that this problem turns out to be fixed parameter tractable. We extend our results on embedding unweighted graph metric into the line in two ways. First, we give an algorithm to construct small-distortion embeddings of weighted graph metrics. The running time of our algorithm is O(n(dW)4(2d + 1)2dW), where W is the largest edge weight of the input graph. To complement this result, we show that the exponential dependence on the maximum edge weight is unavoidable. In particular, we show that deciding whether a weighted graph metric M(G) with maximum weight W < |V(G)| can be embedded into the line with distortion at most d is NP-complete for every fixed rational d ≥ 2. This rules out any possibility of an algorithm with running time O((nW) h(d)) where h is a function of d alone. Second, we consider more general host metrics for which analogous results hold. In particular, we prove that for any tree T with maximum degree δ, embedding M into a shortest path metric of T is fixed parameter tractable, parameterized by (δ, d). © 2013.",Distortion; Embedding; Parameterized algorithms; Tree,Algorithms; Distortion (waves); Graphic methods; Bandwidth minimization problem; Edge-weighted graph; Embedding; Exponential dependence; Fixed-parameter tractable algorithms; Parameterized algorithm; Parameterized complexity; Tree; Graph theory
Robust satisfiability for csps: Hardness and algorithmic results,2013,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890398572&doi=10.1145%2f2540090&partnerID=40&md5=bf44f54833ecf581c18834aee9393fc0,"An algorithm for a constraint satisfaction problem is called robust if it outputs an assignment satisfying at least a (1-f (ε))-fraction of constraints for each (1-ε)-satisfiable instance (i.e., such that at most a ε-fraction of constraints needs to be removed to make the instance satisfiable), where f (ε) ? 0 as ε ? 0. We establish an algebraic framework for analyzing constraint satisfaction problems admitting an efficient robust algorithm with functions f of a given growth rate. We use this framework to derive hardness results. We also describe three classes of problems admitting an efficient robust algorithm such that f is O(1/ log (1/ε)), O(ε1/k) for some k > 1, and O(ε), respectively. Finally, we give a complete classification of robust satisfiability with a given f for the Boolean case. © 2013.",Approximation algorithms; Constraint satisfaction problem; Homomorphism duality; Universal algebra,Approximation algorithms; Constraint theory; Formal logic; Hardness; Algebraic framework; Complete classification; Hardness result; Homomorphism duality; Robust algorithm; Robust satisfiability; Universal algebra; Constraint satisfaction problems
On the usefulness of predicates,2013,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878567883&doi=10.1145%2f2462896.2462897&partnerID=40&md5=c2789de5b9d7b8ba0309a3968f2008a1,"Motivated by the pervasiveness of strong inapproximability results for Max-CSPs, we introduce a relaxed notion of an approximate solution of a Max-CSP. In this relaxed version, loosely speaking, the algorithm is allowed to replace the constraints of an instance by some other (possibly real-valued) constraints, and then only needs to satisfy as many of the new constraints as possible. To be more precise, we introduce the following notion of a predicate P being useful for a (real-valued) objective Q: given an almost satisfiable Max-P instance, there is an algorithm that beats a random assignment on the corresponding Max-Q instance applied to the same sets of literals. The standard notion of a nontrivial approximation algorithm for a Max-CSP with predicate P is exactly the same as saying that P is useful for P itself. We say that P is useless if it is not useful for any Q. This turns out to be equivalent to the following pseudo-randomness property: given an almost satisfiable instance of Max-P, it is hard to find an assignment such that the induced distribution on k-bit strings defined by the instance is not essentially uniform. Under the unique games conjecture, we give a complete and simple characterization of useful Max-CSPs defined by a predicate: such a Max-CSP is useless if and only if there is a pairwise independent distribution supported on the satisfying assignments of the predicate. It is natural to also consider the case when no negations are allowed in the CSP instance, and we derive a similar complete characterization (under the UGC) there as well. Finally, we also include some results and examples shedding additional light on the approximability of certain Max-CSPs. © 2013 ACM.",Approximation resistance; Constraint satisfaction problem; Unique games conjecture; Usefulness,Approximation algorithms; Approximability; Approximate solution; Inapproximability; Pseudorandomness; Random assignment; Satisfying assignments; Unique games conjecture; Usefulness; Constraint satisfaction problems
Verifying proofs in constant depth,2013,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878619822&doi=10.1145%2f2462896.2462898&partnerID=40&md5=9892e03d83f4b5a9422c33e0bccfd80e,"In this paper we initiate the study of proof systems where verification of proofs proceeds by NC0 circuits. We investigate the question which languages admit proof systems in this very restricted model. Formulated alternatively, we ask which languages can be enumerated by NC0 functions. Our results show that the answer to this problem is not determined by the complexity of the language. On the one hand, we construct NC0 proof systems for a variety of languages ranging from regular to NP complete. On the other hand, we show by combinatorial methods that even easy regular languages such as Exact-OR do not admit NC0 proof systems. We also show that Majority does not admit NC0 proof systems. Finally, we present a general construction of NC0 proof systems for regular languages with strongly connected NFA's. © 2013 ACM.",Circuit complexity; Proof circuits; Proof complexity; Small depth proofs,Computation theory; Computer science; Circuit complexity; Combinatorial method; NP Complete; Proof complexity; Proof system; Restricted models; Small depth proofs; Strongly connected; Computational methods
On multiway cut parameterized above lower bounds,2013,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878552515&doi=10.1145%2f2462896.2462899&partnerID=40&md5=b37e454b35c272b93909c37057255459,"We introduce a concept of parameterizing a problem above the optimum solution of its natural linear programming relaxation and prove that the node multiway cut problem is fixed-parameter tractable (FPT) in this setting. As a consequence we prove that node multiway cut is FPT, when parameterized above the maximum separating cut, resolving an open problem of Razgon. Our results imply O*(4k) algorithms for vertex cover above maximum matching and almost 2-SAT as well as an O*(2k) algorithm for node multiway cut with a standard parameterization by the solution size, improving previous bounds for these problems. © 2013 ACM.",Fixed parameter tractability; Linear programming; Multiway cut; Parameterized algorithms,Algorithms; Linear programming; Fixed-parameter tractability; Linear programming relaxation; Maximum matchings; Multiway cut; MULTIWAY CUT problems; Optimum solution; Parameterized algorithm; Parameterizing; Parameterization
Quantum rejection sampling,2013,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897013895&doi=10.1145%2f2493252.2493256&partnerID=40&md5=beef1bdee33c8cb3dd343abe0c8a1b53,"Rejection sampling is a well-known method to sample from a target distribution, given the ability to sample from a given distribution. The method has been first formalized by von Neumann [1951] and has many applications in classical computing. We define a quantum analogue of rejection sampling: given a black box producing a coherent superposition of (possibly unknown) quantum states with some amplitudes, the problem is to prepare a coherent superposition of the same states, albeit with different target amplitudes. The main result of this article is a tight characterization of the query complexity of this quantum state generation problem. We exhibit an algorithm, which we call quantum rejection sampling, and analyze its cost using semidefinite programming. Our proof of a matching lower bound is based on the automorphism principle that allows to symmetrize any algorithm over the automorphism group of the problem. Our main technical innovation is an extension of the automorphism principle to continuous groups that arise for quantum state generation problems where the oracle encodes unknown quantum states, instead of just classical data. Furthermore, we illustrate how quantum rejection sampling may be used as a primitive in designing quantum algorithms, by providing three different applications.We first show that it was implicitly used in the quantum algorithm for linear systems of equations by Harrow et al. [2009]. Second we show that it can be used to speed up the main step in the quantum Metropolis sampling algorithm by Temme et al. [2011]. Finally, we derive a new quantum algorithm for the hidden shift problem of an arbitrary Boolean function and relate its query complexity to ""water-filling"" of the Fourier spectrum.©2013 ACM 1942-3454/2013/08-ART12 15.00.",Boolean hidden shift problem; Quantum algorithms; Quantum Metropolis sampling; Query complexity; Rejection sampling,Algorithms; Boolean functions; Linear systems; Mathematical programming; Quantum optics; Boolean hidden shift problem; Metropolis sampling; Quantum algorithms; Query complexity; Rejection samplings; Quantum theory
Compressed matrix multiplication,2013,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897008774&doi=10.1145%2f2493252.2493254&partnerID=40&md5=41968e11caeadfd17a8e426569bc174f,"We present a simple algorithm that approximates the product of n-by-n real matrices A and B. Let ||AB||F denote the Frobenius norm of AB, and b be a parameter determining the time/accuracy trade-off. Given 2-wise independent hash functions h1, h2 : [n] ? [b], and s1, s2 : [n] ? {-1,+1} the algorithm works by first ""compressing"" the matrix product into the polynomial p(x) = n k=1 n i=1 Aiks1(i) xh1(i) ? ? n j=1 Bkjs2( j) xh2( j) ? ?. Using the fast Fourier transform to compute polynomial multiplication, we can compute c0 cb-1 such that i cixi = (p(x) mod xb)+(p(x) div xb) in time O(n2 +nb). An unbiased estimator of (AB)ij with variance at most ||AB||2 F/b can then be computed as: Cij = s1(i) s2(j) c(h1(i)+h2(j)) mod b. Our approach also leads to an algorithm for computing AB exactly, with high probability, in time O (N + nb) in the case where A and B have at most N nonzero entries, and AB has at most b nonzero entries.©2013 ACM 1942-3454/2013/08-ART12 15.00.",Approximation; Matrix multiplication; Sparse recovery,Fast Fourier transforms; Hash functions; Approximation; High probability; MAtrix multiplication; Matrix products; Polynomial multiplication; SIMPLE algorithm; Sparse recovery; Unbiased estimator; Matrix algebra
Economical caching,2013,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896936386&doi=10.1145%2f2493246.2493247&partnerID=40&md5=2c22c0bd9478a36909cf18f90016f585,"We study the management of buffers and storages in environments with unpredictably varying prices in a competitive analysis. In the economical caching problem, there is a storage with a certain capacity. For each time step, an online algorithm is given a price from the interval [1, a], a consumption, and possibly a buying limit. The online algorithm has to decide the amount to purchase from some commodity, knowing the parameter a but without knowing how the price evolves in the future. The algorithm can purchase at most the buying limit. If it purchases more than the current consumption, then the excess is stored in the storage; otherwise, the gap between consumption and purchase must be taken from the storage. The goal is to minimize the total cost. Interesting motivating applications are, for example, stream caching on mobile devices with different classes of service, battery management in micro hybrid cars, and the efficient purchase of resources. © 2013 ACM.",Competitive analysis; Online algorithms; Storage management,Algorithms; Costs; Mobile devices; Purchasing; Sales; Battery Management; Competitive analysis; Current consumption; Different class; Hybrid cars; On-line algorithms; Time step; Storage management
Hard functions for low-degree polynomials over prime fields,2013,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896989622&doi=10.1145%2f2493246.2493248&partnerID=40&md5=7cabaf6accafcefa555c784783660925,"In this article, we present a new hardness amplification for low-degree polynomials over prime fields, namely, we prove that if some function is mildly hard to approximate by any low-degree polynomials then the sum of independent copies of the function is very hard to approximate by them. This result generalizes the XOR lemma for low-degree polynomials over the binary field given by Viola and Wigderson [2008]. The main technical contribution is the analysis of the Gowers norm over prime fields. For the analysis, we discuss a generalized low-degree test, which we call the Gowers test, for polynomials over prime fields, which is a natural generalization of that over the binary field given by Alon et al. [2003]. This Gowers test provides a new technique to analyze the Gowers norm over prime fields. Actually, the rejection probability of the Gowers test can be analyzed in the framework of Kaufman and Sudan [2008]. However, our analysis is selfcontained and quantitatively better. By using our argument, we also prove the hardness of modulo functions for low-degree polynomials over prime fields. © 2013 ACM.",Hardness amplification; Low-degree polynomials; Property testing,Amplification; Hardness; Binary fields; Gowers norm; Hard function; Hardness amplification; Natural generalization; Property-testing; Rejection probability; Technical contribution; Polynomials
On approximating the number of relevant variables in a function,2013,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896983526&doi=10.1145%2f2493246.2493250&partnerID=40&md5=6debbad116394d055ea4b160a0efa536,"In this work we consider the problem of approximating the number of relevant variables in a function given query access to the function. Since obtaining a multiplicative factor approximation is hard in general, we consider several relaxations of the problem. In particular, we consider a relaxation of the property testing variant of the problem and we consider relaxations in which we have a promise that the function belongs to a certain family of functions (e.g., linear functions). In the former relaxation the task is to distinguish between the case that the number of relevant variables is at most k, and the case in which it is far from any function in which the number of relevant variables is more than (1 + γ )k for a parameter ?. We give both upper bounds and almost matching lower bounds for the relaxations we study. © 2013 ACM.",Property testing,Computation theory; Computer science; Linear functions; Lower bounds; Multiplicative factors; Property-testing; Upper Bound; Computational methods
High-confidence predictions under adversarial uncertainty,2013,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896995163&doi=10.1145%2f2493252.2493257&partnerID=40&md5=1ba660780fd7767150a68fa845d534e3,"We study the setting in which the bits of an unknown infinite binary sequence x are revealed sequentially to an observer. We show that very limited assumptions about x allow one to make successful predictions about unseen bits of x. First, we study the problem of successfully predicting a single 0 from among the bits of x. In our model, we have only one chance to make a prediction, but may do so at a time of our choosing. This model is applicable to a variety of situations in which we want to perform an action of fixed duration, and need to predict a ""safe"" time-interval to perform it. Letting Nt denote the number of 1s among the first t bits of x, we say that x is ""e-weakly sparse"" if lim inf(Nt/t) = e. Our main result is a randomized algorithm that, given any e-weakly sparse sequence x, predicts a 0 of x with success probability as close as desired to 1 - e. Thus, we can perform this task with essentially the same success probability as under the much stronger assumption that each bit of x takes the value 1 independently with probability e. We apply this result to show how to successfully predict a bit (0 or 1) under a broad class of possible assumptions on the sequence x. The assumptions are stated in terms of the behavior of a finite automaton M reading the bits of x. We also propose and solve a variant of the well-studied ""ignorant forecasting"" problem. For every e < 0, we give a randomized forecasting algorithm Se that, given sequential access to a binary sequence x, makes a prediction of the form: ""A p fraction of the next N bits will be 1s."" (The algorithm gets to choose p, N, and the time of the prediction.) For any fixed sequence x, the forecast fraction p is accurate to within e with probability 1 - e.©2013 ACM 1942-3454/2013/08-ART12 15.00.",Binary sequences; Prediction; Worst-case prediction,Algorithms; Binary sequences; Fixed sequence; Forecasting algorithm; P fractions; Randomized Algorithms; Sequential access; Sparse sequences; Time interval; Forecasting
"Alternation-trading proofs, linear programming, and lower bounds",2013,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896960563&doi=10.1145%2f2493246.2493249&partnerID=40&md5=285e21efc6ac3f2a4c0d51f6b9e719f7,"A fertile area of recent research has demonstrated concrete polynomial-time lower bounds for natural hard problems on restricted computational models. Among these problems are Satisfiability, Vertex Cover, Hamilton Path, MOD6-SAT, Majority-of-Majority-SAT, and Tautologies, to name a few. The proofs of these lower bounds follow a proof-by-contradiction strategy that we call resource trading or alternation trading. An important open problem is to determine how powerful such proofs can possibly be. We propose a methodology for studying these proofs that makes them amenable to both formal analysis and automated theorem proving. We prove that the search for better lower bounds can often be turned into a problem of solving a large series of linear programming instances. Implementing a small-scale theorem prover based on these results, we extract new human-readable time lower bounds for several problems and identify patterns that allow for further generalization. The framework can also be used to prove concrete limitations on the current techniques. © 2013 ACM.",Alternation; Linear programming; Lower bounds; Time-space trade-offs,Commerce; Concretes; Linear programming; Theorem proving; Alternation; Alternation-trading proofs; Automated theorem proving; Computational model; Lower bounds; Recent researches; Theorem provers; Time-space; Problem solving
Linear-Time decoding of regular expander codes,2013,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896960842&doi=10.1145%2f2493252.2493255&partnerID=40&md5=b40a08fddef646307e33cddf80aa94f1,"Sipser and Spielman (IEEE IT, [1996]) showed that any (c, d)-regular expander code with expansion parameter <34 is decodable in linear time from a constant fraction of errors. Feldman et al. (IEEE IT, [2007]) proved that expansion parameter <23 + 1 3c is sufficient to correct a constant fraction of errors in polynomial time using LP decoding. In this work, we give a simple combinatorial algorithm that achieves even better parameters. In particular, our algorithm runs in linear time and works for any expansion parameter <23 - 1 6c. We also prove that our decoding algorithm can be executed in logarithmic time on a linear number of parallel processors.©2013 ACM 1942-3454/2013/08-ART12 15.00©2013 ACM 1942-3454/2013/08-ART12 15.00.",Decoding; Error-correcting codes; Expander codes; LDPC codes,Algorithms; Decoding; Errors; Expansion; Polynomial approximation; Combinatorial algorithm; Decoding algorithm; Error correcting code; Expander codes; LDPC codes; Logarithmic time; Parallel processor; Polynomial-time; Parameter estimation
Introduction to the special issue on innovations in theoretical computer science 2012,2013,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896988945&doi=10.1145%2f2493252.2493253&partnerID=40&md5=c7ff2769ee600dc41fc5b5c433b3bdfd,[No abstract available],,
Approximating the influence of monotone Boolean functions in O(√n) query complexity,2012,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870691494&doi=10.1145%2f2382559.2382562&partnerID=40&md5=9dde554e4ae7c72e7613ad2a804b6df3,"The Total Influence (Average Sensitivity) of a discrete function is one of its fundamental measures. We study the problem of approximating the total influence of a monotone Boolean function, which we denote by I[f]. We present a randomized algorithm that approximates the influence of such functions to within a multiplicative factor of (1 ± ∈) by performing O (equation) queries. We also prove a lower bound of Ω (equation) on the query complexity of any constant factor approximation algorithm for this problem (which holds for I[f] = Ω(1)), hence showing that our algorithm is almost optimal in terms of its dependence on n. For general functions, we give a lower bound of Ω ([n/I[f]]), which matches the complexity of a simple sampling algorithm. © 2012 ACM.",Influence of a Boolean function; Sublinear query approximation algorithms; Symmetric chains,Approximation algorithms; A-monotone; Average sensitivities; Constant-factor approximation algorithms; Discrete functions; General functions; Lower bounds; Monotone Boolean functions; Multiplicative factors; Query complexity; Randomized Algorithms; Sampling algorithm; Sub-linear queries; Boolean functions
On the computational complexity of stochastic controller optimization in POMDPs,2012,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870666193&doi=10.1145%2f2382559.2382563&partnerID=40&md5=bd6d93ba664f1e4998105de5a2e88c38,"We show that the problem of finding an optimal stochastic blind controller in a Markov decision process is an NP-hard problem. The corresponding decision problem is NP-hard in PSPACE and SQRT-SUM-hard, hence placing it in NP would imply breakthroughs in long-standing open problems in computer science. Our result establishes that the more general problem of stochastic controller optimization in POMDPs is also NP-hard. Nonetheless, we outline a special case that is convex and admits efficient global solutions. © 2012 ACM.",Bilinear program; Computational complexity; Computations on polynomials; Matrix fractional program; Motzkin-Straus theorem; Nonlinear optimization; Partially observable Markov decision process; Stochastic controller; Sum-of-square-roots problem,Markov processes; Optimization; Stochastic control systems; Bilinear program; Motzkin-Straus theorem; Non-linear optimization; Partially observable Markov decision process; Sum-of-square-roots problem; Computational complexity
On the sum of square roots of polynomials and related problems,2012,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870654048&doi=10.1145%2f2382559.2382560&partnerID=40&md5=5b0542f13e0199c3877c4e81ce4cf68d,"The sum of square roots over integers problem is the task of deciding the sign of a nonzero sum, S = Sigma;ni-=1 δi · √ai, where δi ∈ {+1,-1} and ai's are positive integers that are upper bounded by N (say). A fundamental open question in numerical analysis and computational geometry is whether |S| ≥ 1/2(nlogN)o(1) when S ≠ 0. We study a formulation of this problem over polynomials. Given an expression S = Sigma; ni=1 ci · √ fi(x), where ci's belong to a field of characteristic 0 and fi's are univariate polynomials with degree bounded by d and fi(0) ≠ 0 for all i, is it true that the minimum exponent of x that has a nonzero coefficient in the power series S is upper bounded by (n · d)o(1), unless S = 0? We answer this question affirmatively Further, we show that this result over polynomials can be used to settle (positively) the sum of square roots problem for a special class of integers: Suppose each integer ai is of the form, ai = Xdi + bi1X di-1 +⋯ + bidi, di > 0, where X is a positive real number and bij's are integers. Let B = max({|bij|}i,j, 1) and d = maxi{d i}. If X > (B + 1)(n.d)o(1)# then a nonzero S = Σni = 1 δi · √a i is lower bounded as |S| ≥ 1/X(n.d)o(1). The constant in O(1), as fixed by our analysis, is roughly 2. We then consider the following more general problem. Given an arithmetic circuit computing a multivariate polynomial f(X) and integer d, is the degree of f(X) less than or equal to d? We give a coRP-algorithm for this problem, improving previous results of Allender et al. [2009] and Koiran and Perifel [2007]. © 2012 ACM.",Arithmetic circuits; Polynomials; Sum of square roots,Computational geometry; Integrating circuits; Logic circuits; Numerical analysis; Arithmetic circuit; Multivariate polynomial; Non-zero coefficients; Positive integers; Positive real; Power series; Special class; Sum of squares; Univariate; Polynomials
A parallel repetition theorem for constant-round Arthur-Merlin Proofs,2012,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870692566&doi=10.1145%2f2382559.2382561&partnerID=40&md5=ad9161ab3cb9bda415f251c9976ad517,"We show a parallel-repetition theorem for constant-round Arthur-Merlin Proofs, using an efficient reduction. As a consequence, we show that parallel repetition reduces the soundness-error at an optimal rate (up to a negligible factor) in constant-round public-coin argument systems, and constant-round public-coin proofs of knowledge. The first of these results resolves an open question posed by Bellare, Impagliazzo, and Naor (FOCS'97). © 2012 ACM.",Interactive proofs; Parallel-repetition; Public-coin protocols; Soundness error,Computation theory; Computer science; Argument systems; Interactive proofs; Optimal rate; Parallel-repetition; Computational methods
Relativized Worlds without Worst-Case to Average-Case Reductions for NP,2012,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901682662&doi=10.1145%2f2355580.2355583&partnerID=40&md5=6d6249e24dfc27626e53ac8eed716d30,"We prove that, relative to an oracle, there is no worst-case to average-case reduction for NP. We also handle classes that are somewhat larger than NP, as well as worst-case to errorless-average-case reductions. In fact, we prove that relative to an oracle, there is no worst-case to errorless-average-case reduction from NP to BPPNP‖. We also handle reductions from NP to the polynomial-time hierarchy and beyond, under strong restrictions on the number of queries the reductions can make. © 2012, ACM. All rights reserved.",Algorithms; Average-case complexity; oracles; Theory,
Parameterized bounded-depth Frege is not optimal,2012,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868675131&doi=10.1145%2f2355580.2355582&partnerID=40&md5=83d0a5ad58767b1a5e5868f3c2bd41c2,"A general framework for parameterized proof complexity was introduced by Dantchev et al. [2007]. There, the authors show important results on tree-like Parameterized Resolution-a parameterized version of classical Resolution-and their gap complexity theorem implies lower bounds for that system. The main result of this article significantly improves upon this by showing optimal lower bounds for a parameterized version of bounded-depth Frege. More precisely, we prove that the pigeonhole principle requires proofs of size n Ω(k) in parameterized bounded-depth Frege, and, as a special case, in dag-like Parameterized Resolution. This answers an open question posed in Dantchev et al. [2007]. In the opposite direction, we interpret a well-known technique for FPT algorithms as a DPLL procedure for Parameterized Resolution. Its generalization leads to a proof search algorithm for Parameterized Resolution that in particular shows that tree-like Parameterized Resolution allows short refutations of all parameterized contradictions given as bounded-width CNFs. © 2012 ACM.",Bounded-depth Frege; Parameterized complexity; Proof complexity; Resolution,Optical Activity; Optimization; Parameters; Resolution; Forestry; Optical resolving power; Optimization; Trees (mathematics); Bounded-depth Frege; Complexity theorem; DPLL procedure; FPT algorithms; Lower bounds; Optimal lower bound; Parameterized; Parameterized complexity; Pigeonhole principle; Proof complexity; Proof search; Parameterization
Collusion-resistant mechanisms with verification yielding optimal solutions,2012,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863656260&doi=10.1145%2f2189778.2189781&partnerID=40&md5=e188cf63c571bd8e61b324d80f87407c,"A truthful mechanism consists of an algorithm augmented with a suitable payment function that guarantees that the players cannot improve their utilities by cheating. Mechanism design approaches are particularly appealing for designing protocols that cannot be manipulated by rational players. We present new constructions of so-called mechanisms with verification introduced by Nisan and Ronen [2001]. We first show how to obtain mechanisms that, for single-parameter domains, are resistant to coalitions of colluding agents even if they can exchange compensations. Based on this result we derive a class of exact truthful mechanisms with verification for arbitrary bounded domains. This class of problems includes most of the problems studied in the algorithmic mechanism design literature and for which exact solutions cannot be obtained with truthful mechanisms without verification. This result is an improvement over all known previous constructions of exact mechanisms with verification. © 2012 ACM.",Algorithmic mechanism design; Collusion; Game theory,Game theory; Machine design; Algorithmic mechanism design; Bounded domain; Collusion; Collusion-resistant mechanisms; Exact solution; Mechanism design; Mechanisms with verification; New constructions; Optimal solutions; Payment functions; Truthful mechanisms; Algorithms
The computational complexity of nash equilibria in concisely represented games,2012,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863708832&doi=10.1145%2f2189778.2189779&partnerID=40&md5=332627746792fc468c679d164b004c1a,"Games may be represented in many different ways, and different representations of games affect the complexity of problems associated with games, such as finding a Nash equilibrium. The traditional method of representing a game is to explicitly list all the payoffs, but this incurs an exponential blowup as the number of agents grows. We study two models of concisely represented games: circuit games, where the payoffs are computed by a given boolean circuit, and graph games, where each agent's payoff is a function of only the strategies played by its neighbors in a given graph. For these two models, we study the complexity of four questions: determining if a given strategy is a Nash equilibrium, finding a Nash equilibrium, determining if there exists a pure Nash equilibrium, and determining if there exists a Nash equilibrium in which the payoffs to a player meet some given guarantees. In many cases, we obtain tight results, showing that the problems are complete for various complexity classes. © 2012 ACM.",Circuit games; Computational game theory; Concise games; Graph games; Nash equilibrium,Computational complexity; Electric network analysis; Game theory; Boolean circuit; Complexity class; Concise games; Graph games; Nash equilibria; Pure Nash equilibrium; Telecommunication networks
Complexity theory for operators in analysis,2012,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863649116&doi=10.1145%2f2189778.2189780&partnerID=40&md5=b46d85a09902cb8a0db33b688838c9dd,"We propose an extension of the framework for discussing the computational complexity of problems involving uncountably many objects, such as real numbers, sets and functions, that can be represented only through approximation. The key idea is to use a certain class of string functions as names representing these objects. These are more expressive than infinite sequences, which served as names in prior work that formulated complexity in more restricted settings. An advantage of using string functions is that we can define their size in a way inspired by higher-type complexity theory. This enables us to talk about computation on string functions whose time or space is bounded polynomially in the input size, giving rise to more general analogues of the classes P, NP, and PSPACE. We also define NP- and PSPACE-completeness under suitable many-one reductions. Because our framework separates machine computation and semantics, it can be applied to problems on sets of interest in analysis once we specify a suitable representation (encoding). As prototype applications, we consider the complexity of functions (operators) on real numbers, real sets, and real functions. For example, the task of numerical algorithms for solving a certain class of differential equations is naturally viewed as an operator taking real functions to real functions. As there was no complexity theory for operators, previous results only stated how complex the solution can be. We now reformulate them and show that the operator itself is polynomial-space complete. © 2012 ACM.",Computable analysis; Computational complexity; Higher-type complexity; Second-order polynomials,Algorithms; Differential equations; Semantics; Complexity theory; Computable analysis; Higher-type complexity; Input size; Machine computations; Numerical algorithms; Real functions; Real number; Real sets; Second-order polynomial; Computational complexity
Approximating linear threshold predicates,2012,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859376220&doi=10.1145%2f2141938.2141940&partnerID=40&md5=a167dab2b8dbd1c2abc6ed46c4c5fe39,"We study constraint satisfaction problems on the domain {-1, 1}, where the given constraints are homogeneous linear threshold predicates, that is, predicates of the form sgn(w 1x 1 + · · · + w nx n) for some positive integer weights w 1, . . . , w n. Despite their simplicity, current techniques fall short of providing a classification of these predicates in terms of approximability. In fact, it is not easy to guess whether there exists a homogeneous linear threshold predicate that is approximation resistant or not. The focus of this article is to identify and study the approximation curve of a class of threshold predicates that allow for nontrivial approximation. Arguably the simplest such predicate is the majority predicate sgn(x 1 + · · · + x n), for which we obtain an almost complete understanding of the asymptotic approximation curve, assuming the Unique Games Conjecture. Our techniques extend to a more general class of ""majority-like"" predicates and we obtain parallel results for them. In order to classify these predicates, we introduce the notion of Chow-robustness that might be of independent interest. © 2012 ACM 1942-3462/2012/03-ART2 $10.00.",Approximation algorithms; Constraint satisfactory problems; Linear threshold predicates,Computation theory; Computational methods; Computer science; Approximability; Approximation curves; Asymptotic approximation; Constraint satisfaction problems; Constraint satisfactory problems; General class; IS approximation; Linear threshold predicates; Positive integers; Unique games conjecture; Approximation algorithms
Exact quantum algorithms for the leader election problem,2012,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859416486&doi=10.1145%2f2141938.2141939&partnerID=40&md5=cea1fb2c6eef14048dc059b4e9fd466d,"This article gives a separation between quantum and classical models in pure (i.e., noncryptographic) computing abilities with no restriction on the amount of available computing resources, by considering the exact solvability of the leader election problem in anonymous networks, a celebrated unsolvable problem in classical distributed computing. The goal of the leader election problem is to elect a unique leader from among distributed parties. In an anonymous network, all parties with the same number of communication links are identical. It is well-known that no classical algorithm can exactly solve (i.e., in bounded time without error) the leader election problem in anonymous networks, even if the number of parties is given. This article devises a quantum algorithm that, if the number of parties is given, exactly solves the problem for any network topology in polynomial rounds with polynomial communication/time complexity with respect to the number of parties, when the parties are connected with quantum communication links and they have the ability of quantum computing. Our algorithm works even when only an upper bound of the number of parties is given. In such a case, no classical algorithm can solve the problem even under the zero-error setting, the setting in which error is not allowed but running time may be unbounded. © 2012 ACM 1942-3462/2012/03-ART1 $10.00.",Distributed computing; Qquantum algorithms; Quantum communication,Algorithms; Computer networks; Distributed computer systems; Electric network topology; Quantum communication; Quantum computers; Quantum theory; Anonymous Networks; Classical model; Computing abilities; Computing resource; Leader Election Problem; Network topology; Quantum algorithms; Quantum Computing; Running time; Unsolvable problems; Upper Bound; Problem solving
Extractors and lower bounds for locally samplable sources,2012,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859416948&doi=10.1145%2f2141938.2141941&partnerID=40&md5=4ccf709b830f472f87cd964360d23a2c,"We consider the problem of extracting randomness from sources that are efficiently samplable, in the sense that each output bit of the sampler only depends on some small number d of the random input bits. As our main result, we construct a deterministic extractor that, given any d-local source with min-entropy k on n bits, extracts Ω(k 2/nd) bits that are 2 -nΩ(1) -close to uniform, provided d ≤ o(log n) and k ≥ n 2/3+γ (for arbitrarily small constants γ > 0). Using our result, we also improve a result of Viola [2010] who proved a 1/2-O(1/ log n) statistical distance lower bound for o(log n)-local samplers trying to sample input-output pairs of an explicit boolean function, assuming the samplers use at most n+ n 1-δ random bits for some constant δ > 0. Using a different function, we simultaneously improve the lower bound to 1/2 - 2 -nΩ(1) and eliminate the restriction on the number of random bits. © 2012 ACM 1942-3462/2012/03-ART3 $10.00.",Extractors; Locally samplable sources; Lower bounds,Computation theory; Computational methods; Computer science; Deterministic extractor; Extractors; IMPROVE-A; Input-output; Lower bounds; Min-entropy; Random bits; Random input; Statistical distance; Boolean functions
Three-query locally decodable codes with higher correctness require exponential length,2012,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857764498&doi=10.1145%2f2077336.2077338&partnerID=40&md5=6227d9da5d9e92f62e6297b69e925ca0,"Locally decodable codes are error-correcting codes with the extra property that, in order to retrieve the value of a single input position, it is sufficient to read a small number of positions of the codeword. We refer to the probability of getting the correct value as the correctness of the decoding algorithm. A breakthrough result by Yekhanin [2007] showed that 3-query linear locally decodable codes may have subexponential length. The construction of Yekhanin, and the three query constructions that followed, achieve correctness only up to a certain limit which is 1 - 3δ for nonbinary codes, where an adversary is allowed to corrupt up to δ fraction of the codeword. The largest correctness for a subexponential length 3-query binary code is achieved in a construction by Woodruff [2008], and it is below 1 - 3δ. We show that achieving slightly larger correctness (as a function of δ) requires exponential codeword length for 3-query codes. Previously, there were no larger than quadratic lower bounds known for locally decodable codes with more than 2 queries, even in the case of 3-query linear codes. Our lower bounds hold for linear codes over arbitrary finite fields and for binary nonlinear codes. Considering larger number of queries, we obtain lower bounds for q-query codes for q > 3, under certain assumptions on the decoding algorithm that have been commonly used in previous constructions. We also prove bounds on the largest correctness achievable by these decoding algorithms, regardless of the length of the code. Our results explain the limitations on correctness in previous constructions using such decoding algorithms. In addition, our results imply trade-offs on the parameters of error-correcting data structures. © 2012 ACM.",Locally decodable codes; Lower bounds,Algorithms; Data structures; Codeword; Codeword length; Decoding algorithm; Error correcting code; Finite fields; Linear codes; Locally-decodable codes; Lower bounds; Non-binary codes; Nonlinear codes; Query construction; Single input; Decoding
The value of multiple read/write streams for approximating frequency moments,2012,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857741145&doi=10.1145%2f2077336.2077339&partnerID=40&md5=3ead16f042a833bbfce27079164dc4f3,"We consider the read/write streams model, an extension of the standard data stream model in which an algorithm can create and manipulate multiple read/write streams in addition to its input data stream. Like the data stream model, the most important parameter for this model is the amount of internal memory used by such an algorithm. The other key parameters are the number of streams the algorithm uses and the number of passes it makes on these streams. We consider how the addition of multiple streams impacts the ability of algorithms to approximate the frequency moments of the input stream. We show that any randomized read/write stream algorithm with a fixed number of streams and a sublogarithmic number of passes that produces a constant factor approximation of the k-th frequency moment Fk of an input sequence of length of at most N from {1,..., N} requires space Ω(N 1-4/k-δ) for any δ > 0. For comparison, it is known that with a single read-only one-pass data stream there is a randomized constantfactor approximation for F k using O(N 1-2/k) space, and that by sorting, which can be done deterministically in O(log N) passes using 3 read/write streams, F k can be computed exactly. Therefore, although the ability to manipulate multiple read/write streams can add substantial power to the data stream model, with a sublogarithmic number of passes this does not significantly improve the ability to approximate higher frequency moments efficiently. We obtain our results by showing a new connection between the read/write streams model and the multiparty number-in-hand communication model. © 2012 ACM.",Communication complexity; Data streams; External-memory algorithms; Frequency moments; Read/write streams,Communication; Data communication systems; Information theory; Parameter estimation; Communication complexity; Data stream; External memory algorithms; Frequency moments; Read/write streams; Approximation algorithms
Pebbles and branching programs for tree evaluation,2012,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857693322&doi=10.1145%2f2077336.2077337&partnerID=40&md5=aedd98fea3f9cb40651b30b5f1e5a66b,"We introduce the tree evaluation problem, show that it is in LogDCFL (and hence in P), and study its branching program complexity in the hope of eventually proving a superlogarithmic space lower bound. The input to the problem is a rooted, balanced d-ary tree of height h, whose internal nodes are labeled with d-ary functions on [k] = {1,..., k}, and whose leaves are labeled with elements of [k]. Each node obtains a value in [k] equal to its d-ary function applied to the values of its d children. The output is the value of the root. We show that the standard black pebbling algorithm applied to the binary tree of height h yields a deterministic k-way branching program with O(kh) states solving this problem, and we prove that this upper bound is tight for h = 2 and h = 3. We introduce a simple semantic restriction called thrifty on k-way branching programs solving tree evaluation problems and show that the same state bound of Θ(k h) is tight for all h ≥ 2 for deterministic thrifty programs. We introduce fractional pebbling for trees and show that this yields nondeterministic thrifty programs with Θ(kh/2+1) states solving the Boolean problem determine whether the root has value 1"", and prove that this bound is tight for h = 2, 3, 4. We also prove that this same bound is tight for unrestricted nondeterministic k-way branching programs solving the Boolean problem for h = 2, 3. © 2012 ACM.",Branching programs; Log space; LogDCFL; Lower bounds,Algorithms; Binary Systems; Computer Programs; Problem Solving; Binary trees; Forestry; Function evaluation; Semantics; Branching programs; Internal nodes; Log space; LogDCFL; Lower bounds; Tree evaluations; Upper Bound; Problem solving
Approximate query complexity,2011,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053198199&doi=10.1145%2f2003685.2003688&partnerID=40&md5=c2208cd06b3b085252c24abcda6e7016,"Let f: {0, 1}n → {0, 1}. Let μ be a product probability measure on {0, 1}n. For ε = 0, we define Dε (f), the ε-approximate decision tree complexity of f, to be the minimum depth of a decision tree T with μ(T(x) = f (x)) = ε. For j = 0 or 1 and for δ ≥ 0, we define Cj,δ (f), the δ-approximate j-certificate complexity of f, to be the minimum certificate complexity of a set A ⊂ ω with μ(AΔf-1(j)) = ε. Note that if μ(x) > 0 for all x then D0(f) = D(f) and Cj,0(f) = Cj(f) are the ordinary decision tree and j-certificate complexities of f, respectively. We extend the well-known result, D(f) ≤ C 1(f)C0(f) [Blum and Impagliazzo 1987; Hartmanis and Hemachandra 1991; Tardos 1989], proving that for all ε > 0 there exists a δ > 0 and a constant K = K(ε, δ) > 0 such that for all n, μ, f, Dε (f) ≤ KC1,δ (f)C 0,δ (f). We also give a partial answer to a related question on query complexity raised by Tardos [1989]. We prove generalizations of these results to general product probability spaces. © 2011 ACM.",Approximate certificate complexity; Approximate decision tree complexity; Certificate complexity; Communication complexity; Decision tree complexity,Decision trees; Query processing; Approximate certificate complexity; Approximate decision tree complexity; Certificate complexity; Communication complexity; Decision tree complexity; Plant extracts
On the power of isolation in planar graphs,2011,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053216038&doi=10.1145%2f2003685.2003687&partnerID=40&md5=c16b3e9763e46280e79be04fa1c247bc,"We study (deterministic) isolation for certain structures in directed and undirected planar graphs. The motivation for undertaking such a study comes from recent positive results on this topic. For example: Bourke et al. [2009] isolate a directed path in planar graphs and subsequently Datta et al. [2010b] isolate a perfect matching in bipartite planar graphs. Our first observation is that sufficiently strong (and plausible) isolations for certain structures in planar graphs would have strong consequences such as: NL ⊂ L, Bipartite-Matching ε NC, and NP ⊂ P. Our second observation is that although we do not yet have such strong isolations for arbitrary planar graphs, we do have them for bipartite planar graphs, that is, nonbipartiteness is the main bottleneck. © 2011 ACM.",Isolating lemma; NC; NL; Perfect matching; Planar graphs,Graphic methods; Isolating lemma; NC; NL; Perfect matchings; Planar graph; Graph theory
Kolmogorov complexity in randomness extraction,2011,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053188811&doi=10.1145%2f2003685.2003686&partnerID=40&md5=df7c804e2ffe4f95463b83817ff5e861,"We clarify the role of Kolmogorov complexity in the area of randomness extraction. We show that a computable function is an almost randomness extractor if and only if it is a Kolmogorov complexity extractor, thus establishing a fundamental equivalence between two forms of extraction studied in the literature: Kolmogorov extraction and randomness extraction. We present a distribution Mk based on Kolmogorov complexity that is complete for randomness extraction in the sense that a computable function is an almost randomness extractor if and only if it extracts randomness from Mk. © 2011 ACM.",Extractors; Kolmogorov complexity; Randomness,Functions; Computable functions; Extractors; Kolmogorov; Kolmogorov complexity; Randomness; Randomness extractors; Random processes
Lower bounds for coin-weighing problems,2011,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953234950&doi=10.1145%2f1944857.1944858&partnerID=40&md5=0daa10ebbc4d95a01c2563f8e4ce1918,"Among a set of n coins of two weights (good and bad), and using a balance, we wish to determine the numberof bad coins using as few measurements as possible. There is a known adaptive decision tree that answers this question in O((log(n))2) measurements, and a slight modification of this decision tree determines the parity of the number of bad coins in O(log n). In this article, we prove an Ω(√n) lower bound on the depth of any oblivious decision tree which solves either the counting or the parity problem. Our lower bound can also be applied to any function of high average sensitivity, which includes most random functions and most random symmetric functions. With a slight generalization of this result, we derive lower bounds for the size of threshold circuits for a wide class of Boolean functions. We demonstrate an exponential gap between the nonadaptive and adaptive coin-weighing complexities of the counting and parity problems. We prove a tight θ(log n) bound on the adaptive coin-weighing complexity of the parity problem. © 2011 ACM.",Coin-weighing problems,Decision trees; Weighing; Average sensitivities; Lower bounds; Parity problems; Random functions; Symmetric functions; Threshold circuits; Weighing problem; Boolean functions
A complexity dichotomy for finding disjoint solutions of vertex deletion problems,2011,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953237099&doi=10.1145%2f1944857&partnerID=40&md5=59f893495e36bf03e31ec7d5a4b2c807,"We investigate the computational complexity of a general ""compression task"" centrally occurring in the recently developed technique of iterative compression for exactly solving NP-hard minimization problems. The core issue (particularly but not only motivated by iterative compression) is to determine the computational complexity of the following task: given an already inclusion-minimal solution for an underlying (typically NP-hard) vertex deletion problem in graphs, find a smaller disjoint solution. The complexity of this task is so far lacking a systematic study. We consider a large class of vertex deletion problems on undirected graphs and show that a few cases are polynomial-time solvable, and the others are NP-hard. The considered class of vertex deletion problems includes Vertex Cover (where the compression task is polynomial time) and Undirected Feedback Vertex Set (where the compression task is NP-complete). © 2011 ACM.",Computational complexity; Graph algorithms; Hereditary graph properties; Iterative compression; Parameterized complexity; Vertex deletion problems,Algorithms; Graph theory; Polynomial approximation; Graph algorithms; Hereditary graph properties; Iterative compression; Parameterized complexity; Vertex deletion problems; Computational complexity
A Complexity Dichotomy for Finding Disjoint Solutions of Vertex Deletion Problems,2011,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025412921&doi=10.1145%2f1944857.1944860&partnerID=40&md5=6533386070ee960df5b46dd93518c273,"We investigate the computational complexity of a general “compression task” centrally occurring in the recently developed technique of iterative compression for exactly solving NP-hard minimization problems. The core issue (particularly but not only motivated by iterative compression) is to determine the computational complexity of the following task: given an already inclusion-minimal solution for an underlying (typically NP-hard) vertex deletion problem in graphs, find a smaller disjoint solution. The complexity of this task is so far lacking a systematic study. We consider a large class of vertex deletion problems on undirected graphs and show that a few cases are polynomial-time solvable, and the others are NP-hard. The considered class of vertex deletion problems includes Vertex Cover (where the compression task is polynomial time) and Undirected Feedback Vertex Set (where the compression task is NP-complete). © 2011, ACM. All rights reserved.",Algorithms; computational complexity; graph algorithms; hereditary graph properties; Iterative compression; parameterized complexity; Theory; vertex deletion problems,
Solvable group isomorphism is (almost) in NP CoNP,2011,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953246396&doi=10.1145%2f1944857.1944859&partnerID=40&md5=08a792d0aca494ec13f9de40d023b8f9,"The Group Isomorphism problem consists in deciding whether two input groups G1 and G2 given by their multiplication tables are isomorphic. We first give a 2-round Arthur-Merlin protocol for the Group Nonisomorphism problem such that on input groups (G1,G2) of size n, Arthur uses O(log6 n) random bits and Merlin uses O(log2 n) nondeterministic bits. We derandomize this protocol for the case of solvable groups showing the following two results: (a) We give a uniform NP machine for solvable Group Nonisomorphism, that works correctly on all but 2logO(1)(n) inputs of any length n. Furthermore, this NP machine is always correct when the input groups are nonisomorphic. The NP machine is obtained by an unconditional derandomization of the aforesaid AM protocol. (b) Under the assumption that EXP i.o-PSPACE we get a complete derandomization of the aforesaid AM protocol. Thus, EXP i.o-PSPACE implies that Group Isomorphism for solvable groups is in NP n coNP. © 2011 ACM.",Arthur-merlin games; Computational and structural complexity; Derandomization; Mlimited nondeterminism,Set theory; Arthur-Merlin games; Computational and structural complexity; Derandomization; Isomorphism problems; Mlimited nondeterminism; Random bits; Solvable group; Computational complexity
Cell-probe proofs,2010,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649579402&doi=10.1145%2f1867719.1867720&partnerID=40&md5=5e88f76574f665496dee68ab2f0ee34a,"We study the nondeterministic cell-probe complexity of static data structures. We introduce cellprobe proofs (CPP), a proof system for the cell-probe model, which describes verification instead of computation in the cell-probe model. We present a combinatorial characterization of CPP. With this novel tool, we prove the following lower bounds for the nondeterministic cell-probe complexity of static data structures. -There exists a data structure problem with high nondeterministic cell-probe complexity. -For the exact nearest neighbor search (NNS) problem or the partial match problem in high dimensional Hamming space, for any data structure with Poly(n) cells, each of which contains O (nC) bits where C < 1, the nondeterministic cell-probe complexity is at least ω (log(d/ log n)), where d is the dimension and n is the number of points in the data set. -For the polynomial evaluation problem of d-degree polynomial over finite field of size 2k where d ≤ 2k, for any data structure with s cells, each of which contains b bits, the nondeterministic cell-probe complexity is at least min (k/b (d- 1), k-log(d-1)/log s). © 2010 ACM.",Cell-probe model; Nondeterminism,Cytology; Data structures; Probes; Cell-probe models; Data sets; Finite fields; Hamming space; High-dimensional; Lower bounds; Nearest Neighbor search; Nondeterminism; Partial matches; Polynomial evaluation; Proof system; Cells
Tradeoffs and average-case equilibria in selfish routing,2010,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649619909&doi=10.1145%2f1867719.1867721&partnerID=40&md5=6c47cafd15590f76273be8192877d151,"We study Nash equilibria in a selfish routing game on m parallel links with transmission speeds. Each player seeks to communicate a message by choosing one of the links, and each player desires to minimize his experienced transmission time (latency). For evaluating the social cost of Nash equilibria, we consider the price of anarchy, which is the largest ratio of the cost of any Nash equilibrium compared to the optimum solution. Similarly, we consider the price of stability, which is the smallest ratio. The main purpose of this article is to quantify the influence of three parameters upon the prices of the game: the total traffic in the network; restrictions of the players in terms of link choice; and fluctuations in message lengths. Our main interest is to bound the sum of all player latencies, which we refer to as collective latency. For this cost, the prices of anarchy and stability are Θ(n/t), where n is the number of players and t the sum of message lengths (total traffic); that is, Nash equilibria approximate the optimum solution up to a constant factor if the traffic is high. If each player is restricted to choose from a subset of links, these link restrictions can cause a degradation in performance of order Θ (√m) . The prices of anarchy and stability increase to Θ (n√m/t) . We capture fluctuations in message lengths through a stochastic model, in which we valuate Nash equilibria in terms of their expected price of anarchy. The expected price is Θ(n/E[T]), where E[T] is the expected traffic. The stochastic model resembles the deterministic one, even for the efficiency loss of order Θ (√m)for link restrictions. For the social cost function maximum latency, the (expected) price of anarchy is 1+m 2/t. In this case, Nash equilibria are almost optimal solutions for congested networks. Similar results hold when the cost function is a polynomial of the link loads. © 2010 ACM.",Average-case analysis; Price of anarchy; Selfish routing,Cost functions; Stochastic models; Stochastic systems; Telecommunication networks; Average-case; Average-case analysis; Congested networks; Constant factors; Efficiency loss; Link Loads; Maximum latency; Message length; Nash equilibria; Nash Equilibrium; Optimal solutions; Optimum solution; Parallel links; Price of anarchy; Price of Stability; Selfish routing; Social cost; Three parameters; Transmission speed; Transmission time; Costs
Formula caching in DPLL,2010,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949558045&doi=10.1145%2f1714450.1714452&partnerID=40&md5=8df6fc25ca87ed45f96b6e2dfc9df1c3,"We consider extensions of the DPLL approach to satisfiability testing that add a version of memoization, in which formulas that the algorithm has previously shown to be unsatisfiable are remembered for later use. Such formula caching algorithms have been suggested for satisfiability and stochastic satisfiability by several authors. We formalize these methods by developing extensions of the fruitful connection that has previously been developed between DPLL algorithms for satisfiability and tree-like resolution proofs of unsatisfiability. We analyze a number of variants of these formula caching methods and characterize their strength in terms of proof systems. These proof systems are new and simple, and have a rich structure. We compare them to several studied proof systems: tree-like resolution, regular resolution, general resolution, Res(k), and Frege systems and present both simulation and separations. One of our most interesting results is the introduction of a natural and implementable form of DPLL with caching, FCW reason. This system is surprisingly powerful: we prove that it can polynomially simulate regular resolution, and furthermore, it can produce short proofs of some formulas that require exponential-size resolution proofs. © 2010 ACM.",Proof complexity; Resolution; Satisfiability,Caching algorithm; Caching methods; DPLL algorithm; Memoization; Proof complexity; Proof system; Resolution proofs; Rich structure; Satisfiability; Satisfiability testing; Stochastic satisfiability; Boolean functions
"Planarity, determinants, permanents, and (Unique) matchings",2010,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949542763&doi=10.1145%2f1714450.1714453&partnerID=40&md5=5c8d5a55e60fbb3638e0de01ca1e1a87,"Viewing the computation of the determinant and the permanent of integer matrices as combinatorial problems on associated graphs, we explore the restrictiveness of planarity on their complexities and show that both problems remain as hard as in the general case, that is, GapL- and P- complete. On the other hand, both bipartite planarity and bimodal planarity bring the complexity of permanents down (but no further) to that of determinants. The permanent or the determinant modulo 2 is complete for ⊕L, and we show that parity of paths in a layered grid graph (which is bimodal planar) is also complete for this class. We also relate the complexity of grid graph reachability to that of testing existence/uniqueness of a perfect matching in a planar bipartite graph. © 2010 ACM.",Bipartite graphs; Counting classes; Determinant; Perfect matching; Permanent; Planarity,Associated graph; Bipartite graphs; Combinatorial problem; Counting class; Existence/uniqueness; Grid graphs; Integer matrices; Matchings; Modulo 2; P-complete; Perfect matchings; Planarity; Reachability; Graph theory
Logspace reduction of directed reachability for bounded genus graphs to the planar case,2010,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949552315&doi=10.1145%2f1714450.1714451&partnerID=40&md5=560f25ad88fd3f184020a5d8105c2048,"Directed reachability (or briefly reachability) is the following decision problem: given a directed graph G and two of its vertices s, t, determine whether there is a directed path from s to t in G. Directed reachability is a standard complete problem for the complexity class NL. Planar reachability is an important restricted version of the reachability problem, where the input graph is planar. Planar reachability is hard for L and is contained in NL but is not known to be NLcomplete or contained in L. Allender et al. [2009] showed that reachability for graphs embedded on the torus is logspace-reducible to the planar case. We generalize this result to graphs embedded on a fixed surface of arbitrary genus. © 2010 ACM.",Directed graph reachability; Graphs on surfaces; Log-space reduction,Graph theory; Graphic methods; Bounded-genus graphs; Complete problems; Complexity class; Decision problems; Graphs on surfaces; Log space; Reachability; Reachability problem; Directed graphs
Sound 3-query PCPPs are long,2009,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955601245&doi=10.1145%2f1595391.1595394&partnerID=40&md5=9cec2c932e977947fb8a6207dc92c2b4,"We initiate the study of the trade-off between the length of a probabilistically checkable proof of proximity (PCPP) and the maximal soundness that can be guaranteed by a 3-query verifier with oracle access to the proof. Our main observation is that a verifier limited to querying a short proof cannot obtain the same soundness as that obtained by a verifier querying a long proof. Moreover, we quantify the soundness deficiency as a function of the proof-length and show that any verifier obtaining ""best possible"" soundness must query an exponentially long proof. In terms of techniques, we focus on the special class of inspective verifiers that read at most 2 proof-bits per invocation. For such verifiers, we prove exponential length-soundness trade-offs that are later on used to imply our main results for the case of general (i.e., not necessarily inspective) verifiers. To prove the exponential trade-off for inspective verifiers, we show a connection between PCPP proof length and property-testing query complexity that may be of independent interest. The connection is that any linear property that can be verified with proofs of length L by linear inspective verifiers must be testable with query complexity ≈ log L. © 2009 ACM.",PCP; PCP of proximity,Linear properties; Oracle access; PCP; PCP of proximity; Probabilistically checkable proof; Proof-length; Property-testing; Query complexity; Special class; Commerce
Hardness of solving sparse overdetermined linear systems: A 3-Query PCP over Integers,2009,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955640314&doi=10.1145%2f1595391.1595393&partnerID=40&md5=65341603c5c32dbfbf09c584531fe208,"A classic result due to Ha°stad established that for every constant ε > 0, given an overdetermined system of linear equations over a finite field Fq where each equation depends on exactly 3 variables and at least a fraction (1-ε) of the equations can be satisfied, it is NP-hard to satisfy even a fraction 1/q+ε of the equations. In this work, we prove the analog of Ha°stad's result for equations over the integers (as well as the reals). Formally, we prove that for every ε, δ > 0, given a system of linear equations with integer coefficients where each equation is on 3 variables, it is NP-hard to distinguish between the following two cases: (i) there is an assignment of integer values to the variables that satisfies at least a fraction (1 -ε) of the equations, and (ii) no assignment even of real values to the variables satisfies more than a fraction δ of the equations. © 2009 ACM.",Hardness of approximation; Linearity testing; Probabilistically checkable proofs; Sparse linear equations,Computational complexity; Hardness; Hardness testing; Linear equations; Linear systems; Linearization; Finite fields; Hardness of approximation; Integer coefficient; Integer values; Linearity testing; NP-hard; Overdetermined systems; Probabilistically checkable proof; Real values; Sparse linear equations; System of linear equations; Real variables
Improved separations between nondeterministic and randomized multiparty communication,2009,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955635105&doi=10.1145%2f1595391.1595392&partnerID=40&md5=4490df0a64279d9412b6cec42fc7edc0,"We exhibit an explicit function f : {0, 1}n → {0, 1} that can be computed by a nondeterministic number-on-forehead protocol communicating O(log n) bits, but that requires nω(1) bits of communication for randomized number-on-forehead protocols with κ = δ· log n players, for any fixed δ <1. Recent breakthrough results for the Set-Disjointness function [Lee and Shraibman 2008; Chattopadhyay and Ada 2008] based on the work of Sherstov [2009; 2008a] imply such a separation but only when the number of players is κ < log log n. We also show that for any κ = A ̇log log n the above function f is computable by a small circuit whose depth is constant whenever A is a (possibly large) constant. Recent results again give such functions but only when the number of players is κ < log log n. © 2009 ACM.",Communication complexity; Lower bound; Nondeterminism; Number on forehead; Randomization,Communication complexity; Lower bounds; Non-determinism; Number on forehead; Randomization; Communication
Directed planar reachability is in unambiguous log-space,2009,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77749310908&doi=10.1145%2f1490270.1490274&partnerID=40&md5=a0ecc06ef66b4753fb8abe4526e411e8,"We make progress in understanding the complexity of the graph reachability problem in the context of unambiguous logarithmic space computation; a restricted form of nondeterminism. As our main result, we show a new upper bound on the directed planar reachability problem by showing that it can be decided in the class unambiguous logarithmic space (UL). We explore the possibility of showing the same upper bound for the general graph reachability problem. We give a simple reduction showing that the reachability problem for directed graphs with thickness two is complete for the class nondeterministic logarithmic space (NL). Hence an extension of our results to directed graphs with thickness two will unconditionally collapse NL to UL. © 2009 ACM.",Directed graph reachability; Planar graphs; Unambiguous log-space,Directed graphs; General graph; Non-determinism; Planar graph; Reachability; Reachability problem; Upper Bound; Graph theory
Algebrization: A new barrier in complexity theory,2009,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350664499&doi=10.1145%2f1490270.1490272&partnerID=40&md5=5c1b0e98015e012625b3139a2ca660d3,"Any proof of P ≠ NP will have to overcome two barriers: relativization and natural proofs. Yet over the last decade, we have seen circuit lower bounds (e.g., that PP does not have linear-size circuits) that overcome both barriers simultaneously. So the question arises of whether there is a third barrier to progress on the central questions in complexity theory. In this article, we present such a barrier, which we call algebraic relativization or algebrization. The idea is that, when we relativize some complexity class inclusion, we should give the simulating machine access not only to an oracle A, but also to a low-degree extension of A over a finite field or ring. We systematically go through basic results and open problems in complexity theory to delineate the power of the new algebrization barrier. First, we show that all known nonrelativizing results based on arithmetization - -both inclusions such as IP = PSPACE and MIP = NEXP, and separations such as MAEXP ⊄ P/poly - -do indeed algebrize. Second, we show that almost all of the major open problems - -including P versus NP, P versus RP, and NEXP versus P/poly - -will require non-algebrizing techniques. In some cases, algebrization seems to explain exactly why progress stopped where it did: for example, why we have superlinear circuit lower bounds for PromiseMA but not for NP. Our second set of results follows from lower bounds in a new model of algebraic query complexity, which we introduce in this article and which is interesting in its own right. Some of our lower bounds use direct combinatorial and algebraic arguments, while others stem from a surprising connection between our model and communication complexity. Using this connection, we are also able to give an MA-protocol for the Inner Product function with O (√nlogn) communication (essentially matching a lower bound of Klauck), as well as a communication complexity conjecture whose truth would imply NL ≠ NP. © 2009 ACM.",Arithmetization; Communication complexity; Interactive proofs; Low-degree polynomials; Oracles; Query complexity,Algebra; Computational complexity; Inclusions; Internet protocols; Plasma waves; Algebraic queries; Basic results; Circuit lower bounds; Communication complexity; Complexity class; Complexity theory; Finite fields; Inner product; Interactive proofs; Lower bounds; Natural proofs; New model; Open problems; Query complexity; Superlinear; Communication
ACM Transactions on Computation Theory: Editors foreword,2009,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77749339112&doi=10.1145%2f1490270.1490271&partnerID=40&md5=f7543cd5008ec26612cd584fedc264ec,[No abstract available],,
A simple proof of Bazzi's theorem,2009,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77749330248&doi=10.1145%2f1490270.1490273&partnerID=40&md5=4c30b97c5054eea5c77200c98a867e7c,"Linial and Nisan [1990] asked if any polylog-wise independent distribution fools any function in AC0. In a recent remarkable development, Bazzi solved this problem for the case of DNF formulas. The aim of this note is to present a simplified version of his proof. © 2009 ACM.",DNF; Pseudo-random generators,DNF formulas; Pseudorandom generators; Theorem proving
Point-hyperplane Incidence Geometry and the Log-rank Conjecture,2022,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174877423&doi=10.1145%2f3543684&partnerID=40&md5=63cd1b5687e2289eff0eefa0d72d2b89,"We study the log-rank conjecture from the perspective of point-hyperplane incidence geometry. We formulate the following conjecture: Given a point set in Rd that is covered by constant-sized sets of parallel hyperplanes, there exists an affine subspace that accounts for a large (i.e., 2−polylog(d)) fraction of the incidences, in the sense of containing a large fraction of the points and being contained in a large fraction of the hyperplanes. In other words, the point-hyperplane incidence graph for such configurations has a large complete bipartite subgraph. Alternatively, our conjecture may be interpreted linear-algebraically as follows: Any rank-d matrix containing at most O(1) distinct entries in each column contains a submatrix of fractional size 2−polylog(d), in which each column is constant. We prove that our conjecture is equivalent to the log-rank conjecture; the crucial ingredient of this proof is a reduction from bounds for parallel k-partitions to bounds for parallel (k−1)partitions. We also introduce an (apparent) strengthening of the conjecture, which relaxes the requirements that the sets of hyperplanes be parallel. Motivated by the connections above, we revisit well-studied questions in point-hyperplane incidence geometry without structural assumptions (i.e., the existence of partitions). We give an elementary argument for the existence of complete bipartite subgraphs of density Ω(ϵ2d/d) in any d-dimensional configuration with incidence density ϵ, qualitatively matching previous results proved using sophisticated geometric techniques. We also improve an upper-bound construction of Apfelbaum and Sharir [2], yielding a configuration whose complete bipartite subgraphs are exponentially small and whose incidence density is Ω(1/√d). Finally, we discuss various constructions (due to others) of products of Boolean matrices which yield configurations with incidence density Ω(1) and complete bipartite subgraph density 2−Ω(√d), and pose several questions for this special case in the alternative language of extremal set combinatorics. Our framework and results may help shed light on the difficulty of improving Lovett’s Õ(√rank(f )) bound [20] for the log-rank conjecture. In particular, any improvement on this bound would imply the first complete bipartite subgraph size bounds for parallel 3-partitioned configurations which beat our generic bounds for unstructured configurations. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",extremal set theory; incidence geometry; Log-rank conjecture; low-rank matrices,Computation theory; Geometry; Graph theory; Matrix algebra; Affine subspaces; Bipartite subgraphs; Extremal set theory; Extremal sets; Incidence geometry; Log-rank conjecture; Low-rank matrices; Point set; Polylogs; Sets theory; Set theory
A Polynomial Degree Bound on Equations for Non-rigid Matrices and Small Linear Circuits,2022,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186423766&doi=10.1145%2f3543685&partnerID=40&md5=b92f55213dec5b072ea57fa25cc38457,"We show that there is an equation of degree at most poly(n) for the (Zariski closure of the) set of the nonrigid matrices: That is, we show that for every large enough field F, there is a non-zero n2-variate polynomial P ∈ F[x1,1, . . ., xn,n] of degree at most poly(n) such that every matrix M that can be written as a sum of a matrix of rank at most n/100 and a matrix of sparsity at most n2/100 satisfies P(M) = 0. This confirms a conjecture of Gesmundo, Hauenstein, Ikenmeyer, and Landsberg [9] and improves the best upper bound known for this problem down from exp(n2) [9, 12] to poly(n). We also show a similar polynomial degree bound for the (Zariski closure of the) set of all matrices M such that the linear transformation represented by M can be computed by an algebraic circuit with at most n2/200 edges (without any restriction on the depth). As far as we are aware, no such bound was known prior to this work when the depth of the circuits is unbounded. Our methods are elementary and short and rely on a polynomial map of Shpilka and Volkovich [21] to construct low-degree “universal” maps for non-rigid matrices and small linear circuits. Combining this construction with a simple dimension counting argument to show that any such polynomial map has a low-degree annihilating polynomial completes the proof. As a corollary, we show that any derandomization of the polynomial identity testing problem will imply new circuit lower bounds. A similar (but incomparable) theorem was proved by Kabanets and Impagliazzo [11]. © 2022 Association for Computing Machinery.",Algebraic complexity theory; algebraic geometry; linear circuits; rigid matrices; tensor rank,Linear networks; Linear transformations; Polynomials; Timing circuits; Algebraic complexity theories; Algebraic geometry; Degree bounds; Linear circuits; Non-rigid; Polynomial degree; Polynomial maps; Rigid matrixes; Tensor ranks; Matrix algebra
Planar Graph Isomorphism Is in Log-Space,2022,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166613747&doi=10.1145%2f3543686&partnerID=40&md5=422d1afbd8cfb2e525404178295de2b4,"Graph Isomorphism is the prime example of a computational problem with a wide difference between the best-known lower and upper bounds on its complexity. The gap between the known upper and lower bounds continues to be very significant for many subclasses of graphs as well. We bridge the gap for a natural and important class of graphs, namely, planar graphs, by presenting a log-space upper bound that matches the known log-space hardness. In fact, we show a stronger result that planar graph canonization is in log-space. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Computational complexity; log-space; planar graph isomorphism,Computational complexity; Graph theory; Graphic methods; Computational problem; Graph isomorphism; Log space; Lower and upper bounds; Planar graph; Planar graph isomorphism; Upper and lower bounds; Upper Bound; Set theory
Optimal Distribution-Free Sample-Based Testing of Subsequence-Freeness with One-Sided Error,2022,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127521085&doi=10.1145%2f3512750&partnerID=40&md5=68693471778ae42f46a55a51e1b68d3b,"In this work, we study the problem of testing subsequence-freeness. For a given subsequence (word) w = w1.. wk, a sequence (text) T = t1.. tn is said to contain w if there exist indices 1 ≤ i1 <.. < ik ≤ n such that tij = wj for every 1 ≤ j ≤ k. Otherwise, T is w-free. While a large majority of the research in property testing deals with algorithms that perform queries, here we consider sample-based testing (with one-sided error). In the ""standard""sample-based model (i.e., under the uniform distribution), the algorithm is given samples (i, ti) where i is distributed uniformly independently at random. The algorithm should distinguish between the case that T is w-free, and the case that T is ϵ-far from being w-free (i.e., more than an ϵ-fraction of its symbols should be modified so as to make it w-free). Freitag, Price, and Swartworth (Proceedings of RANDOM, 2017) showed that O((k2 log k)ϵ) samples suffice for this testing task. We obtain the following results.-The number of samples sufficient for one-sided error sample-based testing (under the uniform distribution) is O(kϵ). This upper bound builds on a characterization that we present for the distance of a text T from w-freeness in terms of the maximum number of copies of w in T, where these copies should obey certain restrictions.-We prove a matching lower bound, which holds for every word w. This implies that the above upper bound is tight.-The same upper bound holds in the more general distribution-free sample-based model. In this model, the algorithm receives samples (i, ti) where i is distributed according to an arbitrary distribution p (and the distance from w-freeness is measured with respect to p).We highlight the fact that while we require that the testing algorithm work for every distribution and when only provided with samples, the complexity we get matches a known lower bound for a special case of the seemingly easier problem of testing subsequence-freeness with one-sided error under the uniform distribution and with queries (Canonne et al., Theory of Computing, 2019). © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Property testing; subsequence freeness,Distribution-free; Low bound; Matchings; Number of samples; Optimal distributions; Property-testing; Standard samples; Subsequence freeness; Uniform distribution; Upper Bound; Errors
Erasure-Resilient Sublinear-Time Graph Algorithms,2022,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127490147&doi=10.1145%2f3488250&partnerID=40&md5=8c6b3213025966ca137e1a2ce0c5060d,"We investigate sublinear-time algorithms that take partially erased graphs represented by adjacency lists as input. Our algorithms make degree and neighbor queries to the input graph and work with a specified fraction of adversarial erasures in adjacency entries. We focus on two computational tasks: testing if a graph is connected or ϵ-far from connected and estimating the average degree. For testing connectedness, we discover a threshold phenomenon: when the fraction of erasures is less than ϵ, this property can be tested efficiently (in time independent of the size of the graph); when the fraction of erasures is at least ϵ, then a number of queries linear in the size of the graph representation is required. Our erasure-resilient algorithm (for the special case with no erasures) is an improvement over the previously known algorithm for connectedness in the standard property testing model and has optimal dependence on the proximity parameter ϵ. For estimating the average degree, our results provide an ""interpolation""between the query complexity for this computational task in the model with no erasures in two different settings: with only degree queries, investigated by Feige (SIAM J. Comput. 06), and with degree queries and neighbor queries, investigated by Goldreich and Ron (Random Struct. Algorithms 08) and Eden et al. (ICALP 17). We conclude with a discussion of our model and open questions raised by our work. © 2021 Association for Computing Machinery.",approximating graph parameters; computing with incomplete information; Graph property testing,Approximating graph parameter; Average degree; Computational task; Computing with incomplete information; Graph algorithms; Graph parameters; Graph property testing; Incomplete information; Neighbor query; Sublinear time; Parameter estimation
Quantum Distributed Complexity of Set Disjointness on a Line,2022,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127508808&doi=10.1145%2f3512751&partnerID=40&md5=40eed388e478b73a01d8551486f8eb7f,"Given, Set Disjointness consists in deciding whether for some index. We study the problem of computing this function in a distributed computing scenario in which the inputs and are given to the processors at the two extremities of a path of length. Each vertex of the path has a quantum processor that can communicate with each of its neighbours by exchanging qubits per round. We are interested in the number of rounds required for computing Set Disjointness with constant probability bounded away from. We call this problem ""Set Disjointness on a Line"".Set Disjointness on a Line was introduced by Le Gall and Magniez [14] for proving lower bounds on the quantum distributed complexity of computing the diameter of an arbitrary network in the CONGEST model. However, they were only able to provide a lower bound when the local memory used by the processors on the intermediate vertices of the path is severely limited. More precisely, their bound applies only when the local memory of each intermediate processor consists of qubits.In this work, we prove an unconditional lower bound of rounds for Set Disjointness on a Line with processors. This is the first non-trivial lower bound when there is no restriction on the memory used by the processors. The result gives us a new lower bound of on the number of rounds required for computing the diameter of any-node network with quantum messages of size in the CONGEST model.We draw a connection between the distributed computing scenario above and a new model of query complexity. In this model, an algorithm computing a bi-variate function (such as Set Disjointness) has access to the inputs and through two separate oracles and, respectively. The restriction is that the algorithm is required to alternately make queries to and queries to, with input-independent computation in between queries. The model reflects a ""switching delay""of queries between a ""round""of queries to and the following ""round""of queries to. The information-theoretic technique we use for deriving the round lower bound for Set Disjointness on a Line also applies to the number of rounds in this query model. We provide an algorithm for Set Disjointness in this query model with round complexity that matches the round lower bound stated above, up to a polylogarithmic factor. This presents a barrier for obtaining a better round lower bound for Set Disjointness on the Line. At the same time, it hints at the possibility of better communication protocols for the problem. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",CONGEST model; Diameter Problem; Quantum distributed computing; rounds; Set Disjointness,Complex networks; Qubits; CONGEST model; Diameter problem; Disjointness; Local memory; Low bound; Quantum distributed computing; Quantum processors; Query model; Round; Set disjointness; Information theory
"Approximate Degree, Weight, and Indistinguishability",2022,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127464163&doi=10.1145%2f3492338&partnerID=40&md5=6b1159ff67986b461f4456611ba83b15,"We prove that the OR function on-1,1n can be pointwise approximated with error ϵ by a polynomial of degree O(k) and weight 2O(n log (1/ϵ)/k), for any k ≥ n log (1/ϵ). This result is tight for any k ≤ (1-ω (1))n. Previous results were either not tight or had ϵ = ω (1). In general, we obtain a tight approximate degree-weight result for any symmetric function. Building on this, we also obtain an approximate degree-weight result for bounded-width CNF. For these two classes no such result was known.We prove that the function on can be pointwise approximated with error by a polynomial of degree and weight, for any. This result is tight for any. Previous results were either not tight or had. In general, we obtain a tight approximate degree-weight result for any symmetric function. Building on this, we also obtain an approximate degree-weight result for bounded-width. For these two classes no such result was known.One motivation for such results comes from the study of indistinguishability. Two distributions, over-bit strings are-indistinguishable if their projections on any bits have statistical distance at most. The above approximations give values of that suffice to fool, symmetric functions, and bounded-width, and the first result is tight for all while the second result is tight for. We also show that any two-indistinguishable distributions are-close to two distributions that are-indistinguishable, improving the previous bound of. Finally, we present proofs of some known approximate degree lower bounds in the language of indistinguishability, which we find more intuitive. © 2022 Copyright held by the owner/author(s).",Approximate degree; approximate weight; CNF; indistinguishability; polynomial approximation; symmetric functions,Computation theory; Approximate degree; Approximate weight; Bit-strings; CNF; Indistinguishability; Low bound; Point wise; Statistical distance; Symmetric functions; Polynomial approximation
The (Coarse) Fine-Grained Structure of NP-Hard SAT and CSP Problems,2022,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127515304&doi=10.1145%2f3492336&partnerID=40&md5=2d45a614db7dfe648ff94bb65b9dbc33,"We study the fine-grained complexity of NP-complete satisfiability (SAT) problems and constraint satisfaction problems (CSPs) in the context of the strong exponential-time hypothesis(SETH), showing non-trivial lower and upper bounds on the running time. Here, by a non-trivial lower bound for a problem SAT (σ) (respectively CSP (σ)) with constraint language σ, we mean a value c0 > 1 such that the problem cannot be solved in time O(cn) for any c <c0 unless SETH is false, while a non-trivial upper bound is simply an algorithm for the problem running in time O(cn) for some c< 2. Such lower bounds have proven extremely elusive, and except for cases where c0=2 effectively no such previous bound was known. We achieve this by employing an algebraic framework, studying constraint languages σ in terms of their algebraic properties. We uncover a powerful algebraic framework where a mild restriction on the allowed constraints offers a concise algebraic characterization. On the relational side we restrict ourselves to Boolean languages closed under variable negation and partial assignment, called sign-symmetric languages. On the algebraic side this results in a description via partial operations arising from system of identities, with a close connection to operations resulting in tractable CSPs, such as near unanimity operations and edge operations. Using this connection we construct improved algorithms for several interesting classes of sign-symmetric languages, and prove explicit lower bounds under SETH. Thus, we find the first example of an NP-complete SAT problem with a non-trivial algorithm which also admits a non-trivial lower bound under SETH. This suggests a dichotomy conjecture with a close connection to the CSP dichotomy theorem: an NP-complete SAT problem admits an improved algorithm if and only if it admits a non-trivial partial invariant of the above form. © 2021 Copyright held by the owner/author(s).",constraint satisfaction problems; Fine-grained complexity; satisfiability problems; universal algebra,Algebra; C (programming language); Formal logic; Constraint-satisfaction problems; Fine grained; Fine-grained complexity; Low bound; Non-trivial; NP Complete; Satisfiability; Satisfiability problems; Strong exponential time hypothesis; Universal algebra; Constraint satisfaction problems
Variants of Homomorphism Polynomials Complete for Algebraic Complexity Classes,2021,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114371458&doi=10.1145%2f3470858&partnerID=40&md5=6df986343e5f6f49f675788df7671ef3,"We present polynomial families complete for the well-studied algebraic complexity classes VF, VBP, VP, and VNP. The polynomial families are based on the homomorphism polynomials studied in the recent works of Durand et al. (2014) and Mahajan et al. (2018). We consider three different variants of graph homomorphisms, namely injective homomorphisms, directed homomorphisms, and injective directed homomorphisms, and obtain polynomial families complete for VF, VBP, VP, and VNP under each one of these. The polynomial families have the following properties: • The polynomial families complete for VF, VBP, and VP are model independent, i.e., they do not use a particular instance of a formula, algebraic branching programs, or circuit for characterising VF, VBP, or VP, respectively. • All the polynomial families are hard under p-projections.  © 2021 Association for Computing Machinery.",Algebraic circuit complexity; directed homomorphism; injective and directed homomorphism; injective homomorphism,Computational complexity; Algebraic branching programs; Algebraic complexity; Graph homomorphisms; Model independent; Polynomials
Algorithms and Lower Bounds for De Morgan Formulas of Low-Communication Leaf Gates,2021,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114379335&doi=10.1145%2f3470861&partnerID=40&md5=e12849ed1c21bcf9ef3ffff0d915e82a,"The class FORMULA[s]oG consists of Boolean functions computable by size-s De Morgan formulas whose leaves are any Boolean functions from a class G. We give lower bounds and (SAT, Learning, and pseudorandom generators (PRGs)) algorithms for FORMULA[n1.99]oG, for classes G of functions with low communication complexity. Let R(k)G be the maximum k-party number-on-forehead randomized communication complexity of a function in G. Among other results, we show the following: •The Generalized Inner Product function GIPkn cannot be computed in FORMULA[s]oG on more than 1/2+ϵ fraction of inputs for 'Equation Presented'. This significantly extends the lower bounds against bipartite formulas obtained by [62]. As a corollary, we get an average-case lower bound for GIPkn against FORMULA[n1.99]oPTFk-1, i.e., sub-quadratic-size De Morgan formulas with degree-k-1) PTF (polynomial threshold function) gates at the bottom. Previously, it was open whether a super-linear lower bound holds for AND of PTFs. •There is a PRG of seed length n/2+O(√s·R(2)(G)·log (s/ϵ)·log (1/ϵ)) that ϵ-fools FORMULA[s]oG. For the special case of FORMULA[s]oLTF, i.e., size-s formulas with LTF (linear threshold function) gates at the bottom, we get the better seed length O(n1/2·s1/4·log (n)·log (n/ϵ)). In particular, this provides the first non-trivial PRG (with seed length o(n)) for intersections of n halfspaces in the regime where ϵ≤1/n, complementing a recent result of [45]. •There exists a randomized 2n-t #SAT algorithm for FORMULA[s]oG, where 'Equation Presented'. In particular, this implies a nontrivial #SAT algorithm for FORMULA[n1.99]oLTF. •The Minimum Circuit Size Problem is not in FORMULA[n1.99]oXOR; thereby making progress on hardness magnification, in connection with results from [14, 46]. On the algorithmic side, we show that the concept class FORMULA[n1.99]oXOR can be PAC-learned in time 2O(n/log n).  © 2021 Association for Computing Machinery.",circuit lower bounds; communication complexity; De Morgan formulas; learning; parities; pseudorandom generators (PRGs); satisfiability (SAT),Boolean functions; Computational complexity; Average case; Communication complexity; Concept class; Generalized inner products; Linear threshold functions; Minimum circuit size problem; Polynomial threshold functions; Pseudorandom generators; Computer circuits
On the Parameterized Approximability of Contraction to Classes of Chordal Graphs,2021,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114383853&doi=10.1145%2f3470869&partnerID=40&md5=d116ec769c4ac8cde60ffac268d51c5a,"A graph operation that contracts edges is one of the fundamental operations in the theory of graph minors. Parameterized Complexity of editing to a family of graphs by contracting k edges has recently gained substantial scientific attention, and several new results have been obtained. Some important families of graphs, namely, the subfamilies of chordal graphs, in the context of edge contractions, have proven to be significantly difficult than one might expect. In this article, we study the F-Contraction problem, where F is a subfamily of chordal graphs, in the realm of parameterized approximation. Formally, given a graph G and an integer k, F-Contraction asks whether there exists X ⊆ E(G) such that G/X ∈ F and |X| ≤ k. Here, G/X is the graph obtained from G by contracting edges in X. We obtain the following results for the F-Contraction problem: •Clique Contraction is known to be FPT. However, unless NP⊆ coNP/poly, it does not admit a polynomial kernel. We show that it admits a polynomial-size approximate kernelization scheme (PSAKS). That is, it admits a (1 + ϵ)-approximate kernel with O(kf(ϵ)) vertices for every ϵ > 0. •Split Contraction is known to be W[1]-Hard. We deconstruct this intractability result in two ways. First, we give a (2+ϵ)-approximate polynomial kernel for Split Contraction (which also implies a factor (2+ϵ)-FPT-approximation algorithm for Split Contraction). Furthermore, we show that, assuming Gap-ETH, there is no (5/4-δ)-FPT-approximation algorithm for Split Contraction. Here, ϵ, δ> 0 are fixed constants. •Chordal Contraction is known to be W[2]-Hard. We complement this result by observing that the existing W[2]-hardness reduction can be adapted to show that, assuming FPT ≠ W[1], there is no F(k)-FPT-approximation algorithm for Chordal Contraction. Here, F(k) is an arbitrary function depending on k alone. We say that an algorithm is an h(k)-FPT-approximation algorithm for the F-Contraction problem, if it runs in FPT time, and on any input (G, k) such that there exists X ⊆ E(G) satisfying G/X ∈ F and |X| ≤ k, it outputs an edge set Y of size at most h(k) · k for which G/Y is in F.  © 2021 Association for Computing Machinery.",FPT-approximation; Graph contraction; inapproximability; lossy kernels,Graph theory; Graphic methods; Parameterization; Polynomial approximation; Approximate polynomials; Arbitrary functions; Edge contractions; Fundamental operations; Graph operations; Parameterized complexity; Polynomial kernels; Polynomial size; Approximation algorithms
Fast Algorithms for General Spin Systems on Bipartite Expanders,2021,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114302534&doi=10.1145%2f3470865&partnerID=40&md5=182f75fe8fe5ba25f1d985eef388b5c3,"A spin system is a framework in which the vertices of a graph are assigned spins from a finite set. The interactions between neighbouring spins give rise to weights, so a spin assignment can also be viewed as a weighted graph homomorphism. The problem of approximating the partition function (the aggregate weight of spin assignments) or of sampling from the resulting probability distribution is typically intractable for general graphs. In this work, we consider arbitrary spin systems on bipartite expander Δ-regular graphs, including the canonical class of bipartite random Δ-regular graphs. We develop fast approximate sampling and counting algorithms for general spin systems whenever the degree and the spectral gap of the graph are sufficiently large. Roughly, this guarantees that the spin system is in the so-called low-temperature regime. Our approach generalises the techniques of Jenssen et al. and Chen et al. by showing that typical configurations on bipartite expanders correspond to ""bicliques""of the spin system; then, using suitable polymer models, we show how to sample such configurations and approximate the partition function in Õ(n2) time, where n is the size of the graph.  © 2021 Association for Computing Machinery.",approximate counting; Bipartite expanders; spin systems,Graph algorithms; Graph theory; Probability distributions; Temperature; Arbitrary spin; Fast algorithms; General graph; Low-temperature regime; Partition functions; Polymer models; Regular graphs; Weighted graph; Spin fluctuations
The Complexity of Promise SAT on Non-Boolean Domains,2021,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114289728&doi=10.1145%2f3470867&partnerID=40&md5=6d721d89485d39e994394b4d39d014bf,"While 3-SAT is NP-hard, 2-SAT is solvable in polynomial time. Austrin et al. [SICOMP'17] proved a result known as ""(2 + ϵ)-SAT is NP-hard.""They showed that the problem of distinguishing k-CNF formulas that are g-satisfiable (i.e., some assignment satisfies at least g literals in every clause) from those that are not even 1-satisfiable is NP-hard if g/k < 1/2 and is in P otherwise. We study a generalisation of SAT on arbitrary finite domains, with clauses that are disjunctions of unary constraints, and establish analogous behaviour. Thus, we give a dichotomy for a natural fragment of promise constraint satisfaction problems (PCSPs) on arbitrary finite domains. The hardness side is proved using the algebraic approach via a new general NP-hardness criterion on polymorphisms, which is based on a gap version of the Layered Label Cover problem. We show that previously used criteria are insufficient - the problem hence gives an interesting benchmark of algebraic techniques for proving hardness of approximation in problems such as PCSPs.  © 2021 Association for Computing Machinery.",algebraic approach; label cover; PCSP; polymorphisms; Promise constraint satisfaction,Hardness; NP-hard; Polynomial approximation; Algebraic approaches; Algebraic techniques; Boolean domain; Finite domains; Generalisation; Hardness of approximation; K-CNF formulas; Polynomial-time; Constraint satisfaction problems
Improved Bounds on Fourier Entropy and Min-entropy,2021,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114356171&doi=10.1145%2f3470860&partnerID=40&md5=41b72c5fc47991aef80ebb53840e0953,"Given a Boolean function f:{ -1,1}n → {-1,1}, define the Fourier distribution to be the distribution on subsets of [n], where each S ⊆ [n] is sampled with probability f (S)2. The Fourier Entropy-influence (FEI) conjecture of Friedgut and Kalai [28] seeks to relate two fundamental measures associated with the Fourier distribution: does there exist a universal constant C > 0 such that H(f2) ≤ C · Inf (f), where H(f2) is the Shannon entropy of the Fourier distribution of f and Inf(f) is the total influence of f? In this article, we present three new contributions toward the FEI conjecture: (1) Our first contribution shows that H(f2) ≤ 2 · aUC⊕(f), where a UC⊕(f) is the average unambiguous parity-certificate complexity of f. This improves upon several bounds shown by Chakraborty et al. [20]. We further improve this bound for unambiguous DNFs. We also discuss how our work makes Mansour's conjecture for DNFs a natural next step toward resolution of the FEI conjecture. (2) We next consider the weaker Fourier Min-entropy-influence (FMEI) conjecture posed by O'Donnell and others [50, 53], which asks if H∞ (f2) ≤ C · Inf(f), where H ∞ (f2) is the min-entropy of the Fourier distribution. We show H∞(f2) ≤ 2·Cmin⊕(f), where Cmin⊕(f) is the minimum parity-certificate complexity of f. We also show that for all ϵ≥0, we have H∞(f2) ≤ 2 log (||f||1,ϵ/(1-ϵ)), where ||f||1,ϵ is the approximate spectral norm of f. As a corollary, we verify the FMEI conjecture for the class of read-k DNFs (for constant k). (3) Our third contribution is to better understand implications of the FEI conjecture for the structure of polynomials that 1/3-approximate a Boolean function on the Boolean cube. We pose a conjecture: no flat polynomial(whose non-zero Fourier coefficients have the same magnitude) of degree d and sparsity 2ω(d) can 1/3-approximate a Boolean function. This conjecture is known to be true assuming FEI, and we prove the conjecture unconditionally (i.e., without assuming the FEI conjecture) for a class of polynomials. We discuss an intriguing connection between our conjecture and the constant for the Bohnenblust-Hille inequality, which has been extensively studied in functional analysis.  © 2021 Association for Computing Machinery.",approximate degree; certificate complexity; DNFs; entropy; FEI conjecture; Fourier analysis of Boolean functions; Mansour's conjecture; polynomial approximation; query complexity,Fourier analysis; Fourier transforms; Polynomials; Probability distributions; Fourier; Min-entropy; Non-zero Fourier coefficients; Shannon entropy; Spectral norms; Universal constants; Boolean functions
Sign-rank Can Increase under Intersection,2021,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114362835&doi=10.1145%2f3470863&partnerID=40&md5=a6025b1d2044cffa4f4633d70b9774b8,"The communication class UPPcc is a communication analog of the Turing Machine complexity class PP. It is characterized by a matrix-analytic complexity measure called sign-rank (also called dimension complexity), and is essentially the most powerful communication class against which we know how to prove lower bounds. For a communication problem f, let f Λ f denote the function that evaluates f on two disjoint inputs and outputs the AND of the results. We exhibit a communication problem f with UPPcc(f) = O(log n), and UPPcc(f Λ f) = θ (log2 n). This is the first result showing that UPP communication complexity can increase by more than a constant factor under intersection. We view this as a first step toward showing that UPPcc, the class of problems with polylogarithmic-cost UPP communication protocols, is not closed under intersection. Our result shows that the function class consisting of intersections of two majorities on n bits has dimension complexity nω(log n). This matches an upper bound of (Klivans, O'Donnell, and Servedio, FOCS 2002), who used it to give a quasipolynomial time algorithm for PAC learning intersections of polylogarithmically many majorities. Hence, fundamentally new techniques will be needed to learn this class of functions in polynomial time.  © 2021 Association for Computing Machinery.",communication complexity; dimension complexity; learning theory; Sign rank,Computational complexity; Polynomial approximation; Technology transfer; Communication complexity; Communication problems; Complexity class; Complexity measures; Constant factors; Matrix analytic; Polynomial-time; Quasi-polynomial time; Turing machines
On Computing Multilinear Polynomials Using Multi-r-ic Depth Four Circuits,2021,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122475777&doi=10.1145%2f3460952&partnerID=40&md5=56e385889536d050d99d3fd6e28fd8a3,"In this article, we are interested in understanding the complexity of computing multilinear polynomials using depth four circuits in which the polynomial computed at every node has a bound on the individual degree of r≥ 1 with respect to all its variables (referred to as multi-r-ic circuits). The goal of this study is to make progress towards proving superpolynomial lower bounds for general depth four circuits computing multilinear polynomials, by proving better bounds as the value of r increases. Recently, Kayal, Saha and Tavenas (Theory of Computing, 2018) showed that any depth four arithmetic circuit of bounded individual degree r computing an explicit multilinear polynomial on nO(1) variables and degree d must have size at least (n/r1.1)ω(ĝd/r). This bound, however, deteriorates as the value of r increases. It is a natural question to ask if we can prove a bound that does not deteriorate as the value of r increases, or a bound that holds for a larger regime of r. In this article, we prove a lower bound that does not deteriorate with increasing values of r, albeit for a specific instance of d = d(n) but for a wider range of r. Formally, for all large enough integers n and a small constant η•, we show that there exists an explicit polynomial on nO(1) variables and degree η (log2 n) such that any depth four circuit of bounded individual degree r ≤ nη• must have size at least exp(ω(log2 n)). This improvement is obtained by suitably adapting the complexity measure of Kayal et al. (Theory of Computing, 2018). This adaptation of the measure is inspired by the complexity measure used by Kayal et al. (SIAM J. Computing, 2017).  © 2021 ACM.",Arithmetic circuits; Depth four; Iterated Matrix Multiplication; Lower bounds; Multi-r-ic; Multilinear polynomials,Computer circuits; Electric network analysis; Integrated circuits; Matrix algebra; Polynomials; Timing circuits; Arithmetic circuit; Complexity measures; Depth four; Integer-N; Iterated matrix multiplication; Low bound; MAtrix multiplication; Multi-r-ic; Multilinear polynomials; Super-polynomials; Logic circuits
Can a Skywalker Localize the Midpoint of a Rope?,2021,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122464610&doi=10.1145%2f3460954&partnerID=40&md5=86fceb86f96ffd7bd5a9314df15282a8,"This article poses a question about a simple localization problem. The question is if an oblivious walker on a line segment can localize the midpoint of the line segment in a finite number of steps observing the direction (i.e., Left or Right) and the distance to the nearest end point. This problem arises from self-stabilizing location problems by autonomous mobile robots with limited visibility, which is an abstract model attracting a wide interest in distributed computing. Contrary to appearances, it is far from trivial whether this simple problem is solvable, and it is not settled yet. This article is concerned with three variants of the problem with a minimal relaxation and presents self-stabilizing algorithms for them. We also show an easy impossibility theorem for bilaterally symmetric algorithms.  © 2021 ACM.",Autonomous mobile robot; Choice axiom; Computable real; Self-stabilization; Small space algorithm,Computation theory; Mobile robots; Navigation; Autonomous Mobile Robot; Choice axiom; Computable reals; Finite number; Line-segments; Localization problems; Self stabilization; Simple++; Small space algorithm; Space algorithms; Abstracting
Fractal Intersections and Products via Algorithmic Dimension,2021,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122483319&doi=10.1145%2f3460948&partnerID=40&md5=caa35b18249b504703dfd501155e1340,"Algorithmic fractal dimensions quantify the algorithmic information density of individual points and may be defined in terms of Kolmogorov complexity. This work uses these dimensions to bound the classical Hausdorff and packing dimensions of intersections and Cartesian products of fractals in Euclidean spaces. This approach shows that two prominent, fundamental results about the dimension of Borel or analytic sets also hold for arbitrary sets.  © 2021 ACM.",Effective dimension; Fractal geometry; Kolmogorov complexity,Computational complexity; Parallel processing systems; Algorithmic informations; Algorithmics; Cartesian Products; Effective dimensions; Euclidean spaces; Fractal geometry; Hausdorff dimension; Information density; Kolmogorov complexity; Packing dimension; Fractal dimension
Complexity of Shift Bribery in Committee Elections,2021,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122485987&doi=10.1145%2f3470647&partnerID=40&md5=2d03e4abd31fc59df9445def8fd9f1ac,"Given an election, a preferred candidate p, and a budget, the SHIFT BRIBERY problem asks whether p can win the election after shifting p higher in some voters' preference orders. Of course, shifting comes at a price (depending on the voter and on the extent of the shift) and one must not exceed the given budget. We study the (parameterized) computational complexity of SHIFT BRIBERY for multiwinner voting rules where winning the election means to be part of some winning committee. We focus on the well-established SNTV, Bloc, k-Borda, and Chamberlin-Courant rules, as well as on approximate variants of the Chamberlin-Courant rule. We show that SHIFT BRIBERY tends to be harder in the multiwinner setting than in the single-winner one by showing settings where SHIFT BRIBERY is computationally easy in the single-winner cases, but is hard (and hard to approximate) in the multiwinner ones.  © 2021 ACM.",Approximation; Committee elections; Parameterized complexity; Shift-bribery,Approximation; Committee election; Parameterized; Parameterized complexity; Preference order; Shift-and; Shift-bribery; Voting rules; Budget control
Counting Homomorphisms to Trees Modulo a Prime,2021,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122437970&doi=10.1145%2f3460958&partnerID=40&md5=dfcfb43c8fdaebcc9bca7a59ef85cdd8,"Many important graph-theoretic notions can be encoded as counting graph homomorphism problems, such as partition functions in statistical physics, in particular independent sets and colourings. In this article, we study the complexity of #pHOMSTOH, the problem of counting graph homomorphisms from an input graph to a graph H modulo a prime number p. Dyer and Greenhill proved a dichotomy stating that the tractability of non-modular counting graph homomorphisms depends on the structure of the target graph. Many intractable cases in non-modular counting become tractable in modular counting due to the common phenomenon of cancellation. In subsequent studies on counting modulo 2, however, the influence of the structure of H on the tractability was shown to persist, which yields similar dichotomies. Our main result states that for every tree H and every prime p the problem #pHOMSTOH is either polynomial time computable or #pP-complete. This relates to the conjecture of Faben and Jerrum stating that this dichotomy holds for every graph H when counting modulo 2. In contrast to previous results on modular counting, the tractable cases of #pHOMSTOH are essentially the same for all values of the modulo when H is a tree. To prove this result, we study the structural properties of a homomorphism. As an important interim result, our study yields a dichotomy for the problem of counting weighted independent sets in a bipartite graph modulo some prime p. These results are the first suggesting that such dichotomies hold not only for the modulo 2 case but also for the modular counting functions of all primes p.  © 2021 Owner/Author.",Complexity dichotomy; Graph homomorphisms; Modular counting,Forestry; Statistical Physics; Trees (mathematics); Complexity dichotomies; Counting graphs; Graph homomorphisms; Graph theoretics; Independent set; Input graphs; Modular counting; Modulo 2; Partition functions; Prime number; Polynomial approximation
Multiplicative Parameterization above a Guarantee,2021,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122437979&doi=10.1145%2f3460956&partnerID=40&md5=1b48f2a3489440baa4f213e37073a76c,"Parameterization above a guarantee is a successful paradigm in Parameterized Complexity. To the best of our knowledge, all fixed-parameter tractable problems in this paradigm share an additive form defined as follows. Given an instance (I,k) of some (parameterized) problem πwith a guarantee g(I), decide whether I admits a solution of size at least (or at most) k + g(I). Here, g(I) is usually a lower bound on the minimum size of a solution. Since its introduction in 1999 for MAX SAT and MAX CUT (with g(I) being half the number of clauses and half the number of edges, respectively, in the input), analysis of parameterization above a guarantee has become a very active and fruitful topic of research. We highlight a multiplicative form of parameterization above (or, rather, times) a guarantee: Given an instance (I,k) of some (parameterized) problem πwith a guarantee g(I), decide whether I admits a solution of size at least (or at most) k · g(I). In particular, we study the Long Cycle problem with a multiplicative parameterization above the girth g(I) of the input graph, which is the most natural guarantee for this problem, and provide a fixed-parameter algorithm. Apart from being of independent interest, this exemplifies how parameterization above a multiplicative guarantee can arise naturally. We also show that, for any fixed constant ϵ > 0, multiplicative parameterization above g(I)1+ϵ of Long Cycle yields para-NP-hardness, thus our parameterization is tight in this sense. We complement our main result with the design (or refutation of the existence) of fixed-parameter algorithms as well as kernelization algorithms for additional problems parameterized multiplicatively above girth.  © 2021 ACM.",Above-guarantee; Girth; Long cycle; Multiplicative above-guarantee; Parameterized complexity,Graph theory; Parameter estimation; Above-guarantee; Fixed-parameter algorithms; Girth; Long cycles; Longest cycle problem; Low bound; MAX CUT; Multiplicative above-guarantee; Parameterized complexity; Parameterized problems; Parameterization
On the Sensitivity Complexity of k-Uniform Hypergraph Properties,2021,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108024375&doi=10.1145%2f3448643&partnerID=40&md5=e6efc4f1c4e1a5be9edafe143e0e849f,"In this article, we investigate the sensitivity complexity of hypergraph properties. We present a k-uniform hypergraph property with sensitivity complexity O(n(g k/3g ‰) for any k≥3, where n is the number of vertices. Moreover, we can do better when k1 (mod 3) by presenting a k-uniform hypergraph property with sensitivity O(ng k/3g ‰-1/2). This result disproves a conjecture of Babai, which conjectures that the sensitivity complexity of k-uniform hypergraph properties is at least ω (nk/2). We also investigate the sensitivity complexity of other symmetric functions and show that for many classes of transitive Boolean functions the minimum achievable sensitivity complexity can be O(N1/3), where N is the number of variables.  © 2021 ACM.",Boolean function; k-uniform hypergraph properties; Sensitivity complexity; Turán's question,Computation theory; Computer science; Hypergraph; Symmetric functions; Computational methods
Fine-Grained Reductions from Approximate Counting to Decision,2021,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108024706&doi=10.1145%2f3442352&partnerID=40&md5=652c9d46baf83204a8b4314d57d47159,"In this article, we introduce a general framework for fine-grained reductions of approximate counting problems to their decision versions. (Thus, we use an oracle that decides whether any witness exists to multiplicatively approximate the number of witnesses with minimal overhead.) This mirrors a foundational result of Sipser (STOC 1983) and Stockmeyer (SICOMP 1985) in the polynomial-time setting, and a similar result of Müller (IWPEC 2006) in the FPT setting. Using our framework, we obtain such reductions for some of the most important problems in fine-grained complexity: the Orthogonal Vectors problem, 3SUM, and the Negative-Weight Triangle problem (which is closely related to All-Pairs Shortest Path). While all these problems have simple algorithms over which it is conjectured that no polynomial improvement is possible, our reductions would remain interesting even if these conjectures were proved; they have only polylogarithmic overhead and can therefore be applied to subpolynomial improvements such as the n3/ exp(Θ (ĝ log n))-time algorithm for the Negative-Weight Triangle problem due to Williams (STOC 2014). Our framework is also general enough to apply to versions of the problems for which more efficient algorithms are known. For example, the Orthogonal Vectors problem over GF(m)d for constant m can be solved in time n · poly (d) by a result of Williams and Yu (SODA 2014); our result implies that we can approximately count the number of orthogonal pairs with essentially the same running time. We also provide a fine-grained reduction from approximate #SAT to SAT. Suppose the Strong Exponential Time Hypothesis (SETH) is false, so that for some 1 < c < 2 and all k there is an O(cn)-time algorithm for k-SAT. Then we prove that for all k, there is an O(( c + o(1))n)-time algorithm for approximate #k-SAT. In particular, our result implies that the Exponential Time Hypothesis (ETH) is equivalent to the seemingly weaker statement that there is no algorithm to approximate #3-SAT to within a factor of 1+I in time 2o(n)/ I2 (taking I > 0 as part of the input).  © 2021 ACM.",approximate counting; Fine-grained complexity; satisfiability,Computation theory; Computational methods; Computer science; All pairs shortest paths; Approximate counting; Decision version; Exponential time hypothesis; Orthogonal vectors; Polynomial-time; SIMPLE algorithm; Strong exponential time hypothesis; Polynomial approximation
Computation of Hadwiger Number and Related Contraction Problems: Tight Lower Bounds,2021,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108030716&doi=10.1145%2f3448639&partnerID=40&md5=af8fdbe4fdf4fb45e8d1fe91e2d2b151,"We prove that the Hadwiger number of an n-vertex graph G (the maximum size of a clique minor in G) cannot be computed in time no(n), unless the Exponential Time Hypothesis (ETH) fails. This resolves a well-known open question in the area of exact exponential algorithms. The technique developed for resolving the Hadwiger number problem has a wider applicability. We use it to rule out the existence of no(n)-time algorithms (up to the ETH) for a large class of computational problems concerning edge contractions in graphs.  © 2021 ACM.",edge contraction problems; exact algorithms; exponential-time hypothesis; Hadwiger number,Graph algorithms; Computational problem; Edge contractions; Exact exponential algorithms; Exponential time hypothesis; Hadwiger number; Lower bounds; N-vertex graph; Time algorithms; Graph theory
"On a Theorem of Lovász that (., H) Determines the Isomorphism Type of H",2021,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108028360&doi=10.1145%2f3448641&partnerID=40&md5=d8372257f667a68fdc10381a3bb5d1b4,"Graph homomorphism has been an important research topic since its introduction [20]. Stated in the language of binary relational structures in that paper [20], Lovász proved a fundamental theorem that, for a graph H given by its 0-1 valued adjacency matrix, the graph homomorphism function G → hom(G, H) determines the isomorphism type of H. In the past 50 years, various extensions have been proved by many researchers [1, 15, 21, 24, 26]. These extend the basic 0-1 case to admit vertex and edge weights; but these extensions all have some restrictions such as all vertex weights must be positive. In this article, we prove a general form of this theorem where H can have arbitrary vertex and edge weights. A noteworthy aspect is that we prove this by a surprisingly simple and unified argument. This bypasses various technical obstacles and unifies and extends all previous known versions of this theorem on graphs. The constructive proof of our theorem can be used to make various complexity dichotomy theorems for graph homomorphism effective in the following sense: it provides an algorithm that for any H either outputs a P-time algorithm solving hom(s, H) or a P-time reduction from a canonical #P-hard problem to hom(s, H).  © 2021 ACM.",and tensors; complexity dichotomy; connection matrices; Graph homomorphism; partition function,Graph algorithms; Set theory; Adjacency matrices; Arbitrary vertices; Complexity dichotomies; Constructive proof; Fundamental theorems; Graph homomorphisms; Isomorphism type; Relational structures; Graph theory
The Complexity of Approximating the Matching Polynomial in the Complex Plane,2021,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108028494&doi=10.1145%2f3448645&partnerID=40&md5=ec2253131b71f0f9fb7aeec104ca1a7b,"We study the problem of approximating the value of the matching polynomial on graphs with edge parameter γ, where γ takes arbitrary values in the complex plane. When γ is a positive real, Jerrum and Sinclair showed that the problem admits an FPRAS on general graphs. For general complex values of γ, Patel and Regts, building on methods developed by Barvinok, showed that the problem admits an FPTAS on graphs of maximum degree Δ as long as γ is not a negative real number less than or equal to -1/(4(Δ -1)). Our first main result completes the picture for the approximability of the matching polynomial on bounded degree graphs. We show that for all Δ ≥ 3 and all real γ less than -1/(4(Δ -1)), the problem of approximating the value of the matching polynomial on graphs of maximum degree Δ with edge parameter γ is #P-hard. We then explore whether the maximum degree parameter can be replaced by the connective constant. Sinclair et al. showed that for positive real γ, it is possible to approximate the value of the matching polynomial using a correlation decay algorithm on graphs with bounded connective constant (and potentially unbounded maximum degree). We first show that this result does not extend in general in the complex plane; in particular, the problem is #P-hard on graphs with bounded connective constant for a dense set of γ values on the negative real axis. Nevertheless, we show that the result does extend for any complex value γ that does not lie on the negative real axis. Our analysis accounts for complex values of γ using geodesic distances in the complex plane in the metric defined by an appropriate density function.  © 2021 ACM.",Approximate counting; connective constant; matchings; monomer-dimer model; partition functions,Graph algorithms; Graphic methods; Polynomials; Approximability; Arbitrary values; Bounded degree graphs; Complex planes; Complex values; Geodesic distances; Matching polynomial; Negative real numbers; Graph structures
Popular Matching in Roommates Setting Is NP-hard,2021,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107858584&doi=10.1145%2f3442354&partnerID=40&md5=fc63318d177d64c55519b548517d6623,"An input to the POPULAR MATCHING problem, in the roommates setting (as opposed to the marriage setting), consists of a graph G (not necessarily bipartite) where each vertex ranks its neighbors in strict order, known as its preference. In the POPULAR MATCHING problem the objective is to test whether there exists a matching M∗such that there is no matching M where more vertices prefer their matched status in M (in terms of their preferences) over their matched status in M∗. In this article, we settle the computational complexity of the POPULAR MATCHING problem in the roommates setting by showing that the problem is NP-complete. Thus, we resolve an open question that has been repeatedly and explicitly asked over the last decade.  © 2021 ACM.",NP-hard; Popular matching,NP-hard; Graph G; NP Complete; Popular matching; Popular matching problems; Strict orderings; Graph theory
All Classical Adversary Methods Are Equivalent for Total Functions,2021,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102980069&doi=10.1145%2f3442357&partnerID=40&md5=75c8c375313874d1a6a10ae12d69225a,"We show that all known classical adversary lower bounds on randomized query complexity are equivalent for total functions and are equal to the fractional block sensitivity fbs(f). That includes the Kolmogorov complexity bound of Laplante and Magniez and the earlier relational adversary bound of Aaronson. This equivalence also implies that for total functions, the relational adversary is equivalent to a simpler lower bound, which we call rank-1 relational adversary. For partial functions, we show unbounded separations between fbs(f) and other adversary bounds, as well as between the adversary bounds themselves. We also show that, for partial functions, fractional block sensitivity cannot give lower bounds larger than n ⋅ bs(f), where n is the number of variables and bs(f) is the block sensitivity. Then, we exhibit a partial function f that matches this upper bound, fbs(f) = Ω (n ⋅ bs(f)). © 2021 ACM.",adversary bounds; fractional block sensitivity; lower bounds; Randomized query complexity,Computational complexity; Block sensitivity; Kolmogorov complexity; Lower bounds; Partial functions; Query complexity; Upper Bound; Functions
Abstract Geometrical Computation 10: An Intrinsically Universal Family of Signal Machines,2021,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102983056&doi=10.1145%2f3442359&partnerID=40&md5=132faaa829bce84352e6f83be480ddde,"Signal machines form an abstract and idealized model of collision computing. Based on dimensionless signals moving on the real line, they model particle/signal dynamics in Cellular Automata. Each particle, or signal, moves at constant speed in continuous time and space. When signals meet, they get replaced by other signals. A signal machine defines the types of available signals, their speeds, and the rules for replacement in collision. A signal machine A simulates another one B if all the space-time diagrams of B can be generated from space-time diagrams of A by removing some signals and renaming other signals according to local information. Given any finite set of speeds S we construct a signal machine that is able to simulate any signal machine whose speeds belong to S. Each signal is simulated by a macro-signal, a ray of parallel signals. Each macro-signal has a main signal located exactly where the simulated signal would be, as well as auxiliary signals that encode its id and the collision rules of the simulated machine. The simulation of a collision, a macro-collision, consists of two phases. In the first phase, macro-signals are shrunk, and then the macro-signals involved in the collision are identified and it is ensured that no other macro-signal comes too close. If some do, the process is aborted and the macro-signals are shrunk, so that the correct macro-collision will eventually be restarted and successfully initiated. Otherwise, the second phase starts: The appropriate collision rule is found and new macro-signals are generated accordingly. Considering all finite sets of speeds S and their corresponding simulators provides an intrinsically universal family of signal machines. © 2021 ACM.",Abstract geometrical computation; collision computing; intrinsic universality; signal machine; simulation,Cellular automata; Continuous time systems; Set theory; Abstract geometrical computation; Auxiliary signals; Continuous-time; Idealized models; Local information; Signal machines; Simulated signals; Space-time diagrams; Speed
AC0 Unpredictability,2021,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102982998&doi=10.1145%2f3442362&partnerID=40&md5=8469e6db93214cf1f3205e0e3420eba5,"We prove that for every distribution D on n bits with Shannon entropy ≥ n- A , at most O(2da logd+1g)/γ5 of the bits Di can be predicted with advantage γ by an AC0 circuit of size g and depth D that is a function of all of the bits of D except Di. This answers a question by Meir and Wigderson, who proved a corresponding result for decision trees. We also show that there are distributions D with entropy ≥ n-O(1) such that any subset of O(n/log n) bits of D on can be distinguished from uniform by a circuit of depth 2 and size poly(n). This separates the notions of predictability and distinguishability in this context. © 2021 ACM.",AC0; distinguishability; Entropy; predictability,Computation theory; Computational methods; Computer science; Distinguishability; Shannon entropy; Decision trees
Computational and Proof Complexity of Partial String Avoidability,2021,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102980148&doi=10.1145%2f3442365&partnerID=40&md5=1b0175395b100e2a4d7d647d3d002c91,"The partial string avoidability problem is stated as follows: Given a finite set of strings with possible ""holes""(wildcard symbols), determine whether there exists a two-sided infinite string containing no substrings from this set, assuming that a hole matches every symbol. The problem is known to be NP-hard and in PSPACE, and this article establishes its PSPACE-completeness. Next, string avoidability over the binary alphabet is interpreted as a version of conjunctive normal form satisfiability problem, where each clause has infinitely many shifted variants. Non-satisfiability of these formulas can be proved using variants of classical propositional proof systems, augmented with derivation rules for shifting proof lines (such as clauses, inequalities, polynomials, etc.). First, it is proved that there is a particular formula that has a short refutation in Resolution with a shift rule but requires classical proofs of exponential size. At the same time, it is shown that exponential lower bounds for classical proof systems can be translated for their shifted versions. Finally, it is shown that superpolynomial lower bounds on the size of shifted proofs would separate NP from PSPACE; a connection to lower bounds on circuit complexity is also established. © 2021 ACM.",avoidability; lower bound; Partial strings; partial words; proof complexity; PSPACE-completeness,NP-hard; Circuit complexity; Conjunctive normal forms; Derivation rules; Infinite strings; Proof complexity; Propositional proof systems; Pspace completeness; Satisfiability problems; Formal logic
Polynomial-Time Random Oracles and Separating Complexity Classes,2021,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102982977&doi=10.1145%2f3434389&partnerID=40&md5=05bd18050e4dafe7acaecd5ff329583d,"Bennett and Gill [1981] showed that PA ≠ NPA ≠ coNPA for a random oracle A, with probability 1. We investigate whether this result extends to individual polynomial-time random oracles. We consider two notions of random oracles: P-random oracles in the sense of martingales and resource-bounded measure [Lutz 1992; Ambos-Spies et al. 1997], and p-betting-game random oracles using the betting games generalization of resource-bounded measure [Buhrman et al. 2000]. Every p-betting-game random oracle is also p-random; whether the two notions are equivalent is an open problem. (1) We first show that PA ≠ NPA for every oracle A that is p-betting-game random. Ideally, we would extend (1) to p-random oracles. We show that answering this either way would imply an unrelativized complexity class separation: (2) If PA ≠ NPA relative to every p-random oracle A, then BPP ≠ EXP. (3) If PA ≠ NPA relative to some p-random oracle A, then P ≠ PSPACE. Rossman, Servedio, and Tan [2015] showed that the polynomial-time hierarchy is infinite relative to a random oracle, solving a longstanding open problem. We consider whether we can extend (1) to show that PHA is infinite relative to oracles A that are p-betting-game random. Showing that PHA separates at even its first level would also imply an unrelativized complexity class separation: (4) If NPA ≠ coNPA for a p-betting-game measure 1 class of oracles A, then NP ≠ EXP. (5) If PHA is infinite relative to every p-random oracle A, then PH ≠ EXP. We also consider random oracles for time versus space, for example: (6) LA ≠ PA relative to every oracle A that is p-betting-game random. © 2021 ACM.",betting games; Random oracles; resource-bounded measure,Computational complexity; Polynomial approximation; Complexity class; Polynomial-time; Polynomial-time hierarchy; Random Oracle; Resource-bounded measure; Separation
Lower Bounding the AND-OR Tree via Symmetrization,2021,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102982306&doi=10.1145%2f3434385&partnerID=40&md5=95310a3519864b6d0ad352298ff209ff,"We prove a simple, nearly tight lower bound on the approximate degree of the two-level AND-OR tree using symmetrization arguments. Specifically, we show that deg(ANDm ORn) = (mn). We prove this lower bound via reduction to the OR function through a series of symmetrization steps, in contrast to most other proofs that involve formulating approximate degree as a linear program [6, 10, 21]. Our proof also demonstrates the power of a symmetrization technique involving Laurent polynomials (polynomials with negative exponents) that was previously introduced by Aaronson et al. [2]. © 2021 ACM.",AND-OR tree; approximate degree; Laurent polynomials; polynomial approximation,Linear programming; And-or tree; Laurent polynomial; Linear programs; Lower bounds; Negative exponents; Forestry
Fine-Grained Time Complexity of Constraint Satisfaction Problems,2021,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102980938&doi=10.1145%2f3434387&partnerID=40&md5=7942e899bb3ce0155d2c97a11b312a34,"We study the constraint satisfaction problem (CSP) parameterized by a constraint language Γ (CSPΓ) and how the choice of Γ affects its worst-case time complexity. Under the exponential-time hypothesis (ETH), we rule out the existence of subexponential algorithms for finite-domain NP-complete CSPΓ problems. This extends to certain infinite-domain CSPs and structurally restricted problems. For CSPs with finite domain D and where all unary relations are available, we identify a relation SD such that the time complexity of the NP-complete problem CSP({SD}) is a lower bound for all NP-complete CSPs of this kind. We also prove that the time complexity of CSP({SD}) strictly decreases when |D| increases (unless the ETH is false) and provide stronger complexity results in the special case when |D|=3. © 2021 ACM.",Constraint satisfaction problems; lower bounds; time complexity; universal algebra; upper bounds,NP-hard; Polynomials; Complexity results; Constraint language; Exponential time hypothesis; Infinite domains; Restricted problem; Sub-exponential algorithms; Time complexity; Unary relations; Constraint satisfaction problems
On the power of amortization in secret sharing: D-Uniform secret sharing and CDS with constant information rate,2020,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097256441&doi=10.1145%2f3417756&partnerID=40&md5=978c6bda6f2888a1b1c0b92f05b7c7cf,"Consider the following secret-sharing problem: A file s should be distributed between n servers such that (d-1)-subsets cannot recover the file, (d+1)-subsets can recover the file, and d-subsets should be able to recover s if and only if they appear in some pre-defined list L. The goal is to minimize the information ratio - that is, the number of bits stored on a server per each bit of the secret. We show that for any constant d and any pre-defined list L, if the file is sufficiently long (exponential in nd), the problem can be solved with a constant asymptotic information ratio of cd that does not grow with the number of servers n. This result is based on a new construction of d-party conditional disclosure of secrets for arbitrary predicates over an n-size domain in which each party communicates at most four bits per secret bit. In both settings, previous results achieved a non-constant information ratio that grows asymptotically with n, even for the simpler special case of d = 2. Moreover, our constructions yield the first example of an access structure whose amortized information ratio is constant, whereas its best-known non-amortized information ratio is sub-exponential, thus providing a unique evidence for the potential power of amortization in the context of secret sharing. Our main result applies to exponentially long secrets, and so it should be mainly viewed as a barrier against amortizable lower-bound techniques. We also show that in some natural simple cases (e.g., low-degree predicates), amortization kicks in even for quasi-polynomially long secrets. Finally, we prove some limited lower bounds and point out some limitations of existing lower-bound techniques. © 2020 ACM.",conditional disclosure of secrets; Secret sharing,Recovery; Access structure; Conditional disclosure of secrets; Information rates; Lower bound techniques; Lower bounds; New constructions; Potential power; Secret sharing; Depreciation
A ZPPNP[1]lifting theorem,2020,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097243403&doi=10.1145%2f3428673&partnerID=40&md5=bdc8a5d3b75106398c0125997286d658,"The complexity class ZPPNP[1] (corresponding to zero-error randomized algorithms with access to one NP oracle query) is known to have a number of curious properties. We further explore this class in the settings of time complexity, query complexity, and communication complexity. • For starters, we provide a new characterization: ZPPNP[1] equals the restriction of BPPNP[1] where the algorithm is only allowed to err when it forgoes the opportunity to make an NP oracle query. • Using the above characterization, we prove a query-to-communication lifting theorem, which translates any ZPPNP[1] decision tree lower bound for a function f into a ZPPNP[1] communication lower bound for a two-party version of f. • As an application, we use the above lifting theorem to prove that the ZPPNP[1] communication lower bound technique introduced by Göös, Pitassi, and Watson (ICALP 2016) is not tight. We also provide a ""primal""characterization of this lower bound technique as a complexity class. © 2020 ACM.",Communication complexity; query complexity,Decision trees; Communication complexity; Complexity class; Lower bound techniques; Lower bounds; Query complexity; Randomized Algorithms; Time complexity; Zero errors; Computational complexity
The subgraph testing model,2020,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097251460&doi=10.1145%2f3428675&partnerID=40&md5=e02eda6f6df6cb02663fe755b93b7b9e,"Following Newman (2010), we initiate a study of testing properties of graphs that are presented as subgraphs of a fixed (or an explicitly given) graph. The tester is given free access to a base graph G = ([n], E) and oracle access to a function f : E → {0, 1} that represents a subgraph of G. The tester is required to distinguish between subgraphs that possess a predetermined property and subgraphs that are far from possessing this property. We focus on bounded-degree base graphs and on the relation between testing graph properties in the subgraph model and testing the same properties in the bounded-degree graph model. We identify cases in which testing is significantly easier in one model than in the other as well as cases in which testing has approximately the same complexity in both models. Our proofs are based on the design and analysis of efficient testers and on the establishment of query-complexity lower bounds. © 2020 ACM.",graph properties; Property testing,Graph theory; Bounded degree; Bounded degree graphs; Design and analysis; Graph properties; Lower bounds; Oracle access; Query complexity; Testing modeling; Well testing
Decoding variants of reed-muller codes over finite grids,2020,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097244590&doi=10.1145%2f3417754&partnerID=40&md5=86a7c709a262796eb7f6702de3f4a9cf,"In a recent article, Kim and Kopparty (2017) gave a deterministic algorithm for the unique decoding problem for polynomials of bounded total degree over a general grid S1 × ... × Sm. We show that their algorithm can be adapted to solve the unique decoding problem for the general family of Downset codes. Here, a downset code is specified by a family D of monomials closed under taking factors: The corresponding code is the space of evaluations of all polynomials that can be written as linear combinations of monomials from D. © 2020 ACM.",Downset; downset code; minimum distance; Reed-Muller code; Reed-Solomon code; unique decoding; weighted distance; weighted function,Decoding; Deterministic algorithms; Finite grid; Linear combinations; Reed-Muller codes; Forward error correction
On existential MSO and its relation to ETH,2020,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097213702&doi=10.1145%2f3417759&partnerID=40&md5=d4fb30eee138657ae14abf60f2dba6db,"Impagliazzo et al. proposed a framework, based on the logic fragment defining the complexity class SNP, to identify problems that are equivalent to k-CNF-Sat modulo subexponential-time reducibility (serf-reducibility). The subexponential-time solvability of any of these problems implies the failure of the Exponential Time Hypothesis (ETH). In this article, we extend the framework of Impagliazzo et al. and identify a larger set of problems that are equivalent to k-CNF-Sat modulo serf-reducibility. We propose a complexity class, referred to as Linear Monadic NP, that consists of all problems expressible in existential monadic second-order logic whose expressions have a linear measure in terms of a complexity parameter, which is usually the universe size of the problem. This research direction can be traced back to Fagin's celebrated theorem stating that NP coincides with the class of problems expressible in existential second-order logic. Monadic NP, a well-studied class in the literature, is the restriction of the aforementioned logic fragment to existential monadic second-order logic. The proposed class Linear Monadic NP is then the restriction of Monadic NP to problems whose expressions have linear measure in the complexity parameter. We show that Linear Monadic NP includes many natural complete problems such as the satisfiability of linear-size circuits, dominating set, independent dominating set, and perfect code. Therefore, for any of these problems, its subexponential-time solvability is equivalent to the failure of ETH. We prove, using logic games, that the aforementioned problems are inexpressible in the monadic fragment of SNP, and hence, are not captured by the framework of Impagliazzo et al. Finally, we show that Feedback Vertex Set is inexpressible in existential monadic second-order logic, and hence is not in Linear Monadic NP, and investigate the existence of certain reductions between Feedback Vertex Set (and variants of it) and 3-CNF-Sat. © 2020 ACM.",Exponential time hypothesis (ETH); logic games; monadic second-order logic; serf-reducibility; subexponential time complexity,Computational complexity; Equivalence classes; Complete problems; Complexity class; Existential second-order logic; Exponential time hypothesis; Feedback vertex set; Independent dominating set; Monadic second-order logic; Subexponential time; Computer circuits
Inner product and set disjointness: Beyond logarithmically many parties,2020,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097229672&doi=10.1145%2f3428671&partnerID=40&md5=8644f9ed244472caf568228b32f54723,"A major goal in complexity theory is to understand the communication complexity of number-on-the-forehead problems f:({0, 1} n)k → {0, 1} with k > log n parties. We study the problems of inner product and set disjointness and determine their randomized communication complexity for every k ≥ log n, showing in both cases that Θ(1 + ⌈log n⌉/ log ⌈1 + k/ log n⌉) bits are necessary and sufficient. In particular, these problems admit constant-cost protocols if and only if the number of parties is k ≥ nϵ for some constant ϵ > 0. © 2020 ACM.",Communication complexity; inner product; number on the forehead; set disjointness,Computation theory; Computational methods; Computer science; Communication complexity; Complexity theory; Disjointness; Inner product; Computational complexity
Strongly exponential separation between monotone VP and monotone VNP,2020,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097230988&doi=10.1145%2f3417758&partnerID=40&md5=10af6ba5be109bbb5f10d970b2942127,"We show that there is a sequence of explicit multilinear polynomials Pn (x1, ⋯ ,xn) μ R [x1, ⋯ ,xn] with non-negative coefficients that lies in monotone VNP such that any monotone algebraic circuit for Pn must have size exp (ω (n)) This builds on (and strengthens) a result of Yehudayoff (STOC 2019) who showed a lower bound of exp (ω(n√)). © 2020 ACM.",Algebraic circuits; expander graphs; monotone circuits; multipartition communication complexity,Computation theory; Computer science; Algebraic circuits; Lower bounds; Multilinear polynomials; Non negatives; Computational methods
Complexity of Unordered CNF Games,2020,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089474054&doi=10.1145%2f3397478&partnerID=40&md5=87b471f132a4fcc0c4569799d0ca341b,"The classic TQBF problem is to determine who has a winning strategy in a game played on a given conjunctive normal form formula (CNF), where the two players alternate turns picking truth values for the variables in a given order, and the winner is determined by whether the CNF gets satisfied. We study variants of this game in which the variables may be played in any order, and each turn consists of picking a remaining variable and a truth value for it. For the version where the set of variables is partitioned into two halves and each player may only pick variables from his or her half, we prove that the problem is PSPACE-complete for 5-CNFs and in P for 2-CNFs. Previously, it was known to be PSPACE-complete for unbounded-width CNFs (Schaefer, STOC 1976). For the general unordered version (where each variable can be picked by either player), we also prove that the problem is PSPACE-complete for 5-CNFs and in P for 2-CNFs. Previously, it was known to be PSPACE-complete for 6-CNFs (Ahlroth and Orponen, MFCS 2012) and PSPACE-complete for positive 11-CNFs (Schaefer, STOC 1976).  © 2020 ACM.",CNF; games; linear time; PSPACE-complete,Computation theory; Computer science; Conjunctive normal forms; PSPACE-complete; Truth values; Winning strategy; Computational methods
Coin Flipping in Dynamic Programming is Almost Useless,2020,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089540061&doi=10.1145%2f3397476&partnerID=40&md5=28968822efd5e9b9a6eedf08bc637f2a,"We consider probabilistic circuits working over the real numbers and using arbitrary semialgebraic functions of bounded description complexity as gates. In particular, such circuits can use all arithmetic operations (+, -, ×, ÷), optimization operations (min and max), conditional branching (if-then-else), and many more. We show that probabilistic circuits using any of these operations as gates can be simulated by deterministic circuits with only about a quadratical blowup in size. A slightly larger blowup in circuit size is also shown when derandomizing approximating circuits. The algorithmic consequence, motivating the title, is that randomness cannot substantially speed up dynamic programming algorithms.  © 2020 ACM.",Derandomization; dynamic programming; semialgebraic functions; sign patterns of polynomials,Computation theory; Computational methods; Computer science; Arithmetic operations; Circuit size; Derandomizing; Description complexity; Dynamic programming algorithm; Optimization operation; Real number; Semi-algebraic function; Dynamic programming
Testing Linearity against Non-signaling Strategies,2020,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089477065&doi=10.1145%2f3397474&partnerID=40&md5=85850d058271fdae78b88e800fdf7ba3,"Non-signaling strategies are collections of distributions with certain non-local correlations. They have been studied in physics as a strict generalization of quantum strategies to understand the power and limitations of nature's apparent non-locality. Recently, they have received attention in theoretical computer science due to connections to Complexity and Cryptography. We initiate the study of Property Testing against non-signaling strategies, focusing first on the classical problem of linearity testing (Blum, Luby, and Rubinfeld; JCSS 1993). We prove that any non-signaling strategy that passes the linearity test with high probability must be close to a quasi-distribution over linear functions. Quasi-distributions generalize the notion of probability distributions over global objects (such as functions) by allowing negative probabilities, while at the same time requiring that ""local views""follow standard distributions (with non-negative probabilities). Quasi-distributions arise naturally in the study of quantum mechanics as a tool to describe various non-local phenomena. Our analysis of the linearity test relies on Fourier analytic techniques applied to quasi-distributions. Along the way, we also establish general equivalences between non-signaling strategies and quasi-distributions, which we believe will provide a useful perspective on the study of Property Testing against non-signaling strategies beyond linearity testing.  © 2020 ACM.",linearity testing; non-signaling strategies; Property testing; quasi-distributions,Quantum theory; Signaling; Analytic technique; Classical problems; High probability; Linearity testing; Non-local phenomena; Property-testing; Standard distributions; Theoretical computer science; Probability distributions
Circuit Lower Bounds for MCSP from Local Pseudorandom Generators,2020,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089501300&doi=10.1145%2f3404860&partnerID=40&md5=ae27e9ac514b56fc7c92fd93dcef0f59,"The Minimum Circuit Size Problem (MCSP) asks if a given truth table of a Boolean function f can be computed by a Boolean circuit of size at most θ, for a given parameter θ. We improve several circuit lower bounds for MCSP, using pseudorandom generators (PRGs) that are local; a PRG is called local if its output bit strings, when viewed as the truth table of a Boolean function, can be computed by a Boolean circuit of small size. We get new and improved lower bounds for MCSP that almost match the best-known lower bounds against several circuit models. Specifically, we show that computing MCSP, on functions with a truth table of length N, requires •N3-o(1)-size de Morgan formulas, improving the recent N2-o(1) lower bound by Hirahara and Santhanam (CCC, 2017), •N2-o(1)-size formulas over an arbitrary basis or general branching programs (no non-trivial lower bound was known for MCSP against these models), and •2ω(N1/(d+1.01))-size depth-d AC0 circuits, improving the (implicit, in their work) exponential size lower bound by Allender et al. (SICOMP, 2006). The AC0 lower bound stated above matches the best-known AC0 lower bound (for PARITY) up to a small additive constant in the depth. Also, for the special case of depth-2 circuits (i.e., CNFs or DNFs), we get an optimal lower bound of 2ω(N) for MCSP.  © 2020 ACM.",branching programs; circuit lower bounds; constant-depth circuits; de Morgan formulas; local PRGs; Minimum circuit size problem (MCSP); pseudorandom generators (PRGs),Logic circuits; Timing circuits; Boolean circuit; Branching programs; Circuit lower bounds; Circuit models; Lower bounds; Minimum circuit size problem; Optimal lower bound; Pseudorandom generators; Boolean functions
A Lower Bound for Sampling Disjoint Sets,2020,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089504939&doi=10.1145%2f3404858&partnerID=40&md5=50325cc63c0ced3231d19e5560dcb7b1,"Suppose Alice and Bob each start with private randomness and no other input, and they wish to engage in a protocol in which Alice ends up with a set xĝ† [n] and Bob ends up with a set yĝ† [n], such that (x,y) is uniformly distributed over all pairs of disjoint sets. We prove that for some constant β < 1, this requires ω (n) communication even to get within statistical distance 1- βn of the target distribution. Previously, Ambainis, Schulman, Ta-Shma, Vazirani, and Wigderson (FOCS 1998) proved that ω ( n) communication is required to get within some constant statistical distance I > 0 of the uniform distribution over all pairs of disjoint sets of size n.  © 2020 ACM.",Communication complexity; sampling; set disjointness,Computation theory; Computational methods; Computer science; Disjoint sets; Lower bounds; Schulman; Statistical distance; Uniform distribution; Tantalum compounds
The Complexity of Approximately Counting Retractions,2020,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089519802&doi=10.1145%2f3397472&partnerID=40&md5=12272b41f1089741c58d2851cb335ab3,"Let G be a graph that contains an induced subgraph H. A retraction from G to H is a homomorphism from G to H that is the identity function on H. Retractions are very well studied: Given H, the complexity of deciding whether there is a retraction from an input graph G to H is completely classified, in the sense that it is known for which H this problem is tractable (assuming P ≠ NP). Similarly, the complexity of (exactly) counting retractions from G to H is classified (assuming FP ≠ #P). However, almost nothing is known about approximately counting retractions. Our first contribution is to give a complete trichotomy for approximately counting retractions to graphs without short cycles. The result is as follows: (1) Approximately counting retractions to a graph H of girth at least 5 is in FP if every connected component of H is a star, a single looped vertex, or an edge with two loops. (2) Otherwise, if every component is an irreflexive caterpillar or a partially bristled reflexive path, then approximately counting retractions to H is equivalent to approximately counting the independent sets of a bipartite graph - a problem that is complete in the approximate counting complexity class RH Π 1. (3) Finally, if none of these hold, then approximately counting retractions to H is equivalent to approximately counting the satisfying assignments of a Boolean formula. Our second contribution is to locate the retraction counting problem for each H in the complexity landscape of related approximate counting problems. Interestingly, our results are in contrast to the situation in the exact counting context. We show that the problem of approximately counting retractions is separated both from the problem of approximately counting homomorphisms and from the problem of approximately counting list homomorphisms - whereas for exact counting all three of these problems are interreducible. We also show that the number of retractions is at least as hard to approximate as both the number of surjective homomorphisms and the number of compactions. In contrast, exactly counting compactions is the hardest of all of these exact counting problems.  © 2020 ACM.",Approximate counting; counting complexity; graph compactions; graph homomorphisms; retractions; surjective homomorphisms,Boolean algebra; Compaction; Equivalence classes; Graph algorithms; Approximate counting; Bipartite graphs; Connected component; Counting problems; Identity functions; Induced subgraphs; List-homomorphisms; Satisfying assignments; Graph theory
Tight Complexity Lower Bounds for Integer Linear Programming with Few Constraints,2020,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088097305&doi=10.1145%2f3397484&partnerID=40&md5=52b0641c2d4991dac81328e058f9d62f,"We consider the standard ILP Feasibility problem: given an integer linear program of the form {Ax = b, x ≥ 0}, where A is an integer matrix with k rows and ℓ columns, x is a vector of ℓ variables, and b is a vector of k integers, we ask whether there exists x ∈ N ℓ that satisfies Ax = b. Each row of A specifies one linear constraint on x; our goal is to study the complexity of ILP Feasibility when both k, the number of constraints, and A∞, the largest absolute value of an entry in A, are small. Papadimitriou was the first to give a fixed-parameter algorithm for ILP Feasibility under parameterization by the number of constraints that runs in time ((A∞ + b∞) ⋅ k)O(k2). This was very recently improved by Eisenbrand and Weismantel, who used the Steinitz lemma to design an algorithm with running time (kA∞)O(k) ⋅ log b∞, which was subsequently refined by Jansen and Rohwedder to O( kA∞)k ⋅ log ( A∞ + b∞) ⋅ log A∞. We prove that for {0, 1}-matrices A, the running time of the algorithm of Eisenbrand and Weismantel is probably optimal: an algorithm with running time 2o(k log k) ⋅ (ℓ + b∞)o(k) would contradict the exponential time hypothesis. This improves previous non-tight lower bounds of Fomin et al. We then consider integer linear programs that may have many constraints, but they need to be structured in a ""shallow""way. Precisely, we consider the parameter dual treedepth of the matrix A, denoted tdD(A), which is the treedepth of the graph over the rows of A, where two rows are adjacent if in some column they simultaneously contain a non-zero entry. It was recently shown by Koutecký et al. that ILP Feasibility can be solved in time A∞2O(tdD(A)) ⋅ (k + ℓ + log b∞)O(1). We present a streamlined proof of this fact and prove that, again, this running time is probably optimal: even assuming that all entries of A and b are in {-1, 0, 1}, the existence of an algorithm with running time 22o(tdD(A)) ⋅ (k + ℓ)O(1) would contradict the exponential time hypothesis.  © 2020 Owner/Author.",ETH; fixed-parameter tractability; Integer linear programming,Matrix algebra; Absolute values; Exponential time hypothesis; Feasibility problem; Fixed-parameter algorithms; Integer Linear Programming; Integer linear programs; Integer matrices; Linear constraints; Integer programming
Approximate Counting CSP Seen from the Other Side,2020,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085254438&doi=10.1145%2f3389390&partnerID=40&md5=49a9700b0998b32b8cac3a92718203ff,"In this article, we study the complexity of counting Constraint Satisfaction Problems (CSPs) of the form #CSP(C, -), in which the goal is, given a relational structure A from a class C of structures and an arbitrary structure B, to find the number of homomorphisms from A to B. Flum and Grohe showed that #CSP(C, -) is solvable in polynomial time if C has bounded treewidth [FOCS'02]. Building on the work of Grohe [JACM'07] on decision CSPs, Dalmau and Jonsson then showed that if C is a recursively enumerable class of relational structures of bounded arity, then, assuming FPTĝ‰ #W[1], there are no other cases of #CSP(C, -) solvable exactly in polynomial time (or even fixed-parameter time) [TCS'04]. We show that, assuming FPT ĝ‰ W[1] (under randomised parameterised reductions) and for C satisfying certain general conditions, #CSP(C,-) is not solvable even approximately for C of unbounded treewidth; that is, there is no fixed parameter tractable (and thus also not fully polynomial) randomised approximation scheme for #CSP(C, -). In particular, our condition generalises the case when C is closed undertaking minors. © 2020 ACM.",Approximate counting; constraint satisfaction; homomorphisms,Constraint satisfaction problems; Polynomial approximation; Approximate counting; Approximation scheme; Arbitrary structures; Bounded treewidth; Parameter-time; Polynomial-time; Relational structures; Tree-width; C (programming language)
Qadratic Simulations of Merlin Arthur Games,2020,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085253834&doi=10.1145%2f3389399&partnerID=40&md5=5c15d858c7d59ab231373c2e99acd40f,"The known proofs of MA C PP incur a quadratic overhead in the running time. We prove that this quadratic overhead is necessary for black-box simulations; in particular, we obtain an oracle relative to which MA-TIME (t) C P-TIME (o(t2)). We also show that 2-sided-error Merlin-Arthur games can be simulated by 1-sided-error Arthur-Merlin games with quadratic overhead. We also present a simple, query complexity based proof (provided by Mika Göös) that there is an oracle relative to which MA C NPBPP (which was previously known to hold by a proof using generics). © 2020 ACM.",Arthur; games; Merlin; Quadratic; simulations,Computation theory; Computer science; Arthur-Merlin games; Black-box simulation; Query complexity; Running time; Computational methods
Sparsification of SAT and CSP Problems via Tractable Extensions,2020,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085248949&doi=10.1145%2f3389411&partnerID=40&md5=247559fbda12bcb25f587e25cd8a5558,"Unlike polynomial kernelization in general, for which many non-trivial results and methods exist, only few non-trival algorithms are known for polynomial-time sparsification. Furthermore, excepting problems on restricted inputs (such as graph problems on planar graphs), most such results rely upon encoding the instance as a system of bounded-degree polynomial equations. In particular, for satisfiability (SAT) problems with a fixed constraint language Γ, every previously known result is captured by this approach; for several such problems, this is known to be tight. In this work, we investigate the limits of this approach - in particular, does it really cover all cases of non-trivial polynomial-time sparsification? We generalize the method using tools from the algebraic approach to constraint satisfaction problems (CSP). Every constraint that can be modelled via a system of linear equations, over some finite field F, also admits a finite domain extension to a tractable CSP with a Maltsev polymorphism; using known algorithms for Maltsev languages, we can show that every problem of the latter type admits a ""basis"" of O(n) constraints, which implies a linear sparsification for the original problem. This generalization appears to be strict; other special cases include constraints modelled via group equations over some finite group G. For sparsifications of polynomial but super-linear size, we consider two extensions of this. Most directly, we can capture systems of bounded-degree polynomial equations in a ""lift-and-project"" manner, by finding Maltsev extensions for constraints over c-tuples of variables, for a basis with O(nc) constraints. Additionally, we may use extensions with k-edge polymorphisms instead of requiring a Maltsev polymorphism. We also investigate characterizations of when such extensions exist. We give an infinite sequence of partial polymorphisms 12, ...which characterizes whether a language Γ has a Maltsev extension (of possibly infinite domain). In the complementary direction of proving lower bounds on kernelizability, we prove that for any language not preserved by †1, the corresponding SAT problem does not admit a kernel of size O(n2-ϵ) for any ϵ > 0 unless the polynomial hierarchy collapses. © 2020 ACM.",Constraint satisfaction problems; kernelization; satisfiability problems; sparsification; universal algebra,Polymorphism; Polynomial approximation; Algebraic approaches; Constraint language; Lift-and-project; Non-trivial polynomial; Polynomial equation; Polynomial hierarchies; Satisfiability problems; System of linear equations; Constraint satisfaction problems
Beating the Generator-Enumeration Bound for Solvable-Group Isomorphism,2020,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085251218&doi=10.1145%2f3389396&partnerID=40&md5=64401c4bddff77cd90658d330881d980,"We consider the isomorphism problem for groups specified by their multiplication tables. Until recently, the best published bound for the worst-case was achieved by the nlogp n + O(1) generator-enumeration algorithm where n is the order of the group and p is the smallest prime divisor of n. In previous work with Fabian Wagner, we showed an n(1 / 2) logp n + O(log n / log log n) -time algorithm for testing isomorphism of p-groups by building graphs with degree bounded by p + O(1) that represent composition series for the groups and applying Luks' algorithm for testing isomorphism of bounded-degree graphs. In this work, we extend this improvement to the more general class of solvable groups to obtain an n(1 / 2) logp n + O(log n / log log n) -time algorithm. In the case of solvable groups, the composition factors can be large which prevents previous methods from outperforming the generator-enumeration algorithm. Using Hall's theory of Sylow bases, we define a new object that generalizes the notion of a composition series with small factors but exists even when the composition factors are large. By constructing graphs that represent these objects and running Luks' algorithm, we obtain our algorithm for solvable-group isomorphism. We also extend our algorithm to compute canonical forms of solvable groups while retaining the same complexity. © 2020 ACM.",Group isomorphism,Set theory; Bounded degree graphs; Canonical form; Enumeration algorithms; General class; Isomorphism problems; p-Group; Solvable group; Time algorithms; Graph algorithms
Cops-Robber Games and the Resolution of Tseitin Formulas,2020,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085257046&doi=10.1145%2f3378667&partnerID=40&md5=afcbba674c0ba44ed21257e5d5e042e7,"We characterize several complexity measures for the resolution of Tseitin formulas in terms of a two person cop-robber game. Our game is a slight variation of the one Seymour and Thomas used in order to characterize the tree-width parameter. For any undirected graph, by counting the number of cops needed in our game in order to catch a robber in it, we are able to exactly characterize the width, variable space, and depth measures for the resolution of the Tseitin formula corresponding to that graph. We also give an exact game characterization of resolution variable space for any formula. We show that our game can be played in a monotone way. This implies that the associated resolution measures on Tseitin formulas correspond exactly to those under the restriction of Davis-Putnam resolution, implying that this kind of resolution is optimal on Tseitin formulas for all the considered measures. Using our characterizations, we improve the existing complexity bounds for Tseitin formulas showing that resolution width, depth, and variable space coincide up to a logarithmic factor, and that variable space is bounded by the clause space times a logarithmic factor. © 2020 ACM.",cops-robber game; Resolution,Computation theory; Computer science; Complexity bounds; Complexity measures; Cop-robber games; Davis-Putnam resolution; Exact games; Tree-width; Undirected graph; Variable space; Computational methods
Editorial from the new editor-in-chief,2020,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080860156&doi=10.1145%2f3381517&partnerID=40&md5=f0e12cebe50acc88d7208e4763ff0c8b,[No abstract available],,
Pseudorandom bits for oblivious branching programs,2020,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079751111&doi=10.1145%2f3378663&partnerID=40&md5=487ce344461adb14aa4cbcb58bb99809,"We construct a pseudorandom generator that fools known-order read-k oblivious branching programs and, more generally, any linear length oblivious branching program. For polynomial width branching programs, the seed lengths in our constructions are Õ (n1−1/2k−1 ) (for the read-k case) and O(n/ log log n) (for the linear length case). Previously, the best construction for these models required seed length (1 − Ω(1))n. © 2020 Association for Computing Machinery.",Oblivious branching programs; Pseudorandom generator; Read-k,Computation theory; Computer science; Branching programs; Pseudorandom bits; Pseudorandom generators; Computational methods
On the power of border of depth-3 arithmetic circuits,2020,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080953297&doi=10.1145%2f3371506&partnerID=40&md5=67133aba01ee7497b5a4661bbd19892c,"We show that over the field of complex numbers, every homogeneous polynomial of degree d can be approximated (in the border complexity sense) by a depth-3 arithmetic circuit of top fan-in at most 2. This is quite surprising, since there exist homogeneous polynomials P on n variables of degree 2, such that any depth-3 arithmetic circuit computing P must have top fan-in at least Ω(n). As an application, we get a new tradeoff between the top fan-in and formal degree in an approximate analog of the celebrated depth reduction result of Gupta, Kamath, Kayal, and Saptharishi [7, 10]. Formally, we show that if a degree d homogeneous polynomial P can be computed by an arithmetic circuit of size s ≥ d, then for every t ≤ d, P is in the border of a depth-3 circuit of top fan-in sO(t ) and formal degree sO(d/t ). To the best of our knowledge, the upper bound on the top fan-in in the original proof of Reference [7] is always at least sΩ(√d) , regardless of the formal degree. © 2020 Association for Computing Machinery.",Algebraic complexity; Border complexity; Depth reductions; Lower bounds,Logic circuits; Timing circuits; Algebraic complexity; Arithmetic circuit; Border complexity; Complex number; Depth-3 circuits; Homogeneous polynomials; Lower bounds; Upper Bound; Polynomials
Reasons for hardness in QBF proof systems,2020,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079753003&doi=10.1145%2f3378665&partnerID=40&md5=976eb72f88615df4fcc037369111fbd6,"We aim to understand inherent reasons for lower bounds for QBF proof systems and revisit and compare two previous approaches in this direction. The first of these relates size lower bounds for strong QBF Frege systems to circuit lower bounds via strategy extraction (Beyersdorff and Pich, LICS'16). Here, we show a refined version of strategy extraction and thereby for any QBF proof system obtain a trichotomy for hardness: (1) via circuit lower bounds, (2) via propositional Resolution lower bounds, or (3) “genuine” QBF lower bounds. The second approach tries to explain QBF lower bounds through quantifier alternations in a system called relaxing QU-Res (Chen, ACM TOCT 2017). We prove a strong lower bound for relaxing QU-Res, which at the same time exhibits significant shortcomings of that model. Prompted by this, we introduce a hierarchy of new systems that improve Chen's model and prove a strict separation for the complexity of proofs in this hierarchy. We show that lower bounds in our new model correspond to the trichotomy obtained via strategy extraction. © 2020 Association for Computing Machinery.",Lower bounds; Proof complexity; Quantified Boolean formulas; Resolution,Extraction; Hardness; Optical resolving power; Chen's model; Circuit lower bounds; Lower bounds; Proof complexity; Proof system; Propositional resolution; Quantified Boolean formulas; Boolean functions
Pattern matching with variables: Efficient algorithms and complexity results,2020,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080958520&doi=10.1145%2f3369935&partnerID=40&md5=52aac4b2a76fcb1301bc75f8de5b8a56,"A pattern a (i.e., a string of variables and terminals) matches a word w, if w can be obtained by uniformly replacing the variables of a by terminal words. The respective matching problem, i.e., deciding whether or not a given pattern matches a given word, is generally NP-complete, but can be solved in polynomial-time for restricted classes of patterns. We present efficient algorithms for the matching problem with respect to patterns with a bounded number of repeated variables and patterns with a structural restriction on the order of variables. Furthermore, we show that it is NP-complete to decide, for a given number k and a word w, whether w can be factorised into k distinct factors. As an immediate consequence of this hardness result, the injective version (i.e., different variables are replaced by different words) of the matching problem is NP-complete even for very restricted classes of patterns. © 2020 Association for Computing Machinery.",Combinatorial pattern matching; Combinatorics on words; NP-complete string problems; Patterns with variables,Pattern matching; Polynomial approximation; Algorithms and complexity; Combinatorial pattern matching; Combinatorics on words; Matching problems; NP Complete; Order of variables; Patterns with variables; Structural restrictions; Computational complexity
Exact learning: On the boundary between horn and CNF,2020,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080937431&doi=10.1145%2f3369930&partnerID=40&md5=4004cf73fb5f4b0c0fa9b31da252d0ef,"A major problem in computational learning theory is whether the class of formulas in conjunctive normal form (CNF) is efficiently learnable. Although it is known that this class cannot be polynomially learned using either membership or equivalence queries alone, it is open whether the CNF class can be polynomially learned using both types of queries. One of the most important results concerning a restriction of the CNF class is that propositional Horn formulas are polynomial time learnable in Angluin's exact learning model with membership and equivalence queries. In this work, we push this boundary and show that the class of multivalued dependency formulas (MVDF), which non-trivially extends propositional Horn, is polynomially learnable from interpretations. We then provide a notion of reduction between learning problems in Angluin's model, showing that a transformation of the algorithm suffices to efficiently learn multivalued database dependencies from data relations. We also show via reductions that our main result extends well known previous results and allows us to find alternative solutions for them. © 2020 Association for Computing Machinery.",Complexity analysis; Exact learning; Multivalued dependencies,Equivalence classes; Metadata; Polynomial approximation; Alternative solutions; Complexity analysis; Computational learning theory; Conjunctive normal forms; Database dependencies; Exact learning; Multi-valued; Propositional Horn formulas; Learning systems
Search versus decision for election manipulation problems,2020,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080929195&doi=10.1145%2f3369937&partnerID=40&md5=f11c337c5e8916d757643b88892e342e,"Most theoretical definitions about the complexity of manipulating elections focus on the decision problem of recognizing which instances can be successfully manipulated rather than the search problem of finding the successful manipulative actions. Since the latter is a far more natural goal for manipulators, that definitional focus may be misguided if these two complexities can differ. Our main result is that they probably do differ: If P ≠ NP n coNP (which itself iswell known to hold if integer factoring is hard), then for election manipulation, election bribery, and some types of election control, there are election systems for which the problem of recognizing which instances can be successfully manipulated is polynomial-time solvable, yet the task of producing the successful manipulations cannot be done in polynomial time. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Borodin-demers theorem; Elections; NP intersect coNP; Search versus decision; Structural complexity theory; Typical-case complexity,Polynomial approximation; Borodin-demers theorem; Elections; Search versus decision; Structural complexity; Typical case complexity; Decision theory
Separation between read-once oblivious algebraic branching programs (ROABPs) and multilinear depth-three circuits,2020,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080954910&doi=10.1145%2f3369928&partnerID=40&md5=68b6aa6984fcc789ec322eb98a6f4a48,"We showan exponential separation between twowell-studied models of algebraic computation, namely, readonce oblivious algebraic branching programs (ROABPs) and multilinear depth-three circuits. In particular, we show the following: (1) There exists an explicit n-variate polynomial computable by linear sized multilinear depth-three circuits (with only two product gates) such that every ROABP computing it requires 2O(n) size. (2) Any multilinear depth-three circuit computing IMMn,d (the iterated matrix multiplication polynomial formed by multiplying d, n × n symbolic matrices) has nO(d) size. IMMn,d can be easily computed by a poly(n,d) sized ROABP. (3) Further, the proof of (2) yields an exponential separation between multilinear depth-four and multilinear depth-three circuits: There is an explicit n-variate, degree d polynomial computable by a poly(n) sized multilinear depth-four circuit such that any multilinear depth-three circuit computing it has size nO(d) . This improves upon the quasi-polynomial separation of Reference [36] between these two models. The hard polynomial in (1) is constructed using a novel application of expander graphs in conjunction with the evaluation dimension measure [15, 33, 34, 36], while (2) is proved via a new adaptation of the dimension of the partial derivatives measure of Reference [32]. Our lower bounds hold over any field. © 2020 Association for Computing Machinery.",Evaluation dimension; Expander graphs; Iterated matrix multiplication; Multilinear depth-three circuits; Read-once oblivious algebraic branching programs; Skewed partial derivatives,Computer circuits; Multiplying circuits; Polynomials; Separation; Timing circuits; Algebraic branching programs; Depth three circuits; Evaluation dimension; Expander graphs; MAtrix multiplication; Partial derivatives; Matrix algebra
Toward a general direct product testing theorem,2019,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076986100&doi=10.1145%2f3369939&partnerID=40&md5=4fc35cc698e7c8ce898dd5aa210ed4fc,"The direct product encoding of a string a ϵ {0, 1}n on an underlying domain V ϵ [nk] is a function DPV (a) that gets as input a set S ϵ V and outputs a restricted to S. In the direct product testing problem, we are given a function F: V ϵ {0, 1}k, and our goal is to test whether F is close to a direct product encoding-that is, whether there exists some a ϵ {0, 1}n such that on most sets S, we have F (S) = DPV (a)(S). A natural test is as follows: select a pair (S, S) ϵ V according to some underlying distribution over V × V, query F on this pair, and check for consistency on their intersection. Note that the preceding distribution may be viewed as a weighted graph over the vertex set V and is referred to as a test graph. The testability of direct products was studied over various domains and test graphs: Dinur and Steurer (CCC'14) analyzed it when V equals the k-th slice of the Boolean hypercube and the test graph is a member of the Johnson graph family. Dinur and Kaufman (FOCS'17) analyzed it for the case where V is the set of faces of a Ramanujan complex, where in this case |V | = Ok (n). In this article, we study the testability of direct products in a general setting, addressing the question: what properties of the domain and the test graph allow one to prove a direct product testing theoremϵ Towards this goal, we introduce the notion of coordinate expansion of a test graph. Roughly speaking, a test graph is a coordinate expander if it has global and local expansion, and has certain nice intersection properties on sampling. We show that whenever the test graph has coordinate expansion, it admits a direct product testing theorem. Additionally, for every k and n, we provide a direct product domain V ϵ nk of size n, called the sliding window domain, for which we prove direct product testability. © 2019 Association for Computing Machinery.",Derandomization; Direct product; Johnson graph; PCP; Property testing; Ramanujan complex,Encoding (symbols); Expansion; Signal encoding; Testing; Derandomization; Direct product; Johnson graph; Property-testing; Ramanujan complex; Graph theory
FPT algorithms for embedding into low-complexity graphic metrics,2019,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076980614&doi=10.1145%2f3369933&partnerID=40&md5=a59bd1246a038bef200b090ec0ea01ac,"Given metric spaces (X, DX) and (Y, DY), an embedding F: X → Y is an injective mapping from X to Y. Expansion eF and contraction cF of an embedding F: X → Y are defined as respectively, and distortion dF is defined as dF = eF • cF. Observe that dF = 1. An embedding F: X → Y is noncontracting if cF = 1. When d = 1, then F is isometry. The Metric Embedding problem takes as input two metric spaces (X, DX) and (Y, DY), and a positive integer d. The objective is to determine whether there is an embedding F: X → Y such that dF = d. Such an embedding is called a distortion d embedding. The bijective Metric Embedding problem is a special case of the Metric Embedding problem where |X | = |Y |. In parameterized complexity, the Metric Embedding problem, in full generality, is known to be W-hard and, therefore, not expected to have an FPT algorithm. In this article, we consider the Gen-Graph Metric Embedding problem, where the two metric spaces are graph metrics. We explore the extent of tractability of the problem in the parameterized complexity setting. We determine whether an unweighted graph metric (G, DG) can be embedded, or bijectively embedded, into another unweighted graph metric (H, DH), where the graph H has low structural complexity. For example, H is a cycle, or H has bounded treewidth or bounded connected treewidth. The parameters for the algorithms are chosen from the upper bound d on distortion, bound → on the maximum degree of H, treewidth a of H, and connected treewidth ac of H. Our general approach to these problems can be summarized as trying to understand the behavior of the shortest paths in G under a low-distortion embedding into H, and the structural relation the mapping of these paths has to shortest paths in H. © 2019 Association for Computing Machinery.",Dynamic programming; FPT; Low-distortion embeddings; Metric embedding; Metric spaces,Dynamic programming; Graph theory; Mapping; Parameter estimation; Set theory; Bounded treewidth; Low-distortion embeddings; Metric embeddings; Metric spaces; Parameterized complexity; Positive integers; Structural complexity; Unweighted graphs; Embeddings
Tolerant Junta Testing and the Connection to Submodular Optimization and Function Isomorphism,2019,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075558333&doi=10.1145%2f3337789&partnerID=40&md5=7448d1bdf22a711a9f4adcec82fb64a1,"A function f : {-1, 1}n → {-1, 1} is a k-junta if it depends on at most k of its variables. We consider the problem of tolerant testing of k-juntas, where the testing algorithm must accept any function that is €-close to some k-junta and reject any function that is €-far from every k-junta for some € = O(€ ) and k = O(k). Our first result is an algorithm that solves this problemwith query complexity polynomial in k and 1/€. This result is obtained via a new polynomial-Time approximation algorithm for submodular function minimization (SFM) under large cardinality constraints, which holds even when only given an approximate oracle access to the function. Our second result considers the case where k = k.We show how to obtain a smooth tradeoff between the amount of tolerance and the query complexity in this setting. Specifically, we design an algorithm that, given p (0, 1), accepts any function that is €p 16-close to some k-junta and rejects any function that is €-far from every k-junta. The query complexity of the algorithm is O( k log k €p(1-p)k ). Finally, we show how to apply the second result to the problem of tolerant isomorphism testing between two unknown Boolean functions f and. We give an algorithm for this problem whose query complexity only depends on the (unknown) smallest k such that either f or is close to being a k-junta. © 2019 Association for Computing Machinery. All rights reserved.",Boolean functions; function isomorphism; juntas; Property testing; submodular optimization; tolerant testing,Acceptance tests; Approximation algorithms; Computational complexity; Polynomial approximation; Set theory; Cardinality constraints; Function isomorphisms; Isomorphism testing; juntas; Polynomial time approximation algorithms; Property-testing; Submodular function minimization; Submodular optimizations; Boolean functions
Bounded Independence versus Symmetric Tests,2019,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075582195&doi=10.1145%2f3337783&partnerID=40&md5=e2404a94f93530f3790e758e7ef80061,"For a test T ⊆ {0, 1}n, define k ∗ (T ) to be the maximum k such that there exists a k-wise uniform distribution over {0, 1}n whose support is a subset of T . For Ht = {x ∈ {0, 1}n : | i xi - n/2| ≤ t }, we prove k ∗ (Ht ) = Θ(t 2/n + 1). For Sm,c = {x ∈ {0, 1}n : i xi = c (mod m)}, we prove that k ∗ (Sm,c ) = Θ(n/m2). For some k = O(n/m) we also show that any k-wise uniform distribution puts probability mass at most 1/m + 1/100 over Sm,c . Finally, for any fixed odd m we show that there is an integer k = (1 - ω(1))n such that any k-wise uniform distribution lands inT with probability exponentially close to |Sm,c |/2n; and this result is false for any evenm. © 2019 Association for Computing Machinery. All rights reserved.",bounded independence; modulus; Pseudorandomness; symmetric functions; thresholds,Computation theory; Computational methods; Computer science; Bounded independence; modulus; Pseudorandomness; Symmetric functions; thresholds; Probability distributions
Approximating Pairwise Correlations in the Ising Model,2019,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075578492&doi=10.1145%2f3337785&partnerID=40&md5=064ce024ab608a06ee62a6cbf8b6e841,"In the Ising model, we consider the problem of estimating the covariance of the spins at two specified vertices. In the ferromagnetic case, it is easy to obtain an additive approximation to this covariance by repeatedly sampling from the relevant Gibbs distribution. However, we desire a multiplicative approximation, and it is not clear how to achieve this by sampling, given that the covariance can be exponentially small. Our main contribution is a fully polynomial time randomised approximation scheme (FPRAS) for the covariance in the ferromagnetic case. We also show that the restriction to the ferromagnetic case is essential-there is no FPRAS for multiplicatively estimating the covariance of an antiferromagnetic Ising model unless RP = #P. In fact, we show that even determining the sign of the covariance is #P-hard in the antiferromagnetic case. © 2019 Association for Computing Machinery. All rights reserved.",Ising model; Markov chain Monte Carlo,Antiferromagnetism; Ferromagnetic materials; Ferromagnetism; Ising model; Markov processes; Antiferromagnetic Ising model; Antiferromagnetics; Approximation scheme; Gibbs distribution; Markov Chain Monte-Carlo; Multiplicative approximations; Pairwise correlation; Polynomial-time; Polynomial approximation
Representations of monotone boolean functions by linear programs,2019,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075615893&doi=10.1145%2f3337787&partnerID=40&md5=1ea1eaa03a0e374997a54f9a32358f7f,"We introduce the notion of monotone linear programming circuits (MLP circuits), a model of computation for partial Boolean functions. Using this model, we prove the following results.1 (1) MLP circuits are superpolynomially stronger than monotone Boolean circuits. (2) MLP circuits are exponentially stronger than monotone span programs over the reals. (3) MLP circuits can be used to provide monotone feasibility interpolation theorems for Lovász-Schrijver proof systems and for mixed Lovász-Schrijver proof systems. (4) The Lovász-Schrijver proof system cannot be polynomially simulated by the cutting planes proof system. Finally, we establish connections between the problem of proving lower bounds for the size of MLP circuits and the field of extension complexity of polytopes. © 2019 Copyright held by the owner/author(s).",Feasible interpolation; Lovász-schrijver proof systems; Monotone linear programming circuits,Boolean functions; Interpolation; Linear programming; Boolean circuit; Extension complexity; Feasible interpolation; Linear programs; Model of computation; Monotone Boolean functions; Monotone span programs; Proof system; Linear networks
PPSZ for k ≥ 5: More Is Better,2019,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075582512&doi=10.1145%2f3349613&partnerID=40&md5=34d2d8f2414e231c6e1ba7493db54982,"We show that for k ≥ 5, the PPSZ algorithm for k-SAT runs exponentially faster if there is an exponential number of satisfying assignments. More precisely, we show that for every k ≥ 5, there is a strictly increasing function f : [0, 1] → R with f (0) = 0 that has the following property. If F is a k-CNF formula over n variables and |sat(F ) | = 2δn solutions, then PPSZ finds a satisfying assignment with probability at least 2 -ckn-o(n)+f (δ )n. Here, 2 -ckn-o(n) is the success probability proved by Paturi et al. [11] for k-CNF formulas with a unique satisfying assignment. Our proof rests on a combinatorial lemma: given a set S {0, 1}n, we can partition {0, 1}n into subcubes such that each subcube B contains exactly one element of S. Such a partition B induces a distribution on itself, via Pr[B] = |B|/2n for each B B. We are interested in partitions that induce a distribution of high entropy. We show that, in a certain sense, the worst case (minS: |S |=s maxB H(B)) is achieved if S is a Hamming ball. This lemma implies that every set S of exponential size allows a partition of linear entropy. This in turn leads to an exponential improvement of the success probability of PPSZ. © 2019 Association for Computing Machinery. All rights reserved.",Boolean satisfiability; exponential algorithms; randomized algorithms,Entropy; Boolean satisfiability; Exponential algorithms; Exponential numbers; Increasing functions; Linear entropy; More is betters; Randomized Algorithms; Satisfying assignments; Probability
New Resolution-Based QBF Calculi and Their Proof Complexity,2019,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075566481&doi=10.1145%2f3352155&partnerID=40&md5=b4f0d559dc069e804f4151eb627916de,"Modern QBF solvers typically use two different paradigms, conflict-driven clause learning (CDCL) solving or expansion solving. Proof systems for quantified Boolean formulas (QBFs) provide a theoretical underpinning for the performance of these solvers, with Q-Resolution and its extensions relating to CDCL solving and Exp+Res relating to expansion solving. This article defines two novel calculi, which are resolution-based and enable unification of some of the principal existing resolution-based QBF calculi, namely Q-resolution, long-distance Q-resolution and the expansion-based calculus Exp+Res. However, the proof complexity of the QBF resolution proof systems is currently notwell understood. In this article, we completely determine the relative power of the main QBF resolution systems, settling in particular the relationship between the two different types of resolution-based QBF calculi: proof systems for CDCLbased solvers (Q-resolution, universal, and long-distance Q-resolution) and proof systems for expansionbased solvers (Exp+Res and its generalizations IR-calc and IRM-calc defined here). The most challenging part of this comparison is to exhibit hard formulas that underlie the exponential separations of the aforementioned proof systems. To this end, we exhibit a new and elegant proof technique for showing lower bounds in QBF proof systems based on strategy extraction. This technique provides a direct transfer of circuit lower bounds to lengths-of-proofs lower bounds. We use our method to show the hardness of a natural class of parity formulas for Q-resolution and universal Q-resolution. Variants of the formulas are hard for even stronger systems such as long-distance Q-resolution and extensions. With a completely different and novel counting argument,we showthe hardness of the prominent formulas of Kleine Being et al. [51] for the strong expansion-based calculus IR-calc. © 2019 Association for Computing Machinery. All rights reserved.",lower bound techniques; Proof complexity; QBF; separations,Biomineralization; Calculations; Hardness; Pathology; Separation; Circuit lower bounds; Direct transfer; Lower bound techniques; Parity formulas; Proof complexity; Quantified Boolean formulas; Resolution proofs; Resolution systems; Boolean functions
Constant-Error Pseudorandomness Proofs from Hardness Require Majority,2019,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075558092&doi=10.1145%2f3322815&partnerID=40&md5=eb687bdf9b8356d8210fc761c5fa05b6,"Research in the 1980s and 1990s showed how to construct a pseudorandom generator from a function that is hard to compute on more than 99% of the inputs. A more recent line of works showed, however, that if the generator has small error, then the proof of correctness cannot be implemented in subclasses of TC0, and hence the construction cannot be applied to the known hardness results. This article considers a typical class of pseudorandom generator constructions, and proves an analogous result for the case of large error. © 2019 Association for Computing Machinery. All rights reserved.",black box; lower bound; Majority; pseudorandom generator,Hardness; Black boxes; Hardness result; Lower bounds; Majority; Proof of correctness; Pseudorandom generator construction; Pseudorandom generators; Pseudorandomness; Errors
Optimal Sparsification for Some Binary CSPs Using Low-Degree Polynomials,2019,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075551042&doi=10.1145%2f3349618&partnerID=40&md5=30e87a1254845064c6732068ebd3be84,"This article analyzes to what extent it is possible to efficiently reduce the number of clauses in NP-hard satisfiability problems without changing the answer. Upper and lower bounds are established using the concept of kernelization. Existing results show that if NP coNP/poly, no efficient preprocessing algorithm can reduce n-variable instances of cnf-sat with d literals per clause to equivalent instances with O(nd-ϵ ) bits for any ϵ > 0. For the Not-All-Eqal sat problem, a compression to size O(nd-1) exists. We put these results in a common framework by analyzing the compressibility of CSPs with a binary domain. We characterize constraint types based on the minimum degree of multivariate polynomials whose roots correspond to the satisfying assignments, obtaining (nearly) matching upper and lower bounds in several settings. Our lower bounds show that not just the number of constraints, but also the encoding size of individual constraints plays an important role. For example, for Exact Satisfiability with unbounded clause length it is possible to efficiently reduce the number of constraints to n + 1, yet no polynomial-Time algorithm can reduce to an equivalent instance with O(n2-ϵ ) bits for any ϵ > 0, unless NP coNP/poly. © 2019 Association for Computing Machinery. All rights reserved.",Constraint satisfaction problem; kernelization; satisfiability; sparsification,Formal logic; Polynomial approximation; Kernelization; Polynomial-time algorithms; Pre-processing algorithms; Satisfiability; Satisfiability problems; Satisfying assignments; Sparsification; Upper and lower bounds; Constraint satisfaction problems
New Insights on the (Non-)Hardness of Circuit Minimization and Related Problems,2019,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075548551&doi=10.1145%2f3349616&partnerID=40&md5=831f9b494044a05f4ead9a24a8b667f3,"The Minimum Circuit Size Problem (MCSP) and a related problem (MKTP) that deal with time-bounded Kolmogorov complexity are prominent candidates for NP-intermediate status.We show that, under very modest cryptographic assumptions (such as the existence of one-way functions), the problem of approximating the minimum circuit size (or time-bounded Kolmogorov complexity) within a factor of n1-o(1) is indeed NPintermediate. To the best of our knowledge, these problems are the first natural NP-intermediate problems under the existence of an arbitrary one-way function. Our technique is quite general; we use it also to show that approximating the size of the largest clique in a graph within a factor of n1-o(1) is also NP-intermediate unless NP ⊆ P/poly. We also prove that MKTP is hard for the complexity class DET under non-uniform NC0 reductions. This is surprising, since prior work on MCSP and MKTP had highlighted weaknesses of ""local"" reductions such as ≤NC0 m . We exploit this local reduction to obtain several new consequences:-MKTP is not in AC0[p].-Circuit size lower bounds are equivalent to hardness of a relativized version MKTPA of MKTP under a class of uniform AC0 reductions, for a significant class of sets A.-Hardness of MCSPA implies hardness of MKTPA for a significant class of sets A. This is the first result directly relating the complexity of MCSPA and MKTPA, for any A. © 2019 Association for Computing Machinery. All rights reserved.",circuit size; Computational complexity; Kolmogorov complexity; MCSP,Equivalence classes; Hardness; Timing circuits; Circuit size; Complexity class; Cryptographic assumptions; Kolmogorov complexity; Lower bounds; MCSP; Minimum circuit size problem; One-way functions; Computational complexity
Split contraction: The untold story,2019,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074832879&doi=10.1145%2f3319909&partnerID=40&md5=3f96719a9a47c4e1ce0d29af62874bab,"The edit operation that contracts edges, which is a fundamental operation in the theory of graph minors, has recently gained substantial scientific attention from the viewpoint of Parameterized Complexity. In this article, we examine an important family of graphs, namely, the family of split graphs, which in the context of edge contractions is proven to be significantly less obedient than one might expect. Formally, given a graph G and an integer k, Split Contraction asks whether there exists X ⊆ E(G) such that G/X is a split graph and |X| ≤ k. Here, G/X is the graph obtained from G by contracting edges in X. Guo and Cai [Theoretical Computer Science, 2015] claimed that Split Contraction is fixed-parameter tractable. However, our findings are different. We show that Split Contraction, despite its deceptive simplicity, is W[1]-hard. Our main result establishes the following conditional lower bound: Under the Exponential Time Hypothesis, Split Contraction cannot be solved in time 2o(ℓ2) · nO(1), where ℓ is the vertex cover number of the input graph. We also verify that this lower bound is essentially tight. To the best of our knowledge, this is the first tight lower bound of the form 2o(ℓ2) · nO(1) for problems parameterized by the vertex cover number of the input graph. In particular, our approach to obtain this lower bound borrows the notion of harmonious coloring from Graph Theory, and might be of independent interest. © 2019 Association for Computing Machinery.",Edge contraction; Parameterized complexity; Split contraction; Split graphs,Parameterization; Edge contractions; Exponential time hypothesis; Fundamental operations; Parameterized; Parameterized complexity; Split graphs; Theoretical computer science; Vertex cover; Graph theory
Lower bounds for sum and sum of products of read-once formulas,2019,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065759765&doi=10.1145%2f3313232&partnerID=40&md5=f4f645855c27ee0934dcb82aaf201baa,"We study limitations of polynomials computed by depth-2 circuits built over read-once formulas (ROFs). In particular: • We prove a 2Ω(n) lower bound for the sum of ROFs computing the 2n-variate polynomial in VP defined by Raz and Yehudayoff [21]. • We obtain a 2Ω(n) lower bound on the size of ΣΠ[n1/15] arithmetic circuits built over restricted ROFs of unbounded depth computing the permanent of an n × n matrix (superscripts on gates denote bound on the fan-in). The restriction is that the number of variables with + gates as a parent in a proper sub formula of the ROF has to be bounded by n. This proves an exponential lower bound for a subclass of possibly non-multilinear formulas of unbounded depth computing the permanent polynomial. • We also show an exponential lower bound for the above model against a polynomial in VP. • Finally, we observe that the techniques developed yield an exponential lower bound on the size of ΣΠ[N1/30] arithmetic circuits built over syntactically multi-linear ΣΠΣ[N1/4] arithmetic circuits computing a product of variable disjoint linear forms on N variables, where the superscripts on gates denote bound on the fan-in. Our proof techniques are built on the measure developed by Kumar et al. [14] and are based on a non-trivial analysis of ROFs under random partitions. Further, our results exhibit strengths and provide more insight into the lower bound techniques introduced by Raz [19]. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Algebraic complexity theory; Arithmetic formulas; Lower bounds,Electric network analysis; Logic circuits; Polynomials; Algebraic complexity theories; Arithmetic circuit; Arithmetic formulas; Lower bound techniques; Lower bounds; Random partitions; Read-once formulas; Sum of products; Computer circuits
Communication complexity and graph families,2019,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065755671&doi=10.1145%2f3313234&partnerID=40&md5=7e097494bb03f59d7eaeda74502eb692,"Given a graph G and a pair (F1, F2) of graph families, the function GDISJG, F1, F2 takes as input, two induced subgraphsG1 andG2 ofG, such thatG1 ∈ F1 andG2 ∈ F2 and returns 1 ifV(G1) ∩ V(G2) = ∅ and 0 otherwise. We study the communication complexity of this problem in the two-party model. In particular, we look at pairs of hereditary graph families. We show that the communication complexity of this function, when the two graph families are hereditary, is sublinear if and only if there are finitely many graphs in the intersection of these two families. Then, using concepts from parameterized complexity, we obtain nuanced upper bounds on the communication complexity of GDISJG, F1, F2. A concept related to communication protocols is that of a (F1, F2)-separating family of a graph G. A collection F of subsets of V(G) is called a (F1, F2)-separating family for G, if for any two vertex disjoint induced subgraphs G1 ∈ F1,G2 ∈ F2, there is a set F ∈ F with V(G1) ⊆ F andV(G2) ∩ F = ∅. Given a graphG onn vertices, for any pair (F1, F2) of hereditary graph families with sublinear communication complexity for GDISJG, F1, F2, we give an enumeration algorithm that finds a subexponential sized (F1, F2)-separating family. In fact, we give an enumeration algorithm that finds a 2o(k)nO(1) sized (F1, F2)-separating family, where k denotes the size of a minimum sized set S of vertices such that V(G) \ S has a bipartition (V1,V2) with G[V1] ∈ F1 and G[V2] ∈ F2. We exhibit a wide range of applications for these separating families, to obtain combinatorial bounds, enumeration algorithms, as well as exact and FPT algorithms for several problems. © 2019 Association for Computing Machinery.",Communication complexity; FPT algorithms; Separating family,Graph theory; Separation; Combinatorial bounds; Communication complexity; Enumeration algorithms; FPT algorithms; Induced subgraphs; Parameterized complexity; Separating family; Vertex disjoint; Computational complexity
"Multi-party protocols, information complexity and privacy",2019,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065786082&doi=10.1145%2f3313230&partnerID=40&md5=f735307447967d6859b539bfef8759d7,"We introduce a new information-theoretic measure, which we call Public Information Complexity (PIC), as a tool for the study of multi-party computation protocols, and of quantities such as their communication complexity, or the amount of randomness they require in the context of information-theoretic private computations. We are able to use this measure directly in the natural asynchronous message-passing peer-to-peer model and show a number of interesting properties and applications of our new notion: The Public Information Complexity is a lower bound on the Communication Complexity and an upper bound on the Information Complexity; the difference between the Public Information Complexity and the Information Complexity provides a lower bound on the amount of randomness used in a protocol; any communication protocol can be compressed to its Public Information Cost; and an explicit calculation of the zero-error Public Information Complexity of the k-party, n-bit Parity function, where a player outputs the bitwise parity of the inputs. The latter result also establishes that the amount of randomness needed by a private protocol that computes this function is Ω(n). © 2019 Association for Computing Machinery.",Multi-party communication complexity; Peer-to-peer communication complexity; Private computation; Randomness,Information theory; Message passing; Peer to peer networks; Random processes; Communication complexity; Information complexity; Information theoretic measure; Multi-party communication; Multi-party computation protocols; Peer-to-peer communications; Private computation; Randomness; Computational complexity
Gadgets and anti-gadgets leading to a complexity dichotomy,2019,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062233699&doi=10.1145%2f3305272&partnerID=40&md5=e565273399d9ce04420d3abff20b7940,"                             We introduce an idea called anti-gadgetsfor reductions in complexity theory. These anti-gadgets are presented as graph fragments, but their effect is equivalent to erasing the presence of other graph fragments, as if we had managed to include a negative copy of a certain graph gadget. We use this idea to prove a complexity dichotomy theorem for the partition function Z(G) of spin systems over 3-regular directed graphs G, Z(G) = f (σ(u),σ(v)), σ:V (G)→{0,1} (u,v)∈E(G) where each edge is given a (not necessarily symmetric) complex-valued binary function f : {0, 1}                             2                              → C. We show that Z(G) is either computable in polynomial time or #P-hard, depending explicitly on f. When the input graph G is planar, there is an additional class of polynomial time computable partition functions Z(G), while everything else remains #P-hard. Furthermore, this additional class is precisely those that can be transformed by a holographic reduction to matchgates, followed by the Fisher-Kasteleyn-Temperley algorithm via Pfaffians.                          © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Counting complexity; Dichotomy theorem; Holant problem; Holographic algorithms; Interpolation; Partition function,Directed graphs; Holography; Interpolation; Polynomial approximation; Counting complexity; Dichotomy theorem; Holant problems; Holographic algorithms; Partition functions; Graph theory
Characterization and lower bounds for branching program size using projective dimension,2019,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062243420&doi=10.1145%2f3305274&partnerID=40&md5=51b70f495c673bd601f1908cbc5c6c49,"                             We study projective dimension, a graph parameter, denoted by pd(G) for a bipartite graph G, introduced by Pudlák and Rödl (1992). For a Boolean function f (on n bits), Pudlák and Rödl associated a bipartite graph G                             f                              and showed that size of the optimal branching program computing f , denoted by bpsize(f ), is at least pd(G                             f                              ) (also denoted by pd(f )). Hence, proving lower bounds for pd(f ) implies lower bounds for bpsize(f ). Despite several attempts (Pudlák and Rödl (1992), Rónyai et al. (2000)), proving super-linear lower bounds for projective dimension of explicit families of graphs has remained elusive. We observe that there exist a Boolean function f for which the gap between the pd(f ) and bpsize(f )) is 2                             Ω(                             n                             )                             . Motivated by the argument in Pudlák and Rödl (1992), we define two variants of projective dimension: projective dimension with intersection dimension 1, denoted by upd(f ), and bitwise decomposable projective dimension, denoted by bitpdim(f ). We show the following results: (a) We observe that there exist a Boolean function f for which the gap between upd(f ) and bpsize(f ) is 2                             Ω(                             n                             )                             . In contrast, we also show that the bitwise decomposable projective dimension characterizes size of the branching program up to a polynomial factor. That is, there exists a constant c > 0 and for any function f , bitpdim(f )/6 ≤ bpsize(f ) ≤ (bitpdim(f ))                             c                             . (b) We introduce a new candidate family of functions f for showing super-polynomial lower bounds for bitpdim(f ). As our main result, for this family of functions, we demonstrate gaps between pd(f ) and the above two new measures for f : <eou>. We adapt Nechiporuk's techniques for our linear algebraic setting to prove the best-known bpsize lower bounds for bitpdim. Motivated by this linear algebraic setting of our main result, we derive exponential lower bounds for two restricted variants of pd(f ) and upd(f ) by observing that they are exactly equal to well-studied graph parameters-bipartite clique cover number and bipartite partition number, respectively.                          © 2019 Association for Computing Machinery.",Branching programs; Lower bounds; Projective dimension,Graph theory; Bipartite graphs; Bipartite partition; Branching programs; Lower bounds; Nechiporuk's technique; Polynomial factor; Projective dimension; Super-polynomials; Boolean functions
Distribution testing lower bounds via reductions from communication complexity,2019,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062239708&doi=10.1145%2f3305270&partnerID=40&md5=b22670a596b58191a6a3b757a37cec91,"                             We present a new methodology for proving distribution testing lower bounds, establishing a connection between distribution testing and the simultaneous message passing (SMP) communication model. Extending the framework of Blais, Brody, and Matulef [15], we show a simple way to reduce (private-coin) SMP problems to distribution testing problems. This method allows us to prove new distribution testing lower bounds, as well as to provide simple proofs of known lower bounds. Our main result is concerned with testing identity to a specific distribution, p, given as a parameter. In a recent and influential work, Valiant and Valiant [55] showed that the sample complexity of the aforementioned problem is closely related to the                              2                             /                             3                             -quasinorm of p. We obtain alternative bounds on the complexity of this problem in terms of an arguably more intuitive measure and using simpler proofs. More specifically, we prove that the sample complexity is essentially determined by a fundamental operator in the theory of interpolation of Banach spaces, known as Peetre's K-functional. We show that this quantity is closely related to the size of the effective support of p (loosely speaking, the number of supported elements that constitute the vast majority of the mass of p). This result, in turn, stems from an unexpected connection to functional analysis and refined concentration of measure inequalities, which arise naturally in our reduction.                          © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Communication complexity; Distribution testing; K-functional; Lower bounds; Property testing,Banach spaces; Computational complexity; Communication complexity; Distribution testing; K-functional; Lower bounds; Property-testing; Message passing
Tight lower bounds for the complexity of multicoloring,2019,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065814989&doi=10.1145%2f3313906&partnerID=40&md5=d1d456762091f4cd8a87b1eb77c28d1c,"In the multicoloring problem, also known as (a:b)-coloring or b-fold coloring, we are given a graph G and a set of a colors, and the task is to assign a subset of b colors to each vertex of G so that adjacent vertices receive disjoint color subsets. This natural generalization of the classic coloring problem (the b = 1 case) is equivalent to finding a homomorphism to the Kneser graph KGa,b and gives relaxations approaching the fractional chromatic number. We study the complexity of determining whether a graph has an (a:b)-coloring. Our main result is that this problem does not admit an algorithm with runtime f (b) · 2o(logb)·n for any computable f (b) unless the Exponential Time Hypothesis (ETH) fails. A (b + 1)n · poly(n)-time algorithm due to Nederlof [33] shows that this is tight. A direct corollary of our result is that the graph homomorphism problem does not admit a 2O(n+h) algorithm unless the ETH fails even if the target graph is required to be a Kneser graph. This refines the understanding given by the recent lower bound of Cygan et al. [9]. The crucial ingredient in our hardness reduction is the usage of detectingmatricesof Lindström [28], which is a combinatorial tool that, to the best of our knowledge, has not yet been used for proving complexity lower bounds. As a side result, we prove that the runtime of the algorithms of Abasi et al. [1] and of Gabizon et al. [14] for the r-monomial detection problem are optimal under the ETH. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Detecting matrix; Exponential Time Hypothesis; Homomorphism; Kneser graph,Color; Combinatorial tools; Detection problems; Exponential time hypothesis; Fractional chromatic number; Graph homomorphisms; Homomorphism; Kneser graph; Natural generalization; Graph theory
Monotone properties of k-uniform hypergraphs are weakly evasive,2019,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065826837&doi=10.1145%2f3313908&partnerID=40&md5=0d28f76458d1ec70f943c7f5078debbf,"A Boolean function in n variables is weakly evasive if its decision-tree complexity is Ω(n). By k-graphs, we mean k-uniform hypergraphs. A k-graph property on v vertices is a Boolean function on n = (vk ) variables corresponding to the k-subsets of a v-set that is invariant under the v! permutations of the v-set (isomorphisms of k-graphs). In 1976, Rivest and Vuillemin proved that all nonconstant monotone graph properties (k = 2) are weakly evasive, confirming a conjecture of Aanderaa and Rosenberg in 1973. Then, in 2013, Kulkarni, Qiao, and Sun (KQS) proved the analogous result for 3-graphs. We extend these results to k-graphs for every fixed k. From this, we show that monotone Boolean functions invariant under the action of a large primitive group are weakly evasive. Although KQS employ the powerful topological approach of Kahn et al. in 1984 combined with heavy number theory, our argument is elementary and self-contained (modulo some basic group theory). Inspired by the outline of the KQS approach, we formalize the general framework of “orbit augmentation sequences” of sets with group actions. We show that a parameter of such sequences, called the spacing, is a lower bound on the decision-tree complexity for any nontrivial monotone property that is Γ-invariant for all groups Γ involved in the orbit augmentation sequence, assuming all those groups are p-groups. We develop operations on such sequences such as composition and direct product that will provide helpful machinery for our applications. We apply this general technique to k-graphs via certain liftings of k-graphs with wreath product action of p-groups. © 2019 Association for Computing Machinery.",,Boolean functions; Data mining; Decision trees; Graphic methods; Group theory; Machinery; Number theory; Direct product; Graph properties; Monotone Boolean functions; Monotone properties; Primitive group; Topological approach; Tree complexity; Wreath products; Graph theory
On minrank and forbidden subgraphs,2019,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065786733&doi=10.1145%2f3322817&partnerID=40&md5=087bc7724e203d8ee59d83e1c54b5767,"The minrank over a field F of a graph G on the vertex set {1, 2, . . ., n} is the minimum possible rank of a matrix M ∈ Fn×n such that Mi,i ≠ 0 for every i, and Mi, j = 0 for every distinct non-adjacent vertices i and j in G. For an integer n, a graph H, and a field F, let д(n, H, F) denote the maximum possible minrank over F of an n-vertex graph whose complement contains no copy of H. In this article, we study this quantity for various graphs H and fields F. For finite fields, we prove by a probabilistic argument a general lower bound on д(n, H, F), which yields a nearly tight bound of Ω(√n/ log n) for the triangle H = K3. For the real field, we prove by an explicit construction that for every non-bipartite graph H, д(n, H, R) ≥ nδ for some δ = δ(H) > 0. As a by-product of this construction, we disprove a conjecture of Codenotti et al. [11]. The results are motivated by questions in information theory, circuit complexity, and geometry. © 2019 Association for Computing Machinery.",Minrank; Shannon capacity,Information theory; Circuit complexity; Explicit constructions; Forbidden subgraphs; Minrank; Nonadjacent vertices; Probabilistic arguments; Rank of a matrixes; Shannon capacity; Graph theory
Strong locally testable codes with relaxed local decoders,2019,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065822009&doi=10.1145%2f3319907&partnerID=40&md5=4117f42021e6e026dec75bf12d95ced3,"Locally testable codes (LTCs) are error-correcting codes that admit very efficient codeword tests. An LTC is said to be strong if it has a proximity-oblivious tester, that is, a tester that makes only a constant number of queries and rejects non-codewords with a probability that depends solely on their distance from the code. Locally decodable codes (LDCs) are complementary to LTCs. While the latter allow for highly efficient rejection of strings that are far from being codewords, LDCs allow for highly efficient recovery of individual bits of the information that is encoded in strings that are close to being codewords. Constructions of strong-LTCs with nearly-linear length are known, but the existence of a constant-query LDC with polynomial length is a major open problem. In an attempt to bypass this barrier, Ben-Sasson et al. (SICOMP 2006) introduced a natural relaxation of local decodability, called relaxed-LDCs. This notion requires local recovery of nearly all individual information-bits, yet allows for recovery-failure (but not error) on the rest. Ben-Sasson et al. constructed a constant-query relaxed-LDC with nearly-linear length (i.e., length k1+α for an arbitrarily small constant α > 0, where k is the dimension of the code). This work focuses on obtaining strong testability and relaxed decodability simultaneously. We construct a family of binary linear codes of nearly-linear length that are both strong-LTCs (with one-sided error) and constant-query relaxed-LDCs. This improves upon the previously known constructions, which either obtain only weak LTCs or require polynomial length. Our construction heavily relies on tensor codes and PCPs. In particular, we provide strong canonical PCPs of proximity for membership in any linear code with constant rate and relative distance. Loosely speaking, these are PCPs of proximity wherein the verifier is proximity oblivious (similarly to strong-LTCs) and every valid statement has a unique canonical proof. Furthermore, the verifier is required to reject non-canonical proofs (even for valid statements). As an application, we improve the best known separation result between the complexity of decision and verification in the setting of property testing. © 2019 Association for Computing Machinery.",Error correcting codes; Local decodability; Local testability,Decoding; Error correction; Recovery; Binary linear codes; Error correcting code; Local decodability; Locally testable codes; Locally-decodable codes; Natural relaxation; Relative distances; Testability; Query processing
Beyond worst-case (in)approximability of nonsubmodular influence maximization,2019,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065847806&doi=10.1145%2f3313904&partnerID=40&md5=c55c5a250e2c90ff1408516be27f903d,"We consider the problem of maximizing the spread of influence in a social network by choosing a fixed number of initial seeds, formally referred to as the influence maximization problem. It admits a (1 − 1/e)-factor approximation algorithm if the influence function is submodular. Otherwise, in the worst case, the problem is NP-hard to approximate to within a factor of N1−ε. This article studies whether this worst-case hardness result can be circumvented by making assumptions about either the underlying network topology or the cascade model. All our assumptions are motivated by many real-life social network cascades. First, we present strong inapproximability results for a very restricted class of networks called the (stochastic) hierarchical blockmodel, a special case of the well-studied (stochastic) blockmodel in which relationships between blocks admit a tree structure. We also provide a dynamic-programming-based polynomial time algorithm, which optimally computes a directed variant of the influence maximization problem on hierarchical blockmodel networks. Our algorithm indicates that the inapproximability result is due to the bidirectionality of influence between agent-blocks. Second, we present strong inapproximability results for a class of influence functions that are “almost” submodular, called 2-quasi-submodular. Our inapproximability results hold even for any 2-quasi-submodular f fixed in advance. This result also indicates that the “threshold” between submodularity and nonsubmodularity is sharp, regarding the approximability of influence maximization. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",(Non)submodular optimization; Approximability; Influence maximization,Approximation algorithms; Method of moments; Polynomial approximation; Stochastic systems; Subsidence; Topology; Trees (mathematics); Approximability; Factor approximation algorithms; Inapproximability; Influence functions; Influence maximizations; Polynomial-time algorithms; Submodular optimizations; Underlying networks; Dynamic programming
On the entropy loss and gap of condensers,2019,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065832331&doi=10.1145%2f3317691&partnerID=40&md5=0abb4fab9314c497b888c417a63a7c60,"Many algorithms are proven to work under the assumption that they have access to a source of random, uniformly distributed bits. However, in practice, sources of randomness are often imperfect, giving n random bits that have only k < n min-entropy. The value n − k is called the entropy gap of the source. Randomness condensers are hash functions that hash any such source to a shorter source with reduced entropy gap д. The goal is to lose as little entropy as possible in this process. Condensers also have an error parameter ε and use a small seed of uniformly distributed bits whose length we desire to minimize as well. In this work, we study the exact dependencies between the different parameters of seeded randomness condensers. We obtain a non-explicit upper bound, showing the existence of condensers with entropy loss log(1 + logдε1 ) + O(1) and seed length log(nεд−k ) + O(1). In particular, this implies the existence of condensers with O(log ε1 ) entropy gap and constant entropy loss. This extends (with slightly improved parameters) the non-explicit upper bound for condensers presented in the work of Dodis et al. (2014), which gives condensers with entropy loss at least log log ε1 . We also give a non-explicit upper bound for lossless condensers, which have entropy gap д ≥ logεε1 + O(1) and seed length log(nε−2дk ) + O(1). Furthermore, we address an open question raised in (Dodis et al. 2014), where Dodis et al. showed an explicit construction of condensers with constant gap and O(log log ε1 ) loss, using seed length O(n log ε1 ). In the same article they improve the seed length to O(k log k) and ask whether it can be further improved. In this work, we reduce the seed length of their construction to O(log(nε ) log(kε )) by a simple concatenation. In the analysis, we use and prove a tight equivalence between condensers and extractors with multiplicative error. We note that a similar, but non-tight, equivalence was already proven by Dodis et al. (Dodis et al. 2014) using a weaker variant of extractors called unpredictability extractors. We also remark that this equivalence underlies the work of Ben-Aroya et al. (Ben-Aroya et al. 2016) and later work on explicit two-source extractors, and we believe it is interesting in its own right. © 2019 Association for Computing Machinery.",Entropy gap; Entropy loss; Key derivation; Randomness condensers; Randomness extractors; Unpredictability extractors,Hash functions; Random processes; Entropy loss; Error parameters; Explicit constructions; Key derivations; Multiplicative errors; Randomness extractors; Reduced entropy; Unpredictability extractors; Entropy
Read-once branching programs for tree evaluation problems,2018,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057747161&doi=10.1145%2f3282433&partnerID=40&md5=f83d85ab52672f8d5827733257d9e618,"Toward the ultimate goal of separating L and P, Cook, McKenzie, Wehr, Braverman, and Santhanam introduced the Tree Evaluation Problem (TEP). For fixed integers h and k > 0, FTh (k) is given as a complete, rooted binary tree of height h, in which each root node is associated with a function from [k]2 to [k], and each leaf node with a number in [k]. The value of an internal node ν is defined naturally; that is, if it has a function f and the values of its two child nodes are a and b, then the value of ν is f (a,b). Our task is to compute the value of the root node by sequentially executing this function evaluation in a bottom-up fashion. The problem is obviously in P, and, if we could prove that any branching program solving FTh (k) needs at least kr (h) states for any unbounded function r , then this problem is not in L, thus achieving our goal. The mentioned authors introduced a restriction called thrifty against the structure of BP's (i,e., against the algorithm for solving the problem) and proved that any thrifty BP needs ω(kh ) states. This article proves a similar lower bound for read-once branching programs, which allows us to get rid of the restriction on the order of nodes read by the BP that is the nature of the thrifty restriction. © 2018 Association for Computing Machinery.",Branching programs; Lower bounds; Read-once branching programs; Space complexity; Tree evaluation problem,Binary trees; Fluorine compounds; Function evaluation; Thorium compounds; Branching programs; Lower bounds; Read-once branching programs; Space complexity; Tree evaluations; Problem solving
Surjective H-colouring over reflexive digraphs,2018,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057763340&doi=10.1145%2f3282431&partnerID=40&md5=cdd75bdbbc05d61758dcaa108f282aea,"The Surjective H-Colouring problem is to test if a given graph allows a vertex-surjective homomorphism to a fixed graph H. The complexity of this problem has been well studied for undirected (partially) reflexive graphs.We introduce endo-triviality, the property of a structure that all of its endomorphisms that do not have range of size 1 are automorphisms, as a means to obtain complexity-theoretic classifications of Surjective H-Colouring in the case of reflexive digraphs. Chen (2014) proved, in the setting of constraint satisfaction problems, that Surjective H-Colouring is NP-complete if H has the property that all of its polymorphisms are essentially unary. We give the first concrete application of his result by showing that every endo-trivial reflexive digraph H has this property.We then use the concept of endo-triviality to prove, as our main result, a dichotomy for SurjectiveH-Colouring whenHis a reflexive tournament: ifHis transitive, then Surjective H-Colouring is in NL; otherwise, it is NP-complete. By combining this result with some known and new results, we obtain a complexity classification for Surjective H-Colouring when H is a partially reflexive digraph of size at most 3. © 2018 Association for Computing Machinery.",Algorithmic graph theory; Computational complexity; Constraint satisfaction; Surjective H-Coloring; Universal algebra,Computational complexity; Constraint satisfaction problems; Directed graphs; Parallel processing systems; Automorphisms; Concrete applications; Constraint Satisfaction; Fixed graphs; New results; NP Complete; Surjective; Universal algebra; Graph theory
Reconstruction of full rank algebraic branching programs,2018,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057717645&doi=10.1145%2f3282427&partnerID=40&md5=e80cbf624e58f5b41a1feb7eb943e492,"An algebraic branching program (ABP) A can be modelled as a product expression X1 · X2 . . .Xd, where X1 and Xd are 1 × w and w × 1 matrices, respectively, and every other Xk is a w × w matrix; the entries of these matrices are linear forms in m variables over a field F (which we assume to be either Q or a field of characteristic poly(m)). The polynomial computed by A is the entry of the 1 × 1 matrix obtained from the product Πdk=1Xk. We say A is a full rank ABP if the w2 (d - 2) + 2w linear forms occurring in the matrices X1,X2, . . . ,Xd are F-linearly independent. Our main result is a randomized reconstruction algorithm for full rank ABPs: Given blackbox access to anm-variate polynomial f of degree at mostm, the algorithm outputs a full rank ABP computing f if such an ABP exists, or outputs ""no full rank ABP exists"" (with high probability). The running time of the algorithm is polynomial inm and β, where β is the bit length of the coefficients of f . The algorithm works even if Xk is a wk-1 × wk matrix (with w0 = wd = 1), and w = (W1, . . . ,wd-1) is unknown. The result is obtained by designing a randomized polynomial time equivalence test for the family of iterated matrix multiplication polynomial IMMw,d , the (1, 1)-th entry of a product of d rectangular symbolic matrices whose dimensions are according to w ∈ Nd-1. At its core, the algorithm exploits a connection between the irreducible invariant subspaces of the Lie algebra of the group of symmetries of a polynomial f that is equivalent toIMMw,d and the ""layer spaces"" of a full rank ABP computing f . This connection also helps determine the group of symmetries of IMMw,d and show that IMMw,d is characterized by its group of symmetries. © 2018 Association for Computing Machinery.",Equivalence testing; Group of symmetries; Iterated matrix multiplication; Layer spaces; Lie algebra,Equivalence classes; Lie groups; Polynomial approximation; Product design; Equivalence testing; Group of symmetries; Layer spaces; Lie Algebra; MAtrix multiplication; Matrix algebra
The complexity of boolean surjective general-valued CSPs,2018,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057767816&doi=10.1145%2f3282429&partnerID=40&md5=ef546173fafeaec6f6cdd17720236639,"Valued constraint satisfaction problems (VCSPs) are discrete optimisation problems with a (Q ∪ {∞})-valued objective function given as a sum of fixed-arity functions. In Boolean surjective VCSPs, variables take on labels from D = {0, 1}, and an optimal assignment is required to use both labels from D. Examples include the classical global Min-Cut problem in graphs and the Minimum Distance problem studied in coding theory. We establish a dichotomy theorem and thus give a complete complexity classification of Boolean surjective VCSPs with respect to exact solvability. Our work generalises the dichotomy for {0, ∞}-valued constraint languages (corresponding to surjective decision CSPs) obtained by Creignou and Hébrard. For the maximisation problem of Q≥0-valued surjective VCSPs, we also establish a dichotomy theorem with respect to approximability. Unlike in the case of Boolean surjective (decision) CSPs, there appears a novel tractable class of languages that is trivial in the non-surjective setting. This newly discovered tractable class has an interesting mathematical structure related to downsets and upsets. Our main contribution is identifying this class and proving that it lies on the borderline of tractability. A crucial part of our proof is a polynomial-time algorithm for enumerating all near-optimal solutions to a generalised Min-Cut problem, which might be of independent interest. © 2018 Association for Computing Machinery.",Constraint satisfaction problems; Min-cut; Multimorphisms; Polymorphisms; Surjective CSP; Valued CSP,Computer programming; Optimization; Polymorphism; Polynomial approximation; Min-cut; Minimum distance problems; Multimorphisms; Near-optimal solutions; Polynomial-time algorithms; Surjective; Valued constraint satisfaction problems; Valued CSP; Constraint satisfaction problems
A universal tree balancing theorem,2018,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055820657&doi=10.1145%2f3278158&partnerID=40&md5=aefe8678747d1cf384351cdb445fae00,"We present a general framework for balancing expressions (terms) in the form of so-called tree straight-line programs. The latter can be seen as circuits over the free term algebra extended by contexts (terms with a hole) and the operations, which insert terms/contexts into contexts. In Ref. [16], it was shown that one can compute for a given term of size n in logspace a tree straight-line program of depth O(log n) and size O(n/ log n). In the present article, it is shown that the conversion can be done in DLOGTIME-uniform TC0. This allows reducing the term evaluation problem over an arbitrary algebra A to the term evaluation problem over a derived two-sorted algebra F (A). Three applications are presented: (i) an alternative proof for a recent result by Krebs et al. [25] on the expression evaluation problem is given; (ii) it is shown that expressions for an arbitrary (possibly non-commutative) semiring can be transformed in DLOGTIME-uniform TC0 into equivalent circuits of logarithmic depth and size O(n/ log n); and, (iii) a corresponding result for regular expressions is shown. © 2018 Association for Computing Machinery.",Depth reduction; Tree contraction; Tree evaluation,Algebra; Computer circuits; Equivalent circuits; Evaluation problems; Free term; Non-commutative; Regular expressions; Semi-ring; Straight line program; Tree contraction; Tree evaluations; Forestry
Streaming communication protocols,2018,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061178670&doi=10.1145%2f3276748&partnerID=40&md5=9edf399b63c5b43eee0a28ec9e1198a8,"We defne the Streaming Communication model that combines the main aspects of communication complexity and streaming. Input arrives as a stream, spread between several agents across a network. Each agent has a bounded memory, which can be updated upon receiving a new bit, or a message from another agent. We provide tight tradeoffs between the necessary resources, i.e., communication between agents and memory, for some of the canonical problems from communication complexity by proving a strong general lower bound technique. Second, we analyze the Approximate Matching problem and show that the complexity of this problem (i.e., the achievable approximation ratio) in the one-way variant of our model is strictly different both from the streaming complexity and the one-way communication complexity thereof. © 2018 Association for Computing Machinery.",Approximate matching; Communication complexity; Networks; Streaming algorithms,Computational complexity; Networks (circuits); Approximate matching; Approximation ratios; Canonical problems; Communication between agents; Communication complexity; Communication modeling; Lower bound techniques; Streaming algorithm; Complex networks
Simultaneous feedback vertex set: A parameterized perspective,2018,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053891680&doi=10.1145%2f3265027&partnerID=40&md5=080ecbad75fcbb13296310fcfa01c26b,"For a family F of graphs, a graph G, and a positive integer k, the F-Deletion problem asks whether we can delete at most k vertices from G to obtain a graph in F. F-Deletion generalizes many classical graph problems such as Vertex Cover, Feedback Vertex Set, and Odd Cycle Transversal. For an integer α ≥ 1, an n-vertex (multi) graph G = (V, ∪i=1                             α Ei), where the edge set of G is partitioned into α color classes, is called an α-edge-colored (multi) graph. A natural extension of the F-Deletion problem to edge-colored graphs is the Simultaneous F-Deletion problem. In the latter problem, we are given an α-edge-colored graph G and the goal is to find a set S of at most k vertices such that each graph Gi - S, where Gi = (V, Ei) and 1 ≤ i ≤ α, is in F. In this work, we study Simultaneous F-Deletion for F being the family of forests. In other words, we focus on the Simultaneous Feedback Vertex Set (SimFVS) problem. Algorithmically, we show that, like its classical counterpart, SimFVS parameterized by k is fixed-parameter tractable (FPT) and admits a polynomial kernel, for any fixed constant α. In particular, we give an algorithm running in 2O(αk)nO(1) time and a kernel with O(αk3(α+1)) vertices. The running time of our algorithm implies that SimFVS is FPT even when α ∈ o(log n). We complement this positive result by showing that if we allow α to be in O(log n), where n is the number of vertices in the input graph, then SimFVS becomes W[1]-hard. In particular, when α is roughly equal to c log n, for a non-zero positive constant c, the problem becomes W[1]-hard. Our positive results answer one of the open problems posed by Cai and Ye (MFCS 2014). © 2018 Association for Computing Machinery.",Edge-colored graphs; Feedback vertex set; Kernel; Parameterized complexity,Parameterization; Classical counterpart; Edge colored graphs; Feedback vertex set; Kernel; Odd cycle transversals; Parameterized complexity; Polynomial kernels; Positive constant; Graph theory
"Uniqueness, spatial mixing, and approximation for ferromagnetic 2-spin systems",2018,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053912486&doi=10.1145%2f3265025&partnerID=40&md5=5698a59185cdf410b582e8f82a1a6ad6,"We give fully polynomial-time approximation schemes (FPTAS) for the partition function of ferromagnetic 2-spin systems in certain parameter regimes. The threshold we obtain is almost tight up to an integrality gap. Our technique is based on the correlation decay framework. The main technical contribution is a new potential function, with which we establish a new kind of spatial mixing. © 2018 Association for Computing Machinery.",Approximate counting; Correlation decay; Spin systems,Ferromagnetic materials; Ferromagnetism; Mixing; Approximate counting; Fully polynomial time approximation schemes; Integrality gaps; Parameter regimes; Partition functions; Potential function; Spin systems; Technical contribution; Polynomial approximation
Circuits and expressions over finite semirings,2018,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053554376&doi=10.1145%2f3241375&partnerID=40&md5=c8446b63407c119d14a4404017905451,"The computational complexity of the circuit and expression evaluation problem for finite semirings is considered, where semirings are not assumed to have an additive or a multiplicative identity. The following dichotomy is shown: If a finite semiring is such that (i) the multiplicative semigroup is solvable and (ii) it does not contain a subsemiring with an additive identity 0 and a multiplicative identity 1 0, then the circuit evaluation problem is in DET ⊆ NC2, and the expression evaluation problem for the semiring is in TC0. For all other finite semirings, the circuit evaluation problem is P-complete and the expression evaluation problem is NC1-complete. As an application, we determine the complexity of intersection non-emptiness problems for given context-free grammars (regular expressions) with a fixed regular language. © 2018 ACM 1942-3454/2018/08-ART15 $15.00",Circuit evaluation problem; Expression evaluation; Semirings,Computer programming languages; Context free grammars; Timing circuits; Circuit evaluation; Emptiness problem; Evaluation problems; Expression evaluation; P-complete; Regular expressions; Semi-group; Semirings; Context free languages
Property testing of joint distributions using conditional samples,2018,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052818120&doi=10.1145%2f3241377&partnerID=40&md5=80d48dfd10ded1e896450edbf889a4ae,"In this article, we consider the problem of testing properties of joint distributions under the Conditional Sampling framework. In the standard sampling model, sample complexity of testing properties of joint distributions are exponential in the dimension, resulting in inefficient algorithms for practical use. While recent results achieve efficient algorithms for product distributions with significantly smaller sample complexity, no efficient algorithm is expected when the marginals are not independent. In this article, we initialize the study of conditional sampling in the multidimensional setting. We propose a subcube conditional sampling model where the tester can condition on a (adaptively) chosen subcube of the domain. Due to its simplicity, this model is potentially implementable in many practical applications, particularly when the distribution is a joint distribution over Σn for some set Σ. We present algorithms for various fundamental properties of distributions in the subcube-conditioning model and prove that the sample complexity is polynomial in the dimension n (and not exponential as in the traditional model). We present an algorithm for testing identity to a known distribution using Õ(n2)subcube-conditional samples, an algorithm for testing identity between two unknown distributions using Õ(n5)-subcube-conditional samples and an algorithm for testing identity to a product distribution using Õ(n5)-subcube-conditional samples. The central concept of our technique involves an elegant chain rule, which can be proved using basic techniques of probability theory, yet it is powerful enough to avoid the curse of dimensionality. © 2018 ACM",Conditional sampling; Distribution testing; Subcube conditioning,Probability; Conditional sampling; Curse of dimensionality; Distribution testing; Fundamental properties; Joint distributions; Multi-dimensional settings; Product distributions; Traditional models; Sampling
Complete derandomization of identity testing and reconstruction of read-once formulas,2018,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053783194&doi=10.1145%2f3196836&partnerID=40&md5=5fb0406d003ac7b57add28febcf0c6e6,"In this article,we study the identity testing problem of arithmetic read-once formulas (ROFs) and some related models. An ROF is a formula (a circuit whose underlying graph is a tree) in which the operations are {+, ×} and such that every input variable labels at most one leaf. We obtain the first polynomial-time deterministic identity testing algorithm that operates in the black-box setting for ROFs, as well as some other related models. As an application, we obtain the first polynomial-time deterministic reconstruction algorithm for such formulas. Our results are obtained by improving and extending the analysis of the algorithm of Shpilka and Yolkovich [51]. 7copy; 2018 ACM. © 2018 ACM.",Arithmetic circuit; Arithmetic formula; Circuit reconstruction; Derandomization; Identity testing,Polynomial approximation; Trees (mathematics); Arithmetic circuit; Arithmetic formulas; Circuit reconstruction; Derandomization; Identity testing; Black-box testing
Invariance principle on the slice,2018,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053792206&doi=10.1145%2f3186590&partnerID=40&md5=489b21d7427bf2f5d86b4dd3c6475d6b,"The non-linear invariance principle of Mossel, O'Donnell, and Oleszkiewicz establishes that if f (x1, . . . , xn) is a multilinear low-degree polynomial with low influences, then the distribution of f (B1, . . . , Bn ) is close (in various senses) to the distribution of f (G1, . . . , Gn ), where Bi ∈R{-1, 1} are independent Bernoulli random variables and Gi ~ N(0, 1) are independent standard Gaussians. The invariance principle has seen many applications in theoretical computer science, including the Majority is Stablest conjecture, which shows that the Goemans-Williamson algorithm for MAX-CUT is optimal under the Unique Games Conjecture. More generally, MOO's invariance principle works for any two vectors of hypercontractive random variables (x1, . . . , xn ), (Y1, . . . ,Yn ) such that (i) Matching moments: Xi and Yi have matching first and second moments and (ii) Independence: the variablesx1, . . . , xn are independent, as are Y1, . . . ,Yn. The independence condition is crucial to the proof of the theorem, yet in some cases we would like to use distributions (x1, . . . , xn ) in which the individual coordinates are not independent. A common example is the uniform distribution on the slice([n]                             k which consists of all vectors (x1, . . . , xn ) ∈ {0, 1}n with Hamming weight k. The slice shows up in theoretical computer science (hardness amplification, direct sum testing), extremal combinatorics (Erdös-Ko-Rado theorems), and coding theory (in the guise of the Johnson association scheme). Our main result is an invariance principle in which (X1, . . . ,Xn ) is the uniform distribution on a slice([n]                             pn) and (Y1, . . . ,Yn ) consists either of n independent Ber(p) random variables, or of n independent N(p,p(1 - p)) random variables. As applications, we prove a version of Majority is Stablest for functions on the slice, a version of Bourgain's tail theorem, a version of the Kindler-Safra structural theorem, and a stability version of the t-intersecting Erdös-Ko-Rado theorem, combining techniques of Wilson and Friedgut. Our proof relies on a combination of ideas from analysis and probability, algebra, and combinatorics. In particular, we make essential use of recent work of the first author which describes an explicit Fourier basis for the slice. © 2018 ACM.",Analysis of Boolean functions; Invariance principle; Johnson scheme; Slice,Combinatorial mathematics; Computer programming; Random variables; Extremal combinatorics; Hardness amplification; Independent Bernoulli random variables; Invariance principle; Johnson scheme; Slice; Theoretical computer science; Unique games conjecture; Computer games
The limits of SDP relaxations for general-valued CSPs                     ,2018,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053813230&doi=10.1145%2f3201777&partnerID=40&md5=028859878bfe2704e612eb9c61a0600c,"It has been shown that for a general-valued constraint language G the following statements are equivalent: (1) any instance of VCSP(G) can be solved to optimality using a constant level of the Sherali-Adams LP hierarchy, (2) any instance of VCSP(G) can be solved to optimality using the third level of the Sherali-Adams LP hierarchy, and (3) the support of G satisfies the ""bounded width condition"" (i.e., it containsweak near-unanimity operations of all arities). We show that if the support of G violates the bounded width condition then not only is VCSP(G) not solved by a constant level of the Sherali-Adams LP hierarchy, but it also requires linear levels of the Lasserre SDP hierarchy (also known as the sum-of-squares SDP hierarchy). For G corresponding to linear equations in an Abelian group, this result follows from existing work on inapproximability of Max-CSPs. By a breakthrough result of Lee, Raghavendra, and Steurer [STOC'15], our result implies that for any G whose support violates the bounded width condition no SDP relaxation of polynomial size solves VCSP(G). We establish our result by proving that various reductions preserve exact solvability by the Lasserre SDP hierarchy (up to a constant factor in the level of the hierarchy). Our results hold for general-valued constraint languages (i.e., sets of functions on a fixed finite domain that take on rational or infinite values) and thus also hold in notable special cases of {0, ∞}-valued languages (CSPs), {0, 1}-valued languages (Min-CSPs/Max- CSPs), and Q-valued languages (finite-valued CSPs). © 2018 ACM.",Convex relaxations; Discrete optimization; Lasserre hierarchy; SDP; Sum of squares; Valued constraint satisfaction problems,Constraint theory; Group theory; Optimization; Rational functions; Relaxation processes; Convex relaxation; Discrete optimization; Lasserre hierarchy; Sum of squares; Valued constraint satisfaction problems; Constraint satisfaction problems
"Directed multicut is W[1]-hard, even for four terminal pairs",2018,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053812024&doi=10.1145%2f3201775&partnerID=40&md5=96c2ca180e5c70b0b0a3a75a566d93dd,"We prove that Multicut in directed graphs, parameterized by the size of the cutset, isW[1]-hard and hence unlikely to be fixed-parameter tractable even if restricted to instances with only four terminal pairs. This negative result almost completely resolves one of the central open problems in the area of parameterized complexity of graph separation problems, posted originally by Marx and Razgon [SIAM J. Comput. 43(2):355- 388 (2014)], leaving only the case of three terminal pairs open. The case of two terminal pairs was shown to be FPT by Chitnis et al. [SIAM J. Comput. 42(4):1674-1696 (2013)]. Our gadget methodology also allows us to prove W[1]-hardness of the Steiner Orientation problem parameterized by the number of terminal pairs, resolving an open problem of Cygan, Kortsarz, and Nutov [SIAM J. Discrete Math. 27(3):1503-1513 (2013)]. © 2018 ACM.",Directed multicut; Directed treewidth; Fixed-parameter tractability; Steiner orientation,Parameterization; Fixed-parameter tractability; Multicuts; Parameterized; Parameterized complexity; Separation problems; Steiner; Tree-width; Two terminals; Directed graphs
The coin problem for product tests,2018,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053760538&doi=10.1145%2f3201787&partnerID=40&md5=49b016592e77325a9a4537c621cbde42,"Let Xm                             e be the distribution over m bits (X1, . . . ,Xm), where the Xi are independent and each Xi equals 1 with probability (1 + ∈ )/2 and 0 with probability (1 - ∈ )/2. We consider the smallest value ∈∗ of ∈ such that the distributions Xm                             e and Xm                             0 can be distinguished with constant advantage by a function f : {0, 1}m → S, which is the product of k functions f1, f2, . . . , fk on disjoint inputs of n bits, where each fi : {0, 1}n → S and m = nk. We prove that ∈∗ = Θ(1/n log k) if S = [-1, 1], while ∈∗ = Θ(1/nk) if S is the set of unit-norm complex numbers. © 2018 ACM.",Coin problem; Product test,Computation theory; Computer science; Coin problem; Complex number; K-function; Computational methods
Ground state connectivity of local Hamiltonians,2018,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064536631&doi=10.1145%2f3186587&partnerID=40&md5=61ecd9ece1848f664eb406d7955e6b14,"The study of ground state energies of local Hamiltonians has played a fundamental role in quantum complexity theory. In this article, we take a new direction by introducing the physically motivated notion of “ground state connectivity” of local Hamiltonians, which captures problems in areas ranging from quantum stabilizer codes to quantum memories. Roughly, “ground state connectivity” corresponds to the natural question: Given two ground states |ψ and |ϕ of a local Hamiltonian H, is there an “energy barrier” (with respect to H) along any sequence of local operations mapping |ψ to |ϕ? We show that the complexity of this question can range from QCMA-complete to PSPACE-complete, as well as NEXP-complete for an appropriately defined “succinct” version of the problem. As a result, we obtain a natural QCMA-complete problem, a goal which has generally proven difficult since the conception of QCMA over a decade ago. Our proofs rely on a new technical tool, the Traversal Lemma, which analyzes the Hilbert space a local unitary evolution must traverse under certain conditions. We show that this lemma is essentially tight with respect to the length of the unitary evolution in question. © 2018 ACM",Ground state connectivity; Local Hamiltonian; Quantum Hamiltonian complexity; Reconfiguration problem,Ground state; Quantum optics; Complete problems; Ground-state energies; Local hamiltonians; Local operations; Quantum complexity; Quantum Hamiltonians; Quantum stabilizer codes; Reconfiguration problems; Hamiltonians
The weakness of CTC qubits and the power of approximate counting,2018,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064570296&doi=10.1145%2f3196832&partnerID=40&md5=d91c4a5b66e0d8b00017fb108b64722f,"We present results in structural complexity theory concerned with the following interrelated topics: computation with postselection/restarting, closed timelike curves (CTCs), and approximate counting. The first result is a new characterization of the lesser known complexity class BPPpath in terms of more familiar concepts. Precisely, BPPpath is the class of problems that can be efficiently solved with a nonadaptive oracle for the approximate counting problem. Similarly, PP equals the class of problems that can be solved efficiently with nonadaptive queries for the related approximate difference problem. Another result is concerned with the computational power conferred by CTCs, or equivalently, the computational complexity of finding stationary distributions for quantum channels. Using the preceding characterization of PP, we show that any poly(n)time quantum computation using a CTC of O(log n) qubits may as well just use a CTC of 1 classical bit. This result essentially amounts to showing that one can find a stationary distribution for a poly(n)-dimensional quantum channel in PP. © 2018 ACM",Approximate counting; BPP-path; Closed timelike curves; Postselection; Quantum channels; Quantum computation,Computational complexity; Quantum channel; Quantum computers; Quantum entanglement; Qubits; Approximate counting; BPP-path; Closed timelike curves; Complexity class; Computational power; Postselection; Stationary distribution; Structural complexity; Communication channels (information theory)
Hardness of approximation for H-free edge modification problems,2018,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064533831&doi=10.1145%2f3196834&partnerID=40&md5=2445f25e34f3a3a4d967dd934fa6eb68,"The H-free Edge Deletion problem asks, for a given graph G and integer k, whether it is possible to delete at most k edges from G to make it H-free—that is, not containing H as an induced subgraph. The H-free Edge Completion problem is defined similarly, but we add edges instead of deleting them. The study of these two problem families has recently been the subject of intensive studies from the point of view of parameterized complexity and kernelization. In particular, it was shown that the problems do not admit polynomial kernels (under plausible complexity assumptions) for almost all graphs H, with several important exceptions occurring when the class of H-free graphs exhibits some structural properties. In this work, we complement the parameterized study of edge modification problems to H-free graphs by considering their approximability. We prove that whenever H is 3-connected and has at least two nonedges, then both H-free Edge Deletion and H-free Edge Completion are very hard to approximate: they do not admit poly(OPT)-approximation in polynomial time, unless P = NP, or even in time subexponential in OPT, unless the exponential time hypothesis fails. The assumption of the existence of two nonedges appears to be important: we show that whenever H is a complete graph without one edge, then H-free Edge Deletion is tightly connected to the Min Horn Deletion problem, whose approximability is still open. Finally, in an attempt to extend our hardness results beyond 3-connected graphs, we consider the cases of H being a path or a cycle, and we achieve an almost complete dichotomy there. © 2018 ACM.",Exponential time hypothesis; Fixed-parameter tractability; Graph modification problems; H-free graphs; Hardness of approximation,Graphic methods; Hardness; Polynomial approximation; Exponential time hypothesis; Fixed-parameter tractability; Graph modification problems; H-free graphs; Hardness of approximation; Graph theory
Corrigendum to Affine Relativization: Unifying the Algebrization and Relativization Barriers (ACM Transactions on Computation Theory (2018) 10:1 (1:1–1:67) 10.1145/3170704),2018,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065837635&doi=10.1145%2f3317693&partnerID=40&md5=9440abe11c7db99c4e8570f2661cac26,"It has come to our attention that in Reference [2], Theorem 8, and its continuation, Theorem 10, are proven using an argument that contradicts a theorem of Löb [3] (see Reference [1, Theorem 1] for an accessible account). Therefore, we retract these assertions. All of the other theorems/lemmas/propositions in the article remain valid, because they do not depend on the retracted assertions. The only implication of the retraction is that the reader cannot automatically conclude, from a theorem of the form “ψ relativizes (affinely),” that ψ has a proof that relativizes (affinely). In particular, the assertion in the “Summary of Results”, that certain statements have proofs that relativize affinely, does not automatically follow from the theorems given in Section 6. That assertion is still correct; however, because whenever we prove a statement of the form “ψ relativizes (affinely)” in the article, in particular in Section 6, we do in fact give a proof of ψ that relativizes (affinely). We thank Moritz Müller for pointing out this error. © 2019 Association for Computing Machinery.",Algebrization; Barriers; Coding theory; Interactive proofs; Relativization,
Tight running time lower bounds for vertex deletion problems,2018,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064542436&doi=10.1145%2f3186589&partnerID=40&md5=9c7e6b596deba7d83db1518c23f24d4f,"For a graph class Π, the Π-Vertex Deletion problem has as input an undirected graph G = (V, E) and an integer k and asks whether there is a set of at most k vertices that can be deleted from G such that the resulting graph is a member of Π. By a classic result of Lewis and Yannakakis [17], Π-Vertex Deletion is NP-hard for all hereditary properties Π. We adapt the original NP-hardness construction to show that under the exponential time hypothesis (ETH), tight complexity results can be obtained. We show that Π-Vertex Deletion does not admit a 2o(n)-time algorithm where n is the number of vertices in G. We also obtain a dichotomy for running time bounds that include the number m of edges in the input graph. On the one hand, if Π contains all edgeless graphs, then there is no 2o(n+m)-time algorithm for Π-Vertex Deletion. On the other hand, if there is a fixed edgeless graph that is not contained in Π and containment in Π can be determined in 2O(n) time or 2o(m) time, then Π-Vertex Deletion can be solved in 2O(m) + O(n) or 2o(m) + O(n) time, respectively. We also consider restrictions on the domain of the input graph G. For example, we obtain that Π-Vertex Deletion cannot be solved in 2o(n) time if G is planar and Π is hereditary and contains and excludes infinitely many planar graphs. Finally, we provide similar results for the problem variant where the deleted vertex set has to induce a connected graph. © 2018 ACM.",Exponential time hypothesis; Fine-grain complexity analysis; Graph properties,Computation theory; Computational methods; Computer science; Complexity analysis; Complexity results; Connected graph; Exponential time hypothesis; Graph properties; Hereditary property; Undirected graph; Vertex deletion problems; Graph theory
"Algorithmic information, plane kakeya sets, and conditional dimension",2018,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053197896&doi=10.1145%2f3201783&partnerID=40&md5=e8dea3e7e9d6814c82f7ba625457240d,"We formulate the conditional Kolmogorov complexity of x given y at precision r, where x and y are points in Euclidean spaces and r is a natural number. We demonstrate the utility of this notion in two ways; (1) We prove a point-to-set principle that enables one to use the (relativized, constructive) dimension of a single point in a set E in a Euclidean space to establish a lower bound on the (classical) Hausdorff dimension of E. We then use this principle, together with conditional Kolmogorov complexity in Euclidean spaces, to give a new proof of the known, two-dimensional case of the Kakeya conjecture. This theorem of geometric measure theory, proved by Davies in 1971, says that every plane set containing a unit line segment in every direction has Hausdorff dimension 2. (2) We use conditional Kolmogorov complexity in Euclidean spaces to develop the lower and upper conditional dimensions dim(x|y) and Dim(x|y) of x given y, where x and y are points in Euclidean spaces. Intuitively, these are the lower and upper asymptotic algorithmic information densities of x conditioned on the information in y. We prove that these conditional dimensions are robust and that they have the correct information-theoretic relationships with the well-studied dimensions dim(x) and Dim(x) and the mutual dimensions mdim(x : y) and Mdim(x : y) . © 2018 ACM",Effective dimension; Kakeya sets; Kolmogorov complexity,Fractals; Information theory; Algorithmic informations; Effective dimensions; Euclidean spaces; Geometric measure theory; Hausdorff dimension; Kakeya sets; Kolmogorov complexity; Natural number; Computational complexity
Randomized communication versus partition number,2018,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042503748&doi=10.1145%2f3170711&partnerID=40&md5=bce523ec370620145ff5e4bcaa38d5bf,"We show that randomized communication complexity can be superlogarithmic in the partition number of the associated communication matrix, and we obtain near-optimal randomized lower bounds for the Clique versus Independent Set problem. These results strengthen the deterministic lower bounds obtained in prior work (Göös, Pitassi, and Watson, FOCS'15). One of our main technical contributions states that information complexity when the cost is measured with respect to only 1-inputs (or only 0-inputs) is essentially equivalent to information complexity with respect to all inputs. © 2018 ACM.",Communication; Number; Partition; Randomized,Communication; Computation theory; Computational methods; Computer science; Partitions (building); Communication complexity; Communication matrixes; Independent set problems; Information complexity; Number; Partition numbers; Randomized; Technical contribution; Computational complexity
Identity testing and lower bounds for read-k oblivious algebraic branching programs,2018,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040777590&doi=10.1145%2f3170709&partnerID=40&md5=d33af10a5da3605a007424d0004e6bb9,"Read-k oblivious algebraic branching programs are a natural generalization of the well-studied model of read-once oblivious algebraic branching program (ABP). In this work, we give an exponential lower bound of exp(n/kO(k) ) on the width of any read-k oblivious ABP computing some explicit multilinear polynomial f that is computed by a polynomial-size depth-3 circuit. We also study the polynomial identity testing (PIT) problem for this model and obtain a white-box subexponential-Time PIT algorithm. The algorithm runs in time 2 O (n1-1/2k-1 ) and needs white box access only to know the order in which the variables appear in the ABP.",Algebraic circuits; Algebraic complexity theory; Lower bounds; Polynomial identity testing,Polynomials; Algebraic branching programs; Algebraic circuits; Algebraic complexity theories; Lower bounds; Multilinear polynomials; Natural generalization; Polynomial identity testing; Subexponential time; Algebra
Affine relativization: Unifying the algebrization and relativization barriers,2018,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042500038&doi=10.1145%2f3170704&partnerID=40&md5=0e6c42822fc0f7dede98f8c3f999bda1,"We strengthen existing evidence for the so-called ""algebrization barrier."" Algebrization - short for algebraic relativization - was introduced by Aaronson and Wigderson (AW) (STOC 2008) to characterize proofs involving arithmetization, simulation, and other ""current techniques."" However, unlike relativization, eligible statements under this notion do not seem to have basic closure properties, making it conceivable to take two proofs, both with algebrizing conclusions, and combine them to get a proof without. Further, the notion is undefined for most types of statements and does not seem to yield a general criterion by which we can tell, given a proof, whether it algebrizes. In fact, the very notion of an algebrizing proof is never made explicit, and casual attempts to define it are problematic. All these issues raise the question of what evidence, if any, is obtained by knowing whether some statement does or does not algebrize. We give a reformulation of algebrization without these shortcomings. First, we define what it means for any statement/proof to hold relative to any language, with no need to refer to devices like a Turing machine with an oracle tape. Our approach dispels the widespread misconception that the notion of oracle access is inherently tied to a computational model.We also connect relativizing statements to proofs, by showing that every proof that some statement relativizes is essentially a relativizing proof of that statement. We then define a statement/proof as relativizing affinely if it holds relative to every affine oracle - here an affine oracle is the result of a particular error correcting code applied to the characteristic string of a language. We show that every statement that AWdeclare as algebrizing does relativize affinely, in fact, has a proof that relativizes affinely, and that no such proof exists for any of the statements shown not-algebrizing by AW in the classical computation model. Our work complements, and goes beyond, the subsequent work by Impagliazzo, Kabanets, and Kolokolova (STOC 2009), which also proposes a reformulation of algebrization, but falls short of recovering some key results of AW, most notably regarding the NEXP versus P/poly question. Using our definitions, we obtain new streamlined proofs of several classic results in complexity, including PSPACE ⊂ IP and NEXP ⊂ MIP. This may be of separate interest. © 2018 ACM.",Algebrization; Barriers; Coding theory; Interactive Proofs; Relativization,Computer programming; Algebrization; Barriers; Coding Theory; Interactive proofs; Relativization; Turing machines
On space efficiency of algorithms working on structural decompositions of graphs,2018,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042516214&doi=10.1145%2f3154856&partnerID=40&md5=d90a5cc50a31f767821a178ca40cc0b2,"Dynamic programming on path and tree decompositions of graphs is a technique that is ubiquitous in the field of parameterized and exponential-time algorithms. However, one of its drawbacks is that the space usage is exponential in the decomposition's width. Following the work of Allender et al. [5], we investigate whether this space complexity explosion is unavoidable. Using the idea of reparameterization of Cai and Juedes [18], we prove that the question is closely related to a conjecture that the Longest Common Subseqence problem parameterized by the number of input strings does not admit an algorithm that simultaneously uses XP time and FPT space. Moreover, we extend the complexity landscape sketched for pathwidth and treewidth by Allender et al. by considering the parameter tree-depth.We prove that computations on tree-depth decompositions correspond to a model of non-deterministic machines that work in polynomial time and logarithmic space, with access to an auxiliary stack of maximum height equal to the decomposition's depth. Together with the results of Allender et al., this describes a hierarchy of complexity classes for polynomial-time nondeterministic machines with different restrictions on the access to working space, which mirrors the classic relations between treewidth, pathwidth, and tree-depth. © 2017 ACM.",Completeness; Complexity classes; Complexity theory; CSP; LCS; Pathwidth; Time-space tradeoff; Tree-depth; Treewidth,Aluminum; Computational complexity; Dynamic programming; Formal logic; Parameter estimation; Polynomial approximation; Completeness; Complexity class; Complexity theory; Pathwidth; Time space tradeoffs; Tree-depth; Tree-width; Trees (mathematics)
Communication complexity of statistical distance,2018,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042514896&doi=10.1145%2f3170708&partnerID=40&md5=003fc0a379b42fd80a160e97d1def528,"We prove nearly matching upper and lower bounds on the randomized communication complexity of the following problem: Alice and Bob are each given a probability distribution over n elements, and they wish to estimate within ±∈ the statistical (total variation) distance between their distributions. For some range of parameters, there is up to a logn factor gap between the upper and lower bounds, and we identify a barrier to using information complexity techniques to improve the lower bound in this case. We also prove a side result that we discovered along the way: the randomized communication complexity of n-bit Majority composed with n-bit Greater Than is O(n logn). © 2018 ACM.",Communication; Complexity; Distance; Statistical,Communication; Computational complexity; Communication complexity; Complexity; Distance; Following problem; Information complexity; Statistical; Statistical distance; Upper and lower bounds; Probability distributions
Metric 1-Median Selection: Query Complexity vs. Approximation Ratio,2017,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041474984&doi=10.1145%2f3154858&partnerID=40&md5=091e0898e51ce7089f7da41f424a869d,"Consider the problem of finding a point in a metric space ({1, 2, . . . ,n},d) with the minimum average distance to other points. We show that this problem has no deterministic o(n1+1/(h-1)/h)-query (2h (1 - ϵ ))- approximation algorithms for any constant ϵ < 0 and any h = h(n) ϵ ℤ+ \ {1} satisfying h = o(n1/(h-1) ). Combining our result with existing ones, we determine the best approximation ratio achievable by deterministic O(n1+ϵ )-query (respectively, O(n1+ϵ )-time) algorithms to be 2⌈1/ϵ⌉, for all constants ϵ ϵ (0, 1).",1-median selection; Closeness centrality; Metric space; Sublinear-query computation,Set theory; Topology; 1-Median; Approximation ratios; Best approximations; Closeness centralities; Metric spaces; Minimum average distance; Query complexity; Sub-linear queries; Approximation algorithms
Parameterized property testing of functions,2017,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041818606&doi=10.1145%2f3155296&partnerID=40&md5=538dc57f11597a4a5a26af2d33517c3e,"We investigate the parameters in terms of which the complexity of sublinear-time algorithms should be expressed. Our goal is to find input parameters that are tailored to the combinatorics of the specific problem being studied and design algorithms that run faster when these parameters are small. This direction enables us to surpass the (worst-case) lower bounds, expressed in terms of the input size, for several problems. Our aim is to develop a similar level of understanding of the complexity of sublinear-time algorithms to the one that was enabled by research in parameterized complexity for classical algorithms. Specifically,we focus on testing properties of functions. By parameterizing the query complexity in terms of the size r of the image of the input function, we obtain testers for monotonicity and convexity of functions of the form f : [n] ℝ with query complexity O(log r ), with no dependence on n. The result for monotonicity circumvents the ω(logn) lower bound by Fischer (Inf. Comput. 2004) for this problem. We present several other parameterized testers, providing compelling evidence that expressing the query complexity of property testers in terms of the input size is not always the best choice.",Convexity; Monotonicity; Parameterization; Property testing; Sublinear algorithms,Computational complexity; Parameterization; Convexity; Monotonicity; Parameterized complexity; Properties of Functions; Property-testing; Specific problems; Sublinear algorithm; Sublinear time algorithms; Parameter estimation
Parameterized Testability,2017,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041863139&doi=10.1145%2f3155294&partnerID=40&md5=fedc843eee38f3a33ad53b397d4d583d,"This article studies property testing for NP optimization problems with parameter k under the general graph model with an augmentation of random edge sampling capability. It is shown that a variety of such problems, including κ-Vertex Cover, κ-Feedback Vertex Set, κ-Multicut, κ-Path-Free, and κ-Dominating Set, are constant-query testable if k is constant. It should be noted that the first four problems are fixed parameter tractable (FPT) and it turns out that algorithmic techniques for their FPT algorithms (branch-and-bound search, color coding, etc.) are also useful for our testers. κ-Dominating Set isW[2]-hard, but we can still test the property with a constant number of queries, since the definition of ϵ-farness makes the problem trivial for non-sparse graphs that are the source of hardness for the original optimization problem. We also consider κ-Odd Cycle Transversal, which is another well-known FPT problem, but we only give a sublinear-query tester when κ is a constant.",Fixed parameter tractability; Property testing,Optimization; Algorithmic techniques; Branch and bound search; Fixed-parameter tractability; General graph models; NP optimization problems; Odd cycle transversals; Optimization problems; Property-testing; Graph theory
An O(n∈) space and polynomial time algorithm for reachability in directed layered planar graphs,2017,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038362764&doi=10.1145%2f3154857&partnerID=40&md5=8a0b63a18fa92e7956aa4a66f4f58072,"Given a graph G and two vertices s and t in it, graph reachability is the problem of checking whether there exists a path from s to t in G. We show that reachability in directed layered planar graphs can be decided in polynomial time and O(n∈) space, for any ∈ > 0. The previous best-known space bound for this problem with polynomial time was approximately O(√n) space (Imai et al. 2013). Deciding graph reachability in SC (Steve's class) is an important open question in complexity theory, and in this article, we make progress toward resolving this question. © 2017 ACM.",Directed reachability problem; Layered grid graph; Layered planar graph; Time-space tradeoff,Directed graphs; Graphic methods; Polynomial approximation; Polynomials; Complexity theory; Grid graphs; Planar graph; Polynomial-time; Polynomial-time algorithms; Reachability problem; Space bounds; Time space tradeoffs; Graph theory
Canonizing graphs of bounded tree width in logspace,2017,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033242762&doi=10.1145%2f3132720&partnerID=40&md5=f258950b4998a244650b034d5ec708f9,"Graph canonization is the problem of computing a unique representative, a canon, from the isomorphism class of a given graph. This implies that two graphs are isomorphic exactly if their canons are equal. We show that graphs of bounded tree width can be canonized by logarithmic-space (logspace) algorithms. This implies that the isomorphism problem for graphs of bounded tree width can be decided in logspace. In the light of isomorphism for trees being hard for the complexity class logspace, this makes the ubiquitous class of graphs of bounded tree width one of the few classes of graphs for which the complexity of the isomorphism problem has been exactly determined. © 2017 ACM.",Algorithmic graph theory; Computational complexity; Graph canonization; Graph isomorphism; Logspace algorithms; Tree width,Computational complexity; Forestry; Graph theory; Graphic methods; Parallel processing systems; Set theory; Complexity class; Graph canonization; Graph isomorphism; Isomorphism class; Isomorphism problems; Log-space algorithms; Tree-width; Two-graphs; Trees (mathematics)
Asking the metaquestions in constraint tractability,2017,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033223384&doi=10.1145%2f3134757&partnerID=40&md5=88f93966f07a59c1678885240732bef3,"The constraint satisfaction problem (CSP) involves deciding, given a set of variables and a set of constraints on the variables, whether or not there is an assignment to the variables satisfying all of the constraints. One formulation of the CSP is as the problem of deciding, given a pair (G, H) of relational structures, whether or not there is a homomorphism from the first structure to the second structure. The CSP is generally NP-hard; a common way to restrict this problem is to fix the second structure H so that each structure H gives rise to a problem CSP(H). The problem family CSP(H) has been studied using an algebraic approach, which links the algorithmic and complexity properties of each problem CSP(H) to a set of operations, the so-called polymorphisms of H. Certain types of polymorphisms are known to imply the polynomial-time tractabil-ity of CSP(H), and others are conjectured to do so. This article systematically studies-for various classes of polymorphisms-the computational complexity of deciding whether or not a given structure H admits a polymorphism from the class. Among other results, we prove the NP-completeness of deciding a condition conjectured to characterize the tractable problems CSP(H), as well as the NP-completeness of deciding if CSP(H) has bounded width. © 2017 ACM.",Constraint satisfaction; Homomorphisms; Metaquestions; Polymorphisms; Tractability,Computational complexity; Parallel processing systems; Polymorphism; Polynomial approximation; Algebraic approaches; Constraint Satisfaction; Homomorphisms; Metaquestions; Np-completeness; Polynomial-time; Relational structures; Tractability; Constraint satisfaction problems
Hardness of approximation for strip packing,2017,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029784272&doi=10.1145%2f3092026&partnerID=40&md5=be4a6ae570457358fa3434ff0038971e,"Strip packing is a classical packing problem, where the goal is to pack a set of rectangular objects into a strip of a given width, while minimizing the total height of the packing. The problem has multiple applications, for example, in scheduling and stock-cutting, and has been studied extensively. When the dimensions of the objects are allowed to be exponential in the total input size, it is known that the problem cannot be approximated within a factor better than 3/2, unless P = NP. However, there was no corresponding lower bound for polynomially bounded input data. In fact, Nadiradze and Wiese [SODA 2016] have recently proposed a (1.4 + ϵ)-approximation algorithm for this variant, thus showing that strip packing with polynomially bounded data can be approximated better than when exponentially large values are allowed in the input. Their result has subsequently been improved to a (4/3 + ϵ)-approximation by two independent research groups [FSTTCS 2016,WALCOM 2017]. This raises a questionwhether strip packing with polynomially bounded input data admits a quasi-polynomial time approximation scheme, as is the case for related twodimensional packing problems like maximum independent set of rectangles or two-dimensional knapsack. In this article, we answer this question in negative by proving that it is NP-hard to approximate strip packing within a factor better than 12/11, even when restricted to polynomially bounded input data. In particular, this shows that the strip packing problem admits no quasi-polynomial time approximation scheme, unless NP ⊆ DTIME(2polylog(n)).",Approximation hardness; Packing problems; Strip packing,Approximation algorithms; Approximation theory; Hardness; Input output programs; Approximation hardness; Hardness of approximation; Maximum independent sets; Multiple applications; Packing problems; Strip packing; Strip packing problem; Two-dimensional packing; Polynomial approximation
Finding consensus strings with small length difference between input and solution strings,2017,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029791356&doi=10.1145%2f3110290&partnerID=40&md5=c67c669ac63dd3bc1752b019079f0933,"The Closest Substring Problem is to decide, for given strings s1, ⋯, sk of length at most l and numbers m and d, whether there is a length-m string s and length-m substrings si′ of si, such that s has a Hamming distance of at most d from each si′. If we instead require the sum of all the Hamming distances between s and each si′ to be bounded by d, then it is called the Consensus Patterns Problem. We contribute to the parameterised complexity analysis of these classical NP-hard string problems by investigating the parameter (l-m), i.e., the length difference between input and solution strings. For most combinations of (l-m) and one of the classical parameters (m, l, k, or d), we obtain fixed-parameter tractability. However, even for constant (l-m) and constant alphabet size, both problems remain NP-hard. While this follows from known results with respect to the Closest Substring, we need a new reduction in the case of the Consensus Patterns. As a by-product of this reduction, we obtain an exact exponential-time algorithm for both problems, which is based on an alphabet reduction.",Hard string problems; Multivariate algorithmics; Parameterised complexity,Computational complexity; Algorithmics; Closest substring problems; Complexity analysis; Consensus patterns; Exact exponential-time algorithms; Fixed-parameter tractability; Hard string problems; Parameterised complexity; Hamming distance
Proof complexity modulo the polynomial hierarchy: Understanding alternation as a source of hardness,2017,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029802608&doi=10.1145%2f3087534&partnerID=40&md5=63e38f3bf143121165080fa64c464a1b,"We present and study a framework in which one can present alternation-based lower bounds on proof length in proof systems for quantified Boolean formulas. A key notion in this framework is that of proof system ensemble, which is (essentially) a sequence of proof systems where, for each, proof checking can be performed in the polynomial hierarchy.We introduce a proof system ensemble called relaxing QU-res that is based on the established proof system QU-resolution. Our main results include an exponential separation of the treelike and general versions of relaxing QU-res and an exponential lower bound for relaxing QU-res; these are analogs of classical results in propositional proof complexity. © 2017 ACM.",Polynomial hierarchy; Proof complexity; Quantified formulas,Polynomials; Programmable logic controllers; General version; Polynomial hierarchies; Proof checking; Proof complexity; Proof-length; Propositional proof complexity; Quantified Boolean formulas; Quantified formulas; Boolean algebra
Min/Max-Poly Weighting Schemes and the NL versus UL Problem,2017,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019632349&doi=10.1145%2f3070902&partnerID=40&md5=c58578dc7b035c21c358b370c7e5c6b8,"For a graph G(V, E) (|V| = n) and a vertex s ∈ V, a weighting scheme (W : E ↔→ Z +) is called a min-unique (resp. max-unique) weighting scheme if, for any vertex v of the graph G, there is a unique path of minimum (resp. maximum) weight from s to v, where weight of a path is the sum of the weights assigned to the edges. Instead, if the number of paths of minimum (resp. maximum) weight is bounded by nc for some constant c, then the weighting scheme is called a min-poly (resp. max-poly) weighting scheme. In this article, we propose an unambiguous nondeterministic log-space (UL) algorithm for the problem of testing reachability graphs augmented with a min-poly weighting scheme. This improves the result in Reinhardt and Allender [2000], in which a UL algorithm was given for the case when the weighting scheme is min-unique. Our main technique involves triple inductive counting and generalizes the techniques of Immerman [1988], Szelepcśenyi [1988], and Reinhardt and Allender [2000], combined with a hashing technique due to Fredman et al. [1984] (also used in Garvin et al. [2014]).We combine this with a complementary unambiguous verification method to give the desired UL algorithm. At the other end of the spectrum, we propose a UL algorithm for testing reachability in layered DAGs augmented with max-poly weighting schemes. To achieve this, we first reduce reachability in layered DAGs to the longest path problem for DAGs with a unique source, such that the reduction also preserves the max-unique and max-poly properties of the graph. Using our techniques, we generalize the double inductive counting method in Limaye et al. [2009], in which the UL algorithm was given for the longest path problem on DAGs with a unique sink and augmented with a max-unique weighting scheme. An important consequence of our results is that, to show NL = UL, it suffices to design log-space computable min-poly (or max-poly) weighting schemes for layered DAGs. © 2017 ACM.",Graph reachability problem; Unambiguous logspace; Weighting schemes,Aluminum; Hashing techniques; Logspace; Longest path problems; Reachability; Reachability graphs; Reachability problem; Verification method; Weighting scheme; Graph theory
Pseudorandom generators with optimal seed length for non-boolean poly-size circuits,2017,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018305579&doi=10.1145%2f3018057&partnerID=40&md5=135cce27e84a4039a7ac124b2291031f,"A sampling procedure for a distribution P over {0, 1}ℓ is a function C : {0, 1}n → {0, 1}ℓ such that the distribution C(Un) (obtained by applying C on the uniform distribution Un) is the ""desired distribution"" P. Let n > r ≥ ℓ = nΩ(1). An ∈-nb-PRG (defined by Dubrov and Ishai [2006]) is a function G : {0, 1}r → {0, 1}n such that for every C : {0, 1}n → {0, 1}ℓ in some class of ""interesting sampling procedures,"" C′(Ur) = C(G(Ur)) is ∈-close to C(Un) in statistical distance. We construct poly-time computable nb-PRGs with r = O(ℓ) for poly-size circuits relying on the assumption that there exists β > 0 and a problem L in E = DTIME(2O(n)) such that for every large enough n, nondeterministic circuits of size 2βn that have NP-gates cannot solve L on inputs of length n. This assumption is a scaled nonuniform analog of (the widely believed) EXP ≠ ΣP2, and similar assumptions appear in various contexts in derandomization. Previous nb-PRGs of Dubrov and Ishai have r = Ω(ℓ2) and are based on very strong cryptographic assumptions or, alternatively, on nonstandard assumptions regarding incompressibility of functions on random inputs. When restricting to poly-size circuits C : {0, 1}n → {0, 1}ℓ with Shannon entropy H(C(Un)) ≤ k, for ℓ > k = nΩ(1), our nb-PRGs have r = O(k). The nb-PRGs of Dubrov and Ishai use seed length r = Ω(k2) and require that the probability distribution of C(Un) is efficiently computable. Our nb-PRGs follow from a notion of ""conditional PRGs,"" which may be of independent interest. These are PRGs where G(Ur) remains pseudorandom even when conditioned on a ""large"" event {A(G(Ur)) = 1}, for an arbitrary poly-size circuit A. A related notion was considered by Shaltiel and Umans [2005] in a different setting, and our proofs use ideas from that paper, as well as ideas of Dubrov and Ishai. We also give an unconditional construction of poly-time computable nb-PRGs for poly(n)-size, depth d circuits C : {0, 1}n → {0, 1}ℓ with r = O(ℓ · logd+O(1)n). This improves upon the previous work of Dubrov and Ishai that has r ≥ ℓ2. This result follows by adapting a recent PRG construction of Trevisan and Xue [2013] to the case of nb-PRGs. We also show that this PRG can be implemented by a uniform family of constant-depth circuits with slightly increased seed length. © 2017 ACM.",Hardness versus randomness; Pseudorandom generators; Pseudorandomness; Randomness complexity of sampling,Probability distributions; Random processes; Sampling; Timing circuits; Constant-depth circuits; Cryptographic assumptions; Derandomization; Pseudorandom generators; Pseudorandomness; Sampling procedures; Statistical distance; Uniform distribution; C (programming language)
Exact perfect matching in complete graphs,2017,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018341492&doi=10.1145%2f3041402&partnerID=40&md5=e90d6eaa83d0f117a3355b657230d2a0,"A red-blue graph is a graph where every edge is colored either red or blue. The exact perfect matching problem asks for a perfect matching in a red-blue graph that has exactly a given number of red edges. We show that for complete and bipartite complete graphs, the exact perfect matching problem is logspace equivalent to the perfect matching problem. Hence, an efficient parallel algorithm for perfect matching would carry over to the exact perfect matching problem for this class of graphs. We also report some progress in extending the result to arbitrary graphs. © 2017 ACM.",Complete graphs; Computational complexity; Exact perfect matching; Perfect matching,Computational complexity; Graph theory; Arbitrary graphs; Complete graphs; Logspace; Perfect matchings; Red edge; Graphic methods
A complexity trichotomy for approximately counting list H-colorings,2017,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018338972&doi=10.1145%2f3037381&partnerID=40&md5=3750f00038170b49b1db52a4693e5842,"We examine the computational complexity of approximately counting the list H-colorings of a graph. We discover a natural graph-theoretic trichotomy based on the structure of the graph H. If H is an irreflexive bipartite graph or a reflexive complete graph, then counting list H-colorings is trivially in polynomial time. Otherwise, if H is an irreflexive bipartite permutation graph or a reflexive proper interval graph, then approximately counting list H-colorings is equivalent to #BIS, the problem of approximately counting independent sets in a bipartite graph. This is a well-studied problem that is believed to be of intermediate complexity-it is believed that it does not have an FPRAS, but that it is not as difficult as approximating the most difficult counting problems in #P. For every other graph H, approximately counting list H-colorings is complete for #P with respect to approximation-preserving reductions (so there is no FPRAS unless NP = RP). Two pleasing features of the trichotomy are (1) it has a natural formulation in terms of hereditary graph classes, and (2) the proof is largely self-contained and does not require any universal algebra (unlike similar dichotomies in the weighted case). We are able to extend the hardness results to the bounded-degree setting, showing that all hardness results apply to input graphs with maximum degree at most 6. © 2017 ACM.",Approximate counting; Graph homomorphisms; List colourings,Asymptotic stability; Graph theory; Hardness; Polynomial approximation; Approximate counting; Approximation-preserving reductions; Bipartite permutation graphs; Counting independent sets; Graph homomorphisms; Intermediate complexity; List-colourings; Proper interval graphs; Coloring
Lower bounds for constant query affine-invariant LCCs and LTCs,2017,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018291664&doi=10.1145%2f3016802&partnerID=40&md5=a0a3293978e910475bd742b2fde1a1d6,"Affine-invariant codes are codes whose coordinates form a vector space over a finite field and which are invariant under affine transformations of the coordinate space. They form a natural, well-studied class of codes; they include popular codes such as Reed-Muller and Reed-Solomon. A particularly appealing feature of affine-invariant codes is that they seem well suited to admit local correctors and testers. In this work, we give lower bounds on the length of locally correctable and locally testable affine-invariant codes with constant query complexity. We show that if a code C ⊂ ΣKn is an r-query affine invariant locally correctable code (LCC), where K is a finite field and Σ is a finite alphabet, then the number of codewords in C is at most exp(OK,r,|Σ|(nr-1)). Also, we show that if C ⊂ ΣKn is an r-query affine invariant locally testable code (LTC), then the number of codewords in C is at most exp(OK,r,|Σ|(nr-2)). The dependence on n in these bounds is tight for constant-query LCCs/LTCs, since Guo, Kopparty, and Sudan (ITCS'13) constructed affine-invariant codes via lifting that have the same asymptotic tradeoffs. Note that our result holds for non-linear codes, whereas previously, Ben-Sasson and Sudan (RANDOM'11) assumed linearity to derive similar results. Our analysis uses higher-order Fourier analysis. In particular, we show that the codewords corresponding to an affine-invariant LCC/LTC must be far from each other with respect to Gowers norm of an appropriate order. This then allows us to bound the number of codewords, using known decomposition theorems, which approximate any bounded function in terms of a finite number of low-degree non-classical polynomials, up to a small error in the Gowers norm. © 2017 ACM.",Affine invariance; Gowers uniformity norm; Locally correctable code; Locally testable code,Codes (symbols); Domain decomposition methods; Fourier analysis; Query processing; Vector spaces; Affine invariance; Affine invariant; Affine transformations; Bounded function; Decomposition theorems; Gowers uniformity norm; Locally correctable codes; Locally testable codes; C (programming language)
On hardness of multilinearization and VNP-completeness in characteristic 2,2016,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009085885&doi=10.1145%2f2940323&partnerID=40&md5=e58ea0dc652109576305abf746dec641,"For a Boolean function f : {0, 1}n → {0, 1}, let f̂ be the unique multilinear polynomial such that f (x) = f̂(x) holds for every x ∈ {0, 1}n. We show that, assuming VP ≠ VNP, there exists a polynomial-time computable f such that f̂ requires superpolynomial arithmetic circuits. In fact, this f can be taken as a monotone 2-CNF, or a product of affine functions. This holds over any field. To prove the results in characteristic 2, we design new VNP-complete families in this characteristic. This includes the polynomial ECn counting edge covers in a graph and the polynomial mcliquen counting cliques in a graph with deleted perfect matching. They both correspond to polynomial-time decidable problems, a phenomenon previously encountered only in characteristic ≠ 2. © 2016 ACM.",Multilinear polynomial; VNP-completeness,Boolean functions; Polynomial approximation; Affine function; Arithmetic circuit; Characteristic 2; Edge cover; Multilinear polynomials; Perfect matchings; Polynomial-time; VNP-completeness; Polynomials
Relative discrepancy does not separate information and communication complexity,2016,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008951527&doi=10.1145%2f2967605&partnerID=40&md5=e33d5987f5e332dc9cdf79ad12aea75c,"Does the information complexity of a function equal its communication complexity? We examine whether any currently known techniques might be used to show a separation between the two notions. Ganor et al. [2014] recently provided such a separation in the distributional case for a specific input distribution. We show that in the non-distributional setting, the relative discrepancy bound is smaller than the information complexity; hence, it cannot separate information and communication complexity. In addition, in the distributional case, we provide a linear program formulation for relative discrepancy and relate it to variants of the partition bound, resolving also an open question regarding the relation of the partition bound and information complexity. Last, we prove the equivalence between the adaptive relative discrepancy and the public-coin partition, implying that the logarithm of the adaptive relative discrepancy bound is quadratically tight with respect to communication. © 2016 ACM.",Communication complexity; Information complexity; Partition; Relative discrepancy,Linear programming; Partitions (building); Separation; Communication complexity; Discrepancy bounds; Information and communication; Information complexity; Input distributions; Linear programs; Relative discrepancy; Computational complexity
Nondeterminism and an abstract formulation of Nečiporuk's lower bound method,2016,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008978645&doi=10.1145%2f3013516&partnerID=40&md5=9a1cdf987ed2c972d0e02ee744fc4242,"A formulation of Nečiporuk's lower bound method slightly more inclusive than the usual complexity-measure-specific formulation is presented. Using this general formulation, limitations to lower bounds achievable by the method are obtained for several computation models, such as branching programs and Boolean formulas having access to a sublinear number of nondeterministic bits. In particular, it is shown that any lower bound achievable by the method of Nečiporuk for the size of nondeterministic and parity branching programs is at most O(n3/2 / log n). © 2016 ACM.",Binary formulas; Branching programs; Limited nondeterminism,Computation theory; Computational methods; Computer science; Binary formulas; Boolean formulae; Branching programs; Complexity measures; Computation model; Limited nondeterminism; Lower bound method; nocv1; Non-determinism; Boolean algebra
Noncommutative valiant's classes: Structure and complete problems,2016,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994481883&doi=10.1145%2f2956230&partnerID=40&md5=3a6f94034aef85626ae158c3fc4c0741,"In this article, we explore the noncommutative analogues, VPnc and VNPnc, of Valiant's algebraic complexity classes and show some striking connections to classical formal language theory. Our main results are the following: -We show that Dyck polynomials (defined from the Dyck languages of formal language theory) are complete for the class VPnc under≤abp reductions. To the best of our knowledge, these are the first natural polynomial families shown to be VPnc-complete. Likewise, it turns out that PAL (palindrome polynomials defined from palindromes) are complete for the class VSKEWnc (defined by polynomial-size skew circuits) under ≤abp reductions. The proof of these results is by suitably adapting the classical Chomsky-Sch utzenberger theorem showing that Dyck languages are the hardest CFLs. -Assuming that VPnc ≠= VNPnc, we exhibit a strictly infinite hierarchy of p-families, with respect to the projection reducibility, between the complexity classes VPnc and VNPnc (analogous to Ladner's theorem [Ladner 1975]). -Additionally, inside VPnc, we show that there is a strict hierarchy of p-families (based on the nesting depth of Dyck polynomials) with respect to the ≤abp reducibility (defined explicitly in this article).",Algebraic branching programs; Noncommutative algebraic complexity classes; Noncommutative arithmetic circuits; Projection reductions,Algebra; Computational complexity; Formal languages; Linguistics; Reconfigurable hardware; Algebraic branching programs; Algebraic complexity; Arithmetic circuit; Complete problems; Complexity class; Formal language theory; Ladner's theorems; Polynomial size; Polynomials
Small depth proof systems,2016,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994493986&doi=10.1145%2f2956229&partnerID=40&md5=c20d85af4fb01171857f6f04b017d395,"A proof system for a language L is a function f such that Range( f) is exactly L. In this article, we look at proof systems from a circuit complexity point of view and study proof systems that are computationally very restricted. The restriction we study is proof systems that can be computed by bounded fanin circuits of constant depth (NC0) or of O(log log n) depth but with O(1) alternations (poly logAC0). Each output bit depends on very few input bits; thus such proof systems correspond to a kind of local error correction on a theorem-proof pair. We identify exactly how much power we need for proof systems to capture all regular languages. We show that all regular languages have poly logAC0 proof systems, and from a previous result (Beyersdorff et al. [2011a], where NC0 proof systems were first introduced), this is tight. Our technique also shows that MAJ has poly logAC0 proof system. We explore the question of whether TAUT has NC0 proof systems. Addressing this question about 2TAUT, and since 2TAUT is closely related to reachability in graphs, we ask the same question about Reachability. We show that if Directed reachability has NC0 proof systems, then so does 2TAUT. We then show that both Undirected Reachability and Directed UnReachability have NC0 proof systems, but Directed Reachability is still open. In the context of how much power is needed for proof systems for languages in NP, we observe that proof systems for a good fraction of languages in NP do not need the full power of AC0; they have SAC0 or coSAC0 proof systems. © 2016 ACM.",Circuit complexity; Proof circuits; Proof complexity; Small depth proofs,Beryllium compounds; Electric network analysis; Error correction; Circuit complexity; Proof complexity; Proof system; Reachability; Small depth proofs; Programmable logic controllers
Planarizing gadgets for perfect matching do not exist,2016,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042752542&doi=10.1145%2f2934310&partnerID=40&md5=e6eec98aff140fb7984a4209a75add92,"To reduce a graph problem to its planar version, a standard technique is to replace crossings in a drawing of the input graph by planarizing gadgets. We show unconditionally that such a reduction is not possible for the perfect matching problem and also extend this to some other problems related to perfect matching. We further show that there is no planarizing gadget for the Hamiltonian cycle problem. © 2016 ACM",Parallel complexity; Perfect matching; Planar graphs,Computation theory; Computational methods; Computer science; Graph problems; Hamiltonian cycle; Input graphs; Parallel complexity; Perfect matchings; Planar graph; Planarizing; Hamiltonians
Space-efficient approximations for subset sum,2016,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978127775&doi=10.1145%2f2894843&partnerID=40&md5=380d52c18a21de86e49d136c00d30bc4,"SUBSETSUM is a well-known NP-complete problem: given t ϵ Z+ and a set S of m positive integers, output YES if and only if there is a subset S′ ⊆ S such that the sum of all numbers in S′ equals t. The problem and its search and optimization versions are known to be solvable in pseudopolynomial time in general. We develop a one-pass deterministic streaming algorithm that uses space O(log t/ϵ) and decides if some subset of the input stream adds up to a value in the range {(1 ± ϵ)t}. Using this algorithm, we design space-efficient fully polynomial-time approximation schemes (FPTAS) solving the search and optimization versions of SUBSETSUM. Our algorithms run in O(1/ϵm2) time and O(1/ϵ) space on unit-cost RAMs, where 1+ϵ is the approximation factor. This implies constant space quadratic time FPTAS on unit-cost RAMs when ϵ is a constant. Previous FPTAS used space linear in m. In addition, we show that on certain inputs, when a solution is located within a short prefix of the input sequence, our algorithms may run in sublinear time.We apply our techniques to the problem of finding balanced separators, and we extend our results to some other variants of themore general knapsack problem. When the input numbers are encoded in unary, the decision version has been known to be in log space. We give streaming space lower and upper bounds for unary SUBSETSUM (USS). If the input length is N when the numbers are encoded in unary, we show that randomized s-pass streaming algorithms for exact SUBSETSUM need space Ω(√N/s) and give a simple deterministic two-pass streaming algorithm using O(√N log N) space. Finally, we formulate an encoding under which USS is monotone and show that the exact and approximate versions in this formulation have monotone O(log2 t) depth Boolean circuits. We also show that any circuit using ϵ-approximator gates for SUBSETSUM under this encoding needs Ω(n/log n) gates to compute the disjointness function. © 2016 ACM.",Fully polynomial-time approximations chemes FPTAS; Knapsack problem; Monotone circuits; One-pass streaming algorithm,Algorithms; Approximation algorithms; Combinatorial optimization; Computational complexity; Encoding (symbols); Integer programming; Polynomial approximation; Reconfigurable hardware; Approximation factor; Balanced separators; Fully polynomial time approximation; Fully polynomial time approximation schemes; Knapsack problems; Lower and upper bounds; Monotone circuit; Streaming algorithm; Optimization
Quadratic maps are hard to sample,2016,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976422124&doi=10.1145%2f2934308&partnerID=40&md5=dd36d34c0836222d88d38a96cd3fcbef,"This note proves the existence of a quadratic GF(2) map p: {0, 1}n → {0, 1} such that no constant-depth circuit of size poly(n) can sample the distribution (u, p(u)) for uniform u. © 2016 ACM.",Complexity of distributions; Extractors; Polynomials; Sampling,Computation theory; Computer science; Polynomials; Sampling; Complexity of distributions; Constant-depth circuits; Extractors; Quadratic maps; Computational methods
Limitations of deterministic auction design for correlated bidders,2016,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978141496&doi=10.1145%2f2934309&partnerID=40&md5=8f6da051413adc7f6cde6ee3402b2c47,"The seminal work of Myerson (Mathematics of OR '81) characterizes incentive-compatible single-item auctions among bidders with independent valuations. In this setting, relatively simple deterministic auction mechanisms achieve revenue optimality. When bidders have correlated valuations, designing the revenueoptimal deterministic auction is a computationally demanding problem; indeed, Papadimitriou and Pierrakos (STOC '11) proved that it is APX-hard, obtaining an explicit inapproximability factor of 1999/2000 = 99.95%. In the current article, we strengthen this inapproximability factor to 63/64 ≈ 98.5%. Our proof is based on a gap-preserving reduction from the MAX-NM 3SAT problem; a variant of the maximum satisfiability problem where each clause has exactly three literals and no clause contains both negated and unnegated literals. We furthermore show that the gap between the revenue of deterministic and randomized auctions can be as low as 13/14 ≈ 92.9%, improving an explicit gap of 947/948 ≈ 99.9% by Dobzinski, Fu, and Kleinberg (STOC '11). © 2016 ACM.",Algorithms; Economics; Theory,Algorithms; Commerce; 3-SAT problems; Auction design; Auction mechanisms; Inapproximability factor; Incentive compatible; Maximum satisfiability problems; Optimality; Theory; Economics
The power of an example: Hidden set size approximation using group queries and conditional sampling,2016,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976449592&doi=10.1145%2f2930657&partnerID=40&md5=fa05a512e7aca8d839d12748eb0e1446,"We study a basic problem of approximating the size of an unknown set S in a known universe U. We consider two versions of the problem. In both versions, the algorithm can specify subsets T ⊆ U. In the first version, which we refer to as the group query or subset query version, the algorithm is told whether T ∩ S is nonempty. In the second version, which we refer to as the subset sampling version, if T ∩ S is nonempty, then the algorithm receives a uniformly selected element from T ∩ S. We study the difference between these two versions in both the case that the algorithm is adaptive and the case in which it is nonadaptive. Our main focus is on a natural family of allowed subsets, which correspond to intervals, as well as variants of this family. © 2016 ACM.",Approximation; Sampling,Algorithms; Sampling; Approximation; Conditional sampling; Subset queries; Set theory
A rank lower bound for cutting Planes Proofs of Ramsey's Theorem,2016,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976444863&doi=10.1145%2f2903266&partnerID=40&md5=c0aeee0c0c3a6de923cd662896845c1e,"Ramsey's Theorem is a cornerstone of combinatorics and logic. In its simplest formulation it says that for every k > 0 and s > 0, there is a minimum number r(k, s) such that any simple graph with at least r(k, s) vertices contains either a clique of size k or an independent set of size s. We study the complexity of proving upper bounds for the number r(k, k). In particular, we focus on the propositional proof system cutting planes; we show that any cutting plane proof of the upper bound ""r(k, k) ≤ 4k"" requires high rank. In order to do that we show a protection lemma which could be of independent interest. © 2016 ACM.",Cutting planes; Integer programming; Proof complexity; Ramsey theory; Rank,Integer programming; Combinatorics; Cutting planes; Independent set; Proof complexity; Propositional proof systems; Ramsey theory; Ramsey's theorem; Rank; Graph theory
Arithmetic circuit lower bounds via maximum-rank of partial derivative matrices,2016,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010769875&doi=10.1145%2f2898437&partnerID=40&md5=98d379a47b358d96dfa6316a0ba09dbe,"We introduce the polynomial coefficient matrix and identify the maximum rank of this matrix under variable substitution as a complexity measure for multivariate polynomials. We use our techniques to prove superpolynomial lower bounds against several classes of non-multilinear arithmetic circuits. In particular, we obtain the following results: —As our first main result, we prove that any homogeneous depth-3 circuit for computing the product of d matrices of dimension n × n requires (nd −1/2d) size. This improves the lower bounds in Nisan and Wigderson [1995] for d = ω(1). —As our second main result, we show that there is an explicit polynomial on n variables and degree at most2 n for which any depth-3 circuit of product dimension at most10 n (dimension of the space of affine forms feeding into each product gate) requires size 2(n). This generalizes the lower bounds against diagonal circuits proved in Saxena [2008]. Diagonal circuits are of product dimension 1. —We prove a n(log n) lower bound on the size of product-sparse formulas. By definition, any multilinear formula is a product-sparse formula. Thus, this result extends the known super-polynomial lower bounds on the size of multilinear formulas [Raz 2006]. —We prove a 2(n) lower bound on the size of partitioned arithmetic branching programs. This result extends the known exponential lower bound on the size of ordered arithmetic branching programs [Jansen 2008]. c 2016 ACM",Arithmetic circuits; Partial derivative matrix,Logic circuits; Polynomials; Timing circuits; Arithmetic circuit; Branching programs; Complexity measures; Multivariate polynomial; Partial derivatives; Polynomial coefficients; Super-polynomials; Variable substitution; Matrix algebra
"Counting homomorphisms to square-free graphs, modulo 2",2016,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045546262&doi=10.1145%2f2898441&partnerID=40&md5=c6a7b77911b906c4921ad62b331a40e3,"We study the problem HOMSTOH of counting, modulo 2, the homomorphisms from an input graph to a fixed undirected graph H. A characteristic feature of modular counting is that cancellations make wider classes of instances tractable than is the case for exact (nonmodular) counting; thus, subtle dichotomy theorems can arise. We show the following dichotomy: for any H that contains no 4-cycles, HOMSTOH is either in polynomial time or is P-complete. This partially confirms a conjecture of Faben and Jerrum that was previously only known to hold for trees and for a restricted class of tree-width-2 graphs called cactus graphs. We confirm the conjecture for a rich class of graphs, including graphs of unbounded tree-width. In particular, we focus on square-free graphs, which are graphs without 4-cycles. These graphs arise frequently in combinatorics, for example, in connection with the strong perfect graph theorem and in certain graph algorithms. Previous dichotomy theorems required the graph to be tree-like so that tree-like decompositions could be exploited in the proof. We prove the conjecture for a much richer class of graphs by adopting a much more general approach. © 2016 ACM",Counting modulo 2; Graph homomorphism; Parity complexity dichotomy,Forestry; Graph theory; Graphic methods; Polynomial approximation; Complexity dichotomies; Dichotomy theorem; Graph algorithms; Graph homomorphisms; Modular counting; Modulo 2; Tree-like decomposition; Undirected graph; Trees (mathematics)
Tractable parameterizations for the MINIMUM LINEAR ARRANGEMENT problem,2016,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969963442&doi=10.1145%2f2898352&partnerID=40&md5=adea76adfc54c590e65cc726525a5cce,"The MINIMUM LINEAR ARRANGEMENT (MLA) problem involves embedding a given graph on the integer line so that the sum of the edge lengths of the embedded graph is minimized. Most layout problems are either intractable or not known to be tractable, parameterized by the treewidth of the input graph. We investigate MLA with respect to three parameters that provide more structure than treewidth. In particular, we give a factor (1+ϵ)-approximation algorithm for MLA parameterized by (ϵ, k), where k is the vertex cover number of the input graph. By a similar approach, we obtain two FPT algorithms that exactly solve MLA parameterized by, respectively, the max leaf and edge clique cover numbers of the input graph. © 2016 ACM.",Fixed parameter tractability; MINIMUM LINEAR ARRANGEMENT; Parameterized algorithms,Approximation algorithms; Parameter estimation; Embedded graphs; Fixed-parameter tractability; FPT algorithms; Layout problems; Minimum linear arrangement; Parameterized; Parameterized algorithm; Three parameters; Parameterization
Structural properties of nonautoreducible sets,2016,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969915694&doi=10.1145%2f2898440&partnerID=40&md5=7121701fc2b17aa99e5fdb55242016da,"We investigate autoreducibility properties of complete sets for NEXP under different polynomial-time reductions. Specifically, we show under some polynomial-time reductions that there are complete sets for NEXP that are not autoreducible. We obtain the following main results: - For any positive integers s and k such that 2s - 1 > k, there is a ≤ps-T-complete set for NEXP that is not ≤pk-tt-autoreducible. - For every constant c > 1, there is a ≤p2-T-complete set for NEXP that is not autoreducible under nonadaptive reductions that make no more than three queries, such that each of them has a length between n1/c and nc, where n is input size. - For any positive integer k, there is a ≤pk-tt-complete set for NEXP that is not autoreducible under ≤pk-tt- reductions whose truth table is not a disjunction or a negated disjunction. Finally, we show that settling the question of whether every ≤pdtt-complete set for NEXP is ≤pNOR-tt-autoreducible either positively or negatively would lead to major results about the exponential time complexity classes. © 2016 ACM.",Autoreducibility; Diagonalization; NEXP; Structural complexity,
A galois connection for Weighted (relational) clones of infinite size,2016,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969922828&doi=10.1145%2f2898438&partnerID=40&md5=017cb3ae6bf068f7330401edff2e9bdc,"A Galois connection between clones and relational clones on a fixed finite domain is one of the cornerstones of the so-called algebraic approach to the computational complexity of non-uniform Constraint Satisfaction Problems (CSPs). Cohen et al. established a Galois connection between finitely-generated weighted clones and finitely-generated weighted relational clones [SICOMP'13], and asked whether this connection holds in general. We answer this question in the affirmative for weighted (relational) clones with real weights and show that the complexity of the corresponding valued CSPs is preserved. © 2016 ACM.",Discrete optimisation; Galois connection; Universal algebra; Valued constraint satisfaction problems; Weighted polymorphisms,Algebra; Cloning; Constraint theory; Optimization; Algebraic approaches; Discrete optimisation; Fixed finite domain; Galois connection; Non-uniform; Universal algebra; Valued constraint satisfaction problems; Constraint satisfaction problems
Testing read-once formula satisfaction,2016,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968883551&doi=10.1145%2f2897184&partnerID=40&md5=06d267fa025fe81a9f213a4c83b2f3ec,"We study the query complexity of testing for properties defined by read-once formulas, as instances of massively parametrized properties, and prove several testability and nontestability results. First, we prove the testability of any property accepted by a Boolean read-once formula involving any bounded arity gates, with a number of queries exponential in ϵ, doubly exponential in the arity, and independent of all other parameters. When the gates are limited to being monotone, we prove that there is an estimation algorithm that outputs an approximation of the distance of the input from satisfying the property. For formulas only involving And/Or gates, we provide amore efficient test whose query complexity is only quasipolynomial in ϵ. On the other hand, we show that such testability results do not hold in general for formulas over non-Boolean alphabets. Specifically, we construct a property defined by a read-once arity 2 (non-Boolean) formula over an alphabet of size 4, such that any 1/4-test for it requires a number of queries depending on the formula size. We also present such a formula over an alphabet of size 5 that also satisfies a strong monotonicity condition. © 2016 ACM.",Massively parametrized properties; Property testing; Read-once formula,Approximation algorithms; Query processing; Reconfigurable hardware; Estimation algorithm; Formula size; Massively parametrized properties; Monotonicity conditions; Property-testing; Quasi-poly-nomial; Query complexity; Read-once formulas; Boolean algebra
On sample-based testers,2016,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968735521&doi=10.1145%2f2898355&partnerID=40&md5=b828d9883f43e587613c58730a5b63d8,"The standard definition of property testing endows the tester with the ability to make arbitrary queries to ""elements"" of the tested object. In contrast, sample-based testers only obtain independently distributed elements (a.k.a. labeled samples) of the tested object.While sample-based testers were defined by Goldreich, Goldwasser, and Ron (JACM 1998), with few exceptions, most research in property testing has focused on query-based testers. In this work, we advance the study of sample-based property testers by providing several general positive results as well as by revealing relations between variants of this testing model. In particular: -We show that certain types of query-based testers yield sample-based testers of sublinear sample complexity. For example, this holds for a natural class of proximity oblivious testers. -We study the relation between distribution-free sample-based testers and one-sided error sample-based testers w.r.t. the uniform distribution. While most of this work ignores the time complexity of testing, one part of it does focus on this aspect. The main result in this part is a sublinear-time sample-based tester, in the dense graphs model, for k-colorability, for any k ≥ 2. © 2016 ACM.",Property Testing; Sampling,Gold; Sampling; Distributed elements; Distribution-free; Property-testing; Sample complexity; Standard definitions; Testing modeling; Time complexity; Uniform distribution; Ability testing
The list-decoding size of fourier-sparse Boolean functions,2016,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969924819&doi=10.1145%2f2898439&partnerID=40&md5=0d1770c0e78e380f235c19b3cebeb6b3,"A function defined on the Boolean hypercube is k-Fourier-sparse if it has at most knonzero Fourier coefficients. For a function f: Fn2 → ℝ and parameters k and d, we prove a strong upper bound on the number of k-Fourier-sparse Boolean functions that disagree with f on at most d inputs. Our bound implies that the number of uniform and independent random samples needed for learning the class of k-Fourier-sparse Boolean functions on n variables exactly is at most O(n·klog k). As an application, we prove an upper bound on the query complexity of testing Booleanity of Fourier-sparse functions. Our bound is tight up to a logarithmic factor and quadratically improves on a result due to Gur and Tamuz [2013]. © 2016 ACM.",Fourier-sparse Boolean functions; Learning theory; Property testing,Fourier analysis; Fourier transforms; Fourier; Fourier coefficients; Learning Theory; List decoding; Property-testing; Query complexity; Random sample; Upper Bound; Boolean functions
An omega((n log n)/R) lower bound for fourier transform computation in the R-well conditioned model,2016,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958581232&doi=10.1145%2f2858785&partnerID=40&md5=f244706a756437bfd722e0cd6416b353,"Obtaining a nontrivial (superlinear) lower bound for computation of the Fourier transform in the linear circuit model has been a long-standing open problem for more than 40 years. An early result by Morgenstern from 1973, provides an Ω(nlog n) lower bound for the unnormalized Fourier transform when the constants used in the computation are bounded. The proof uses a potential function related to a determinant. That result does not explain why the normalized Fourier transform (of unit determinant) should be difficult to compute in the same model. Hence, it is not scale insensitive. More recently, Ailon [2013] showed that if only unitary 2-by-2 gates are used, and additionally no extra memory is allowed, then the normalized Fourier transform requires Ω (nlog n) steps. This rather limited result is also sensitive to scaling, but highlights the complexity inherent in the Fourier transform arising from introducing entropy, unlike, say, the identity matrix (which is as complex as the Fourier transform using Morgenstern's arguments, under proper scaling). This work improves on Ailon [2013] in two ways: First, we eliminate the scaling restriction and provide a lower bound for computing any scaling of the Fourier transform. Second, we allow the computational model to use extra memory. Our restriction is that the composition of all gates up to any point must be a well- conditioned linear transformation. The lower bound is Ω (R-1nlog n), where R is the uniform condition number. Wellconditioned is a natural requirement for algorithms accurately computing linear transformations on machine architectures of bounded word size. Hence, this result can be seen as a tradeoff between speed and accuracy. The main technical contribution is an extension of matrix entropy used in Ailon [2013] for unitary matrices to a potential function computable for any invertible matrix, using ""quasi-entropy"" of ""quasi-probabilities."" © 2016 ACM.",Fourier transform; Lower bounds,Entropy; Fourier transforms; Linear transformations; Matrix algebra; Number theory; Reconfigurable hardware; Computational model; Condition numbers; Identity matrices; Invertible matrices; Linear circuit models; Lower bounds; Potential function; Technical contribution; Mathematical transformations
Complexity hierarchies beyond elementary,2016,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958559132&doi=10.1145%2f2858784&partnerID=40&md5=7d11a4ead049ea8e1c2a2cd70c91224f,"We introduce a hierarchy of fast-growing complexity classes and show its suitability for completeness statements of many nonelementary problems. This hierarchy allows the classification of many decision problems with a nonelementary complexity, which occur naturally in areas such as logic, combinatorics, formal languages, and verification, with complexities ranging from simple towers of exponentials to Ackermannian and beyond. © 2016 ACM.",Fast-growing complexity; Subrecursion; Well-quasi-order,Computation theory; Computational methods; Computer science; Combinatorics; Complexity class; Complexity hierarchies; Decision problems; Exponentials; Fast-growing complexity; Subrecursion; Well quasi orderings; Formal languages
Characterizing arithmetic read-once formulae,2016,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958559709&doi=10.1145%2f2858783&partnerID=40&md5=bf3c19ce0f0607ac3d5e01caced66df6,"An arithmetic Read-Once Formula (ROF for short) is a formula (i.e., a tree of computation) in which the operations are {+, ×} and such that every input variable labels at most one leaf. We give a simple characterization of such formulae. Other than being interesting in its own right, our characterization gives rise to a property-testing algorithm for functions computable by such formulae. To the best of our knowledge, prior to our work, no characterization and/or property-testing algorithm was known for this kind of formulae. © 2016 ACM.",Arithmetic circuits; Polynomial characterization; Property testing,Logic circuits; Arithmetic circuit; Input variables; Property-testing; Read-once formulas; Characterization
Parameterized complexity and kernelizability of max ones and exact ones problems,2016,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958568933&doi=10.1145%2f2858787&partnerID=40&md5=e00aa1e388fbe3babb7bb77ad51d7a2d,"For a finite set Γ of Boolean relations,MAX ONES SAT(Γ) and EXACT ONES SAT(Γ) are generalized satisfiability problems where every constraint relation is from Γ, and the task is to find a satisfying assignment with at least/exactly k variables set to 1, respectively. We study the parameterized complexity of these problems, including the question whether they admit polynomial kernels. ForMAX ONES SAT(Γ), we give a classification into five different complexity levels: polynomial-time solvable, admits a polynomial kernel, fixed-parameter tractable, solvable in polynomial time for fixed k, and NP-hard already for k = 1. For EXACT ONES SAT(Γ), we refine the classification obtained earlier by taking a closer look at the fixed-parameter tractable cases and classifying the sets Γ for which EXACT ONES SAT(Γ) admits a polynomial kernel. © 2016 ACM.",Complexity characterization; CSP; FPT algorithms; Kernelization,Constraint satisfaction problems; Formal logic; Polynomial approximation; Complexity levels; Constraint relations; FPT algorithms; Kernelization; Parameterized complexity; Polynomial kernels; Satisfiability problems; Satisfying assignments; Polynomials
On Parity Decision Trees for Fourier-sparse Boolean Functions,2024,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195839793&doi=10.1145%2f3647629&partnerID=40&md5=3a6d30ba34bfb0f19e41893e9eca5f5b,"We study parity decision trees for Boolean functions. The motivation of our study is the log-rank conjecture for XOR functions and its connection to Fourier analysis and parity decision tree complexity. Our contributions are as follows: Let f : Fn2 → {−1, 1} be a Boolean function with Fourier support S and Fourier sparsity k. — We prove via the probabilistic method that there exists a parity decision tree of depth O(√k) that computes f . This matches the previously best-known upper bound on the parity decision tree complexity of Boolean functions (Tsang, Wong, Xie, and Zhang, FOCS’13). Moreover, while previous constructions (Tsang et al., FOCS 2013, Shpilka, Tal, and Volk, Comput. Complex. 2017) build the trees by carefully choosing the parities to be queried in each step, our proof shows that a naive sampling of the parities suffices. — We generalize the above result by showing that if the Fourier spectra of Boolean functions satisfy a natural “folding property,” then the above proof can be adapted to establish existence of a tree of complexity polynomially smaller than O(√k). More concretely, the folding property we consider is that for most distinct γ, δ in S, there are at least a polynomial (in k) number of pairs {α, β} of parities in S such that α + β = γ +δ. We pose a question in this regard that, if answered in the positive, implies that the communication complexity of an XOR function is bounded above by the fourth root of the rank of its communication matrix, improving upon the previously known upper bound of square root of rank (Tsang et al., FOCS’13, Lovett, J. ACM. 2016). — Motivated by the above, we present some structural results about the Fourier spectra of Boolean functions. It can be shown by elementary techniques that for any Boolean function f and all {α, β} in (S2), there exists another pair {γ, δ} in (S2) such that α + β = γ + δ. One can view this as a “trivial” folding property that all Boolean functions satisfy. Prior to our work, it was conceivable that for all {α, β} ∈ (S2), there exists exactly one other pair {γ, δ} ∈ (S2) with α + β = γ + δ. We show, among other results, that there must exist several γ ∈ Fn2 such that there are at least three pairs of parities {α1, α2} ∈ (S2) with α1 + α2 = γ. This, in particular, rules out the possibility stated earlier. © 2024 Copyright held by the owner/author(s).",analysis of Boolean functions; communication complexity; log-rank conjecture; Parity decision trees,Computational complexity; Decision trees; Fourier analysis; Fourier transforms; Analyse of boolean function; Communication complexity; Folding properties; Fourier; Fourier spectra; Log-rank conjecture; Parity decision tree; Tree complexity; Upper Bound; XOR function; Boolean functions
Faster Counting and Sampling Algorithms Using Colorful Decision Oracle,2024,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195856304&doi=10.1145%2f3657605&partnerID=40&md5=9b10dec94bbe957467ff878a0c14224e,"In this work, we consider d-Hyperedge Estimation and d-Hyperedge Sample problems that deal with estimation and uniform sampling of hyperedges in a hypergraph H (U (H), F (H)) in the query complexity framework, where U (H) denotes the set of vertices and F (H) denotes the set of hyperedges. The oracle access to the hypergraph is called Colorful Independence Oracle (CID), which takes d (non-empty) pairwise disjoint subsets of vertices A1, . . ., Ad ⊆ U (H) as input and answers whether there exists a hyperedge in H having exactly one vertex in each Ai for all i ∈ {1, 2, . . ., d}. Apart from the fact that d-Hyperedge Estimation and d-Hyperedge Sample problems with CID oracle access seem to be nice combinatorial problems, Dell et al. [SODA’20 & SICOMP’22] established that decision vs. counting complexities of a number of combinatorial optimization problems can be abstracted out as d-Hyperedge Estimation problem with a CID oracle access. The main technical contribution of this article is an algorithm that estimates m = |F (H)| with m̂ such that by using at most Cd logd+2 n CID queries, where n denotes the number of vertices in the hypergraph H and Cd is a constant that depends only on d. Our result, when coupled with the framework proposed by Dell et al. (SODA’20 & SICOMP’22), leads to implies improved bounds for (1 ± ε)-approximation (where ε ∈ (0, 1)) for the following fundamental problems: Edge Estimation using the Bipartite Independent Set (BIS) query. We improve the bound obtained by Beame et al. (ITCS’18 & TALG’20). Triangle Estimation using the Tripartite Independent Set (TIS) query. Currently, Dell et al.’s result gives the best bound for the case of triangle estimation in general graphs (SODA’20 & SICOMP’22). The previous best bound for the case of graphs with low co-degree (co-degree of a graph is the maximum number of triangles incident over any edge of the graph) was due to Bhattacharya et al. (ISAAC’19 & TOCS’21). We improve both of these bounds. Hyperedge Estimation & Sampling using Colorful Independence Oracle (CID). We give an improvement over the bounds obtained by Dell et al. (SODA’20 & SICOMP’22). © 2024 Copyright held by the owner/author(s).",Group query; hyperedge estimation; query complexity,Graph theory; Counting and samplings; Fast counting; Fast sampling; Group query; Hyper graph; Hyperedge estimation; Hyperedges; Independent set; Oracle access; Query complexity; Combinatorial optimization
Synchronous Dynamical Systems on Directed Acyclic Graphs: Complexity and Algorithms,2024,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195879383&doi=10.1145%2f3653723&partnerID=40&md5=59687c7e8af64ee386831261b924b83e,"Discrete dynamical systems serve as useful formal models to study diffusion phenomena in social networks. Several recent articles have studied the algorithmic and complexity aspects of some decision problems on synchronous Boolean networks, which are discrete dynamical systems whose underlying graphs are directed, and may contain directed cycles. Such problems can be regarded as reachability problems in the phase space of the corresponding dynamical system. Previous work has shown that some of these decision problems become efficiently solvable for systems on directed acyclic graphs (DAGs). Motivated by this line of work, we investigate a number of decision problems for dynamical systems whose underlying graphs are DAGs. We show that computational intractability (i.e., PSPACE-completeness) results for reachability problems hold even for dynamical systems on DAGs. We also identify some restricted versions of dynamical systems on DAGs for which reachability problem can be solved efficiently. In addition, we show that a decision problem (namely, Convergence), which is efficiently solvable for dynamical systems on DAGs, becomes PSPACE-complete for Quasi-DAGs (i.e., graphs that become DAGs by the removal of a single edge). In the process of establishing the above results, we also develop several structural properties of the phase spaces of dynamical systems on DAGs.  Copyright © 2024 held by the owner/author(s).",algorithms; complexity; Convergence Guarantee; directed acyclic graphs; Discrete dynamical systems; reachability,Computational complexity; Directed graphs; Graphic methods; Parallel processing systems; Acyclic graphs; Complexity; Convergence guarantee; Decision problems; Directed acyclic graph; Discrete dynamical systems; Phase spaces; Reachability; Reachability problem; Underlying graphs; Dynamical systems
A Complexity Dichotomy in Spatial Reasoning via Ramsey Theory,2024,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195842336&doi=10.1145%2f3649445&partnerID=40&md5=2c3a38735f2dfd49ebf2e7e6bedfea6e,"Constraint satisfaction problems (CSPs) for first-order reducts of finitely bounded homogeneous structures form a large class of computational problems that might exhibit a complexity dichotomy, P versus NP-complete. A powerful method to obtain polynomial-time tractability results for such CSPs is a certain reduction to polynomial-time tractable finite-domain CSPs defined over k-types, for a sufficiently large k. We give sufficient conditions when this method can be applied and apply these conditions to obtain a new complexity dichotomy for CSPs of first-order expansions of the basic relations of the well-studied spatial reasoning formalism RCC5. We also classify which of these CSPs can be expressed in Datalog. Our method relies on Ramsey theory; we prove that RCC5 has a Ramsey order expansion.  Copyright © 2024 held by the owner/author(s).",computational complexity; Constraint satisfaction; model theory; ramsey theory; RCC5; spatial reasoning; universal algebra,Computational complexity; Polynomial approximation; Complexity dichotomies; Constraint Satisfaction; Constraint-satisfaction problems; First order; Model theory; Polynomial-time; Ramsey theory; RCC5; Spatial reasoning; Universal algebra; Constraint satisfaction problems
Small-Space Spectral Sparsification via Bounded-Independence Sampling,2024,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195860857&doi=10.1145%2f3637034&partnerID=40&md5=1e625a22ef54da62bcf3f3bf44989d46,"We give a deterministic, nearly logarithmic-space algorithm for mild spectral sparsification of undirected graphs. Given a weighted, undirected graph G on n vertices described by a binary string of length N, an integer k ≤ logn, and an error parameter ϵ > 0, our algorithm runs in space Õ(k log(N · wmax/wmin)), where wmax and wmin are the maximum and minimum edge weights in G, and produces a weighted graph H with Õ(n1+2/k/ϵ2) edges that spectrally approximates G, in the sense of Spielman and Teng, up to an error of ϵ. Our algorithm is based on a new bounded-independence analysis of Spielman and Srivastava's effective resistance-based edge sampling algorithm and uses results from recent work on space-bounded Laplacian solvers. In particular, we demonstrate an inherent trade-off (via upper and lower bounds) between the amount of (bounded) independence used in the edge sampling algorithm, denoted by k above, and the resulting sparsity that can be achieved.  © 2024 Copyright held by the owner/author(s).",Derandomization; graph sparsification; space-bounded computation,Economic and social effects; Learning algorithms; Bounded computation; Bounded independence; Derandomization; Deterministics; Graph sparsification; Sampling algorithm; Space algorithms; Space-bounded computation; Sparsification; Undirected graph; Undirected graphs
Tight Sum-of-squares Lower Bounds for Binary Polynomial2 Optimization Problems,2024,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187796785&doi=10.1145%2f3626106&partnerID=40&md5=86dc4c68c2e2f6a5c3b6a6a2c89431e0,"For binary polynomial optimization problems of degree 2d with n variables Sakaue, Takeda, Kim, and Ito [33] proved that the 〈n+2d-1/2〉th semidefinite (SDP) relaxation in the SoS/Lasserre hierarchy of SDP relaxations provides the exact optimal value. When n is an odd number, we show that their analysis is tight, i.e., we prove that n+2d-1/2 levels of the SoS/Lasserre hierarchy are also necessary. Laurent [24] showed that the Sherali-Adams hierarchy requires n levels to detect the empty integer hull of a linear representation of a set with no integral points. She conjectured that the SoS/Lasserre rank for the same problem is n - 1. In this article, we disprove this conjecture and derive lower and upper bounds for the rank.  © 2024 Copyright held by the owner/author(s).",Sum-of-squares,Linear representation; Low bound; Lower and upper bounds; Optimal values; Optimization problems; Polynomial optimization problem; SDP relaxation; Sherali-adams hierarchies; Sums of squares; Optimization
Max Weight Independent Set in Graphs with No Long Claws: An Analog of the Gyárfás’ Path Argument,2024,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195863850&doi=10.1145%2f3636422&partnerID=40&md5=0d878c76d21af5d263d7efe18257643b,"We revisit recent developments for the Maximum Weight Independent Set problem in graphs excluding a subdivided claw St,t,t as an induced subgraph and provide a subexponential-time algorithm with improved running time 2O(√nt log n) and a quasipolynomial-time approximation scheme with improved running time 2O(ε−1t log5 n). The Gyárfás’ path argument, a powerful tool that is the main building block for many algorithms in Ptfree graphs, ensures that given an n-vertex Pt-free graph, in polynomial time we can find a set P of at most t − 1 vertices such that every connected component of G − N[P] has at most n/2 vertices. Our main technical contribution is an analog of this result for St,t,t-free graphs: given an n-vertex St,t,t-free graph, in polynomial time we can find a set P of O(t log n) vertices and an extended strip decomposition (an appropriate analog of the decomposition into connected components) of G − N[P] such that every particle (an appropriate analog of a connected component to recurse on) of the said extended strip decomposition has at most n/2 vertices. © 2024 Copyright held by the owner/author(s).",Max independent set; QPTAS; subdivided claw; subexponential-time algorithm,Approximation algorithms; Graph theory; Polynomial approximation; Connected component; Free graphs; Max independent sets; Path arguments; QPTAS; Running time; Subdivided claw; Subexponential time; Subexponential-time algorithm; Time algorithms; Graphic methods
Optimal Polynomial-Time Compression for Boolean Max CSP,2024,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187788865&doi=10.1145%2f3624704&partnerID=40&md5=d7528a80c6a6a54d2a06e9bb8e6ed99f,"In the Boolean maximum constraint satisfaction problem-Max CSP(Γ)-one is given a collection of weighted applications of constraints from a finite constraint language Γ, over a common set of variables, and the goal is to assign Boolean values to the variables so that the total weight of satisfied constraints is maximized. There exists a concise dichotomy theorem providing a criterion on Γ for the problem to be polynomial-time solvable and stating that otherwise, it becomes NP-hard. We study the NP-hard cases through the lens of kernelization and provide a complete characterization of Max CSP(Γ) with respect to the optimal compression size. Namely, we prove that Max CSP(Γ) parameterized by the number of variables n is either polynomial-time solvable, or there exists an integer d ≥ 2 depending on Γ, such that: (1) An instance of Max CSP(Γ) can be compressed into an equivalent instance with O(nd logn) bits in polynomial time, (2) Max CSP(Γ) does not admit such a compression to O(nd-ϵ) bits unless NP ⊆ co-NP/poly. Our reductions are based on interpreting constraints as multilinear polynomials combined with the framework of ""constraint implementations"", formerly used in the context of APX-hardness. As another application of our reductions, we reveal tight connections between optimal running times for solving Max CSP(Γ). More precisely, we show that obtaining a running time of the form O(2(1-ϵ)n) for particular classes of Max CSPs is as hard as breaching this barrier for Max d-SAT for some d.  © 2024 Copyright held by the owner/author(s).",Constraint satisfaction problem; exponential-time algorithms; kernelization,Optimization; Polynomial approximation; % reductions; Constraint-satisfaction problems; Exponential time algorithm; Finite constraints; Kernelization; Maximum constraint satisfaction problems; NP-hard; Polynomial-time; Running time; Time compression; Constraint satisfaction problems
Quantum Communication Complexity of Linear Regression,2024,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187802643&doi=10.1145%2f3625225&partnerID=40&md5=227d4632a84db881dae89ea4a5c21984,"Quantum computers may achieve speedups over their classical counterparts for solving linear algebra problems. However, in some cases—such as for low-rank matrices—dequantized algorithms demonstrate that there cannot be an exponential quantum speedup. In this work, we show that quantum computers have provable polynomial and exponential speedups in terms of communication complexity for some fundamental linear algebra problems if there is no restriction on the rank. We mainly focus on solving linear regression and Hamiltonian simulation. In the quantum case, the task is to prepare the quantum state of the result. To allow for a fair comparison, in the classical case, the task is to sample from the result. We investigate these two problems in two-party and multiparty models, propose near-optimal quantum protocols, and prove quantum/classical lower bounds. In this process, we propose an efficient quantum protocol for quantum singular value transformation, which is a powerful technique for designing quantum algorithms. We feel this will be helpful in developing efficient quantum protocols for many other problems. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",communication complexity; linear regression; Quantum computing; quantum singular value transformation,Computational complexity; Linear algebra; Linear transformations; Quantum communication; Quantum computers; Quantum theory; Classical counterpart; Communication complexity; Exponentials; Linear algebra problems; Quanta computers; Quantum communication complexity; Quantum Computing; Quantum protocols; Quantum singular value transformation; Singular values; Linear regression
"On p-Group Isomorphism: Search-to-Decision, Counting-to-Decision, and Nilpotency Class Reductions via Tensors",2024,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187939049&doi=10.1145%2f3625308&partnerID=40&md5=677d24a961292c54b7216de1d4e9ea14,[No abstract available],,
Hard QBFs for Merge Resolution,2024,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195835178&doi=10.1145%2f3638263&partnerID=40&md5=b3a2fab13698c04660729578d91ecb33,"We prove the first genuine quantified Boolean formula (QBF) proof size lower bounds for the proof system Merge Resolution (MRes), a refutational proof system for prenex QBFs with a CNF matrix. Unlike most QBF resolution systems in the literature, proofs in MRes consist of resolution steps together with information on countermodels, which are syntactically stored in the proofs as merge maps. This makes MRes quite powerful: it has strategy extraction by design and allows short proofs for formulas that are hard for classical QBF resolution systems. Here, we show the first genuine QBF exponential lower bounds for MRes, thereby uncovering limitations of MRes. Technically, the results are either transferred from bounds from circuit complexity (for restricted versions of MRes) or directly obtained by combinatorial arguments (for full MRes). Our results imply that the MRes approach is largely orthogonal to other QBF resolution models such as the QCDCL resolution systems QRes and QURes and the expansion systems ∀Exp + Res and IR.  Copyright © 2024 held by the owner/author(s).",lower bounds; proof complexity; QBF; resolution,Mergers and acquisitions; Circuit complexity; Counter-models; Exponentials; Low bound; matrix; Proof complexity; Proof system; Quantified Boolean formulas; Resolution; Resolution systems; Boolean functions
Bounded Degree Nonnegative Counting CSP,2024,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195873229&doi=10.1145%2f3632184&partnerID=40&md5=9d246149f290587c1619c9a40746032a,"Constraint satisfaction problems (CSP) encompass an enormous variety of computational problems. In particular, all partition functions from statistical physics, such as spin systems, are special cases of counting CSP (#CSP). We prove a complete complexity classification for every counting problem in #CSP with nonnegative valued constraint functions that is valid when every variable occurs a bounded number of times in all constraints. We show that, depending on the set of constraint functions ℱ, every problem in the complexity class #CSP(ℱ) defined by ℱ is either polynomial-time computable for all instances without the bounded occurrence restriction, or is #P-hard even when restricted to bounded degree input instances. The constant bound in the degree depends on ℱ. The dichotomy criterion on ℱ is decidable. As a second contribution, we prove a slightly modified but more streamlined decision procedure (from [14]) to test for the tractability of #CSP(ℱ). This procedure on an input ℱ tells us which case holds in the dichotomy for #CSP(ℱ). This more streamlined decision procedure enables us to fully classify a family of directed weighted graph homomorphism problems. This family contains both P-time tractable problems and #P-hard problems. To our best knowledge, this is the first family of such problems explicitly classified that are not acyclic, thereby the Lovász-goodness criterion of Dyer-Goldberg-Paterson [24] cannot be applied.  Copyright © 2024 held by the owner/author(s).",complexity dichotomy; Computational counting complexity; constraint satisfaction problems; counting CSPs; graph homomorphisms; nonnegative counting CSP,Computational complexity; Directed graphs; Polynomial approximation; Statistical Physics; Bounded degree; Complexity dichotomies; Computational counting complexity; Constraint-satisfaction problems; Counting complexity; Counting CSP; Graph homomorphisms; Non negatives; Nonnegative counting constraint satisfaction problem; Constraint satisfaction problems
Catalytic Branching Programs from Groups and General Protocols,2023,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181480206&doi=10.1145%2f3583085&partnerID=40&md5=c4cd506d95158b7643b76eb00f252a59,"CCCatalytic branching programs (catalytic bps) compute the same n-bit Boolean function f at multiple entry points that need to be remembered at the exit nodes of the branching program (bp). When a doubly exponential number of entry points is allowed, linear amortized catalytic bp size is known to be achievable for any f . Here a method is introduced that produces a catalytic bp out of a reversible bp and a deterministic dag-like communication protocol. In a multiplicity range as low as linear, approximating a threshold is shown possible at linear amortized cost. In the same low range, computing Maj and Mod are shown possible at a cost that beats the brute force repetition of the best known bp for these functions by a polylog factor. In the exponential range, the method yields O(n log n) amortized cost for any symmetric function. © 2023 Copyright held by the owner/author(s)",Boolean functions; Branching programs; catalytic space; space complexity,Costs; Branching programs; Brute force; Catalytic space; Communications protocols; Deterministics; Entry point; Exponential numbers; Polylogs; Program size; Space complexity; Boolean functions
The Complexity Landscape of Fixed-Parameter Directed Steiner Network Problems,2023,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181496507&doi=10.1145%2f3580376&partnerID=40&md5=39a6040d8f637a0b16b9df96a4fc8d01,"Given a directed graph G and a list (s1, t1), . . ., (sd, td) of terminal pairs, the Directed Steiner Network problem asks for a minimum-cost subgraph of G that contains a directed si → ti path for every 1 ≤ i ≤ d. The special case Directed Steiner Tree (when we ask for paths from a root r to terminals t1, . . ., td) is known to be fixed-parameter tractable parameterized by the number of terminals, while the special case Strongly Connected Steiner Subgraph (when we ask for a path from every ti to every other tj) is known to be W[1]-hard parameterized by the number of terminals. We systematically explore the complexity landscape of directed Steiner problems to fully understand which other special cases are FPT or W[1]-hard. Formally, if H is a class of directed graphs, then we look at the special case of Directed Steiner Network where the list (s1, t1), . . ., (sd, td) of demands form a directed graph that is a member of H . Our main result is a complete characterization of the classes H resulting in fixed-parameter tractable special cases: we show that if every pattern in H has the combinatorial property of being “transitively equivalent to a bounded-length caterpillar with a bounded number of extra edges,” then the problem is FPT, and it is W[1]-hard for every recursively enumerable H not having this property. This complete dichotomy unifies and generalizes the known results showing that Directed Steiner Tree is FPT [Dreyfus and Wagner, Networks 1971], q-Root Steiner Tree is FPT for constant q [Suchý, WG 2016], Strongly Connected Steiner Subgraph is W[1]-hard [Guo et al., SIAM J. Discrete Math. 2011], and Directed Steiner Network is solvable in polynomial-time for constant number of terminals [Feldman and Ruhl, SIAM J. Comput. 2006], and moreover reveals a large continent of tractable cases that were not known before. © 2023 Copyright held by the owner/author(s)",Directed Steiner networks; fixed-parameter tractability,Complex networks; Polynomial approximation; Trees (mathematics); Directed steiner network; Directed steiner trees; Fixed-parameter tractability; Graph G; Network problems; Parameterized; Steiner; Steiner network; Strongly connected; Subgraphs; Directed graphs
Forgetfulness Can Make You Faster: An O∗(8.097k)-time Algorithm for Weighted 3-set k-packing,2023,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181531123&doi=10.1145%2f3599722&partnerID=40&md5=986c9c782760a05da5f8b810ea85bd56,"In this article, we study the classic Weighted 3-Set k-Packing problem: given a universe U, a family S of subsets of size 3 of U, a weight function w : S → R, W ∈ R, and a parameter k ∈ N, the objective is to decide if there is a subfamily S ´ ⊆ S of k disjoint sets and total weight at least W . We present a deterministic parameterized algorithm for this problem that runs in time O∗ (8.097k ), where O∗ hides factors polynomial in the input size. This substantially improves upon the previously best deterministic algorithm for Weighted 3-Set k-Packing, which runs in time O∗ (12.155k ) SIDMA [18], and was also the best deterministic algorithm for the unweighted version of this problem. Our algorithm is based on a novel application of the method of representative sets that might be of independent interest. © 2023 Copyright held by the owner/author(s)",3-set k-packing; P<sub>2</sub>-packing; Representative sets,3-set k-packing; Deterministic algorithms; Deterministics; Disjoint sets; P2-packing; Packing problems; Parameterized algorithm; Representative set; Time algorithms; Weight functions; Parameter estimation
Parameterized Complexity of Feature Selection for Categorical Data Clustering,2023,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181502168&doi=10.1145%2f3604797&partnerID=40&md5=ad9ce2631e4bb71821792c00c5d04688,"We develop new algorithmic methods with provable guarantees for feature selection in regard to categorical data clustering. While feature selection is one of the most common approaches to reduce dimensionality in practice, most of the known feature selection methods are heuristics. We study the following mathematical model. We assume that there are some inadvertent (or undesirable) features of the input data that unnecessarily increase the cost of clustering. Consequently, we want to select a subset of the original features from the data such that there is a small-cost clustering on the selected features. More precisely, for given integers ι (the number of irrelevant features) and k (the number of clusters), budget B, and a set of n categorical data points (represented by m-dimensional vectors whose elements belong to a finite set of values Σ), we want to select m − ι relevant features such that the cost of any optimal k-clustering on these features does not exceed B. Here the cost of a cluster is the sum of Hamming distances (ι0-distances) between the selected features of the elements of the cluster and its center. The clustering cost is the total sum of the costs of the clusters. We use the framework of parameterized complexity to identify how the complexity of the problem depends on parameters k, B, and |Σ|. Our main result is an algorithm that solves the Feature Selection problem in time f (k, B, |Σ|) · mд(k, |Σ |) · n2 for some functions f and д. In other words, the problem is fixed-parameter tractable parameterized by B when |Σ| and k are constants. Our algorithm for Feature Selection is based on a solution to a more general problem, Constrained Clustering with Outliers. In this problem, we want to delete a certain number of outliers such that the remaining points could be clustered around centers satisfying specific constraints. One interesting fact about Constrained Clustering with Outliers is that besides Feature Selection, it encompasses many other fundamental problems regarding categorical data such as Robust Clustering and Binary and Boolean Low-rank Matrix Approximation with Outliers. Thus, as a byproduct of our theorem, we obtain algorithms for all these problems. We also complement our algorithmic findings with complexity lower bounds. © 2023 Copyright held by the owner/author(s)",hypergraph enumeration; Low-rank approximation; PCA; Robust clustering,Budget control; Cluster analysis; Computational complexity; Feature Selection; Heuristic methods; Parallel processing systems; Parameterization; Statistics; Categorical data; Categorical data clustering; Clusterings; Features selection; Hyper graph; Hypergraph enumeration; Low rank approximations; Parameterized complexity; PCA; Robust clustering; Clustering algorithms
Sparsification Lower Bounds for List H-Coloring,2023,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181516534&doi=10.1145%2f3612938&partnerID=40&md5=89def3aafc99b23d958685ccf7b9956d,"We investigate the List H-Coloring problem, the generalization of graph coloring that asks whether an input graph G admits a homomorphism to the undirected graph H (possibly with loops), such that each vertex v ∈ V (G) is mapped to a vertex on its list L(v) ⊆ V (H). An important result by Feder, Hell, and Huang [JGT 2003] states that List H-Coloring is polynomial-time solvable if H is a so-called bi-arc graph, and NP-complete otherwise. We investigate the NP-complete cases of the problem from the perspective of polynomial-time sparsification: can an n-vertex instance be efficiently reduced to an equivalent instance of bitsize O(n2−ε ) for some ε > 0? We prove that if H is not a bi-arc graph, then List H-Coloring does not admit such a sparsification algorithm unless NP ⊆ coNP/poly. Our proofs combine techniques from kernelization lower bounds with a study of the structure of graphs H which are not bi-graphs. © 2023 Copyright held by the owner/author(s)",constraint satisfaction problem; List H-coloring; sparsification,Coloring; Polynomial approximation; Undirected graphs; Coloring problems; Constraint-satisfaction problems; Generalisation; Graph colorings; Input graphs; List H-coloring; Low bound; NP Complete; Polynomial-time; Sparsification; Constraint satisfaction problems
On Protocols for Monotone Feasible Interpolation,2023,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164243386&doi=10.1145%2f3583754&partnerID=40&md5=6e2930cf072dfeacfac3c6edb95c8281,"Dag-like communication protocols, a generalization of the classical tree-like communication protocols, are useful objects in the realm of proof complexity (most importantly for monotone feasible interpolation) and circuit complexity. We consider three kinds of protocols in this article (d is the degree of a protocol): - IEQ-d-dags: feasible sets of these protocols are described by inequality which means that the feasible sets are combinatorial triangles; these protocols are also called triangle-dags in the literature, - EQ-d-dags: feasible sets are described by equality, and - c-IEQ-d-dags: feasible sets are described by a conjunction of c inequalities.Garg, Göös, Kamath, and Sokolov (Theory of Computing, 2020) mentioned all these protocols, and they noted that EQ-d-dags are a special case of c-IEQ-d-dags. The exact relationship between these types of protocols is unclear. As our main contribution, we prove the following statement: EQ-2-dags can efficiently simulate c-IEQ-d-dags when c and d are constants. This implies that EQ-2-dags are at least as strong as IEQ-d-dags and that EQ-2-dags have the same strength as c-IEQ-d-dags for c ≥ 2 (because 2-IEQ-2-dags can trivially simulate EQ-2-dags).Hrubeš and Pudlák (Information Processing Letters, 2018) proved that IEQ-d-dags over the monotone Karchmer-Wigderson relation are equivalent to monotone real circuits which implies that we have exponential lower bounds for these protocols. Lower bounds for EQ-2-dags would directly imply lower bounds for the proof system R(LIN).  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesDag-like protocols; circuit complexity; communication complexity; proof complexity,Computational complexity; Timing circuits; Additional key word and phrasesdag-like protocol; Circuit complexity; Communication complexity; Communications protocols; Feasible interpolation; Feasible set; Generalisation; Key words; Low bound; Proof complexity; Interpolation
Constructing Faithful Homomorphisms over Fields of Finite Characteristic,2023,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164239458&doi=10.1145%2f3580351&partnerID=40&md5=e7a03e13df3dab196c4c00751a5fb8b3,"We study the question of algebraic rank or transcendence degree preserving homomorphisms over finite fields. This concept was first introduced by Beecken et al. [3] and exploited by them, and Agrawal et al. [2] to design algebraic independence-based identity tests using the Jacobian criterion over characteristic zero fields. An analogue of such constructions over finite characteristic fields was unknown due to the failure of the Jacobian criterion over finite characteristic fields. Building on a recent criterion of Pandey et al. [15], we construct explicit faithful maps for some natural classes of polynomials in the positive characteristic field setting, when a certain parameter called the inseparable degree of the underlying polynomials is bounded (this parameter is always 1 in fields of characteristic zero). This presents the first generalisation of some of the results of Beecken et al. [3] and Agrawal et al. [2] in the positive characteristic setting.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and Phrases: Faithful homomorphisms; algebraic independence; finite characteristic fields; identity testing,Additional key word and phrase: faithful homomorphism; Algebraic independence; Finite characteristic field; Finite fields; Identity testing; In-field; Jacobian criterions; Key words; Key-phrase; Zero fields; Algebra
Quantum Time-Space Tradeoff for Finding Multiple Collision Pairs,2023,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164247063&doi=10.1145%2f3589986&partnerID=40&md5=10b37d668ef2bea64985472c34bcffe0,"We study the problem of finding K collision pairs in a random function f : [N] → [N] by using a quantum computer. We prove that the number of queries to the function in the quantum random oracle model must increase significantly when the size of the available memory is limited. Namely, we demonstrate that any algorithm using S qubits of memory must perform a number T of queries that satisfies the tradeoff T3 S ≥ ω (K3 N). Classically, the same question has only been settled recently by Dinur [22], who showed that the Parallel Collision Search algorithm of van Oorschot and Wiener [36] achieves the optimal time-space tradeoff of T2 S = Θ (K2 N). Our result limits the extent to which quantum computing may decrease this tradeoff. Our method is based on a novel application of Zhandry's recording query technique [42] for proving lower bounds in the exponentially small success probability regime. As a second application, we give a simpler proof of the time-space tradeoff T2 S ≥ ω (N3) for sorting N numbers on a quantum computer, which was first obtained by Klauck, Špalek, and de Wolf [30].  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesQuantum computing; lower bound; query complexity; time-space tradeoff,Quantum optics; Additional key word and phrasesquantum computing; Collision pairs; Key words; Low bound; Multiple collisions; Quanta computers; Query complexity; Random functions; Random Oracle model; Time-space tradeoffs; Quantum computers
Groups with ALOGTIME-hard Word Problems and PSPACE-complete Compressed Word Problems,2023,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147898727&doi=10.1145%2f3569708&partnerID=40&md5=ffff0121678a2218dd6bde71dca0dc60,"We give lower bounds on the complexity of the word problem for a large class of non-solvable infinite groups that we call strongly efficiently non-solvable groups. This class includes free groups, Grigorchuk's group, and Thompson's groups. We prove that these groups have an NC1-hard word problem and that for some of them (including Grigorchuk's group and Thompson's groups) the compressed word problem (which is equivalent to the circuit evaluation problem) is PSPACE-complete. © 2022 held by the owner/author(s). Publication rights licensed to ACM.",G-programs; Grigorchuk's group; NC<sup>1</sup>-hardness; non-solvable groups; self-similar groups; straight-line programs; Thompson's groups; word problem,G-program; Grigorchuk group; NC1-hardness; Non-solvable group; Self-similar; Self-similar group; Solvable group; Straight line program; Thompson; Thompson group; Word problem
Linearly Ordered Colourings of Hypergraphs,2023,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148018828&doi=10.1145%2f3570909&partnerID=40&md5=b5309c0cf36c7022601f26067e74d054,"A linearly ordered (LO) k-colouring of an r-uniform hypergraph assigns an integer from {1, ... , k } to every vertex so that, in every edge, the (multi)set of colours has a unique maximum. Equivalently, for r = 3, if two vertices in an edge are assigned the same colour, then the third vertex is assigned a larger colour (as opposed to a different colour, as in classic non-monochromatic colouring). Barto, Battistelli, and Berg [STACS'21] studied LO colourings on 3-uniform hypergraphs in the context of promise constraint satisfaction problems (PCSPs). We show two results.First, given a 3-uniform hypergraph that admits an LO 2-colouring, one can find in polynomial time an LO k-colouring with .Second, given an r-uniform hypergraph that admits an LO 2-colouring, we establish NP-hardness of finding an LO k-colouring for every constant uniformity r≥k+2. In fact, we determine relationships between polymorphism minions for all uniformities r≥ 3, which reveals a key difference between r< k+2 and r≥ k+2 and which may be of independent interest. Using the algebraic approach to PCSPs, we actually show a more general result establishing NP-hardness of finding an LO k-colouring for LO ĝ.,""-colourable r-uniform hypergraphs for 2 ≤ gℓ ≤ k and r ≥ k - gℓ + 4. © 2022 Association for Computing Machinery.",algebraic approach; Hypegraph colourings; minions; PCSP; polymorphisms; promise constraint satisfaction,Color; Graph theory; Hardness; Polynomial approximation; 3-uniform hypergraphs; Algebraic approaches; Constraint Satisfaction; Constraint-satisfaction problems; Hypegraph coloring; K-coloring; Minion; Promise constraint satisfaction; Promise constraint satisfaction problem; R-uniform hypergraphs; Constraint satisfaction problems
Inapproximability of Counting Hypergraph Colourings,2023,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142804429&doi=10.1145%2f3558554&partnerID=40&md5=c75d7285f9ca4d87e430f4c41b914ad0,"Recent developments in approximate counting have made startling progress in developing fast algorithmic methods for approximating the number of solutions to constraint satisfaction problems (CSPs) with large arities, using connections to the Lovász Local Lemma. Nevertheless, the boundaries of these methods for CSPs with non-Boolean domain are not well-understood. Our goal in this article is to fill in this gap and obtain strong inapproximability results by studying the prototypical problem in this class of CSPs, hypergraph colourings.More precisely, we focus on the problem of approximately counting q-colourings on K-uniform hypergraphs with bounded degree Î"". An efficient algorithm exists if [Jain et al. 25; He et al. 23]. Somewhat surprisingly however, a hardness bound is not known even for the easier problem of finding colourings. For the counting problem, the situation is even less clear and there is no evidence of the right constant controlling the growth of the exponent in terms of K. To this end, we first establish that for general q computational hardness for finding a colouring on simple/linear hypergraphs occurs at Δĝ‰3 KqK, almost matching the algorithm from the Lovász Local Lemma. Our second and main contribution is to obtain a far more refined bound for the counting problem that goes well beyond the hardness of finding a colouring and which we conjecture is asymptotically tight (up to constant factors). We show in particular that for all even q ≥ 4 it is NP-hard to approximate the number of colourings when Δ‰3 qK/2. Our approach is based on considering an auxiliary weighted binary CSP model on graphs, which is obtained by ""halving""the K-ary hypergraph constraints. This allows us to utilise reduction techniques available for the graph case, which hinge upon understanding the behaviour of random regular bipartite graphs that serve as gadgets in the reduction. The major challenge in our setting is to analyse the induced matrix norm of the interaction matrix of the new CSP which captures the most likely solutions of the system. In contrast to previous analyses in the literature, the auxiliary CSP demonstrates both symmetry and asymmetry, making the analysis of the optimisation problem severely more complicated and demanding the combination of delicate perturbation arguments and careful asymptotic estimates. © 2022 held by the owner/author(s). Publication rights licensed to ACM.",Approximate counting; colouring; counting complexity; hypergraph; local lemma,Graph theory; Hardness; Matrix algebra; Algorithmic methods; Approximate counting; Constraint-satisfaction problems; Counting complexity; Counting problems; Hyper graph; Hypergraph coloring; Inapproximability; Local lemmata; Lovasz local lemma; Constraint satisfaction problems
An Algorithmic Meta-Theorem for Graph Modification to Planarity and FOL,2023,ACM Transactions on Computation Theory,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148040040&doi=10.1145%2f3571278&partnerID=40&md5=b3a92908cc2f17aeab93b0aa6c964630,"In general, a graph modification problem is defined by a graph modification operation and a target graph property . Typically, the modification operation may be vertex deletion, edge deletion, edge contraction, or edge addition and the question is, given a graph G and an integer k, whether it is possible to transform G to a graph in after applying the operation k times on G. This problem has been extensively studied for particular instantiations of and . In this article, we consider the general property of being planar and, additionally, being a model of some First-Order Logic (FOL) sentence (an FOL-sentence). We call the corresponding meta-problem Graph -Modification to Planarity and and prove the following algorithmic meta-theorem: there exists a function f : ĝ.,•2 → ĝ.,• such that, for every and every FOL-sentence , the Graph -Modification to Planarity and is solvable in f(k,||)g n2 time. The proof constitutes a hybrid of two different classic techniques in graph algorithms. The first is the irrelevant vertex technique that is typically used in the context of Graph Minors and deals with properties such as planarity or surface-embeddability (that are not FOL-expressible) and the second is the use of Gaifman's locality theorem that is the theoretical base for the meta-algorithmic study of FOL-expressible problems. © 2022 Association for Computing Machinery.",algorithmic meta-theorems; First-Order Logic; Graph modification problems; irrelevant vertex technique; planar graphs,Formal logic; Graph theory; Algorithmic meta-theorem; Algorithmics; First order logic; Graph modification problems; Graph modifications; Irrelevant vertex technique; Meta-theorems; Planar graph; Planarity; Property; Computer circuits
