Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
On the Analysis and Evaluation of Proximity-based Load-balancing Policies,2022,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146437581&doi=10.1145%2f3549933&partnerID=40&md5=d10bd5bfb83a05f7b0a3617338cebac8,"Distributed load balancing is the act of allocating jobs among a set of servers as evenly as possible. The static interpretation of distributed load balancing leads to formulating the load-balancing problem as a classical balls-and-bins problem with jobs (balls) never leaving the system and accumulating at the servers (bins). While most of the previous work in the static setting focus on studying the maximum number of jobs allocated to a server or maximum load, little importance has been given to the implementation cost, or the cost of moving a job/data to/from its allocated server, for such policies. This article designs and evaluates server proximity aware static load-balancing policies with a goal to reduce the implementation cost. We consider a class of proximity aware Power of Two (POT) choice-based assignment policies for allocating jobs to servers, where both jobs and servers are located on a two-dimensional Euclidean plane. In this framework, we investigate the tradeoff between the implementation cost and load-balancing performance of different allocation policies. To this end, we first design and evaluate a Spatial Power of two (sPOT) policy in which each job is allocated to the least loaded server among its two geographically nearest servers. We provide expressions for the lower bound on the asymptotic expected maximum load on the servers and prove that sPOT does not achieve classical POT load-balancing benefits. However, experimental results suggest the efficacy of sPOT with respect to expected implementation cost. We also propose two non-uniform server sampling-based POT policies that achieve the best of both implementation cost and load-balancing performance. We then extend our analysis to the case where servers are interconnected as an n-vertex graph G(S, E). We assume each job arrives at one of the servers, u, chosen uniformly at random from the vertex set S. We then assign each job to the server with minimum load among servers u and v where v is chosen according to one of the following two policies: (i) Unif-POT(k): Sample a server v uniformly at random from k-hop neighborhood of u; (ii) InvSq-POT(k): Sample a server v from k-hop neighborhood of u with probability proportional to the inverse square of the distance between u and v. An extensive simulation over a wide range of topologies validates the efficacy of both the policies. Our simulation results show that both policies consistently produce a load distribution that is much similar to that of a classical POT. Depending on topology, we observe the total variation distance to be of the order of 0.002-0.08 for both the policies while achieving a 8%-99% decrease in implementation cost as compared to the classical POT.  © 2022 Association for Computing Machinery.",balls and bins; load balancing; Power of Two choices; proximity aware,Balancing; Graph theory; Ball and bin; Distributed load balancing; Implementation cost; Load balancing policies; Load-Balancing; Maximum load; Performance; Power of two choice; Power-of-two; Proximity aware; Geometry
Focused Layered Performance Modelling by Aggregation,2022,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146439018&doi=10.1145%2f3549539&partnerID=40&md5=d1308e6f730826a015e99e101a2f94e1,"Performance models of server systems, based on layered queues, may be very complex. This is particularly true for cloud-based systems based on microservices, which may have hundreds of distinct components, and for models derived by automated data analysis. Often only a few of these many components determine the system performance, and a smaller simplified model is all that is needed. To assist an analyst, this work describes a focused model that includes the important components (the focus) and aggregates the rest in groups, called dependency groups. The method Focus-based Simplification with Preservation of Tasks described here fills an important gap in a previous method by the same authors. The use of focused models for sensitivity predictions is evaluated empirically in the article on a large set of randomly generated models. It is found that the accuracy depends on a ""saturation ratio""(SR) between the highest utilization value in the model and the highest value of a component excluded from the focus; evidence suggests that SR must be at least 2 and must be larger to evaluate larger model changes. This dependency was captured in an ""Accurate Sensitivity Hypothesis""based on SR, which can be used to indicate trustable sensitivity results.  © 2022 Association for Computing Machinery.",layered queuing network models; model aggregation; model sensitivity; model simplification; Performance models,Queueing networks; Cloud-based; Layered queuing; Layered queuing network model; Model aggregations; Model sensitivity; Model simplification; Performance Modeling; Queuing network model; Saturation ratio; Server system; Network layers
A New Upper Bound on Cache Hit Probability for Non-Anticipative Caching Policies,2022,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146435963&doi=10.1145%2f3547332&partnerID=40&md5=973f01aabf6078aa31907b6dce22447d,"Caching systems have long been crucial for improving the performance of a wide variety of network and web-based online applications. In such systems, end-to-end application performance heavily depends on the fraction of objects transferred from the cache, also known as the cache hit probability. Many caching policies have been proposed and implemented to improve the hit probability. In this work, we propose a new method to compute an upper bound on hit probability for all non-anticipative caching policies and for policies that have no knowledge of future requests. Our key insight is to order the objects according to the ratio of their Hazard Rate(HR) function values to their sizes, and place in the cache the objects with the largest ratios till the cache capacity is exhausted. When object request processes are conditionally independent, we prove that this cache allocation based on the HR-to-size ratio rule guarantees the maximum achievable expected number of object hits across all non-anticipative caching policies. Further, the HR ordering rule serves as an upper bound on cache hit probability when object request processes follow either independent delayed renewal process or a Markov modulated Poisson process. We also derive closed form expressions for the upper bound under some specific object request arrival processes. We provide simulation results to validate its correctness and to compare it to the state-of-the-art upper bounds, such as produced by Bélády's algorithm. We find it to be tighter than state-of-the-art upper bounds for some specific object request arrival processes such as independent renewal, Markov modulated, and shot noise processes.  © 2022 Association for Computing Machinery.",Caching; content delivery network; Hazard Rate; hit probability; upper bound; variable size objects,Hazards; Online systems; Arrival process; Cache hits; Caching; Caching policy; Content delivery network; Hazard rates; Hit probability; Upper Bound; Variable size object; Variable sizes; Shot noise
Big Winners and Small Losers of Zero-rating,2022,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139339358&doi=10.1145%2f3539731&partnerID=40&md5=343d375f5e47ac77c33457946f3bc5d2,"An objective of network neutrality is to design regulations for the Internet and ensure that it remains a public, open platform where innovations can thrive. While there is broad agreement that preserving the content quality of service falls under the purview of net neutrality, the role of differential pricing, especially the practice of zero-rating, remains controversial. Zero-rating refers to the practice of providing free Internet access to some users under certain conditions, which usually concurs with differentiation among users or content providers. Even though some countries (India, Canada) have banned zero-rating, others have either taken no stance or explicitly allowed it (South Africa, Kenya, U.S.). In this article, we model zero-rating between Internet service providers and content providers (CPs) to better understand the conditions under which offering zero-rating is preferred, and who gains in utility. We develop a formulation in which providers' incomes vary, from low-income startups to high-income incumbents, where their decisions to zero-rate are a variation of the traditional prisoner's dilemma game. We find that if zero-rating is permitted, low-income CPs often lose utility, whereas high-income CPs often gain utility. We also study the competitiveness of the CP markets via the Herfindahl Index. Our findings suggest that in most cases the introduction of zero-rating reduces competitiveness.  © 2022 Association for Computing Machinery.",Differential pricing; network neutrality; zero rating,Competition; Developing countries; Game theory; Quality of service; Condition; Content providers; Contents qualities; Differential pricing; Low incomes; Net neutralities; Network neutralities; Open platforms; Quality-of-service; Zero rating; Costs
RL-QN: A Reinforcement Learning Framework for Optimal Control of Queueing Systems,2022,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139178811&doi=10.1145%2f3529375&partnerID=40&md5=8ff7f76d7b43a38d6f351dc9b4d3e091,"With the rapid advance of information technology, network systems have become increasingly complex and hence the underlying system dynamics are often unknown or difficult to characterize. Finding a good network control policy is of significant importance to achieve desirable network performance (e.g., high throughput or low delay). In this work, we consider using model-based reinforcement learning (RL) to learn the optimal control policy for queueing networks so that the average job delay (or equivalently the average queue backlog) is minimized. Traditional approaches in RL, however, cannot handle the unbounded state spaces of the network control problem. To overcome this difficulty, we propose a new algorithm, called RL for Queueing Networks (RL-QN), which applies model-based RL methods over a finite subset of the state space while applying a known stabilizing policy for the rest of the states. We establish that the average queue backlog under RL-QN with an appropriately constructed subset can be arbitrarily close to the optimal result. We evaluate RL-QN in dynamic server allocation, routing, and switching problems. Simulation results show that RL-QN minimizes the average queue backlog effectively. © 2022 Association for Computing Machinery.",Queueing networks; reinforcement learning,Learning systems; Queueing networks; Queueing theory; Learning frameworks; Model-based reinforcement learning; Network systems; Network-control; Optimal controls; Queueing system; Reinforcement learnings; State-space; Technology network; Underlying systems; Reinforcement learning
Performance Health Index for Complex Cyber Infrastructures,2022,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137542821&doi=10.1145%2f3538646&partnerID=40&md5=95e98824e3256dff59778c8ff91c624d,"Most IT systems depend on a set of configuration variables (CVs), expressed as a name/value pair that collectively defines the resource allocation for the system. While the ill effects of misconfiguration or improper resource allocation are well-known, there are no effective a priori metrics to quantify the impact of the configuration on the desired system attributes such as performance, availability, etc. In this paper, we propose a Configuration Health Index (CHI) framework specifically attuned to the performance attribute to capture the influence of CVs on the performance aspects of the system. We show how CHI, which is defined as a configuration scoring system, can take advantage of the domain knowledge and the available (but rather limited) performance data to produce important insights into the configuration settings. We compare the CHI with both well-advertised segmented non-linear models and state-of-the-art data-driven models, and show that the CHI not only consistently provides better results but also avoids the dangers of a pure data drive approach which may predict incorrect behavior or eliminate some essential configuration variables from consideration.  © 2022 Association for Computing Machinery.",configuration behavior; configuration modeling; health index; health score; Highly configurable systems; nonlinear regression; performance model; resource allocation,Digital storage; Domain Knowledge; Health; Configurable systems; Configuration behavior; Configuration modeling; Health indices; Health scores; Highly configurable system; Non-linear regression; Performance; Performance Modeling; Resources allocation; Resource allocation
PMU-Events-Driven DVFS Techniques for Improving Energy Efficiency of Modern Processors,2022,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139230902&doi=10.1145%2f3538645&partnerID=40&md5=3bbdb0a967678cd03938e2896d545897,"This paper describes the results of our measurement-based study, conducted on an Intel Core i7 processor running the SPEC CPU2017 benchmark suites, that evaluates the impact of dynamic voltage frequency scaling (DVFS) on performance (P), energy efficiency (EE), and their product (PxEE). The results indicate that the default DVFS-based power management techniques heavily favor performance, resulting in poor energy efficiency. To remedy this problem, we introduce, implement, and evaluate four DVFS-based power management techniques driven by the following metrics derived from the processor's performance monitoring unit: (i) the total pipeline slot stall ratio (FS-PS), (ii) the total cycle stall ratio (FS-TS), (iii) the total memory-related cycle stall ratio (FS-MS), and (iv) the number of last level cache misses per kilo instructions (FS-LLCM). The proposed techniques linearly map these metrics onto the available processor clock frequencies. The experimental evaluation results show that the proposed techniques significantly improve EE and PxEE metrics compared to the existing approaches. Specifically, EE improves from 44% to 92%, and PxEE improves from 31% to 48% when all the benchmarks are considered together. Furthermore, we find that the proposed techniques are particularly effective for a class of memory-intensive benchmarks - they improve EE from 121% to 183% and PxEE from 100% to 141%. Finally, we elucidate the advantages and disadvantages of each of the proposed techniques and offer recommendations on using them. © 2022 Association for Computing Machinery.",benchmarking; DVFS; energy-efficiency; performance; performance monitoring unit; power management multicores; SPEC CPU2017,Dynamic frequency scaling; Energy efficiency; Pipeline processing systems; Voltage scaling; Dynamic voltage; Dynamic voltage frequency scaling; Frequency-scaling; Multi-cores; Performance; Performance monitoring unit; Performance-monitoring; Power management multicore; SPEC cpu2017; Voltage frequency; Benchmarking
Modeling Communication over Terrain for Realistic Simulation of Outdoor Sensor Network Deployments,2021,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128312835&doi=10.1145%2f3510306&partnerID=40&md5=4fff4ae0c25d1f1e1c4b91bababcf5ed,"Popular wireless network simulators have few available propagation models for outdoor Internet of Things applications. Of the available models, only a handful use real terrain data, yet an inaccurate propagation model can skew the results of simulations. In this article, we present TerrainLOS, a low-overhead propagation model for outdoor Internet of Things applications that uses real terrain data to determine whether two nodes can communicate. To the best of our knowledge, TerrainLOS is the first terrain-Aware propagation model that specifically targets outdoor IoT deployments and that uses height maps to represent terrain. In addition, we present a new terrain classification method based on terrain ""roughness,""which allows us to select a variety of terrain samples to demonstrate how TerrainLOS can capture the effects of terrain on communication. We also propose a technique to generate synthetic terrain samples based on ""roughness.""Furthermore, we implemented TerrainLOS in the COOJA-Contiki network simulation/emulation platform, which targets IoT deployments and uses TerrainLOS to evaluate how often a network is fully connected based on the roughness of terrain, as well as how two popular power-Aware routing protocols, RPL and ORPL, perform when terrain is considered.  © 2022 Copyright held by the owner/author(s).",Modeling communication; network performance; terrain,Internet of things; Power management (telecommunication); Simulation platform; Wireless sensor networks; Classification methods; Height map; Low overhead; Network deployment; Network simulators; Propagation models; Real terrain datum; Realistic simulation; Sensors network; Terrain classification; Landforms
Adversarial Deep Learning for Online Resource Allocation,2021,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128293199&doi=10.1145%2f3494526&partnerID=40&md5=5009ea239fc17240a15ad4c266e5ddf7,"Online algorithms are an important branch in algorithm design. Designing online algorithms with a bounded competitive ratio (in terms of worst-case performance) can be hard and usually relies on problem-specific assumptions. Inspired by adversarial training from Generative Adversarial Net and the fact that the competitive ratio of an online algorithm is based on worst-case input, we adopt deep neural networks (NNs) to learn an online algorithm for a resource allocation and pricing problem from scratch, with the goal that the performance gap between offline optimum and the learned online algorithm can be minimized for worst-case input.Specifically, we leverage two NNs as the algorithm and the adversary, respectively, and let them play a zero sum game, with the adversary being responsible for generating worst-case input while the algorithm learns the best strategy based on the input provided by the adversary. To ensure better convergence of the algorithm network (to the desired online algorithm), we propose a novel per-round update method to handle sequential decision making to break complex dependency among different rounds so that update can be done for every possible action instead of only sampled actions. To the best of our knowledge, our work is the first using deep NNs to design an online algorithm from the perspective of worst-case performance guarantee. Empirical studies show that our updating methods ensure convergence to Nash equilibrium and the learned algorithm outperforms state-of-The-Art online algorithms under various settings.  © 2022 Association for Computing Machinery.",adversarial learning; Neural networks; online algorithm,Computer games; Decision making; Deep neural networks; E-learning; Economics; Learning algorithms; Resource allocation; Adversarial learning; Algorithm design; Competitive ratio; Learn+; Neural-networks; On-line algorithms; Online resources; Resources allocation; Worst case inputs; Worst-case performance; Game theory
MDP-based Network Friendly Recommendations,2021,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128273915&doi=10.1145%2f3513131&partnerID=40&md5=bb07e45c79aa4f7f96fd0d5f2c11ab7a,"Controlling the network cost by delivering popular content to users, as well as improving streaming quality and overall user experience, have been key goals for content providers (CP) in recent years. While proposals to improve performance, through caching or other mechanisms (DASH, multicasting, etc.) abound, recent works have proposed to turn the problem on its head and complement such efforts. Instead of trying to reduce the cost to deliver every possible content to a user, a potentially very expensive endeavour, one could leverage omnipresent recommendations systems to nudge users towards the content of low(er) network cost, regardless of where this cost is coming from. In this paper, we focus on this latter problem, namely optimal policies for ""Network Friendly Recommendations""(NFR). A key contribution is the use of a Markov Decision Process (MDP) framework that offers significant advantages, compared to existing works, in terms of both modeling flexibility as well as computational efficiency. Specifically we show that this framework subsumes some state-of-The-Art approaches, and can also optimally tackle additional, more sophisticated setups. We validate our findings with real traces that suggest up to almost 2X in cost performance, and 10X in computational speed-up compared to recent state-of-The-Art works.  © 2022 Association for Computing Machinery.",Markov decision problem (MDP); recommendations,Computational efficiency; Multicasting; Content providers; Improve performance; Markov decision problem; Markov Decision Processes; Network costs; Optimal policies; Problem-based; Recommendation; Users' experiences; Markov processes
PathTracer: Understanding Response Time of Signal Processing Applications on Heterogeneous MPSoCs,2021,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128312911&doi=10.1145%2f3513003&partnerID=40&md5=20f629c205d5ec3054c6214cda8d2448,"In embedded and cyber-physical systems, the design of a desired functionality under constraints increasingly requires parallel execution of a set of tasks on a heterogeneous architecture. The nature of such parallel systems complicates the process of understanding and predicting performance in terms of response time. Indeed, response time depends on many factors related to both the functionality and the target architecture. State-of-The-Art strategies derive response time by examining the operations required by each task for both processing and accessing shared resources. This procedure is often followed by the addition or elimination of potential interference due to task concurrency. However, such approaches require an advanced knowledge of the software and hardware details, rarely available in practice.This work presents an alternative ""top-down""strategy, called PathTracer, aimed at understanding software response time and extending the cases in which it can be analyzed and estimated. PathTracer leverages on dataflow-based application representation and response time estimation of signal processing applications mapped on heterogeneous Multiprocessor Systems-on-A-Chip (MPSoCs). Experimental results demonstrate that PathTracer provides (i) information on the nature of the application (work-dominated, span-dominated, or balanced parallel), and (ii) response time modeling which can reach high accuracy when performed post-execution, leading to prediction errors with average and standard deviation under 5% and 3% respectively.  © 2022 Association for Computing Machinery.",dataflow; design space exploration; Model-based design; MPSoC; processing latency; signal processing applications,Data flow analysis; Digital signal processors; Integrated circuit design; Parallel processing systems; Response time (computer systems); Signal analysis; System-on-chip; Systems analysis; Dataflow; Design space exploration; Heterogeneous architectures; Heterogeneous multiprocessor systems; Model-based design; Multi processor systems; Multiprocessor system-on-A-chip; Parallel executions; Processing latency; Signal processing applications; Embedded systems
Discrete-Time Modeling of NFV Accelerators that Exploit Batched Processing,2021,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124018207&doi=10.1145%2f3488243&partnerID=40&md5=4535d16ff7312e3287472598bbf32593,"Network Functions Virtualization (NFV) is among the latest network revolutions, promising increased flexibility and avoiding network ossification. At the same time, all-software NFV implementations on commodity hardware raise performance issues when comparing to ASIC solutions. To address these issues, numerous software acceleration frameworks for packet processing have been proposed in the last few years. One central mechanism of many of these frameworks is the use of batching techniques, where packets are processed in groups as opposed to individually. This is required to provide high-speed capabilities by minimizing framework overhead, reducing interrupt pressure, and leveraging instruction-level cache hits. Several such system implementations have been proposed and experimentally benchmarked in the past. However, the scientific community has so far only to a limited extent attempted to model the system dynamics of modern NFV routers exploiting batching acceleration. In this article, we propose a simple, generic model for this type of batching-based systems that can be applied to predict all relevant key performance indicators. In particular, we extend our previous work and formulate the calculation of the queue size as well as waiting time distributions in addition to the batch size distribution and the packet loss probability. Furthermore, we introduce the waiting time distribution as a relevant QoS parameter and perform an in-depth parameter study, widening the set of investigated variables as well as the range of values. Finally, we contrast the model prediction with experimental results gathered in a high-speed testbed including an NFV router, showing that the model not only correctly captures system performance under simple conditions, but also in more realistic scenarios in which traffic is processed by a mixture of functions. © 2021 Association for Computing Machinery.",discrete-time analysis; DPDK; network measurement; Packet processing; performance model; vector packet processing; waiting time,Network function virtualization; Packet networks; Probability distributions; Discrete time analysis; DPDK; Network measurement; Packet processing; Performance Modeling; Simple++; Vector packet processing; Waiting time; Waiting time distributions; Benchmarking
Malicious Node Identification in Coded Distributed Storage Systems under Pollution Attacks,2021,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124009979&doi=10.1145%2f3491062&partnerID=40&md5=692be11c502c6e73b088b09f7e5dedd8,"In coding-based distributed storage systems (DSSs), a set of storage nodes (SNs) hold coded fragments of a data unit that collectively allow one to recover the original information. It is well known that data modification (a.k.a. pollution attack) is the Achilles' heel of such coding systems; indeed, intentional modification of a single coded fragment has the potential to prevent the reconstruction of the original information because of error propagation induced by the decoding algorithm. The challenge we take in this work is to devise an algorithm to identify polluted coded fragments within the set encoding a data unit and to characterize its performance.To this end, we provide the following contributions: (i) We devise MIND (Malicious node IdeNtification in DSS), an algorithm that is general with respect to the encoding mechanism chosen for the DSS, it is able to cope with a heterogeneous allocation of coded fragments to SNs, and it is effective in successfully identifying polluted coded fragments in a low-redundancy scenario; (ii) We formally prove both MIND termination and correctness; (iii) We derive an accurate analytical characterization of MIND performance (hit probability and complexity); (iv) We develop a C++ prototype that implements MIND to validate the performance predictions of the analytical model.Finally, to show applicability of our work, we define performance and robustness metrics for an allocation of coded fragments to SNs and we apply the results of the analytical characterization of MIND performance to select coded fragments allocations yielding robustness to collusion as well as the highest probability to identify actual attackers. © 2021 Association for Computing Machinery.",Algorithm performance; distributed storage; pollution attack,C++ (programming language); Digital storage; Encoding (symbols); Pollution; Signal encoding; Algorithm performance; Analytical characterization; Data units; Distributed storage; Distributed storage system; Malicious nodes; Node identifications; Performance; Pollution attack; Storage nodes; Multiprocessing systems
Performance Analysis of the IOTA DAG-Based Distributed Ledger,2021,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124001239&doi=10.1145%2f3485188&partnerID=40&md5=156f496b774f59a4670eb7a78bdc8a8c,"Distributed ledgers (DLs) provide many advantages over centralized solutions in Internet of Things projects, including but not limited to improved security, transparency, and fault tolerance. To leverage DLs at scale, their well-known limitation (i.e., performance) should be adequately analyzed and addressed. Directed acyclic graph-based DLs have been proposed to tackle the performance and scalability issues by design. The first among them, IOTA, has shown promising signs in addressing the preceding issues. IOTA is an open source DL designed for the Internet of Things. It uses a directed acyclic graph to store transactions on its ledger, to achieve a potentially higher scalability over blockchain-based DLs. However, due to the uncertainty and centralization of the deployed consensus, the current IOTA implementation exposes some performance issues, making it less performant than the initial design. In this article, we first extend an existing simulator to support realistic IOTA simulations and investigate the impact of different design parameters on IOTA's performance. Then, we propose a layered model to help the users of IOTA determine the optimal waiting time to resend the previously submitted but not yet confirmed transaction. Our findings reveal the impact of the transaction arrival rate, tip selection algorithms, weighted tip selection algorithm randomness, and network delay on the throughput. Using the proposed layered model, we shed some light on the distribution of the confirmed transactions. The distribution is leveraged to calculate the optimal time for resending an unconfirmed transaction to the DL. The performance analysis results can be used by both system designers and users to support their decision making. © 2021 Association for Computing Machinery.",DAG; distributed acyclic graph; distributed ledger; Internet of Things; IOTA; Performance analysis; quality of service; throughput,Decision making; Directed graphs; Distributed ledger; Fault tolerance; Graphic methods; Quality control; Quality of service; Scalability; Acyclic graphs; DAG; Distributed acyclic graph; IOTA; Layered modeling; Performance; Performance issues; Performances analysis; Quality-of-service; Selection algorithm; Internet of things
A Combinatorial Reliability Analysis of Generic Service Function Chains in Data Center Networks,2021,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123998064&doi=10.1145%2f3477046&partnerID=40&md5=7e0569ce28cd176fe1c71b0693036813,"In data center networks, the reliability of Service Function Chain (SFC)-an end-to-end service presented by a chain of virtual network functions (VNFs)-is a complex and specific function of placement, configuration, and application requirements, both in hardware and software. Existing approaches to reliability analysis do not jointly consider multiple features of system components, including, (i) heterogeneity, (ii) disjointness, (iii) sharing, (iv) redundancy, and (v) failure interdependency. To this end, we develop a novel analysis of service reliability of the so-called generic SFC, consisting of n = k + r sub-SFCs, whereby k≥ 1 and r≥ 0 are the numbers of arbitrary placed primary and backup (redundant) sub-SFCs, respectively. Our analysis is based on combinatorics and a reduced binomial theorem-resulting in a simple approach, which, however, can be utilized to analyze rather complex SFC configurations. The analysis is practically applicable to various VNF placement strategies in arbitrary data center configurations, and topologies and can be effectively used for evaluation and optimization of reliable SFC placements. © 2021 Association for Computing Machinery.",availability; combinatorial analysis; common cause failures; failure propagation; NFV; parallel service function chaining; reliability; reliable VNF placement; SFC; VNF,Application programs; Complex networks; Failure (mechanical); Network function virtualization; Redundancy; Reliability analysis; Software reliability; Combinatorial analysis; Common cause failure; Data center networks; Failure propagation; Generic services; Parallel service function chaining; Reliable virtual network function placement; Service function chain; Service functions; Availability
Mansard Roofline Model: Reinforcing the Accuracy of the Roofs,2021,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117124277&doi=10.1145%2f3475866&partnerID=40&md5=c862c5b709dde5535931463a0673c5b9,"Continuous enhancements and diversity in modern multi-core hardware, such as wider and deeper core pipelines and memory subsystems, bring to practice a set of hard-to-solve challenges when modeling their upper-bound capabilities and identifying the main application bottlenecks. Insightful roofline models are widely used for this purpose, but the existing approaches overly abstract the micro-architecture complexity, thus providing unrealistic performance bounds that lead to a misleading characterization of real-world applications. To address this problem, the Mansard Roofline Model (MaRM), proposed in this work, uncovers a minimum set of architectural features that must be considered to provide insightful, but yet accurate and realistic, modeling of performance upper bounds for modern processors. By encapsulating the retirement constraints due to the amount of retirement slots, Reorder-Buffer and Physical Register File sizes, the proposed model accurately models the capabilities of a real platform (average rRMSE of 5.4%) and characterizes 12 application kernels from standard benchmark suites. By following a herein proposed MaRM interpretation methodology and guidelines, speed-ups of up to 5× are obtained when optimizing real-world bioinformatic application, as well as a super-linear speedup of 18.5× when parallelized.  © 2021 Copyright held by the owner/author(s).",application characterization; Performance modeling; roofline modeling,Benchmarking; Applications characterizations; Architectural features; Memory subsystems; Micro architectures; Multi-cores; Performance bounds; Performance Modeling; Real-world; Roofline models; Upper Bound; Memory architecture
Optimal Online Algorithms for File-Bundle Caching and Generalization to Distributed Caching,2021,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108902063&doi=10.1145%2f3445028&partnerID=40&md5=0e98d5ee2a47a9d1feacc6aa402e9f9b,"We consider a generalization of the standard cache problem called file-bundle caching, where different queries (tasks), each containing l ≥ 1 files, sequentially arrive. An online algorithm that does not know the sequence of queries ahead of time must adaptively decide on what files to keep in the cache to incur the minimum number of cache misses. Here a cache miss refers to the case where at least one file in a query is missing among the cache files. In the special case where l = 1, this problem reduces to the standard cache problem. We first analyze the performance of the classic least recently used (LRU) algorithm in this setting and show that LRU is a near-optimal online deterministic algorithm for file-bundle caching with regard to competitive ratio. We then extend our results to a generalized (h,k)-paging problem in this file-bundle setting, where the performance of the online algorithm with a cache size k is compared to an optimal offline benchmark of a smaller cache size h < k. In this latter case, we provide a randomized O(l ln k/k-h)-competitive algorithm for our generalized (h, k)-paging problem, which can be viewed as an extension of the classic marking algorithm. We complete this result by providing a matching lower bound for the competitive ratio, indicating that the performance of this modified marking algorithm is within a factor of 2 of any randomized online algorithm. Finally, we look at the distributed version of the file-bundle caching problem where there are m ≥ 1 identical caches in the system. In this case, we show that for m = l + 1 caches, there is a deterministic distributed caching algorithm that is (l2 + l)-competitive and a randomized distributed caching algorithm that is O(l ln (2l + 1)-competitive when l ≥ 2. We also provide a general framework to devise other efficient algorithms for the distributed file-bundle caching problem and evaluate the performance of our results through simulations.  © 2021 ACM.",competitive ratio; distributed caching; File-bundle caching; generalized h; k-paging; LRU algorithm; marking algorithm; online algorithms,Cache memory; Competitive algorithms; Competitive ratio; Deterministic algorithms; Distributed caching; File-bundle caching; Least recently used algorithms; Marking algorithm; On-line algorithms; Benchmarking
The Role of Hysteresis in Caching Systems,2021,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108910114&doi=10.1145%2f3450564&partnerID=40&md5=6d72c9ea1ab3fd67184f1430bc336dc0,"Caching is a fundamental element of networking systems since the early days of the Internet. By filtering requests toward custodians, caches reduce the bandwidth required by the latter and the delay experienced by clients. The requests that are not served by a cache, in turn, comprise its miss stream. We refer to the dependence of the cache state and miss stream on its history as hysteresis. Although hysteresis is at the core of caching systems, a dimension that has not been systematically studied in previous works relates to its impact on caching systems between misses, evictions, and insertions. In this article, we propose novel mechanisms and models to leverage hysteresis on cache evictions and insertions. The proposed solutions extend TTL-like mechanisms and rely on two knobs to tune the time between insertions and evictions given a target hit rate. We show the general benefits of hysteresis and the particular improvement of the two thresholds strategy in reducing download times, making the system more predictable and accounting for different costs associated with object retrieval.  © 2021 ACM.",Caching; hysteresis; performance evaluation,Computer network performance evaluation; Computer programming; Computer science; Caching system; Hit rate; Networking systems; Object retrieval; Hysteresis
Covert Cycle Stealing in a Single FIFO Server,2021,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115670012&doi=10.1145%2f3462774&partnerID=40&md5=12b5fc1f83945ad4773bca13db9490c2,"Consider a setting where Willie generates a Poisson stream of jobs and routes them to a single server that follows the first-in first-out discipline. Suppose there is an adversary Alice, who desires to receive service without being detected. We ask the question: What is the number of jobs that she can receive covertly, i.e. without being detected by Willie? In the case where bothWillie and Alice jobs have exponential service times with respective rates μ1 and μ2, we demonstrate a phase-transitionwhen Alice adopts the strategy of inserting a single job probabilistically when the server idles: over n busy periods, she can achieve a covert throughput, measured by the expected number of jobs covertly inserted, of O( √ n) when μ1 < 2μ2, O( - n/logn) when μ1 = 2μ2, and O(nμ2/μ1 ) when μ1 > 2μ2. When both Willie and Alice jobs have general service times, we establish an upper bound for the number of jobs Alice can execute covertly. This bound is related to the Fisher information. More general insertion policies are also discussed 2021 Association for Computing Machinery. © 2021 Association for Computing Machinery. All rights reserved.",covert communication; Cycle stealing; queue,Computation theory; Machinery; Busy period; Covert communications; Cycle-stealing; Exponentials; First in first outs; Fisher information; Poisson stream; Service time; Single server; Upper Bound; Fisher information matrix
Host-Based Virtual Machine Workload Characterization Using Hypervisor Trace Mining,2021,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108911133&doi=10.1145%2f3460197&partnerID=40&md5=be933a431e887af4851806e9c7d4bd56,"Cloud computing is a fast-growing technology that provides on-demand access to a pool of shared resources. This type of distributed and complex environment requires advanced resource management solutions that could model virtual machine (VM) behavior. Different workload measurements, such as CPU, memory, disk, and network usage, are usually derived from each VM to model resource utilization and group similar VMs. However, these course workload metrics require internal access to each VM with the available performance analysis toolkit, which is not feasible with many cloud environments privacy policies. In this article, we propose a non-intrusive host-based virtual machine workload characterization using hypervisor tracing. VM blockings duration, along with virtual interrupt injection rates, are derived as features to reveal multiple levels of resource intensiveness. In addition, the VM exit reason is considered, as well as the resource contention rate due to the host and other VMs. Moreover, the processes and threads preemption rates in each VM are extracted using the collected tracing logs. Our proposed approach further improves the selected features by exploiting a page ranking based algorithm to filter non-important processes running on each VM. Once the metric features are defined, a two-stage VM clustering technique is employed to perform both coarse- and fine-grain workload characterization. The inter-cluster and intra-cluster similarity metrics of the silhouette score is used to reveal distinct VM workload groups, as well as the ones with significant overlap. The proposed framework can provide a detailed vision of the underlying behavior of the running VMs. This can assist infrastructure administrators in efficient resource management, as well as root cause analysis.  © 2021 ACM.",K-Means; machine learning; PageRank; performance analysis; time series; tracing; vCPU states; virtual interrupts; VM clustering; workload characterization,Cloud computing; Filtration; Natural resources management; Network security; Privacy by design; Resource allocation; Clustering techniques; Complex environments; Infrastructure administrator; Performance analysis; Resource management; Resource utilizations; Workload characterization; Workload measurements; Virtual machine
On Renting Edge Resources for Service Hosting,2021,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117094531&doi=10.1145%2f3478433&partnerID=40&md5=a8aec11ea32a47e20fabaaf0417f79b7,"The rapid proliferation of shared edge computing platforms has enabled application service providers to deploy a wide variety of services with stringent latency and high bandwidth requirements. A key advantage of these platforms is that they provide pay-as-you-go flexibility by charging clients in proportion to their resource usage through short-term contracts. This affords the client significant cost-saving opportunities by dynamically deciding when to host its service on the platform, depending on the changing intensity of requests. A natural policy for our setting is the Time-To-Live (TTL) policy. We show that TTL performs poorly both in the adversarial arrival setting, i.e., in terms of the competitive ratio, and for i.i.d. stochastic arrivals with low arrival rates, irrespective of the value of the TTL timer. We propose an online policy called RetroRenting (RR) and characterize its performance in terms of the competitive ratio. Our results show that RR overcomes the limitations of TTL. In addition, we provide performance guarantees for RR for i.i.d. stochastic arrival processes coupled with negatively associated rent cost sequences and prove that it compares well with the optimal online policy. Further, we conduct simulations using both synthetic and real-world traces to compare the performance of RR with the optimal offline and online policies. The simulations show that the performance of RR is near optimal for all settings considered. Our results illustrate the universality of RR.  © 2021 Association for Computing Machinery.",competitive ratio; edge computing; online policies; Service hosting; task offloading; Time-To-Live policies,Stochastic systems; Competitive ratio; Computing platform; Edge computing; Edge resources; Online policy; Performance; Service hosting; Task offloading; Time-to-live; Time-to-live policy; Edge computing
Performance Analysis of Work Stealing in Large-scale Multithreaded Computing,2021,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117080655&doi=10.1145%2f3470887&partnerID=40&md5=ab7b66384a6402e0803b69b79c528901,"Randomized work stealing is used in distributed systems to increase performance and improve resource utilization. In this article, we consider randomized work stealing in a large system of homogeneous processors where parent jobs spawn child jobs that can feasibly be executed in parallel with the parent job. We analyse the performance of two work stealing strategies: one where only child jobs can be transferred across servers and the other where parent jobs are transferred. We define a mean-field model to derive the response time distribution in a large-scale system with Poisson arrivals and exponential parent and child job durations. We prove that the model has a unique fixed point that corresponds to the steady state of a structured Markov chain, allowing us to use matrix analytic methods to compute the unique fixed point. The accuracy of the mean-field model is validated using simulation. Using numerical examples, we illustrate the effect of different probe rates, load, and different child job size distributions on performance with respect to the two stealing strategies, individually, and compared to each other.  © 2021 Association for Computing Machinery.",distributed computing; matrix analytic methods; Mean-field model; performance analysis,Distributed computer systems; Large scale systems; Markov processes; Matrix algebra; Poisson distribution; Fixed points; Homogeneous processors; Large system; Large-scales; Matrix analytic methods; Mean field models; Multithreaded; Performance; Performances analysis; Resources utilizations; Mean field theory
Design and Evaluation of a Simple Data Interface for Efficient Data Transfer across Diverse Storage,2021,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108917210&doi=10.1145%2f3452007&partnerID=40&md5=fa2ec7ba156c456f6d5fff466a34371a,"Modern science and engineering computing environments often feature storage systems of different types, from parallel file systems in high-performance computing centers to object stores operated by cloud providers. To enable easy, reliable, secure, and performant data exchange among these different systems, we propose Connector, a plug-able data access architecture for diverse, distributed storage. By abstracting low-level storage system details, this abstraction permits a managed data transfer service (Globus, in our case) to interact with a large and easily extended set of storage systems. Equally important, it supports third-party transfers: that is, direct data transfers from source to destination that are initiated by a third-party client but do not engage that third party in the data path. The abstraction also enables management of transfers for performance optimization, error handling, and end-to-end integrity. We present the Connector design, describe implementations for different storage services, evaluate tradeoffs inherent in managed vs. direct transfers, motivate recommended deployment options, and propose a model-based method that allows for easy characterization of performance in different contexts without exhaustive benchmarking.  © 2021 ACM.",cloud storage; Data transfer; storage interface,Benchmarking; Computer aided engineering; Data transfer; Digital storage; Electronic data interchange; File organization; Connector design; Design and evaluations; Distributed storage; High performance computing; Model-based method; Parallel file system; Performance optimizations; Storage services; Storage as a service (STaaS)
VidCloud: Joint Stall andQuality Optimization for Video Streaming over Cloud,2021,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102873133&doi=10.1145%2f3442187&partnerID=40&md5=681e565257ee6158b3f923aa8877a58f,"As video-streaming services have expanded and improved, cloud-based video has evolved into a necessary feature of any successful business for reaching internal and external audiences. In this article, video streaming over distributed storage is considered where the video segments are encoded using an erasure code for better reliability. We consider a representative system architecture for a realistic (typical) content delivery network (CDN). Given multiple parallel streams/link between each server and the edge router, we need to determine, for each client request, the subset of servers to stream the video, as well as one of the parallel streams from each chosen server. To have this scheduling, this article proposes a two-stage probabilistic scheduling. The selection of video quality is also chosen with a certain probability distribution that is optimized in our algorithm. With these parameters, the playback time of video segments is determined by characterizing the download time of each coded chunk for each video segment. Using the playback times, a bound on the moment generating function of the stall duration is used to bound the mean stall duration. Based on this, we formulate an optimization problem to jointly optimize the convex combination of mean stall duration and average video quality for all requests, where the two-stage probabilistic scheduling, video quality selection, bandwidth split among parallel streams, and auxiliary bound parameters can be chosen. This non-convex problem is solved using an efficient iterative algorithm. Based on the offline version of our proposed algorithm, an online policy is developed where servers selection, quality, bandwidth split, and parallel streams are selected in an online manner. Experimental results show significant improvement in QoE metrics for cloud-based video as compared to the considered baselines.  © 2021 ACM.",erasure codes; mean stall duration; two-stage probabilistic scheduling; video quality; Video streaming over cloud,Bandwidth; Image segmentation; Iterative methods; Probability distributions; Scheduling; Content delivery network; Iterative algorithm; Moment generating function; Optimization problems; Probabilistic scheduling; System architectures; Video quality selection; Video streaming services; Video streaming
Adaptive Performance Modeling of Data-intensive Workloads for Resource Provisioning in Virtualized Environment,2021,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102873037&doi=10.1145%2f3442696&partnerID=40&md5=f49177f8ae0b26dc1c58424dcbfd9935,"The processing of data-intensive workloads is a challenging and time-consuming task that often requires massive infrastructure to ensure fast data analysis. The cloud platform is the most popular and powerful scale-out infrastructure to perform big data analytics and eliminate the need to maintain expensive and high-end computing resources at the user side. The performance and the cost of such infrastructure depend on the overall server configuration, such as processor, memory, network, and storage configurations. In addition to the cost of owning or maintaining the hardware, the heterogeneity in the server configuration further expands the selection space, leading to non-convergence. The challenge is further exacerbated by the dependency of the application's performance on the underlying hardware. Despite an increasing interest in resource provisioning, few works have been done to develop accurate and practical models to proactively predict the performance of data-intensive applications corresponding to the server configuration and provision a cost-optimal configuration online. In this work, through a comprehensive real-system empirical analysis of performance, we address these challenges by introducing ProMLB: a proactive machine-learning-based methodology for resource provisioning. We first characterize diverse types of data-intensive workloads across different types of server architectures. The characterization aids in accurately capture applications' behavior and train a model for prediction of their performance. Then, ProMLB builds a set of cross-platform performance models for each application. Based on the developed predictive model, ProMLB uses an optimization technique to distinguish close-to-optimal configuration to minimize the product of execution time and cost. Compared to the oracle scheduler, ProMLB achieves 91% accuracy in terms of application-resource matching. On average, ProMLB improves the performance and resource utilization by 42.6% and 41.1%, respectively, compared to baseline scheduler. Moreover, ProMLB improves the performance per cost by 2.5× on average.  © 2021 ACM.",big data; cloud; optimization; Performance,Data Analytics; Digital storage; Scheduling; Cross platform performance; Data-intensive application; Data-intensive workloads; Optimization techniques; Predictive modeling; Resource utilizations; Time-consuming tasks; Virtualized environment; Predictive analytics
A Message from the New Editor-in-Chief,2021,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102855847&doi=10.1145%2f3432597&partnerID=40&md5=db4f421d4a82f018a19aade8518a5c3a,[No abstract available],,
Online Thread and Data Mapping Using a Sharing-Aware Memory Management Unit,2021,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102895257&doi=10.1145%2f3433687&partnerID=40&md5=06ef7b50b9cc7e945776848d6e7749d9,"Current and future architectures rely on thread-level parallelism to sustain performance growth. These architectures have introduced a complex memory hierarchy, consisting of several cores organized hierarchically with multiple cache levels and NUMA nodes. These memory hierarchies can have an impact on the performance and energy efficiency of parallel applications as the importance of memory access locality is increased. In order to improve locality, the analysis of the memory access behavior of parallel applications is critical for mapping threads and data. Nevertheless, most previous work relies on indirect information about the memory accesses, or does not combine thread and data mapping, resulting in less accurate mappings. In this paper, we propose the Sharing-Aware Memory Management Unit (SAMMU), an extension to the memory management unit that allows it to detect the memory access behavior in hardware. With this information, the operating system can perform online mapping without any previous knowledge about the behavior of the application. In the evaluation with a wide range of parallel applications (NAS Parallel Benchmarks and PARSEC Benchmark Suite), performance was improved by up to 35.7% (10.0% on average) and energy efficiency was improved by up to 11.9% (4.1% on average). These improvements happened due to a substantial reduction of cache misses and interconnection traffic.  © 2021 ACM.",cache memory; communication; data mapping; data sharing; memory management unit; NUMA; shared memory; Thread mapping,Benchmarking; Cache memory; Energy efficiency; Mapping; Physical addresses; Accurate mapping; Benchmark suites; Future architectures; Memory hierarchy; NAS parallel benchmarks; Parallel application; Substantial reduction; Thread level parallelism; Memory management units
Toward Efficient Block Replication Management in Distributed Storage,2020,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096783248&doi=10.1145%2f3412450&partnerID=40&md5=2acd99a9b5ad71c1f0bf55ddbbd1e564,"Distributed/parallel file systems commonly suffer from load imbalance and resource contention due to the bursty characteristic exhibited in scientific applications. This article presents an adaptive scheme supporting dynamic block data replication and an efficient replica placement policy to improve the I/O performance of a distributed file system. Our goal is not only to yield a balanced data replication among storage servers but also a high degree of data access parallelism for the applications. We first present mathematical cost models to formulate the cost of data block replication by considering both the overhead and reduced data access time to the replicated data. To verify the validity and feasibility of the proposed cost model, we implement our proposal in a prototype distributed file system and evaluate it using a set of representative database-relevant application benchmarks. Our results demonstrate that the proposed approach can boost the usage efficiency of the data replicas with acceptable overhead of data replication management. Consequently, the overall data throughput of storage system can be noticeably improved. In summary, the proposed replication management scheme works well, especially for the database-relevant applications that exhibit an uneven access frequency and pattern to different parts of files.  © 2020 ACM.",access load balance; block data replication; Distributed file systems; modeling; replica placement,Benchmarking; Digital storage; File organization; Access frequency; Data replication; Distributed file systems; Distributed storage; Management scheme; Replica placement; Resource contention; Scientific applications; Distributed database systems
Analysis of a Queueing Model for Energy Storage Systems with Self-discharge,2020,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096793517&doi=10.1145%2f3422711&partnerID=40&md5=e0628ee1b9b5419c8215caa1c135e724,"This article presents an analysis of a recently proposed queueing system model for energy storage with discharge. Even without a load, energy storage systems experience a reduction of the stored energy through self-discharge. In some storage technologies, the rate of self-discharge can exceed 50% of the stored energy per day. We consider a queueing model, referred to as leakage queue, where, in addition to an arrival and a service process, there is a leakage process that reduces the buffer content by a factor I£ ( 0 < I£ < 1) in each time slot. When the average drift is positive, we discover that the leakage queue operates in one of two regimes, each with distinct characteristics. In one of the regimes, the stored energy always stabilizes at a point that lies below the storage capacity, and the stored energy closely follows a Gaussian distribution. In the other regime, the storage system behaves similar to a conventional finite capacity system. For both regimes, we derive expressions for the probabilities of underflow and overflow. In particular, we develop a new martingale argument to estimate the probability of underflow in the second regime. The methods are validated in a numerical example where the energy supply resembles a wind energy source.  © 2020 ACM.",energy storage; exponential martingale; Queueing theory,Energy storage; Numerical methods; Wind power; Energy storage systems; Energy supplies; Queueing system; Service process; Storage capacity; Storage systems; Storage technology; Wind energy sources; Queueing theory
Scalable Application-and User-aware Resource Allocation in Enterprise Networks Using End-Host Pacing,2020,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096806519&doi=10.1145%2f3381996&partnerID=40&md5=37a044ddd3d3e2ea57acfaec4a113ef3,"Providing scalable user-and application-aware resource allocation for heterogeneous applications sharing an enterprise network is still an unresolved problem. The main challenges are as follows: (i) How do we define user-and application-aware shares of resources? (ii) How do we determine an allocation of shares of network resources to applications? (iii) How do we allocate the shares per application in heterogeneous networks at scale? In this article, we propose solutions to the three challenges and introduce a system design for enterprise deployment. Defining the necessary resource shares per application is hard, as the intended use case, the user's environment, e.g., big or small display, and the user's preferences influence the resource demand. We tackle the challenge by associating application flows with utility functions from subjective user experience models, selected Key Performance Indicators, and measurements. The specific utility functions then enable a mapping of network resources in terms of throughput and latency budget to a common user-level utility scale. A sensible distribution of the resources is determined by formulating a multi-objective mixed integer linear program to solve the throughput-and delay-aware embedding of each utility function in the network for a max-min fairness criteria. The allocation of resources in traditional networks with policing and scheduling cannot distinguish large numbers of classes and interacts badly with congestion control algorithms. We propose a resource allocation system design for enterprise networks based on Software-Defined Networking principles to achieve delay-constrained routing in the network and application pacing at the end-hosts. The system design is evaluated against best effort networks in a proof-of-concept set-up for scenarios with increasing number of parallel applications competing for the throughput of a constrained link. The competing applications belong to the five application classes web browsing, file download, remote terminal work, video streaming, and Voice-over-IP. The results show that the proposed methodology improves the minimum and total utility, minimizes packet loss and queuing delay at bottlenecks, establishes fairness in terms of utility between applications, and achieves predictable application performance at high link utilization.  © 2020 Owner/Author.",browsing; enterprise; HAS; network; pacing; QoE; QoS; SDN; VoIP,Benchmarking; Budget control; Heterogeneous networks; Integer programming; Resource allocation; Systems analysis; User experience; Voice/data communication systems; Application performance; Best effort networks; Delay constrained routing; Key performance indicators; Mixed integer linear program; Parallel application; Resource allocation systems; User experience model; Application programs
Effectiveness of Neural Networks for Power Modeling for Cloud and HPC,2020,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096762404&doi=10.1145%2f3388322&partnerID=40&md5=576545526b142b5965b6d69cf0c50dfb,"Power consumption of servers and applications are of utmost importance as computers are becoming ubiquitous, from smart phones to IoT and full-fledged computers. To optimize their power consumption, knowledge is necessary during execution at different levels: for the Operating System to take decisions of scheduling, for users to choose between different applications. Several models exist to evaluate the power consumption of computers without relying on actual wattmeters: Indeed, these hardware are costly but also usually have limits on their pooling frequency (usually a one-second frequency is observed) except for dedicated professional hardware. The models link applications behavior with their power consumption, but up to now there is a 5% wall: Most models cannot reduce their error under this threshold and are usually linked to a particular hardware configuration. This article demonstrates how to break the 5% wall of power models. It shows that by using neural networks it is possible to create models with 1% to 2% error. It also quantifies the reachable precision obtainable with other classical methods such as analytical models.  © 2020 ACM.",analytical models; Cloud; HPC; machine learning; Power models,Computer hardware; Electric power utilization; Green computing; Smartphones; Classical methods; Hardware configurations; Power model; Neural networks
Some Parameterized Dynamic Priority Policies for Two-Class M/G/1 Queues: Completeness and Applications,2020,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091012189&doi=10.1145%2f3384390&partnerID=40&md5=7a555922a0ad18d30b193538a380baa6,"Completeness of a dynamic priority scheduling scheme is of fundamental importance for the optimal control of queues in areas as diverse as computer communications, communication networks, supply/value chains, and manufacturing systems. Our first main contribution is to identify the mean waiting time completeness as a unifying aspect for four different dynamic priority scheduling schemes by proving their completeness and equivalence in two-class M/G/1 queues. These dynamic priority schemes are earliest due date based, head of line priority jump, relative priority, and probabilistic priority. We discuss major challenges in extending our results to three or more classes. In our second main contribution, we characterize the optimal scheduling policies for the case studies in different domains by exploiting the completeness of the above dynamic priority schemes. The major theme of the second main contribution is resource allocation/optimal control in revenue management problems for contemporary systems such as cloud computing, high performance computing, ans so forth, where congestion is inherent. Using completeness and the theoretically tractable nature of relative priority policy, we study the impact of approximation in a fairly generic data network utility framework. Next, we simplify a complex joint pricing and scheduling problem for a wider class of scheduling policies. © 2020 ACM.",Achievable region; cloud computing; delay dependent priority; dynamic priority scheduling; earliest due date; head-of-line priority-jump; high performance computing; optimal control; pricing; probabilistic priority; relative priority; utility in data networks,Computer control systems; Economics; Manufacture; Queueing theory; Scheduling; Computer Communications; Dynamic-priority scheduling; Earliest due dates; High performance computing; Optimal scheduling; Relative priorities; Revenue management; Scheduling policies; Equivalence classes
Should i Stay or Should i Go: Analysis of the Impact of Application QoS on User Engagement in YouTube,2020,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091049497&doi=10.1145%2f3377873&partnerID=40&md5=481841c0c44966867ea0e128202994d7,"To improve the user engagement, especially under moderate to high traffic demand, it is important to understand the impact of the network and application QoS on user experience. This article comparatively evaluates the impact of impairments, with emphasis on rebufferings, startup delay, and bitrate changes, and their intensity and temporal dynamics, on user engagement in the context of video streaming. The analysis employed two large YouTube datasets. To characterize the user engagement and the impact of impairments, several new metrics were defined. We assessed whether or not there is a statistically significant relationship between different types of impairments and user engagement metrics, taking into account not only the characteristics of the impairments but also the covariates of the session (e.g., video duration, mean data rate). After observing the relationships across the entire dataset, we tested whether these relationships also persist under specific conditions with respect to the covariates. The introduction of several new metrics and of various covariates in the analysis are two innovative aspects of this work. We found that the presence of negative bitrate changes (BR-) is a stronger predictor of abandonment than rebufferrings (RB). Positive bitrate changes (BR+) in low resolution sessions are not well received. High rebufferring ratio has a prominent impact on the video watching percentage. These results can be used to guide the video streaming adaptation as well as suggest which parameters should be varied in controlled field studies. © 2020 ACM.",adaptive streaming; bitrate changes; network impairments; performance; rebufferring; startup delay; User engagement; video streaming; YouTube,Large dataset; Video streaming; Application QoS; Covariates; Field studies; Low resolution; Mean data rate; Temporal dynamics; Traffic demands; User engagement; User experience
User Interaction with Online Advertisements: Temporal Modeling and Optimization of Ads Placement,2020,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091027975&doi=10.1145%2f3377144&partnerID=40&md5=0629921e26c377ec66da432c8603b88b,"We consider an online advertisement system and focus on the impact of user interaction and response to targeted advertising campaigns. We analytically model the system dynamics accounting for the user behavior and devise strategies to maximize a relevant metric called click-Through-intensity (CTI), defined as the number of clicks per time unit. With respect to the traditional click-Through-rate (CTR) metric, CTI better captures the success of advertisements for services that the users may access several times, making multiple purchases or subscriptions. Examples include advertising of on-line games or airplane tickets. The model we develop is validated through traces of real advertising systems and allows us to optimize CTI under different scenarios depending on the nature of ad delivery and of the information available at the system. Experimental results show that our approach can increase the revenue of an ad campaign, even when user's behavior can only be estimated. © 2020 ACM.",ads placement; CTR; Online advertisements; recommendation systems; user behaviour,Marketing; Publishing; Advertising systems; Click-through rate; Online advertisements; System Dynamics; Targeted advertising; Temporal modeling; User behaviors; User interaction; Behavioral research
IModel: Automatic Derivation of Analytic Performance Models,2020,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091039382&doi=10.1145%2f3374220&partnerID=40&md5=c4612691475d9f97a6024cc8f4721da7,"Deriving analytic performance models requires detailed knowledge of the architecture and behavior of the computer system being modeled as well as modeling skills. This detailed knowledge may not be readily available (or it may be impractical to gather) given the dynamic nature of production computing environments. This article presents a framework, called iModel, for automatically deriving and parameterizing analytic performance models for multi-Tiered computer systems. Analytic performance models consist of a workload model and a system model. iModel uses system logs and configuration files to generate a high-level characterization of the system; e.g., open queuing network (QN) model versus closed QN model. By harvesting more information from the system logs and configuration files, iModel generates a workload model by inferring user-system interaction patterns in the form of a Customer Behavior Model Graph (CBMG) and generates a system model by discovering system components and their interaction patterns in the form of a Client-Server Interaction Diagram (CSID). iModel includes a library of well-known single-queue and QN models and their solutions stored in an XML-based repository. The generated workload model and system model are compared to the model repository to determine which model in the repository best matches the system's observable behavior and architecture. This article also presents a black-box optimization approach that is used to derive analytic model parameters by observing the input-output relationships of a real system. This optimization approach can be used in any computer system (multi-Tier or not) that can be modeled by single queues or QNs. The important question is whether the automatically generated and parameterized performance model has predictive power, i.e., can the derived model predict the output values that would be observed in the real system for different values of the input? The results presented in this article demonstrate that the analytic performance models derived by iModel are relatively robust and have predictive power over a wide range of input values. © 2020 ACM.",AMVA; black-box optimization; MVA; non-linear optimization; parameter estimation; Queuing network models,Computer architecture; Network architecture; Optimization; Queueing networks; Queueing theory; Automatic derivation; Automatically generated; Black-box optimization; Client-server interactions; Computing environments; Customer behavior model graphs; Optimization approach; User-system interaction; Analytical models
The limit of horizontal scaling in public clouds,2020,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079484634&doi=10.1145%2f3373356&partnerID=40&md5=f5d0472533c3912f77aa95a1ead9b819,"Public cloud users are educated to practice horizontal scaling at the application level, with the assumption that more processing capacity can be achieved by adding nodes into the server fleet. In reality, however, applications-even those specifically designed to be horizontally scalable-often face unpredictable scalability issues when running at scale. In this article, we study the limit of horizontal scaling in public clouds by identifying sources of such limitations and quantitatively measuring their impact on processing capacity. To this end, we develop ScaleBench as a distributed and parallel cloud-scale testing framework and propose a capacity degradation index (CDI) to describe the level of capacity degradation observed in our benchmark studies. We have conducted extensive experiments in four real public clouds to identify possible bottlenecks in compute, block storage, networking, and object storage. Further, we carry out large-scale experiments with a real-life video transcoding application on worker fleets with up to 3200 vCPU cores. Our experimental results provide the quantitative evidence on the limit of horizontal scaling in public clouds. This helps cloud users make better design decisions on horizontally scalable applications. © 2020 Association for Computing Machinery.",Benchmark; Cloud computing; Horizontal scaling,Benchmarking; Cloud computing; Computer network performance evaluation; Computer programming; Computer science; Application level; Capacity degradation; Horizontal scaling; Large scale experiments; Processing capacities; Scalability issue; Testing framework; Video-transcoding; Video signal processing
Dynamic resource allocation in fork-join queues,2020,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079486741&doi=10.1145%2f3372376&partnerID=40&md5=f61a32224a85211271925a3b2f6c2273,"Fork-join systems play a pivotal role in the analysis of distributed systems, telecommunication infrastructures, and storage systems. In this article, we consider a fork-join system consisting of K parallel servers, each of which works on one of the K tasks that form each job. The system allocates a fixed amount of computational resources among the K servers, hence determining their service speed. The goal of this article is that of studying the resource allocation policies among the servers. We assume that the queueing disciplines of the fork- and join-queues are First Come First Served. At each epoch, at most K tasks are in service while the others wait in the fork-queues. We propose an algorithm with a very simple implementation that allocates the computational resources in a way that aims at minimizing the join-queue lengths, and hence at reducing the expected job service time. We study its performance in saturation and under exponential service time. The model has an elegant closed-form stationary distribution. Moreover, we provide an algorithm to numerically or symbolically derive the marginal probabilities for the join-queue lengths. Therefore, the expressions for the expected join-queue length and the expected response time under immediate join can be derived. Finally, we compare the performance of the proposed resource allocation algorithm with that of other strategies. © 2020 Association for Computing Machinery.",Fork-join queues; Processor sharing; Reversible models,Resource allocation; Computational resources; Dynamic resource allocations; First come first served; Fork-join queues; Processor sharing; Resource allocation algorithms; Resource allocation policy; Telecommunication infrastructures; Queueing theory
QoS provision in a dynamic channel allocation based on admission control decisions,2020,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079504573&doi=10.1145%2f3372786&partnerID=40&md5=6aa47d2330187b71f02bef5fbaa2bbbe,"Cognitive Radio Networks have emerged in the last decades as a solution of two problems: spectrum underutilization and spectrum scarcity. In this work, we propose a dynamic spectrum sharing mechanism, where primary users have strict priority over secondary ones in order to improve the mean spectrum utilization with the objective of providing to secondary users a satisfactory grade of service with a small interruption probability. We study a stochastic model for Cognitive Radio Networks with fluid limits techniques. Our main findings consist in a Gaussian limit theorem in the sub-critical case, and a non-Gaussian limit theorem, under a different scaling scheme, in the critical case. These results provide us practical QoS criteria for sharing policies. We support our analysis with representative simulated examples in both scenarios. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Asymptotic distribution; Cognitive radio networks; Fluid limits; QoS,Quality of service; Radio; Radio systems; Stochastic control systems; Stochastic models; Stochastic systems; Admission control decision; Asymptotic distributions; Cognitive radio network; Dynamic channel allocation; Dynamic spectrum sharing; Fluid limits; Interruption probability; Spectrum utilization; Cognitive radio
Introduction to the special section on quantitative evaluation of systems (QEST 2018),2020,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079500850&doi=10.1145%2f3376999&partnerID=40&md5=09ccdc752b2d2099175b5fb1deb01945,[No abstract available],,
Exploring performance characteristics of the optane 3D xpoint storage technology,2020,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079487597&doi=10.1145%2f3372783&partnerID=40&md5=afe3b5fdfedb54bfce18d9d91bee63ba,"Intel's Optane solid-state nonvolatile storage device is constructed using their new 3D Xpoint technology. Although it is claimed that this technology can deliver substantial performance improvements compared to NAND-based storage systems, its performance characteristics have not been well studied. In this study, intensive experiments and measurements have been carried out to extract the intrinsic performance characteristics of the Optane SSD, including the basic I/O performance behavior, advanced interleaving technology, performance consistency under a highly intensive I/O workload, influence of unaligned request size, elimination of write-driven garbage collection, read disturb issues, and tail latency problem. The performance is compared to that of a conventional NAND SSD to indicate the performance difference of the Optane SSD in each scenario. In addition, by using TPC-H, a read-intensive benchmark, a database system's performance has been studied on our target storage devices to quantify the potential benefits of the Optane SSD to a real application. Finally, the performance impact of hybrid Optane and NAND SSD storage systems on a database application has been investigated. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",3D Xpoint memory; NAND SSD; Nonvolatile memory; Optane SSD; Performance evaluation; Relational database management system; Solid-state drive,NAND circuits; Nonvolatile storage; Virtual storage; NAND SSD; Non-volatile memory; Optane SSD; Performance evaluation; Relational database management systems; Solid state drives; X-point; Benchmarking
A Lyapunov approach for time-bounded reachability of CTMCs and CTMDPS,2020,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079504942&doi=10.1145%2f3371923&partnerID=40&md5=3085a22c117cada0c84ef82478a3e193,"Time-bounded reachability is a fundamental problem in model checking continuous-time Markov chains (CTMCs) and Markov decision processes (CTMDPs) for specifications in continuous stochastic logics. It can be computed by numerically solving a characteristic linear dynamical system, but the procedure is computationally expensive. We take a control-theoretic approach and propose a reduction technique that finds another dynamical system of lower dimension (number of variables), such that numerically solving the reduced dynamical system provides an approximation to the solution of the original system with guaranteed error bounds. Our technique generalizes lumpability (or probabilistic bisimulation) to a quantitative setting. Our main result is a Lyapunov function characterization of the difference in the trajectories of the two dynamics that depends on the initial mismatch and exponentially decreases over time. In particular, the Lyapunov function enables us to compute an error bound between the two dynamics as well as a convergence rate. Finally, we show that the search for the reduced dynamics can be computed in polynomial time using a Schur decomposition of the transition matrix. This enables us to efficiently solve the reduced dynamical system by computing the exponential of an upper-triangular matrix characterizing the reduced dynamics. For CTMDPs, we generalize our approach using piecewise quadratic Lyapunov functions for switched affine dynamical systems. We synthesize a policy for the CTMDP via its reduced-order switched system that guarantees that the time-bounded reachability probability lies above a threshold. We provide error bounds that depend on the minimum dwell time of the policy. We demonstrate the technique on examples from queueing networks, for which lumpability does not produce any state space reduction, but our technique synthesizes policies using a reduced version of the model. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Continuous-time Markov chains; Control theory; Lyapunov stability; Markov decision processes; Probabilistic bisimulation; Time-bounded reachability,Continuous time systems; Control theory; Decision theory; Dynamics; Error analysis; Linear control systems; Lyapunov functions; Markov chains; Matrix algebra; Model checking; Polynomial approximation; Stochastic models; Stochastic systems; Continuous time Markov chain; Lyapunov stability; Markov Decision Processes; Probabilistic bisimulations; Time-bounded reachability; Dynamical systems
Consistent sampling of churn under periodic non-stationary arrivals in distributed systems,2019,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076785240&doi=10.1145%2f3368510&partnerID=40&md5=051816627964a9c93e64d027fd2f9544,"Characterizing user churn has become an important research area of networks and distributed systems, both in theoretical analysis and system design. A realistic churn model, often measured using periodic observation, should replicate two key properties of deployed systems - (1) the arrival process and (2) the lifetime distribution of participating agents. Because users can be sampled only by sending packets to them and eliciting responses, there is an inherent tradeoff between overhead (i.e., bandwidth needed to perform the measurement) and accuracy of obtained results. Furthermore, all observations are censored, i.e., rounded up or down to a multiple of Δ, where Δ is the minimum delay between repeat visits to the same user. Assuming a stationary arrival process, previous work shows that consistent (i.e., asymptotically accurate) estimation of the lifetime distribution is possible; however, the problem remains open for non-stationary cases. Questions include what distributions these methods sample when the assumptions on the arrival process are violated, under what conditions consistency is possible with existing techniques, and what avenues exist for improving their accuracy and overhead. To investigate these issues, we first use random-measure theory to develop a novel churn model that allows rather general non-stationary scenarios and even synchronized joins (e.g., flash crowds). We not only dispose with common assumptions, such as existence of arrival rate and ergodicity, but also show that this model can produce all metrics of interest (e.g., sampled lifetime distributions, bandwidth overhead) using simple expressions. We apply these results to study the accuracy of prior techniques and discover that they are biased unless user lifetimes are exponential or the arrival measure is stationary. To overcome these limitations, we then create a new lifetime-sampling technique that remains asymptotically robust under all periodic arrival measures and provide a methodology for undoing the bias in the sampled arrival rate created by missed users. We demonstrate that the proposed approach exhibits accuracy advantages and 1-2 orders of magnitude less bandwidth consumption compared to the alternatives. We finish by implementing the proposed framework and applying it to experimental data from massive crawls of Gnutella. © 2019 Association for Computing Machinery.",Lifetime estimation; Network sampling; Stochastic analysis,Bandwidth; Stochastic systems; Bandwidth consumption; Bandwidth overheads; Consistent samplings; Life-time distribution; Lifetime estimation; Network samplings; Networks and distributed systems; Stochastic analysis; Distributed computer systems
On the role of clustering in personalized PageRank estimation,2019,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076754870&doi=10.1145%2f3366635&partnerID=40&md5=dfee1115bd7ef3529cd4f2d9079dce17,"Personalized PageRank (PPR) is a measure of the importance of a node from the perspective of another (we call these nodes the target and the source, respectively). PPR has been used in many applications, such as offering a Twitter user (the source) recommendations of whom to follow (targets deemed important by PPR); additionally, PPR has been used in graph-theoretic problems such as community detection. However, computing PPR is infeasible for large networks like Twitter, so efficient estimation algorithms are necessary. In this work, we analyze the relationship between PPR estimation complexity and clustering. First, we devise algorithms to estimate PPR for many source/target pairs. In particular, we propose an enhanced version of the existing single pair estimator Bidirectional-PPR that is more useful as a primitive for many pair estimation. We then show that the common underlying graph can be leveraged to efficiently and jointly estimate PPR for many pairs rather than treating each pair separately using the primitive algorithm. Next, we show the complexity of our joint estimation scheme relates closely to the degree of clustering among the sources and targets at hand, indicating that estimating PPR for many pairs is easier when clustering occurs. Finally, we consider estimating PPR when several machines are available for parallel computation, devising a method that leverages our clustering findings, specifically the quantities computed in situ, to assign tasks to machines in a manner that reduces computation time. This demonstrates that the relationship between complexity and clustering has important consequences in a practical distributed setting. © 2019 Association for Computing Machinery.",Clustering; Personalized PageRank; Social and information networks,Graph theory; Information services; Social networking (online); Clustering; Degree of clustering; Efficient estimation; Estimation complexity; Graph-theoretic problem; Information networks; Parallel Computation; Personalized PageRank; Complex networks
AMIR: Analytic method for improving responsiveness by reducing burstiness,2019,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076742687&doi=10.1145%2f3365669&partnerID=40&md5=ca962f81bd99f5c6cd074c903835426c,"Service demand burstiness, or serial correlations in resource service demands, has previously been shown to have an adverse impact on system performance metrics such as response time. This article proposes AMIR, an analytic framework to characterize burstiness and identify strategies to reduce its impact on performance. AMIR considers an overtake-free system model consisting of multiple queues that service multiple classes of sessions, i.e., sequences of requests. Given the per-class service demand distributions and number of sessions belonging to each class, AMIR can identify an ordering of sessions, i.e., a schedule, that minimizes burstiness at the bottleneck. Hence, it is likely to improve system responsiveness metrics, including mean session wait time and total schedule processing time. To characterize burstiness, the technique uses an order O schedule burstiness metric β representing the mean probability that O + 1 consecutive sessions in the schedule have resource demands at the bottleneck greater than the mean bottleneck demand of the schedule. For a given O, AMIR uses Integer Linear Programming (ILP) to identify schedules that progressively minimize β' Ai ϵ {1,⋯ O}. We conduct an extensive simulation study to provide insights on the conditions under which such schedules can improve system responsiveness. These results show that schedules derived from AMIR can significantly outperform those derived from baseline policies such as Shortest Job First (SJF) and random scheduling when session classes are dissimilar from one another in terms of their service demand distributions. Furthermore, minimizing for higher orders of schedule burstiness is most critical when the bottleneck is heavily utilized and when the service demands of a workload are highly variable. For the system model that we consider, we are not aware of other techniques that are designed to analytically derive insights on the performance impact of high-order service demand burstiness. © 2019 Association for Computing Machinery.",Bottleneck mitigation; Burstiness minimization; High-order burstiness; System performance improvement; Wait time reduction,Computer network performance evaluation; Computer programming; Computer science; Bottleneck mitigation; Burstiness; Extensive simulations; Integer Linear Programming; Performance metrics; Serial correlation; System performance improvement; Time reduction; Integer programming
A framework for allocating server time to spot and on-demand services in cloud computing,2019,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076768714&doi=10.1145%2f3366682&partnerID=40&md5=effa1f74ba60fcd273f36f3fbcba10cb,"Cloud computing delivers value to users by facilitating their access to servers at any time period needed. An approach is to provide both on-demand and spot services on shared servers. The former allows users to access servers on demand at a fixed price and users occupy different time periods on servers. The latter allows users to bid for the remaining unoccupied time periods via dynamic pricing; however, without appropriate design, such time periods may be arbitrarily short since on-demand users arrive randomly. This is also the current service model adopted by Amazon Elastic Cloud Compute. In this article, we provide the first integral framework for sharing time on servers between on-demand and spot services while optimally pricing spot service. It guarantees that on-demand users can get served quickly while spot users can stably use servers for a properly long period once accepted, which is a key feature in making both on-demand and spot services accessible. Simulation results show that, by complementing the on-demand market with a spot market, a cloud provider can improve revenue by up to 461.5%. The framework is designed under assumptions that are met in real environments. It is a new tool that other cloud operators can use to quantify the advantage of a hybrid spot and on-demand service, making the case for eventually integrating this service model into their own infrastructures. © 2019 Association for Computing Machinery.",Cloud computing; Pricing; Spot and on-demand services; Time allocation,Commerce; Costs; Appropriate designs; Cloud providers; Dynamic pricing; First integral; On-demand services; Real environments; Service Model; Time allocation; Cloud computing
"Packet clustering introduced by routers: Modeling, analysis, and experiments",2019,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075754483&doi=10.1145%2f3345032&partnerID=40&md5=cf1f09667309f82dc5aeddf44423be5e,"In this article, we investigate a router's inherent variation on packet processing time and its effect on interpacket delay and packet clustering.We propose a simple pipeline model incorporating the inherent variation, and two metrics-one to measure packet clustering and one to quantify inherent variation. To isolate the effect of the inherent variation, we begin our analysis with no cross traffic and step through setups where the input streams have different data rates, packet size, and go through a different number of hops.We show that a homogeneous input stream with a sufficiently large interpacket gap will emerge at the router's output with interpacket delays that are negative correlated with adjacent values and have symmetrical distributions. We show that for smaller interpacket gaps, the change in packet clustering is smaller. It is also shown that the degree of packet clustering could in fact decrease for a clustered input. We generalize our results by adding cross traffic. All the results predicted by the model are validated with experiments with real routers.We also investigated several factors that can affect the inherent variation as well as some potential applications of this study. © 2019 Association for Computing Machinery.",Packet clustering; processing delay; Queueing,Computer network performance evaluation; Computer programming; Computer science; Cross-traffic; Inter-packet gaps; Interpacket delays; Packet clustering; Packet processing; Processing delay; Queueing; Simple pipelines; Pipeline processing systems
Investigating characteristics of internet paths,2019,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075717719&doi=10.1145%2f3342286&partnerID=40&md5=cce2962bb20548dac4a1e56d51542dc8,"Interactive and multimedia applications depend on the stability of end-to-end paths for predictable performance and good quality of service. On the other hand, network providers depend on multiple paths to ensure fault tolerance and use load balancing between these paths to enhance the overall network throughput. In this study, we analyze path dynamics for both end-to-end paths and path segments within service providers' networks using 2 months of measurement data from the RIPE Atlas platform, which collects path traces between a fixed set of source and destination pairs every 15 minutes. We observe that 78% of the end-to-end routes have at least two alternative Autonomous System (AS) paths with some end-to-end routes going through hundreds of different AS paths during the 2 months of analysis. While AS level paths are often prevalent for a day, there are considerable changes in the routing of the trace packets over the ASes for a longer duration of a month or longer. Analyzing end-to-end paths for routing anomalies, we observe that 4.4% of the path traces (involving 18% of the ASes) contain routing loops indicating misconfiguration of routers. Some of the ASes had over 100 routers involved in loops in path traces through their networks.We observe a much higher rate of anomalies in the AS level, with 45% of path traces containing an AS loop. Additionally, we discovered that few of the ASes bounce-back packets where some traces through their network traverse routers in both forward and backward directions. Determining path segments belonging to each AS, we further explore ingress to egress paths of ASes in addition to the source to destination paths within the AS. Analyzing trace segments between ingresses and egresses of an AS, we realized more than half of the ASes have the same router level path between any ingress-egress pair for the 2 months, but others implement load balancing. These results are different from earlier studies that indicated a high level of path dynamism. Our results indicate that the endto-end path dynamism is due to the Border Gateway Protocol level rather than the router level within ASes. © 2019 Association for Computing Machinery.",Autonomous system; End-to-end paths; Internet measurement; Path stability; Routing anomalies,Border Gateway Protocol; Fault tolerance; Fixed platforms; Quality of service; Routers; Autonomous systems; End-to-end path; Internet measurement; Path stability; Routing anomalies; Gateways (computer networks)
Scheduling for optimal file-Transfer delay using chunked random linear network coding broadcast,2019,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075632304&doi=10.1145%2f3340242&partnerID=40&md5=dddfee1dee921911d99eb35ef9134124,"We study the broadcast transmission of a single file to an arbitrary number of receivers using Random Linear Network Coding (RLNC) in a network with unreliable channels. Due to the increased computational complexity of the decoding process (especially for large files), we apply chunked RLNC (i.e., RLNC is applied within non-overlapping subsets of the file). In our work, we show the optimality of the Least Received (LR) batch scheduling policy with regards to the expected file transfer completion time. The LR policy strives to keep the receiver queues balanced. This is done by transmitting packets (corresponding to encoded batches) that are needed by the receivers with the shortest queues of successfully received packets. Furthermore, we provide formulas for the expected time for the file transmission to all receivers using the LR batch scheduling policy and the minimum achievable coding window size in the case of a pre-defined delay constraint. Moreover, we evaluate through simulations a modification of the LR policy in a more realistic system setting with reduced feedback from the receivers. Finally, we provide an initial analysis and further modifications to the LR policy for time-correlated channels and asymmetric channels. © 2019 Association for Computing Machinery.",Asymmetric channels; Broadcast; Chunked random linear network coding; Optimal batch scheduling policy; Sample path analysis; Time-correlated channels; Wireless channels,Broadcasting; Linear networks; Regression analysis; Scheduling; Scheduling algorithms; Signal receivers; Asymmetric channel; Batch-scheduling; Random Linear Network Coding; Sample-path analysis; Time-correlated channels; Wireless channel; Network coding
Generalization of LRU cache replacement policy with applications to video streaming,2019,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075755669&doi=10.1145%2f3345022&partnerID=40&md5=1f33ddbc8e3d6bb1073dd4bfb2214d74,"Caching plays a crucial role in networking systems to reduce the load on the network and is commonly employed by content delivery networks (CDNs) to improve performance. One of the commonly used mechanisms, Least Recently Used (LRU), works well for identical file sizes. However, for asymmetric file sizes, the performance deteriorates. This article proposes an adaptation to the LRU strategy, called gLRU, where the file is sub-divided into equal-sized chunks. In this strategy, a chunk of the newly requested file is added in the cache, and a chunk of the least-recently-used file is removed from the cache. Even though approximate analysis for the hit rate has been studied for LRU, the analysis does not extend to gLRU, since the metric of interest is no longer the hit rate as the cache has partial files. This article provides a novel approximation analysis for this policy where the cache may have partial file contents. The approximation approach is validated by simulations. Further, gLRU outperforms the LRU strategy for a Zipf file popularity distribution and censored Pareto file size distribution for the file download times. Video streaming applications can further use the partial cache contents to help the stall duration significantly, and the numerical results indicate significant improvements (32%) in stall duration using the gLRU strategy as compared to the LRU strategy. Furthermore, the gLRU replacement policy compares favorably to two other cache replacement policies when simulated on MSR Cambridge Traces obtained from the SNIA IOTTA repository. © 2019 Association for Computing Machinery.",Caching; Characteristic time approximation; Che's approximation; Least recently used; Video streaming,Cache memory; Cache replacement policy; Caching; Characteristic time; Che's approximation; Content delivery network; File size distributions; Least recently used; Video Streaming Applications; Video streaming
A new framework for evaluating straggler detection mechanisms in mapreduce,2019,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075630435&doi=10.1145%2f3328740&partnerID=40&md5=e1ab68f5ed83bc0d8e692ed220a9977c,"Big Data systems (e.g., Google MapReduce, Apache Hadoop, Apache Spark) rely increasingly on speculative execution to mask slow tasks, also known as stragglers, because a job's execution time is dominated by the slowest task instance. Big Data systems typically identify stragglers and speculatively run copies of those taskswith the expectation that a copy may complete faster to shorten job execution times. There is a rich body of recent results on straggler mitigation in MapReduce. However, the majority of these do not consider the problem of accurately detecting stragglers. Instead, they adopt a particular straggler detection approach and then study its effectiveness in terms of performance, e.g., reduction in job completion time or higher efficiency, e.g., high resource utilization. In this article, we consider a complete framework for straggler detection and mitigation. We start with a set of metrics that can be used to characterize and detect stragglers including Precision, Recall, Detection Latency, Undetected Time, and Fake Positive. We then develop an architectural model by which these metrics can be linked to measures of performance including execution time and system energy overheads.We further conduct a series of experiments to demonstrate which metrics and approaches are more effective in detecting stragglers and are also predictive of effectiveness in terms of performance and energy efficiencies. For example, our results indicate that the default Hadoop straggler detector could be made more effective. In a certain case, Precision is low and only 55% of those detected are actual stragglers and the Recall, i.e., percent of actual detected stragglers, is also relatively low at 56%. For the same case, the hierarchical approach (i.e., a green-driven detector based on the default one) achieves a Precision of 99% and a Recall of 29%. This increase in Precision can be translated to achieve lower execution time and energy consumption, and thus higher performance and energy efficiency; compared to the default Hadoop mechanism, the energy consumption is reduced by almost 31%. These results demonstrate howour framework can offer useful insights and be applied in practical settings to characterize and design newstraggler detection mechanisms for MapReduce systems. © 2019 Association for Computing Machinery.",,Big data; Computer software; Energy utilization; Architectural modeling; Detection approach; Detection latency; Detection mechanism; Hierarchical approach; Measures of performance; Resource utilizations; Speculative execution; Energy efficiency
Optimizing N-tier application scalability in the cloud: A study of soft resource allocation,2019,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075739674&doi=10.1145%2f3326120&partnerID=40&md5=f1ac062b0e17580dd9d7f8e429c235ef,"An effective cloud computing environment requires both good performance and high efficiency of computing resources. Through extensive experiments using a representative n-tier benchmark application (Rice University Bulletin Board System (RUBBoS)), we show that the soft resource allocation (e.g., thread pool size and database connection pool size) in component servers has a significant impact on the overall system performance, especially at high system utilization scenarios. Concretely, the same software resource allocation can be a good setting in one hardware configuration and then becomes an either under-or over-allocation in a slightly different hardware configuration, causing a significant performance drop. We have also observed some interesting phenomena that were caused by the non-trivial dependencies between the soft resources of servers in different tiers. For instance, the thread pool size in an Apache web server can limit the total number of concurrent requests to the downstream servers, which surprisingly decreases the Central Processing Unit (CPU) utilization of the Clustered Java Database Connectivity (C-JDBC) clustering middleware as the workload increases. To provide a globally optimal (or near-optimal) soft resource allocation of each tier in the system, we propose a practical iterative solution approach by combining a soft resource aware queuing network model and the fine-grained measurement data of every component server. Our results show that to truly scale complex distributed systems such as n-tier web applications with expected performance in the cloud, we need to carefully manage soft resource allocation in the system. © 2019 Association for Computing Machinery.",Cloud computing; Configuration; Parallel processing; Scalability; Soft resource; Web application,Bulletin boards; Cloud computing; Computer hardware; Lakes; Middleware; Program processors; Resource allocation; Scalability; Cloud computing environments; Complex distributed system; Configuration; Hardware configurations; Java data base connectivities; Parallel processing; Soft resources; WEB application; Benchmarking
SORT: Semi online reliable task mapping for embedded multi-core systems,2019,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075761107&doi=10.1145%2f3322899&partnerID=40&md5=0a2e1dd419522fac6bb3ea024681264f,"This article proposes a Semi Online Reliable Task (SORT) mapping approach to many-core platforms divided into two sections: Offline and online. The offline section is a twofolded approach. It maintains the reliability of the mapped task graph against soft errors considering the reliability threshold defined by designers. As wear-out mechanisms decrease the lifetime of the system, our proposed approach increases the lifetime of the system using task migration scenarios. It specifies task migration plans with the minimum overhead using a novel heuristic approach. SORT maintains the required level of reliability of the task graph in the whole lifetime of the system using a replication technique with minimum replica overhead, maximum achievable performance, and minimum temperature increase. The online segment uses migration plans obtained in the offline segment to increase the lifetime and also permanently maintains the reliability threshold for the task graph during runtime. Results show that the effectiveness of SORT improves on bigger mesh sizes and higher reliability thresholds. Simulation results obtained from real benchmarks show that the proposed approach decreases design-time calculation up to 4,371% compared to exhaustive exploration while achieving a lifetime negligibly lower than the exhaustive solution (up to 5.83%). © 2019 Association for Computing Machinery.",Multi-core; Reliability; Soft error; Task mapping; Wear-out,Computer architecture; Embedded systems; Error correction; Heuristic methods; Mapping; Radiation hardening; Reliability; Wear of materials; Achievable performance; Minimum temperatures; Multi core; Multi-core systems; Reliability threshold; Replication techniques; Soft error; Task mapping; Online systems
Performance of a fixed reward incentive scheme for two-hop DTNs with competing relays,2019,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075717076&doi=10.1145%2f3325288&partnerID=40&md5=73832f82f93e181a68b5929c66946572,"We analyze the performance of an incentive scheme for two-hop Delay-Tolerant Networks (DTNs) in which a backlogged source proposes a fixed reward to the relays to deliver a message. Only one message at a time is proposed by the source. For a given message, only the first relay to deliver it gets the reward corresponding to this message thereby inducing a competition between the relays. The relays seek to maximize the expected reward for eachmessage, whereas the objective of the source is to satisfy a given constraint on the probability of message delivery. We show that the optimal policy of a relay is of threshold type: It accepts a message until a first threshold and then keeps the message until it either meets the destination or reaches the second threshold. Formulas for computing the thresholds as well as probability of message delivery are derived for a backlogged source. © 2019 Association for Computing Machinery.",Modeling and performance evaluation; Opportunistic and delay tolerant networks,Wireless networks; Delaytolerant networks (DTNs); Incentive schemes; Message delivery; Optimal policies; Delay tolerant networks
On the endurance of the d-choices garbage collection algorithm for flash-based SSDs,2019,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074676275&doi=10.1145%2f3326121&partnerID=40&md5=fc6a11d709531c330a667ab5d57e58a7,"Garbage collection (GC) algorithms for flash-based solid-state drives (SSDs) have a profound impact on its performance and many studies have focused on assessing the so-called write amplification of various GC algorithms. In this article, we consider the family of d-choices GC algorithms and study (a) the extent in which these algorithms induce unequal wear and (b) the manner in which they affect the lifetime of the drive. For this purpose, we introduce two performance measures: PE fairness and SSD endurance. We study the impact of the d-choices GC algorithm on both these measures under different workloads (uniform, synthetic and trace-based) when combined with two different write modes. Numerical results show that the more complex of the two write modes, which requires hot/cold data identification, may not necessarily give rise to a significantly better SSD endurance. Further, the d-choices GC algorithm is often shown to strike a good balance between garbage collection and wear leveling for small d values (e.g., d = 10), yielding high endurance. © 2019 Association for Computing Machinery.",Endurance; Garbage collection; Hot/cold data identification; Hot/cold data separation; SSD; Wear leveling; Write amplification,Durability; Refuse collection; Wear of materials; Data identification; Data separation; Garbage collection; Wear leveling; Write amplifications; Flash-based SSDs
Performance of redundancy(d) with identical/independent replicas,2019,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075751108&doi=10.1145%2f3316768&partnerID=40&md5=8acecef029a71334e22e175ee86793b8,"Queueing systems with redundancy have received considerable attention recently. The idea of redundancy is to reduce latency by replicating each incoming job a number of times and to assign these replicas to a set of randomly selected servers. As soon as one replica completes service the remaining replicas are cancelled. Most priorwork on queueing systemswith redundancy assumes that the job durations of the different replicas are independent and identically distributed (i.i.d.), which yields insights that can be misleading for computer system design. In this article, we develop a differential equation, using the cavity method, to assess the workload and response time distribution in a large homogeneous system with redundancy without the need to rely on this independence assumption. More specifically, we assume that the duration of each replica of a single job is identical across the servers and follows a general service time distribution. Simulation results suggest that the differential equation yields exact results as the system size tends to infinity and can be used to study the stability of the system. We also compare our system to the one with i.i.d. replicas and show the similarity in the analysis used for independent, respectively, identical replicas. © 2019 Association for Computing Machinery.",Large-scale computer network; Load balancing; Redundancy(d),Differential equations; Queueing networks; Resource allocation; System stability; Cavity method; Exact results; Homogeneous system; Independence assumption; Queueing system; Response time distribution; Service time distribution; System size; Redundancy
Managing Response Time Tails by Sharding,2019,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074676254&doi=10.1145%2f3300143&partnerID=40&md5=241dbfc47e2d8a85dd826cf3508f0f13,"Matrix analytic methods are developed to compute the probability distribution of response times (i.e., data access times) in distributed storage systems protected by erasure coding, which is implemented by sharding a data object into N fragments, only K < N of which are required to reconstruct the object. This leads to a partial-fork-join model with a choice of canceling policies for the redundant N - K tasks. The accuracy of the analytical model is supported by tests against simulation in a broad range of setups. At increasing workload intensities, numerical results show the extent to which increasing the redundancy level reduces the mean response time of storage reads and significantly flattens the tail of their distribution; this is demonstrated at medium-high quantiles, up to the 99th. The quantitative reduction in response time achieved by two policies for canceling redundant tasks is also shown: For cancel-at-finish and cancel-at-start, which limits the additional load introduced whilst losing the benefit of selectivity amongst fragment service times. © 2019 Copyright held by the owner/author(s).",Parallel task processing; Performance; Quality of service; Response time; Sharding,Multiprocessing systems; Probability distributions; Quality of service; Response time (computer systems); Distributed storage system; Matrix analytic methods; Mean response time; Numerical results; Parallel task; Performance; Sharding; Workload intensities; Digital storage
6 Provisioning and performance evaluation of parallel systems with output synchronization,2019,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074674517&doi=10.1145%2f3300142&partnerID=40&md5=7ab63fb69443d2f60ec00be95ff7c8ff,"Parallel server frameworks are widely deployed in modern large-data processing applications. Intuitively, splitting and parallel processing of the workload provides accelerated application response times and scaling flexibility. Examples of such frameworks include MapReduce, Hadoop, and Spark. For many applications, the dynamics of such systems are naturally captured by a Fork-Join (FJ) queuing model, where incoming jobs are split into tasks each of which is mapped to exactly one server. When all the tasks that belong to one job are executed, the job is reassembled and leaves the system. We consider this behavior at the output as a synchronization constraint. In this article, we study the performance of such parallel systems for different server properties, e.g., workconservingness, phase-type behavior, and as suggested by recent evidence, for bursty input job arrivals. We establish a Large Deviations Principle for the steady-state job waiting times in an FJ system based on Markovadditive processes. Building on that,we present a performance analysis framework for FJ systems and provide computable bounds on the tail probabilities of the steady-state waiting times. We validate our bounds using estimates obtained through simulations. In addition, we define and analyze provisioning, a flexible division of jobs into tasks, in FJ systems. Finally, we use this framework together with real-world traces to show the benefits of an adaptive provisioning system that adjusts the service within an FJ system based on the arrival intensity. © 2019 Association for Computing Machinery.",Fork-join queues; Markov additive processes; Parallel systems; Performance evaluation; Queuing systems,Data handling; Queueing theory; Fork-join queues; Markov additive process; Parallel system; Performance evaluation; Queuing systems; Parallel processing systems
Efficient straggler replication in large-scale parallel computing,2019,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074674337&doi=10.1145%2f3310336&partnerID=40&md5=f166ceae3cd978a4e3a36ef5b04d838e,"In a cloud computing job with many parallel tasks, the tasks on the slowest machines (straggling tasks) become the bottleneck in the job completion. Computing frameworks such as MapReduce and Spark tackle this by replicating the straggling tasks and waiting for any one copy to finish. Despite being adopted in practice, there is little analysis of how replication affects the latency and the cost of additional computing resources. In this article, we provide a framework to analyze this latency-cost tradeoff and find the best replication strategy by answering design questions, such as (1) when to replicate straggling tasks, (2) how many replicas to launch, and (3) whether to kill the original copy or not. Our analysis reveals that for certain execution time distributions, a small amount of task replication can drastically reduce both latency and the cost of computing resources. We also propose an algorithm to estimate the latency and cost based on the empirical distribution of task execution time. Evaluations using samples in the Google Cluster Trace suggest further latency and cost reduction compared to the existing replication strategy used in MapReduce. © 2019 Association for Computing Machinery.",Straggler replication; Task scheduling,Cost reduction; Computing frameworks; Computing resource; Empirical distributions; Large-scale parallel computing; Replication strategies; Straggler replication; Task replications; Task-scheduling; Cost benefit analysis
A quantitative and comparative study of network-level efficiency for cloud storage services,2019,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072821932&doi=10.1145%2f3274526&partnerID=40&md5=314cd440b818c12b783ce98069451ee9,"Cloud storage services such as Dropbox and OneDrive provide users with a convenient and reliable way to store and share data from anywhere, on any device, and at any time. Their cornerstone is the data synchronization (sync) operation, which automatically maps the changes in users' local file systems to the cloud via a series of network communications in a timely manner. Without careful design and implementation, however, the data sync mechanisms could generate overwhelming traffic, causing tremendous financial overhead and performance penalties to both service providers and end users. This article addresses a simple yet critical question: Is the current data sync traffic of cloud storage services efficiently used? We first define a novel metric TUE to quantify the Traffic Usage Efficiency of data synchronization. Then, by conducting comprehensive benchmark experiments and reverse engineering the data sync processes of eight widely used cloud storage services, we uncover their manifold practical endeavors for optimizing the TUE, including three intra-file approaches (compression, incremental sync, and interrupted transfer resumption), two cross-file/-user approaches (i.e., deduplication and peer-assisted offloading), two batching approaches (file bundling and sync deferment), and two web-specific approaches (thumbnail views and dynamic content loading). Our measurement results reveal that a considerable portion of the data sync traffic is, in a sense, wasteful and can be effectively avoided or significantly reduced via carefully designed data sync mechanisms. Most importantly, our study not only offers practical, actionable guidance for providers to build more efficient, traffic-economic services, but also helps end users pick appropriate services that best fit their use cases and budgets. © 2019 Association for Computing Machinery.",Cloud storage service; Data synchronization; Network-level efficiency; Traffic usage efficiency,Budget control; Efficiency; Electronic document exchange; Reverse engineering; Synchronization; Benchmark experiments; Cloud storage services; Comparative studies; Data synchronization; Design and implementations; Network communications; Network level; Performance penalties; Digital storage
Effect of recommendations on serving content with unknown demand,2019,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074674146&doi=10.1145%2f3289324&partnerID=40&md5=f7bd89ec164265b910bf4b0c3c5b3d07,"We consider the task of content replication in distributed content delivery systems used by Video-on-Demand (VoD) services with large content catalogs. The prior work in this area focuses on the setting where each request is generated independent of all past requests. Motivated by the fact that most popular VoD services offer recommendations to users based on their viewing history, in a departure from existing studies, we study the setting with time-correlation in requests coming from each user. We use a Markovian process to model each user's request process. In addition to introducing time-correlation in user requests, our model is consistent with empirically observed properties of the request process for VoD services with recommendation engines. In the setting where the underlying Markov Chain is unknown and has to be learned from the very requests the system is trying to serve, we show that separating the task of estimating content popularity and using the estimates to design a static content replication policy is strictly sub-optimal. To prove this, we show that an adaptive policy, which jointly performs the task of estimation and content replication, outperforms all policies that separate the task of estimation and content replication. © 2019 Association for Computing Machinery.",Caching; Content delivery network; Performance analysis,Distributed computer systems; Markov processes; Caching; Content delivery network; Content popularities; Content replication; Distributed content delivery; Markovian process; Performance analysis; Video on demand services; Video on demand
Efficient traffic load-balancing via incremental expansion of routing choices,2019,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074675987&doi=10.1145%2f3243173&partnerID=40&md5=4fa0946ff53361e2da8155c699080f77,"Routing policies play a major role in the performance of communication networks. Backpressure-based adaptive routing algorithms where traffic is load balanced along different routing paths on a per-packet basis have been studied extensively in the literature. Although backpressure-based algorithms have been shown to be networkwide throughput optimal, they typically have poor delay performance under light or moderate loads, because packets may be sent over unnecessarily long routes. Further, backpressure-based algorithms have required every node to compute differential backlogs for every per-destination queue with the corresponding per-destination queue at every adjacent node, which is expensive given the large number of possible pairwise differential backlogs between neighbor nodes. In this article, we propose new backpressure-based adaptive routing algorithms that only use shortest-path routes to destinations when they are sufficient to accommodate the given traffic load, but the proposed algorithms will incrementally expand routing choices as needed to accommodate increasing traffic loads. We show analytically by means of fluid analysis that the proposed algorithms retain networkwide throughput optimality, and we show empirically by means of simulations that our proposed algorithms provide substantial improvements in delay performance. Our evaluations further show that, in practice, our approach dramatically reduces the number of pairwise differential backlogs that have to be computed and the amount of corresponding backlog information that has to be exchanged, because routing choices are only incrementally expanded as needed. © 2019 Association for Computing Machinery.",Dynamic routing; Load balancing; Multipath routing; Traffic engineering,Highway engineering; Resource allocation; Adaptive routing algorithm; Delay performance; Dynamic routing; Multi path routing; Routing policies; Throughput Optimality; Throughput-optimal; Traffic Engineering; Balancing
EnergyQARE: QoS-aware data center participation in smart grid regulation service reserve provision,2019,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068728107&doi=10.1145%2f3243172&partnerID=40&md5=dedc2b8125daa1613712c8fa9a195eca,"Power market operators have recently introduced smart grid demand response (DR), in which electricity consumers regulate their power usage following market requirements. DR helps stabilize the grid and enables integrating a larger amount of intermittent renewable power generation. Data centers provide unique opportunities for DR participation due to their flexibility in both workload servicing and power consumption. While prior studies have focused on data center participation in legacy DR programs such as dynamic energy pricing and peak shaving, this article studies data centers in emerging DR programs, i.e., demand side capacity reserves. Among different types of capacity reserves, regulation service reserves (RSRs) are especially attractive due to their relatively higher value. This article proposes EnergyQARE, the Energy and Quality-of-Service (QoS) Aware RSR Enabler, an approach that enables data center RSR provision in real-life scenarios. EnergyQARE not only provides a bidding strategy in RSR provision, but also contains a runtime policy that adaptively modulates data center power through server power management and server provisioning based on workload QoS feedback. To reflect real-life scenarios, this runtime policy handles a heterogeneous set of jobs and considers transition time delay of servers. Simulated numerical results demonstrate that in a general data center scenario, EnergyQARE provides close to 50% of data center average power consumption as reserves to the market and saves up to 44% in data center electricity cost, while still meeting workload QoS constraints. Case studies in this article show that the percentages of savings are not sensitive to a specific type of non-interactive workload, or the size of the data center, although they depend strongly on data center utilization and parameters of server power states. © 2019 Association for Computing Machinery.",Data center; Demand response; Power management; Power market; Quality of service; Regulation service reserves; Smart grid; Workload control,Commerce; Costs; Electric power measurement; Electric power system control; Electric power transmission networks; Electric power utilization; Green computing; Power management; Power markets; Quality of service; Telecommunication services; Data centers; Demand response; Regulation services; Smart grid; Workload control; Smart power grids
Production application performance data streaming for system monitoring,2019,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074674879&doi=10.1145%2f3319498&partnerID=40&md5=0efd1376b35fd26cdeeac2d12d71fb33,"In this article, we present an approach to streaming collection of application performance data. Practical application performance tuning and troubleshooting in production high-performance computing (HPC) environments requires an understanding of how applications interact with the platform, including (but not limited to) parallel programming libraries such as Message Passing Interface (MPI). Several profiling and tracing tools exist that collect heavy runtime data traces either in memory (released only at application exit) or on a file system (imposing an I/O load that may interfere with the performance being measured). Although these approaches are beneficial in development stages and post-run analysis, a systemwide and low-overhead method is required to monitor deployed applications continuously. This method must be able to collect information at both the application and system levels to yield a complete performance picture. In our approach, an application profiler collects application event counters. A sampler uses an efficient inter-process communication method to periodically extract the application counters and stream them into an infrastructure for performance data collection. We implement a tool-set based on our approach and integrate it with the Lightweight Distributed Metric Service (LDMS) system, a monitoring system used on large-scale computational platforms. LDMS provides the infrastructure to create and gather streams of performance data in a low overhead manner. We demonstrate our approach using applications implemented with MPI, as it is one of the most common standards for the development of large-scale scientific applications. We utilize our tool-set to study the impact of our approach on an open source HPC application, Nalu. Our tool-set enables us to efficiently identify patterns in the behavior of the application without source-level knowledge. We leverage LDMS to collect system-level performance data and explore the correlation between the system and application events. Also, we demonstrate how our tool-set can help detect anomalies with a low latency. We run tests on two different architectures: a system enabled with Intel Xeon Phi and another system equipped with Intel Xeon processor. Our overhead study shows our method imposes at most 0.5% CPU usage overhead on the application in realistic deployment scenarios. © 2019 Association for Computing Machinery.",Application and system monitoring; Application profiling; Performance data streaming,Data acquisition; Data reduction; Message passing; Open source software; Parallel programming; Application profiling; High performance computing (HPC); Interprocess communication; Message passing interface; Performance data; Scientific applications; System monitoring; System-level performance; Monitoring
Ensuring persistent content in opportunistic networks via stochastic stability analysis,2018,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074676469&doi=10.1145%2f3232161&partnerID=40&md5=0b0545e2aeb223e01c87b1e01b0d4bda,"The emerging device-to-device communication solutions and the abundance of mobile applications and services make opportunistic networking not only a feasible solution but also an important component of future wireless networks. Specifically, the distribution of locally relevant content could be based on the community of mobile users visiting an area, if long-term content survival can be ensured this way. In this article, we establish the conditions of content survival in such opportunistic networks, considering the user mobility patterns, as well as the time users keep forwarding the content, as the controllable system parameter. We model the content spreading with an epidemic process, and derive a stochastic differential equations based approximation. By means of stability analysis, we determine the necessary user contribution to ensure content survival. We show that the required contribution from the users depends significantly on the size of the population, that users need to redistribute content only in a short period within their stay, and that they can decrease their contribution significantly in crowded areas. Hence, with the appropriate control of the system parameters, opportunistic content sharing can be both reliable and sustainable. © 2018 ACM.",Content sharing; Mobility; Opportunistic networks; Stochastic epidemic modeling,Carrier mobility; Differential equations; Mobile telecommunication systems; Stochastic models; Wireless ad hoc networks; Content Sharing; Device-to-Device communications; Future wireless networks; Mobile applications and services; Opportunistic networks; Stochastic differential equations; Stochastic epidemic models; Stochastic stability analysis; Stochastic systems
On the convergence of the TTL approximation for an LRU cache under independent stationary request processes,2018,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069760924&doi=10.1145%2f3239164&partnerID=40&md5=9392b5c86e6d2726a0847492115ecc7f,"The modeling and analysis of an LRU cache is extremely challenging as exact results for the main performance metrics (e.g., hit rate) are either lacking or cannot be used because of their high computational complexity for large caches. As a result, various approximations have been proposed. The state-of-the-art method is the so-called TTL approximation, first proposed and shown to be asymptotically exact for IRM requests by Fagin [13]. It has been applied to various other workload models and numerically demonstrated to be accurate but without theoretical justification. In this article, we provide theoretical justification for the approximation in the case where distinct contents are described by independent stationary and ergodic processes. We show that this approximation is exact as the cache size and the number of contents go to infinity. This extends earlier results for the independent reference model. Moreover, we establish results not only for the aggregate cache hit probability but also for every individual content. Last, we obtain bounds on the rate of convergence. © 2018 Association for Computing Machinery.",Asymptotic exactness; Cache; Characteristic time; Convergence; LRU; Stationary request processes; TTL approximation,Computer network performance evaluation; Computer programming; Computer science; Hardware; Asymptotic exactness; Cache; Characteristic time; Convergence; Independent reference models; Performance metrics; Rate of convergence; State-of-the-art methods; Transistor transistor logic circuits
Mean field games in nudge systems for societal networks,2018,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074674947&doi=10.1145%2f3232076&partnerID=40&md5=8572e6a55891699150f6a9c01ed3e9fc,"We consider the general problem of resource sharing in societal networks, consisting of interconnected communication, transportation, energy, and other networks important to the functioning of society. Participants in such network need to take decisions daily, both on the quantity of resources to use as well as the periods of usage. With this in mind, we discuss the problem of incentivizing users to behave in such a way that society as a whole benefits. To perceive societal level impact, such incentives may take the form of rewarding users with lottery tickets based on good behavior and periodically conducting a lottery to translate these tickets into real rewards. We will pose the user decision problem as a mean field game and the incentives question as one of trying to select a good mean field equilibrium (MFE). In such a framework, each agent (a participant in the societal network) takes a decision based on an assumed distribution of actions of his/her competitors and the incentives provided by the social planner. The system is said to be at MFE if the agent's action is a sample drawn from the assumed distribution. We will show the existence of such an MFE under general settings, and also illustrate how to choose an attractive equilibrium using as an example demand-response in the (smart) electricity network. © 2018 ACM.",Lottery; Mean field games; Nudge system; Smart grid; Societal networks,Computer network performance evaluation; Computer programming; Hardware; Decision problems; Demand response; Electricity networks; Lottery; Mean field games; Nudge system; Resource sharing; Smart grid; Computer science
QMLE: A methodology for statistical inference of service demands from queueing data,2018,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074673740&doi=10.1145%2f3233180&partnerID=40&md5=001e4ee2ddb8b3ce94a46b74c396c389,"Estimating the demands placed by services on physical resources is an essential step for the definition of performance models. For example, scalability analysis relies on these parameters to predict queueing delays under increasing loads. In this article, we investigate maximum likelihood (ML) estimators for demands at load-independent and load-dependent resources in systems with parallelism constraints. We define a likelihood function based on state measurements and derive necessary conditions for its maximization. We then obtain novel estimators that accurately and inexpensively obtain service demands using only aggregate state data. With our approach, and also thanks to approximation methods for computing marginal and joint distributions for the load-dependent case, confidence intervals can be rigorously derived, explicitly taking into account both topology and concurrency levels of the services. Our estimators and their confidence intervals are validated against simulations and real system measurements for two multi-tier applications, showing high accuracy also in models with load-dependent resources. © 2018 ACM",Estimation; Maximum likelihood; Queueing networks; Service demand,Estimation; Maximum likelihood; Queueing networks; Queueing theory; Statistical methods; Approximation methods; Joint distributions; Likelihood functions; Maximum likelihood estimator; Multi-tier applications; Scalability analysis; Service demand; Statistical inference; Maximum likelihood estimation
"Quantifying cloud performance and dependability: Taxonomy, metric design, and emerging challenges",2018,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063779472&doi=10.1145%2f3236332&partnerID=40&md5=d03fefee42d63358ed375ff421d00ee0,"In only a decade, cloud computing has emerged from a pursuit for a service-driven information and communication technology (ICT), becoming a significant fraction of the ICT market. Responding to the growth of the market, many alternative cloud services and their underlying systems are currently vying for the attention of cloud users and providers. To make informed choices between competing cloud service providers, permit the cost-benefit analysis of cloud-based systems, and enable system DevOps to evaluate and tune the performance of these complex ecosystems, appropriate performance metrics, benchmarks, tools, and methodologies are necessary. This requires re-examining old system properties and considering new system properties, possibly leading to the re-design of classic benchmarking metrics such as expressing performance as throughput and latency (response time). In this work, we address these requirements by focusing on four system properties: (i) elasticity of the cloud service, to accommodate large variations in the amount of service requested, (ii) performance isolation between the tenants of shared cloud systems and resulting performance variability, (iii) availability of cloud services and systems, and (iv) the operational risk of running a production system in a cloud environment. Focusing on key metrics for each of these properties, we review the state-of-the-art, then select or propose new metrics together with measurement approaches. We see the presented metrics as a foundation toward upcoming, future industry-standard cloud benchmarks. © 2018 ACM.",,Commerce; Cost benefit analysis; Distributed database systems; Web services; Benchmarking metrics; Cloud service providers; Industry standards; Information and Communication Technologies; Performance isolations; Performance metrics; Performance variability; Underlying systems; Benchmarking
Scale-out vs scale-up: A study of ARM-based SoCs on server-class workloads,2018,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067306560&doi=10.1145%2f3232162&partnerID=40&md5=d054b923c03234cdc4e37c8a5258aa25,"ARM 64-bit processing has generated enthusiasm to develop ARM-based servers that are targeted for both data centers and supercomputers. In addition to the server-class components and hardware advancements, the ARM software environment has grown substantially over the past decade. Major development ecosystems and libraries have been ported and optimized to run on ARM, making ARM suitable for server-class workloads. There are two trends in available ARM SoCs: mobile-class ARM SoCs that rely on the heterogeneous integration of a mix of CPU cores, GPGPU streaming multiprocessors (SMs), and other accelerators, and the server-class SoCs that instead rely on integrating a larger number of CPU cores with no GPGPU support and a number of IO accelerators. For scaling the number of processing cores, there are two different paradigms: mobile-class SoCs that use scale-out architecture in the form of a cluster of simpler systems connected over a network, and server-class ARM SoCs that use the scale-up solution and leverage symmetric multiprocessing to pack a large number of cores on the chip. In this article, we present ScaleSoC cluster, which is a scale-out solution based on mobile class ARM SoCs. ScaleSoC leverages fast network connectivity and GPGPU acceleration to improve performance and energy efficiency compared to previous ARM scale-out clusters. We consider a wide range of modern server-class parallel workloads to study both scaling paradigms, including 8 latency-sensitive transactional workloads, MPI-based CPU and GPGPU-accelerated scientific applications, and emerging artificial intelligence workloads. We study the performance and energy efficiency of ScaleSoC compared to server-class ARM SoCs and discrete GPGPUs in depth. We quantify the network overhead on the performance of ScaleSoC and show that packing a large number of ARM cores on a single chip does not necessarily guarantee better performance, due to the fact that shared resources, such as last-level cache, become performance bottlenecks. We characterize the GPGPU accelerated workloads and demonstrate that for applications that can leverage the better CPU-GPGPU balance of the ScaleSoC cluster, performance and energy efficiency improve compared to discrete GPGPUs. © 2018 ACM",ARM computing; GPGPU acceleration; Scale-out clusters,Cluster computing; Energy efficiency; Green computing; Multiprocessing systems; Program processors; Supercomputers; ARM computing; Heterogeneous integration; Performance bottlenecks; Scale outs; Scale-out architectures; Scientific applications; Streaming multiprocessors; Symmetric multi processing; ARM processors
GPSonflow: Geographic positioning of storage for optimal nice flow,2018,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074675396&doi=10.1145%2f3197656&partnerID=40&md5=8641b7e26c86e2deee15cca2e0e75f73,"This article evaluates the maximum data flow from a sender to a receiver via the internet when all transmissions are scheduled for early morning hours. The significance of early morning hours is that internet congestion is low while users sleep. When the sender and receiver lie in proximal time zones, a direct transmission from sender to receiver can be scheduled for early morning hours. When the sender and receiver are separated by several time zones such that their sleep times are non-overlapping, data can still be transmitted during early morning hours with an indirect store-and-forward transfer. The data are transmitted from the sender to intermediate end networks or data centers that serve as storage hops en route to receiver. The storage hops are placed in zones that are time proximal to the sender or the receiver so that all transmissions to and from storage hops occur during low-congestion early morning hours. This article finds the optimal locations and bandwidth distributions of storage hops for maximum nice internet flow from a sender to a receiver. © 2018 ACM",Bulk data transfers; Flow network; Inter-datacenter transfers; Internet application; Maximum flow problem; System-graph model,Data transfer; Flow graphs; Bulk data transfer; Datacenter; Flow network; Graph model; Internet application; Maximum flow problems; Digital storage
Efficiency and optimality of largest deficit first prioritization: Dynamic user prioritization for soft real-time applications,2018,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074676216&doi=10.1145%2f3200479&partnerID=40&md5=aa95be9222c8b07e1b065a5b335fb3f0,"An increasing number of real-time applications with compute and/or communication deadlines are being supported on a shared infrastructure. Such applications can often tolerate occasional deadline violations without substantially impacting their Quality of Service (QoS). This article explores the efficient allocation of shared resources to satisfy such QoS requirements. We study a simple framework which decouples this problem into two subproblems: (1) dynamic prioritization of users based on arbitrary functions of their deficits (difference of achieved vs. required QoS), and (2) priority-based resource allocation on the underlying compute fabric. In this article, we shall assume the solution to the latter is fixed, e.g., as realized in the task prioritization capabilities of current hardware/software, and focus on compatible solutions to the former. We first characterize the set of feasible QoS requirements and show the optimality of max weight-like prioritization. We then consider simple weighted Largest Deficit First (w-LDF) prioritization policies, where users with higher weighted QoS deficits are given higher priority. The article gives an inner bound for the feasible set under w-LDF policies, and, under an additional monotonicity assumption, characterizes its geometry leading to a sufficient condition for optimality. Additional insights on the efficiency ratio of w-LDF policies, the optimality of hierarchical-LDF, and characterization of clustering of failures are also discussed. © 2018 ACM",Class-based hierarchical prioritization; Cloud-computing; Feasibility optimal; Feasibility region; Geometry of inner bound; Largest deficit first prioritization; Soft real-time applications,Cloud computing; Efficiency; Condition for optimality; Efficient allocations; Feasibility optimal; Feasibility region; Prioritization; Real-time application; Shared infrastructure; Soft real-time applications; Quality of service
Searching for a single community in a graph,2018,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074673835&doi=10.1145%2f3200863&partnerID=40&md5=3c7f24fb92f7a2a36d0e1256473b2ab8,"In standard graph clustering/community detection, one is interested in partitioning the graph into more densely connected subsets of nodes. In contrast, the search problem of this article aims to only find the nodes in a single such community, the target, out of the many communities that may exist. To do so, we are given suitable side information about the target; for example, a very small number of nodes from the target are labeled as such. We consider a general yet simple notion of side information: all nodes are assumed to have random weights, with nodes in the target having higher weights on average. Given these weights and the graph, we develop a variant of the method of moments that identifies nodes in the target more reliably, and with lower computation, than generic community detection methods that do not use side information and partition the entire graph. Our empirical results show significant gains in runtime, and also gains in accuracy over other graph clustering algorithms. © 2018 ACM",Community detection; Graph clustering; Search; Semi-supervised; Side information,Clustering algorithms; Method of moments; Population dynamics; Community detection; Graph clustering; Search; Semi-supervised; Side information; Graph theory
System and architecture level characterization of big data applications on big and little core server architectures,2018,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071107916&doi=10.1145%2f3229049&partnerID=40&md5=db2cf19c62e61df67ad13ec0174a1141,"The rapid growth in data yields challenges to process data efficiently using current high-performance server architectures such as big Xeon cores. Furthermore, physical design constraints, such as power and density, have become the dominant limiting factor for scaling out servers. Low-power embedded cores in servers such as little Atom have emerged as a promising solution to enhance energy-efficiency to address these challenges. Therefore, the question of whether to process the big data applications on big Xeon- or Little Atom-based servers becomes important. In this work, through methodical investigation of power and performance measurements, and comprehensive application-level, system-level, and micro-architectural level analysis, we characterize dominant big data applications on big Xeon- and little Atom-based server architectures. The characterization results across a wide range of real-world big data applications, and various software stacks demonstrate how the choice of big- versus little-core-based server for energy-efficiency is significantly influenced by the size of data, performance constraints, and presence of accelerator. In addition, we analyze processor resource utilization of this important class of applications, such as memory footprints, CPU utilization, and disk bandwidth, to understand their run-time behavior. Furthermore, we perform microarchitecture-level analysis to highlight where improvement is needed in big- and little-core microarchitec-tures to address their performance bottlenecks. © 2018 ACM",Accelerator; Big data; Characterization; High-performance server; Low-power server; Performance; Power,Application programs; Atoms; Characterization; Computer architecture; Embedded systems; Energy efficiency; Green computing; Particle accelerators; Architectural levels; Big data applications; Low power servers; Performance; Performance bottlenecks; Performance constraints; Performance measurements; Power; Big data
CloudHeat: An efficient online market mechanism for datacenter heat harvesting,2018,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055687129&doi=10.1145%2f3199675&partnerID=40&md5=3dd8af22a13fa4b5668df58a345c857b,"Datacenters are major energy consumers and dissipate an enormous amount of waste heat. Simple outdoor discharging of datacenter heat is energy-consuming and environmentally unfriendly. By harvesting datacenter waste heat and selling to the district heating system (DHS), both energy cost compensation and environment protection can be achieved. To realize such benefits in practice, an efficient market mechanism is required to incentivize the participation of datacenters. This work proposes CloudHeat, an online reverse auction mechanism for the DHS to solicit heat bids from datacenters. To minimize long-term social operational cost of the DHS and the datacenters, we apply a RFHC approach for decomposing the long-term problem into a series of one-round auctions, guaranteeing a small loss in competitive ratio. The one-round optimization is still NP-hard, and we employ a randomized auction framework to simultaneously guarantee truthfulness, polynomial running time, and an approximation ratio of 2. The performance of CloudHeat is validated through theoretical analysis and trace-driven simulation studies. © 2018 ACM",,District heating; Polynomial approximation; Waste heat; Approximation ratios; Competitive ratio; District heating system; Environment protection; Long-term problems; Market mechanisms; Online reverse auctions; Trace driven simulation; Electronic commerce
Disk prefetching mechanisms for increasing HTTP streaming video server throughput,2018,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073558177&doi=10.1145%2f3164536&partnerID=40&md5=4f362205e9872188ed325ebc66454f33,"Most video streaming traffic is delivered over HTTP using standard web servers. While traditional web server workloads consist of requests that are primarily for small files that can be serviced from the file system cache, HTTP video streaming workloads often service a long tail of large infrequently requested videos. As a result, optimizing disk accesses is critical to obtaining good server throughput. In this article we explore serialized, aggressive disk prefetching, a technique that can be used to improve the throughput of HTTP streaming video web servers. We identify how serialization and aggressive prefetching affect performance, and, based on our findings, we construct and evaluate Libception, an application-level shim library that implements both techniques. By dynamically linking against Libception at runtime, applications are able to transparently obtain benefits from serialization and aggressive prefetching without needing to change their source code. In contrast to other approaches that modify applications, make kernel changes, or attempt to optimize kernel tuning, Libception provides a portable and relatively simple system in which techniques for optimizing I/O in HTTP video streaming servers can be implemented and evaluated. We empirically evaluate the efficacy of serialization and aggressive prefetching both with and without Libception, using three web servers (Apache, nginx, and the userver) running on two operating systems (FreeBSD and Linux). We find that, by using Libception, we can improve streaming throughput for all three web servers by at least a factor of 2 on FreeBSD and a factor of 2.5 on Linux. Additionally, we find that with significant tuning of Linux kernel parameters, we can achieve similar performance to Libception by globally modifying Linux's disk prefetch behaviour. Finally, we demonstrate Libception's ability to reduce the completion time of a microbenchmark involving two applications competing for disk resources. © 2018 ACM.",Block i/o schedulers; Disk prefetching; Disk request serialization; Disk throughput; HTTP streaming video; Web server performance,Linux; Throughput; Video streaming; Web services; Block i/o schedulers; Disk request serialization; Http streaming; Prefetching; Web server performance; HTTP
RAPL in action: Experiences in using RAPL for power measurements,2018,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057252724&doi=10.1145%2f3177754&partnerID=40&md5=49dadace1ce44c47e3440ed831b60ad1,"To improve energy efficiency and comply with the power budgets, it is important to be able to measure the power consumption of cloud computing servers. Intel's Running Average Power Limit (RAPL) interface is a powerful tool for this purpose. RAPL provides power limiting features and accurate energy readings for CPUs and DRAM, which are easily accessible through different interfaces on large distributed computing systems. Since its introduction, RAPL has been used extensively in power measurement and modeling. However, the advantages and disadvantages of RAPL have not been well investigated yet. To fill this gap, we conduct a series of experiments to disclose the underlying strengths and weaknesses of the RAPL interface by using both customized microbenchmarks and three well-known application level benchmarks: Stream, Stress-ng, and ParFullCMS. Moreover, to make the analysis as realistic as possible, we leverage two production-level power measurement datasets from the Taito, a supercomputing cluster of the Finnish Center of Scientific Computing and also replicate our experiments on Amazon EC2. Our results illustrate different aspects of RAPL and document the findings through comprehensive analysis. Our observations reveal that RAPL readings are highly correlated with plug power, promisingly accurate enough, and have negligible performance overhead. Experimental results suggest RAPL can be a very useful tool to measure and monitor the energy consumption of servers without deploying any complex power meters. We also show that there are still some open issues, such as driver support, non-atomicity of register updates, and unpredictable timings that might weaken the usability of RAPL in certain scenarios. For such scenarios, we pinpoint solutions and workarounds. © 2018 ACM.",Dram power; Power modeling; RAPL; Rapl accuracy; Rapl validation,Automobile drivers; Benchmarking; Budget control; Cluster computing; Energy efficiency; Energy utilization; Program processors; Average power limit; Comprehensive analysis; Distributed computing systems; Highly-correlated; Power model; RAPL; Rapl accuracy; Rapl validation; Green computing
An empirical analysis of Amazon EC2 spot instance features affecting cost-effective resource procurement,2018,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072313295&doi=10.1145%2f3164538&partnerID=40&md5=7f9fbab8bd1fbdad19341edd254a1f92,"Many cost-conscious public cloud workloads (""tenants"") are turning to Amazon EC2's spot instances because, on average, these instances offer significantly lower prices (up to 10 times lower) than on-demand and reserved instances of comparable advertised resource capacities. To use spot instances effectively, a tenant must carefully weigh the lower costs of these instances against their poorer availability. Toward this, we empirically study four features of EC2 spot instance operation that a cost-conscious tenant may find useful to model. Using extensive evaluation based on historical spot instance data, we show shortcomings in the state-of-the-art modeling of these features that we overcome. As an extension to our prior work, we conduct data analysis on a rich dataset of the latest spot price traces collected from a variety of EC2 spot markets. Our analysis reveals many novel properties of spot instance operation, some of which offer predictive value whereas others do not. Using these insights, we design predictors for our features that offer a balance between computational efficiency (allowing for online resource procurement) and cost efficacy. We explore ""case studies"" wherein we implement prototypes of dynamic spot instance procurement advised by our predictors for two types of workloads. Compared to the state of the art, our approach achieves (i) comparable cost but much better performance (fewer bid failures) for a latency-sensitive in-memory Memcached cache and (ii) an additional 18% cost savings with comparable (if not better than) performance for a delay-tolerant batch workload. © 2018 ACM.",Resource procurement; Spot instance features,Computational efficiency; Cost effectiveness; Costs; Batch workloads; Empirical analysis; Online resources; Predictive values; Resource capacity; Resource procurement; Spot instances; State of the art; Cost benefit analysis
Special issue: Selected paper from the 8th ACM/SPEC international conference on performance engineering (ICPE 2017),2018,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074673300&doi=10.1145%2f3186329&partnerID=40&md5=8c7dc0df533af61b5e66070f5e6ed968,[No abstract available],,
An online emergency demand response mechanism for cloud computing,2018,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074675817&doi=10.1145%2f3177755&partnerID=40&md5=a17ab7dcb6bce95f4e26fe6a6fb85295,"This article studies emergency demand response (EDR) mechanisms from a data center perspective, where a cloud participates in a mandatory EDR program while receiving computing job bids from cloud users in an online fashion. We target a realistic EDR mechanism where (i) the cloud provider dynamically packs different types of resources on servers into requested VMs and computes job schedules to meet users’ requirements, (ii) the power consumption of servers in the cloud is limited by the grid through the EDR program, and (iii) the operation cost of the cloud is considered in the calculation of social welfare, measured by an electricity cost that consists of both volume charge and peak charge. We propose an online auction for dynamic cloud resource provisioning that is under the control of the EDR program, runs in polynomial time, achieves truthfulness, and close-to-optimal social welfare for the cloud ecosystem. In the design of the online auction, we first propose a new framework, compact exponential LPs, to handle job scheduling constraints in the time domain. We then develop a posted pricing auction framework toward the truthful online auction design, which leverages the classic primal-dual technique for approximation algorithm design. We evaluate our online auctions through both theoretical analysis and empirical studies driven by real-world traces. © 2018 ACM.",Approximation algorithms; Cloud computing; Demand response; Mechanism design,Approximation algorithms; Cloud computing; Costs; Electronic commerce; Emergency services; Machine design; Polynomial approximation; Time domain analysis; Cloud ecosystems; Demand response; Electricity costs; Emergency demand response (EDR); Empirical studies; Mechanism design; Online auction design; Resource provisioning; Green computing
EQ: A QoE-centric rate control mechanism for VoIP calls,2018,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066035866&doi=10.1145%2f3170430&partnerID=40&md5=cee55657140e95a582c0cd835a7d0b70,"The rising popularity of data calls and the slowed global economy have posed a challenge to voice data networking—how to satisfy the growing user demand for VoIP calls under limited network resources. In a bandwidth-constrained network in particular, raising the bitrate for one call implies a lowered bitrate for another. Therefore, knowing whether it is worthwhile to raise one call’s bitrate while other users might complain is crucial to the design of a user-centric rate control mechanism. To this end, previous work (Chen et al. 2012) has reported a log-like relationship between bitrate and user experience (i.e., QoE) in Skype calls. To show that the relationship extends to more general VoIP calls, we conduct a 60-participant user study via the Amazon Mechanical Turk crowdsourcing platform and reaffirm the log-like relationship between the call bitrate and user experience in widely used AMR-WB. The relationship gives rise to a simple and practical rate control scheme that exponentially quantizes the steps of rate change, therefore the name—exponential quantization (EQ). To support that EQ is effective in addressing the challenge, we show through a formal analysis that the resulting bandwidth allocation is optimal in both the overall QoE and the number of calls served. To relate EQ to existing rate control mechanisms, we show in a simulation study that the bitrates of calls administered by EQ converge over time and outperform those controlled by a (naïve) greedy mechanism and the mechanism implemented in Skype. © 2018 ACM.",Proportional Fairness; QoE; Rate Control; Skype; VoIP,Bandwidth; Voice/data communication systems; Amazon mechanical turks; Bandwidth-constrained; Crowdsourcing platforms; Proportional fairness; Rate controls; Rate-control mechanism; Skype; VoIP; Internet telephony
Selecting the top-quality item through crowd scoring,2018,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074676628&doi=10.1145%2f3157736&partnerID=40&md5=2ad27cee88aec778f221390fec4cbf33,"We investigate crowdsourcing algorithms for finding the top-quality item within a large collection of objects with unknown intrinsic quality values. This is an important problem with many relevant applications, such as in networked recommendation systems. The core of the algorithms is that objects are distributed to crowd workers, who return a noisy and biased evaluation. All received evaluations are then combined to identify the top-quality object. We first present a simple probabilistic model for the system under investigation. Then we devise and study a class of efficient adaptive algorithms to assign in an effective way objects to workers. We compare the performance of several algorithms, which correspond to different choices of the design parameters/metrics. In the simulations, we show that some of the algorithms achieve near optimal performance for a suitable setting of the system parameters. © 2017 ACM.",Crowd scoring; Recommendation systems; Resource allocation,Adaptive algorithms; Quality control; Recommender systems; Resource allocation; Crowd scoring; Design parameters; Near-optimal performance; Probabilistic modeling; Quality value; Top qualities; Parameter estimation
Bargaining game-based scheduling for performance guarantees in cloud computing,2018,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062722086&doi=10.1145%2f3141233&partnerID=40&md5=8d85d368c85f74f242df6fdc61dc358d,"In this article, we focus on request scheduling with performance guarantees of all users in cloud computing. Each cloud user submits requests with average response time requirement, and the cloud provider tries to find a scheduling scheme, i.e., allocating user requests to limited servers, such that the average response times of all cloud users can be guaranteed. We formulate the considered scenario into a cooperative game among multiple users and try to find a Nash bargaining solution (NBS), which can simultaneously satisfy all users’ performance demands. We first prove the existence of NBS and then analyze its computation. Specifically, for the situation when all allocating substreams are strictly positive, we propose a computational algorithm (CA), which can find the NBS very efficiently. For the more general case, we propose an iterative algorithm (IA), which is based on duality theory. The convergence of our proposed IA algorithm is also analyzed. Finally, we conduct some numerical calculations. The experimental results show that our IA algorithm can find an appropriate scheduling strategy and converges to a stable state very quickly. © 2018 ACM.",Cloud computing; Cooperative game; Nash bargaining solution; Performance guarantees,Cloud computing; Computation theory; Computational efficiency; Game theory; Iterative methods; Scheduling; Computational algorithm; Cooperative game; Iterative algorithm; Nash bargaining solution; Numerical calculation; Performance guarantees; Request scheduling; Scheduling strategies; Computer games
Mean-field analysis of coding versus replication in large data storage systems,2018,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074673540&doi=10.1145%2f3159172&partnerID=40&md5=24b088d588c16cd860d765b22d1dd894,"We study cloud storage systems with a very large number of files stored in a very large number of servers. In such systems, files are either replicated or coded to ensure reliability, i.e., to guarantee file recovery from server failures. This redundancy in storage can further be exploited to improve system performance (mean file-access delay) through appropriate load-balancing (routing) schemes. However, it is unclear whether coding or replication is better from a system performance perspective since the corresponding queueing analysis of such systems is, in general, quite difficult except for the trivial case when the system load asymptotically tends to zero. Here, we study the more difficult case where the system load is not asymptotically zero. Using the fact that the system size is large, we obtain a mean-field limit for the steady-state distribution of the number of file access requests waiting at each server. We then use the mean-field limit to show that, for a given storage capacity per file, coding strictly outperforms replication at all traffic loads while improving reliability. Further, the factor by which the performance improves in the heavy traffic is at least as large as in the light-traffic case. Finally, we validate these results through extensive simulations. © 2018 ACM.",Cloud storage systems; File coding; Heavy-traffic analysis; Load-balancing; Mean-field-analysis,Codes (symbols); Resource allocation; Cloud storage systems; Extensive simulations; File coding; Heavy traffics; Mean field analysis; Mean field limits; Queueing analysis; Steady-state distributions; Digital storage
What you lose when you snooze: How duty cycling impacts on the contact process in opportunistic networks,2017,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059083476&doi=10.1145%2f3149007&partnerID=40&md5=7d0e15a3b92328fe462b3f90f692be0d,"In opportunistic networks, putting devices in energy-saving mode is crucial to preserve their battery, and hence to increase the lifetime of the network and foster user participation. A popular strategy for energy saving is duty cycling. However, when in energy-saving mode, users cannot communicate with each other. The side effects of duty cycling are twofold. On the one hand, duty cycling may reduce the number of usable contacts for delivering messages, increasing intercontact times, and delays. On the other hand, duty cycling may break long contacts into smaller contacts, thus also reducing the capacity of the opportunistic network. Despite the potential serious effects, the role played by duty cycling in opportunistic networks has been often neglected in the literature. In order to fill this gap, in this article, we propose a general model for deriving the pairwise contact and intercontact times measured when a duty cycling policy is superimposed on the original encounter process determined only by node mobility. The model we propose is general, i.e., not bound to a specific distribution of contact and intercontact times, and very accurate, as we show exploiting two traces of real human mobility for validation. Using this model, we derive several interesting results about the properties of measured contact and intercontact times with duty cycling: their distribution, how their coefficient of variation changes depending on the duty cycle value, and how the duty cycling affects the capacity and delay of an opportunistic network. The applicability of these results is broad, ranging from performance models for opportunistic networks that factor in the duty cycling effect, to the optimisation of the duty cycle to meet a certain target performance. © 2017 ACM.",Contact times; DTN; Duty cycling; Energy-saving; Intercontact times; Opportunistic networks,Computer network performance evaluation; Computer programming; Computer science; Hardware; Coefficient of variation; Contact time; Duty-cycling; Energy saving modes; Intercontact times; Opportunistic networks; Specific distribution; User participation; Energy conservation
Access-time-aware cache algorithms,2017,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051339667&doi=10.1145%2f3149001&partnerID=40&md5=c0ef7dcee07e627d13188b18dd500d66,"Most of the caching algorithms are oblivious to requests' timescale, but caching systems are capacity constrained and, in practical cases, the hit rate may be limited by the cache's impossibility to serve requests fast enough. In particular, the hard-disk access time can be the key factor capping cache performance. In this article, we present a new cache replacement policy that takes advantage of a hierarchical caching architecture, and in particular of access-time difference between memory and disk. Our policy is optimal when requests follow the independent reference model and significantly reduces the hard-disk load, as shown also by our realistic, trace-driven evaluation. Moreover, we show that our policy can be considered in a more general context, since it can be easily adapted to minimize any retrieval cost, as far as costs add over cache misses. © 2017 ACM.",Cache; Cache replacement policy; Caching architecture; Content delivery network (CDN); Hard disk access time; Knapsack problem,Combinatorial optimization; Distributed computer systems; Network architecture; Access time; Cache; Cache replacement policy; Caching architecture; Content delivery network; Knapsack problems; Cache memory
Resource auto-scaling and sparse content replication for video storage systems,2017,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074673851&doi=10.1145%2f3079045&partnerID=40&md5=fece63997cf826a99b6b652a3eb2d45e,"Many video-on-demand (VoD) providers are relying on public cloud providers for video storage, access, and streaming services. In this article, we investigate how a VoD provider may make optimal bandwidth reservations from a cloud service provider to guarantee the streaming performance while paying for the bandwidth, storage, and transfer costs.We propose a predictive resource auto-scaling system that dynamically books the minimum amount of bandwidth resources from multiple servers in a cloud storage system to allow the VoD provider to match its short-term demand projections.We exploit the anti-correlation between the demands of different videos for statistical multiplexing to hedge the risk of under-provisioning. The optimal load direction from video channels to cloud servers without replication constraints is derived with provable performance. We further study the joint load direction and sparse content placement problem that aims to reduce bandwidth reservation cost under sparse content replication requirements. We propose several algorithms, and especially an iterative L1-norm penalized optimization procedure, to efficiently solve the problem while effectively limiting the video migration overhead. The proposed system is backed up by a demand predictor that forecasts the expectation, volatility, and correlation of the streaming traffic associated with different videos based on statistical learning. Extensive simulations are conducted to evaluate our proposed algorithms, driven by the real-world workload traces collected from a commercial VoD system. © 2017 ACM.",Auto-scaling; Cloud computing; Content placement; Load direction; Optimization; Prediction; Sparse design; Video-on-demand,Bandwidth; Cloud computing; Facsimile; Forecasting; Iterative methods; Optimization; Auto-scaling; Bandwidth reservation; Cloud service providers; Content placement; Extensive simulations; Load direction; Optimization procedures; Video on demands (VoD); Video on demand
Behavioral model of IEEE 802.15.4 beacon-enabled mode based on colored petri net,2017,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041745770&doi=10.1145%2f3115389&partnerID=40&md5=ea8cbc4a0dd75f3c0e32d77b7794280c,"The IEEE 802.15.4 standard is widely employed in power-constrained scenarios, such as Wireless Sensor Networks deployments. Therefore, modeling this standard is useful to predict network performance and fine-tune parameter settings. Previous work rely on determining all reachable network states, usually by a Markov chain, which is often complex and error prone. In contrast, we provide a novel behavioral approach to the IEEE 802.15.4 modeling, which covers the literature gap in assessing all metrics of interest, modeling asymmetric trafic condition and fully comprising the beacon-enabled mode. In addition, it is possible to test different values for the parameters of the standard, such as a Max Frame Retries, mac Max CSMA Backoffs, initialCW, and a Unit Back of Period. The model was validated by NS2 simulations and by a testbed composed of telosB motes. © 2017 ACM.",Colored petri net model; IEEE 802.15.4; Wireless sensor networks,Markov processes; Petri nets; Wireless sensor networks; Beacon-Enabled modes; Behavioral approaches; Behavioral model; Colored Petri net modeling; Colored Petri Nets; IEEE 802.15.4; IEEE 802.15.4 standards; Parameter setting; IEEE Standards
Fast power and energy management for future many-core systems,2017,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058811556&doi=10.1145%2f3086504&partnerID=40&md5=f388c6efbb559959e6fa0655a166d41d,"Future servers will incorporate many active low-power modes for each core and for the main memory subsystem. Though these modes provide flexibility for power and/or energy management via Dynamic Voltage and Frequency Scaling (DVFS), prior work has shown that they must be managed in a coordinated manner. This requirement creates a combinatorial space of possible power mode configurations. As a result, it becomes increasingly challenging to quickly select the configuration that optimizes for both performance and power/energy efficiency. In this article, we propose a novel queuing model for working with the abundant active low-power modes in many-core systems. Based on the queuing model, we derive two fast algorithms that optimize for performance and efficiency using both CPU and memory DVFS. Our first algorithm, called FastCap, maximizes the performance of applications under a full-system power cap, while promoting fairness across applications. Our second algorithm, called FastEnergy, maximizes the full-system energy savings under predefined application performance loss bounds. Both FastCap and FastEnergy operate online and efficiently, using a small set of performance counters as input. To evaluate them, we simulate both algorithms for a many-core server running different types of workloads. Our results show that FastCap achieves better application performance and fairness than prior power capping techniques for the same power budget, whereas FastEnergy conserves more energy than prior energy management techniques for the same performance constraint. FastCap and FastEnergy together demonstrate the applicability of the queuing model for managing the abundant active low-power modes in many-core systems. © 2017 ACM.",Queuing theory and optimization,Budget control; Dynamic frequency scaling; Energy conservation; Voltage scaling; Application performance; Dynamic voltage and frequency scaling; Fast algorithms; Low power modes; Management techniques; Performance constraints; Performance counters; Queuing theory; Queueing theory
Scheduling for cloud-based computing systems to support soft real-time applications,2017,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067472817&doi=10.1145%2f3063713&partnerID=40&md5=03b4eae29ce92677faf4b96b0214a6b7,"Cloud-based computing infrastructure provides an efficient means to support real-time processing workloads, for example, virtualized base station processing, and collaborative video conferencing. This article addresses resource allocation for a computing system with multiple resources supporting heterogeneous soft real-time applications subject to Quality of Service (QoS) constraints on failures to meet processing deadlines. We develop a general outer bound on the feasible QoS region for non-clairvoyant resource allocation policies and an inner bound for a natural class of policies based on dynamically prioritizing applications' tasks by favoring those with the largest (QoS) deficits. This provides an avenue to study the efficiency of two natural resource allocation policies: (1) priority-based greedy task scheduling for applications with variable workloads and (2) priority-based task selection and optimal scheduling for applications with deterministic workloads. The near-optimality of these simple policies emerges when task processing deadlines are relatively large and/or when the number of compute resources is large. Analysis and simulations show substantial resource savings for such policies over reservation-based designs. © 2017 ACM 2376-3639/2017/06-ART13 $15.00.",Cloud-computing; Efficiency ratio; Feasibility region; Greedy task scheduling; Largest deficit first; Non-clairvoyant resource allocation; Soft real-time applications; Task selection and optimal scheduling,Cloud computing; Efficiency; Mobile telecommunication systems; Multitasking; Quality of service; Real time systems; Resource allocation; Scheduling; Scheduling algorithms; Video conferencing; Efficiency ratio; Feasibility region; Largest deficit first; Optimal scheduling; Soft real-time applications; Task-scheduling; Distributed computer systems
Insertion of PETSc in the OpenFOAM framework,2017,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054962383&doi=10.1145%2f3098821&partnerID=40&md5=b5211b51bb197cd8621c98465a985558,"OpenFOAM is a widely used open source framework for simulation in several areas of computational fluid dynamics and engineering. As a partial differential equation (PDE)-based framework, OpenFOAM suffers from a performance bottleneck in solving large-scale sparse linear systems of equations. To address the problem, this article proposes a novel OpenFOAM-PETSc framework by inserting PETSc, a dedicated numerical solving package, into the OpenFOAM to speed up the process of solving linear equation systems. The design of the OpenFOAM-PETSc framework is described, and the implementation of an efficient matrix conversion algorithm is given as a case study. Validation tests on a high-performance computing cluster show that OpenFOAM-PETSc reduces the time of solving PDEs by about 27% in the lid-driven cavity flow case and by more than 50% in flow around the cylinder case in comparison with OpenFOAM, without compromising the scalability. In addition, this article also gives a preliminary performance analysis of different numerical solution methods, which may provide guidelines for further optimizations. © 2017 ACM 2376-3639/2017/08-ART16 $15.00.",Cfd; OpenFOAM; PDE solving; Performance optimization; PETSc,Cluster computing; Computational fluid dynamics; Linear equations; Linear systems; Two phase flow; High-performance computing clusters; Numerical solution method; OpenFOAM; Partial differential equations (PDE); PDE solving; Performance bottlenecks; Performance optimizations; PETSc; Numerical methods
Copula analysis of temporal dependence structure in Markov modulated poisson process and its applications,2017,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065492671&doi=10.1145%2f3089254&partnerID=40&md5=ee6c7665f6c61b813a5d8fb27d8d92c9,"The Markov Modulated Poisson Process (MMPP) has been extensively studied in random process theory and widely applied in various applications involving Poisson arrivals whose rate varies following a Markov process. Despite the rich literature on MMPP, very little is known on its intricate temporal dependence structure. No exact solution is available so far to capture the functional temporal dependence of MMPP at the stationary state over slotted times. This article tackles the above challenges with copula analysis. It not only presents a novel analytical framework to capture the temporal dependence of MMPP but also provides the exact copula-based solutions for single MMPP as well as the aggregate of independent MMPP. This theoretical contribution discloses functional dependence structure of MMPP. It also lays the foundation for many applications that rely on the temporal dependence of MMPP for adaptive control or predictive resource provisioning. We demonstrate case studies, with real-world trace data as well as simulation, to illustrate the practical significance of our analytical results. © 2017 ACM 2376-3639/2017/06-ART14 $15.00.",Copula analysis; Markov Modulated Poisson Process; Traffic prediction,Markov processes; Poisson distribution; Traffic control; Analytical results; Copula analysis; Functional dependence; Markov modulated Poisson process; Random process theory; Resource provisioning; Temporal dependence; Traffic prediction; Distribution functions
Controlling the variability of capacity allocations using service deferrals,2017,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061079126&doi=10.1145%2f3086506&partnerID=40&md5=50db2e83e2e55868b18d6ebee516d93d,"Ensuring predictability is a crucial goal for service systems. Traditionally, research has focused on designing systems that ensure predictable performance for service requests. Motivated by applications in cloud computing and electricity markets, this article focuses on a different form of predictability: predictable allocations of service capacity. The focus of the article is a new model where service capacity can be scaled dynamically and service deferrals (subject to deadline constraints) can be used to control the variability of the active service capacity. Four natural policies for the joint problem of scheduling and managing the active service capacity are considered. For each, the variability of service capacity and the likelihood of deadline misses are derived. Further, the paper illustrates how pricing can be used to provide incentives for jobs to reveal deadlines and thus enable the possibility of service deferral in systems where the flexibility of jobs is not known to the system a priori. © 2017 ACM 2376-3639/2017/08-ART15 $15.00.",Deadlines; Incentives; Scheduling; Service variability,Computer network performance evaluation; Computer programming; Computer science; Hardware; Capacity allocation; Deadline constraint; Deadlines; Designing systems; Incentives; Service capacity; Service requests; Service variability; Scheduling
The economics of the cloud,2017,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045515704&doi=10.1145%2f3086574&partnerID=40&md5=b627c055093e0f9bc82e9cc4ddd8d7f4,"This article proposes a model to study the interaction of price competition and congestion in the cloud computing marketplace. Specifically, we propose a three-tier market model that captures a marketplace with users purchasing services from Software-as-a-Service (SaaS) providers, which in turn purchase computing resources from either Provider-as-a-Service (PaaS) or Infrastructure-as-a-Service (IaaS) providers. Within each level, we define and characterize market equilibria. Further, we use these characterizations to understand the relative profitability of SaaSs and PaaSs/IaaSs and to understand the impact of price competition on the user experienced performance, that is, the ""price of anarchy"" of the cloud marketplace. Our results highlight that both of these depend fundamentally on the degree to which congestion results from shared or dedicated resources in the cloud. © 2017 ACM.",Cloud market; Equilibrium; Game theory; Network economics,Commerce; Competition; Computation theory; Costs; Game theory; Infrastructure as a service (IaaS); Phase equilibria; Cloud markets; Computing resource; Market equilibria; Market model; Network economics; Price competition; Price of anarchy; Software as a service (SaaS)
Advance Reservation Games,2017,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041528215&doi=10.1145%2f3053046&partnerID=40&md5=061ec2e2c5d06ec5edd6dba2ad437a03,"Advance reservation (AR) services form a pillar of several branches of the economy, including transportation, lodging, dining, and, more recently, cloud computing. In this work, we use game theory to analyze a slotted AR system in which customers differ in their lead times. For each given time slot, the number of customers requesting service is a random variable following a general probability distribution. Based on statistical information, the customers decide whether or not to make an advance reservation of server resources in future slots for a fee. We prove that only two types of equilibria are possible: either none of the customers makes AR or only customers with lead time greater than some threshold make AR. Our analysis further shows that the fee that maximizes the provider's profit may lead to other equilibria, one of which yields zero profit. In order to prevent ending up with no profit, the provider can elect to advertise a lower fee yielding a guaranteed but smaller profit. We refer to the ratio of the maximum possible profit to the maximum guaranteed profit as the price of conservatism. When the number of customers is a Poisson random variable, we prove that the price of conservatism is one in the single-server case, but can be arbitrarily high in a many-server system. © 2017 ACM.",Advance reservations; game theory; queueing theory,Computation theory; Game theory; Probability distributions; Profitability; Random variables; Advance reservation; Poisson random variables; Provider's profits; Server resources; Server system; Single server; Statistical information; Time slots; Sales
"Measurement, Modeling, and Analysis of the Mobile App Ecosystem",2017,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043300815&doi=10.1145%2f2993419&partnerID=40&md5=f88703be616514d534493010cf9d31db,"Mobile applications (apps) have been gaining popularity due to the advances in mobile technologies and the large increase in the number of mobile users. Consequently, several app distribution platforms, which provide a new way for developing, downloading, and updating software applications in modern mobile devices, have recently emerged. To better understand the download patterns, popularity trends, and development strategies in this rapidly evolving mobile app ecosystem, we systematically monitored and analyzed four popular third-party Android app marketplaces. Our study focuses on measuring, analyzing, and modeling the app popularity distribution and explores how pricing and revenue strategies affect app popularity and developers' income. Our results indicate that unlike web and peer-to-peer file sharing workloads, the app popularity distribution deviates from commonly observed Zipf-like models. We verify that these deviations can be mainly attributed to a new download pattern, which we refer to as the clustering effect. We validate the existence of this effect by revealing a strong temporal affinity of user downloads to app categories. Based on these observations, we propose a new formal clustering model for the distribution of app downloads and demonstrate that it closely fits measured data. Moreover, we observe that paid apps follow a different popularity distribution than free apps and show how free apps with an ad-based revenue strategy may result in higher financial benefits than paid apps. We believe that this study can be useful to appstore designers for improving content delivery and recommendation systems, as well as to app developers for selecting proper pricing policies to increase their income. © 2017 ACM.",app popularity; app pricing; Appstores; clustering effect; mobile apps; revenue; workload characterization,Costs; Economics; Ecosystems; Electronic document exchange; Mobile telecommunication systems; Clustering effect; Development strategies; Financial benefits; Mobile applications; Mobile Technology; Peer-to-peer file sharing; Popularity distribution; Software applications; Application programs
Cocoa: Dynamic Container-Based Group Buying Strategies for Cloud Computing,2017,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043693914&doi=10.1145%2f3022876&partnerID=40&md5=69808f781d2dc7172257db8695f570c8,"Although the Infrastructure-as-a-Service (IaaS) cloud offers diverse instance types to users, a significant portion of cloud users, especially those with small and short demands, cannot find an instance type that exactly fits their needs or fully utilize purchased instance-hours. In the meantime, cloud service providers are also faced with the challenge to consolidate small, short jobs, which exhibit strong dynamics, to effectively improve resource utilization. To handle such inefficiencies and improve cloud resource utilization, we propose Cocoa (COmputing in COntAiners), a novel group buying mechanism that organizes jobs with complementary resource demands into groups and allocates them to group buying deals predefined by cloud providers. Each group buying deal offers a resource pool for all the jobs in the deal, which can be implemented as either a virtual machine or a physical server. By running each user job on a virtualized container, our mechanism allows flexible resource sharing among different users in the same group buying deal, while improving resource utilization for cloud providers. To organize jobs with varied resource demands and durations into groups, we model the initial static group organization as a variable-sized vector bin packing problem, and the subsequent dynamic group organization problem as an online multidimensional knapsack problem. Through extensive simulations driven by a large amount of real usage traces from a Google cluster, we evaluate the potential cost reduction achieved by Cocoa. We show that through the effective combination and interaction of the proposed static and dynamic group organization strategies, Cocoa greatly outperforms the existing cloud workload consolidation mechanism, substantiating the feasibility of group buying in cloud computing. © 2017 ACM.",container; cost saving; Group buying,Cocoa; Combinatorial optimization; Containers; Cost reduction; Dynamics; Bin packing problem; Cloud service providers; Extensive simulations; Flexible resources; Multidimensional knapsack problems; Resource demands; Resource utilizations; Workload consolidation; Infrastructure as a service (IaaS)
Obtaining and Managing Answer Quality for Online Data-Intensive Services,2017,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052013385&doi=10.1145%2f3055280&partnerID=40&md5=18fbdbc47989b16183e0c8ce865b04e2,"Online data-intensive (OLDI) services use anytime algorithms to compute over large amounts of data and respond quickly. Interactive response times are a priority, so OLDI services parallelize query execution across distributed software components and return best effort answers based on the data so far processed. Omitted data from slow components could lead to better answers, but tracing online how much better the answers could be is difficult. We propose Ubora, a design approach to measure the effect of slow-running components on the quality of answers. Ubora randomly samples online queries and executes them a second time. The first online execution omits data from slow components and provides interactive answers. The second execution uses mature results from intermediate components completed after the online execution finishes. Ubora uses memoization to speed up mature executions by replaying network messages exchanged between components. Our systems-level implementation works for a wide range of services, including Hadoop/Yarn, Apache Lucene, the EasyRec Recommendation Engine, and the OpenEphyra question-answering system. Ubora computes answer quality with more mature executions per second than competing approaches that do not use memoization. With Ubora, we show that answer quality is effective at guiding online admission control. While achieving the same answer quality on high-priority queries, our adaptive controller had 55% higher peak throughput on low-priority queries than a competing controller guided by the rate of timeouts. © 2017 ACM.",Answer quality; big data; services,Computer network performance evaluation; Computer programming; Computer science; Adaptive controllers; Any-time algorithms; Design approaches; Distributed software; Intermediate components; Large amounts of data; Network messages; Question answering systems; Query processing
Evaluating the Combined Effect of Memory Capacity and Concurrency for Many-Core Chip Design,2017,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084799174&doi=10.1145%2f3038915&partnerID=40&md5=8bc724afb87154aba6018ccfcec3df09,"Modern memory systems are structured under hierarchy and concurrency. The combined impact of hierarchy and concurrency, however, is application dependent and difficult to describe. In this article, we introduce C2-Bound, a data-driven analytical model that serves the purpose of optimizing many-core design. C2-Bound considers both memory capacity and data access concurrency. It utilizes the combined power of the newly proposed latency model, concurrent average memory access time, and the well-known memory-bounded speedup model (Sun-Ni's law) to facilitate computing tasks. Compared to traditional chip designs that lack the notion of memory capacity and concurrency, the C2-Bound model finds that memory bound factors significantly impact the optimal number of cores as well as their optimal silicon area allocations, especially for data-intensive applications with a non-parallelizable sequential portion. Therefore, our model is valuable to the design of next-generation many-core architectures that target big data processing, where working sets are usually larger than the conventional scientific computing. These findings are evidenced by our detailed simulations, which show, with C2-Bound, the design space of chip design can be narrowed down significantly up to four orders of magnitude. C2-Bound analytic results can be either used in reconfigurable hardware environments or, by software designers, applied to scheduling, partitioning, and allocating resources among diverse applications. © 2017 ACM.",chip design; concurrent average memory access time (C-AMAT); data access concurrency; data stall time; memory bound; Memory wall; Sun-Ni's law,Application programs; Data handling; Memory architecture; Reconfigurable hardware; Average memory access time; Combined effect; Data-intensive application; Diverse applications; Many-core architecture; Memory capacity; Orders of magnitude; Software designers; C (programming language)
Efficient Redundancy Techniques for Latency Reduction in Cloud Systems,2017,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046413248&doi=10.1145%2f3055281&partnerID=40&md5=19b96d54411a2542dfaf5ff4dfc74801,"In cloud computing systems, assigning a task to multiple servers and waiting for the earliest copy to finish is an effective method to combat the variability in response time of individual servers and reduce latency. But adding redundancy may result in higher cost of computing resources, as well as an increase in queueing delay due to higher traffic load. This work helps in understanding when and how redundancy gives a cost-efficient reduction in latency. For a general task service time distribution, we compare different redundancy strategies in terms of the number of redundant tasks and the time when they are issued and canceled. We get the insight that the log-concavity of the task service time creates a dichotomy of when adding redundancy helps. If the service time distribution is log-convex (i.e., log of the tail probability is convex), then adding maximum redundancy reduces both latency and cost. And if it is log-concave (i.e., log of the tail probability is concave), then less redundancy, and early cancellation of redundant tasks is more effective. Using these insights, we design a general redundancy strategy that achieves a good latency-cost trade-off for an arbitrary service time distribution. This work also generalizes and extends some results in the analysis of fork-join queues. © 2017 ACM.",latency-cost analysis; Performance modeling,Cost reduction; Economic and social effects; Probability distributions; Computing resource; Fork-join queues; Latency reduction; Maximum redundancies; Multiple servers; Redundancy techniques; Service time distribution; Tail probability; Redundancy
On fair attribution of costs under peak-based pricing to cloud tenants,2016,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040514329&doi=10.1145%2f2970815&partnerID=40&md5=a20ff730904a7e2f70bb93af3955b406,"The costs incurred by cloud providers towards operating their data centers are often determined in large part by their peak demands. The pricing schemes currently used by cloud providers to recoup these costs from their tenants, however, do not distinguish tenants based on their contributions to the cloud's overall peak demand. Using the concrete example of peak-based pricing as employed by many electric utility companies, we show that this “gap” may lead to unfair attribution of costs to the tenants. Simple enhancements of existing cloud pricing (e.g., analogous to the coincident peak pricing (CPP) used by some electric utilities) do not adequately address these shortcomings and suffer from short-term unfairness and undesirable oscillatory price-vs.demand relationships offered to tenants. To overcome these shortcomings, we define an alternative pricing scheme to more fairly distribute a cloud's costs among its tenants. We demonstrate the efficacy of our scheme under price-sensitive tenant demand response using a combination of (i) extensive empirical evaluation with recent workloads from commercial data centers operated by IBM and (ii) analytical [modeling] through non-cooperative game theory for a special case of tenant demand model. © 2016 ACM",Cloud tenant; Fairness; Game; Pricing design,Electric utilities; Game theory; Demand modeling; Demand response; Empirical evaluations; Fairness; Game; Non-cooperative game theory; Price sensitive; Utility companies; Costs
Self-similarity in social network dynamics,2016,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068230681&doi=10.1145%2f2994142&partnerID=40&md5=b108eefeee4d022376a2aeab96c10bb5,"Analyzing and modeling social network dynamics are key to accurately predicting resource needs and system behavior in online social networks. The presence of statistical scaling properties, that is, self-similarity, is critical for determining how to model network dynamics. In this work, we study the role that self-similarity scaling plays in a social network edge creation (that is, links created between users) process, through analysis of two detailed, time-stamped traces, a 199 million edge trace over 2 years in the Renren social network, and 876K interactions in a 4-year trace of Facebook. Using wavelet-based analysis, we find that the edge creation process in both networks is consistent with self-similarity scaling, once we account for periodic user activity that makes edge creation process non-stationary. Using these findings, we build a complete model of social network dynamics that combines temporal and spatial components. Specifically, the temporal behavior of our model reflects self-similar scaling properties, and accounts for certain deterministic non-stationary features. The spatial side accounts for observed long-term graph properties, such as graph distance shrinkage and local declustering. We validate our model against network dynamics in Renren and Facebook datasets, and show that it succeeds in producing desired properties in both temporal patterns and graph structural features. © 2016 ACM",Self-similarity; Social network dynamics; Social network measurement; Social network modeling,Dynamics; Network dynamics; Network measurement; Network modeling; On-line social networks; Self-similarities; Statistical scaling; Temporal and spatial; Wavelet based analysis; Social networking (online)
Fairness and incentive considerations in energy apportionment policies,2016,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032362298&doi=10.1145%2f2970816&partnerID=40&md5=d2a3b069f8e00de20522db59b8c6206c,"The energy consumption of a system is determined by the system component usage patterns and interactions between the coexisting entities and resources. Energy accounting plays an essential role in revealing the contribution of each entity to the total consumption and for energy management. Unfortunately, energy accounting inherits the apportionment problem of accounting in general, which does not have a general single best solution. In this article, we leverage cooperative game theory, which is commonly used in cost allocation problems to study the energy apportionment problem, that is, the problem of prescribing the actual energy consumption of a system to the consuming entities (e.g., applications, processes, or users of the system). We identify five relevant fairness properties for energy apportionment and present a detailed categorisation and analysis of eight previously proposed energy apportionment policies from different fields in computer and communication systems. In addition, we propose two novel energy apportionment policies based on cooperative game theory that provide strong fairness notion and a rich incentive structure. Our comparative analysis in terms of the identified five fairness properties as well as information requirement and computational complexity shows that there is a tradeoff between fairness and the other evaluation criteria. We provide guidelines to select an energy apportionment policy depending on the purpose of the apportionment and the characteristics of the system. © 2016 ACM",Cooperative game theory; Energy accounting; Energy apportionment; Energy management,Energy management; Energy utilization; Petroleum reservoir evaluation; Apportionment problems; Comparative analysis; Cooperative game theory; Cost allocation problem; Energy accounting; Energy apportionment; Fairness properties; Information requirement; Game theory
Contention-aware workload placement for in-memory databases in cloud environments,2016,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029459116&doi=10.1145%2f2961888&partnerID=40&md5=0bfea50102eaa258cd7f6bfb346ca9ee,"Big data processing is driven by new types of in-memory database systems. In this article, we apply performance modeling to efficiently optimize workload placement for such systems. In particular, we propose novel response time approximations for in-memory databases based on fork-join queuing models and contention probabilities to model variable threading levels and per-class memory occupation under analytical workloads. We combine these approximations with a nonlinear optimization methodology that seeks optimal load dispatching probabilities in order to minimize memory swapping and resource utilization. We compare our approach with state-of-the-art response time approximations using real data from an SAP HANA in-memory system and show that our models markedly improve accuracy over existing approaches, at similar computational costs. © 2016 ACM",Fork-join; In-memory database; Response times; Variable threading level,Data handling; Database systems; Electric load dispatching; Nonlinear programming; Queueing theory; Response time (computer systems); Cloud environments; Computational costs; In-memory database; In-memory database systems; Non-linear optimization; Resource utilizations; Response times; Variable threading level; Big data
Detecting sponsored recommendations,2016,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057121210&doi=10.1145%2f2988543&partnerID=40&md5=ae1db09d146a43d0dfd3d674a7df598d,"With the vast number of items, Web pages, and news from which to choose, online services and customers both benefit tremendously from personalized recommender systems. Such systems additionally provide great opportunities for targeted advertisements by displaying ads alongside genuine recommendations. We consider a biased recommendation system in which such ads are displayed without any tags (disguised as genuine recommendations), rendering them indistinguishable to a single user. We ask whether it is possible for a small subset of collaborating users to detect such bias. We propose an algorithm that can detect this type of bias through statistical analysis on the collaborating users' feedback. The algorithm requires only binary information indicating whether a user was satisfied with each of the recommended item or not. This makes the algorithm widely appealing to real-world issues such as identification of search engine bias and pharmaceutical lobbying. We prove that the proposed algorithm detects the bias with high probability for a broad class of recommendation systems when a sufficient number of users provides feedback on a sufficient number of recommendations. We provide extensive simulations with real datasets and practical recommender systems, which confirm the trade-offs in the theoretical guarantees. © 2016 ACM",Anomaly detection; Bias detection; Native advertising; Recommender systems; Sponsored content,Economic and social effects; Recommender systems; Search engines; Websites; Anomaly detection; Binary information; Extensive simulations; High probability; Personalized recommender systems; Sponsored content; Targeted advertisements; Theoretical guarantees; Online systems
File dissemination in dynamic graphs: The case of independent and correlated links in series,2016,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029550902&doi=10.1145%2f2981344&partnerID=40&md5=0539489b8810b5e043c0a9ece78a4238,"In this article, we investigate the traversal time of a file across N communication links subject to stochastic changes in the sending rate of each link. Each link's sending rate is modeled by a finite-state Markov process. Two cases, one where links evolve independently of one another (N mutually independent Markov processes) and the second where their behaviors are dependent (these N Markov processes are not mutually independent), are considered. A particular instance where the above is encountered is ad hoc delay/tolerant networks where links are subject to intermittent unavailability. © 2016 ACM",Dynamic communication path; Laplace-Stietljes transform; Markov process; Pearson correlation coefficient; Stochastic bound; Traversal time,Correlation methods; Stochastic systems; Correlated link; Dynamic communication; File disseminations; Finite state Markov process; Mutually independents; Pearson correlation coefficients; Stochastic bound; Traversal time; Markov processes
A truthful incentive mechanism for emergency demand response in geo-distributed colocation data centers,2016,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062443863&doi=10.1145%2f2950046&partnerID=40&md5=82ae4a80e582d34cd555c40207c28ab2,"Data centers are key participants in demand response programs, including emergency demand response (EDR), in which the grid coordinates consumers of large amounts of electricity for demand reduction in emergency situations to prevent major economic losses. While existing literature concentrates on owner-operated data centers, this work studies EDR in geo-distributed multitenant colocation data centers in which servers are owned and managed by individual tenants. EDR in colocation data centers is significantly more challenging due to lack of incentives to reduce energy consumption by tenants who control their servers and are typically on fixed power contracts with the colocation operator. Consequently, to achieve demand reduction goals set by the EDR program, the operator has to rely on the highly expensive and/or environmentally unfriendly on-site energy backup/generation. To reduce cost and environmental impact, an efficient incentive mechanism is therefore needed, motivating tenants' voluntary energy reduction in the case of EDR. This work proposes a novel incentive mechanism, Truth-DR, which leverages a reverse auction to provide monetary remuneration to tenants according to their agreed energy reduction. Truth-DR is computationally efficient, truthful, and achieves 2-approximation in colocation-wide social cost. Trace-driven simulations verify the efficacy of the proposed auction mechanism. © 2016 ACM 2376-3639/2016/09-ART18 $15.00",Approximation algorithms; Colocation data centers; Emergency demand response; Mechanism design,Approximation algorithms; Emergency services; Energy utilization; Environmental impact; Green computing; Losses; Machine design; Computationally efficient; Data centers; Demand response; Demand response programs; Emergency demand response (EDR); Mechanism design; Reduce energy consumption; Trace driven simulation; Data reduction
False-positive probability and compression optimization for tree-structured bloom filters,2016,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053326152&doi=10.1145%2f2940324&partnerID=40&md5=ad072d0c4e45081d31a14b8e94f52278,"Bloom filters are frequently used to to check the membership of an item in a set. However, Bloom filters face a dilemma: the transmission bandwidth and the accuracy cannot be optimized simultaneously. This dilemma is particularly severe for transmitting Bloom filters to remote nodes when the network bandwidth is limited. We propose a novel Bloom filter called BloomTree that consists of a tree-structured organization of smaller Bloom filters, each using a set of independent hash functions. BloomTree spreads items across levels that are compressed to reduce the transmission bandwidth need. We show how to find optimal configurations for BloomTree and investigate in detail by how much BloomTree outperforms the standard Bloom filter or the compressed Bloom filter. Finally, we use the intersection of BloomTrees to predict the set intersection, decreasing the false-positive probabilities by several orders of magnitude compared to both the compressed Bloom filter and the standard Bloom filter. © 2016 ACM 2376-3639/2016/09-ART19 $15.00",Bloom filter; Compression; Genetic algorithm; Set query; Tree,Bandwidth; Compaction; Forestry; Genetic algorithms; Hash functions; Trees (mathematics); Wave transmission; Bloom filters; Network bandwidth; Orders of magnitude; Set intersection; Set query; Transmission bandwidth; Tree; Tree-structured; Data structures
A control-theoretic analysis of low-priority congestion control reprioritization under AQM,2016,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007003020&doi=10.1145%2f2934652&partnerID=40&md5=bca5e2cd15720a73a8fd2e0a620e58ec,"Recently, a negative interplay has been shown to arise when scheduling/Active Queue Management (AQM) techniques and low-priority congestion control protocols are used together; namely, AQM resets the relative level of priority among congestion control protocols. This work explores this issue by carrying out a control-theoretic analysis of the dynamical system to prove some fundamental properties that fully characterize the reprioritization phenomenon. In particular, (i) we provide the closed-form solution of the equilibrium in the open loop (i.e., fixing a target loss probability p); (ii) we provide a stability analysis and a characterization of the reprioritization phenomenon when closing the loop with AQM (i.e., that dynamically adjusts the system loss probability). Our results are important as the characterization of the reprioritization phenomenon is not only quantitatively accurate for the specific protocols and AQM considered but also qualitatively accurate for a broader range of congestion control protocol and AQM combinations. Finally, while we find a sufficient condition to avoid the reprioritization phenomenon, we also show, at the same time, such conditions to be likely impractical: Therefore, we propose a simple and practical system-level solution that is able to reinstate priorities among protocols. © 2016 ACM 2376-3639/2016/08-ART17 $15.00",AQM; Bufferbloat; Scavenger protocol; Simulation,Dynamical systems; Bufferbloat; Closed form solutions; Congestion control protocols; Fundamental properties; Practical systems; Re-prioritization; Simulation; Theoretic analysis; Scheduling
PEAS: A performance evaluation framework for auto-scaling strategies in cloud applications,2016,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074674305&doi=10.1145%2f2930659&partnerID=40&md5=39dafecaa11d9622b5424d3fcb7da914,"Numerous auto-scaling strategies have been proposed in the past few years for improving various Quality of Service (QoS) indicators of cloud applications, for example, response time and throughput, by adapting the amount of resources assigned to the application to meet the workload demand. However, the evaluation of a proposed auto-scaler is usually achieved through experiments under specific conditions and seldom includes extensive testing to account for uncertainties in the workloads and unexpected behaviors of the system. These tests by no means can provide guarantees about the behavior of the system in general conditions. In this article, we present a Performance Evaluation framework for Auto-Scaling (PEAS) strategies in the presence of uncertainties. The evaluation is formulated as a chance constrained optimization problem, which is solved using scenario theory. The adoption of such a technique allows one to give probabilistic guarantees of the obtainable performance. Six different auto-scaling strategies have been selected from the literature for extensive test evaluation and compared using the proposed framework. We build a discrete event simulator and parameterize it based on real experiments. Using the simulator, each auto-scaler's performance is evaluated using 796 distinct real workload traces from projects hosted on the Wikimedia foundations' servers, and their performance is compared using PEAS. The evaluation is carried out using different performance metrics, highlighting the flexibility of the framework, while providing probabilistic bounds on the evaluation and the performance of the algorithms. Our results highlight the problem of generalizing the conclusions of the original published studies and show that based on the evaluation criteria, a controller can be shown to be better than other controllers. © 2016 ACM 2376-3639/2016/08-ART15 $15.00",Auto-scaling; Cloud computing; Elasticity; Performance evaluation; Randomized optimization,Cloud computing; Computation theory; Constrained optimization; Elasticity; Frequency dividing circuits; Auto-scaling; Chance constrained optimization problems; Discrete-event simulators; Performance evaluation frameworks; Performance evaluations; Probabilistic bounds; Probabilistic guarantees; Randomized optimizations; Quality of service
Analysis of an offloading scheme for data centers in the framework of fog computing,2016,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021374773&doi=10.1145%2f2950047&partnerID=40&md5=4a1f3fc7be07324e6c53748ccc27500c,"In the context of fog computing, we consider a simple case where data centers are installed at the edge of the network and assume that if a request arrives at an overloaded data center, then it is forwarded to a neighboring data center with some probability. Data centers are assumed to have a large number of servers, and traffic at some of them is assumed to cause saturation. In this case, the other data centers may help to cope with this saturation regime by accepting some of the rejected requests. Our aim is to qualitatively estimate the gain achieved via cooperation between neighboring data centers. After proving some convergence results related to the scaling limits of loss systems for the process describing the number of free servers at both data centers, we show that the performance of the system can be expressed in terms of the invariant distribution of a random walk in the quarter plane. By using and developing existing results in the technical literature, explicit formulas for the blocking rates of such a system are derived. © 2016 ACM 2376-3639/2016/09-ART16 $15.00",Functional equations; Markov processes; Queueing theory; Random walks,Computation theory; Markov processes; Queueing theory; Convergence results; Explicit formula; Functional equation; Invariant distribution; Random Walk; Saturation regime; Scaling limits; Technical literature; Fog computing
Scheduling storms and streams in the cloud,2016,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009427182&doi=10.1145%2f2904080&partnerID=40&md5=22f7469c865dd5210851d037105165d9,"Motivated by emerging big streaming data processing paradigms (e.g., Twitter Storm, Streaming MapReduce), we investigate the problem of scheduling graphs over a large cluster of servers. Each graph is a job, where nodes represent compute tasks and edges indicate data flows between these compute tasks. Jobs (graphs) arrive randomly over time and, upon completion, leave the system. When a job arrives, the scheduler needs to partition the graph and distribute it over the servers to satisfy load balancing and cost considerations. Specifically, neighboring compute tasks in the graph that are mapped to different servers incur load on the network; thus a mapping of the jobs among the servers incurs a cost that is proportional to the number of “broken edges.” We propose a low-complexity randomized scheduling algorithm that, without service preemptions, stabilizes the system with graph arrivals/departures; more importantly, it allows a smooth tradeoff between minimizing average partitioning cost and average queue lengths. Interestingly, to avoid service preemptions, our approach does not rely on a Gibbs sampler; instead, we show that the corresponding limiting invariant measure has an interpretation stemming from a loss system. © 2016 ACM 2376-3639/2016/08-ART14 $15.00",Dynamic resource allocation; Graph partitioning; Markov chains; Stability,Computational complexity; Convergence of numerical methods; Data flow analysis; Data handling; Flow graphs; Markov processes; Scheduling; Scheduling algorithms; Storms; Average queue lengths; Dynamic resource allocations; Gibbs samplers; Graph Partitioning; Invariant measure; Large clusters; Randomized scheduling algorithm; Streaming data processing; Graph theory
A unified approach to the performance analysis of caching systems,2016,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074675553&doi=10.1145%2f2896380&partnerID=40&md5=5007b772db46dd9a7cfbb140cd8df404,"We propose a unified methodology to analyze the performance of caches (both isolated and interconnected), by extending and generalizing a decoupling technique originally known as Che's approximation, which provides very accurate results at low computational cost. We consider several caching policies (including a very attractive one, called k-LRU), taking into account the effects of temporal locality. In the case of interconnected caches, our approach allows us to do better than the Poisson approximation commonly adopted in prior work. Our results, validated against simulations and trace-driven experiments, provide interesting insights into the performance of caching systems. © 2016 ACM 2376-3639/2016/05-ART12 $15.00",Caching; Content delivery networks; Information-centric networking,Computer network performance evaluation; Computer programming; Computer science; Hardware; Caching; Content delivery network; Decoupling technique; Information-centric networkings; Inter-connected caches; Performance analysis; Poisson approximations; Trace driven experiments; Distributed computer systems
Design and analysis of incentive and reputation mechanisms for online crowdsourcing systems,2016,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034216544&doi=10.1145%2f2897510&partnerID=40&md5=e7fce91728f0a1cb3cdfdf60a8b6393a,"Today, online crowdsourcing services like Amazon Mechanical Turk, UpWork, and Yahoo! Answers are gaining in popularity. For such online services, it is important to attract “workers” to provide high-quality solutions to the “tasks” outsourced by “requesters.” The challenge is that workers have different skill sets and can provide different amounts of effort. In this article, we design a class of incentive and reputation mechanisms to solicit high-quality solutions from workers. Our incentive mechanism allows multiple workers to solve a task, splits the reward among workers based on requester evaluations of the solution quality, and guarantees that high-skilled workers provide high-quality solutions. However, our incentive mechanism suffers the potential risk that a requester will eventually collects low-quality solutions due to fundamental limitations in task assigning accuracy. Our reputation mechanism ensures that low-skilled workers do not provide low-quality solutions by tracking workers' historical contributions and penalizing those workers having poor reputations. We show that by coupling our reputation mechanism with our incentive mechanism, a requester can collect at least one high-quality solution. We present an optimization framework to select parameters for our reputation mechanism. We show that there is a trade-off between system efficiency (i.e., the number of tasks that can be solved for a given reward) and revenue (i.e., the amount of transaction fees), and we present the optimal trade-off curve between system efficiency and revenue. We demonstrate the applicability and effectiveness of our mechanisms through experiments using a real-world dataset from UpWork. We infer model parameters from this data, use them to determine proper rewards, and select the parameters of our incentive and reputation mechanisms for UpWork. Experimental results show that our incentive and reputation mechanisms achieve 98.82% of the maximum system efficiency while only sacrificing 4% of revenue. © 2016 ACM 2376-3639/2016/05-ART13 $15.00",Bayesian game; Equilibrium; Repeated game,Crowdsourcing; Economic and social effects; Efficiency; Online systems; Phase equilibria; Amazon mechanical turks; Bayesian game; Fundamental limitations; High-quality solutions; Incentive and reputations; Optimization framework; Repeated games; Reputation mechanism; Quality control
Estimation of flow distributions from sampled traffic,2016,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038585903&doi=10.1145%2f2891106&partnerID=40&md5=eb0ce550a46d7c88235dd6d04afcfb9d,"This work addresses the problem of estimating the distributions of packet flow sizes and durations under several methods of sampling packets. Two approaches, one based on inversion and the other on asymptotics, are considered. For the duration distribution, in particular, both approaches require modeling the structure of flows, with the duration distribution being characterized in terms of the IATs (interarrival times between packets) and size distributions of a flow. The inversion of the flow IAT distribution from sampled flow quantities, along with the inversion of the flow size distribution (already used in the literature) allows estimating the flow duration distribution. Motivated by the limitations of the inversion approach in estimating the distribution tails for some sampling methods, an asymptotic approach is developed to estimate directly the distribution tails of flow durations and sizes from sampled quantities. The adequacy of both approaches to estimate the flow distributions is checked against two real Internet traces. © 2016 ACM 2376-3639/2016/05-ART11 $15.00",Asymptotic analysis; Estimation; Flow size and duration distributions; Inversion problem; Measuring and monitoring internet traffic; Sampling packets,Asymptotic analysis; Estimation; Flow distribution; Flow quantities; Flow sizes; Inter-arrival time; Internet traces; Internet traffic; Inversion problems; Sampling method; Size distribution
Prefigure: An analytic framework for HDD management,2016,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074675508&doi=10.1145%2f2872331&partnerID=40&md5=bc1974a1e6c07f89458377ce49e152de,"Low disk drive utilization suggests that placing the drive into a power saving mode during idle times may decrease power consumption. We present PREFiguRE, a robust framework that aims at harvesting future idle intervals for power savings while meeting strict quality constraints: first, it contains potential delays in serving IO requests that occur during power savings since the time to bring up the disk is not negligible, and second, it ensures that the power saving mechanism is triggered a few times only, such that the disk wear-out due to powering up and down does not compromise the disk's lifetime. PREFiguRE is based on an analytic methodology that uses the histogram of idle times to determine schedules for power saving modes as a function of the preceding constraints. PREFiguRE facilitates analysis for the evaluation of the trade-offs between power savings and quality targets for the current workload. Extensive experimentation on a set of enterprise storage traces illustrates PREFiguRE's effectiveness to consistently achieve high power savings without undermining disk reliability and performance. © 2016 ACM 2376-3639/2016/05-ART10 $15.00",Disk drives; Histograms; Performance modeling; Power savings; Reliability; Scheduling,Economic and social effects; Graphic methods; Hard disk storage; Mobile telecommunication systems; Reliability; Scheduling; Disk drive; Disk reliabilities; Histograms; Performance Model; Power saving mechanism; Power saving modes; Power savings; Quality constraints; Quality control
Performance evaluation of dynamic page allocation strategies in SSDs,2016,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055871946&doi=10.1145%2f2829974&partnerID=40&md5=19afe42568bff587ee0c50631fddbb31,"Solid-state drives (SSDs) with tens of NAND flash chips and highly parallel architectures are widely used in enterprise and client storage systems. As any write operation in NAND flash is preceded by a slow erase operation, an out-of-place update mechanism is used to distribute writes through SSD storage space to postpone erase operations as far as possible. SSD controllers use a mapping table along with a specific allocation strategy to map logical host addresses to physical page addresses within storage space. The allocation strategy is further responsible for accelerating I/O operations through better striping of physical addresses over SSD parallel resources. Proposals already exist for using static logical-to-physical address mapping that does not balance the I/O traffic load within the SSD, and its efficiency highly depends on access patterns. A more balanced distribution of I/O operations is to alternate resource allocation in a round-robin manner irrespective of logical addresses. The number of resources that can be dynamically allocated in this fashion is defined as the degree of freedom, and to the best of our knowledge, there has been no research thus far to show what happens if different degrees of freedom are used in allocation strategy. This article explores the possibility of using dynamic resource allocation and identifies key design opportunities that it presents to improve SSD performance. Specifically, using steady-state analysis of SSDs, we show that dynamism helps to mitigate performance and endurance overheads of garbage collection. Our steady-state experiments indicate that midrange/high-end SSDs with dynamic allocation can provide I/O operations per second (IOPS) improvement of up to 3.3x/9.6x, response time improvement of up to 56%/32%, and about 88%/96% average reduction in the standard deviation of erase counts of NAND flash blocks. © 2016 ACM.",Dynamic page allocation; Garbage collection; Performance evaluation; Solid-state drive,Degrees of freedom (mechanics); Dynamics; Mapping; NAND circuits; Parallel architectures; Physical addresses; Program translators; Refuse collection; Resource allocation; Routers; Allocation strategy; Dynamic allocations; Dynamic pages; Dynamic resource allocations; Garbage collection; Performance evaluations; Solid state drives; Steady-state analysis; Flash-based SSDs
Dealing with dead ends: Efficient routing in darknets,2016,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074675733&doi=10.1145%2f2809779&partnerID=40&md5=ff3eae71cddd44927e3502c4fa366e75,"Darknets,membership-concealing peer-to-peer networks, suffer from highmessage delivery delays due to insufficient routing strategies. They form topologies restricted to a subgraph of the social network of their users by limiting connections to peers with a mutual trust relationship in real life. Whereas centralized, highly successful social networking services entail a privacy loss of their users, Darknets at higher performance represent an optimal private and censorship-resistant communication substrate for social applications. Decentralized routing so far has been analyzed under the assumption that the network resembles a perfect lattice structure. Freenet, currently the only widely used Darknet, attempts to approximate this structure by embedding the social graph into a metric space. Considering the resulting distortion, the common greedy routing algorithm is adapted to account for local optima. Yet the impact of the adaptation has not been adequately analyzed. We thus suggest a model integrating inaccuracies in the embedding. In the context of this model, we show that the Freenet routing algorithm cannot achieve polylog performance. Consequently, we design NextBestOnce, a provable poylog algorithm based only on information about neighbors. Furthermore, we show that the routing length of NextBestOnce is further decreased by more than a constant factor if neighborof- neighbor information is included in the decision process. © 2016 ACM.",Censorship resilience; Darknets; Routing algorithm,Distributed computer systems; Peer to peer networks; Routing algorithms; Social networking (online); Topology; Censorship resilience; Censorship resistants; Darknets; Decentralized routing; Efficient routing; Routing strategies; Social applications; Social networking services; Network routing
Introduction,2016,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074674539&doi=10.1145%2f2893179&partnerID=40&md5=2bd7f0dd681e67bc2b0573fa2cabff0d,[No abstract available],,
Validating the simulation of large-scale parallel applications using statistical characteristics,2016,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063109024&doi=10.1145%2f2809778&partnerID=40&md5=840583a460f041c0e750ea0324182014,"Simulation is a widely adopted method to analyze and predict the performance of large-scale parallel applications. Validating the hardware model is highly important for complex simulations with a large number of parameters. Common practice involves calculating the percent error between the projected and the real execution time of a benchmark program. However, in a high-dimensional parameter space, this coarse-grained approach often suffers from parameter insensitivity, which may not be known a priori. Moreover, the traditional approach cannot be applied to the validation of software models, such as application skeletons used in online simulations. In this work, we present a methodology and a toolset for validating both hardware and software models by quantitatively comparing fine-grained statistical characteristics obtained from execution traces. Although statistical information has been used in tasks like performance optimization, this is the first attempt to apply it to simulation validation. Our experimental results show that the proposed evaluation approach offers significant improvement in fidelity when compared to evaluation using total execution time, and the proposed metrics serve as reliable criteria that progress toward automating the simulation tuning process. © 2016 ACM.",Evaluation metrics; Simulation evaluation; Software skeleton,Hardware; Musculoskeletal system; Coarse-grained approaches; Evaluation metrics; Hardware and software; Performance optimizations; Simulation evaluation; Statistical characteristics; Statistical information; Traditional approaches; Application programs
Sojourn time approximations for a discriminatory processor sharing queue,2016,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074675440&doi=10.1145%2f2812807&partnerID=40&md5=f947c5af70181dc67e1bc4a3ba080b21,"We study a multiclass time-sharing discipline with relative priorities known as discriminatory processor sharing (DPS), which provides a natural framework to model service differentiation in systems. The analysis of DPS is extremely challenging, and analytical results are scarce. We develop closed-form approximations for the mean conditional (on the service requirement) and unconditional sojourn times. The main benefits of the approximations lie in its simplicity, the fact that it applies for general service requirements with finite second moments, and that it provides insights into the dependency of the performance on the system parameters. We show that the approximation for the mean conditional and unconditional sojourn time of a customer is decreasing as its relative priority increases. We also show that the approximation is exact in various scenarios, and that it is uniformly bounded in the second moments of the service requirements. Finally, we numerically illustrate that the approximation for exponential, hyperexponential, and Pareto service requirements is accurate across a broad range of parameters. © 2016 ACM.",Discriminatory processor sharing; Heavy traffic; Interpolation; Light traffic; Sojourn time,Discriminators; Interpolation; Pareto principle; Closed form approximations; Discriminatory processor-sharing; Heavy traffics; Light traffic; Relative priorities; Service differentiation; Service requirements; Sojourn time; Time sharing systems
Energy-performance trade-offs via the EP queue,2016,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061761168&doi=10.1145%2f2818726&partnerID=40&md5=9e8b685395a86feb5d672a29b2733921,"We introduce the EP queue - a significant generalization of the MB/G/1 queue that has state-dependent service time probability distributions and incorporates power-up for first arrivals and power-down for idle periods. We derive exact results for the busy-time and response-time distributions. From these, we derive power consumption metrics during nonidle periods and overall response time metrics, which together provide a single measure of the trade-off between energy and performance. We illustrate these trade-offs for some policies and show how numerical results can provide insights into system behavior. The EP queue has application to storage systems, especially hard disks, and other data-center components such as compute servers, networking, and even hyperconverged infrastructure. © 2016 ACM.",Energy-performance trade-off; Modified M/G/1 queue; Power management; Response timess; Storage systems,Digital storage; Energy efficiency; Power management; Probability distributions; Queueing theory; Response time (computer systems); Energy performance; M/G/1 Queue; Numerical results; Response time distribution; Response timess; State dependent service; Storage systems; System behaviors; Economic and social effects
Estimating the transmission probability in wireless networks with configuration models,2016,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047864190&doi=10.1145%2f2858795&partnerID=40&md5=2b822bcc98259677643e8f50a4359e8d,"We propose a new methodology to estimate the probability of successful transmissions for random access scheduling in wireless networks, in particular those using Carrier Sense Multiple Access (CSMA). Instead of focusing on spatial configurations of users, we model the interference between users as a random graph. Using configurationmodels for random graphs, we show how the properties of the medium access mechanism are captured by some deterministic differential equations when the size of the graph gets large. Performance indicators such as the probability of connection of a given node can then be efficiently computed from these equations. We also perform simulations to illustrate the results on different types of random graphs. Even on spatial structures, these estimates get very accurate as soon as the variance of the interference is not negligible. © 2016 ACM.",Medium access probability; Parking process; Random graphs; Wireless networks,Carrier communication; Carrier sense multiple access; Differential equations; Graph theory; Probability; Carrier sense multiple access (CSMA); Configuration model; Medium access probabilities; Performance indicators; Random graphs; Spatial configuration; Spatial structure; Transmission probabilities; Wireless networks
Coding rate analysis of forbidden overlap codes in high-speed buses,2016,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015450330&doi=10.1145%2f2846091&partnerID=40&md5=6b29875365163cd5e4a4a277951f77c1,"One of the main problems in deep submicron designs of high-speed buses is propagation delay due to the crosstalk effect. To alleviate the crosstalk effect, there are several types of crosstalk avoidance codes proposed in the literature. In this article, we analyze the coding rates of forbidden overlap codes (FOCs) that avoid ""010 ? 101"" transition and ""101 ? 010"" transition on any three adjacent wires in a bus. We first compute the maximum achievable coding rate of FOCs and the maximum coding rate of memoryless FOCs. Our numerical results show that there is a significant gap between the maximum coding rate of memoryless FOCs and the maximum achievable rate. We then analyze the coding rates of FOCs generated from the bit-stuffing algorithm. Our worst-case analysis yields a tight lower bound of the coding rate of the bit-stuffing algorithm. Under the assumption of Bernoulli inputs, we use a Markov chain model to compute the coding rate of a bus with n wires under the bit-stuffing algorithm. The main difficulty of solving such a Markov chain model is that the number of states grows exponentially with respect to the number of wires n. To tackle the problem of the curse of dimensionality, we derive an approximate analysis that leads to a recursive closed-form formula for the coding rate over the nth wire. Our approximations match extremely well with the numerical results from solving the original Markov chain for n ≤ 10 and the simulation results for n = 3000. Our analysis of coding rates of FOCs could be helpful in understanding the trade-off between propagation delay and coding rate among various crosstalk avoidance codes in the literature. In comparison with the forbidden transition codes (FTCs) that have shorter propagation delay than that of FOCs, our numerical results show that the coding rates of FOCs are much higher than those of FTCs. © 2016 ACM.",Bit-stuffing algorithm; Bus encoding; Maximum achievable rate,Buses; Chains; Crosstalk; Economic and social effects; Markov processes; Wire; Approximate analysis; Bit stuffing algorithm; Bus encoding; Closed-form formulae; Curse of dimensionality; Deep sub-micron designs; Forbidden transitions; Worst-case analysis; Codes (symbols)
Employing software-managed caches in OpenACC: Opportunities and benefits,2016,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968914834&doi=10.1145%2f2798724&partnerID=40&md5=e52d12302676442772b0c6af1f9b04cc,"The OpenACC programming model has been developed to simplify accelerator programming and improve development productivity. In this article, we investigate the main limitations faced by OpenACC in harnessing all capabilities of GPU-like accelerators. We build on our findings and discuss the opportunity to exploit a software-managed cache as (i) a fast communication medium and (ii) a cache for data reuse. To this end, we propose a new directive and communication model for OpenACC. Investigating several benchmarks, we show that the proposed directive can improve performance up to 2.54×, and at the cost of minor programming effort. © 2016 ACM.",Accelerator; CUDA; OpenACC; Software-managed cache,Benchmarking; Particle accelerators; Accelerator programming; Communication medium; Communication modeling; CUDA; Development productivity; Improve performance; Openacc; Programming models; Computer software reusability
Queuing Network Models of Multiservice RANs,2024,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191156669&doi=10.1145%2f3649307&partnerID=40&md5=61c602ea656ffd57e9d8d8dd4dd9a758,"In this article, we present a new queuing network model for the analysis of a portion of a radio access network (RAN) comprising macro cell base stations (BSs) and small cell BSs offering ""streaming""and ""elastic""services. Streaming services require a certain data rate for a random time. The required data rates depend on the type of service (e.g., audio and video). Elastic services require the transfer of random data volumes, and their data rate adjusts dynamically based on the capacity not utilized by the streaming services. To derive performance measures for the proposed model, we develop a computationally efficient framework that exploits a new product form result for streaming services, relying on a well-known blocking policy, and an approximate product form for elastic services. Insensitivity to the distribution of service requirements holds in the case of negligible end user mobility.We show the high accuracy of our model in predicting the performance of practical system configurations by conducting a thorough comparison between the model's results and those obtained from a detailed discrete-event simulator. Through this analysis, we uncover significant counter-intuitive behaviors that arise from the competition between streaming services with diverse demands, and that are effectively captured and predicted by our modeling approach.Our computationally efficient queuing model is a useful new tool to support design and planning of multiservice RANs whose complex structures result from the coexistence of BSs of different generations in dense areas.  © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesRadio access network; insensitivity; performance evaluation; product form; queuing network model; streaming and elastic services,Computational efficiency; Discrete event simulation; Queueing networks; Queueing theory; Radio access networks; Access network; Additional key word and phrasesradio access network; Data-rate; Elastic services; Insensitivity; Key words; Performances evaluation; Product forms; Queuing network model; Streaming service; Data transfer
Load Balancing with Job-Size Testing: Performance Improvement or Degradation?,2024,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191191650&doi=10.1145%2f3651154&partnerID=40&md5=6d9980f715820115232aefeeb5706c89,"In the context of decision making under explorable uncertainty, scheduling with testing is a powerful technique used in the management of computer systems to improve performance via better job-dispatching decisions. Upon job arrival, a scheduler may run some testing algorithm against the job to extract some information about its structure, e.g., its size, and properly classify it. The acquisition of such knowledge comes with a cost because the testing algorithm delays the dispatching decisions, though this is under control. In this article, we analyze the impact of such extra cost in a load balancing setting by investigating the following questions: does it really pay off to test jobs? If so, under which conditions? Under mild assumptions connecting the information extracted by the testing algorithm in relationship with its running time, we show that whether scheduling with testing brings a performance degradation or improvement strongly depends on the traffic conditions, system size and the coefficient of variation of job sizes. Thus, the general answer to the above questions is non-trivial and some care should be considered when deploying a testing policy. Our results are achieved by proposing a load balancing model for scheduling with testing that we analyze in two limiting regimes. When the number of servers grows to infinity in proportion to the network demand, we show that job-size testing actually degrades performance unless short jobs can be predicted reliably almost instantaneously and the network load is sufficiently high. When the coefficient of variation of job sizes grows to infinity, we construct testing policies inducing an arbitrarily large performance gain with respect to running jobs untested.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesLoad balancing; dispatching policies; explorable uncertainty; scheduling with testing; size-based routing,Classification (of information); Load testing; Additional key word and phrasesload balancing; Dispatching policy; Explorable uncertainty; Job size; Key words; Routings; Scheduling with testing; Size-based routing; Testing algorithm; Uncertainty; Decision making
A Product-form Network for Systems with Job Stealing Policies,2024,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191164656&doi=10.1145%2f3643845&partnerID=40&md5=5da8a4a1ce45a3ebb7efd197415ea5bd,"In queueing networks, product-form solutions are of fundamental importance to efficiently compute performance metrics in complex models of computer systems. The product-form property entails that the steady-state probabilities of the joint stochastic process underlying the network can be expressed as the normalized product of functions that only depend on the local state of the components. In many relevant cases, product-forms are the only way to perform exact quantitative analyses of large systems.In this work, we introduce a novel class of product-form queueing networks where servers are always busy. Applications include model of systems where successive refinements on jobs improve the processes quality but are not strictly required to obtain a result. To this aim, we define a job movement policy that admits instantaneous migrations of jobs from non-empty waiting buffers to empty ones. Thus, the resulting routing scheme is state-dependent. This class of networks maximizes the system throughput.This model can be implemented with arbitrary topology, including feedback, and both in an open and closed setting. As far as closed systems are concerned, we give a convolution algorithm and the corresponding mean value analysis to compute expected performance indices for closed models.  © 2024 Copyright held by the owner/author(s).",Additional Key Words and PhrasesQueueing theory; fetching policy; product-form solutions; reversed compound agent theorem,Random processes; Stochastic systems; Topology; Additional key word and phrasesqueueing theory; Compound agents; Fetching policy; Key words; Network product; Performance metrices; Product forms; Product-form networks; Product-form solutions; Reversed compound agent theorem; Queueing networks
VM Matters: A Comparison of WASM VMs and EVMs in the Performance of Blockchain Smart Contracts,2024,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191201393&doi=10.1145%2f3641103&partnerID=40&md5=e650c5472256f0ff535294056b8ad762,"Beyond an emerging popular web applications runtime supported in almost all commodity browsers, WebAssembly (WASM) is further regarded to be the next-generation execution environment for blockchain-based applications. Indeed, many popular blockchain platforms such as EOSIO and NEAR have adopted WASM-based execution engines. Most recently, WASM has been favored by Ethereum, the largest smart contract platform, to replace the state-of-the-art EVM. However, whether and how well current WASM outperforms EVM on blockchain clients is still unknown. This article conducts the first measurement study to understand the performance on WASM VMs and EVM for executing smart contracts for blockchain-based applications. To our surprise, the current WASM VM does not provide expected satisfactory performance. The overhead introduced by WASM is really non-trivial. Our results shed the light on challenges when deploying WASM in practice, and provide insightful implications for improvement space.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesWebAssembly; and smart contract; blockchain,Blockchain; 'current; Additional key word and phraseswebassembly; And smart contract; Block-chain; Execution environments; Key words; Performance; Runtimes; WEB application; Web applications; Smart contract
Configuring and Coordinating End-to-end QoS for Emerging Storage Infrastructure,2024,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183328396&doi=10.1145%2f3631606&partnerID=40&md5=5958c74fe31827af1531c254cffff834,"Modern data center storage systems are invariably networked to allow for consolidation and flexible management of storage. They also include high-performance storage devices based on flash or other emerging technologies, generally accessed through low-latency and high-throughput protocols such as Non-volatile Memory Express (NVMe) (or its derivatives) carried over the network. With the increasing complexity and data-centric nature of the applications, properly configuring the quality of service (QoS) for the storage path has become crucial for ensuring the desired application performance. Such QoS is substantially influenced by the QoS in the network path, in the access protocol, and in the storage device. In this article, we define a new transport-level QoS mechanism for the network segment and demonstrate how it can augment and coordinate with the access-level QoS mechanism defined for NVMe, and a similar QoS mechanism configured in the device. We show that the transport QoS mechanism not only provides the desired QoS to different classes of storage accesses but is also able to protect the access to the shared persistent memory devices located along with the storage but requiring much lower latency than storage. We demonstrate that a proper coordinated configuration of the three QoS's on the path is crucial to achieve the desired differentiation, depending on where the bottlenecks appear.  © 2024 Copyright held by the owner/author(s).",data center TCP; latency; NVMe; persistent memory; QoS; RDMA,Flash memory; Information management; Transmission control protocol; Virtual storage; Data center TCP; Datacenter; Latency; Low latency; Non-volatile memory; Non-volatile memory express; Persistent memory; Quality-of-service; RDMA; Service mechanism; Quality of service
Approximation Method for a Non-preemptive Multiserver Queue with Quasi-Poisson Arrivals,2023,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183321301&doi=10.1145%2f3624474&partnerID=40&md5=3aeec974cbe098900178762d81905505,"We consider a non-preemptive multiserver queue with multiple priority classes. We assume distinct exponentially distributed service times and separate quasi-Poisson arrival processes with a predefined maximum number of requests that can be present in the system for each class. We present an approximation method to obtain the steady-state probabilities for the number of requests of each class in our system. In our method, the priority levels (classes) are solved ""nearly separately,""linked only by certain conditional probabilities determined approximately from the solution of other priority levels. Several numerical examples illustrate the accuracy of our approximate solution. The proposed approach significantly reduces the complexity of the problem while featuring generally good accuracy.  © 2023 Copyright held by the owner/author(s).",approximate solution; multiserver; non preemptive; Queueing; reduced complexity,Queueing theory; Stochastic systems; Approximate solution; Approximation methods; Distributed service; Multi-server queue; Multiservers; Non-preemptive; Priority class; Priority levels; Queueing; Reduced-complexity; Approximation theory
Online Partial Service Hosting at the Edge,2023,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183330696&doi=10.1145%2f3616866&partnerID=40&md5=a7a9c3071cfb097d982734e23761cdd1,"We consider the problem of service hosting where an application provider can dynamically rent edge computing resources and serve user requests from the edge to deliver a better quality of service. A key novelty of this work is that we allow the service to be hosted partially at the edge that enables a fraction of the user query to be served by the edge. We model the total cost for (partially) hosting a service at the edge as a combination of the latency in serving requests, the bandwidth consumption, and the time-varying cost for renting edge resources. We propose an online policy called α-RetroRenting (α-RR) that dynamically determines the fraction of the service to be hosted at the edge in any time-slot, based on the history of the request arrivals and the rent cost sequence. As our main result, we derive an upper bound on α-RR's competitive ratio with respect to the offline optimal policy that knows the entire request arrival and rent cost sequence in advance. In addition, we provide performance guarantees for our policy in the setting where the request arrival process is stochastic. We conduct extensive numerical evaluations to compare the performance of α-RR with various benchmarks for synthetic and trace-based request arrival and rent cost processes and find several parameter regimes where α-RR's ability to store the service partially greatly improves cost-efficiency.  © 2023 Copyright held by the owner/author(s).",competitive ratio; edge computing; online policies; Partial service hosting; task offloading,Benchmarking; Quality of service; Stochastic systems; Application providers; Competitive ratio; Computing resource; Edge computing; Online policy; Partial service hosting; Quality-of-service; Service hosting; Task offloading; User query; Edge computing
From Compositional Petri Net Modeling to Macro and Micro Simulation by Means of Stochastic Simulation and Agent-Based Models,2023,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183316055&doi=10.1145%2f3617681&partnerID=40&md5=88c67beed386844485f17e145435ec66,"Computational modeling has become a widespread approach for studying real-world phenomena by using different modeling perspectives, in particular, the microscopic point of view concentrates on the behavior of the single components and their interactions from which the global system evolution emerges, while the macroscopic point of view represents the system's overall behavior abstracting as much as possible from that of the single components. The preferred point of view depends on the effort required to develop the model, on the detail level of the available information about the system to be modeled, and on the type of measures that are of interest to the modeler each point of view may lead to a different modeling language and simulation paradigm. An approach adequate for the microscopic point of view is Agent-Based Modeling and Simulation, which has gained popularity in the last few decades but lacks a formal definition common to the different tools supporting it. This may lead to modeling mistakes and wrong interpretation of the results, especially when comparing models of the same system developed according to different points of view. The aim of the work described in this paper is to provide a common compositional modeling language from which both a macro and a micro simulation model can be automatically derived: these models are coherent by construction and may be studied through different simulation approaches and tools. A framework is thus proposed in which a model can be composed using a Petri Net formalism and then studied through both an Agent-Based Simulation and a classical Stochastic Simulation Algorithm, depending on the study goal. © 2023 Association for Computing Machinery. All rights reserved.",Agent-Based Models; Stochastic Simulation Algorithm; Stochastic symmetric nets,Autonomous agents; Computational methods; Petri nets; Stochastic models; Stochastic systems; Agent-based model; Microsimulation; Petri net models; Simulation-based models; Single components; Stochastic simulation algorithms; Stochastic simulations; Stochastic symmetric net; Stochastics; Symmetric nets; Modeling languages
No-regret Caching via Online Mirror Descent,2023,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168807462&doi=10.1145%2f3605209&partnerID=40&md5=d93ab90cb1d022ff12275f80bd3cab52,"We study an online caching problem in which requests can be served by a local cache to avoid retrieval costs from a remote server. The cache can update its state after a batch of requests and store an arbitrarily small fraction of each file. We study no-regret algorithms based on Online Mirror Descent (OMD) strategies. We show that bounds for the regret crucially depend on the diversity of the request process, provided by the diversity ratio R/h, where R is the size of the batch and h is the maximum multiplicity of a request in a given batch. We characterize the optimality of OMD caching policies w.r.t. regret under different diversity regimes. We also prove that, when the cache must store the entire file, rather than a fraction, OMD strategies can be coupled with a randomized rounding scheme that preserves regret guarantees, even when update costs cannot be neglected. We provide a formal characterization of the rounding problem through optimal transport theory, and moreover we propose a computationally efficient randomized rounding scheme.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesRandomized algorithms; adversarial analysis; gradient methods,Computation theory; Gradient methods; Statistical mechanics; Additional key word and phrasesrandomized algorithm; Adversarial analyse; Gradient's methods; Key words; Local cache; Maximum multiplicity; Optimality; Randomized rounding; Remote servers; Rounding schemes; Mirrors
Optimal Pricing in a Single Server System,2023,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168801875&doi=10.1145%2f3607252&partnerID=40&md5=0710af8e587eeb5ce76a149505dd8ff6,"We study optimal pricing in a single server queue when the customers valuation of service depends on their waiting time. In particular, we consider a very general model, where the customer valuations are random and are sampled from a distribution that depends on the queue length. The goal of the service provider is to set dynamic state dependent prices in order to maximize its revenue, while also managing congestion. We model the problem as a Markov decision process and present structural results on the optimal policy. We also present an algorithm to find an approximate optimal policy. We further present a myopic policy that is easy to evaluate and present bounds on its performance. We finally illustrate the quality of our approximate solution and the myopic solution using numerical simulations.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSingle server queue; Markov decision process; pricing; service valuation,Economics; Markov processes; Queueing theory; Additional key word and phrasessingle server queue; Customer valuation; Key words; Markov Decision Processes; Optimal policies; Optimal pricing; Server queue; Server system; Service valuation; Single server; Costs
Program Analysis and Machine Learning-based Approach to Predict Power Consumption of CUDA Kernel,2023,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168804979&doi=10.1145%2f3603533&partnerID=40&md5=32da5813633b30c4515daf6e3f30c418,"The General Purpose Graphics Processing Unit has secured a prominent position in the High-Performance Computing world due to its performance gain and programmability. Understanding the relationship between Graphics Processing Unit (GPU) power consumption and program features can aid developers in building energy-efficient sustainable applications. In this work, we propose a static analysis-based power model built using machine learning techniques. We have investigated six machine learning models across three NVIDIA GPU architectures: Kepler, Maxwell, and Volta with Random Forest, Extra Trees, Gradient Boosting, CatBoost, and XGBoost reporting favorable results. We observed that the XGBoost technique-based prediction model is the most efficient technique with an R2 value of 0.9646 on Volta Architecture. The dataset used for these techniques includes kernels from different benchmarks suits, sizes, nature (e.g., compute-bound, memory-bound), and complexity (e.g., control divergence, memory access patterns). Experimental results suggest that the proposed solution can help developers precisely predict GPU applications power consumption using program analysis across GPU architectures. Developers can use this approach to refactor their code to build energy-efficient GPU applications.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesGPGPU computing; CatBoost; CUDA; static analysis; sustainable computing; XGBoost,Adaptive boosting; Application programs; Computer graphics; Computing power; Electric power utilization; Energy efficiency; Forecasting; Machine learning; Memory architecture; Program processors; Static analysis; Additional key word and phrasesgpgpu computing; Catboost; CUDA; Energy efficient; Key words; Learning-based approach; Machine-learning; Program analysis; Sustainable computing; Xgboost; Graphics processing unit
Efficient Computation of Optimal Thresholds in Cloud Auto-scaling Systems,2023,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168805565&doi=10.1145%2f3603532&partnerID=40&md5=735ed28edcc7029aecbfa0d0f54ce0f8,"We consider a horizontal and dynamic auto-scaling technique in a cloud system where virtual machines hosted on a physical node are turned on and off to minimise energy consumption while meeting performance requirements. Finding cloud management policies that adapt the system to the load is not straightforward, and we consider here that virtual machines are turned on and off depending on queue load thresholds. We want to compute the optimal threshold values that minimize consumption costs and penalty costs (when performance requirements are not met). To solve this problem, we propose several optimisation methods, based on two different mathematical approaches. The first one is based on queueing theory and uses local search heuristics coupled with the stationary distributions of Markov chains. The second approach tackles the problem using Markov Decision Process (MDP) in which we assume that the policy is of a special multi-threshold type called hysteresis. We improve the heuristics of the former approach with the aggregation of Markov chains and queues approximation techniques. We assess the benefit of threshold-aware algorithms for solving MDPs. Then we carry out theoretical analyzes of the two approaches. We also compare them numerically and we show that all of the presented MDP algorithms strongly outperform the local search heuristics. Finally, we propose a cost model for a real scenario of a cloud system to apply our optimisation algorithms and to show their practical relevance. The major scientific contribution of the article is a set of fast (almost in real time) load-based threshold computation methods that can be used by a cloud provider to optimize its financial costs.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesDecision processes; cloud system; heuristics; hysteresis queues; Markov processes,Computation theory; Energy utilization; Heuristic algorithms; Hysteresis; Local search (optimization); Markov processes; Network security; Queueing theory; Additional key word and phrasesdecision process; Cloud systems; Efficient computation; Heuristic; Hysteresis queue; Key words; Local search heuristics; Markov Decision Processes; Optimal threshold; Performance requirements; Virtual machine
Load-optimization in Reconfigurable Data-center Networks: Algorithms and Complexity of Flow Routing,2023,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168764467&doi=10.1145%2f3597200&partnerID=40&md5=9ac289817dbe8f71b091d36236388fb4,"Emerging reconfigurable data centers introduce unprecedented flexibility in how the physical layer can be programmed to adapt to current traffic demands. These reconfigurable topologies are commonly hybrid, consisting of static and reconfigurable links, enabled by e.g., an Optical Circuit Switch (OCS) connected to top-of-rack switches in Clos networks. Even though prior work has showcased the practical benefits of hybrid networks, several crucial performance aspects are not well understood. For example, many systems enforce artificial segregation of the hybrid network parts, leaving money on the table.In this article, we study the algorithmic problem of how to jointly optimize topology and routing in reconfigurable data centers, in order to optimize a most fundamental metric, maximum link load. The complexity of reconfiguration mechanisms in this space is unexplored at large, especially for the following cross-layer network-design problem: given a hybrid network and a traffic matrix, jointly design the physical layer and the flow routing in order to minimize the maximum link load.We chart the corresponding algorithmic landscape in our work, investigating both un-/splittable flows and (non-)segregated routing policies. A topological complexity classification of the problem reveals NP-hardness in general for network topologies that are trees of depth at least two, in contrast to the tractability on trees of depth one. We moreover prove that the problem is not submodular for all these routing policies, even in multi-layer trees.However, networks that can be abstracted by a single packet switch (e.g., nonblocking Fat-Tree topologies) can be optimized efficiently, and we present optimal polynomial-time algorithms accordingly. We complement our theoretical results with trace-driven simulation studies, where our algorithms can significantly improve the network load in comparison to the state-of-the-art.  © 2023 Copyright held by the owner/author(s).",algorithms; complexity; load optimization; Network design; optical networks,Complex networks; Computational complexity; Fiber optic networks; Network layers; Network routing; Network topology; Packet switching; Switching networks; Complexity; Datacenter; Flow routing; Hybrid network; Link Loads; Load optimization; Network design; Optimisations; Physical layers; Reconfigurable; Polynomial approximation
Flydeling: Streamlined Performance Models for Hardware Acceleration of CNNs through System Identification,2023,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168758722&doi=10.1145%2f3594870&partnerID=40&md5=fed1d249de73573492011a85ce45a8d0,"The introduction of deep learning algorithms, such as Convolutional Neural Networks (CNNs) in many near-sensor embedded systems, opens new challenges in terms of energy efficiency and hardware performance. An emerging solution to address these challenges is to use tailored heterogeneous hardware accelerators combining processing elements of different architectural natures such as Central Processing Unit (CPU), Graphics Processing Unit (GPU), Field Programmable Gate Array (FPGA), or Application Specific Integrated Circuit (ASIC). To progress towards heterogeneity, a great asset would be an automated design space exploration tool that chooses, for each accelerated partition of a CNN, the most appropriate architecture considering available resources. To feed such a design space exploration process, models are required that provide very fast yet precise evaluations of alternative architectures or alternative forms of CNNs. Quick configuration estimation could be achieved with few parameters from representative input sequences. This article studies a solution called flydeling (as a contraction of flyweight modeling) for obtaining these models by inspiring from the black-box System Identification (SI) domain. We refer to models derived using the proposed approach as flyweight models (flydels).A methodology is proposed to generate these flydels, using CNN properties as predictor features together with SI techniques with a stochastic excitation input at a feature map dimensions level. For an embedded CPU-FPGA-GPU heterogeneous platform, it is demonstrated that it is possible to learn these Key Performance Indicators (KPIs) flydels at an early design stage and from high-level application features. For latency, energy, and resource utilization, flydels obtain estimation errors varying between 5% and 10% with less model parameters compared to state-of-the-art solutions and are built automatically from platform measurements.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Convolutional Neural Networks; Heterogeneous computing; model performance estimation,Benchmarking; Computer graphics; Computer graphics equipment; Convolution; Convolutional neural networks; Deep learning; Embedded systems; Energy efficiency; Field programmable gate arrays (FPGA); Graphics processing unit; Integrated circuit design; Network architecture; Parameter estimation; Religious buildings; Stochastic systems; Convolutional neural network; Field programmables; Hardware acceleration; Heterogeneous computing; Model performance estimation; Modeling performance; Performance estimation; Performance Modeling; Programmable gate array; System-identification; Program processors
"Delay and Price Differentiation in Cloud Computing: A Service Model, Supporting Architectures, and Performance",2023,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168771338&doi=10.1145%2f3592852&partnerID=40&md5=1463c2d6ada85c8665d83bfcec36add7,"Many cloud service providers (CSPs) offer an on-demand service with a small delay. Motivated by the reality of cloud ecosystems, we study non-interruptible services and consider a differentiated service model to complement the existing market by offering multiple service level agreements (SLAs) to satisfy users with different delay tolerance. The model itself is incentive compatible by construction. Two typical architectures are considered to fulfill SLAs: (i) non-preemptive priority queues and (ii) multiple independent groups of servers. We leverage queueing theory to establish guidelines for the resultant market: (a) Under the first architecture, the service model can only improve the revenue marginally over the pure on-demand service model and (b) under the second architecture, we give a closed-form expression of the revenue improvement when a CSP offers two SLAs and derive a condition under which the market is viable. Additionally, under the second architecture, we give an exhaustive search procedure to find the optimal SLA delays and prices when a CSP generally offers multiple SLAs. Numerical results show that the achieved revenue improvement can be significant even if two SLAs are offered. Our results can help CSPs design optimal delay-differentiated services and choose appropriate serving architectures.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cloud computing; incentive compatible; Service differentiation,Cloud computing; Commerce; Computation theory; Computer architecture; Quality of service; Queueing theory; Cloud service providers; Cloud-computing; Delay differentiation; Incentive compatible; Multiple services; On-demand services; Price differentiation; Service differentiation; Service modeling; Servicelevel agreement (SLA); Architecture
"Dynamic Scheduling in a Partially Fluid, Partially Lossy Queueing System",2023,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168759248&doi=10.1145%2f3582884&partnerID=40&md5=fdec40b0118336f51aeb1e1231ebcc80,"We consider a single server queueing system with two classes of jobs: eager jobs with small sizes that require service to begin almost immediately upon arrival, and tolerant jobs with larger sizes that can wait for service. While blocking probability is the relevant performance metric for the eager class, the tolerant class seeks to minimize its mean sojourn time. In this article, we analyse the performance of each class under dynamic scheduling policies, where the scheduling of both classes depends on the instantaneous state of the system. This analysis is carried out under a certain fluid limit, where the arrival rate and service rate of the eager class are scaled to infinity, holding the offered load constant. Our performance characterizations reveal a (dynamic) pseudo-conservation law that ties the performance of both classes to the standalone blocking probabilities associated with the scheduling policies for the eager class. Furthermore, the performance is robust to other specifics of the scheduling policies. We also characterize the Pareto frontier of the achievable region of performance vectors under the same fluid limit, and identify a (two-parameter) class of Pareto-completescheduling policies.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",delay systems; fluid limit; loss systems; Multi-class queueing systems; Pareto-optimal policies,Pareto principle; Queueing networks; Queueing theory; Delays system; Dynamic scheduling; Fluid limits; Loss system; Multi-class queueing systems; Optimal policies; Pareto-optimal; Pareto-optimal policy; Performance; Scheduling policies; Blocking probability
On the Cost of Near-Perfect Wear Leveling in Flash-Based SSDs,2023,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161997215&doi=10.1145%2f3576855&partnerID=40&md5=083f8fa6df7b07089ca9749223905173,"Wear leveling (WL) techniques in flash-based SSDs aim at distributing the erase cycles as uniformly as possible across the memory blocks within the SSD to extend its life span. The downside of any WL technique is that it causes additional internal write operations, thereby increasing the so-called write amplification (WA) factor, which equals the ratio between the total number of writes performed and the number of writes requested by the host system.In this article, we address the question whether near-perfect WL is possible at low costs in terms of the WA factor. We answer this question affirmatively by presenting a simple randomized algorithm that combines WL with garbage collection. This algorithm guarantees that the wear is nearly perfectly balanced at all times while causing a low increase in the WA. This is demonstrated mathematically using a mean field model in case of uniform random writes and using trace-driven simulation experiments for general workloads.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesSSD; garbage collection; Wear Leveling; write amplification,Mean field theory; Refuse collection; Additional key word and phrasesssd; Amplification factors; Garbage collection; Key words; Lifespans; Low-costs; Memory blocks; Wear-Leveling; Write amplifications; Write operations; Wear of materials
Trading Throughput for Freshness: Freshness-aware Traffic Engineering and In-Network Freshness Control,2023,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162054409&doi=10.1145%2f3576919&partnerID=40&md5=9d883c0a83b94c48587550d6d3eddad3,"With the advent of the Internet of Things (IoT), applications are becoming increasingly dependent on networks to not only transmit content at high throughput but also deliver it when it is fresh, i.e., synchronized between source and destination. Existing studies have proposed the metric age of information (AoI) to quantify freshness and have system designs that achieve low AoI. However, despite active research in this area, existing results are not applicable to general wired networks for two reasons. First, they focus on wireless settings, where AoI is mostly affected by interference and collision, while queueing issues are more prevalent in wired settings. Second, traditional high-throughput/low-latency legacy drop-adverse (LDA) flows are not taken into account in most system designs; hence, the problem of scheduling mixed flows with distinct performance objectives is not addressed. In this article, we propose a hierarchical system design to treat wired networks shared by mixed flow traffic, specifically LDA and AoI flows, and study the characteristics of achieving a good tradeoff between throughput and AoI. Our approach to the problem consists of two layers: freshness-aware traffic engineering (FATE) and in-network freshness control (IFC). The centralized FATE solution studies the characteristics of the source flow to derive the sending rate/update frequency for flows via the optimization problem LDA-AoI Coscheduling. The parameters specified by FATE are then distributed to IFC, which is implemented at each outport of the network's nodes and used for efficient scheduling between LDA and AoI flows. We present a Linux implementation of IFC and demonstrate the effectiveness of FATE/IFC through extensive emulations. Our results show that it is possible to trade a little throughput (5% lower) for much shorter AoI (49% to 71% shorter) compared to state-of-the-art traffic engineering.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesAge of Information (AoI); freshness-aware traffic engineering; in-network traffic control; mixed traffic scheduling,Computer operating systems; Hierarchical systems; Internet of things; Additional key word and phrasesage of information (age of information); Freshness-aware traffic engineering; In networks; In-network traffic control; Information age; Key words; Mixed traffic; Mixed traffic scheduling; Network traffic control; Traffic Engineering; Traffic scheduling; Systems analysis
Pulsed Power Load Coordination in Mission- and Time-critical Cyber-physical Systems,2023,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162075359&doi=10.1145%2f3573197&partnerID=40&md5=d132406f6857b55ee83596431098b67b,"Many mission- and time-critical cyber-physical systems deploy an isolated power system for their power supply. Under extreme conditions, the power system must process critical missions by maximizing the Pulsed Power Load (PPL) utility while maintaining the normal loads in the cyber-physical system. Optimal operation requires careful coordination of PPL deployment and power supply processes. In this work, we formulate the coordination problem for maximizing PPL utility under available resources, capacity, and demand constraints. The coordination problem has two scenarios for different use cases, fixed and general normal loads. We develop an exact pseudo-polynomial time dynamic programming algorithm for each scenario with a proven guarantee to produce an optimal coordination schedule. The performance of the algorithms is also experimentally evaluated, and the results agree with our theoretical analysis, showing the practicality of the solutions.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesDecision making; dynamic programming; mission- and time-critical cyber-physical systems,Cyber Physical System; Dynamics; Embedded systems; Polynomial approximation; Additional key word and phrasesdecision making; Cybe-physical systems; Cyber-physical systems; Key words; Mission critical; Mission- and time-critical cybe-physical system; Power load; Power supply; Pulsed-power; Time-critical; Dynamic programming
On the Capacity Region of Bipartite and Tripartite Entanglement Switching,2023,ACM Transactions on Modeling and Performance Evaluation of Computing Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161973465&doi=10.1145%2f3571809&partnerID=40&md5=8bdcfa072ecdb8f313d4fb5bf1b5ad6f,"We study a quantum entanglement distribution switch serving a set of users in a star topology with equal-length links. The quantum switch, much like a quantum repeater, can perform entanglement swapping to extend entanglement across longer distances. Additionally, the switch is equipped with entanglement switching logic, enabling it to implement switching policies to better serve the needs of the network. In this work, the function of the switch is to create bipartite or tripartite entangled states among users at the highest possible rates at a fixed ratio. Using Markov chains, we model a set of randomized switching policies. Discovering that some are better than others, we present analytical results for the case where the switch stores one qubit per user, and find that the best policies outperform a time division multiplexing policy for sharing the switch between bipartite and tripartite state generation. This performance improvement decreases as the number of users grows. The model is easily augmented to study the capacity region in the presence of quantum state decoherence and associated cut-off times for qubit storage, obtaining similar results. Moreover, decoherence-associated quantum storage cut-off times appear to have little effect on capacity in our identical-link system. We also study a smaller class of policies when the switch stores two qubits per user.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesQuantum switch; entanglement distribution; Markov chain,Computation theory; Markov processes; Quantum optics; Qubits; Additional key word and phrasesquantum switch; Bipartite entanglement; Capacity regions; Decoherence; Entanglement distribution; Key words; OFF time; Off-time; Star topology; Tripartite entanglement; Quantum entanglement
