Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
A new benchmark dataset with production methodology for short text semantic similarity algorithms,2013,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891861842&doi=10.1145%2f2537046&partnerID=40&md5=5fa8dfcabab742acd4911d8ae122cd8c,"This research presents a new benchmark dataset for evaluating Short Text Semantic Similarity (STSS) measurement algorithms and the methodology used for its creation. The power of the dataset is evaluated by using it to compare two established algorithms, STASIS and Latent Semantic Analysis. This dataset focuses on measures for use in Conversational Agents; other potential applications include email processing and data mining of social networks. Such applications involve integrating the STSS algorithm in a complex system, but STSS algorithms must be evaluated in their own right and compared with others for their effectiveness before systems integration. Semantic similarity is an artifact of human perception; therefore its evaluation is inherently empirical and requires benchmark datasets derived from human similarity ratings. The new dataset of 64 sentence pairs, STSS-131, has been designed to meet these requirements drawing on a range of resources from traditional grammar to cognitive neuroscience. The human ratings are obtained from a set of trials using new and improved experimental methods, with validated measures and statistics. The results illustrate the increased challenge and the potential longevity of the STSS-131 dataset as the Gold Standard for future STSS algorithm evaluation. © 2013 ACM 1550-4875/2013/12-ART17 15.00.",Conversational agents; Evaluation/methodology; Semantic similarity; Similarity measures; Text analysis; Text processing,Algorithms; Semantics; Text processing; Conversational agents; Evaluation/methodology; Semantic similarity; Similarity measure; Text analysis; Data processing
Lattice BLEU oracles in machine translation,2013,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891861055&doi=10.1145%2f2513147&partnerID=40&md5=b4222f9b5a172fb08239299375b1b2b4,"The search space of Phrase-Based Statistical Machine Translation (PBSMT) systems can be represented as a directed acyclic graph (lattice). By exploring this search space, it is possible to analyze and understand the failures of PBSMT systems. Indeed, useful diagnoses can be obtained by computing the so-called oracle hypotheses, which are hypotheses in the search space that have the highest quality score. For standard SMT metrics, this problem is, however, NP-hard and can only be solved approximately. In this work, we present two new methods for efficiently computing oracles on lattices: the first one is based on a linear approximation of the corpus BLEU score and is solved using generic shortest distance algorithms; the second one relies on an Integer Linear Programming (ILP) formulation of the oracle decoding that incorporates count clipping constraints. It can either be solved directly using a standard ILP solver or using Lagrangian relaxation techniques. These new decoders are evaluated and compared with several alternatives from the literature for three language pairs, using lattices produced by two PBSMT systems. © 2013 ACM 1550-4875/2013/12-ART17 15.00.",BLEU; Integer linear programming; Lattices; Machine translation; Oracle decoding,Computer aided language translation; Crystal lattices; Decoding; Integer programming; BLEU; Directed acyclic graph (DAG); Distance algorithm; Integer Linear Programming; Lagrangian relaxation techniques; Linear approximations; Machine translations; Phrase-based statistical machine translation; Approximation algorithms
Learning to control listening-oriented dialogue using partially observable markov decision processes,2013,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891858375&doi=10.1145%2f2513145&partnerID=40&md5=f7b495e97bae705bd05895254e5a484c,"Our aim is to build listening agents that attentively listen to their users and satisfy their desire to speak and have themselves heard. This article investigates how to automatically create a dialogue control component of such a listening agent.We collected a large number of listening-oriented dialogues with their user satisfaction ratings and used them to create a dialogue control component that satisfies users by means of Partially Observable Markov Decision Processes (POMDPs). Using a hybrid dialog controller where high-level dialog acts are chosen with a statistical policy and low-level slot values are populated by a wizard, we evaluated our dialogue control method in aWizard-of-Oz experiment. The experimental results show that our POMDPbased method achieves significantly higher user satisfaction than other stochastic models, confirming the validity of our approach. This article is the first to verify, by using human users, the usefulness of POMDPbased dialogue control for improving user satisfaction in nontask-oriented dialogue systems. © 2013 ACM 1550-4875/2013/12-ART17 15.00.",Dialogue control; Dialogue systems; Listening-oriented dialogue; Partially observable Markov decision processes,Speech processing; Dialog acts; Dialogue control; Dialogue systems; Human users; Listening-oriented dialogue; Partially observable Markov decision process; User satisfaction; Stochastic control systems
Editorial,2013,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891884876&doi=10.1145%2f2556529&partnerID=40&md5=6d4c7d986304ad73e9a10c62373d05b2,[No abstract available],,
Cognitive canonicalization of natural language queries using semantic strata,2013,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891861533&doi=10.1145%2f2539053&partnerID=40&md5=67d12047b8a4c17c9513322579b2c965,"Natural language search relies strongly on perceiving semantics in a query sentence. Semantics is captured by the relationship among the query words, represented as a network (graph). Such a network of words can be fed into larger ontologies, like DBpedia or Google Knowledge Graph, where they appear as subgraphs-fashioning the name subnetworks (subnets). Thus, subnet is a canonical form for interfacing a natural language query to a graph database and is an integral step for graph-based searching. In this article, we present a novel standalone NLP technique that leverages the cognitive psychology notion of semantic strata for semantic subnetwork extraction from natural language queries. The cognitive model describes some of the fundamental structures employed by the human cognition to construct semantic information in the brain, called semantic strata. We propose a computational model based on conditional random fields to capture the cognitive abstraction provided by semantic strata, facilitating cognitive canonicalization of the query. Our results, conducted on approximately 5000 queries, suggest that the cognitive canonicals based on semantic strata are capable of significantly improving parsing and role labeling performance beyond pure lexical approaches, such as parts-of-speech based techniques. We also find that cognitive canonicalized subnets are more semantically coherent compared to syntax trees when explored in graph ontologies like DBpedia and improve ranking of retrieved documents. © 2013 ACM 1550-4875/2013/12-ART17 15.00.",Cognitive linguistics; Query; Search; Semantic; Strata; Subnets,Natural language processing systems; Query processing; Trees (mathematics); Cognitive linguistics; Query; Search; Strata; Subnets; Semantics
Composition of semantic relations: Theoretical framework and case study,2013,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891850030&doi=10.1145%2f2513146&partnerID=40&md5=9f01fc7d51b738e2812af2604a4078ba,"Extracting semantic relations from text is a preliminary step towards understanding the meaning of text. The more semantic relations are extracted from a sentence, the better the representation of the knowledge encoded into that sentence. This article introduces a framework for the Composition of Semantic Relations (CSR). CSR aims to reveal more text semantics than existing semantic parsers by composing new relations out of previously extracted relations. Semantic relations are defined using vectors of semantic primitives, and an algebra is suggested to manipulate these vectors according to a CSR algorithm. Inference axioms that combine two relations and yield another relation are generated automatically. CSR is a language-agnostic, inventory-independent method to extract semantic relations. The formalism has been applied to a set of 26 well-known relations and results are reported. © 2013 ACM 1550-4875/2013/12-ART17 15.00.",Relation extraction; Relation inference; Semantic relations,Computational methods; Computer science; Speech communication; CSR algorithm; Relation extraction; Relation inference; Semantic relations; Theoretical framework; Semantics
On collocations and topic models,2013,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880243080&doi=10.1145%2f2483969.2483972&partnerID=40&md5=2ac293387ecdc77fcad43c9d63edbf69,"We investigate the impact of preextracting and tokenizing bigram collocations on topic models. Using extensive experiments on four different corpora, we show that incorporating bigram collocations in the document representation creates more parsimonious models and improves topic coherence. We point out some problems in interpreting test likelihood and test perplexity to compare model fit, and suggest an alternate measure that penalizes model complexity. We show how the Akaike information criterion is a more appropriate measure, which suggests that using a modest number (up to 1000) of top-ranked bigrams is the optimal topic modelling configuration. Using these 1000 bigrams also results in improved topic quality over unigram tokenization. Further increases in topic quality can be achieved by using up to 10,000 bigrams, but this is at the cost of a more complex model. We also show that multiword (bigram and longer) named entities give consistent results, indicating that they should be represented as single tokens. This is the first work to explicitly study the effect of re-gram tokenization on LDA topic models, and the first work to make empirical recommendations to topic modelling practitioners, challenging the standard practice of unigram-based tokenization. © 2013 ACM.",,Computational methods; Computer science; Speech communication; Akaike information criterion; Complex model; Document Representation; Model complexity; Named entities; Parsimonious models; Standard practices; Tokenization; Models
Sentiment profiles of multiword expressions in test-taker essays: The case of noun-noun compounds,2013,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880242764&doi=10.1145%2f2483969.2483974&partnerID=40&md5=a4e2dfec91a7b79651167ad92908b757,"The property of idiomaticity vs. compositionality of a multiword expression traditionally pertains to the semantic interpretation of the expression. In this article, we consider this property as it applies to the expression's sentiment profile (relative degree of positivity, negativity, and neutrality). Thus, while heart attack is idiomatic in terms of semantic interpretation, the sentiment profile of the expression (strongly negative) can, in fact, be determined from the strongly negative profile of the head word. In this article, we (1) propose a way to measure compositionality of a multiword expression's sentiment profile, and perform the measurement on noun-noun compounds; (2) evaluate the utility of using sentiment profiles of noun-noun compounds in a sentence-level sentiment classification task. We find that the sentiment profiles of noun-noun compounds in test-taker essays tend to be highly compositional and that their incorporation improves the performance of a sentiment classification system. © 2013 ACM.",Compound; Discourse; Idiom; Multi-word expression; MWE; Negative; Neutral; Noun-noun compound; Positive; Sentiment; Sentiment analysis; Sentiment polarity,Computational methods; Computer science; Speech communication; Compound; Discourse; Idiom; Multi-word expressions; MWE; Negative; Neutral; Noun-noun compounds; Positive; Sentiment; Sentiment analysis; Sentiment polarity; Semantics
Combining compound recognition and PCFG-LA parsing with word lattices and conditional random fields,2013,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880238385&doi=10.1145%2f2483969.2483970&partnerID=40&md5=76c26c12e590c0cbef997117a36bb458,"The integration of compounds in a parsing procedure has been shown to improve accuracy in an artificial context where such expressions have been perfectly preidentified. This article evaluates two empirical strategies to incorporate such multiword units in a real PCFG-LA parsing context: (1) the use of a grammar including compound recognition, thanks to specialized annotation schemes for compounds; (2) the use of a state-of-the-art discriminative compound prerecognizer integrating endogenous and exogenous features. We show how these two strategies can be combined with word lattices representing possible lexical analyses generated by the recognizer. The proposed systems display significant gains in terms of multiword recognition and often in terms of standard parsing accuracy. Moreover, we show through an Oracle analysis that this combined strategy opens promising new research directions. © 2013 ACM.",Conditional random fields; Multiword expressions; Parsing; Word lattice,Random processes; Speech recognition; Annotation scheme; Conditional random field; Lexical analysis; Multi-word expressions; Multi-word units; Parsing; Parsing procedures; Word lattice; Formal languages
Word sense and semantic relations in noun compounds,2013,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880199189&doi=10.1145%2f2483969.2483971&partnerID=40&md5=14dd5a5ea345bedc7c75b9a7ecc98b71,"In this article, we investigate word sense distributions in noun compounds (NCs). Our primary goal is to disambiguate the word sense of component words in NCs, based on investigation of ""semantic collocation"" between them. We use sense collocation and lexical substitution to build supervised and unsupervised word sense disambiguation (WSD) classifiers, and show our unsupervised learner to be superior to a benchmark WSD system. Further, we develop a word sense-based approach to interpreting the semantic relations in NCs. © 2013 ACM.",Artificial intelligence; Noun compounds; Semantic relations; Word sense disambiguation,Artificial intelligence; Component words; Noun compounds; Semantic relations; Word sense; Word Sense Disambiguation; Semantics
Semantic interpretation of noun compounds using verbal and other paraphrases,2013,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880214824&doi=10.1145%2f2483969.2483975&partnerID=40&md5=e6cd8b7af85f4c52b9e5a71017849fdd,"We study the problem of semantic interpretation of noun compounds such as bee honey, malaria mosquito, apple cake, and stem cell. In particular, we explore the potential of using predicates that make explicit the hidden relation that holds between the nouns that form the noun compound. For example, mosquito that carries malaria is a paraphrase of the compound malaria mosquito in which the verb explicitly states the semantic relation between the two nouns. We study the utility of using such paraphrasing verbs, with associated weights, to build a representation of the semantics of a noun compound, for example, malaria mosquito can be represented as follows: carry (23), spread (16), cause (12), transmit (9), and so on. We also explore the potential of using multiple paraphrasing verbs as features for predicting abstract semantic relations such as CAUSE, and we demonstrate that using explicit paraphrases can help improve statistical machine translation. © 2013 ACM.",Lexical semantics; Machine translation; Multiword expressions; Noun compounds; Paraphrases; Web as a corpus,Diseases; Semantics; Stem cells; Lexical semantics; Machine translations; Multi-word expressions; Noun compounds; Paraphrases; Web as a corpus; Semantic Web
A computational model of logical metonymy,2013,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880222689&doi=10.1145%2f2483969.2483973&partnerID=40&md5=87a34cf0925d755febfe552dfbbe364b,"The use of figurative language is ubiquitous in natural language texts and it is a serious bottleneck in automatic text understanding. A system capable of interpreting figurative expressions would be an invaluable addition to the real-world natural language processing (NLP) applications that need to access semantics, such as machine translation, opinion mining, question answering and many others. In this article we focus on one type of figurative language, logical metonymy, and present a computational model of its interpretation bringing together statistical techniques and the insights from linguistic theory. Compared to previous approaches this model is both more informative and more accurate. The system produces sense-level interpretations of metonymic phrases and then automatically organizes them into conceptual classes, or roles, discussed in the majority of linguistic literature on the phenomenon. © 2013 ACM.",Logical metonymy; Semantic interpretation; Word sense disambiguation,Computational methods; Linguistics; Semantics; Computational model; Logical metonymy; Machine translations; NAtural language processing; Natural language text; Semantic interpretation; Statistical techniques; Word Sense Disambiguation; Natural language processing systems
Lexical semantic factors in the acceptability of english support-verb-nominalization constructions,2013,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880356097&doi=10.1145%2f2483691.2483694&partnerID=40&md5=261981b337084a5cf1842d21a18fb80f,"We explore the properties of support-verb and nominalization (SVN) pairs in English, a type of multiword expression in which a semantically impoverished verb combines with a complement nominalization sharing an unexpressed role with the verb. This study follows others in seeking syntactic or lexical semantic factors correlated with the acceptability of these constructions. In particular, following recent work showing certain semantic verb class features to improve SVN classification [Tu and Roth 2011], we explore the possibility that support verbs and the verbal roots of nominalizations in acceptable SVN pairs are clustered according to the classes of Levin [1993]. We compare the compatibility correlation of these results with those of the Aktionsart-class-based proposal of Barrett and Davis [2002]. We find the evidence that Levin classes are a factor in the acceptability of SVN constructions to be equivocal, and conclude with a discussion of the reasons for this finding. © ACM 2013.",Clustering; Lexical semantics; Nominalizations; Support verbs,Computational methods; Computer science; Speech communication; Clustering; Lexical semantics; Multi-word expressions; Nominalization; Nominalizations; Semantics
How many multiword expressions do people know?,2013,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880377713&doi=10.1145%2f2483691.2483693&partnerID=40&md5=fc5a378fa3e014634ef75bf60ca7b869,"What is a multiword expression (MWE) and how many are there? Mark Liberman gave a great invited talk at ACL-89, titled ""How Many Words Do People Know?"" where he spent the entire hour questioning the question. Many of the same questions apply to multiword expressions. What is a word? An expression? What is many? What is a person? What does it mean to know? Rather than answer these questions, this article will use them as Liberman did, as an excuse for surveying how such issues are addressed in a variety of fields: computer science, Web search, linguistics, lexicography, educational testing, psychology, statistics, and so on. © ACM 2013.",Intelligence; Knowledge; Multiword expression; Phrase; Web search; Word,Information retrieval; Websites; Intelligence; Knowledge; Multi-word expressions; Phrase; Web searches; Word; Linguistics
Learning to detect english and Hungarian light verb constructions,2013,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880339544&doi=10.1145%2f2483691.2483695&partnerID=40&md5=718f31c92d69f3aa556a39c752bf8a38,"Light verb constructions consist of a verbal and a nominal component, where the noun preserves its original meaning while the verb has lost it (to some degree). They are syntactically flexible and their meaning can only be partially computed on the basis of the meaning of their parts, thus they require special treatment in natural language processing. For this purpose, the first step is to identify light verb constructions. In this study, we present our conditional random fields-based tool-called FXTagger-for identifying light verb constructions. The flexibility of the tool is demonstrated on two, typologically different, languages, namely, English and Hungarian. As earlier studies labeled different linguistic phenomena as light verb constructions, we first present a linguistics-based classification of light verb constructions and then show that FXTagger is able to identify different classes of light verb constructions in both languages. Different types of texts may contain different types of light verb constructions; moreover, the frequency of light verb constructions may differ from domain to domain. Hence we focus on the portability of models trained on different corpora, and we also investigate the effect of simple domain adaptation techniques to reduce the gap between the domains. Our results show that in spite of domain specificities, out-domain data can also contribute to the successful LVC detection in all domains. © ACM 2013.",Conditional random fields; Corpora; Domain adaptation; English; Hungarian; Light verb constructions; Multiword expressions,Classification (of information); Natural language processing systems; Random processes; Tools; Conditional random field; Corpora; Domain adaptation; English; Hungarians; Multi-word expressions; Linguistics
Modeling the internal variability of multiword expressions through a pattern-based method,2013,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880357379&doi=10.1145%2f2483691.2483696&partnerID=40&md5=99b89841b5a71c10c432faaa24da8e1e,"The issue of internal variability of multiword expressions (MWEs) is crucial towards their identification and extraction in running text.We present a corpus-supported and computational study on Italian MWEs, aimed at defining an automatic method for modeling internal variation, exploiting frequency and part-of-speech (POS) information. We do so by deriving an XML-encoded lexicon of MWEs based on a manually compiled dictionary, which is then projected onto a a large corpus. Since a search for fixed forms suffers from low recall, while an unconstrained flexible search for lemmas yields a loss in precision, we suggest a procedure aimed at maximizing precision in the identification of MWEs within a flexible search. Our method builds on the idea that internal variability can be modelled via the novel introduction of variation patterns, which work over POS patterns, and can be used as working tools for controlling precision. We also compare the performance of variation patterns to that of association measures, and explore the possibility of using variation patterns in MWE extraction in addition to identification. Finally, we suggest that corpus-derived, pattern-related information can be included in the original MWE lexicon by means of an enriched coding and the creation of an XML-based repository of patterns. © ACM 2013.",Italian; Lexical resources; Morphology; Multiword expressions; Natural Language Processing,Extraction; Morphology; Natural language processing systems; XML; Association measures; Computational studies; Internal variability; Italian; Lexical resources; Multi-word expressions; NAtural language processing; Variation pattern; Linguistics
Introduction to the special issue on multiword expressions: From theory to practice and use,2013,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880358959&doi=10.1145%2f2483691.2483692&partnerID=40&md5=d65c7c2b3925b024700a1641f210a6a2,"We are in 2013, and multiword expressions have been around for a while in the computational linguistics research community. Since the first ACL workshop on MWEs 12 years ago in Sapporo, Japan, much has been discussed, proposed, experimented, evaluated and argued about MWEs. And yet, they deserve the publication of a whole special issue of the ACM Transactions on Speech and Language Processing. But what is it about multiword expressions that keeps them in fashion? Who are the people and the institutions who perform and publish groundbreaking fundamental and applied research in this field? What is the place and the relevance of our lively research community in the bigger picture of computational linguistics? Where do we come from as a community, and most importantly, where are we heading? In this introductory article, we share our point of view about the answers to these questions and introduce the articles that compose the current special issue. © ACM 2013.",,Computational linguistics; Applied research; Language processing; Multi-word expressions; Point of views; Research communities; Research
Managing information disparity in multilingual document collections,2013,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875723043&doi=10.1145%2f2442076.2442077&partnerID=40&md5=d39255f7a338ee0f63e6e7ac758bd818,"Information disparity is a major challenge with multilingual document collections. When documents are dynamically updated in a distributed fashion, information content among different language editions may gradually diverge. We propose a framework for assisting human editors to manage this information disparity, using tools from machine translation and machine learning. Given source and target documents in two different languages, our system automatically identifies information nuggets that are new with respect to the target and suggests positions to place their translations. We perform both real-world experiments and large-scale simulations on Wikipedia documents and conclude our system is effective in a variety of scenarios. © 2013 ACM.",Algorithms; Experimentation; Languages,Algorithms; Experiments; Query languages; Experimentation; Information contents; Large scale simulations; Machine translations; Multilingual documents; Real world experiment; Wikipedia; Translation (languages)
Combining co-clustering with noise detection for theme-based summarization,2013,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891838026&doi=10.1145%2f2513563&partnerID=40&md5=79d296cec3cd1b9c528e8843daf5c304,"To overcome the fact that the length of sentences is short and their content is limited, we regard words as independent text objects rather than features of sentences in sentence clustering and develop two coclustering frameworks, namely integrated clustering and interactive clustering, to cluster sentences and words simultaneously. Since real-world datasets always contain noise, we incorporate noise detection and removal to enhance clustering of sentences and words. Meanwhile, a semisupervised approach is explored to incorporate the query information (and the sentence information in early document sets) in themebased summarization. Thorough experimental studies are conducted. When evaluated on the DUC2005-2007 datasets and TAC 2008-2009 datasets, the performance of the two noise-detecting co-clustering approaches is comparable with that of the top three systems. The results also demonstrate that the interactive with noise detection algorithm is more effective than the noise-detecting integrated algorithm. © 2013 ACM 1550-4875/2013/12-ART17 15.00.",Document analysis; Noise detection; Sentence and word co-clustering; Theme-based summarization,Computer science; Speech communication; Co-clustering; Document analysis; Integrated algorithm; Noise detection; Query information; Real-world datasets; Sentence clustering; Theme-based summarization; Computational methods
Towards content-level coherence with aspect-guided summarization,2013,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875724665&doi=10.1145%2f2442076.2442078&partnerID=40&md5=ca34362093738de3fc7edcced2e9421d,"The TAC 2010 summarization track initiated a new task - aspect-guided summarization - that centers on textual aspects embodied as particular kinds of information of a text. We observe that aspect-guided summaries not only address highly specific user need, but also facilitate content-level coherence by using aspect information. In this article, we present a full-fledged approach to aspect-guided summarization with a focus on summary coherence. Our summarization approach depends on two prerequisite subtasks: recognizing aspect-bearing sentences in order to do sentence extraction, and modeling aspect-based coherence with an HMM model in order to predict a coherent sentence ordering. Using the manually annotated TAC 2010 and 2010 datasets, we validated the effectiveness of our proposed methods for those subtasks. Drawing on the empirical results, we proceed to develop an aspect-guided summarizer based on a simple but robust base summarizer. With sentence selection guided by aspect information, our system is one of the best on TAC 2011. With sentence ordering predicted by the aspect-based HMM model, the summaries achieve good coherence. © 2013 ACM.",Algorithms; Experimentation,Algorithms; Computer science; Speech communication; Content level; Experimentation; HMM models; Modeling aspects; Sentence extraction; Sentence ordering; Sentence selection; User need; Computational methods
An information-theoretic measure to evaluate parsing difficulty across treebanks,2013,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873667463&doi=10.1145%2f2407736.2407737&partnerID=40&md5=75985a8d83866a734004baa65f233749,"With the growing interest in statistical parsing, special attention has recently been devoted to the problem of comparing different treebanks to assess which languages or domains are more difficult to parse relative to a given model. A common methodology for comparing parsing difficulty across treebanks is based on the use of the standard labeled precision and recall measures. As an alternative, in this article we propose an information-theoretic measure, called the expected conditional cross-entropy (ECC). One important advantage with respect to standard performance measures is that ECC can be directly expressed as a function of the parameters of the model. We evaluate ECC across several treebanks for English, French, German, and Italian, and show that ECC is an effective measure of parsing difficulty, with an increase in ECC always accompanied by a degradation in parsing accuracy. © 2013 ACM.",Natural language parsing; Probabilistic context-free grammars,Information theory; Cross entropy; Effective measures; Natural language parsing; Precision and recall; Probabilistic context free grammars; Standard performance; Statistical parsing; Treebanks; Forestry
Contextual and active learning-based affect-sensing from virtual drama improvisation,2013,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873621241&doi=10.1145%2f2407736.2407738&partnerID=40&md5=b00deb8d633d0572fba1fb096df6d4ea,"Affect interpretation from open-ended drama improvisation is a challenging task. This article describes experiments in using latent semantic analysis to identify discussion themes and potential target audiences for those improvisational inputs without strong affect indicators. A context-based affect-detection is also implemented using a supervised neural network with the consideration of emotional contexts of most intended audiences, sentence types, and interpersonal relationships. In order to go beyond the constraints of predefined scenarios and improve the system's robustness, min-margin-based active learning is implemented. This active learning algorithm also shows great potential in dealing with imbalanced affect classifications. Evaluation results indicated that the context-based affect detection achieved an averaged precision of 0.826 and an averaged recall of 0.813 for affect detection of the test inputs from the Crohn's disease scenario using three emotion labels: positive, negative, and neutral, and an averaged precision of 0.868 and an average recall of 0.876 for the test inputs from the school bullying scenario. Moreover, experimental evaluation on a benchmark data set for active learning demonstrated that active learning was able to greatly reduce human annotation efforts for the training of affect detection, and also showed promising robustness in dealing with open-ended example inputs beyond the improvisation of the chosen scenarios. © 2013 ACM.",Active learning and drama improvisation; Affect detection; Semantic interpretation,Learning algorithms; Active Learning; Active-learning algorithm; Affect detection; Benchmark data; Context-based; Crohn's disease; Evaluation results; Experimental evaluation; Human annotations; Interpersonal relationship; Latent Semantic Analysis; Potential targets; Semantic interpretation; Supervised neural networks; Test inputs; Semantics
Syllable specific unit selection cost functions for text-to-speech synthesis,2012,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870711187&doi=10.1145%2f2382434.2382435&partnerID=40&md5=a2741db07682827d0cc4ee7fca3ce320,"This paper presents the design and development of syllable specific unit selection cost functions for improving the quality of text-to-speech synthesis. Appropriate unit selection cost functions, namely concatenation cost and target cost, are proposed for syllable based synthesis. Concatenation costs are defined based on the type of segments present at the syllable joins. Proposed concatenation costs have shown significant reduction in perceptual discontinuity at syllable joins. Three-stage target cost formulation is proposed for selecting appropriate units from database. Subjective evaluation has shown improvement in the quality of speech at each stage. © 2012 ACM.",Concatenation cost; Improvement; Perceptual distortion; Quality of speech; Target cost; Text-to-speech synthesis; Unit selection,Cost functions; Speech synthesis; Improvement; Perceptual distortion; Quality of speech; Target cost; Unit selection; Costs
Investigating metaphorical language in sentiment analysis: A sense-to-sentiment perspective,2012,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870675926&doi=10.1145%2f2382434.2382436&partnerID=40&md5=4d79f0aa1611b8c8f9e0ae10a88ba099,"Intuition dictates that figurative language and especially metaphorical expressions should convey sentiment. It is the aim of this work to validate this intuition by showing that figurative language (metaphors) appearing in a sentence drive the polarity of that sentence. Towards this target, the current article proposes an approach for sentiment analysis of sentences where figurative language plays a dominant role. This approach applies Word Sense Disambiguation aiming to assign polarity to word senses rather than tokens. Sentence polarity is determined using the individual polarities for metaphorical senses as well as other contextual information. Experimental evaluation shows that the proposed method achieves high scores in comparison with other state-of-the-art approaches tested on the same corpora. Finally, experimental results provide supportive evidence that this method is also well suited for corpora consisting of literal and figurative language sentences. © 2012 ACM.",Figurative language; Metaphorical expressions; Sentiment analysis; Sentiment classification of sentences; Sentiment classification of word senses; Word Sense Disambiguation,Computational methods; Computer science; Speech communication; Figurative language; Metaphorical expressions; Sentiment analysis; Sentiment classification; Word sense; Word Sense Disambiguation; Data mining
"Perspective-oriented generation of football match summaries: Old tasks, new challenges",2012,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878542775&doi=10.1145%2f2287710.2287711&partnerID=40&md5=fda3b253e6c976aa152504eaa64ddb5f,"Team sports commentaries call for techniques that are able to select content and generate wordings to reflect the affinity of the targeted reader for one of the teams. The existing works tend to have in common that they either start from knowledge sources of limited size to whose structures then different ways of realization are explicitly assigned, or they work directly with linguistic corpora, without the use of a deep knowledge source. With the increasing availability of large-scale ontologies this is no longer satisfactory: techniques are needed that are applicable to general purpose ontologies, but which still take user preferences into account. We take the best of both worlds in that we use a two-layer ontology. The first layer is composed of raw domain data modelled in an application-independent base OWL ontology. The second layer contains a rich perspective generation-motivated domain communication knowledge ontology, inferred from the base ontology. The two-layer ontology allows us to take into account user perspective-oriented criteria at different stages of generation to generate perspective-oriented commentaries. We show how content selection, discourse structuring, information structure determination, and lexicalization are driven by these criteria and how stage after stage a truly user perspective-tailored summary is generated. The viability of our proposal has been evaluated for the generation of football match summaries of the First Spanish Football League. The reported outcome of the evaluation demonstrates that we are on the right track. © 2012 ACM.",Content selection; Discourse structuring; Domain communication knowledge; Information structuring; Lexicalization; Multilevel generation; Ontology; OWL; User perspective,Communication; Digital storage; Ontology; Content selection; Discourse structuring; Domain communication knowledge; Information structuring; Lexicalization; Multilevel generation; OWL; User perspectives; SportS
Distributed speech translation technologies for multiparty multilingual communication,2012,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878543201&doi=10.1145%2f2287710.2287712&partnerID=40&md5=9672422c6aa6beaafa8ed69367227fc6,"Developing amultilingual speech translation system requires efforts in constructing automatic speech recognition (ASR), machine translation (MT), and text-to-speech synthesis (TTS) components for all possible source and target languages. If the numerous ASR, MT, and TTS systems for different language pairs developed independently in different parts of the world could be connected, multilingual speech translation systems for a multitude of language pairs could be achieved. Yet, there is currently no common, flexible framework that can provide an entire speech translation process by bringing together heterogeneous speech translation components. In this article we therefore propose a distributed architecture framework for multilingual speech translation in which all speech translation components are provided on distributed servers and cooperate over a network. This framework can facilitate the connection of different components and functions. To show the overall mechanism, we first present our state-of-the-art technologies for multilingual ASR, MT, and TTS components, and then describe how to combine those systems into the proposed network-based framework. The client applications are implemented on a handheld mobile terminal device, and all data exchanges among client users and spoken language technology servers are managed through aWeb protocol. To support multiparty communication, an additional communication server is provided for simultaneously distributing the speech translation results from one user to multiple users. Field testing shows that the system is capable of realizingmultiparty multilingual speech translation for real-time and location-independent communication. © 2012 ACM.",Distributed architecture platforms; Machine translation; Multiparty multilingual communication; Speech recognition; Speech-to-speech translation; Text-to-speech,Computer aided language translation; Electronic data interchange; Speech recognition; Speech synthesis; Speech transmission; Distributed architecture; Machine translations; Multilingual communications; Speech-to-speech translation; Text to speech; Speech communication
Unsupervised similarity-based word sense disambiguation using context vectors and sentential word importance,2012,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861614438&doi=10.1145%2f2168748.2168750&partnerID=40&md5=0f75b246519af413ed79de81996e82cb,"The process of identifying the actual meanings of words in a given text fragment has a long history in the field of computational linguistics. Due to its importance in understanding the semantics of natural language, it is considered one of the most challenging problems facing this field. In this article we propose a new unsupervised similarity-based word sense disambiguation (WSD) algorithm that operates by computing the semantic similarity between glosses of the target word and a context vector. The sense of the target word is determined as that for which the similarity between gloss and context vector is greatest. Thus, whereas conventional unsupervised WSD methods are based on measuring pairwise similarity between words, our approach is based on measuring semantic similarity between sentences. This enables it to utilize a higher degree of semantic information, and is more consistent with the way that human beings disambiguate; that is, by considering the greater context in which the word appears. We also show how performance can be further improved by incorporating a preliminary step in which the relative importance of words within the original text fragment is estimated, thereby providing an ordering that can be used to determine the sequence in which words should be disambiguated. We provide empirical results that show that our method performs favorably against the state-of-the-art unsupervised word sense disambiguation methods, as evaluated on several benchmark datasets through different models of evaluation. © 2012 ACM.",Semantic similarity; Unsupervised similarity-based; Word importance; Word sense disambiguation; WordNet,Computational linguistics; Semantics; Semantic similarity; Unsupervised similarity-based; Word importance; Word Sense Disambiguation; Wordnet; Natural language processing systems
Optimizing the turn-taking behavior of task-oriented spoken dialog systems,2012,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861651883&doi=10.1145%2f2168748.2168749&partnerID=40&md5=e9f555de4b9bc625ffab23f25c12a1ec,"Even as progress in speech technologies and task and dialog modeling has allowed the development of advanced spoken dialog systems, the low-level interaction behavior of those systems often remains rigid and inefficient. Based on an analysis of human-human and human-computer turn-taking in naturally occurring task-oriented dialogs, we define a set of features that can be automatically extracted and show that they can be used to inform efficient end-of-turn detection. We then frame turn-taking as decision making under uncertainty and describe the Finite-State Turn-Taking Machine (FSTTM), a decision-theoretic model that combines data-driven machine learning methods and a cost structure derived from Conversation Analysis to control the turn-taking behavior of dialog systems. Evaluation results on CMU Let's Go, a publicly deployed bus information system, confirm that the FSTTM significantly improves the responsiveness of the system compared to a standard threshold-based approach, as well as previous data-driven methods. © 2012 ACM.",Machine learning; Spoken dialog systems; Turn-taking,Learning systems; Conversation analysis; Cost structure; Data-driven methods; Decision making under uncertainty; Decision-theoretic; Dialog modeling; Dialog systems; Evaluation results; Finite-state; Interaction behavior; Machine learning methods; Naturally occurring; Speech technology; Spoken dialog systems; Turn-taking; Human computer interaction
Uncertainty-based active learning with instability estimation for text classification,2012,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863267844&doi=10.1145%2f2093153.2093154&partnerID=40&md5=3b723ec51f9e830a1692b3d33c219c74,"This article deals with pool-based active learning with uncertainty sampling. While existing uncertainty sampling methods emphasize selection of instances near the decision boundary to increase the likelihood of selecting informative examples, our position is that this heuristic is a surrogate for selecting examples for which the current learning algorithm iteration is likely to misclassify. To more directly model this intuition, this article augments such uncertainty sampling methods and proposes a simple instability-based selective sampling approach to improving uncertainty-based active learning, in which the instability degree of each unlabeled example is estimated during the learning process. Experiments on seven evaluation datasets show that instability-based sampling methods can achieve significant improvements over the traditional uncertainty sampling method. In terms of the average percentage of actively selected examples required for the learner to achieve 99% of its performance when training on the entire dataset, instability sampling and sampling by instability and density methods achieve better effectiveness in annotation cost reduction than random sampling and traditional entropy-based uncertainty sampling. Our experimental results have also shown that instability-based methods yield no significant improvement for active learning with SVMs when a popular sigmoidal function is used to transform SVM outputs to posterior probabilities. © 2012 ACM.",Active learning; Data annotation; Instability estimation; Text classification; Uncertainty sampling,Heuristic methods; Learning algorithms; Stability; Uncertainty analysis; Active Learning; Data annotation; Data sets; Decision boundary; Density methods; Directly model; Entropy-based; Learning process; Pool-based; Posterior probability; Random sampling; Sampling method; Selected examples; Selective sampling; Sigmoidal functions; Text classification; Learning systems
Active learning with semi-automatic annotation for extractive speech summarization,2012,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863232635&doi=10.1145%2f2093153.2093155&partnerID=40&md5=417c2b9300035c8c6e4ebd35725478c6,We propose using active learning for extractive speech summarization in order to reduce human effort in generating reference summaries. Active learning chooses a selective set of samples to be labeled.We propose a combination of informativeness and representativeness criteria for selection. We further propose a semiautomatic method to generate reference summaries for presentation speech by using Relaxed Dynamic Time Warping (RDTW) alignment between presentation speech and its accompanied slides. Our summarization results show that the amount of labeled data needed for a given summarization accuracy can be reduced by more than 23% compared to random sampling. © 2012 ACM.,Active learning; Extractive speech summarization,Computational methods; Computer science; Active Learning; Dynamic time warping; Extractive speech summarization; Informativeness; Labeled data; Random sampling; Semi-automatic annotation; Semiautomatic methods; Speech summarization; Speech communication
Spatial role labeling: Towards extraction of spatial relations from natural language,2011,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855211938&doi=10.1145%2f2050104.2050105&partnerID=40&md5=10af0b876955a1e57a83fd0bd5b9ad20,"This article reports on the novel task of spatial role labeling in natural language text. It proposes machine learning methods to extract spatial roles and their relations. This work experiments with both a step-wise approach, where spatial prepositions are found and the related trajectors, and landmarks are then extracted, and a joint learning approach, where a spatial relation and its composing indicator, trajector, and landmark are classified collectively. Context-dependent learning techniques, such as a skip-chain conditional random field, yield good results on the GUM-evaluation (Maptask) data and CLEF-IAPR TC-12 Image Benchmark. An extensive error analysis, including feature assessment, and a cross-domain evaluation pinpoint the main bottlenecks and avenues for future research. © 2011 ACM.",Semantic labeling; Spatial information extraction; Spatial relations,Error analysis; Image segmentation; Semantics; Conditional random field; Context dependent; Cross-domain; Learning approach; Learning techniques; Machine learning methods; Natural language text; Natural languages; Novel task; Semantic labeling; Spatial information extraction; Spatial prepositions; Spatial relations; Learning systems
Learning to identify educational materials,2011,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855252933&doi=10.1145%2f2050100.2050101&partnerID=40&md5=f4187798c730892e7e712399d0090188,"In this article, we explore the task of automatically identifying educational materials by classifying documents with respect to their educational value. Through experiments carried out on a dataset of manually annotated documents, we show that the generally accepted notion of a learning object's ""educational value"" is indeed a property that can be reliably assigned through automatic classification. Moreover, an analysis of cross-topic and cross-domain portability shows that the automatic classifier can be ported to other topics and domains, with minimal performance loss. © 2011 ACM.",Computer-assisted education; Identification of educational materials; Natural language processing applications,Classification (of information); Computational linguistics; Computer aided instruction; Information retrieval systems; Natural language processing systems; Automatic classification; Automatic classifiers; Computer-assisted education; Cross-domain; Data sets; Educational materials; Learning objects; NAtural language processing; Performance loss; Education
Semantic relations in bilingual lexicons,2011,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855250465&doi=10.1145%2f2050100.2050102&partnerID=40&md5=c878e16c0923d52c4f79067075a5c38b,"Bilingual lexicons, essential to many NLP applications, can be constructed automatically on the basis of parallel or comparable corpora. In this article, wemake two contributions to their induction from comparable corpora. The first one concerns the creation of these lexicons.We show that seed lexicons can be improved by adding a bootstrapping procedure that uses cross-lingual distributional similarity. The second contribution concerns the evaluation of bilingual lexicons. It is generally based on translation lexicons, which corresponds to the implicit assumption that (cross-lingual) synonymy is the semantic relation of primary interest, even though other semantic relations like (cross-lingual) hyponymy or cohyponymymake up a considerable portion of translation pair candidates proposed by distributional methods. We argue that the focus on synonymy is an oversimplification and that many applications can profit from the inclusion of other semantic relations. We study what effect these semantic relations have on two crosslingual tasks: the cross-lingual projection of polarity scores and the cross-lingual modeling of selectional preferences. We find that the presence of non-synonymous semantic relations may negatively affect the former of these tasks, but benefit the latter. © 2011 ACM.",Bilingual lexicons; Multilingual knowledge induction; Selectional preferences; Semantic relations; Sentiment analysis; Vector space semantics,Natural language processing systems; Profitability; Vector spaces; Bilingual lexicons; Multilingual knowledge induction; Selectional preferences; Semantic relations; Sentiment analysis; Vector space semantics; Semantics
"Speech retrieval from unsegmented finnish audio using statistical morpheme-like units for segmentation, recognition, and retrieval",2011,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80455130027&doi=10.1145%2f2036916.2036917&partnerID=40&md5=df39245bdf514d42bf90f07c27d76d76,"This article examines the use of statistically discovered morpheme-like units for Spoken Document Retrieval (SDR). The morpheme-like units (morphs) are used both for language modeling in speech recognition and as index terms. Traditional word-based methods suffer from out-of-vocabulary words. If a word is not in the recognizer vocabulary, any occurrence of the word in speech will be missing from the transcripts. The problem is especially severe for languages with a high number of distinct word forms such as Finnish. With the morph language model, even previously unseen words can be recognized by identifying its component morphs. Similarly in information retrieval queries, complex word forms, even unseen ones, can be matched to data after segmenting them to morphs. Retrieval performance can be further improved by expanding the transcripts with alternative recognition results from confusion networks. In this article, a novel retrieval evaluation corpus consisting of unsegmented Finnish radio programs, 25 queries and corresponding human relevance assessments was constructed. Previous results on using morphs and confusion networks for Finnish SDR are confirmed and extended to the unsegmented case. As previously, using morphs or base forms as index terms yields about equal performance but combination methods, including a new one, are found to work better than either alone. Using alternative morph segmentations of the query words is found to further improve the results. Lexical similarity-based story segmentation was applied and performance using morphs, base forms, and their combinations was compared for the first time. © 2011 ACM.",Confusion networks; Lattices; Morphemes; Spoken document retrieval; Story segmentation; Subword indexing; Topic segmentation,Computational linguistics; Information retrieval; Natural language processing systems; Query processing; Radio broadcasting; Confusion networks; Morphemes; Spoken document retrieval; Story segmentation; Subwords; Topic segmentation; Speech recognition
My Science Tutor: A conversational multimedia virtual tutor for elementary school science,2011,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052007190&doi=10.1145%2f1998384.1998392&partnerID=40&md5=17015f91bba8cd7310ca73bb304435c5,"This article describes My Science Tutor (MyST), an intelligent tutoring system designed to improve science learning by students in 3rd, 4th, and 5th grades (7 to 11 years old) through conversational dialogs with a virtual science tutor. In our study, individual students engage in spoken dialogs with the virtual tutor Marni during 15 to 20 minute sessions following classroom science investigations to discuss and extend concepts embedded in the investigations. The spoken dialogs in MyST are designed to scaffold learning by presenting open-ended questions accompanied by illustrations or animations related to the classroom investigations and the science concepts being learned. The focus of the interactions is to elicit self-expression from students. To this end, Marni applies some of the principles of Questioning the Author, a proven approach to classroom conversations, to challenge students to think about and integrate new concepts with prior knowledge to construct enriched mental models that can be used to explain and predict scientific phenomena. In this article, we describe how spoken dialogs using Automatic Speech Recognition (ASR) and natural language processing were developed to stimulate students' thinking, reasoning and self explanations. We describe the MyST system architecture and Wizard of Oz procedure that was used to collect data from tutorial sessions with elementary school students. Using data collected with the procedure, we present evaluations of the ASR and semantic parsing components. A formal evaluation of learning gains resulting from system use is currently being conducted. This paper presents survey results of teachers' and children's impressions of MyST. © 2011 ACM.",Avatar; Dialog management; Language model; Semantic parsing,Computational linguistics; Computer aided instruction; Distance education; Education computing; Investments; Natural language processing systems; Scaffolds; School buildings; Semantics; Students; Surveys; Systems analysis; Teaching; Automatic speech recognition; Avatar; Classroom conversation; Dialog management; Elementary schools; Intelligent tutoring system; Language model; Learning gain; Mental model; NAtural language processing; Open-ended questions; Prior knowledge; Science learning; Semantic parsing; System architectures; System use; Virtual tutors; Wizard of Oz; Speech recognition
Two methods for assessing oral reading prosody,2011,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051988806&doi=10.1145%2f1998384.1998388&partnerID=40&md5=ba4bf7a613ea9251dce117ce77a22200,"We compare two types of models to assess the prosody of children's oral reading. Template models measure how well the child's prosodic contour in reading a given sentence correlates in pitch, intensity, pauses, or word reading times with an adult narration of the same sentence. We evaluate template models directly against a common rubric used to assess fluency by hand, and indirectly by their ability to predict fluency and comprehension test scores and gains of 10 children who used Project LISTEN's Reading Tutor; the template models outpredict the human assessment. We also use the same set of adult narrations to train generalized models for mapping text to prosody, and use them to evaluate children's prosody. Using only durational features for both types of models, the generalized models perform better at predicting fluency and comprehension posttest scores of 55 children ages 7-10, with adjusted R2 of 0.6. Such models could help teachers identify which students are making adequate progress. The generalized models have the additional advantage of not requiring an adult narration of every sentence. © 2011 ACM.",Assessment; Children; Intelligent tutoring system; Oral reading fluency; Prosody,Computer aided instruction; Education computing; Assessment; Children; Intelligent tutoring system; Oral reading fluency; Prosody; Rating
Automatically assessing the ABCs: Verification of children's spoken letter-names and letter-sounds,2011,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052004028&doi=10.1145%2f1998384.1998389&partnerID=40&md5=bde346004be62d831e0b01aacb5224a1,"Automatic literacy assessment is an area of research that has shown significant progress in recent years. Technology can be used to automatically administer reading tasks and analyze and interpret children's reading skills. It has the potential to transform the classroom dynamic by providing useful information to teachers in a repeatable, consistent, and affordable way. While most previous research has focused on automatically assessing children reading words and sentences, assessments of children's earlier foundational skills is needed. We address this problem in this research by automatically verifying preliterate children's pronunciations of English letter-names and the sounds each letter represents (letter-sounds). The children analyzed in this study were from a diverse bilingual background and were recorded in actual kindergarten to second grade classrooms. We first manually verified (accept/reject) the letter-name and letter-sound utterances, which serve as the ground-truth in this study. Next, we investigated four automatic verification methods that were based on automatic speech recognition techniques. We attained percent agreement with human evaluations of 90% and 85% for the letter-name and letter-sound tasks, respectively. Humans agree between themselves an average of 95% of the time for both tasks. We discuss the various confounding factors for this assessment task, such as background noise and the presence of disfluencies, that impact automatic verification performance. © 2011 ACM.",Automatic literacy assessment; Children's read speech; Letter-names; Letter-sounds; Pronunciation verification,Rating; Research; School buildings; Teaching; Automatic literacy assessment; Children's read speech; Letter-names; Letter-sounds; Pronunciation verification; Speech recognition
Age and gender detection in the I-DASH project,2011,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051960853&doi=10.1145%2f1998384.1998387&partnerID=40&md5=e88a011a6b08f1f68746c6425a2f7529,"This article presents a description of the INESC-ID Age and Gender classification systems which were developed for aiding the detection of child abuse material within the scope of the European project I-DASH. The Age and Gender classification systems are composed respectively by the fusion of four and six individual subsystems trained with short-and long-term acoustic and prosodic features, different classification strategies, Gaussian Mixture Models-Universal Background Model (GMM-UBM), Multi-Layer Perceptrons (MLP) and Support Vector Machines (SVM), trained over five different speech corpus. The best results obtained by the calibration and linear logistic regression fusion back-end show an absolute improvement of 2% on the unweighted accuracy value for the Age and 1% for the Gender when compared to the best individual frontend systems in the development set. The final age/gender detection system evaluated using a six-hour child abuse (CA) test set achieved promising results given the extremely difficult conditions of this type of video material. In order to further improve the performance in the CA domain, the classification modules were adapted using unsupervised selection of training data. An automatic data selection algorithm using frame-level posterior probabilities was developed. Performance improvement after adapting the classification modules was around 10% relative when compared with the baseline classifiers. © 2011 ACM.",Age; Fusion of acoustic and prosodic features; Gender,Data reduction; Pattern recognition systems; Support vector machines; Age; Background model; Child abuse; Data Selection; Detection system; European project; Gaussian mixtures; Gender; Gender classification; Gender detection; Logistic regressions; Multi-layer perceptrons; Performance improvements; Posterior probability; Prosodic features; Speech corpora; Test sets; Training data; Video material; Speech recognition
Introduction to the special issue on speech and language processing of children's speech for child-machine interaction applications,2011,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051961203&doi=10.1145%2f1998384.1998385&partnerID=40&md5=87260ccfae0283bf926b8c874a930997,[No abstract available],,
FLORA: Fluent oral reading assessment of children's speech,2011,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052010602&doi=10.1145%2f1998384.1998390&partnerID=40&md5=05d54a1264e1b1c30afea46d458f40d4,"We present initial results of FLORA, an accessible computer program that uses speech recognition to provide an accurate measure of children's oral reading ability. FLORA presents grade-level text passages to children, who read the passages out loud, and computes the number of words correct per minute (WCPM), a standard measure of oral reading fluency. We describe the main components of the FLORA program, including the system architecture and the speech recognition subsystems. We compare results of FLORA to human scoring on 783 recordings of grade level text passages read aloud by first through fourth grade students in classroom settings. On average, FLORA WCPM scores were within 3 to 4 words of human scorers across students in different grade levels and schools. © 2011 ACM.",Fluency assessment; Oral reading fluency; Reading tracker; Speech recognition; Speech verification,Character recognition; Teaching; Classroom settings; Computer program; Fluency assessment; Human scorers; Main component; Oral reading fluency; Reading tracker; System architectures; Speech recognition
An automatic version of a reading disorder test,2011,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051977347&doi=10.1145%2f1998384.1998391&partnerID=40&md5=f6a89329d36617aed1e2fd5decbec3bd,"We present a novel system to automatically diagnose reading disorders. The system is based on a speech recognition engine with a module for prosodic analysis. The reading disorder test is based on eight different subtests. In each of the subtests, the system achieves a recognition accuracy of at least 95%. As in the perceptual version of the test, the results of the subtests are then joined into a final test result to determine whether the child has a reading disorder. In the final classification stage, the system identifies 98.3% of the 120 children correctly. In the future, our system will facilitate the clinical evaluation of reading disorders. © 2011 ACM.",Automatic reading assessment; Automatic speech processing; Reading disorders,Speech processing; Automatic reading; Automatic speech processing; Clinical evaluation; Reading disorders; Recognition accuracy; Speech recognition engine; Test results; Speech recognition
Tandem decoding of children's speech for keyword detection in a child-robot interaction scenario,2011,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051977348&doi=10.1145%2f1998384.1998386&partnerID=40&md5=32c28c236f908dcd8e21727633eee527,"In this article, we focus on keyword detection in children's speech as it is needed in voice command systems. We use the FAU Aibo Emotion Corpus which contains emotionally colored spontaneous children's speech recorded in a child-robot interaction scenario and investigate various recent keyword spotting techniques. As the principle of bidirectional Long Short-Term Memory (BLSTM) is known to be well-suited for context-sensitive phoneme prediction, we incorporate a BLSTM network into a Tandem model for flexible coarticulation modeling in children's speech. Our experiments reveal that the Tandem model prevails over a triphone-based Hidden Markov Model approach. © 2011 ACM.",Children's speech; Dynamic bayesian networks; Keyword spotting; Long Short-Term Memory,Bayesian networks; Brain; Hidden Markov models; Inference engines; Children's speech; Coarticulation modeling; Context-sensitive; Dynamic bayesian networks; Keyword spotting; Markov model; Short term memory; Voice command; Speech recognition
Spatially-aware dialogue control using hierarchical reinforcement learning,2011,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052069446&doi=10.1145%2f1966407.1966410&partnerID=40&md5=f0b85f27e52e98041e590bce0712ee72,"This article addresses the problem of scalable optimization for spatially-aware dialogue systems. These kinds of systems must perceive, reason, and act about the spatial environment where they are embedded.We formulate the problem in terms of Semi-Markov Decision Processes and propose a hierarchical reinforcement learning approach to optimize subbehaviors rather than full behaviors. Because of the vast number of policies that are required to control the interaction in a dynamic environment (e.g., a dialogue system assisting a user to navigate in a building from one location to another), our learning approach is based on two stages: (a) the first stage learns low-level behavior, in advance; and (b) the second stage learns high-level behavior, in real time. For such a purpose we extend an existing algorithm in the literature of reinforcement learning in order to support reusable policies and therefore to perform fast learning. We argue that our learning approach makes the problem feasible, and we report on a novel reinforcement learning dialogue system that performs a joint optimization between dialogue and spatial behaviors. Our experiments, using simulated and real environments, are based on a text-based dialogue system for indoor navigation. Experimental results in a realistic environment reported an overall user satisfaction result of 89%, which suggests that our proposed approach is attractive for its application in real interactions as it combines fast learning with adaptive and reasonable behavior. © 2011 ACM.",Dialogue optimization; Dialogue systems; Dynamic environments; Hierarchical control; Machine learning; Policy reuse; Reinforcement learning; Route instruction generation; Spatial cognition; System evaluation,Computer software reusability; Hierarchical systems; Learning algorithms; Markov processes; Navigation; Optimization; Speech processing; Dialogue optimization; Dialogue systems; Dynamic environments; Hierarchical control; Machine-learning; Route instruction generation; Spatial cognition; System evaluation; Reinforcement learning
Classifying dialogue in high-dimensional space,2011,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052049673&doi=10.1145%2f1966407.1966413&partnerID=40&md5=d644567a6116a4049bb42fbafb39598f,"The richness of multimodal dialogue makes the space of possible features required to describe it very large relative to the amount of training data. However, conventional classifier learners require large amounts of data to avoid overfitting, or do not generalize well to unseen examples. To learn dialogue classifiers using a rich feature set and fewer data points than features, we apply a recent technique, ℓ1-regularized logistic regression. We demonstrate this approach empirically on real data from Project LISTEN's Reading Tutor, which displays a story on a computer screen and listens to a child read aloud. We train a classifier to predict task completion (i.e., whether the student will finish reading the story) with 71% accuracy on a balanced, unseen test set. To characterize differences in the behavior of children when they choose the story they read, we likewise train and test a classifier that with 73.6% accuracy infers who chose the story based on the ensuing dialogue. Both classifiers significantly outperform baselines and reveal relevant features of the dialogue. © 2011 ACM.",ℓ<sub>1</sub>-regularized logistic regression; Feature engineering; Feature selection; Project LISTEN's Reading Tutor; Spoken dialogue systems; Task completion,Behavioral research; Speech processing; Feature engineering; Logistic regressions; Project LISTEN's Reading Tutor; Spoken dialogue system; Task completion; Regression analysis
Natural actor and belief critic: Reinforcement algorithm for learning parameters of dialogue systems modelled as POMDPs,2011,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052051092&doi=10.1145%2f1966407.1966411&partnerID=40&md5=c7391e712f989c30d090f483c535cf38,"This article presents a novel algorithm for learning parameters in statistical dialogue systems which are modeled as Partially Observable Markov Decision Processes (POMDPs). The three main components of a POMDP dialogue manager are a dialogue model representing dialogue state information; a policy that selects the system's responses based on the inferred state; and a reward function that specifies the desired behavior of the system. Ideally both the model parameters and the policy would be designed to maximize the cumulative reward. However, while there are many techniques available for learning the optimal policy, no good ways of learning the optimal model parameters that scale to real-world dialogue systems have been found yet. The presented algorithm, called the Natural Actor and Belief Critic (NABC), is a policy gradient method that offers a solution to this problem. Based on observed rewards, the algorithm estimates the natural gradient of the expected cumulative reward. The resulting gradient is then used to adapt both the prior distribution of the dialogue model parameters and the policy parameters. In addition, the article presents a variant of the NABC algorithm, called the Natural Belief Critic (NBC), which assumes that the policy is fixed and only the model parameters need to be estimated. The algorithms are evaluated on a spoken dialogue system in the tourist information domain. The experiments show that model parameters estimated to maximize the expected cumulative reward result in significantly improved performance compared to the baseline hand-crafted model parameters. The algorithms are also compared to optimization techniques using plain gradients and state-of-the-art random search algorithms. In all cases, the algorithms based on the natural gradient work significantly better. © 2011 ACM.",POMDP; Reinforcement learning; Spoken dialogue systems,Gradient methods; Learning algorithms; Markov processes; Optimization; Speech processing; Dialogue manager; Dialogue models; Dialogue systems; Information domains; Learning parameters; Main component; Model parameters; Natural gradient; Novel algorithm; Optimal model; Optimal policies; Optimization techniques; Partially observable Markov decision process; Policy gradient methods; POMDP; Prior distribution; Random search algorithm; Reinforcement algorithms; Reward function; Spoken dialogue system; State information; Parameter estimation
Comparing user simulations for dialogue strategy learning,2011,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052072473&doi=10.1145%2f1966407.1966414&partnerID=40&md5=026bf34c42cac4c3507dc203f0bd9ee5,"Recent studies show that user simulations can be used to generate training corpora for learning dialogue strategies automatically. However, it is unclear what type of simulation is most suitable in a particular task setting. We observe that a simulation which generates random behaviors in a restricted way outperforms simulations that mimic human user behaviors in a statistical way. Our finding suggests that we do not always need to construct a realistic user simulation. Since constructing realistic user simulations is not a trivial task, we can save engineering cost by wisely choosing simulation models that are appropriate for our task. © 2011 ACM.",Dialogue strategy learning; Dialogue System; User simulation,Behavioral research; Models; Speech processing; Dialogue strategy; Dialogue systems; Engineering costs; Human users; Learning dialogues; Random behavior; Simulation model; Training corpus; User simulation; Computer simulation
Effective handling of dialogue state in the hidden information state POMDP-based dialogue manager,2011,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052060028&doi=10.1145%2f1966407.1966409&partnerID=40&md5=3e2869822287857fe9accfb0a14621f5,"Effective dialogue management is critically dependent on the information that is encoded in the dialogue state. In order to deploy reinforcement learning for policy optimization, dialogue must be modeled as a Markov Decision Process. This requires that the dialogue statemust encode all relevent information obtained during the dialogue prior to that state. This can be achieved by combining the user goal, the dialogue history, and the last user action to form the dialogue state. In addition, to gain robustness to input errors, dialogue must be modeled as a Partially Observable Markov Decision Process (POMDP) and hence, a distribution over all possible states must be maintained at every dialogue turn. This poses a potential computational limitation since there can be a very large number of dialogue states. The Hidden Information State model provides a principled way of ensuring tractability in a POMDP-based dialogue model. The key feature of this model is the grouping of user goals into partitions that are dynamically built during the dialogue. In this article, we extend this model further to incorporate the notion of complements. This allows for a more complex user goal to be represented, and it enables an effective pruning technique to be implemented that preserves the overall system performance within a limited computational resource more effectively than existing approaches. © 2011 ACM.",Dialogue belief monitoring; Dialogue modelling; Dialogue state representation; POMDP; Reinforcement learning; Spoken dialogue systems,Markov processes; Speech processing; Dialogue belief monitoring; Dialogue modelling; POMDP; Spoken dialogue system; State representation; Reinforcement learning
Modeling spoken decision support dialogue and optimization of its dialogue strategy,2011,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052045386&doi=10.1145%2f1966407.1966415&partnerID=40&md5=6edaec4f5f4a1115bf1f61598c8077b8,"This article presents a user model for user simulation and a system state representation in spoken decision support dialogue systems. When selecting from a group of alternatives, users apply different decision-making criteria with different priorities. At the beginning of the dialogue, however, users often do not have a definite goal or criteria in which they place value, thus they can learn about new features while interacting with the system and accordingly create new criteria. In this article, we present a user model and dialogue state representation that accommodate these patterns by considering the user's knowledge and preferences. To estimate the parameters used in the user model, we implemented a trial sightseeing guidance system, collected dialogue data, and trained a user simulator. Since the user parameters are not observable from the system, the dialogue is modeled as a partially observable Markov decision process (POMDP), and a dialogue state representation was introduced based on the model. We then optimized its dialogue strategy so that users can make better choices. The dialogue strategy is evaluated using a user simulator trained from a large number of dialogues collected using a trial dialogue system. © 2011 ACM.",Decision support systems; Dialoguemanagement; Reinforcement learning; Spoken dialogue systems,Artificial intelligence; Computer simulation; Decision making; Knowledge representation; Markov processes; Mathematical models; Optimization; Speech processing; Decision supports; Dialogue strategy; Dialogue systems; Dialoguemanagement; Guidance system; Partially observable Markov decision process; Spoken dialogue system; State representation; System state; User models; User simulation; Decision support systems
An accuracy-enhanced light stemmer for arabic text,2011,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952319526&doi=10.1145%2f1921656.1921657&partnerID=40&md5=09df75be214e4bde588f8f9a5fbc11c9,"Stemming is a key step in most text mining and information retrieval applications. Information extraction, semantic annotation, as well as ontology learning are but a few examples where using a stemmer is a must. While the use of light stemmers in Arabic texts has proven highly effective for the task of information retrieval, this class of stemmers falls short of providing the accuracy required by many text mining applications. This can be attributed to the fact that light stemmers employ a set of rules that they apply indiscriminately and that they do not address stemming of broken plurals at all, even though this class of plurals is very commonly used in Arabic texts. The goal of this work is to overcome these limitations. The evaluation of the work shows that it significantly improves stemming accuracy. It also shows that by improving stemming accuracy, tasks such as automatic annotation and keyphrase extraction can also be significantly improved. © 2011 ACM.",Arabic; Broken plurals; Heuristic rules; Stemming,Data mining; Natural language processing systems; Ontology; Semantics; Text processing; Arabic; Arabic texts; Automatic annotation; Broken plurals; Heuristic rules; Information Extraction; Keyphrase extraction; Ontology learning; Retrieval applications; Semantic annotations; Set of rules; Stemming; Text mining; Information retrieval
Sample-efficient batch reinforcement learning for dialogue management optimization,2011,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052060715&doi=10.1145%2f1966407.1966412&partnerID=40&md5=f40ef7bf4cc0054c2b5ab198732320b5,"Spoken Dialogue Systems (SDS) are systems which have the ability to interact with human beings using natural language as the medium of interaction. A dialogue policy plays a crucial role in determining the functioning of the dialogue management module. Handcrafting the dialogue policy is not always an option, considering the complexity of the dialogue task and the stochastic behavior of users. In recent years approaches based on Reinforcement Learning (RL) for policy optimization in dialogue management have been proved to be an efficient approach for dialogue policy optimization. Yet most of the conventional RL algorithms are data intensive and demand techniques such as user simulation. Doing so, additional modeling errors are likely to occur. This paper explores the possibility of using a set of approximate dynamic programming algorithms for policy optimization in SDS. Moreover, these algorithms are combined to a method for learning a sparse representation of the value function. Experimental results show that these algorithms when applied to dialogue management optimization are particularly sample efficient, since they learn from few hundreds of dialogue examples. These algorithms learn in an off-policy manner, meaning that they can learn optimal policies with dialogue examples generated with a quite simple strategy. Thus they can learn good dialogue policies directly from data, avoiding user modeling errors. © 2011 ACM.",Reinforcement learning; Spoken dialogue systems,Dynamic programming; Speech processing; Stochastic systems; Approximate dynamic programming; Batch reinforcement learning; Dialogue management; Natural languages; Policy optimization; Sparse representation; Spoken dialogue system; Stochastic behavior; Reinforcement learning
Introduction to special issue on machine learning for adaptivity in spoken dialogue systems,2011,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053440838&doi=10.1145%2f1966407.1966408&partnerID=40&md5=75c8b72bd43407354e31d124217a95ad,"An introduction to a special issue on machine learning for adaptivity in spoken dialogue systems is presented. Researchers preparing the introduction to the special issue state that the research in the field of spoken dialogue systems has undergone significant changes over a certain period of time. This transformation has been due to the result of new momentum and fresh insights coming from the investigation of data-driven, statistical machine learning methods in three core areas of dialogue system research. These three core areas of dialogue system research are spoken language understanding, dialogue management, and natural language generation. These methods have the potential of introducing mathematically precise approaches to system design, optimization and evaluation, based on data collected from real user interactions with dialogue systems.",,Artificial intelligence; Metadata; Natural language processing systems; Speech processing; Speech recognition; Dialogue management; Dialogue systems; Natural language generation; On-machines; Spoken dialogue system; Spoken language understanding; Statistical machine learning; User interaction; Learning systems
"Brains, not brawn: The use of ""smart"" comparable corpora in bilingual terminology mining",2010,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958030314&doi=10.1145%2f1839478.1839479&partnerID=40&md5=8221277e6319bb9610201cadd4bc5949,"Current research in text mining favors the quantity of texts over their representativeness. But for bilingual terminologymining, and for many language pairs, large comparable corpora are not available. More importantly, as terms are defined vis-à-vis a specific domain with a restricted register, it is expected that the representativeness rather than the quantity of the corpus matters more in terminology mining. Our hypothesis, therefore, is that the representativeness of the corpus is more important than the quantity and ensures the quality of the acquired terminological resources. This article tests this hypothesis on a French-Japanese bilingual term extraction task. To demonstrate how important the type of discourse is as a characteristic of the comparable corpora, we used a state-of-the-art multilingual terminology mining chain composed of two extraction programs, one in each language, and an alignment program. We evaluated the candidate translations using a reference list, and found that taking discourse type into account resulted in candidate translations of a better quality even when the corpus size was reduced by half. © 2010 ACM.",Comparable corpora; Lexical alignment; Terminology mining,Alignment; Terminology; Translation (languages); Comparable corpora; Corpus size; Current researches; Language pairs; Lexical alignment; Reference list; Term extraction; Terminology mining; Text mining; Software agents
Confidence-based stopping criteria for active learning for data annotation,2010,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953755634&doi=10.1145%2f1753783.1753784&partnerID=40&md5=fe119d473342331b4bb8f9a16adddf17,"The labor-intensive task of labeling data is a serious bottleneck for many supervised learning approaches for natural language processing applications. Active learning aims to reduce the human labeling cost for supervised learning methods. Determiningwhen to stop the active learning process is a very important practical issue in real-world applications. This article addresses the stopping criterion issue of active learning, and presents four simple stopping criteria based on confidence estimation over the unlabeled data pool, including maximum uncertainty, overall uncertainty, selected accuracy, and minimum expected error methods. Further, to obtain a proper threshold for a stopping criterion in a specific task, this article presents a strategy by considering the label change factor to dynamically update the predefined threshold of a stopping criterion during the active learning process. To empirically analyze the effectiveness of each stopping criterion for active learning, we design several comparison experiments on seven real-world datasets for three representative natural language processing applications such as word sense disambiguation, text classification and opinion analysis. © 2010 ACM 1550-4875/2010/04-ART3 $10.00.",Active learning; Confidence estimation; Stopping criterion; Text classification; Uncertainty sampling; Word sense disambiguation,Classification (of information); Computational linguistics; Cost reduction; Estimation; Supervised learning; Text processing; Active Learning; Confidence estimation; Stopping criteria; Text classification; Word Sense Disambiguation; Natural language processing systems
Extrinsic summarization evaluation: A decision audit task,2009,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350335794&doi=10.1145%2f1596517.1596518&partnerID=40&md5=e4f51c6c42fd64ca1a804ba49b162b68,"In this work we describe a large-scale extrinsic evaluation of automatic speech summarization technologies for meeting speech. The particular task is a decision audit, wherein a user must satisfy a complex information need, navigating several meetings in order to gain an understanding of how and why a given decision was made. We compare the usefulness of extractive and abstractive technologies in satisfying this information need, and assess the impact of automatic speech recognition (ASR) errors on user performance. We employ several evaluation methods for participant performance, including post-questionnaire data, human subjective and objective judgments, and a detailed analysis of participant browsing behavior. We find that while ASR errors affect user satisfaction on an information retrieval task, users can adapt their browsing behavior to complete the task satisfactorily. Results also indicate that users consider extractive summaries to be intuitive and useful tools for browsing multimodal meeting data. We discuss areas in which automatic summarization techniques can be improved in comparison with gold-standard meeting abstracts. © 2009 ACM.",Abstraction; Browsing; Evaluation; Extraction; Interfaces; Summarization,Abstracting; Errors; Information services; Remelting; Abstraction; Browsing; Evaluation; Interfaces; Summarization; Speech recognition
Improving text categorization bootstrapping via unsupervised learning,2009,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350339784&doi=10.1145%2f1596515.1596516&partnerID=40&md5=7e98e7ac2041393050f2b31a9ccefa68,"We propose a text-categorization bootstrapping algorithm in which categories are described by relevant seed words. Our method introduces two unsupervised techniques to improve the initial categorization step of the bootstrapping scheme: (i) using latent semantic spaces to estimate the similarity among documents and words, and (ii) the Gaussian mixture algorithm, which differentiates relevant and nonrelevant category information using statistics from unlabeled examples. In particular, this second step maps the similarity scores to class posterior probabilities, and therefore reduces sensitivity to keyword-dependent variations in scores. The algorithm was evaluated on two text categorization tasks, and obtained good performance using only the category names as initial seeds. In particular, the performance of the proposed method proved to be equivalent to a pure supervised approach trained on 70 - 160 labeled documents per category. © 2009 ACM.",Bootstrapping; Text categorization; Unsupervised machine learning,Robot learning; Sensitivity analysis; Bootstrapping; Bootstrapping algorithm; Bootstrapping scheme; Gaussian mixtures; Labeled documents; Latent semantics; Posterior probability; Seed words; Similarity scores; Text categorization; Unsupervised machine learning; Text processing
A comprehensive comparative evaluation of RST-based summarization methods,2009,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952927125&doi=10.1145%2f1767756.1767757&partnerID=40&md5=905f8535b72372231f4f9adf9f20a157,"Motivated by governmental, commercial and academic interests, and due to the growing amount of information, mainly online, automatic text summarization area has experienced an increasing number of researches and products, which led to a countless number of summarization methods. In this paper, we present a comprehensive comparative evaluation of the main automatic text summarization methods based on Rhetorical Structure Theory (RST), claimed to be among the best ones. We compare our results to superficial summarizers, which belong to a paradigm with severe limitations, and to hybrid methods, combining RST and superficial methods. We also test voting systems and machine learning techniques trained on RST features. We run experiments for English and Brazilian Portuguese languages and compare the results obtained by using manually and automatically parsed texts. Our results systematically show that all RST methods have comparable overall performance and that they outperform most of the superficial methods. Machine learning techniques achieved high accuracy in the classification of text segments worth of being in the summary, but were not able to produce more informative summaries than the regular RST methods. © 2010 ACM.",Rhetorical structure theory; Text summarization,Abstracting; Learning algorithms; Learning systems; Voting machines; Amount of information; Automatic text summarization; Comparative evaluations; Hybrid method; Machine learning techniques; Portuguese languages; Rhetorical structure theory; Text segments; Text summarization; Voting systems; Text processing
Summarization system evaluation revisited: N-gram graphs,2008,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-55349091998&doi=10.1145%2f1410358.1410359&partnerID=40&md5=80a880b7d51b76d473913ff6cb0099bc,"This article presents a novel automatic method (AutoSummENG) for the evaluation of summarization systems, based on comparing the character n-gram graphs representation of the extracted summaries and a number of model summaries. The presented approach is language neutral, due to its statistical nature, and appears to hold a level of evaluation performance that matches and even exceeds other contemporary evaluation methods. Within this study, we measure the effectiveness of different representation methods, namely, word and character n-gram graph and histogram, different n-gram neighborhood indication methods as well as different comparison methods between the supplied representations. A theory for the a priori determination of the methods' parameters along with supporting experiments concludes the study to provide a complete alternative to existing methods concerning the automatic summary system evaluation process.",Centering theory; Corpus statistics; Discourse analysis; Discourse salience; Game theory; Game-theoretic pragmatics; Meaning game; Perceptual utility; Pronominalization; Reference probability; Referential coherence,Graph theory; Centering theory; Corpus statistics; Discourse analysis; Discourse salience; Game-theoretic pragmatics; Meaning game; Perceptual utility; Pronominalization; Reference probability; Referential coherence; Game theory
A game-theoretic model of referential coherence and its empirical verification using large Japanese and English corpora,2008,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-55349143163&doi=10.1145%2f1410358.1410360&partnerID=40&md5=16f65334bb208966017b0156a8aaa1d6,"Referential coherence represents the smoothness of discourse resulting from topic continuity and pronominalization. Rational individuals prefer a referentially coherent structure of discourse when they select a language expression and its interpretation. This is a preference for cooperation in communication. By what principle do they share coherent expressions and interpretations? Centering theory is the standard theory of referential coherence [Grosz et al. 1995]. Although it is well designed on the bases of first-order inference rules [Joshi and Kuhn 1979], it does not embody a behavioral principle for the cooperation evident in communication. Hasida [1996] proposed a game-theoretic hypothesis in relation to this issue. We aim to empirically verify Hasida's hypothesis by using corpora of multiple languages. We statistically design language-dependent parameters by using a corpus of the target language. This statistical design enables us to objectively absorb language-specific differences and to verify the universality of Hasida's hypothesis by using corpora. We empirically verified our model by using large Japanese and English corpora. The result proves the language universality of the hypothesis.",Corpus statistics; Discourse analysis; Discourse salience; Game theory; Game-theoretic pragmatics; Meaning game; Perceptual utility; Pronominalization; Reference probability; Referential coherence,Coherent light; Gas dynamic lasers; Information retrieval systems; Linguistics; Query languages; Software agents; Corpus statistics; Discourse analysis; Discourse salience; Game-theoretic pragmatics; Meaning game; Perceptual utility; Pronominalization; Reference probability; Referential coherence; Game theory
Chinese word segmentation and statistical machine translation,2008,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-44849108434&doi=10.1145%2f1363108.1363109&partnerID=40&md5=226aea56d768a8ffea4a853677ddbdc3,"Chinese word segmentation (CWS) is a necessary step in Chinese-English statistical machine translation (SMT) and its performance has an impact on the results of SMT. However, there are many choices involved in creating a CWS system such as various specifications and CWS methods. The choices made will create a new CWS scheme, but whether it will produce a superior or inferior translation has remained unknown to date. This article examines the relationship between CWS and SMT. The effects of CWS on SMT were investigated using different specifications and CWS methods. Four specifications were selected for investigation: Beijing University (PKU), Hong Kong City University (CITYU), Microsoft Research (MSR), and Academia SINICA (AS). We created 16 CWS schemes under different settings to examine the relationship between CWS and SMT. Our experimental results showed that the MSR's specifications produced the lowest quality translations. In examining the effects of CWS methods, we tested dictionary-based and CRF-based approaches and found there was no significant difference between the two in the quality of the resulting translations. We also found the correlation between the CWS F-score and SMT BLEU score was very weak. We analyzed CWS errors and their effect on SMT by evaluating systems trained with and without these errors. This article also proposes two methods for combining advantages of different specifications: a simple concatenation of training data and a feature interpolation approach in which the same types of features of translation models from various CWS schemes are linearly interpolated. We found these approaches were very effective in improving the quality of translations. © 2008 ACM.",Chinese word segmentation; Linear integration; Statistical machine translation; Translation model,Computer aided language translation; Computer networks; Errors; Information theory; Interpolation; Linguistics; Mathematical models; Numerical analysis; Photoacoustic effect; Specifications; Statistical methods; Statistics; Support vector machines; Surface mount technology; Translation (languages); Beijing; Chinese word segmentation (CWS); City University; Experimental results; F score; Hong Kong; Microsoft Research (CO); Statistical Machine Translation (SMT); Training data; Translation models; Speech transmission
Web resources for language modeling in conversational speech recognition,2007,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37849007170&doi=10.1145%2f1322391.1322392&partnerID=40&md5=bf29d3e58b185e685adb534094d9b908,"This article describes a methodology for collecting text from the Web to match a target sublanguage both in style (register) and topic. Unlike other work that estimates n-gram statistics from page counts, the approach here is to select and filter documents, which provides more control over the type of material contributing to the n-gram counts. The data can be used in a variety of ways; here, the different sources are combined in two types of mixture models. Focusing on conversational speech where data collection can be quite costly, experiments demonstrate the positive impact of Web collections on several tasks with varying amounts of data, including Mandarin and English telephone conversations and English meetings and lectures. © 2007 ACM.",Conversational speech; Language modeling; Web data,Data acquisition; Mathematical models; Natural language processing systems; Text processing; Web services; Conversational speech; Language modeling; Web data; Speech recognition
Relation extraction and the influence of automatic named-entity recognition,2007,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37849027331&doi=10.1145%2f1322391.1322393&partnerID=40&md5=14c51d2ac18a1fea013e0f858ec487af,"We present an approach for extracting relations between named entities from natural language documents. The approach is based solely on shallow linguistic processing, such as tokenization, sentence splitting, part-of-speech tagging, and lemmatization. It uses a combination of kernel functions to integrate two different information sources: (i) the whole sentence where the relation appears, and (ii) the local contexts around the interacting entities. We present the results of experiments on extracting five different types of relations from a dataset of newswire documents and show that each information source provides a useful contribution to the recognition task. Usually the combined kernel significantly increases the precision with respect to the basic kernels, sometimes at the cost of a slightly lower recall. Moreover, we performed a set of experiments to assess the influence of the accuracy of named-entity recognition on the performance of the relation-extraction algorithm. Such experiments were performed using both the correct named entities (i.e., those manually annotated in the corpus) and the noisy named entities (i.e., those produced by a machine learning-based named-entity recognizer). The results show that our approach significantly improves the previous results obtained on the same dataset. © 2007 ACM.",Information extraction; Kernel methods; Named-entity recognition; Relation extraction,Computational grammars; Database systems; Feature extraction; Learning algorithms; Information extraction; Kernel methods; Named entity recognition; Relation extraction; Natural language processing systems
Morph-based speech recognition and modeling of out-of-vocabulary words across languages,2007,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37849048345&doi=10.1145%2f1322391.1322394&partnerID=40&md5=232ebfc210de1d7781271b9874602219,"We explore the use of morph-based language models in large-vocabulary continuous-speech recognition systems across four so-called morphologically rich languages: Finnish, Estonian, Turkish, and Egyptian Colloquial Arabic. The morphs are subword units discovered in an unsupervised, data-driven way using the Morfessor algorithm. By estimating n-gram language models over sequences of morphs instead of words, the quality of the language model is improved through better vocabulary coverage and reduced data sparsity. Standard word models suffer from high out-of-vocabulary (OOV) rates, whereas the morph models can recognize previously unseen word forms by concatenating morphs. It is shown that the morph models do perform fairly well on OOVs without compromising the recognition accuracy on in-vocabulary words. The Arabic experiment constitutes the only exception since here the standard word model outperforms the morph model. Differences in the datasets and the amount of data are discussed as a plausible explanation. © 2007 ACM.",Egyptian Colloquial Arabic; Estonian; Finnish; Highly inflecting and compounding languages; LVCSR; Morfessor; Morpheme; Morphologically rich languages; N-gram models; Subword-based language modeling; Turkish,Algorithms; Computer simulation languages; Database systems; Natural language processing systems; Word processing; Egyptian Colloquial Arabic languages; Estonian languages; Highly inflecting and compounding languages; Morpheme; Morphologically rich languages; N-gram models; Subword based language modeling; Speech recognition
An unsupervised method for learning generation dictionaries for spoken dialogue systems by mining user reviews,2007,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148882678&doi=10.1145%2f1289600.1289601&partnerID=40&md5=48ac4d61df585457b3b83a21a70f9ec3,Spoken language generation for dialogue systems requires a dictionary of mappings between the semantic representations of concepts that the system wants to express and the realizations of those concepts. Dictionary creation is a costly process; it is currently done by hand for each dialogue domain. We propose a novel unsupervised method for learning such mappings from user reviews in the target domain and test it in the restaurant and hotel domains. Experimental results show that the acquired mappings achieve high consistency between the semantic representation and the realization and that the naturalness of the realization is significantly higher than the baseline. © 2007 ACM.,Generation dictionary; Natural language generation; Spoken dialogue systems; User reviews,Semantics; Speech processing; User interfaces; Generation dictionary; Natural language generation; Spoken dialogue systems; User reviews; Learning systems
Promoting extension and reuse in a spoken dialog manager: An evaluation of the queen's communicator,2007,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547239167&doi=10.1145%2f1255171.1255173&partnerID=40&md5=515d267623ec5dd3e3eadb6117d3ec85,"This article describes how an object-oriented approach can be applied to the architectural design of a spoken language dialog system with the aim of facilitating the modification, extension, and reuse of discourse-related expertise. The architecture of the developed system is described and a functionally similar VoiceXML system is used to provide a comparative baseline across a range of modification and reuse scenarios. It is shown that the use of an object-oriented dialog manager can provide a capable means of reusing existing discourse expertise in a manner that limits the degree of structural decay associated with system change. © 2007 ACM.",Dialog management; Human-computer interaction; Speech and language processing; Spoken dialog systems,Natural language processing systems; Object oriented programming; Software architecture; Software design; XML; Dialog management; Object oriented dialog managers; Spoken language dialog systems; Human computer interaction
A block bigram prediction model for statistical machine translation,2007,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547369506&doi=10.1145%2f1255171.1255172&partnerID=40&md5=d14f9880c9235bbf22190bdb4a5050ec,"In this article, we present a novel training method for a localized phrase-based prediction model for statistical machine translation (SMT). The model predicts block neighbors to carry out a phrase-based translation that explicitly handles local phrase reordering. We use a maximum likelihood criterion to train a log-linear block bigram model which uses real-valued features (e.g., a language model score) as well as binary features based on the block identities themselves (e.g., block bigram features). The model training relies on an efficient enumeration of local block neighbors in parallel training data. A novel stochastic gradient descent (SGD) training algorithm is presented that can easily handle millions of features. Moreover, when viewing SMT as a block generation process, it becomes quite similar to sequential natural language annotation problems such as part-of-speech tagging, phrase chunking, or shallow parsing. Our novel approach is successfully tested on a standard Arabic-English translation task using two different phrase reordering models: a block orientation model and a phrase-distortion model. © 2007 ACM.",Machine learning; Maximum entropy; Statistical machine translation; Stochastic gradient descent,Formal languages; Gradient methods; Learning systems; Mathematical models; Natural language processing systems; Problem solving; Statistical methods; Block generation; Maximum entropy; Statistical machine translation; Stochastic gradient descent (SGD); Automatic programming
Chinese semantic dependency analysis: Construction of a treebank and its use in classification,2007,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34249307264&doi=10.1145%2f1233912.1233914&partnerID=40&md5=858cf6cd899bbccfff5eaf04589d74d9,"Semantic analysis is a standard tool in the Natural Language Processing (NLP) toolbox with widespread applications. In this article, we look at tagging part of the Penn Chinese Treebank with semantic dependency. Then we take this tagged data to train a maximum entropy classifier to label the semantic relations between headwords and dependents to perform semantic analysis on Chinese sentences. The classifier was able to achieve an accuracy of over 84%. We then analyze the errors in classification to determine the problems and possible solutions for this type of semantic analysis. © 2007 ACM.",Chinese; maximum entropy classifiation; Natural language processing; semantic dependency analysis,Classification (of information); Error analysis; Problem solving; Semantics; Chinese sentences; Maximum entropy classifiation; Semantic analysis; Semantic dependency analysis; Natural language processing systems
The Pyramid Method: Incorporating human content selection variation in summarization evaluation,2007,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34249275304&doi=10.1145%2f1233912.1233913&partnerID=40&md5=1547c0b6c4a14b146cd9c4a0375580ff,"Human variation in content selection in summarization has given rise to some fundamental research questions: How can one incorporate the observed variation in suitable evaluation measures How can such measures reflect the fact that summaries conveying different content can be equally good and informative In this article, we address these very questions by proposing a method for analysis of multiple human abstracts into semantic content units. Such analysis allows us not only to quantify human variation in content selection, but also to assign empirical importance weight to different content units. It serves as the basis for an evaluation method, the Pyramid Method, that incorporates the observed variation and is predictive of different equally informative summaries. We discuss the reliability of content unit annotation, the properties of Pyramid scores, and their correlation with other evaluation methods. © 2007 ACM.",Evaluation; Semantic analysis; Summarization,Abstracting; Correlation methods; Glossaries; Semantics; Content selection; Pyramid Methods; Semantic analysis; Summarization evaluation; Natural language processing systems
Author Verification by Linguistic Profiling: An Exploration of the Parameter Space,2007,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894211536&doi=10.1145%2f1187415.1187416&partnerID=40&md5=2752281b14f807b44dba062a84f46354,"This article explores the effects of parameter settings in linguistic profiling, a technique in which large numbers of counts of linguistic features are used as a text profile which can then be compared to average profiles for groups of texts. Although the technique proves to be quite effective for authorship verification, with the best overall parameter settings yielding an equal error rate of 3% on a test corpus of student essays, the optimal parameters vary greatly depending on author and evaluation criterion. © 2009, ACM. All rights reserved.",Algorithms; Authorship attribution; authorship recognition; authorship verification; Experimentation; machine learning; Security,
Unsupervised models for morpheme segmentation and morphology learning,2007,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846987588&doi=10.1145%2f1217098.1217101&partnerID=40&md5=61dfecf59066bd14451998cd0e072f7d,"We present a model family called Morfessor for the unsupervised induction of a simple morphology from raw text data. The model is formulated in a probabilistic maximum a posteriori framework. Morfessor can handle highly inflecting and compounding languages where words can consist of lengthy sequences of morphemes. A lexicon of word segments, called morphs, is induced from the data. The lexicon stores information about both the usage and form of the morphs. Several instances of the model are evaluated quantitatively in a morpheme segmentation task on different sized sets of Finnish as well as English data. Morfessor is shown to perform very well compared to a widely known benchmark algorithm, in particular on Finnish data. © 2007 ACM.",Efficient storage; Highly inflecting and compounding languages; Language independent methods; Maximum a posteriori (MAP) estimation; Morpheme lexicon and segmentation; Unsupervised learning,Algorithms; Benchmarking; Formal languages; Parameter estimation; Text processing; Highly inflecting and compounding languages; Language independent methods; Maximum a posteriori (MAP) estimation; Morpheme lexicon; Unsupervised learning; Learning systems
A statistical model for near-synonym choice,2007,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846979170&doi=10.1145%2f1217098.1217100&partnerID=40&md5=7562bab5ffd2df87549275be3b8bf32a,We present an unsupervised statistical method for automatic choice of near-synonyms when the context is given. The method uses the Web as a corpus to compute scores based on mutual information. Our evaluation experiments show that this method performs better than two previous methods on the same task. We also describe experiments in using supervised learning for this task. We present an application to an intelligent thesaurus. This work is also useful in machine translation and natural language generation. © 2007 ACM.,Intelligent thesaurus; Lexical choice; Near-synonyms; Semantic similarity; Web as a corpus,Computer aided language translation; Natural language processing systems; Semantics; Statistical methods; Thesauri; World Wide Web; Intelligent thesaurus; Lexical choice; Near-synonyms; Semantic similarity; Web as a corpus; Text processing
Unsupervised Models for Morpheme Segmentation and Morphology Learning,2007,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007276519&doi=10.1145%2f1187415.1187418&partnerID=40&md5=953f9ad51cbfca255eeb59ef31547977,"We present a model family called Morfessor for the unsupervised induction of a simple morphology from raw text data. The model is formulated in a probabilistic maximum a posteriori framework. Morfessor can handle highly inflecting and compounding languages where words can consist of lengthy sequences of morphemes. A lexicon of word segments, called morphs, is induced from the data. The lexicon stores information about both the usage and form of the morphs. Several instances of the model are evaluated quantitatively in a morpheme segmentation task on different sized sets of Finnish as well as English data. © 2009, ACM. All rights reserved.",Algorithms; Efficient storage; Experimentation; highly inflecting and compounding languages; language independent methods; Languages; maximum a posteriori (MAP) estimation; morpheme lexicon and segmentation; Performance; unsupervised learning,
Author verification by linguistic profiling: An exploration of the parameter space,2007,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846949415&doi=10.1145%2f1217098.1217099&partnerID=40&md5=d80072e4bbeed3673a9dee9ae2349e47,"This article explores the effects of parameter settings in linguistic profiling, a technique in which large numbers of counts of linguistic features are used as a text profile which can then be compared to average profiles for groups of texts. Although the technique proves to be quite effective for authorship verification, with the best overall parameter settings yielding an equal error rate of 3% on a test corpus of student essays, the optimal parameters vary greatly depending on author and evaluation criterion. © 2007 ACM.",Authorship attribution; Authorship recognition; Authorship verification; Machine learning,Learning systems; Parameter estimation; Text processing; Authorship attribution; Authorship recognition; Authorship verification; Linguistics
Adaptive Text Correction with Web-Crawled Domain-Dependent Dictionaries,2007,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70450268708&doi=10.1145%2f1289600.1289602&partnerID=40&md5=018417978d05209f2b92f0b5e64599e7,"For the success of lexical text correction, high coverage of the underlying background dictionary is crucial. Still, most correction tools are built on top of static dictionaries that represent fixed collections of expressions of a given language. When treating texts from specific domains and areas, often a significant part of the vocabulary is missed. In this situation, both automated and interactive correction systems produce suboptimal results. In this article, we describe strategies for crawling Web pages that fit the thematic domain of the given input text. Special filtering techniques are introduced to avoid pages with many orthographic errors. Collecting the vocabulary of filtered pages that meet the vocabulary of the input text, dynamic dictionaries of modest size are obtained that reach excellent coverage values. A tool has been developed that automatically crawls dictionaries in the indicated way. Our correction experiments with crawled dictionaries, which address English and German document collections from a variety of thematic fields, show that with these dictionaries even the error rate of highly accurate texts can be reduced, using completely automated correction methods. For interactive text correction, more sensible candidate sets for correcting erroneous words are obtained and the manual effort is reduced in a significant way. To complete this picture, we study the effect when using word trigram models for correction. Again, trigram models from crawled corpora outperform those obtained from static corpora. © 2007, ACM. All rights reserved.",Adaptive techniques; Algorithms; dictionaries; domains; error correction; Experimentation; Languages; Web crawling,
A Statistical Model for Near-Synonym Choice,2007,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053400744&doi=10.1145%2f1187415.1187417&partnerID=40&md5=f48b2f597af70490f67020544d5ec1f3,"We present an unsupervised statistical method for automatic choice of near-synonyms when the context is given. The method uses the Web as a corpus to compute scores based on mutual information. Our evaluation experiments show that this method performs better than two previous methods on the same task. We also describe experiments in using supervised learning for this task. We present an application to an intelligent thesaurus. This work is also useful in machine translation and natural language generation. © 2009, ACM. All rights reserved.",Algorithms; intelligent thesaurus; Languages; Lexical choice; near-synonyms; Performance; semantic similarity; Web as a corpus,
"One story, one flow: Hidden markov story models for multilingual multidocument summarization",2006,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-42949097256&doi=10.1145%2f1149290.1151099&partnerID=40&md5=6218e9ae9bd2be61ccfe3a98790fcfb0,"This article presents a multidocument, multilingual, theme-based summarization system based on modeling text cohesion (story flow). Conventional extractive summarization systems which pick out salient sentences to include in a summary often disregard any flow or sequence that might exist between these sentences. We argue that such inherent text cohesion exists and is (1) specific to a particular story and (2) specific to a particular language. Documents within the same story, and in the same language, share a common story flow, and this flow differs across stories, and across languages. We propose using Hidden Markov Models (HMMs) as story models. An unsupervised segmental K-means method is used to iteratively cluster multiple documents into different topics (stories) and learn the parameters of parallel Hidden Markov Story Models (HMSM), one for each story. We compare story models within and across stories and within and across languages (English and Chinese). The experimental results support our “one story, one flow” and “one language, one flow” hypotheses. We also propose a Naïve Bayes classifier for document summarization. The performance of our summarizer is superior to conventional methods that do not incorporate text cohesion information. Our HMSM method also provides a simple way to compile a single metasummary for multiple documents from individual summaries via state labeled sentences. © 2006 ACM",Hidden Markov models; Multilingual document summarization,Iterative methods; K-means clustering; Conventional methods; Document summarization; Extractive summarizations; Hidden markov models (HMMs); Multi-document summarization; Multilingual documents; Multiple documents; Summarization systems; Hidden Markov models
Automatic expansion of domain-specific lexicons by term categorization,2006,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746443840&doi=10.1145%2f1138379.1138380&partnerID=40&md5=19935b69628d6cc6dab25abb6f4d435f,"We discuss an approach to the automatic expansion of domain-specific lexicons, that is, to the problem of extending, for each ci in a predefined set C = {c1 , . . . , cm} of semantic domains, an initial lexicon L0i into a larger lexicon L 1i. Our approach relies on term categorization, defined as the task of labeling previously unlabeled terms according to a predefined set of domains. We approach this as a supervised learning problem in which term classifiers are built using the initial lexicons as training data. Dually to classic text categorization tasks in which documents are represented as vectors in a space of terms, we represent terms as vectors in a space of documents. We present the results of a number of experiments in which we use a boosting-based learning device for training our term classifiers. We test the effectiveness of our method by using WordNetDomains, a wellknown large set of domain-specific lexicons, as a benchmark. Our experiments are performed using the documents in the Reuters Corpus Volume 1 as implicit representations for our terms. © 2006 ACM.",Lexicons; Machine learning; Text classification,Automation; Benchmarking; Information retrieval systems; Learning systems; Problem solving; Speech recognition; Vectors; Word processing; Domain-specific lexicons; Lexicons; Term categorization; Text classification; Semantics
Acoustic environment classification,2006,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746905459&doi=10.1145%2f1149290.1149292&partnerID=40&md5=b893e491620336f9451e5966276b872c,"The acoustic environment provides a rich source of information on the types of activity, communication modes, and people involved in many situations. It can be accurately classified using recordings from microphones commonly found in PDAs and other consumer devices. We describe a prototype HMM-based acoustic environment classifier incorporating an adaptive learning mechanism and a hierarchical classification model. Experimental results show that we can accurately classify a wide variety of everyday environments. We also show good results classifying single sounds, although classification accuracy is influenced by the granularity of the classification. © 2006 ACM.",Sound classification,Acoustic signal processing; Adaptive systems; Hierarchical systems; Learning systems; Microphones; Software prototyping; Communication modes; Consumer devices; Hierarchical classification models; Sound classification; Classification (of information)
Confidence estimation for NLP applications,2006,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750921794&doi=10.1145%2f1177055.1177057&partnerID=40&md5=c26d30c79ca638382f061034c75414c7,"Confidence measures are a practical solution for improving the usefulness of Natural Language Processing applications. Confidence estimation is a generic machine learning approach for deriving confidence measures. We give an overview of the application of confidence estimation in various fields of Natural Language Processing, and present experimental results for speech recognition, spoken language understanding, and statistical machine translation. © 2006 ACM.",Computer-aided machine translation; Confidence estimation; Confidence measures; Speech recognition; Spoken language understanding; Statistical machine learning,Learning systems; Parameter estimation; Problem solving; Speech recognition; Statistical mechanics; Computer aided machine translation; Confidence estimation; Spoken language understanding; Statistical machine translation; Natural language processing systems
An active approach to spoken language processing,2006,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750924974&doi=10.1145%2f1177055.1177056&partnerID=40&md5=98cf07815ff4b518e5ea52d40de49920,"State of the art data-driven speech and language processing systems require a large amount of human intervention ranging from data annotation to system prototyping. In the traditional supervised passive approach, the system is trained on a given number of annotated data samples and evaluated using a separate test set. Then more data is collected arbitrarily, annotated, and the whole cycle is repeated. In this article, we propose the active approach where the system itself selects its own training data, evaluates itself and re-trains when necessary. We first employ active learning which aims to automatically select the examples that are likely to be the most informative for a given task. We use active learning for both selecting the examples to label and the examples to re-label in order to correct labeling errors. Furthermore, the system automatically evaluates itself using active evaluation to keep track of the unexpected events and decides on-demand to label more examples. The active approach enables dynamic adaptation of spoken language processing systems to unseen or unexpected events for nonstationary input while reducing the manual annotation effort significantly. We have evaluated the active approach with the AT&T spoken dialog system used for customer care applications. In this article, we present our results for both automatic speech recognition and spoken language understanding. © 2006 ACM.",Active evaluation; Active learning; Adaptive learning; Automatic speech recognition; Passive learning; Speech and language processing; Spoken dialog systems; Spoken language understanding; Unsupervised learning,Adaptive systems; Data acquisition; Evaluation; Learning systems; Speech analysis; Speech recognition; Automatic speech recognition; Passive learning; Spoken dialog systems; Spoken language understanding; Unsupervised learning; Natural language processing systems
High-quality speech-to-speech translation for computer-aided language learning,2006,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746864431&doi=10.1145%2f1149290.1149291&partnerID=40&md5=9bbc498bb616efb722813f2d2bddcded,"This article describes our research on spoken language translation aimed toward the application of computer aids for second language acquisition. The translation framework is incorporated into a multilingual dialogue system in which a student is able to engage in natural spoken interaction with the system in the foreign language, while speaking a query in their native tongue at any time to obtain a spoken translation for language assistance. Thus the quality of the translation must be extremely high, but the domain is restricted. Experiments were conducted in the weather information domain with the scenario of a native English speaker learning Mandarin Chinese. We were able to utilize a large corpus of English weather-domain queries to explore and compare a variety of translation strategies: formal, example-based, and statistical. Translation quality was manually evaluated on a test set of 695 spontaneous utterances. The best speech translation performance (89.9% correct, 6.1% incorrect, and 4.0% rejected), is achieved by a system which combines the formal and example-based methods, using parsability by a domain-specific Chinese grammar as a rejection criterion. © 2006 ACM.",Computer-aided language learning; Dialogue systems; Machine translation; Speech translation,Learning systems; Linguistics; Speech processing; Statistical methods; Weather information services; Computer-aided language learning; Dialogue systems; Machine translation; Speech translation; Computer aided language translation
Web-Based Models for Natural Language Processing,2005,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010217018&doi=10.1145%2f1075389.1075392&partnerID=40&md5=184d29c608539030b3d5becdd7a7e32c,"Previous work demonstrated thatWeb counts can be used to approximate bigram counts, suggesting that Web-based frequencies should be useful for a wide variety of Natural Language Processing (NLP) tasks. However, only a limited number of tasks have so far been tested usingWeb-scale data sets. The present article overcomes this limitation by systematically investigating the performance of Web-based models for several NLP tasks, covering both syntax and semantics, both generation and analysis, and a wider range of n-grams and parts of speech than have been previously explored. For the majority of our tasks,we find that simple, unsupervised models perform better when n-gram counts are obtained from theWeb rather than from a large corpus. In some cases, performance can be improved further by using backoff or interpolation techniques that combineWeb counts and corpus counts. However, unsupervisedWeb-based models generally fail to outperform supervised state-of-the-art models trained on smaller corpora. We argue that Web-based models should therefore be used as a baseline for, rather than an alternative to, standard supervised models. © 2005, ACM. All rights reserved.",Algorithms; evaluation; Experimentation; Languages; n-gram models; Web counts,
A Word-to-Phrase Statistical Translation Model,2005,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749665154&doi=10.1145%2f1115686.1115687&partnerID=40&md5=1f83c1170565cf2b24dfd699f88766b3,"This article addresses the development of statistical models for phrase-based machine translation (MT) which extend a popular word-alignment model proposed by IBM in the early 90s. A novel decoding algorithm is directly derived from the optimization criterion which defines the statisticalMT approach. Efficiency in decoding is achieved by applying dynamic programming, pruning strategies, and word reordering constraints. It is known that translation performance can be boosted by exploiting phrase (or multiword) translation pairs automatically extracted from a parallel corpus. New phrase-based models are obtained by introducing extra multiwords in the target language vocabulary and by estimating the corresponding parameters from either: (i) a word-based model, (ii) phrase-based statistics computed on the parallel corpus, or (iii) the interpolation of the two previous estimates. Word-based and phrase-based MT models are evaluated on a traveling domain task in two translation directions: Chinese-English (12k-word vocabulary) and Italian-English (16k-word vocabulary). Phrase-based models show Bleu score improvements over the word-based model by 19% and 13% relative, respectively. © 2005, ACM. All rights reserved.",Algorithms; dynamic programming; Languages; phrase-based translation; phrase-pair extraction; search algorithm; Statistical machine translation,
Towards Efficient Human Machine Speech Communication: The Speech Graffiti Project,2005,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014894017&doi=10.1145%2f1075389.1075391&partnerID=40&md5=c22701435caa3f50d45f0401cf1c6ac4,"This research investigates the design and performance of the Speech Graffiti interface for spoken interaction with simple machines. Speech Graffiti is a standardized interface designed to address issues inherent in the current state-of-the-art in spoken dialog systems such as high word-error rates and the difficulty of developing natural language systems. This article describes the general characteristics of Speech Graffiti, provides examples of its use, and describes other aspects of the system such as the development toolkit. We also present results from a user study comparing Speech Graffiti with a natural language dialog system. These results show that users rated Speech Graffiti significantly better in several assessment categories. Participants completed approximately the same number of tasks with both systems, and although Speech Graffiti users often took more turns to complete tasks than natural language interface users, they completed tasks in slightly less time. © 2005, ACM. All rights reserved.",Design; Experimentation; Human Factors; Human-computer interaction; speech recognition; spoken dialog systems,
Voice Fonts for Individuality Representation and Transformation,2005,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956828655&doi=10.1145%2f1075389.1075393&partnerID=40&md5=193d5216b3ffd790387af7f7f3373a4f,"Speaker individuality transformation is used to modify the speech signal's characteristics so that it sounds as if it is spoken by another speaker. Previous methods for individuality transformation use mapping functions which depend upon a pair of speakers.We introduce the paradigm of voice fonts to represent the individuality of a speaker, independent of other speakers. Several objective and subjective tests are conducted to evaluate the performance of the approaches proposed for the voice fonts paradigm. The results show that the voice fonts paradigm enables independent representation of a speaker's individuality and produces equally good quality of transformed speech compared to previous approaches. This independent representation will be useful in important applications which were not possible with previous methods. © 2005, ACM. All rights reserved.",Experimentation; Performance; Speech individuality; Standardization; voice conversion; voice fonts,
Automatic Summarization of Voicemail Messages Using Lexical and Prosodic Features,2005,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947673332&doi=10.1145%2f1075389.1075390&partnerID=40&md5=0b4029feed7c38895d0d2924fb19e179,"This aticle presents trainable methods for extracting principal content words from voicemail messages. The short text summaries generated are suitable for mobile messaging applications. The system uses a set of classifiers to identify the summary words with each word described by a vector of lexical and prosodic features. We use an ROC-based algorithm, Parcel, to select input features (and classifiers). We have performed a series of objective and subjective evaluations using unseen data from two different speech recognition systems as well as human transcriptions of voicemail speech. © 2005, ACM. All rights reserved.",automatic summarization; feature subset selection; Languages; prosody; receiver operating characteristic; short message service; Voicemail,
Evaluating Discourse Understanding in Spoken Dialogue Systems,2004,ACM Transactions on Speech and Language Processing,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987993997&doi=10.1145%2f1035112.1035113&partnerID=40&md5=6c875dfa2985eaa2effc7e1b9c0a1ce3,"This article describes a method for creating an evaluation measure for discourse understanding in spoken dialogue systems. No well-established measure has yet been proposed for evaluating discourse understanding, which has made it necessary to evaluate it only on the basis of the system's total performance. Such evaluations, however, are greatly influenced by task domains and dialogue strategies. To find a measure that enables good estimation of system performance only from discourse understanding results, we enumerated possible discourse-understanding-related metrics and calculated their correlation with the system's total performance through dialogue experiments. Copyright © 2004, ACM. All rights reserved.",Discourse understanding; evaluation measures; Languages; Measurement; Performance; speech understanding; spoken dialogue systems,
