Title,Year,Source title,Link,Abstract,Author Keywords,Index Keywords
CORDIC-based enhanced systolic array architecture for QR decomposition,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954432413&doi=10.1145%2f2827700&partnerID=40&md5=df63ffb713d511321fa089c8d14eece1,"Multiple input multiple output (MIMO) with orthogonal frequency division multiplexing (OFDM) systems typically use orthogonal-triangular (QR) decomposition. In this article, we present an enhanced systolic array architecture to realize QR decomposition based on the Givens rotation (GR) method for a 4 × 4 real matrix. The coordinate rotation digital computer (CORDIC) algorithm is adopted and modified to speed up and simplify the process of GR. To verify the function and evaluate the performance, the proposed architectures are validated on a Virtex 5 FPGA development platform. Compared to a commercial implementation of vectoring CORDIC, the enhanced vectoring CORDIC is presented that uses 37.7% less hardware resources, dissipates 71.6% less power, and provides a 1.8 times speedup while maintaining the same computation accuracy. The enhanced QR systolic array architecture based on the enhanced vectoring CORDIC saves 24.5% in power dissipation, provides a factor of 1.5-fold improvement in throughput, and the hardware efficiency is improved 1.45-fold with no accuracy penalty when compared to our previously proposed QR systolic array architecture. © 2015 ACM.",Coordinate rotation digital computer (CORDIC); Enhanced systolic array; Givens rotation (GR); Less power; QR decomposition; Virtex 5 FPGA,Computer hardware; Digital computers; Field programmable gate arrays (FPGA); Frequency division multiplexing; Hardware; Local area networks; MIMO systems; Orthogonal frequency division multiplexing; Reconfigurable hardware; Rotation; Co-ordinate rotation digital computers; Givens Rotation; Less power; Q R decomposition; Virtex-5; Systolic arrays
Separation logic for high-level synthesis,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954318387&doi=10.1145%2f2836169&partnerID=40&md5=da2935f2558e802799f07eff650b1741,"High-Level Synthesis (HLS) promises a significant shortening of the FPGA design cycle by raising the abstraction level of the design entry to high-level languages such as C/C++. However, applications using dynamic, pointer-based data structures and dynamic memory allocation remain difficult to implement well, yet such constructs are widely used in software. Automated optimizations that leverage the memory bandwidth of FPGAs by distributing the application data over separate banks of on-chip memory are often ineffective in the presence of dynamic data structures due to the lack of an automated analysis of pointerbased memory accesses. In this work, we take a step toward closing this gap. We present a static analysis for pointer-manipulating programs that automatically splits heap-allocated data structures into disjoint, independent regions. The analysis leverages recent advances in separation logic, a theoretical framework for reasoning about heap-allocated data that has been successfully applied in recent software verification tools. Our algorithm focuses on dynamic data structures accessed in loops and is accompanied by automated source-to-source transformations that enable automatic loop parallelization and memory partitioning by off-the-shelf HLS tools. We demonstrate the successful loop parallelization and memory partitioning by our tool flow using three real-life applications that build, traverse, update, and dispose of dynamically allocated data structures. Our case studies, comparing the automatically parallelized to the direct HLS implementations, show an average latency reduction by a factor of 2× across our benchmarks. Copyright © 2015 ACM.",FPGA; High-level synthesis; Memory system; Separation logic; Static analysis,Application programs; Automation; C (programming language); Computer circuits; Computer programming languages; Data structures; Field programmable gate arrays (FPGA); Formal logic; High level languages; Integrated circuit design; Microprocessor chips; Program compilers; Reconfigurable hardware; Static analysis; Storage allocation (computer); Verification; Automated optimization; Dynamic data structure; Dynamic memory allocation; Memory systems; Pointer-based data structures; Separation logic; Software verification tools; Source-to-source transformations; High level synthesis
Coarse-grained architecture for fingerprint matching,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954233257&doi=10.1145%2f2791296&partnerID=40&md5=767d815c7d837d5460979aa2bf68723f,"Fingerprint matching is a key procedure in fingerprint identification applications. The minutiae-based fingerprint matching algorithm is one of the most typical algorithms achieving a reasonably correct recognition rate. This study proposes a coarse-grained parallel architecture called fingerprint matching core (FMC) to accelerate fingerprint matching. The proposed architecture has a two-level parallel structure (i.e., parallel among groups (PAG) and parallel in group (PIG)). A multirequest controller is added to the PAG structure to obtain a concurrent operation of the multiple processing element group (PEG). The DDR3 controller is used in the PIG structure to read eight minutiae from eight different fingerprints and realize the simultaneous computation of the eight PEs. The whole system is implemented on a Xilinx FPGA board with a Virtex VII XC7VX485T chip. The 16-PEG FMC achieves a throughput of about 9.63 million fingerprint pairs per second, which is larger than that achieved on a Tesla K20c platform. The software execution times are also measured on the 2.93GHz Intel Xeon 5670, 2.3GHz AMD Opteron(tm) Processor 6376, and Tesla K20c platforms. The Intel Xeon 5670 has two processors with 12 cores, and the AMD Opteron(tm) Processor 6376 has two processors with 16 cores. Moreover, the throughput is about 31 times that achieved on a 2.93GHz Intel Xeon 5670 single core. © 2015 ACM.",Coarse-grained parallel; Fingerprint matching; FPGA; Minutia; Parallel among groups; Parallel in group; Processing element; Processing element group,Algorithms; Biometrics; Concurrency control; Field programmable gate arrays (FPGA); Palmprint recognition; Parallel architectures; Parallel processing systems; Reconfigurable hardware; Coarse-grained parallels; Fingerprint matching; Minutia; Parallel among groups; Parallel in group; Processing elements; Pattern matching
High-speed hardware partition generation,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919682785&doi=10.1145%2f2629472&partnerID=40&md5=0856e01f21670ce5ef0e11fe5fbe363f,"We demonstrate circuits that generate set and integer partitions on a set S of n objects at a rate of one per clock. Partitions are ways to group elements of a set together and have been extensively studied by researchers in algorithm design and theory. We offer two versions of a hardware set partition generator. In the first, partitions are produced in lexicographical order in response to successive clock pulses. In the second, an index input determines the set partition produced. Such circuits are useful in the hardware implementation of the optimum distribution of tasks to processors.We show circuits for integer partitions as well. Our circuits are combinational. For large n, they can have a large delay. However, one can easily pipeline them to produce one partition per clock period. We show (1) analytical and (2) experimental time/complexity results that quantify the efficiency of our designs. For example, our results show that a hardware set partition generator running on a 100MHz FPGA produces partitions at a rate that is approximately 10 times the rate of a software implementation on a processor running at 2.26GHz. © 2014 ACM.",Combinatorial objects; Index to partition generator; Integer partition; Partition diagram; Partition tree; Reconfigurable computer; Set partition,Clocks; Group theory; Combinatorial objects; Index to partition generator; Integer partitions; Reconfigurable computer; Set partitions; Computer hardware
Physical security evaluation of the bitstream encryption mechanism of altera stratix II and stratix III FPGAs,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919608503&doi=10.1145%2f2629462&partnerID=40&md5=2941a1d33854e778a8a85e9663b1951e,"To protect Field-Programmable Gate Array (FPGA) designs against Intellectual Property (IP) theft and related issues such as product cloning, all major FPGA manufacturers offer a mechanism to encrypt the bitstream that is used to configure the FPGA. From a mathematical point of view, the employed encryption algorithms (e.g., Advanced Encryption Standard (AES) or 3DES) are highly secure. However, it has been shown that the bitstream encryption feature of several FPGA families is susceptible to side-channel attacks based on measuring the power consumption of the cryptographic module. In this article, we present the first successful attack on the bitstream encryption of the Altera Stratix II and Stratix III FPGA families. To this end, we analyzed the Quartus II software and reverse engineered the details of the proprietary and unpublished schemes used for bitstream encryption on Stratix II and Stratix III. Using this knowledge, we demonstrate that the full 128-bit AES key of a Stratix II as well as the full 256-bit AES key of a Stratix III can be recovered by means of side-channel attacks. In both cases, the attack can be conducted in a few hours. The complete bitstream of these FPGAs that are (seemingly) protected by the bitstream encryption feature can hence fall into the hands of a competitor or criminal-possibly implying system-wide damage if confidential information such as proprietary encryption schemes or secret keys programmed into the FPGA are extracted. In addition to lost IP, reprogramming the attacked FPGA with modified code, for instance, to secretly plant a hardware Trojan, is a particularly dangerous scenario formany security-critical applications. © 2014 ACM.",AES; Altera; Bitstream encryption; Hardware security; Reverse engineering; Side-channel attack; Stratix II; Stratix III,Binary sequences; Clone cells; Crime; Data privacy; Field programmable gate arrays (FPGA); Hardware security; Malware; Reverse engineering; Advanced Encryption Standard; Altera; Bitstream encryption; Confidential information; Encryption algorithms; Security critical applications; Stratix II; Stratix III; Side channel attack
"Secure, remote, dynamic reconfiguration of FPGAs",2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919667294&doi=10.1145%2f2629423&partnerID=40&md5=0ab1f4bafffc69d431fc4d678fdeed87,"With the widespread availability of broadband Internet, Field-Programmable Gate Arrays (FPGAs) can get remote updates in the field. This provides hardware and software updates, and enables issue solving and upgrade ability without device modification. In order to prevent an attacker from eavesdropping or manipulating the configuration data, security is a necessity. This work describes an architecture that allows the secure, remote reconfiguration of an FPGA. The architecture is partially dynamically reconfigurable and it consists of a static partition that handles the secure communication protocol and a single reconfigurable partition that holds the main application. Our solution distinguishes itself from existingwork in twoways: it provides entity authentication and it avoids the use of a trusted third party. The former provides protection against active attackers on the communication channel, while the latter reduces the number of reliable entities. Additionally, this work provides basic countermeasures against simple power-oriented side-channel analysis attacks. The result is an implementation that is optimized toward minimal resource occupation. Because configuration updates occur infrequently, configuration speed is of minor importance with respect to area. A prototype of the proposed design is implemented, using 5, 702 slices and having minimal downtime. © 2014 ACM.",DPR; FPGA; Partial reconfiguration; Remote; Secure,Dynamic models; Field programmable gate arrays (FPGA); Network architecture; Reconfigurable architectures; Side channel attack; Dynamic re-configuration; Entity authentications; Hardware and software; Partial reconfiguration; Remote; Remote reconfiguration; Secure; Trusted third parties; Reconfigurable hardware
Value state flow graph: A dataflow compiler IR for accelerating control-intensive code in spatial hardware,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951917711&doi=10.1145%2f2807702&partnerID=40&md5=96d34273289411e94fb84f7a97f5565f,"Although custom (and reconfigurable) computing can provide orders-of-magnitude improvements in energy efficiency and performance for many numeric, data-parallel applications, performance on nonnumeric, sequential code is often worse than conventional superscalar processors. This work attempts to improve sequential performance in custom hardware by (a) switching from a statically scheduled to a dynamically scheduled (dataflow) execution model and (b) developing a new compiler IR for high-level synthesis-the value state flow graph (VSFG)-that enables aggressive exposition of ILP even in the presence of complex control flow. Compared to existing control-data flow graph (CDFG)-based IRs, the VSFG exposes more instruction-level parallelism from control-intensive sequential code by exploiting aggressive speculation, enabling control dependence analysis, as well as execution along multiple flows of control. This new IR is directly implemented as a static-dataflow graph in hardware by our prototype high-level synthesis tool chain and showsanaverage speedupof1.13×over equivalent hardware generated using LegUp, an existing CDFG-based HLS tool. Furthermore, the VSFG allows us to further trade area and energy for performance through loop unrolling, increasing the average speedup to 1.55×, with a peak speedup of 4.05×. Our VSFG-based hardware approaches the sequential cycle counts of an Intel Nehalem Core i7 processor while consuming only 0.25× the energy of an in-order Altera Nios IIf processor. © 2015 ACM.",Amdahl's law; Compilers; Custom computing; Dark silicon; High-level synthesis; Instruction level parallelism; Reconfigurable computing,Codes (symbols); Computer hardware; Data flow graphs; Energy efficiency; Flow graphs; Graphic methods; Hardware; High level synthesis; Parallel processing systems; Program compilers; Reconfigurable architectures; Signal encoding; Synthesis (chemical); Amdahl's laws; Custom computing; Dark silicons; Instruction level parallelism; Reconfigurable computing; Data flow analysis
RAW 2014: Random number generators on FPGAS,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951962283&doi=10.1145%2f2807699&partnerID=40&md5=03db2e9e8699a8ecfd5dae0fb87c6a34,"Random numbers are important ingredients in a number of applications. Especially in a security context, they must be well distributed and unpredictable. We investigate the practical use of random number generators (RNGs) that are built from digital elements found in FPGAs. For this, we implement different types of ring oscillators (ROs) and memory collision-based circuits on FPGAs from major vendors. Implementing RNGs on the same device as the rest of the system benefits an overall reduction of vulnerability to attacks and wire tapping. Nevertheless, we investigate different attacks by tampering with power supply, chip temperature, and by exposition to strong magnetic fields and X-radiation. We also consider their usability as massively deployed components, whose functionality cannot be tested individually anymore, by conducting a technology invariance experiment. Our experiments show that BlockRAM-based RNGs cannot be considered as a suitable entropy source. We further show that RO-based RNGs work reliably under a wide range of operating conditions. While magnetic fields and X-rays did not induce any notable change, voltage and temperature variations caused an increase in propagation delays within the circuits. We show how reliable RNGs can be constructed and deployed on FPGAs. © 2015 ACM.",Active attack on random number generator; Cryptography; Entropy source; FPGA; Magnetic field; Power supply; Technology invariance; Temperature; True random number generator; X-radiation,Cryptography; Entropy; Field programmable gate arrays (FPGA); Magnetic fields; Magnetism; Number theory; Oscillators (electronic); Temperature; Entropy sources; Power supply; Random number generators; True randoms; X-radiation; Random number generation
A reconfigurable architecture for the detection of Strongly Connected Components,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951916082&doi=10.1145%2f2807700&partnerID=40&md5=e8228220473bde81f6585950ff52d815,"The Strongly Connected Components (SCCs) detection algorithm serves as a keystone for many graph analysis applications. The SCC execution time for large-scale graphs, as with many other graph algorithms, is dominated by memory latency. In this article, we investigate the design of a parallel hardware architecture for the detection of SCCs indirected graphs. We propose a design methodology that alleviates memory latency and problems with irregular memory access. The design is composed of 16 processing elements dedicated to parallel Breadth-First Search (BFS) and eight processing elements dedicated to finding intersection in parallel. Processing elements are organized to reuse resources and utilize memory bandwidth efficiently. We demonstrate a prototype of our design using the Convey HC-2 system, a commercial high-performance reconfigurable computing coprocessor. Our experimental results show a speedup of as much as 17× for detecting SCCs in large-scale graphs when compared to a conventional sequential software implementation. © 2015 ACM.",Breadth-First Search; Convey HC-2; FPGA applications; Graphs; Reconfigurable computing; Strongly Connected Components,Algorithms; Design; Graphic methods; Memory architecture; Reconfigurable hardware; Breadth-first search; FPGA applications; Graphs; Reconfigurable computing; Strongly connected component; Reconfigurable architectures
Graph-based approaches to placement of processing element networks on FPGAs for physical model simulation,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919663823&doi=10.1145%2f2629521&partnerID=40&md5=c5a4296f683aa5f53a08f78aaec848a0,"Physical models utilize mathematical equations to characterize physical systems like airwaymechanics, neuron networks, or chemical reactions. Previous work has shown that field programmable gate arrays (FPGAs) execute physical models efficiently. To improve the implementation of physical models on FPGAs, this article leverages graph theoretic techniques to synthesize physical models onto FPGAs. The first phase maps physical model equations onto a structured virtual processing element (PE) graph using graph theoretic folding techniques. The second phase maps the structured virtual PE graph onto physical PE regions on an FPGA using graph embedding theory. A simulated annealing algorithm is introduced that can map any physical model onto an FPGA regardless of the model's underlying topology.We further extend the simulated annealing approach by leveraging existing graph drawing algorithms to generate the initial placement. Compared to previous work on physical model implementation on FPGAs, embedding increases clock frequency by 25% on average (for applicable topologies), whereas simulated annealing increases frequency by 13% on average. The embedding approach typically produces a circuit whose frequency is limited by the FPGA clock instead of routing. Additionally, complex models that could not previously be routed due to complexity were made routable when using placement constraints. © 2014 ACM.",Cyber-physical system; Differential equation; Field programmable gate array (FPGA); Graph embedding; Physical model; Placement; Simulated annealing,Clocks; Complex networks; Cyber Physical System; Differential equations; Drawing (graphics); Embedded systems; Embeddings; Graph algorithms; Graph theory; Logic gates; Logic Synthesis; Signal receivers; Simulated annealing; Graph drawing algorithms; Graph embeddings; Graph-theoretic technique; Mathematical equations; Physical model; Physical model simulations; Placement; Simulated annealing algorithms; Field programmable gate arrays (FPGA)
Area-Efficient Near-Associative Memories on FPGAs,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924066422&doi=10.1145%2f2629471&partnerID=40&md5=d01c7a32cf976b9cdcc98a0e7937a29c,"Associative memories can map sparsely used keys to values with low latency but can incur heavy area overheads. The lack of customized hardware for associative memories in today's mainstream FPGAs exacerbates the overhead cost of building these memories using the fixed address match BRAMs. In this article, we develop a new, FPGA-friendly, memory system architecture based on a multiple hash scheme that is able to achieve near-associative performance without the area-delay overheads of a fully associative memory on FPGAs. At the same time, we develop a novel memory management algorithm that allows us to statistically mimic an associative memory. Using the proposed architecture as a 64KB L1 data cache, we show that it is able to achieve near-associative miss rates while consuming 3-13× fewer FPGA memory resources for a set of benchmark programs from the SPEC CPU2006 suite than fully associative memories generated by the Xilinx Coregen tool. Benefits for our architecture increase with key width, allowing area reduction up to 100×. Mapping delay is also reduced to 3.7ns for a 1,024-entry flat version or 6.1ns for an area-efficient version compared to 17.6ns for a fully associative memory for a 64-bit key on a Xilinx Virtex 6 device. © 2015 Copyright is held by the owner/author(s).",Associative memory; Bram; Cache; CAM; FPGA; Hashing,Associative processing; Associative storage; Cams; Field programmable gate arrays (FPGA); Image matching; Associative memory; Benchmark programs; Bram; Cache; Hashing; Memory management algorithms; Memory system architectures; Proposed architectures; Cache memory
NCBI BLASTP on high-performance reconfigurable computing systems,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924059753&doi=10.1145%2f2629691&partnerID=40&md5=da234fba3d2858849518ab48355b3bb0,"The BLAST sequence alignment program is a central application in bioinformatics. The de facto standard version, NCBI BLAST, uses complex heuristics that make it challenging to simultaneously achieve both high performance and exact agreement. We propose a system that uses novel FPGA-based filters that reduce the input database by over 99.97% without loss of sensitivity. There are several contributions. First is design of the filters themselves, which perform two-hit seeding, exhaustive ungapped alignment, and exhaustive gapped alignments, respectively. Second is the coupling of the filters, especially the two-hit seeding and the ungapped alignment. Third is pipelining the filters in a single design, including maintaining load balancing as data are reduced by orders of magnitude at each stage. Fourth is the optimization required to maintain operating frequency for the resulting complex design. And finally, there is system integration both in hardware (the Convey HC1-EX) and software (NCBI BLASTP). We present results for various usage scenarios and find complete agreement and a factor of nearly 5x speedup over a fully parallel implementation of the reference code on a contemporaneous CPU. We believe that the resulting system is the leading persocket- Accelerated NCBI BLAST. © 2015 ACM.",Application acceleration; Bioinformatics; Biological sequence alignment; FPGA-based coprocessors; High-performance reconfigurable computing,Application programs; Bioinformatics; Design; Field programmable gate arrays (FPGA); Network management; Reconfigurable architectures; Application accelerations; Biological sequence alignment; FPGA-based coprocessors; High performance reconfigurable computing; Operating frequency; Orders of magnitude; Parallel implementations; Sequence alignments; Reconfigurable hardware
ARC 2014: A multidimensional FPGA-based parallel DBSCAN architecture,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954311389&doi=10.1145%2f2724722&partnerID=40&md5=e87681d1e7d5700c7d8ec1fc38f03616,"Clustering large numbers of data points is a very computationally demanding task that often needs to be accelerated in order to be useful in practical applications. This work focuses on the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm, which is one of the state-of-the-art clustering algorithms, and targets its acceleration using an FPGA device. The article presents an optimized, scalable, and parameterizable architecture that takes advantage of the internal memory structure of modern FPGAs in order to deliver a high-performance clustering system. Post-synthesis simulation results show that the developed system can obtain mean speedups of 31× in real-world tests and 202× in synthetic tests when compared to state-of-the-art software counterparts running on a quad-core 3.4GHz Intel i7-2600k. Additionally, this implementation is also capable of clustering data with any number of dimensions without impacting the performance. Copyright © 2015 ACM.",Clustering; DBSCAN; FPGA; Parallel hardware architectures,Computer software; Field programmable gate arrays (FPGA); Memory architecture; Parallel architectures; Reconfigurable hardware; Software testing; Clustering; Clustering system; DBSCAN; Density-based spatial clustering of applications with noise; Internal memory; Parallel hardware; Real-world tests; State of the art; Clustering algorithms
Program-invariant checking for soft-error detection using reconfigurable hardware,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954322867&doi=10.1145%2f2751563&partnerID=40&md5=380b05b5fd75e4172e8dcc058bc82890,"There is an increasing concern about transient errors in deep submicron processor architectures. Softwareonly error detection approaches that exploit program invariants for silent error detection incur large execution overheads and are unreliable as state can be corrupted after invariant checkpoints. In this article, we explore the use of configurable hardware structures for the continuous evaluation of high-level program invariants at the assembly level. We evaluate the resource requirements and performance of the proposed predicate-evaluation hardware structures when integrated with a 32-bit MIPS soft core on a contemporary reconfigurable hardware device. The results, for a small set of kernel codes, reveal that these hardware structures require a very small number of hardware resources with negligible impact on the processor core that they are integrated in. Moreover, the amount of resources is fairly insensitive to the complexity of the invariants, thus making the proposed structures an attractive alternative to software-only predicate checking. Copyright © 2015 ACM.",FPGA; Processor architecture,Computer architecture; Computer hardware; Error detection; Errors; Field programmable gate arrays (FPGA); Hardware; Radiation hardening; Configurable hardware; Hardware resources; Hardware structures; High-level program; Processor architectures; Reconfigurable hardware devices; Resource requirements; Soft error detection; Reconfigurable hardware
Implementing Curve25519 for side-channel-protected Elliptic Curve Cryptography,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954315491&doi=10.1145%2f2700834&partnerID=40&md5=4a159fc924ab60cef19451879bfd5205,"For security-critical embedded applications Elliptic Curve Cryptography (ECC) has become the predominant cryptographic system for efficient key agreement and digital signatures. However, ECC still involves complex modular arithmetic that is a particular burden for small processors. In this context, Bernstein proposed the highly efficient ECC instance Curve25519 that particularly enables efficient software implementations at a security level comparable to AES-128 with inherent resistance to simple power analysis (SPA) and timing attacks. In this work, we show that Curve25519 is likewise competitive on FPGAs even when countermeasures to thwart side-channel power analysis are included. Our basic multicore DSP-based architectures achieves a maximal performance of more than 32,000 point multiplications per second on a Xilinx Zynq 7020 FPGA. Including a mix of side-channel countermeasures to impede simple and differential power analysis, we still achieve more than 27,500 point multiplications per second with a moderate increase in logic resources. Copyright © 2015 ACM.",Curve25519; Diffie-hellman; ECC; Side-channel attacks; Zynq,Cryptography; Digital signal processing; Geometry; Network security; Public key cryptography; Reconfigurable hardware; Curve25519; Differential power Analysis; Diffie Hellman; ECC; Elliptic curve cryptography; Elliptic Curve Cryptography(ECC); Software implementation; Zynq; Side channel attack
"ARC 2014 over-clocking KLT designs on FPGAs under process, voltage, and temperature variation",2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954324092&doi=10.1145%2f2818380&partnerID=40&md5=33dea1a8e262ec9f7ff58911a01f076a,"Karhunen-Loeve Transformation is a widely used algorithm in signal processing that often implemented with high-throughput requisites. This work presents a novelmethodology to optimise KLT designs on FPGAs that outperform typical design methodologies, through a prior characterisation of the arithmetic units in the datapath of the circuit under various operating conditions. Limited by the ever-increasing process variation, the delay models available in synthesis tools are no longer suitable for extreme performance optimisation of designs, and as they are generic, they need to consider the worst-case performance for a given fabrication process. Hence, they heavily penalise the maximum possible achieved performance of a design by leaving safety margin. This work presents a novel unified optimisation framework which contemplates a prior characterisation of the embedded multipliers on the target FPGA device under process, voltage, and temperature variation. The proposed framework allows a design space exploration leading to designs without any latency overheads that achieve high throughput while producing less errors than typical methodologies, operating with the same throughput. Experimental results demonstrate that the proposed methodology outperforms the typical implementation in three real-life design strategies: high performance, low power, and temperature variation; and it produced circuit designs that performed up to 18dB better when over-clocked. Copyright © 2015 ACM.",Design methodologies; Digital signal processing; Optimisation; Over-clocking; Reconfigurable applications,Algorithms; Clocks; Digital signal processing; Integrated circuit design; Integrated circuit manufacture; Low power electronics; Mathematical transformations; Principal component analysis; Reconfigurable hardware; Signal processing; Temperature; Temperature distribution; Throughput; Design Methodology; Design space exploration; Karhunen Loeve transformation; Optimisations; Over-clocking; Performance optimisation; Reconfigurable; Worst-case performance; Design
ARC 2014: Towards a fast FPGA implementation of a heap-based priority queue for image coding using a parallel index-aware tree,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954310496&doi=10.1145%2f2766454&partnerID=40&md5=68c8c955148c0bfddf86dd0214a3172f,"The embedded image processing systems like smartphones and digital cameras have tight limits on storage, computation power, network connectivity, and battery usage. These limitations make it important to ensure efficient image coding. In the article, we present a novel heap-based priority queue structure employed by an Adaptive Scanning of Wavelet Data scheme (ASWD) targeting an embedded platform. ASWD is a context modeling block implemented via priority queues in a wavelet-based image coder to reorganize the wavelet coefficients into locally stationary sequences. The architecture we propose exploits efficient use of FPGA's onchip dual-port memories in an adaptive manner. Innovations of index-aware system linked to each element in the queue makes the location of queue element traceable in the heap as per the requirements of the ASWD algorithm. Moreover, use of 4-port memories along with intelligent data concatenation of queue elements yielded in a cost effective enhanced memory access. The memory ports are adaptively assigned to different units during different processing phases in a manner to optimally take advantage of memory access required by that phase. The architectural innovations can also be exploited in other applications that require efficient hardware implementations of generic priority queue or classical sorting applications which sort into the index. We designed and validated the hardware on an Altera's Stratix IV FPGA as an IP accelerator in a Nios II processor based System on Chip. We show that our architecture at 150MHz can provide 45X speedup compared to an embedded ARM Cortex-A9 processor at 666MHz targeting the throughput of 10MB/s. Copyright © 2015 ACM.",Adaptive scanning; Embedded system; FPGA; Heapsort; Image compression; Priority queue; Systemon-chip,Application specific integrated circuits; Cost effectiveness; Data compression; Digital image storage; Digital storage; Embedded systems; Field programmable gate arrays (FPGA); Hardware; Image compression; Image processing; Memory architecture; Microprocessor chips; Network architecture; Queueing theory; Random access storage; Reconfigurable hardware; System-on-chip; Wavelet analysis; Adaptive scanning; Architectural innovation; Embedded image processing systems; Hardware implementations; Heapsort; Network connectivity; Priority queues; Wavelet-based images; Image coding
Guest editorial ARC 2014,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946895819&doi=10.1145%2f2831431&partnerID=40&md5=b0194359dccd096cfc6d756912dcc931,[No abstract available],,
An enhanced adaptive recoding rotation CORDIC,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946919975&doi=10.1145%2f2812813&partnerID=40&md5=5c4abe41de43e9f3b3004624464c15d5,"The Conventional Coordinate Rotation Digital Computer (CORDIC) algorithm has been widely used in many applications, particularly in Direct Digital Frequency Synthesizers (DDS) and Fast Fourier Transforms (FFT). However, CORDIC is constrained by the excessive number of iterations, angle data path, and scaling factor compensation. In this article, an enhanced adaptive recoding CORDIC (EARC) is proposed. It uses the enhanced adaptive recoding method to reduce the required iterations and adopts the trigonometric transformation scheme to scale up the rotation angles. Computing sine and cosine is used first to compare the core functionality of EARC with basic CORDIC; then a 16-bit DDS and a 1,024-point FFT based on EARC are evaluated to demonstrate the benefits of EARC in larger applications. All the proposed architectures are validated on a Virtex 5 FPGA development platform. Compared with a commercial implementation of CORDIC, EARC requires 33.3% less hardware resources, provides a twofold speedup, dissipates 70.4% less power, and improves accuracy in terms of the Bit Error Position (BEP). Compared to the state-of-the-art Hybrid CORDIC, EARC reduces latency by 11.1% and consumes 17% less power. Compared with a commercial implementation of DDS, the dissipated power of the proposed DDS is reduced by 27.2%. The proposed DDS improves Spurious-Free Dynamic Range (SFDR) by nearly 7 dBc and dissipates 21.8% less power when compared with a recently published DDS circuit. The FFT based on EARC dissipates a factor of 2.05 less power than the commercial FFT even when choosing the 100% toggle rate for the FFT based on EARC and the 12.5% toggle rate for the commercial FFT. Compared with a recently published FFT, the FFT based on EARC improves Signal-to-Noise Ratio (SNR) by 8.9 dB and consumes 7.78% less power. © 2015 ACM.",Algorithms; Design; Experimentation; Performance,Algorithms; Design; Digital computers; Digital to analog conversion; Signal to noise ratio; Commercial implementation; Coordinate rotation digital computer algorithms; Direct digital frequency synthesizer; Experimentation; Performance; Proposed architectures; Spurious free dynamic range; Transformation scheme; Fast Fourier transforms
On the impact of replacing low-speed configuration buses on FPGAS with the chip's internal configuration infrastructure,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946560687&doi=10.1145%2f2700835&partnerID=40&md5=05d6c87264295cabc6975c62435cee41,"It is common for large hardware designs to have a number of registers or memories whose contents have to be changed very seldom (e.g., only at startup). The conventional way of accessing these memories is through a low-speed memory bus. This bus uses valuable hardware resources, introduces long global connections, and contributes to routing congestion. Hence, it has an impact on the overall design even though it is only rarely used. A Field-Programmable Gate Array (FPGA) already contains a global communication mechanism in the form of its configuration infrastructure. In this article, we evaluate the use of the configuration infrastructure as a replacement for a low-speed memory bus on the Maxeler HPC platform. We find that by removing the conventional low-speed memory bus, the maximum clock frequency of some applications can be improved by 8%. Improvements by 25% and more are also attainable, but constraints of the Xilinx reconfiguration infrastructure prevent fully exploiting these benefits at the moment. We present a number of possible changes to the Xilinx reconfiguration infrastructure and tools that would solve this and make these results more widely applicable. © 2015 ACM.",Block RAM; FPGA; HPC; Partial reconfiguration,Field programmable gate arrays (FPGA); Random access storage; System buses; Block rams; Clock frequency; Global communication; Hardware design; Hardware resources; Overall design; Partial reconfiguration; Routing congestion; Buses
RIFFA 2.1: A reusable integration framework for FPGA accelerators,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942040352&doi=10.1145%2f2815631&partnerID=40&md5=5cd535f68d97aa4cceb4eae54dd45a5e,"We present RIFFA 2.1, a reusable integration framework for Field-Programmable Gate Array (FPGA) accelerators. RIFFA provides communication and synchronization for FPGA accelerated applications using simple interfaces for hardware and software. Our goal is to expand the use of FPGAs as an acceleration platform by releasing, as open source, a framework that easily integrates software running on commodity CPUs with FPGA cores. RIFFA uses PCI Express (PCIe) links to connect FPGAs to a CPU's system bus. RIFFA 2.1 supports FPGAs from Xilinx and Altera, Linux and Windows operating systems, and allows multiple FPGAs to connect to a single host PC system. It has software bindings for C/C++, Java, Python, and Matlab. Tests show that data transfers between hardware and software can reach 97% of the achievable PCIe link bandwidth. © 2015 ACM.",Communication; FPGA; Framework; Integration; Synchronization,Application programs; C++ (programming language); Communication; Data transfer; Integration; MATLAB; Open source software; Open systems; Program processors; Software testing; Synchronization; Windows operating system; Communication and synchronizations; Fpga accelerators; Framework; Hardware and software; Integration frameworks; Link bandwidth; Open sources; PCI-Express (PCIe); Field programmable gate arrays (FPGA)
Low-Overhead FPGA middleware for application portability and productivity,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942012383&doi=10.1145%2f2746404&partnerID=40&md5=6bfe74edc7bffb164e0f8aa21b9048bd,"Reconfigurable computing devices such as field-programmable gate arrays (FPGAs) offer advantages over fixed-logic CPU and GPU architectures, including improved performance, superior power efficiency, and reconfigurability. The challenge of FPGA application development, however, has limited their acceptance in high-performance computing and high-performance embedded computing applications. FPGA development carries similar difficulties to hardware design, requiring that developers iterate through register-transfer level designs with cycle-level accuracy. Furthermore, the lack of hardware and software standards between FPGA platforms limits productivity and application portability, and makes porting applications between heterogeneous platforms a time-consuming and often challenging process. Recent efforts to improve FPGA productivity using high-level synthesis tools and languages show promise, but platform support remains limited and typically is left as a challenge for developers. To address these issues, we present RC Middleware (RCMW), a novel middleware that improves productivity and enables application and tool portability by abstracting away platform-specific details. RCMW provides an application-centric development environment, exposing only the resources and standardized interfaces required by an application, independent of the underlying platform. We demonstrate the portability and productivity benefits of RCMW using four heterogeneous platforms from three vendors. Our results indicate that RCMW enables application productivity and improves developer productivity, and that these benefits are achieved with less than 7% performance and 3% area overhead on average. © 2015 ACM.",Accelerators; FPGA; Heterogeneous computing; Middleware; Portability; Productivity; Reconfigurable computing,Application programs; Computation theory; Computer hardware; Computer software portability; Field programmable gate arrays (FPGA); High level languages; High level synthesis; Middleware; Particle accelerators; Productivity; Reconfigurable architectures; Application portability; Development environment; Heterogeneous computing; Heterogeneous platforms; High performance computing; Reconfigurable computing; Register transfer level; Standardized interfaces; Reconfigurable hardware
SuperDragon: A heterogeneous parallel system for accelerating 3D reconstruction of cryo-electron microscopy images,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942028525&doi=10.1145%2f2740966&partnerID=40&md5=89ea6bcfab27d419130effc91c564c78,"The data deluge in medical imaging processing requires faster and more efficient systems. Due to the advance in recent heterogeneous architecture, there has been a resurgence in research aimed at domain-specific accelerators. In this article, we develop an experimental system SuperDragon for evaluating acceleration of a single-particle Cryo-electron microscopy (Cryo-EM) 3D reconstruction package EMAN through a hybrid of CPU, GPU, and FPGA parallel architecture. Based on a comprehensive workload characterization, we exploit multigrained parallelism in the Cryo-EM 3D reconstruction algorithm and investigate a proper computational mapping to the underlying heterogeneous architecture. The package is restructured with task-level (MPI), thread-level (OpenMP), and data-level (GPU and FPGA) parallelism. Especially, the proposed FPGA accelerator is a stream architecture that emphasizes the importance of optimizing computing dominated data access patterns. Besides, the configurable computing streams are constructed by arranging the hardware modules and bypassing channels to form a linear deep pipeline. Compared to the multicore (six-core) program, the GPU and FPGA implementations achieve speedups of 8.4 and 2.25 times in execution time while improving power efficiency by factors of 7.2 and 14.2, respectively. © 2015 ACM.",3D reconstruction; Cryo-EM; FPGA; Heterogeneous,Application programming interfaces (API); Computer hardware description languages; Data streams; Electron microscopes; Electron microscopy; Field programmable gate arrays (FPGA); Graphics processing unit; Medical imaging; Multicore programming; Parallel architectures; 3-D reconstruction algorithms; 3D reconstruction; Cryo-electron microscopies (cryo- em); Cryo-EM; Heterogeneous; Heterogeneous architectures; Heterogeneous parallel systems; Workload characterization; Image reconstruction
Safe dynamic reshaping of reconfigurable MPSoC embedded systems for self-healing and self-adaption purposes,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942067638&doi=10.1145%2f2700416&partnerID=40&md5=f105dc51746af2cbb63d119b62bce82d,"Multiprocessor system-on-chip (MPSoC) architectures are a huge challenge in embedded system design. This situation arises from the fact that availableMPSoCs and related designs flows are not tailored to the specific needs of embedded systems. This work demonstrates how to provide self-healing properties in embedded MPSoC design. This is achieved by combining the features of a generic approach to create virtualizable MPSoCs out of off-the-shelf embedded processors with a methodology to derive system configurations, such as task-processor bindings, which are optimal in terms of safety and execution time. The virtualization properties enable a reshaping of the MPSoC at runtime. Thus, system configurations may be exchanged rapidly in a dynamic fashion. As a main result of this work, embeddedmultiprocessor systems are introduced, which dynamically adapt to changing operating conditions, possible module defects, and internal state changes. We demonstrate the figures of merit of such reconfigurable MPSoC embedded systems by means of a complex automotive application scenario mapped to an FPGA featuring a virtualizable array of eight soft-core processors. © 2015 ACM.",Embedded system design; Runtime reconfiguration; Virtualization,Embedded software; Embedded systems; Multiprocessing systems; Self-healing materials; System-on-chip; Systems analysis; Virtualization; Automotive applications; Changing operating conditions; Embedded processors; Multiprocessor system on chips; Run time reconfiguration; Self-healing properties; Soft-core processors; System configurations; Integrated circuit design
Memory interface design for 3d stencil kernels on a massively parallel memory system,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942047357&doi=10.1145%2f2800788&partnerID=40&md5=f60504de33f5c0f6da3f15f10a54de14,"Massively parallel memory systems are designed to deliver high bandwidth at relatively low clock speed for memory-intensive applications implemented on programmable logic. For example, the Convey HC-1 provides 1,024 DRAM banks to each of four FPGAs through a full crossbar, presenting a peak bandwidth of 76.8GB/s to the user logic. Such highly parallelmemory systems suffer from high latency, and their effective bandwidth is highly sensitive to access ordering. To achieve high performance, the user must use a customized memory interface that combines scheduling, latency hiding, and data reuse. In this article, we describe the design of a custom memory interface for 3D stencil kernels on the Convey HC-1 that incorporates these features. Experimental results show that the proposed memory interface achieves a speedup in runtime of 2.2 for 6-point stencil and 9.5 for 27-point stencil when compared to a naive memory interface. © 2015 ACM.",3D stencil; Data reuse; Memory access scheduling; Memory interface; Memory latency hiding,Bandwidth; Computer circuits; Integrated circuit design; Memory architecture; Scheduling; 3D stencil; Data reuse; Memory access scheduling; Memory interface; Memory latencies; Dynamic random access storage
The Table-Hadamard GRNG: An area-efficient FPGA Gaussian random number generator,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946011784&doi=10.1145%2f2629607&partnerID=40&md5=0f9d1220028ce2358891508ba5467268,"Gaussian random number generators (GRNGs) are an important component in parallel Monte Carlo simulations using FPGAs, where tens or hundreds of high-quality Gaussian samples must be generated per cycle using very few logic resources. This article describes the Table-Hadamard generator, which is a GRNG designed to generate multiple streams of random numbers in parallel. It uses discrete table distributions to generate pseudo-Gaussian base samples, then a parallel Hadamard transform to efficiently apply the central limit theorem. When generating 64 output samples, the Table-Hadamard requires just 130 slices per generated sample, which is a third of the resources needed by the next best technique, while still providing higher statistical quality. © 2015 ACM.",Arithmetic operations; Parallel algorithms; Reconfigurable applications,Field programmable gate arrays (FPGA); Gaussian distribution; Hadamard transforms; Monte Carlo methods; Number theory; Parallel algorithms; Arithmetic operations; Central Limit Theorem; Gaussian sample; Multiple streams; Parallel Monte Carlo simulation; Random number generators; Reconfigurable; Statistical quality; Random number generation
Low-level flexible architecture with hybrid reconfiguration for evolvable hardware,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930672639&doi=10.1145%2f2700414&partnerID=40&md5=89217d4ae967cf648a6769d01ce33836,"Field-programmable gate arrays (FPGAs) can be considered to be the most popular and successful platform for evolvable hardware. They allow one to establish and later reconfigure candidate solutions. Recent work in the field of evolvable hardware includes the use of virtual and native reconfigurations. Virtual reconfiguration is based on the change of functionality by hardware components implemented on top of FPGA resources. Native reconfiguration changes the FPGA resources directly by means provided by the FPGA manufacturer. Both of these approaches have their disadvantages. The virtual reconfiguration is characterized by lower maximal operational frequency of the resulting solutions, and the native reconfiguration is slower. In this work, a hybrid approach is used merging the advantages while limiting the disadvantages of the virtual and native reconfigurations. The main contribution is the new low-level architecture for evolvable hardware in the new Zynq-7000 all-programmable system-on-chip. The proposed architecture offers high flexibility in comparison with other evolvable hardware systems by considering direct modification of the reconfigurable resources. The impact of the higher reconfiguration time of the native approach is limited by the dense placement of the proposed reconfigurable processing elements. These processing elements also ensure fast evaluation of candidate solutions. The proposed architecture is evaluated by evolutionary design of switching image filters and edge detectors. The experimental results demonstrate advantages over the previous approaches considering the time required for evolution, area overhead, and flexibility. © 2015 ACM.",Architecture; Circuit design; Evolvable hardware; Image filter; Reconfigurable; Zynq,Application specific integrated circuits; Architecture; Computer hardware; Hardware; Microprocessor chips; Reconfigurable architectures; Reconfigurable hardware; System-on-chip; Circuit designs; Evolvable hardware; Image filters; Reconfigurable; Zynq; Field programmable gate arrays (FPGA)
The effect of compiler optimizations on high-level synthesis-generated hardware,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930672991&doi=10.1145%2f2629547&partnerID=40&md5=e4fdf687fb9849b097ddbb8994aca684,"We consider the impact of compiler optimizations on the quality of high-level synthesis (HLS)-generated field-programmable gate array (FPGA) hardware. Using an HLS tool implemented within the state-of-theart LLVM compiler, we study the effect of compiler optimizations on the hardware metrics of circuit area, execution cycles, FMax, and wall-clock time. We evaluate 56 different compiler optimizations implemented within LLVM and show that some optimizations significantly affect hardware quality. Moreover, we show that hardware quality is also affected by some optimization parameter values, as well as the order in which optimizations are applied. We then present a new HLS-directed approach to compiler optimizations, wherein we execute partial HLS and profiling at intermittent points in the optimization process and use the results to judiciously undo the impact of optimization passes predicted to be damaging to the generated hardware quality. Results show that our approach produces circuits with 16% better speed performance, on average, versus using the standard-O3 optimization level. © 2015 ACM.",Erformance; FPGAS; High-level synthesis; Optimization,Field programmable gate arrays (FPGA); Hardware; High level synthesis; Optimization; Compiler optimizations; Erformance; Execution cycles; LLVM compilers; Optimization levels; Optimization parameter; Speed performance; Program compilers
Exploiting FPGA block memories for protected cryptographic implementations,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930652188&doi=10.1145%2f2629552&partnerID=40&md5=9a8e54c2ab9b874c333d16243ad80d1b,"Modern field programmable gate arrays (FPGAs) are power packed with features to facilitate designers. Availability of features like large block memory (BRAM), digital signal processing cores, and embedded CPU makes the design strategy of FPGAs quite different from ASICs. FPGAs are also widely used in security-critical applications where protection against known attacks is of prime importance. We focus on physical attacks that target physical implementations. To design countermeasures against such attacks, the strategy for FPGA designers should be different from that in ASIC. The available features should be exploited to design compact and strong countermeasures. In this article, we propose methods to exploit the BRAMs in FPGAs for designing compact countermeasures. Internal BRAM can be used to optimize intrinsic countermeasures such as masking and dual-rail logics, which otherwise have significant overhead (at least 2×) compared to unprotected ones. The optimizations are applied on a real AES-128 co-processor and tested for area overhead and resistance on Xilinx Virtex-5 chips. The presented masking countermeasure has an overhead of only 16% when applied on AES. Moreover, the dual-rail precharge logic (DPL) countermeasure has been optimized to pack the whole sequential part in the BRAM, hence enhancing the security. Proper robustness evaluations are conducted to analyze the optimization in terms of area and security. © 2015 ACM.",Algorithm architecture adequation; Countermeasures,Design; Digital signal processing; Field programmable gate arrays (FPGA); Mobile security; Radar countermeasures; Signal processing; Algorithm architectures; Cryptographic implementation; Design strategies; Dual-rail logic; Masking countermeasure; Physical attacks; Robustness evaluation; Security critical applications; Integrated circuit design
Efficient fault-tolerant topology reconfiguration using a maximum flow algorithm,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930626872&doi=10.1145%2f2700417&partnerID=40&md5=c3e03347e50eaddd34acb064df7650cb,"With an increasing number of processing elements (PEs) integrated on a single chip, fault-tolerant techniques are critical to ensure the reliability of such complex systems. In current reconfigurable architectures, redundant PEs are utilized for fault tolerance. In the presence of faulty PEs, the physical topologies of various chips may be different, so the concept of virtual topology from network embedding problem has been used to alleviate the burden for the operating systems. With limited hardware resources, how to reconfigure a system into the most effective virtual topology such that the maximum repair rate can be reached presents a significant challenge. In this article, a new approach using a maximum flow (MF) algorithm is proposed for an efficient topology reconfiguration in reconfigurable architectures. In this approach, topology reconfiguration is converted into a network flow problem by constructing a directed graph; the solution is then found by using the MF algorithm. This approach optimizes the use of spare PEs with minimal impacts on area, throughput, and delay, and thus it significantly improves the repair rate of faulty PEs. In addition, it achieves a polynomial reconfiguration time. Experimental results show that compared to previous methods, the MF approach increases the probability to repair faulty PEs by up to 50% using the same redundant resources. Compared to a fault-free system, the throughput only decreases by less than 2.5% and latency increases by less than 4%. To consider various types of PEs in a practical application, a cost factor is introduced into the MF algorithm. An enhanced approach using a minimum-cost MF algorithm is further shown to be efficient in the fault-tolerant reconfiguration of heterogeneous reconfigurable architectures. © 2015 ACM.",Fault tolerance; Reconfigurable architecture; Topology reconfiguration,Algorithms; Complex networks; Directed graphs; Fault tolerance; Fault tolerant computer systems; Flow graphs; Network architecture; Reconfigurable architectures; Fault tolerant technique; Hardware resources; Heterogeneous reconfigurable architectures; Maximum-flow algorithm; Network flow problems; Processing elements; Reconfiguration time; Virtual topologies; Topology
Automating elimination of idle functions by runtime reconfiguration,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930663913&doi=10.1145%2f2700415&partnerID=40&md5=84b19b44fd7167ed3b0c6948b0ef4955,"A design approach is proposed to automatically identify and exploit runtime reconfiguration opportunities with optimised resource utilisation by eliminating idle functions. We introduce Reconfiguration Data Flow Graph, a hierarchical graph structure enabling reconfigurable designs to be synthesised in three steps: function analysis, configuration organisation, and runtime solution generation. The synthesised reconfigurable designs are dynamically evaluated and selected under various runtime conditions. Three applications-barrier option pricing, article filter, and reverse time migration-are used in evaluating the proposed approach. The runtime solutions approximate their theoretical performance by eliminating idle functions and are 1.31 to 2.19 times faster than optimised static designs. FPGA designs developed with the proposed approach are up to 43.8 times faster than optimised CPU reference designs and 1.55 times faster than optimised GPU designs. © 2015 ACM.",High performance computing; Reconfigurable computing; Runtime reconfiguration,Data flow analysis; Data flow graphs; Economics; Field programmable gate arrays (FPGA); Flow graphs; Graphic methods; Reconfigurable architectures; Hierarchical graphs; High performance computing; Reconfigurable computing; Reconfigurable designs; Resource utilisation; Reverse time migrations; Run time reconfiguration; Theoretical performance; Design
Execution trace-driven energy-reliability optimization for multimedia MPSoCs,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929232202&doi=10.1145%2f2665071&partnerID=40&md5=ba8b3dc0be9fa5133aba52517a4cf1bc,"Multiprocessor systems-on-chip (MPSoCs) are becoming a popular design choice in current and future technology nodes to accommodate the heterogeneous computing demand of a multitude of applications enabled on these platform. Streaming multimedia and other communication-centric applications constitute a significant fraction of the application space of these devices. The mapping of an application on an MPSoC is an NP-hard problem. This has attracted researchers to solve this problem both as stand-alone (best-effort) and in conjunction with other optimization objectives, such as energy and reliability. Most existing studies on energy-reliability joint optimization are static-that is, design time based. These techniques fail to capture runtime variability such as resource unavailability and dynamism associated with application behaviors, which are typical of multimedia applications. The few studies that consider dynamic mapping of applications do not consider throughput degradation, which directly impacts user satisfaction. This article proposes a runtime technique to analyze the execution trace of an application modeled as Synchronous Data Flow Graphs (SDFGs) to determine its mapping on a multiprocessor system with heterogeneous processing units for different fault scenarios. Further, communication energy is minimized for each of these mappings while satisfying the throughput constraint. Experiments conducted with synthetic and real SDFGs demonstrate that the proposed technique achieves significant improvement with respect to the state-of-the-art approaches in terms of throughput and storage overhead with less than 20% energy overhead. © 2015 ACM.",Energy consumption; Fault tolerance; Multimedia applications; Synchronous data flow graphs; Task mapping and scheduling,Data flow analysis; Data transfer; Digital storage; Energy utilization; Fault tolerance; Integrated circuit design; Mapping; Multimedia systems; Multiprocessing systems; NP-hard; Optimization; Response time (computer systems); System-on-chip; Heterogeneous computing; Heterogeneous processing; Multimedia applications; Multiprocessor systems on chips; Reliability optimization; State-of-the-art approach; Synchronous data flow graphs; Task mapping; Data flow graphs
COEX: A novel profiling-based algorithm/architecture co-exploration for ASIP design,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929236666&doi=10.1145%2f2629563&partnerID=40&md5=a9e7c9f88ea73e55309bdd2156d70969,"Application-Specific Instruction Set Processors (ASIPs) provide the adequate performance/efficiency tradeoff for their particular application domain. Nevertheless, their design methodologies have stagnated during the past decade and are still based on a series of manual and time-consuming iterative steps. Furthermore, there exists a productivity gap between the point where an application is given as the target for processor customization and the time a customized architecture is available. Therefore, new tools are required that reduce the number of design iterations and bridge the aforementioned productivity gap. This can be achieved by (1) profiling technologies that, by adapting to the designer's needs, help to gain insight into application specifications, and (2) prearchitectural design technologies that give early yet accurate feedback on the impact of algorithmic/architectural design decisions. The first requirement is addressed in this article by proposing the multigrained profiling approach, which identifies the profiling needs at each step of ASIP design and lets the designer tailor the level of detail for application inspection. CoEx, a practical implementation of the approach, is also introduced. The second requirement is addressed by creating a prearchitectural estimation engine. This engine couples CoEx reports for an application with an abstract processor model and generates an estimate of the achievable performance. Both CoEx and the performance estimation engine are respectively evaluated for instrumentation-induced execution overhead and accuracy. Finally, the development of an ASIP architecture for an augmented reality computer vision application is presented. The ASIP achieves a gain of six times compared to the original application performance, after being developed in only 2 days. © 2015 ACM.",Algorithm/architecture co-exploration; ASIP; Performance estimation,Abstracting; Augmented reality; Computer architecture; Engines; Integrated circuit design; Iterative methods; Productivity; Achievable performance; Application performance; Application specific instruction set processor; ASIP; Computer vision applications; Design technologies; Performance estimation; Profiling technology; Bridges
Autonomous soft-error tolerance of FPGA configuration bits,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929150521&doi=10.1145%2f2629580&partnerID=40&md5=14eee377377ef2745920fc92436531b3,"Field-programmable gate arrays (FPGAs) are increasingly susceptible to radiation-induced single event upsets (SEUs). These upsets are predominant in a space environment; however, with increasing use of static RAM (SRAM) in modern FPGAs, these SEUs are gaining prominence even in a terrestrial environment. SEUs can flip SRAM bits of FPGA, potentially altering the functionality of the implemented design. This has motivated FPGA designers to investigate techniques to protect the FPGA configuration bits against such inadvertent bit flips (soft error). Traditionally, triple modular redundancy (TMR) is used to protect the FPGA bit flips. Increasing design complexity and limited battery life motivate for alternative approaches for softerror tolerance. In this article, we propose a technique to improve autonomous fault-masking capabilities of a design by maximizing the number of zeros or ones in lookup tables (LUTs). The technique analyzes critical configuration bits and utilizes spare resources (XOR gates and carry chains) of FPGAs to selectively manipulate the logic implemented in LUTs using two operations: LUT restructuring and LUT decomposition. We implemented the proposed approach for Xilinx Virtex-6 FPGAs and validated the same with a wide set of designs from the MCNC, IWLS 2005, and ITC99 benchmark suites. Results demonstrate that the proposed logic restructuring maximizes logic 0 (or 1) of LUTs by an average of 20%, achieving 80% fault masking with no area overhead. The fault rate of the entire design is reduced by 60% on average as compared to the existing techniques. Furthermore, the logic decomposition algorithm provides incremental fault-tolerance capabilities and achieves an additional 5% fault masking with an average 7% increase in slice usage. The complete methodology is implemented into a tool for Xilinx FPGA and is made available online for the benefit of the research community. The algorithms are lightweight, and the whole design flow (including Xilinx Place and Route) was completed in 75 minutes for the largest benchmark in the set. © 2015 ACM.",FPGAs; Luts; Soft errors; SRAM configuration bits,Carry logic; Computer control systems; Design; Error correction; Fault tolerance; Fault tolerant computer systems; Field programmable gate arrays (FPGA); Microprocessor chips; Radiation hardening; Critical configurations; Decomposition algorithm; Fault-tolerance capability; Lookup tables (LUTs); Luts; Soft error; Terrestrial environments; Triple modular redundancy; Integrated circuit design
Imprecise datapath design: An overclocking approach,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961309989&doi=10.1145%2f2629527&partnerID=40&md5=62a26311dd5ce69beb41085341084d53,"In this article, we describe an alternative circuit design methodology when considering trade-offs between accuracy, performance, and silicon area. We compare two different approaches that could trade accuracy for performance. One is the traditional approach where the precision used in the datapath is limited to meet a target latency. The other is a proposed new approach which simply allows the datapath to operate without timing closure.We demonstrate analytically and experimentally that on average our approach obtains either smaller errors or equivalent faster operating frequencies in comparison to the traditional approach. This is because the worst case caused by timing violations only happens rarely, while precision loss results in errors to most data. We also show that for basic arithmetic operations such as addition, applying our approach to the simple building block of ripple carry adders can achieve better accuracy or performance than using faster adder designs to achieve similar latency. © 2015 ACM.",Approximate computing; Imprecise design; Truncation,Adders; Commerce; Economic and social effects; Errors; Approximate computing; Arithmetic operations; Building blockes; Data path design; Operating frequency; Ripple carry adders; Traditional approaches; Truncation; Design
A hash table for line-rate data processing,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929155482&doi=10.1145%2f2629582&partnerID=40&md5=a389f3be0743d82fa78732a024f152c8,"FPGA-based data processing is becoming increasingly relevant in data centers, as the transformation of existing applications into dataflow architectures can bring significant throughput and power benefits. Furthermore, a tighter integration of computing and network is appealing, as it overcomes traditional bottlenecks between CPUs and network interfaces, and dramatically reduces latency. In this article, we present the design of a novel hash table, a fundamental building block used in many applications, to enable data processing on FPGAs close to the network. We present a fully pipelined design capable of sustaining consistent 10Gbps line-rate processing by deploying a concurrent mechanism to handle hash collisions.We address additional design challenges such as support for a broad range of key sizes without stalling the pipeline through careful matching of lookup time with packet reception time. Finally, the design is based on a scalable architecture that can be easily parameterized to work with different memory types operating at different access speeds and latencies. We have tested the proposed hash table in an FPGA-based memcached appliance implementing a mainmemory key-value store in hardware. The hash table is used to index 2 million entries in 24GB of external DDR3 DRAMwhile sustaining 13 million requests per second, the maximum packet rate that can be achieved with UDP packets on a 10Gbps link for this application. © 2015 ACM.",Data structures; FPGAs; Hash functions; Parallel architecture,Data handling; Data structures; Design; Field programmable gate arrays (FPGA); Hash functions; Integrated circuit design; Metadata; Network architecture; Parallel architectures; Program processors; Data-flow architectures; Design challenges; FPGA-based data processing; Fully pipelined; Fundamental building blocks; Key-value stores; Packet reception; Scalable architectures; Pipeline processing systems
Parallelizing data processing on FPGAs with shifter lists,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929154549&doi=10.1145%2f2629551&partnerID=40&md5=f28b1e002b761d68e70f39a3baab21cd,"Parallelism is currently seen as a mechanism to minimize the impact of the power and heat dissipation problems encountered in modern hardware. Data parallelism-based on partitioning the data-and pipeline parallelism-based on partitioning the computation-are the two main approaches to leverage parallelism on a wide range of hardware platforms. Unfortunately, not all data processing problems are susceptible to either of those strategies. An example is the skyline operator [Borzsonyi et al. 2001], which computes the set of Pareto-optimal points within a multidimensional dataset. Existing approaches to parallelize the skyline operator are based on data parallelism. As a result, they suffer from a high overhead when merging intermediate results because of the lack of a global view of the problem inherent to partitioning the input data. In this article, we show how to combine pipeline with data parallelism on a Field-ProgrammableGate Array (FPGA) for a more efficient utilization of the available hardware parallelism. As we show in our experiments, skyline computation using our proposed technique scales linearly with the number of processing elements, and the performance we achieve on a rather small FPGA is comparable to that of a 64-core high-end server running a state-of-the-art data parallel implementation of skyline [Park et al. 2009]. The proposed approach to parallelize the skyline operator can be generalized to a wider range of data processing problems. We demonstrate this through a novel, highly parallel data structure, a shifter list, that can be efficiently implemented on an FPGA. The resulting template is easy to parametrize to implement a variety of computationally intensive operators such as frequent items, n-closest pairs, or K-means. © 2015 ACM.",Database; FPGA; Frequent items; K-means; N-closest pairs; Parallelism; Pipeline; Shifter list; Skyline query,Computer hardware; Database systems; Field programmable gate arrays (FPGA); Hardware; Pareto principle; Pipelines; Query processing; Frequent items; K-means; Parallelism; Shifter list; Skyline query; Data handling
A runtime FPGA placement and routing using low-complexity graph traversal,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926058391&doi=10.1145%2f2660775&partnerID=40&md5=4a1747361907f52be2cb216bc1d98eeb,"Dynamic Partial Reconfiguration (DPaR) enables efficient allocation of logic resources by adding new functionalities or by sharing and/or multiplexing resources over time. Placement and routing (P&R) is one of the most time-consuming steps in the DPaR flow. P&R are two independent NP-complete problems, and, even for medium size circuits, traditional P&R algorithms are not capable of placing and routing hardware modules at runtime. We propose a novel runtime P&R algorithm for Field-Programmable Gate Array (FPGA)-based designs. Our algorithm models the FPGA as an implicit graph with a direct correspondence to the target FPGA. The P&R is performed as a graph mapping problem by exploring the node locality during a depth-first traversal. We perform the P&R using a greedy heuristic that executes in polynomial time. Unlike state-of-the-art algorithms, our approach does not try similar solutions, thus allowing the P&R to execute in milliseconds. Our algorithm is also suitable for P&R in fragmented regions. We generate results for a manufacturer-independent virtual FPGA. Compared with the most popular P&R tool running the same benchmark suite, our algorithm is up to three orders of magnitude faster. © 2015 ACM 1936-7406/2015/03-ART9 $15.00.",FPGA; Graph traversal; Place and route; Run-time reconfiguration,Algorithms; Computational complexity; Polynomial approximation; Dynamic partial reconfiguration; Efficient allocations; Graph traversals; Place and route; Placement and routing; Run time reconfiguration; State-of-the-art algorithms; Three orders of magnitude; Field programmable gate arrays (FPGA)
Guest editorial FPL 2013,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929178744&doi=10.1145%2f2737805&partnerID=40&md5=57b662b113cef6a18a45379afe3ebde9,[No abstract available],,
Solving the global atmospheric equations through heterogeneous reconfigurable platforms,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929147309&doi=10.1145%2f2629581&partnerID=40&md5=539130a5e3f30402e8e77f3ec2303ec3,"One of the most essential and challenging components in climate modeling is the atmospheric model. To solve multiphysical atmospheric equations, developers have to face extremely complex stencil kernels that are costly in terms of both computing and memory resources. This article aims to accelerate the solution of global shallow water equations (SWEs), which is one of the most essential equation sets describing atmospheric dynamics. We first design a hybrid methodology that employs both the host CPU cores and the field-programmable gate array (FPGA) accelerators to work in parallel. Through a careful adjustment of the computational domains, we achieve a balanced resource utilization and a further improvement of the overall performance. By decomposing the resource-demanding SWE kernel, we manage to map the double-precision algorithm into three FPGAs. Moreover, by using fixed-point and reduced-precision floating point arithmetic, we manage to build a fully pipelined mixed-precision design on a single FPGA, which can perform 428 floating-point and 235 fixed-point operations per cycle. The mixed-precision design with four FPGAs running together can achieve a speedup of 20 over a fully optimized design on a CPU rack with two eight-core processorsand is 8 times faster than the fully optimized Kepler GPU design. As for power efficiency, the mixed-precision design with four FPGAs is 10 times more power efficient than a Tianhe-1A supercomputer node. © 2015 ACM.",Atmospheric equations; FPGAs; High performance computing; Hybrid algorithm; Mixed-precision design,Climate models; Design; Digital arithmetic; Equations of motion; Field programmable gate arrays (FPGA); Fixed point arithmetic; Supercomputers; Computational domains; Fixed point operation; High performance computing; Hybrid algorithms; Mixed precision; Reconfigurable plat-forms; Resource utilizations; Shallow water equation (SWEs); Integrated circuit design
Timing-driven titan: Enabling large benchmarks and exploring the gap between academic and commercial CAD,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929156009&doi=10.1145%2f2629579&partnerID=40&md5=be18582f2557ac97e82dc4b6f1e3b2bb,"Benchmarks play a key role in Field-Programmable Gate Array (FPGA) architecture and CAD research, enabling the quantitative comparison of tools and architectures. It is important that these benchmarks reflect modern large-scale systems that make use of heterogeneous resources; however, most current FPGA benchmarks are both small and simple. In this artile, we present Titan, a hybrid CAD flow that addresses these issues. The flow uses Altera's Quartus II FPGA CAD software to perform HDL synthesis and a conversion tool to translate the result into the academic Berkeley Logic Interchange Format (BLIF). Using this flow, we created the Titan23 benchmark set, which consists of 23 large (90K-1.8M block) benchmark circuits covering a wide range of application domains. Using the Titan23 benchmarks and an enhanced model of Altera's Stratix IV architecture, including a detailed timing model, we compare the performance and quality of VPR andQuartus II targeting the same architecture.We found that VPR is at least 2.8×slower, uses 6.2× more memory, 2.2× more wire, and produces critical paths 1.5× slower compared to Quartus II. Finally, we identified that VPR's focus on achieving a dense packing and an inability to take apart clusters is responsible for a large portion of the wire length and critical path delay gap. © 2015 ACM.",Benchmarks; CAD; FPGA,Computer aided design; Computer hardware description languages; Field programmable gate arrays (FPGA); Large scale systems; Logic Synthesis; Wire; Benchmark circuit; Critical path delays; Critical Paths; Dense packing; Heterogeneous resources; Interchange formats; Quantitative comparison; Timing modeling; Benchmarking
Identification of dynamic circuit specialization opportunities in RTL code,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924803431&doi=10.1145%2f2629640&partnerID=40&md5=241610d848d2c2a35055fcbde60be2bf,"Dynamic Circuit Specialization (DCS) optimizes a Field-Programmable Gate Array (FPGA) design by as-suming a set of its input signals are constant for a reasonable amount of time, leading to a smaller and faster FPGA circuit. When the signals actually change, a new circuit is loaded into the FPGA through runtime reconfiguration. The signals the design is specialized for are called parameters. For certain designs, parameters can be selected so the DCS implementation is both smaller and faster than the original implementation. However, DCS also introduces an overhead that is difficult for the designer to take into account, making it hard to determine whether a design is improved by DCS or not. This article presents extensive results on a profiling methodology that analyses Register-Transfer Level (RTL) implementations of applications to check if DCS would be beneficial. It proposes to use the functional density as a measure for the area efficiency of an implementation, as this measure contains both the overhead and the gains of a DCS implementation. The first step of the methodology is to analyse the dynamic behaviour of signals in the design, to find good parameter candidates. The overhead of DCS is highly dependent on this dynamic behaviour. A second stage calculates the functional density for each candidate and compares it to the functional density of the original design. The profiling methodology resulted in three implementations of a profiling tool, the DCS-RTL profiler. The execution time, accuracy, and the quality of each implementation is assessed based on data from 10 RTL designs. All designs, except for the two 16-bit adaptable Finite Impulse Response (FIR) filters, are analysed in 1 hour or less. © 2015 ACM 1936-7406/2015/02-ART4 $15.00.",Dynamic circuit specialization; RTL profiling,Codes (symbols); Field programmable gate arrays (FPGA); FIR filters; Impulse response; Timing circuits; Area efficiency; Dynamic behaviours; Dynamic Circuits; Functional density; Profiling tools; Register transfer level; RTL profiling; Run time reconfiguration; Integrated circuit design
SATTA: A self-adaptive temperature-based TDF awareness methodology for dynamically reconfigurable FPGAs,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924809462&doi=10.1145%2f2659001&partnerID=40&md5=52e13bc32c0ab10f1ba2c95b9f4bd323,"Dependability issues due to nonfunctional properties are emerging as a major cause of faults in modern digital systems. Effective countermeasures have to be developed to properly manage their critical timing effects. This article presents a methodology to avoid transition delay faults in field-programmable gate array (FPGA)-based systems, with low area overhead. The approach is able to exploit temperature information and aging characteristics to minimize the cost in terms of performances degradation and power consumption. The architecture of a hardware manager able to avoid delay faults is presented and analyzed extensively, as well as its integration in the standard implementation design flow. © 2015 ACM 1936-7406/2015/02-ART1 $15.00.",Aging; FPGA; Partial reconfiguration; Transition delay faults,Aging of materials; Field programmable gate arrays (FPGA); Aging characteristics; Dependability issues; Digital system; Implementation design; Non functional properties; Partial reconfiguration; Temperature information; Transition delay faults; Reconfigurable hardware
The cibola flight experiment,2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924813604&doi=10.1145%2f2629556&partnerID=40&md5=81fb2a45ea70f88ad754ccb80aec884b,"Over the past 15 years many organizations have researched the use of Static-Random Access Memory (SRAM)-based Field-Programmable Gate Arrays (FPGAs) in space. Although the components can provide a performance improvement over radiation-hardened processing components, random soft errors can occur from the naturally occurring space radiation environment. Many organizations have been developing methods for characterizing, emulating, and simulating radiation-induced events; mitigating and removing radiation-induced computational errors; and designing fault-tolerant reconfigurable spacecraft. Los Alamos National Laboratory has fielded one of the longest space-based FPGAs experiments, called the Cibola Flight Experiment (CFE), using Xilinx Virtex FPGAs. CFE has successfully deployed commercial SRAM FPGAs into a low-Earth orbit with Single-Event Upset (SEU) mitigation and was able to exploit effectively the reconfig-urability and customization of FPGAs in a harsh radiation environment. Although older than current state-of-the-art FPGAs, these same concepts are used to deploy newer FPGA-based space systems since the launch of the CFE satellite and will continue to be useful for newer systems. In this article, we present how the system was designed to be fault tolerant, prelaunch predictions of expected on-orbit behaviors, and on-orbit results; © 2015 ACM 1936-7406/2015/02-ART3 $15.00.",,Computer control systems; Fault tolerance; Field programmable gate arrays (FPGA); Radiation hardening; Random errors; Static random access storage; Computational error; Flight experiments; Harsh radiation environment; Los Alamos National Laboratory; Naturally occurring; Single event upsets; Space radiation environment; Static random access memory; Orbits
"A tradeoff analysis of FPGAs, GPUs, and multicores for sliding-window applications",2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924764869&doi=10.1145%2f2659000&partnerID=40&md5=d9e25cfcad996a9968a1609496bddf29,"The increasing usage of hardware accelerators such as Field-Programmable Gate Arrays (FPGAs) and Graphics Processing Units (GPUs) has significantly increased application design complexity. Such complexity results from a larger design space created by numerous combinations of accelerators, algorithms, and hw/sw partitions. Exploration of this increased design space is critical due to widely varying performance and energy consumption for each accelerator when used for different application domains and different use cases. To address this problem, numerous studies have evaluated specific applications across different architectures. In this article, we analyze an important domain of applications, referred to assliding-window applications, implemented on FPGAs, GPUs, and multicore CPUs. For each device, we present optimization strategies and analyze use cases where each device is most effective. The results show that, for large input sizes, FPGAs can achieve speedups of up to 5.6× and 58× compared to GPUs and multicore CPUs, respectively, while also using up to an order of magnitude less energy. For small input sizes and applications with frequency-domain algorithms, GPUs generally provide the best performance and energy. © 2015 ACM 1936-7406/2015/02-ART2 $15.00.",FPGA; GPU; Multicore; Parallelism; Sliding window; Speedup,Computer graphics; Computer hardware; Energy utilization; Frequency domain analysis; Graphics processing unit; Integrated circuit design; Program processors; Frequency domain algorithm; Hardware accelerators; Multi core; Optimization strategy; Parallelism; Sliding Window; Sliding-window applications; Speedup; Field programmable gate arrays (FPGA)
Microkernel architecture and hardware abstraction layer of a Reliable Reconfigurable Real-Time Operating System (R3TOS),2015,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924762044&doi=10.1145%2f2629639&partnerID=40&md5=1077a357cfe8af4e8d4b2e2e49a82f6c,"This article presents a new solution for easing the development of reconfigurable applications using Field-Programable Gate Arrays (FPGAs). Namely, our Reliable Reconfigurable Real-Time Operating System (R3TOS) provides OS-like support for partially reconfigurable FPGAs. Unlike related works, R3TOS is founded on the basis of resource reusability and computation ephemerality. It makes intensive use of reconfiguration atvery fine FPGA granularity, keeping the logic resources used only while performing computation and releasing them as soon as it is completed. To achieve this goal, R3TOS goes beyond the traditional approach of using reconfigurable slots with fixed boundaries interconnected by means of a static communication infrastructure. Instead, R3TOS approaches a static route-free system where nearly everything is reconfig-urable. The tasks are concatenated to form a computation chain through which partial results naturally flow, and data are exchanged among remotely located tasks using FPGA's reconfiguration mechanism or by means of ""removable"" routing circuits. In this article, we describe the R3TOS microkernel architecture as well as its hardware abstraction services and programming interface. Notably, the article presents a set of novel circuits and mechanisms to overcome the limitations and exploit the opportunities of Xilinx reconfigurable technology in the scope of hardware multitasking and dependability. © 2015 ACM 1936-7406/2015/02-ART5 $15.00.",Adaptable computing; FPGA architecture; Hardware virtualization; Operating systems; Runtime reconfiguration,Abstracting; Computation theory; Computer hardware; Computer operating systems; Field programmable gate arrays (FPGA); Real time systems; Reconfigurable architectures; Reusability; Adaptable computing; Communication infrastructure; FPGA architectures; Hardware Abstraction Layers; Hardware virtualization; Reconfigurable technologies; Reconfiguration mechanisms; Run time reconfiguration; Reconfigurable hardware
"Dynamic energy, performance, and accuracy optimization and management using automatically generated constraints for separable 2D FIR filtering for digital video processing",2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920846363&doi=10.1145%2f2629623&partnerID=40&md5=17e6e29b32965cf40212996d9260f621,"There is strong interest in the development of dynamically reconfigurable systems that can meet realtime constraints on energy, performance, and accuracy. The generation of real-time constraints will significantly expand the applicability of dynamically reconfigurable systems to new domains, such as digital video processing. We develop a dynamically reconfigurable 2D FIR filtering system that can meet real-time constraints in energy, performance, and accuracy (EPA). The real-time constraints are automatically generated based on user input, image types associated with video communications, and video content. We first generate a set of Pareto-optimal realizations, described by their EPA values and associated 2D FIR hardware description bitstreams. Dynamic management is then achieved by selecting Pareto-optimal realizations that meet the automatically generated time-varying EPA constraints. We validate our approach using three different 2D Gaussian filters. Filter realizations are evaluated in terms of the required energy per frame, accuracy of the resulting image, and performance in frames per second. We demonstrate dynamic EPA management by applying a Difference of Gaussians (DOG) filter to standard video sequences. For video frame sizes that are equal to or larger than the VGA resolution, compared to a static implementation, our dynamic system provides significant reduction in the total energy consumption (>30%). © 2014 ACM.",2D separable FIR filtering; Dynamic partial reconfiguration; FPGA,Computer graphics; Energy utilization; Field programmable gate arrays (FPGA); FIR filters; Multimedia systems; Pareto principle; Real time systems; Video recording; Visual communication; Automatically generated; Difference of Gaussians; Digital video processing; Dynamic partial reconfiguration; Dynamically reconfigurable systems; FIR filtering; Hardware descriptions; Total energy consumption; Video signal processing
GROK-LAB: Generating real on-chip knowledge for intra-cluster delays using Timing Extraction,2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920827097&doi=10.1145%2f2597889&partnerID=40&md5=a9e3abc3b0f84ae80aba20fcc211fc64,"Timing Extraction identifies the delay of fine-grained components within an FPGA. From these computed delays, the delay of any path can be calculated. Moreover, a comparison of the fine-grained delays allows a detailed understanding of the amount and type of process variation that exists in the FPGA. To obtain these delays, Timing Extraction measures, using only resources already available in the FPGA, the delay of a small subset of the total paths in the FPGA. We apply Timing Extraction to the Logic Array Block (LAB) on an Altera Cyclone III FPGA to obtain a view of the delay down to near-individual LUT SRAM cell granularity, characterizing components with delays on the order of tens to a few hundred picoseconds with a resolution of ±3.2ps, matching the expected error bounds. This information reveals that the 65nm process used has, on average, random variation of σ/μ = 4.0% with components having an average maximum spread of 83ps. Timing Extraction also shows that as Vdd decreases from 1.2V to 0.9V in a Cyclone IV 60nm FPGA, paths slow down, and variation increases from σ/μ = 4.3% to σ/μ = 5.8%, a clear indication that lowering Vdd magnifies the impact of random variation.",Component-specific mapping; In-system measurement; Variation characterization; Variation measurment,Error analysis; Extraction; Altera cyclones; Fine-grained components; Logic array blocks; Maximum spread; Measurment; Process Variation; Random variation; System measurement; Field programmable gate arrays (FPGA)
Mapping adaptive particle filters to heterogeneous reconfigurable systems,2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920829206&doi=10.1145%2f2629469&partnerID=40&md5=f25142da2e168124ce865565ea78913c,"This article presents an approach for mapping real-time applications based on particle filters (PFs) to heterogeneous reconfigurable systems, which typically consist of multiple FPGAs and CPUs. A method is proposed to adapt the number of particles dynamically and to utilise runtime reconfigurability of FPGAs for reduced power and energy consumption. A data compression scheme is employed to reduce communication overhead between FPGAs and CPUs. A mobile robot localisation and tracking application is developed to illustrate our approach. Experimental results show that the proposed adaptive PF can reduce up to 99% of computation time. Using runtime reconfiguration, we achieve a 25% to 34% reduction in idle power. A 1U system with four FPGAs is up to 169 times faster than a single-core CPU and 41 times faster than a 1U CPU server with 12 cores. It is also estimated to be 3 times faster than a system with four GPUs. © 2014 ACM.",Algorithms; Design; Performance,Algorithms; Data compression; Design; Energy utilization; Field programmable gate arrays (FPGA); Mapping; Monte Carlo methods; Program processors; Real time systems; Structural design; Adaptive particle filters; Communication overheads; Particle filters (PFs); Performance; Power and energy consumption; Reconfigurable systems; Run time reconfiguration; Runtime reconfigurability; Reconfigurable hardware
A reconfigurable architecture for binary acceleration of loops with memory accesses,2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911387509&doi=10.1145%2f2629468&partnerID=40&md5=e9721894d65e206d921d000f01dd7ee1,"This article presents a reconfigurable hardware/software architecture for binary acceleration of embedded applications. A Reconfigurable Processing Unit (RPU) is used as a coprocessor of the General Purpose Processor (GPP) to accelerate the execution of repetitive instruction sequences called Megablocks. A toolchain detects Megablocks from instruction traces and generates customized RPU implementations. The implementation of Megablocks with memory accesses uses a memory-sharing mechanism to support concurrent accesses to the entire address space of the GPP's data memory. The scheduling of load/store operations and memory access handling have been optimized to minimize the latency introduced by memory accesses. The system is able to dynamically switch the execution between the GPP and the RPU when executing the original binaries of the input application. Our proof-of-concept prototype achieved geometric mean speedups of 1.60 × and 1.18 × for, respectively, a set of 37 benchmarks and a subset considering the 9 most complex benchmarks. With respect to a previous version of our approach, we achieved geometric mean speedup improvements from 1.22 to 1.53 for the 10 benchmarks previously used. © 2014 ACM.",FPGA; Hardware acceleration; Hardware/software architectures; Instruction trace; Megablock; Memory access; Microblaze; Reconfigurable processor,Computer hardware; Field programmable gate arrays (FPGA); General purpose computers; Hardware; Reconfigurable architectures; Reconfigurable hardware; Hardware acceleration; Hardware/software; Instruction trace; Mega blocks; Memory access; Microblaze; Reconfigurable processors; Memory architecture
Dynamic power and thermal management of NoC-based heterogeneous MPSoCs,2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897811422&doi=10.1145%2f2567658&partnerID=40&md5=5f33b441920777839798d64de8cae890,"Advances in silicon process technology have made it possible to include multiple processor cores on a single die. Billion transistor architectures usually in the form of networks-on-chip present a wide range of challenges in design, microarchitecture, and algorithmic levels with significant impact to system performance and power consumption. In this article, we propose efficient methods and mechanisms that exploit a heterogeneous network-on-chip (NoC) to achieve a power- and thermal-aware coherent system. To this end, we utilize different management techniques which employ dynamic frequency scaling circuitry and power and temperature sensors per node to achieve real-time workload prediction and allocation at node and system level by low-cost threads. The developed heterogeneous multicoprocessing infrastructure is utilized to evaluate diverse policies for power-aware computing in terms of effectiveness and in relation to distributed sensor-conscious management. The proposed reconfigurable architecture supports coprocessor accelerators per node, monitors the program's power profile on-the-fly, and balances power and thermal behavior at the NoC level. Overall, these techniques form a system exploration methodology using a multi-FPGA emulation platform showing a minimum complexity overhead. © 2014 ACM 1936-7406/2014/02-ART1 $15.00.",Algorithms; Design,Algorithms; Design; Field programmable gate arrays (FPGA); Heterogeneous networks; Reconfigurable architectures; VLSI circuits; Dynamic frequency scaling; Management techniques; Micro architectures; Multiple processors; Power-aware computing; Silicon process technology; Transistor architecture; Workload predictions; Microprocessor chips
A fully pipelined FPGA architecture of a factored restricted Boltzmann machine artificial neural network,2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897766132&doi=10.1145%2f2539125&partnerID=40&md5=f6bf1a91e54305e4358917a5c8cfa3a9,"Artificial neural networks (ANNs) are a natural target for hardware acceleration by FPGAs and GPGPUs because commercial-scale applications can require days to weeks to train using CPUs, and the algorithms are highly parallelizable. Previous work on FPGAs has shown how hardware parallelism can be used to accelerate a ""Restricted Boltzmann Machine"" (RBM) ANN algorithm, and how to distribute computation across multiple FPGAs. Here we describe a fully pipelined parallel architecture that exploits ""mini-batch"" training (combining many input cases to compute each set of weight updates) to further accelerate ANN training. We implement on an FPGA, for the first time to our knowledge, a more powerful variant of the basic RBM, the ""Factored RBM"" (fRBM). The fRBM has proved valuable in learning transformations and in discovering features that are present across multiple types of input. We obtain (in simulation) a 100-fold acceleration (vs. CPU software) for an fRBM having N = 256 units in each of its four groups (two input, one output, one intermediate group of units) running on a Virtex-6 LX760 FPGA. Many of the architectural features we implement are applicable not only to fRBMs, but to basic RBMs and other ANN algorithms more broadly. © 2014 ACM 1936-7406/2014/02-ART5 $15.00.",Design,Algorithms; Design; Hardware; Parallel architectures; Program processors; ANN trainings; Architectural features; FPGA architectures; Fully pipelined; Hardware acceleration; Hardware parallelisms; Natural targets; Restricted boltzmann machine; Neural networks
Fast and accurate stereo vision system on FPGA,2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897760679&doi=10.1145%2f2567659&partnerID=40&md5=8acdcd8d2e57c18cad5434b0744ed024,"In this article, we present a fast and high quality stereo matching algorithm on FPGA using cost aggregation (CA) and fast locally consistent (FLC) dense stereo. In many software programs, global matching algorithms are used in order to obtain accurate disparity maps. Although their error rates are considerably low, their processing speeds are far from that required for real-time processing because of their complex processing sequences. In order to realize real-time processing, many hardware systems have been proposed to date. They have achieved considerably high processing speeds; however, their error rates are not as good as those of software programs, because simple local matching algorithms have been widely used in those systems. In our system, sophisticated local matching algorithms (CA and FLC) that are suitable for FPGA implementation are used to achieve low error rate while maintaining the high processing speed. We evaluate the performance of our circuit on Xilinx Vertex-6 FPGAs. Its error rate is comparable to that of top-level software algorithms, and its processing speed is nearly 2 clock cycles per pixel, which reaches 507.9 fps for 640 × 480 pixel images. © 2014 ACM 1936-7406/2014/02-ART3 $15.00.",Algorithms; Design; Performance,Algorithms; Computer software; Computer vision; Design; Pixels; Stereo vision; Complex processing; FPGA implementations; Global matching algorithm; Performance; Realtime processing; Software algorithms; Stereo matching algorithm; Stereo vision system; Image matching
High-level abstractions and modular debugging for FPGA design validation,2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897800330&doi=10.1145%2f2567662&partnerID=40&md5=fe9fcc0f7e3bfe22496a371faf537c47,"Design validation is the most time-consuming task in the FPGA design cycle. Although manufacturers and third-party vendors offer a range of tools that provide visibility and control of the different stages of a design, many require that the design be fully re-implemented for even simple parameter modifications or do not allow the design to be run at full speed. Designs are typically first modeled using a high-level language then later rewritten in a hardware description language, first for simulation and then later modified for synthesis. IP and third-party cores may differ during these final two stages complicating development and validation. The developed approach provides two means of directly validating synthesized hardware designs. The first allows the original high-level model written in C or C++ to be directly coupled to the synthesized hardware, abstracting away the traditional gate-level view of designs. A high-level programmatic interface allows the synthesized design to be validated directly by the software reference model. The second approach provides an alternative view to FPGAs within the scope of a traditional software debugger. This debug framework leverages partially reconfigurable regions to accelerate themodification of dynamic, software-like breakpoints for low-level analysis and provides a automatable, scriptable, command-line interface directly to a running design on an FPGA. © 2014 ACM 1936-7406/2014/02-ART2 $15.00.",Design; Verification,Abstracting; Computer hardware description languages; Design; Hardware; Reconfigurable hardware; Verification; Command-line interfaces; Design validation; High-level abstraction; High-level modeling; Low-level analysis; Parameter modification; Reference modeling; Time-consuming tasks; Program debugging
"Fast design exploration for performance, power and accuracy tradeoffs in FPGA-based accelerators",2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897762086&doi=10.1145%2f2567661&partnerID=40&md5=173c41548520756fbd789e43af3e6943,"The ease-of-use and reconfigurability of FPGAs makes them an attractive platform for accelerating algorithms. However, accelerating becomes a challenging task as the large number of possible design parameters lead to different accelerator variants. In this article, we propose techniques for fast design exploration and multi-objective optimization to quickly identify both algorithmic and hardware parameters that optimize these accelerators. This information is used to run regression analysis and train mathematical models within a nonlinear optimization framework to identify the optimal algorithm and design parameters under various objectives and constraints. To automate and improve the model generation process, we propose the use of L1-regularized least squares regression techniques.We implement two real-time image processing accelerators as test cases: one for image deblurring and one for block matching. For these designs, we demonstrate that by sampling only a small fraction of the design space (0.42% and 1.1%), our modeling techniques are accurate within 2%4% for area and throughput, 8%9% for power, and 5%6% for arithmetic accuracy. We show speedups of 340× and 90× in time for the test cases compared to brute-force enumeration. We also identify the optimal set of parameters for a number of scenarios (e.g., minimizing power under arithmetic inaccuracy bounds). © 2014 ACM 1936-7406/2014/02-ART4 $15.00.",Design; Performance,Algorithms; Field programmable gate arrays (FPGA); Image enhancement; Mathematical models; Multiobjective optimization; Regression analysis; Accelerating algorithm; Design parameters; Hardware parameters; L1-regularized least squares; Modeling technique; Non-linear optimization; Performance; Real-time image processing; Design
Composing multi-ported memories on FPGAs,2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907357382&doi=10.1145%2f2629629&partnerID=40&md5=0a6174daba7ad1a14d76aaa38acbb099,"Multi-ported memories are challenging to implement on FPGAs since the block RAMs included in the fabric typically have only two ports. Hence we must construct memories requiring more than two ports, either out of logic elements or by combining multiple block RAMs. We present a thorough exploration and evaluation of the design space of FPGA-based soft multi-ported memories for conventional solutions, and also for the recently proposed Live Value Table (LVT) [LaForest and Steffan 2010] and XOR [LaForest et al. 2012] approaches to unidirectional portmemories, reporting results for both Altera and Xilinx FPGAs. Additionally, we thoroughly evaluate and compare with a recent LVT-based approach to bidirectional port memories [Choi et al. 2012]. © 2014 ACM.",FPGA; LVT; Memory; Multi-Port; Parallel; XOR,Computer networks; Computer science; Data storage equipment; Field programmable gate arrays (FPGA); Bidirectional ports; Design spaces; Logic elements; LVT; Multi-port; Parallel; Xilinx fpgas; XOR; Random access storage
ULP-SRP: Ultra low-power Samsung reconfigurable processor for biomedical applications,2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907355000&doi=10.1145%2f2629610&partnerID=40&md5=2fbda7df177f271183936e04badb3cc4,"The latest biomedical applications require low energy consumption, high performance, and wide energyperformance scalability to adapt to various working environments. In this study, we present ULP-SRP, an energy-efficient reconfigurable processor for biomedical applications. ULP-SRP uses a Coarse-Grained Reconfigurable Array (CGRA) for high-performance data processing with low energy consumption. We adopted a compact-size CGRA and modified it to support dynamically switchable three performance modes with finegrained power gating in order to further optimize the energy consumption. The energy-performance scalability is also accomplished with multiple performance modes and a Unified Memory Architecture (UMA). Experimental results show that ULP-SRP achieved 59% energy reduction compared to previous works. A technique of dynamic CGRA mode changing gives 18.9% energy reduction. ULP-SRP is a good candidate for future mobile healthcare devices. © 2014 ACM.",Biomedical; Coarse grained; Low power; Power gating; Reconfigurable processor; Wireless sensor node,Energy utilization; Memory architecture; Scalability; Sensor nodes; Biomedical; Coarse-grained; Low Power; Power gatings; Reconfigurable processors; Wireless sensor node; Medical applications
Introduction to the special issue on the 7th international workshop on reconfigurable communication-centric systems-on-chip (ReCoSoC'12),2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907310590&doi=10.1145%2f2655710&partnerID=40&md5=ebf701452320625cefed18199f40bda0,[No abstract available],,
RIVER: Reconfigurable flow and fabric for real-time signal processing on FPGAs,2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907303384&doi=10.1145%2f2655238&partnerID=40&md5=3586c2ab22f7de9d70424feeeaaa4827,"For high-performance embedded hard-real-time systems, ASICs and FPGAs hold advantages over generalpurpose processors and graphics accelerators (GPUs). However, developing signal processing architectures from scratch requires significant resources. Our design methodology is based on sets of configurable building blocks that provide storage, dataflow, computation, and control. Based on our building blocks, we generate hundreds of thousands of our dynamic streaming engine processors that we call DSEs. We store our DSEs in a repository that can be queried for (online) design space exploration. From this repository, DSEs can be downloaded and instantiated within milliseconds on FPGAs. If a loss of flexibility can be tolerated then ASIC implementations are feasible as well. In this article we focus on FPGA implementations. Our DSEs vary in cores, computational lanes, bitwidths, power consumption, and frequency. To the best of our knowledge we are the first to propose online design space exploration based on repositories of precompiled cores that are assembled of common building blocks. For demonstration purposes we map algorithms for image processing and financial mathematics to DSEs and compare the performance to existing highly optimized signal and graphics accelerators. © 2014 ACM.",Finanical mathematics; FPGA; Image processing; Low power; Microarchitecture; Multicore; Reconfigurable systems; Signal processing,Application specific integrated circuits; Computer architecture; Field programmable gate arrays (FPGA); Image processing; Program processors; Signal processing; Structural design; Design space exploration; General purpose processors; Low Power; Micro architectures; Multi core; Processing architectures; Real-time signal processing; Reconfigurable systems; Reconfigurable hardware
Exploiting FPGA-aware merging of custom instructions for runtime reconfiguration,2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907359957&doi=10.1145%2f2655240&partnerID=40&md5=be672ae27df509c6e28ae126edbc165a,"Runtime reconfiguration is a promising solution for reducing hardware cost in embedded systems, without compromising on performance. We present a framework that aims to increase the performance benefits of reconfigurable processors that support full or partial runtime reconfiguration. The proposed framework achieves this by: (1) providing a means for choosing suitable custom instruction selection heuristics, (2) leveraging FPGA-awaremerging of custom instructions to maximize the reconfigurable logic block utilization in each configuration, and (3) incorporating a hierarchical loop partitioning strategy to reduce runtime reconfiguration overhead. We show that the performance gain can be improved by employing suitable custom instruction selection heuristics that, in turn, depend on the reconfigurable resource constraints and the merging factor (extent to which the selected custom instructions can be merged). The hierarchical loop partitioning strategy leads to an average performance gain of over 31% and 46% for full and partial runtime reconfiguration, respectively. Performance gain can be further increased to over 52% and 70% for full and partial runtime reconfiguration, respectively, by exploiting FPGA-aware merging of custom instructions. © 2014 ACM.",Custom instructions; FPGA; Full/partial runtime reconfiguration; Loop partitioning; Reconfigurable processors,Cost reduction; Merging; Program processors; Custom instruction; Loop partitioning; Partial run-time reconfiguration; Performance benefits; Reconfigurable logic blocks; Reconfigurable processors; Reconfigurable resources; Run time reconfiguration; Field programmable gate arrays (FPGA)
"Networks-on-chip for FPGAs: Hard, soft or mixed?",2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907335247&doi=10.1145%2f2629442&partnerID=40&md5=da7f4472899f2315d48c80b5d9a81def,"As FPGA capacity increases, a growing challenge is connecting ever-more components with the current low-level FPGA interconnect while keeping designers productive and on-chip communication efficient. We propose augmenting FPGAs with networks-on-chip (NoCs) to simplify design, and we show that this can be done while maintaining or even improving silicon efficiency. We compare the area and speed efficiency of each NoC component when implemented hard versus soft to explore the space and inform our design choices. We then build on this component-level analysis to architect hard NoCs and integrate them into the FPGA fabric; these NoCs are on average 20-23× smaller and 5-6× faster than soft NoCs. A 64-node hard NoC uses only ∼2% of an FPGA's silicon area and metallization. We introduce a new communication efficiency metric: silicon area required per realized communication bandwidth. Soft NoCs consume 4960 mm2/TBps, but hard NoCs are 84× more efficient at 59 mm 2/TBps. Informed design can further reduce the area overhead of NoCs to 23 mm2/TBps, which is only 2.6× less efficient than the simplest point-to-point soft links (9 mm2/TBps). Despite this almost comparable efficiency, NoCs can switch data across the entire FPGA while point-to-point links are very limited in capability; therefore, hard NoCs are expected to improve FPGA efficiency for more complex styles of communication. © 2014 ACM.",Application-specific integrated circuit; Area; Delay; Field programmable gate array; Hard; Interconnect; Network-on-chip; Resource management; Soft,Design; Efficiency; Field programmable gate arrays (FPGA); Integrated circuit interconnects; Microprocessor chips; Silicon; VLSI circuits; Area; Delay; Hard; Interconnect; Resource management; Soft; Network-on-chip
Introduction to the special issue on the 11th international conference on field-programmable technology (FPT'12),2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907367941&doi=10.1145%2f2655712&partnerID=40&md5=ced97d6f8d50c9e21afc04e28aa5be60,[No abstract available],,
The iDEA DSP block-based soft processor for FPGAs,2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907329433&doi=10.1145%2f2629443&partnerID=40&md5=6354cd07344d92914a18aed476a06d31,"DSP blocks in modern FPGAs can be used for a wide range of arithmetic functions, offering increased performance while saving logic resources for other uses. They have evolved to better support a plethora of signal processing tasks, meaning that in other application domains they may be underutilised. The DSP48E1 primitives in new Xilinx devices support dynamic programmability that can help extend their usefulness; the specific function of aDSP block can be modified on a cycle-by-cycle basis. However, the standard synthesis flow does not leverage this flexibility in the vast majority of cases. The lean DSP Extension Architecture (iDEA) presented in this article builds around the dynamic programmability of a single DSP48E1 primitive, with minimal additional logic to create a general-purpose processor supporting a full instruction-set architecture. The result is a very compact, fast processor that can execute a full gamut of general machine instructions. We show a number of simple applications compiled using an MIPS compiler and translated to the iDEA instruction set, comparing with a Xilinx MicroBlaze to show estimated performance figures. Being based on the DSP48E1, this processor can be deployed across next-generation Xilinx Artix-7, Kintex-7, Virtex-7, and Zynq families. © 2014 ACM.",Field programmable gate arrays; Reconfigurable computing; Soft processors,Field programmable gate arrays (FPGA); General purpose computers; Number theory; Reconfigurable architectures; Additional logic; Arithmetic functions; General purpose processors; Instruction set architecture; Logic resources; Machine instructions; Reconfigurable computing; Soft processors; Digital signal processing
Adaptive parallelism exploitation under physical and real-time constraints for resilient systems,2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907322078&doi=10.1145%2f2556943&partnerID=40&md5=033986cf0e674371c4ab0cc4bf19d6f4,"This article introduces the resilient adaptive algebraic architecture that aims at adapting parallelism exploitation of a matrix multiplication algorithm in a time-deterministic fashion to reduce power consumption while meeting real-time deadlines present inmost DSP-like applications. The proposed architecture provides low-overhead error correction capabilities relying on the hardware implementation of the algorithm-based fault-tolerance method that is executed concurrently with matrix multiplication, providing efficient occupation of memory and power resources. The Resilient Adaptive Algebraic Architecture (RA3) is evaluated using three real-time industrial case studies from the telecom and multimedia application domains to present the design space exploration and the adaptation possibilities the architecture offers to hardware designers. RA3 is compared in its performance and energy efficiency with standard high-performance architectures, namely a GPU and an out-of-order general-purpose processor. Finally, we present the results of fault injection campaigns in order to measure the architecture resilience to soft errors. © 2014 ACM.",Adaptive systems; Embedded systems; Real time; Resilient design; Scaling; Soft error; Worst-case execution time,Adaptive systems; Algebra; Algorithms; Embedded systems; Energy efficiency; Fault tolerance; General purpose computers; Hardware; Matrix algebra; Microprocessor chips; Radiation hardening; Real time; Resilient design; Scaling; Soft error; Worst-case execution time; Error correction
Graph minor approach for application mapping on CGRAs,2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907301835&doi=10.1145%2f2655242&partnerID=40&md5=3734c8049f8283b34fea7e23f9d25e11,"Coarse-Grained Reconfigurable Arrays (CGRAs) exhibit high performance, improved flexibility, low cost, and power efficiency for various application domains. Compute-intensive loop kernels, which are perfect candidates to be executed on CGRAs, are usually mapped through modified modulo scheduling algorithms. These algorithms should be capable of performing both placement and routing. We formalize the CGRA mapping problem as a graph minor containment problem. We essentially test whether the dataflow graph representing the loop kernel is a minor of the modulo routing resource graph representing the CGRA resources and their interconnects.We design an exact graph minor testing approach that exploits the unique properties of both the dataflow graph and the routing resource graph to significantly prune the search space. We introduce additional heuristic strategies that drastically improve the compilation time while still generating optimal or near-optimal mapping solutions. Experimental evaluation confirms the efficiency of our approach. © 2014 ACM.",Coarse-grained reconfigurable arrays (CGRAs); Compilation; Graph minor,Data flow analysis; Mapping; Application mapping; Coarse-grained reconfigurable arrays; Compilation; Experimental evaluation; Graph minors; Heuristic strategy; Placement and routing; Routing resource graph; Optimization
"Extending UML/MARTE to support discrete controller synthesis, application to reconfigurable systems-on-chip modeling",2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907337474&doi=10.1145%2f2629628&partnerID=40&md5=82a99f2a40f2784c1dd4a464309aa411,"This article presents the first framework to design and synthesize a formal controller managing dynamic reconfiguration, using a model-driven engineering methodology based on an extension of UML/MARTE. The implementation technique highlights the combination of hard configuration constraints using weights (control part)-ensured statically and fulfilled by the system at runtime-and soft constraints (decision part) that, given a set of correct and accessible configurations, choose one of them. An application model of an image processing application is presented, then transformed and synthesized to be executed on a Xilinx platform to show how the controller, executed on a Microblaze, manages the hardware reconfigurations. © 2014 ACM.",BZR; Discrete controller synthesis; Feedback loop; Formal method; Model-driven engineering; Reactive systems; Synchronous language; UML/MARTE,Dynamic models; Formal methods; Image processing; Reconfigurable hardware; System-on-chip; BZR; Discrete controller synthesis; Feed-back loop; Model-driven Engineering; Reactive system; Synchronous languages; UML/MARTE; Structural design
Benefits of adding hardware support for broadcast and reduce operations in MPSoC applications,2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907360862&doi=10.1145%2f2629470&partnerID=40&md5=8a3310324e2a537b4a3b0b8e84dd7d5e,"MPI has been used as a parallel programming model for supercomputers and clusters and recently in Multi-Processor Systems-on-Chip (MPSoC). One component of MPI is collective communication and its performance is key for certain parallel applications to achieve good speedups. Previous work showed that, with synthetic communication-only benchmarks, communication improvements of up to 11.4-fold and 22-fold for broadcast and reduce operations, respectively, can be achieved by providing hardware support at the network level in a Network-on-Chip (NoC). However, these numbers do not provide a good estimation of the advantage for actual applications, as there are other factors that affect performance besides communications, such as computation. To this end, we extend our previous work by evaluating the impact of hardware support over a set of five parallel application kernels of varying computation-to-communication ratios. By introducing some useful computation to the performance evaluation, we obtain more representative results of the benefits of adding hardware support for broadcast and reduce operations. The experiments show that applications with lower computation-to-communication ratios benefit the most from hardware support as they highly depend on efficient collective communications to achieve better scalability. We also extend our work by doing more analysis on clock frequency, resource usage, power, and energy. The results show reasonable scalability for resource utilization and power in the network interfaces as the number of channels increases and that, even though more power is dissipated in the network interfaces due to the added hardware, the total energy used can still be less if the actual speedup is sufficient. The application kernels are executed in a 24-embedded-processor system distributed across four FPGAs. © 2014 ACM.",FPGA; MPI; Multiprocessor; Network-on-chip; Parallel computing,Field programmable gate arrays (FPGA); Hardware; Microprocessor chips; Multiprocessing systems; Parallel architectures; Parallel processing systems; Scalability; Servers; Supercomputers; VLSI circuits; Collective communications; Hardware supports; MPI; Multiprocessor; Network-on-chip(NoC); Parallel application; Parallel programming model; Resource utilizations; Network-on-chip
Design tools for implementing self-aware and fault-tolerant systems on FPGAs,2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903849187&doi=10.1145%2f2617597&partnerID=40&md5=c3307fd535f3a7d166975bbc2b1b996a,"To fully exploit the capabilities of runtime reconfigurable FPGAs in self-aware systems, design tools are required that exceed the capabilities of present vendor design tools. Such tools must allow the implementation of scalable reconfigurable systems with various different partial modules that might be loaded to different positions of the device at runtime. This comprises several complex tasks, including floorplanning, communication architecture synthesis, physical constraints generation, physical implementation, and timing verification all the way down to the final bitstream generation. In this article, we present how our GOAHEAD framework helps in implementing self-aware systems on FPGAs with a minimum of user interaction. © 2014 ACM.",Design tools; FPGAs; Reconfigurable computing; Self-aware systems,Field programmable gate arrays (FPGA); Reconfigurable architectures; Reconfigurable hardware; Bitstream generation; Communication architectures; Design tool; Fault-tolerant systems; Reconfigurable computing; Reconfigurable systems; Run-time reconfigurable; Self-aware systems; Structural design
Coordination of independent loops in self-adaptive systems,2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903824849&doi=10.1145%2f2611563&partnerID=40&md5=4acfa6b7e7429a1eaf3bbf2a7848c008,"Nowadays, the same piece of code should run on different architectures, providing performance guarantees in a variety of environments and situations. To this end, designers often integrate existing systems with ad-hoc adaptive strategies able to tune specific parameters that impact performance or energy-for example, frequency scaling. However, these strategies interfere with one another and unpredictable performance degradationmay occur due to the interaction between different entities. In this article, we propose a software approach to reconfiguration when different strategies, called loops, are encapsulated in the system and are available to be activated. Our solution to loop coordination is based on machine learning and it selects a policy for the activation of loops inside of a system without prior knowledge. We implemented our solution on top of GNU/Linux and evaluated it with a significant subset of the PARSEC benchmark suite. © 2014 ACM.",Autonomic computing; Autonomic manager; Performance management,Artificial intelligence; Open source software; Adaptive strategy; Autonomic Computing; Autonomic managers; Impact performance; Independent loops; Performance guarantees; Performance management; Self-adaptive system; Management
Intra-masking dual-rail memory on LUT implementation for SCA-resistant AES on FPGA,2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903831149&doi=10.1145%2f2617595&partnerID=40&md5=ecca42d368a68ae3e86ee4b6bfc380e5,"In current countermeasure design trends against differential power analysis (DPA), security at gate level is required in addition to the security algorithm. Several dual-rail pre-charge logics (DPL) have been proposed to achieve this goal. Designs using ASIC can attain this goal owing to its backend design restrictions on placement and routing. However, implementing these designs on field programmable gate arrays (FPGA) without information leakage is still a problem because of the difficulty involved in the restrictions on placement and routing on FPGA. This article describes our novel masked dual-rail pre-charged memory approach, called ""intra-masking dual-rail memory (IMDRM) on LUT"", and its implementation on FPGA for Side-Channel Attack-resistant (SCA-resistant) AES. In the proposed design, all unsafe nodes, such as unmasking and masking, and parts of dual-rail memory with unsafe buses (buses that are not masked) are packed into a single LUT. This makes them balanced and independent of the placement and routing tools. Inputs and outputs of all LUTs are masked, and so can be considered safe signals. Several LUTs can be combined to create a safe SBox. The design is independent of the cryptographic algorithm, and hence, it can be applied to available cryptographic standards such as DES or AES as well as future standards. It requires no special placement or route constraints in its implementation. A correlation power analysis (CPA) attack on 1,000,000 traces of AES implementation on FPGA showed that the secret information is well protected against first-order sidechannel attacks. Even though the number of LUTs used for memory in this implementation is seven times greater than that of the conventional unprotected single-rail memory table-lookup AES and three times greater than the implementation based on a composite field, it requires a smaller number of LUTs than all other advanced SCA-resistant implementations such as the wave dynamic differential logic, masked dual-rail pre-charge logic, and threshold. © 2014 ACM.",AES; Differential power analysis (DPA); Dual-rail memory; Field programmable gate array (FPGA); Intramasking dual-rail memory on LUT; Masking; SCA resistance; Side-channel attack,Algorithms; Design; Field programmable gate arrays (FPGA); Signal receivers; Speech intelligibility; Table lookup; AES; Correlation power analysis (CPA); Cryptographic algorithms; Differential power Analysis; Dual-rail memory; Dual-Rail Pre-Charge Logic; Side channel attack; Wave dynamic differential logic; Cryptography
Introduction to the TRETS Special section on the workshop on self-awareness in reconfigurable computing systems (SRCS'12),2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903825317&doi=10.1145%2f2611564&partnerID=40&md5=fd99bb75992a843607f5c24e8121e597,[No abstract available],,
A self-aware tuning and self-aware evaluation method for finite-difference applications in reconfigurable systems,2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903825205&doi=10.1145%2f2617598&partnerID=40&md5=8b444ce1a1afda29ccf0f1a5f6f4c039,"Finite-difference methods are computationally intensive and required by many applications. Parameters of a finite-difference algorithm, such as grid size, can be varied to generate design space which contains algorithm instances with different constant coefficients. An algorithm instance with specific coefficients can either be mapped into general operators to construct static designs, or be implemented as constant-specific operators to form dynamic designs, which require runtime reconfiguration to update algorithm coefficients. This article proposes a tuning method to explore the design space to optimise both the static and the dynamic designs, and an evaluation method to select the design with maximum overall throughput, based on algorithm characteristics, design properties, available resources and runtime data size. For benchmark applications option pricing and Reverse-Time Migration (RTM), over 50% reduction in resource consumption has been achieved for both static designs and dynamic designs, while meeting precision requirements. For a single hardware implementation, the RTM design optimised with the proposed approach is expected to run 1.8 times faster than the best published design. The tuned static designs run thousands of times faster than the dynamic designs for algorithms with small data size, while the tuned dynamic designs achieve up to 5.9 times speedup over the corresponding static designs for large-scale finite-difference algorithms. © 2014 ACM.",Algorithm tuning; Finite-difference methods; Reconfigurable computing,Algorithms; Finite difference method; Hardware; Reconfigurable architectures; Seismic prospecting; Benchmark applications; Constant coefficients; Finite-difference algorithms; Hardware implementations; Reconfigurable computing; Reconfigurable systems; Reverse-time migration (RTM); Run time reconfiguration; Structural design
Multi-application network-on-chip design using global mapping and local reconfiguration,2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903844893&doi=10.1145%2f2556944&partnerID=40&md5=609d992e8697a1eceeb1090682597e55,"This article proposes a reconfigurable Network-on-Chip (NoC) architecture based on mesh topology. It provides a local reconfiguration of cores to connect to any of the neighboring routers, depending upon the currently executing application. The area overhead for this local reconfiguration has been shown to be very small.We have also presented the strategy to map the cores of an application set onto this architecture. This has been achieved via a two-phase procedure. In the first phase, the cores of the combined application set are mapped tentatively to individual routers, minimizing the communication cost. In the second phase, for each application, positions of individual cores are finalized. A core gets attached to any neighbor of its tentative allocation. We have proposed Integer Linear Programming (ILP) formulation of both the phases. Since ILP takes large amount of CPU time, we have also formulated a Particle Swarm Optimization (PSO)-based solution for the two phases. A heuristic approach has also been developed for the reconfiguration. Comparison of communication cost, latency and network energy have been carried out for the applications, before and after reconfiguration. It shows significant improvement in performance via reconfiguration. © 2014 ACM.",Combined core graph; Integer linear programming; Particle swarm optimization; Reconfiguration,Communication; Heuristic methods; Integer programming; Network architecture; Particle swarm optimization (PSO); Reconfigurable architectures; Routers; VLSI circuits; Communication cost; Core graph; Heuristic approach; Integer Linear Programming; Local reconfigurations; Network-on-chip architectures; Network-on-chip design; Reconfiguration; Network-on-chip
FPGA implementation of a special-purpose vliw structure for double-precision elementary function,2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903824925&doi=10.1145%2f2617594&partnerID=40&md5=794466359c3bb6ba6647e06e42577145,"In the current article, the capability and flexibility of field programmable gate-arrays (FPGAs) to implement IEEE-754 double-precision floating-point elementary functions are explored. To perform various elementary functions on the unified hardware efficiently, we propose a special-purpose very long instruction word (VLIW) processor, called DP VELP. this processor is equipped with multiple basic units, and its performance is improved through an explicitly parallel technique. Pipelined evaluation of polynomial approximation with Estrin's scheme is proposed, by scheduling basic components in an optimal order to avoid data hazard stalls and achieve minimal latency. the custom VLIW processor can achieve high scalability. Under the control of specific VLIW instructions, the basic units are combined into special-purpose hardware for elementary functions. Common elementary functions are presented as examples to illustrate the design of elementary function in DP VELP in detail. Minimax approximation scheme is used to reduce degree of polynomial. Compromise between the size of lookup table and the latency is discussed, and the internal precision is carefully planned to guarantee accuracy of the result. Finally, we create a prototype of the DP VELP unit and an FPGA accelerator based on the DP VELP unit on a Xilinx XC6VLX760 FPGA chip to implement the SGP4/SDP4 application. © 2014 ACM.",Double-precision floating-point; Elementary function; FPGA; Polynomial,Chebyshev approximation; Digital arithmetic; Hardware; Polynomial approximation; Polynomials; Scheduling; Very long instruction word architecture; Double precision; Elementary function; Field programmables; FPGA implementations; Minimax approximation; Parallel techniques; Special-purpose hardwares; Very long instruction words; Field programmable gate arrays (FPGA)
A mapping-scheduling algorithm for hardware acceleration on reconfigurable platforms,2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903838821&doi=10.1145%2f2611562&partnerID=40&md5=9f3dc8a39cdefefe3ed6eb7823dde739,"Reconfigurable platforms are a promising technology that offers an interesting trade-off between flexibility and performance, which many recent embedded system applications demand, especially in fields such as multimedia processing. These applications typically involvemultiple ad-hoc tasks for hardware acceleration, which are usually represented using formalisms such as Data Flow Diagrams (DFDs), Data Flow Graphs (DFGs), Control and Data Flow Graphs (CDFGs) or Petri Nets. However, none of these models is able to capture at the same time the pipeline behavior between tasks (that therefore can coexist in order to minimize the application execution time), their communication patterns, and their data dependencies. This article proves that the knowledge of all this information can be effectively exploited to reduce the resource requirements and the timing performance of modern reconfigurable systems, where a set of hardware accelerators is used to support the computation. For this purpose, this article proposes a novel task representation model, named Temporal Constrained Data Flow Diagram (TCDFD), which includes all this information. This article also presents a mapping-scheduling algorithm that is able to take advantage of the new TCDFD model. It aims at minimizing the dynamic reconfiguration overhead while meeting the communication requirements among the tasks. Experimental results show that the presented approach achieves up to 75% of resources saving and up to 89% of reconfiguration overhead reduction with respect to other state-of-the-art techniques for reconfigurable platforms. © 2014 ACM.",Mapping; Reconfigurable systems; Reconfiguration overheads; Runtime reconfiguration; Task scheduling,Algorithms; Computer hardware; Data flow analysis; Dynamic models; Hardware; Mapping; Petri nets; Reconfigurable hardware; Control and data flow graphs; Embedded system applications; Reconfigurable plat-forms; Reconfigurable systems; Reconfiguration overhead; Run time reconfiguration; State-of-the-art techniques; Task-scheduling; Data flow graphs
VTR 7.0: Next generation architecture and CAD system for FPGAs,2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903848192&doi=10.1145%2f2617593&partnerID=40&md5=abea412be40e65f0cdc0ab81672cb4c2,"Exploring architectures for large, modern FPGAs requires sophisticated software that can model and target hypothetical devices. Furthermore, research into new CAD algorithms often requires a complete and open source baseline CAD flow. This article describes recent advances in the open source Verilog-to-Routing (VTR) CAD flow that enable further research in these areas. VTR now supports designs with multiple clocks in both timing analysis and optimization. Hard adder/carry logic can be included in an architecture in various ways and significantly improves the performance of arithmetic circuits. The flow now models energy consumption, an increasingly important concern. The speed and quality of the packing algorithms have been significantly improved. VTR can now generate a netlist of the final post-routed circuit which enables detailed simulation of a design for a variety of purposes. We also release new FPGA architecture files and models that are much closer to modern commercial architectures, enabling more realistic experiments. Finally, we show that while this version of VTR supports new and complex features, it has a 1.5× compile time speed-up for simple architectures and a 6× speed-up for complex architectures compared to the previous release, with no degradation to timing or wire-length quality. © 2014 ACM.",Architecture modeling; CAD; FPGA,Computer aided design; Energy utilization; Field programmable gate arrays (FPGA); Open source software; Architecture modeling; Arithmetic circuit; Compile time; Complex architectures; FPGA architectures; Open sources; Packing algorithms; Timing Analysis; Computer aided logic design
Self-awareness as a model for designing and operating heterogeneous multicores,2014,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903839882&doi=10.1145%2f2617596&partnerID=40&md5=45cfeacc90ab3cd2b83366351986791f,"Self-aware computing is a paradigm for structuring and simplifying the design and operation of computing systems that face unprecedented levels of system dynamics and thus require novel forms of adaptivity. The generality of the paradigm makes it applicable to many types of computing systems and, previously, researchers started to introduce concepts of self-awareness to multicore architectures. In our work we build on a recent reference architectural framework as a model for self-aware computing and instantiate it for an FPGA-based heterogeneous multicore running the ReconOS reconfigurable architecture and operating system. After presenting the model for self-aware computing and ReconOS, we demonstrate with a case study how a multicore application built on the principle of self-awareness, autonomously adapts to changes in the workload and system state. Our work shows that the reference architectural framework as a model for self-aware computing can be practically applied and allows us to structure and simplify the design process, which is essential for designing complex future computing systems. © 2014 ACM.",Adaptive system; Multicore; Reconfigurable computing; Self-aware computing,Adaptive systems; Software architecture; Architectural frameworks; Design and operations; Heterogeneous Multi-Cores; Heterogeneous multicore; Multi core; Multicore architectures; Reconfigurable computing; Self-aware; Reconfigurable architectures
A reconfigurable parallel hardware implementation of the self-tuning regulator,2013,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891785879&doi=10.1145%2f2535934&partnerID=40&md5=718dd43ac747e2d927bb8e564d4c0db3,"The self-tuning regulator (STR) is a popular adaptive control algorithm. A high-performance computer is required for its implementation due to the heavy online computational burden. To extend STR for more realtime applications, a parallel hardware implementation on a low-cost reconfigurable computer is presented. The hardware was incorporated with multistage matrix multiplication (MMM) and trace technique to enhance the processing speed. This design was deeply pipelined to achieve high throughput. The algorithm was prototyped on a Xilinx field-programmable gate array (FPGA) device with a maximum operating frequency of 210.436 MHz. Application-specific integrated circuit (ASIC) implementation of STR was reported. © 2013 ACM.",Adaptive control; ASIC; FPGA; Parallel hardware; Reconfigurable computer; Self-tuning regulator,Adaptive control systems; Algorithms; Application specific integrated circuits; Field programmable gate arrays (FPGA); Hardware; Reconfigurable hardware; Adaptive Control; Adaptive control algorithms; High-performance computers; MAtrix multiplication; Maximum operating frequency; Parallel hardware; Reconfigurable computer; Self tuning regulators; Computer hardware
Exploiting task- and data-level parallelism in streaming applications implemented in FPGAs,2013,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891767972&doi=10.1145%2f2535932&partnerID=40&md5=12c2fbbf5f70c4afea31e7ae87b437cf,"This article describes the design and implementation of a novel compilation flow that implements circuits in FPGAs from a streaming programming language. The streaming language supported is called FPGA Brook and is based on the existing Brook language. It allows system designers to express applications in a way that exposes parallelism, which can be exploited through hardware implementation. FPGA Brook supports replication, allowing parts of an application to be implemented as multiple hardware units operating in parallel. Hardware units are interconnected through FIFO buffers which use the small memory modules available in FPGAs. The FPGA Brook automated design flow uses a source-to-source compiler, developed as a part of this work, and combines it with a commercial behavioral synthesis tool to generate the hardware implementation. A suite of benchmark applications was developed in FPGA Brook and implemented using our design flow. Experimental results indicate that performance of many applications scales well with replication. Our benchmark applications also achieve significantly better results than corresponding implementations using a commercial behavioral synthesis tool. We conclude that using an automated design flow for implementation of streaming applications in FPGAs is a promising methodology. © 2013 ACM.",Behavioral synthesis; Data parallelism; Field-programmable gate arrays; High-level synthesis; Parallel reduction; Replication; Scalability; Streaming; Task parallelism; Throughput,Acoustic streaming; Design; Field programmable gate arrays (FPGA); Scalability; Throughput; Tools; Behavioral synthesis; Data parallelism; High Level Synthesis; Replication; Task parallelism; Hardware
An analytical model for evaluating static power of homogeneous FPGA architectures,2013,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891816639&doi=10.1145%2f2535935&partnerID=40&md5=6fed66e9c6c309544795c8408810e673,"As capacity of the field-programmable gate arrays (FPGAs) continues to increase, power dissipated in the logic and routing resources has become a critical concern for FPGA architects. Recent studies have shown that static power is fast approaching the dynamic power in submicron devices. In this article, we propose an analytical model for relating homogeneous island-style-based FPGA architecture to static power. Current FPGA power models are tightly coupled with CAD tools. Our CAD-independent model captures the static power for a given FPGA architecture based on estimates of routing and logic resource utilizations from a pre-technology mapped netlist. We observe an average correlation ratio (C-Ratio) of 95% and a minimum absolute percentage error (MAPE) rate of 15% with respect to the experimental results generated by the Versatile Placement Routing (VPR) tool over the MCNC benchmarks. Our model offers application engineers and FPGA architects the capability to evaluate the impact of their design choices on static power without having to go through CAD-intensive investigations. © 2013 ACM.",Analytical modeling; Homogeneous; Static power,Analytical models; Architecture; Closed loop control systems; Field programmable gate arrays (FPGA); Models; Tools; Correlation ratio; FPGA architectures; FPGA power model; Homogeneous; Percentage error; Routing resources; Static power; Submicron devices; Computer aided design
Optimizing wait states in the synthesis of memory references with unpredictable latencies,2013,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891809791&doi=10.1145%2f2535936&partnerID=40&md5=84bdad1bbb128c08606d9f80c139a082,"We consider the problem of synthesizing circuits (from C to Verilog) that are optimized to handle unpredictable latencies of memory operations. Unpredictable memory latencies can occur due to the use of on chip caches, DRAM memory modules, buffers/queues, or multiport memories. Typically, high-level synthesis compilers assume fixed and known memory latencies, and thus are able to schedule the code's operations efficiently. The operations in the source code are scheduled into states of a state machine whose states will be synthesized to Verilog. The goal is to minimize scheduling length by maximizing the number of operations (and in particular memory operations) that are executed in parallel at the same state. However, with unpredictable latencies, there can be an exponential number of possible orders in which these parallel memory operations can terminate. Thus, in order to minimize the scheduling, we need a different schedule for any such order. This is not practical, and we show a technique of synthesizing a compact state machine that schedules only a small subset of these possible termination orders. Our results show that this compact state machine can improve the execution time compared to a regular scheduling that waits for the termination of all the active memory references in every state. © 2013 ACM.",,Dynamic random access storage; Memory architecture; Optimization; Scheduling; Exponential numbers; High Level Synthesis; Memory latencies; Memory operations; Memory references; Multi-port memory; On-chip cache; Parallel memory; Cache memory
Untangled: A game environment for discovery of creative mapping strategies,2013,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886011944&doi=10.1145%2f2517325&partnerID=40&md5=e693a76bbc563f9655355c8c11087dbc,"The problem of creating efficient mappings of dataflow graphs onto specific architectures (i.e., solving the place and route problem) is incredibly challenging. The difficulty is especially acute in the area of Coarse-Grained Reconfigurable Architectures (CGRAs) to the extent that solving the mapping problem may remove a significant bottleneck to adoption. We believe that the next generation of mapping algorithms will exhibit pattern recognition, the ability to learn from experience, and identification of creative solutions, all of which are human characteristics. This manuscript describes our game UNTANGLED, developed and fine-tuned over the course of a year to allow us to capture and analyze human mapping strategies. It also describes our results to date. We find that the mapping problem can be crowdsourced very effectively, that players can outperform existing algorithms, and that successful player strategies share many elements in common. Based on our observations and analysis, we make concrete recommendations for future research directions for mapping onto CGRAs. Categories and Subject Descriptors: B.7.2 [Integrated Circuits]: Design Aids-Placement and routing General Terms: Design, Algorithms, Performance. © 2013 ACM.",Custom architectures; Design automation; Domain-specific; Game; Mapping algorithms; Placement; Reconfigurable architectures,Algorithms; Computer aided design; Conformal mapping; Data flow analysis; Pattern recognition; Reconfigurable architectures; Design automations; Domain specific; Game; Mapping algorithms; Placement; Problem solving
Self-reconfigurable constant multiplier for FPGA,2013,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886012034&doi=10.1145%2f2490830&partnerID=40&md5=b6e67c60e5df206e52f64d3da99540e8,"Constant multipliers are widely used in signal processing applications to implement the multiplication of signals by a constant coefficient. However, in some applications, this coefficient remains invariable only during an interval of time, and then, its value changes to adapt to new circumstances. In this article, we present a self-reconfigurable constant multiplier suitable for LUT-based FPGAs able to reload the constant in runtime. The pipelined architecture presented is easily scalable to any multiplicand and constant sizes, for unsigned and signed representations. It can be reprogrammed in 16 clock cycles, equivalent to less than 100 ns in current FPGAs. This value is significantly smaller than FPGA partial configuration times. The presented approach is more efficient in terms of area and speed when compared to generic multipliers, achieving up to 91% area reduction and up to 102% speed improvement for the case-study circuits tested. The power consumption of the proposed multipliers are in the range of those of slice-based multipliers rovided by the vendor. Categories and Subject Descriptors: B.2.4 [Arithmetic and Logic Structures]: High-Speed Arithmetic General Terms: Design, Algorithms, Performance © 2013 ACM.",Constant multiplier; FPGA; Runtime reconfiguration,Field programmable gate arrays (FPGA); Signal processing; Arithmetic and logic structures; Constant coefficients; Constant multipliers; High-speed arithmetic; Partial configuration; Pipelined architecture; Run time reconfiguration; Signal processing applications; Reconfigurable hardware
Integration of net-length factor with timing- and routability-riven clustering algorithms,2013,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886031563&doi=10.1145%2f2517324&partnerID=40&md5=5f829d0bf2d4ceb68ab74ca83be73312,"In FPGA CAD flow, the clustering stage builds the foundation for placement and routing stages and affects performance parameters, such as routability, delay, and channel width significantly. Net sharing and criticality are the two most commonly used factors in clustering cost functions. With this study, we first derive third term net-length factor and then design a generic method for integrating net length into the clustering algorithms. Net-length factor enables characterizing the nets based on the routing stress they might cause during later stages of the CAD flow and is essential for enhancing the routability of the design. We evaluate the effectiveness of integrating net length as a factor into the well-known timing (T-VPack)- depopulation (T-NDPack)- and routability (iRAC and T-RPack)-driven clustering algorithms. Through exhaustive experimental studies we show that net-length factor consistently helps improve the channel-width performance of routability- depopulation- and timing-driven clustering algorithms that do not explicitly target low fan-out nets in their cost functions. Particularly net-length factor leads to average reduction in channel width for T-VPack T-RPack and T-NDPack by 11.6% 10.8% and 14.2% respectively and in a majority of the cases improves the critical-path delay without increasing the array size. Categories and Subject Descriptors: B.7.1 [Integrated Circuits]: Types and Design Styles-Gate arrays General Terms: Design Performance. © 2013 ACM.",Channel width; Clustering; Field programmable gate array; Net length prediction,Computer aided design; Cost functions; Field programmable gate arrays (FPGA); Channel widths; Clustering; Critical Paths; Design performance; Generic method; Net length predictions; Performance parameters; Placement and routing; Clustering algorithms
Analyzing system-level information's correlation to FPGA placement,2013,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886009007&doi=10.1145%2f2501985&partnerID=40&md5=8da98cb6a46a3f8573f9489340e18227,"One popular placement algorithms for Field-Programmable Gate Arrays (FPGAs) is called Simulated Annealing (SA). This algorithm tries to create a good quality placement from a flattened design that no longer contains any high-level information related to the original design hierarchy. Placement is an NP-hard problem, and as the size and complexity of designs implemented on FPGAs increases, SA does not scale well to find good solutions in a timely fashion. In this article, we investigate if system-level information can be reconstructed from a flattened netlist and evaluate how that information is realized in terms of its locality in the final placement. If there is a strong relationship between good quality placements and system-level information, then it may be possible to divide a large design into smaller components and improve the time needed to create a good quality placement. Our preliminary results suggest that the locality property of the information embedded in the system-level HDL structure (i.e. ""module"", ""always"", and ""if"" statements) is greatly affected by designer HDL coding style. Therefore, a reconstructive algorithm, called Affinity Propagation, is also considered as a possible method of generating a meaningful coarse-grain picture of the design. Categories and Subject Descriptors: B.7.2 [Integrated Circuits]: Design Aids-Placement and routing General Terms: Design, Algorithms, Performance © 2013 ACM.",CAD; Clustering; FPGA; High-level information; Placement,Algorithms; Computational complexity; Computer aided design; Field programmable gate arrays (FPGA); Simulated annealing; Affinity propagation; Clustering; Coarse grains; High-level information; Original design; Placement; Placement algorithm; System levels; Design
Quipu: A statistical model for predicting hardware resources,2013,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877879466&doi=10.1145%2f2457443.2457446&partnerID=40&md5=330ccfb37ed26b3692cb817504529cd6,"There has been a steady increase in the utilization of heterogeneous architectures to tackle the growing need for computing performance and low-power systems. The execution of computation-intensive functions on specialized hardware enables to achieve substantial speedups and power savings. However, with a large legacy code base and software engineering experts, it is not at all obvious how to easily utilize these new architectures. As a result, there is a need for comprehensive tool support to bridge the knowledge gap of many engineers as well as to retarget legacy code. In this article, we present the Quipu modeling approach, which consists of a set of tools and a modeling methodology that can generate hardware estimation models, which provide valuable information for developers. This information helps to focus their efforts, to partition their application, and to select the right heterogeneous components. We present Quipu's capability to generate domain-specific models, that are up to several times more accurate within their particular domain (error: 4.6%) as compared to domain-agnostic models (error: 23%). Finally, we show how Quipu can generate models for a new toolchain and platform within a few days. © 2013 ACM.",Estimation; Modeling; Reconfigurable architectures; Software complexity metrics; Statistics; System analysis and design,Estimation; Hardware; Models; Reconfigurable architectures; Software engineering; Statistics; Computing performance; Hardware resources; Heterogeneous architectures; Heterogeneous component; Modeling methodology; Software complexity; Specialized hardware; System analysis and design; Mathematical instruments
An FPGA-based accelerator for frequent itemset mining,2013,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877918621&doi=10.1145%2f2457443.2457445&partnerID=40&md5=2efef3bc822c2be7015346357a5353bd,"In this article we describe a Field Programmable Gate Array (FPGA)-based coprocessor architecture for Frequent Itemset Mining (FIM). FIM is a common data mining task used to find frequently occurring subsets amongst a database of sets. FIM is a nonnumerical, data intensive computation and is used in machine learning and computational biology. FIM is particularly expensive - in terms of execution time and memory - when performed on large and/or sparse databases or when applied using a low appearance frequency threshold. Because of this, the development of increasingly efficient FIM algorithms and their mapping to parallel architectures is an active field. Previous attempts to accelerate FIM using FPGAs have relied on performance-limiting strategies such as iterative database loading and runtime logic unit reconfiguration. In this article, we present a novel architecture to implement Eclat, a well-known FIM algorithm. Unlike previous efforts, our technique does not impose limits on the maximum set size as a function of available FPGA logic resources and our design scales well to multiple FPGAs. In addition to a novel hardware design, we also present a corresponding compression scheme for intermediate results that are stored in onchip memory. On a four-FPGA board, experimental results show up to 68X speedup compared to a highly optimized software implementation. © 2013 ACM.",Co-processor; Data intensive; Data mining; Eclat; Frequent itemset mining; High performance computing; Reconfigurable; Reconfigurable applications,Algorithms; Bioinformatics; Database systems; Field programmable gate arrays (FPGA); Iterative methods; Loading; Parallel architectures; Co-processors; Data intensive; Eclat; Frequent itemset mining; High performance computing; Reconfigurable; Data mining
Floating-point exponentiation units for reconfigurable computing,2013,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877908722&doi=10.1145%2f2457443.2457447&partnerID=40&md5=fc43dc39fda582463a82220e725e0cc4,"The high performance and capacity of current FPGAs makes them suitable as acceleration co-processors. This article studies the implementation, for such accelerators, of the floating-point power function xy as defined by the C99 and IEEE 754-2008 standards, generalized here to arbitrary exponent and mantissa sizes. Last-bit accuracy at the smallest possible cost is obtained thanks to a careful study of the various subcomponents: a floating-point logarithm, a modified floating-point exponential, and a truncated floatingpoint multiplier. A parameterized architecture generator in the open-source FloPoCo project is presented in details and evaluated. © 2013 ACM.",Exponentiation unit; Floating-point; Power function; Reconfigurable computing,Reconfigurable architectures; Co-processors; Exponentiations; Floating-point; Open-source; Parameterized; Power functions; Reconfigurable computing; Digital arithmetic
Self-alignment schemes for the implementation of addition-related floating-point operators,2013,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877898997&doi=10.1145%2f2457443.2457444&partnerID=40&md5=54ea45d2854486d1480679229e62ea54,"Advances in semiconductor technology brings to the market incredibly dense devices, capable of handling tens to hundreds floating-point operators on a single chip; so do the latest field programmable gate arrays (FPGAs). In order to alleviate the complexity of resorting to these devices in computationally intensive applications, this article proposes hardware schemes for the realization of addition-related floating-point operators based on the self-alignment technique (SAT). The article demonstrates that the schemes guarantee an accuracy as if summation was computed accurately in the precision of operator's internal mantissa, then faithfully rounded to working precision. To achieve such performance, the article adopts the redundant high radix carry-save (HRCS) format for the rapid addition of wide mantissas. Implementation results show that combining the SAT and the HRCS format allows the implementation of complex operators with reduced area and latency, more so when a fused-path approach is adopted. The article also proposes a new hardware operator for performing endomorphic HRCS additions and presents a new technique for speeding up the conversion from the redundant HRCS to a conventional binary format. © 2013 ACM.",Accumulator; Floating-point; FPGA; Redundant arithmetic; Self-alignment technique; Summation,Alignment; Field programmable gate arrays (FPGA); Hardware; Semiconductor device manufacture; Accumulator; Floating-point; Redundant arithmetic; Self-alignment techniques; Summation; Digital arithmetic
ReShape: Towards a high-level approach to design and operation of modular reconfigurable systems,2013,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877893852&doi=10.1145%2f2457443.2457448&partnerID=40&md5=a6a874f2d524edd68e0b79ce66cbc17d,"The latest FPGA devices provide the headroom to implement large-scale and complex systems. A key requirement is the integration of modules from diverse sources to promote modular design and reuse. A contrary factor is that using dynamic partial reconfiguration typically requires low-level planning of the system implementation. In this article, we introduce ReShape: a high-level approach for designing reconfigurable systems by interconnecting modules, which gives a ""plug and play"" look and feel, is supported by tools that carry out implementation functions, and is carried through to support system reconfiguration during operation. The emphasis is on the inter-module connections and abstracting the communication patterns that are typical between modules: for example, the streaming of data, or the reading and writing of data to and from memory modules. The details of wiring and signaling are hidden from view, via metadata associated with individual modules. This setting allows system reconfiguration at the module level, both by supporting type checking of replacement modules and by managing the overall system implementation, via metadata associated with its FPGA floorplan. The methodology and tools have been implemented in a prototype targeted to a domain-specific setting - high-speed networking - and have been validated on real telecommunications design projects. © 2013 ACM.",High-level tools for FPGAs; Modular system design; Modular system reconfiguration; Networking systems,Metadata; Structural design; Dynamic partial reconfiguration; Large-scale and complex systems; Modular system; Modular system design; Networking systems; Reconfigurable systems; System implementation; System reconfiguration; Field programmable gate arrays (FPGA)
Virtualizable hardware/software design infrastructure for dynamically partially reconfigurable systems,2013,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896998413&doi=10.1145%2f2499625.2499628&partnerID=40&md5=8274232db99990fc21ce93bd7223ceae,"In most existing works, reconfigurable hardware modules are still managed as conventional hardware devices. Further, the software reconfiguration overhead incurred by loading corresponding device drivers into the kernel of an operating system has been overlooked until now. As a result, the enhancement of system performance and the utilization of reconfigurable hardware modules are still quite limited. This work proposes a virtualizable hardware/software design infrastructure (VDI) for dynamically partially reconfigurable systems. Besides the gate-level hardware virtualization provided by the partial reconfiguration technology, VDI supports the device-level hardware virtualization. In VDI, a reconfigurable hardware module can be virtualized such that it can be accessed efficiently by multiple applications in an interleaving way. A Hot-Plugin Connector (HPC) replaces the conventional device driver, such that it not only assists the device-level hardware virtualization but can also be reused across differen hardware modules. To facilitate hardware/software communication and to enhance system scalability, the proposed VDI is realized as a hierarchical design framework. User-designed reconfigurable hardware modules can be easily integrated into VDI, and are then executed as hardware tasks in an operating system for reconfigurable systems (OS4RS). A dynamically partially reconfigurable network security system was designed using VDI, which demonstrated a higher utilization of reconfigurable hardware modules and a reduction by up to 12.83% of the processing time required by using the conventional method in a dynamically partially reconfigurable system. © 2013 ACM.",Dynamically partially reconfigurable systems; Hardware virtualization,Computer hardware; Loading; Reconfigurable hardware; Structural design; Virtual reality; Dynamically partially reconfigurable systems; Hardware virtualization; Hardware/software design; Multiple applications; Partial reconfiguration; Reconfigurable network; Reconfigurable systems; Software reconfiguration; Hardware
Towards development of an analytical model relating FPGA architecture parameters to routability,2013,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896902683&doi=10.1145%2f2499625.2499627&partnerID=40&md5=50166bda650d7889e966cf53dc25c160,"We present an analytical model relating FPGA architectural parameters to the routability of the FPGA. The inputs to the model include the channel width and the connection and the switch block flexibilities. The output is an estimate of the proportion of nets in a large circuit that can be expected to be successfully routed on the FPGA.We assume that the circuit is routed to the FPGA using a single-step combined global/ detailed router.We show that the model correctly predicts routability trends.We also present an example application to demonstrate that this model may be a valuable tool for FPGA architects. When combined with the earlier works on analytical modeling, our model can be used to quickly predict the routability without going through any stage of an expensive CAD flow. We envisage that this model will benefit FPGA architecture designers and vendors to quickly evaluate FPGA routing fabrics. © 2013 ACM.",Analytical model; Architecture development; FPGA; FPGA architecture; Routability,Analytical models; Computer aided design; Models; Architectural parameters; Channel widths; FPGA architectures; Fpga routing; Large circuits; Routability; Single-step; Switch blocks; Field programmable gate arrays (FPGA)
Introduction to the special section on 19th reconfigurable architectures workshop (RAW 2012),2013,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896918137&doi=10.1145%2f2499625.2499626&partnerID=40&md5=aa36e9f8d019b33da21b0902ce0e5767,[No abstract available],,
A comprehensive performance analysis of virtual routers on FPGA,2013,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896902418&doi=10.1145%2f2492187&partnerID=40&md5=85cc1728c8088ede8a95e0d8ebbef333,"Network virtualization has gained much popularity with the advent of datacenter networking. The hardware aspect of network virtualization, router virtualization, allows network service providers to consolidate network hardware, reducing equipment cost and management overhead. Several approaches have been proposed to achieve router virtualization to support several virtual networks on a single hardware platform. However, their performance has not been analyzed quantitatively to understand the benefits of each approach. In this work, we perform a comprehensive analysis of performance of these approaches on Field Programmable Gate Array (FPGA) with respect to memory consumption, throughput, and power consumption. Generalized versions of virtualization approaches are evaluated based on post place-and-route results on a state-of-the-art FPGA. Grouping of routing tables is proposed as a novel approach to improve scalability (i.e., the number of virtual networks hosted on a single chip) of virtual routers on FPGA with respect to memory requirement. Further, we employ floor-planning techniques to efficiently utilize chip resources and achieve high performance for virtualized, pipelined architectures, resulting in 1.6- speedup on the average compared with the non-floor-planned approach. The results indicate that the proposed solution is able to support 100+ and 50 virtual routers per chip in the near-best and near-worst case scenarios, while operating at 20+ Gbps rates. © 2013 ACM.","Architectures; FPGA; IP lookup; Networks; Power; Throughput; Virtualization, routers",Architecture; Floors; Hardware; Networks (circuits); Routers; Throughput; Virtual reality; Comprehensive analysis; Comprehensive performance; IP lookup; Network service providers; Network virtualization; Pipelined architecture; Power; Virtualizations; Field programmable gate arrays (FPGA)
Virtual networks distributed communication resource management,2013,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896954155&doi=10.1145%2f2492186&partnerID=40&md5=8c0915dd43f4fbc5177f722d47509a8a,"Networks-on-Chip (NoC) enable scalability for future manycore architectures, facilitating parallel communication between multiple cores. Applications running in parallel on a NoC-based architecture can affect each other due to overlapping communication. Quality-of-Service (QoS) must be supported by the communication infrastructure to execute communication-, real-time- and safety-critical applications on such an architecture. Different strategies have been proposed to provide QoS for point-to-point connections. These strategies allow each node to set up a limited number of connections to other nodes. In this work Virtual Networks (VN) are proposed to enable QoS for regions of a NoC-based architecture. Virtual Networks overcome the limitation of point-to-point connections. A VN behaves like an exclusive physical network. Virtual Networks can be defined and configured during runtime. The size of the VN region and the assigned bandwidth can be adjusted depending on the application requirements. Virtual Networks enable the decoupling of local from global communication. Therefore, the communication of the application mapped into the region is assigned to a Virtual Network established in that specific region. This concept targets packet-switched networks with virtual channels and is realized by an intelligent hardware unit that manages the virtual channel reservation process at system runtime. Virtual Networks can be established and administrated independent of each other, enabling distributed communication resource management. The proposed concept is implemented as a cycle-accurate SystemC simulation model. The simulation results of executing communicating graphs obtained from real application highlight the usefulness of Virtual Networks by showing improved throughput and reduced delay in the respective scenarios. A hardware implementation demonstrates a low impact on area utilization and power consumption. © 2013 ACM.",Hardware; Network-on-Chip; Quality-of-service; Region-based; Subnetworks; Virtual network,Communication; Communication channels (information theory); Computer hardware; Computer simulation; Hardware; Natural resources management; Quality of service; Resource allocation; Routers; VLSI circuits; Communication infrastructure; Distributed communications; Network on chip; Point-to-point connections; Region-based; Safety critical applications; Subnetworks; Virtual networks; Network architecture
JITPR: A framework for supporting fast application's implementation onto FPGAs,2013,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897005061&doi=10.1145%2f2492185&partnerID=40&md5=5e839367b6bc0f5a5229423812656e52,"The execution runtime usually is a headache for designers performing application mapping onto reconfigurable architectures. In this article we propose a methodology, as well as the supporting toolset, targeting to provide fast application implementation onto reconfigurable architectures with the usage of a Just-In-Time (JIT) compilation framework. Experimental results prove the efficiency of the introduced framework, as we reduce the execution runtime compared to the state-of-the-art approach on average by 53.5×. Additionally, the derived solutions achieve higher operation frequencies by 1.17×, while they also exhibit significant lower fragmentation ratios of hardware resources. © 2013 ACM.",FPGA; Just-in-time compilation; Placement,Computer networks; Computer science; Field programmable gate arrays (FPGA); Application mapping; Hardware resources; Just-in-time compilation; Operation frequency; Placement; Runtimes; State-of-the-art approach; Toolsets; Reconfigurable architectures
Reconfigurable fault tolerance: A comprehensive framework for reliable and adaptive FPGA-based space computing,2012,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878560381&doi=10.1145%2f2392616.2392619&partnerID=40&md5=25cd8c2b4c6511ebd325287447e8c8f1,"Commercial SRAM-based, field-programmable gate arrays (FPGAs) have the potential to provide space applications with the necessary performance to meet next-generation mission requirements. However, mitigating an FPGA's susceptibility to single-event upset (SEU) radiation is challenging. Triple-modular redundancy (TMR) techniques are traditionally used to mitigate radiation effects, but TMR incurs substantial overheads such as increased area and power requirements. In order to reduce these overheads while still providing sufficient radiation mitigation, we propose a reconfigurable fault tolerance (RFT) framework that enables system designers to dynamically adjust a system's level of redundancy and fault mitigation based on the varying radiation incurred at different orbital positions. This framework includes an adaptive hardware architecture that leverages FPGA reconfigurable techniques to enable significant processing to be performed efficiently and reliably when environmental factors permit. To accurately estimate upset rates, we propose an upset rate modeling tool that captures time-varying radiation effects for arbitrary satellite orbits using a collection of existing, publically available tools and models. We perform fault-injection testing on a prototype RFT platform to validate the RFT architecture and RFT performability models. We combine our RFT hardware architecture and the modeled upset rates using phased-mission Markov modeling to estimate performability gains achievable using our framework for two case-study orbits. © 2012 ACM.",FPGA; Reconfigurable fault tolerance; Single-event upsets,Architecture; Computer control systems; Computer hardware; Digital storage; Fault tolerance; Field programmable gate arrays (FPGA); Flash memory; Hardware; Reconfigurable architectures; Space applications; Environmental factors; Fault mitigations; Hardware architecture; Mission requirements; Performability models; Power requirement; Reconfigurable; Single event upsets; Reconfigurable hardware
Power-modes: Power-emulator- and model-based dependability and security evaluations,2012,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878619033&doi=10.1145%2f2392616.2392617&partnerID=40&md5=b958358380f6888d9809ba6e3984e07c,"Innovation cycles have been shortening significantly during the last years. This process puts tremendous pressure on designers of embedded systems for security-or reliability-critical applications. Eventual design problems not detected during design time can lead to lost money, confidentiality, or even loss of life in extreme cases. Therefore it is of vital importance to evaluate a new system for its robustness against intentionally and random induced operational faults. Currently this is generally done using extensive simulation runs using gate-level models or direct measurements on the finished silicon product. These approaches either need a significant amount of time and computational power for these simulations or rely on existing product samples. This article presents a novel system evaluation platform using power emulation and fault injection techniques to provide an additional tool for developers of embedded systems in security-and reliability-critical fields. Faults are emulated using state-of-the-art fault injection methods and a flexible pattern representation approach. The resulting effects of these faults on the power consumption profile are estimated using state-of-the-art power emulation hardware. A modular system augmentation approach provides emulation flexibility similar to fault simulation implementations. The platform enables the efficient evaluation of new hardware or software implementations of critical security or reliability solutions at an early development phase. © 2012 ACM.",Fault attack resistance; Fault emulation; Hardware reliability; Hardware security; Power estimation,Design; Electric power utilization; Embedded systems; Hardware; Fault attack; Fault emulations; Hardware reliability; Hardware security; Power estimations; Software reliability
Adaptive voltage scaling in a dynamically reconfigurable FPGA-based platform,2012,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878619387&doi=10.1145%2f2392616.2392618&partnerID=40&md5=64a6f0d6a18668da521f9e3ac59b05cc,"Power is an important issue limiting the applicability of Field Programmable Gate Arrays (FPGAs) since it is considered to be up to one order of magnitude higher than in ASICs. Recently, dynamic reconfiguration in FPGAs has emerged as a viable technique able to achieve power and cost reductions by time-multiplexing the required functionality at runtime. In this article, the applicability of Adaptive Voltage Scaling (AVS) to FPGAs is considered together with dynamic reconfiguration of logic and clock management resources to further improve the power profile of these devices. AVS is a popular power-saving technique in ASICs that enables a device to regulate its own voltage and frequency based on workload, fabrication, and operating conditions. The resulting processing platform exploits the available application-dependent timing margins to achieve a power reduction up to 85% operating at 0.58 volts compared with operating at a nominal voltage of 1 volt. The results also show that the energy requirements at 0.58 volts are aproximately five times lower compared with nominal voltage and this can be explained by the approximate cubic relation of static energy with voltage and the fact that the static component dominates power consumption in the considered FPGA devices. © 2012 ACM.",Adaptive voltage scaling; Dynamic reconfiguration; Field programmable gate array,Dynamic models; Reconfigurable hardware; Signal receivers; Adaptive voltage scaling; Dynamic re-configuration; Energy requirements; FPGA-based platforms; Operating condition; Processing platform; Required functionalities; Time multiplexing; Field programmable gate arrays (FPGA)
On the evolution of hardware circuits via Reconfigurable Architectures,2012,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878557493&doi=10.1145%2f2392616.2392620&partnerID=40&md5=938b4c65079a1edee8cda6105a52d7cc,"Traditionally, hardware circuits are realized according to techniques that follow the classical phases of design and testing. A completely new approach in the creation of hardware circuits has been proposed - the Evolvable Hardware (EHW) paradigm, which bases the circuit synthesis on a goal-oriented evolutionary process inspired by biological evolution in Nature. FPGA-based approaches have emerged as the main architectural solution to implement EHW systems. Various EHW systems have been proposed by researchers but most of them, being based on outdated chips, do not take advantage of the interesting features introduced in newer FPGAs. This article describes a project named Hardware Evolution over Reconfigurable Architectures (HERA), which aims at creating a complete and performance-oriented framework for the evolution of digital circuits, leveraging the reconfiguration technology available in FPGAs. The project is described from its birth to its current state, presenting its evolutionary technique tailored for FPGA-based circuits and the most recent enhancements to improve the scalability with respect to problem size. The developed EHW system outperforms the state of the art, proving its effectiveness in evolving both standard benchmarks and more complex real-world applications. © 2012 ACM.",Dynamic partial reconfiguration; Evolutionary circuit design; Evolvable hardware; Reconfigurable hardware,Benchmarking; Biology; Field programmable gate arrays (FPGA); Integrated circuit manufacture; Reconfigurable architectures; Reconfigurable hardware; Architectural solutions; Dynamic partial reconfiguration; Evolutionary circuit design; Evolutionary process; Evolutionary techniques; Evolvable hardware; Performance-oriented; Reconfiguration technology; Hardware
A SDM-TDM-based circuit-switched router for on-chip networks,2012,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870265574&doi=10.1145%2f2362374.2362379&partnerID=40&md5=bcecf3e80e0e9a564987ddad76ee41e5,"This article proposes a circuit-switched router that combines Spatial Division Multiplexing (SDM) and Time Division Multiplexing (TDM) in order to increase path diversity in the router while sharing channels among multiple connections. In this way, the probability of establishing paths through the network is increased, thereby significantly reducing contention in the network. Furthermore, Quality of Service (QoS) is easily guaranteed. The proposed router was synthesized on an Stratix III 3SL340F FPGA device. A 4 × 4 2DMesh SDM-TDM Network-on-Chip (NoC) was built with the proposed router and synthesized on the 3SL340F FPGA device. The 4 × 4 2D Mesh SDM-TDM NoC was used to build on an FPGA device, a Multiprocessor System-on-Chip (MPSoC) platform consisted of 16 Nios II/f processors, 16 20-KB On-chip Memories, and 16 Network Interfaces. Synthesis results of the MPSoC platform show that the proposed router architecture can be used to built large practicable MPSoC platforms with the proposed NoC architecture with a reasonable hardware overhead and appreciable clock frequency. Simulation results show that combining SDM and TDM techniques in a router allows the highest probability of establishing paths through the network. © 2012 ACM 1936-7406/2012/10-ART15 $15.00.",,Computer architecture; Field programmable gate arrays (FPGA); Microprocessor chips; Multiprocessing systems; Network architecture; Quality of service; Space division multiple access; Time division multiplexing; VLSI circuits; Clock frequency; FPGA devices; Hardware overheads; Multiple connections; Multiprocessor system on chips; Network on chip; NIOS II; NoC architectures; On chip memory; On-chip networks; Path diversity; Router architecture; Spatial Division Multiplexing; Routers
Enabling adaptive techniques in heterogeneous MPSoCs based on virtualization,2012,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870271208&doi=10.1145%2f2362374.2362381&partnerID=40&md5=c2f3c661a411ae67230f1d28be54b6e1,"This article explores the use of virtualization to enable mechanisms like task migration and dynamic mapping in heterogeneous MPSoCs, thereby targeting the design of systems capable of adapt their behavior to time-changing workloads. Because tasks may have to be mapped to target processors with different instruction set architectures, we propose the use of Low Level Virtual Machine (LLVM) to postcompile the tasks at runtime depending on their target processor. A novel dynamic mapping heuristic is also proposed, aiming to exploit the advantages of specialized processors while taking into account the overheads imposed by virtualization. Extensive experimental work at different levels of abstraction-FPGA prototype, RTL and system-level simulation-is presented to evaluate the proposed techniques. © 2012 ACM 1936-7406/2012/10-ART17 $15.00.",,Computer architecture; Multiprocessing systems; Adaptive technique; Dynamic mapping; Instruction set architecture; Low level; Runtimes; System levels; Task migration; Virtual machines; Virtualizations; Virtual reality
Secure extension of FPGA general purpose processors for symmetric key cryptography with partial reconfiguration capabilities,2012,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870272021&doi=10.1145%2f2362374.2362380&partnerID=40&md5=88d94c57230b6a5d26a3fb0d01c5958c,"In data security systems, general purpose processors (GPPs) are often extended by a cryptographic accelerator. The article presents three ways of extending GPPs for symmetric key cryptography applications. Proposed extensions guarantee secure key storage and management even if the system is facing protocol, software and cache memory attacks. The system is partitioned into processor, cipher, and key memory zones. The three security zones are separated at protocol, system, architecture and physical levels. The proposed principle was validated on Altera NIOS II, Xilinx MicroBlaze and Microsemi Cortex M1 soft-core processor extensions. We show that stringent separation of the cipher zone is helpful for partial reconfiguration of the security module, if the enciphering algorithm needs to be dynamically changed. However, the key zone including reconfiguration controller must remain static in order to maintain the high level of security required. We demonstrate that the principle is feasible in partially reconfigurable field programmable gate arrays (FPGAs) such as Altera Stratix V or Xilinx Virtex 6 and also to some extent in FPGAs featuring hardwired general purpose processors such as Cortex M3 in Microsemi SmartFusion FPGA. Although the three GPPs feature different data interfaces, we show that the processors with their extensions reach the required high security level while maintaining partial reconfiguration capability. © 2012 ACM 1936-7406/2012/10-ART16 $15.00.",,Cache memory; Cryptography; Digital storage; Network architecture; Network security; Reconfigurable hardware; Separation; Cryptographic accelerators; Data interfaces; General purpose processors; High security levels; Key storage; NIOS II; Partial reconfiguration; Physical level; Security modules; Soft-core processors; Symmetric key cryptography; Field programmable gate arrays (FPGA)
Remote FPGA lab for enhancing learning of digital systems,2012,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870259350&doi=10.1145%2f2362374.2362382&partnerID=40&md5=722370a3a4e6c3676447944dd138cd3c,"Learning in digital systems can be enhanced through applying a learn-by-doing approach on practical hardware systems and by using Web-based technology to visualize and animate hardware behavior. The authors have reported the Web-based Remote FPGA Lab (RFL) which provides a novel, real-time control and visualization interface to a remote, always-on FPGA hardware implementation. The RFL helps students to understand and reason about digital systems operation, using interactive animation of signal behavior in an executing digital logic system, at any level of the design hierarchy. The RFL supports the creation of real-time interactive digital systems teaching demos. The article presents student RFL usage data and survey data which highlight improved student engagement, learning and achievement. The article describes the RFL architecture, communication interface, Web page functionality, user access administration and database management. The article also describes the RFLGen program, developed to automate user design integration into the Xilinx ISE VHDL-based RFL project wrapper for creation of FPGA configuration bitstreams and RFL animations. © 2012 ACM 1936-7406/2012/10-ART18 $15.00.",,Animation; Real time control; Students; Websites; Bitstreams; Communication interface; Database management; Design hierarchy; Design integrations; Digital logic systems; Digital system; FPGA configuration; FPGA-hardware implementation; Hardware system; Interactive animations; Signal behavior; Student engagement; Survey data; Usage data; User access; Web-based technologies; Hardware
Introduction to the special issue on ReCoSoC 2011,2012,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870259071&doi=10.1145%2f2362374.2362375&partnerID=40&md5=0b94c7a661b056f192ccee89df075629,[No abstract available],,
Asymmetric cache coherency: Policy modifications to improve multicore performance,2012,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870275347&doi=10.1145%2f2362374.2362376&partnerID=40&md5=08980e03c1b32ebcc0ece760497b2064,"Asymmetric coherency is a new optimization method for coherency policies to support nonuniform workloads in multicore processors. Asymmetric coherency assists in load balancing a workload and this is applicable to SoC multicores where the applications are not evenly spread among the processors and customization of the coherency is possible. Asymmetric coherency is a policy change, and consequently our designs require little or no additional hardware over an existing system. We explore two different types of asymmetric coherency policies. Our bus-based asymmetric coherency policy, generated a 60% coherency cost reduction (reduction of latencies due to coherency messages) for nonshared data. Our directory-based asymmetric coherency policy, showed up to a 5.8% execution time improvement and up to a 22% improvement in average memory latency for the parallel benchmarks Sha, using a statically allocated asymmetry. Dynamically allocated asymmetry was found to generate further improvements in access latency, increasing the effectiveness of asymmetric coherency by up to 73.8% when compared to the static asymmetric solution. © 2012 ACM 1936-7406/2012/10-ART12 $15.00.",,Access latency; Bus-based; Cache coherency; Execution time; Existing systems; Memory latencies; Multi core; Multi-core processor; Multi-cores; Optimization method; Parallel benchmarks; Policy changes
Enhancing reconfigurable platforms programmability for synchronous data-flow applications,2012,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870280017&doi=10.1145%2f2362374.2362378&partnerID=40&md5=d5172513d63fa87567caee84eaf86178,"Recent FPGAs allow the design of efficient and complex Heterogeneous Systems-on-Chip (HSoC). Namely, these systems are composed of several processors, hardware accelerators as well as communication media between all these components. Performances provided by HSoCs make them really interesting for data-flow applications, especially image processing applications. The use of this kind of architecture provides good performances but the drawback is an increase of the programming complexity. This complexity is due to the heterogeneous deployment of the application on the platform. Some functions are implemented in software to run on a processor, whereas other functions are implemented in hardware to run in a reconfigurable partition of the FPGA. This article aims to define a programming model based on the Synchronous Data-Flow model, in order to abstract the heterogeneity of the implementation and to leverage the communication issue between software and hardware actors. © 2012 ACM 1936-7406/2012/10-ART14 $15.00.",,Abstracting; Communication; Computer hardware; Field programmable gate arrays (FPGA); Hardware; Image processing; Communication media; Dataflow; Dataflow model; Hardware accelerators; Image processing applications; Programmability; Programming complexity; Programming models; Reconfigurable plat-forms; Systems on chips; Reconfigurable hardware
Memory latency hiding by load value speculation for reconfigurable computers,2012,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870269205&doi=10.1145%2f2362374.2362377&partnerID=40&md5=caf3f67c7601ffe0969fe45a63c927a0,"Load value speculation has long been proposed as a method to hide the latency of memory accesses. It has seen very limited use in actual processors, often due to the high overhead of reexecuting misspeculated computations. We present PreCoRe, a framework capable of generating application-specific microarchitectures supporting load value speculation on reconfigurable computers. The article examines the lightweight speculation and replay mechanisms, the architecture of the actual data value prediction units as well as the impact on the nonspeculative parts of the memory system. In experiments, using PreCoRe has achieved speedups of up to 2.48 times over nonspeculative implementations. © 2012 ACM 1936-7406/2012/10-ART13 $15.00.",,Actual data values; Load values; Memory access; Memory latencies; Memory systems; Micro architectures; Reconfigurable computer; Memory architecture
Constraint programming approach to reconfigurable processor extension generation and application compilation,2012,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867544588&doi=10.1145%2f2209285.2209289&partnerID=40&md5=712dae87b8fbe31d751d41788bc04156,"In this article, we present a constraint programming approach for solving hard design problems present when automatically designing specialized processor extensions. Specifically, we discuss our approach for automatic selection and synthesis of processor extensions as well as efficient application compilation for these newly generated extensions. The discussed approach is implemented in our integrated design framework, IFPEC, built using Constraint Programming (CP). In our framework, custom instructions, implemented as processor extensions, are defined as computational patterns and represented as graphs. This, along with the graph representation of an application, provides a way to use our CP framework equipped with subgraph isomorphism and connected component constraints for identification of processor extensions as well as their selection, application scheduling, binding, and routing. All design steps assume architectures composed of runtime reconfigurable cells, implementing selected extensions, tightly connected to a processor. An advantage of our approach is the possibility of combining different heterogeneous constraints to represent and solve all our design problems. Moreover, the flexibility and expressiveness of the CP framework makes it possible to solve simultaneously extension selection, application scheduling, and binding and improve the quality of the generated results. The article is largely illustrated with experimental results. © 2012 ACM.",Constraint programming; Reconfigurable architectures; Resource assignment; Scheduling; System-level synthesis,Constraint theory; Design; Reconfigurable architectures; Scheduling; Application scheduling; AS graph; Automatic selection; Computational patterns; Connected component; Constraint programming; Custom instruction; Design problems; Design steps; Graph representation; Hard design problem; Integrated design framework; Reconfigurable processors; Resource assignment; Run-time reconfigurable; Subgraph isomorphism; System levels; Computer programming
Statistical timing and power optimization of architecture and device for FPGAs,2012,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867510709&doi=10.1145%2f2209285.2209288&partnerID=40&md5=f9169281434b53d85e328935c833b94f,"Process variation in nanometer technology is becoming an important issue for cutting-edge FPGAs with a multimillion gate capacity. Considering both die-to-die and within-die variations in effective channel length, threshold voltage, and gate oxide thickness, we first develop closed-form models of chip-level FPGA leakage and timing variations. Experiments show that the mean and standard deviation computed by our models are within 3% from those computed by Monte Carlo simulation. We also observe that the leakage and timing variations can be up to 3X and 1.9X, respectively. We then derive analytical yield models considering both leakage and timing variations, and use such models to evaluate the performance of FPGA device and architecture considering process variations. Compared to the baseline, which uses the VPR architecture and device setup based on the ITRS roadmap, device and architecture tuning improves leakage yield by 10.4%, timing yield by 5.7%, and leakage and timing combined yield by 9.4%. We also observe that LUT size of 4 gives the highest leakage yield, LUT size of 7 gives the highest timing yield, but LUT size of 5 achieves the maximum leakage and timing combined yield. To the best of our knowledge, this is the first in-depth study on FPGA architecture and device coevaluation considering process variation. © 2012 ACM.",FPGA architecture; Leakage; Timing; Yield estimation,Leakage (fluid); Architecture tuning; Chip-level; Closed-form models; Effective channel length; FPGA architectures; FPGA devices; Gate oxide thickness; In-depth study; LUT size; Monte Carlo Simulation; Nanometer technology; Power Optimization; Process Variation; Roadmap; Standard deviation; Statistical timing; Timing; Timing variations; Timing yield; Within-die variations; Yield estimation; Yield models; Monte Carlo methods
"SCF: A framework for task-level coordination in reconfigurable, heterogeneous systems",2012,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867502637&doi=10.1145%2f2209285.2209286&partnerID=40&md5=731ed8639ebf7bc0d1a12ee4c4e50bd6,"Heterogeneous computing systems comprised of accelerators such as FPGAs, GPUs, and manycore processors coupled with standard microprocessors are becoming an increasingly popular solution for future computing systems due to their higher performance and energy efficiency. Although programming languages and tools are evolving to simplify device-level design, programming such systems is still difficult and time-consuming largely due to system-wide challenges involving communication between heterogeneous devices, which currently require ad hoc solutions. Most communication frameworks and APIs which have dominated parallel application development for decades were developed for homogeneous systems, and hence cannot be directly employed for hybrid systems. To solve this problem, this article presents the System Coordination Framework (SCF), which employs message passing to transparently enable communication between tasks described using different programming tools (and languages), and running on heterogeneous processing devices of systems from domains ranging from embedded systems to High-Performance Computing (HPC) systems. By hiding low-level architectural details of the underlying communication from an application designer, SCF can improve application development productivity, provide higher levels of application portability, and offer rapid design-space exploration of different task/device mappings. In addition, SCF enables custom communication synthesis that exploits mechanisms specific to different devices and platforms, which can provide performance improvements over generic solutions employed previously. Our results indicate a performance improvement of 28× and 682× by employing FPGA devices for two applications presented in this article, while simultaneously improving the developer productivity by approximately 2.5 to 5 times by using SCF. © 2012 ACM.",Accelerators; Communication; Coordination; FPGA; Heterogeneous computing; Portability; Productivity; Reconfigurable computing,Application programming interfaces (API); Computer software portability; Computer software selection and evaluation; Design; Energy efficiency; Field programmable gate arrays (FPGA); Hybrid systems; Particle accelerators; Productivity; Program processors; Reconfigurable architectures; Space research; Application development; Application portability; Communication framework; Communication synthesis; Computing system; Coordination; Coordination frameworks; Design space exploration; FPGA devices; Generic solutions; Heterogeneous computing; Heterogeneous computing system; Heterogeneous devices; Heterogeneous processing; Heterogeneous systems; High performance computing systems; Homogeneous system; Many-core; Parallel application; Performance improvements; Programming tools; Reconfigurable computing; Communication
Dynamic defragmentation of reconfigurable devices,2012,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867556089&doi=10.1145%2f2209285.2209287&partnerID=40&md5=94121d656350ed80bcb3852478579809,"We propose a new method for defragmenting the module layout of a reconfigurable device, enabled by a novel approach for dealing with communication needs between relocated modules and with inhomogeneities found in commonly used FPGAs. Our method is based on dynamic relocation of module positions during runtime, with only very little reconfiguration overhead; the objective is to maximize the length of contiguous free space that is available for new modules. We describe a number of algorithmic aspects of good defragmentation, and present an optimization method based on tabu search. Experimental results indicate that we can improve the quality of module layout by roughly 50% over the static layout. Among other benefits, this improvement avoids unnecessary rejections of modules. © 2012 ACM.",Complexity; Defragmentation; Dynamic reconfiguration; Hardware/software codesign; Local search; Physical sorting; Reconfigurable devices,Dynamic models; Field programmable gate arrays (FPGA); Tabu search; Complexity; De-fragmentation; Dynamic re-configuration; Hardware/software co-design; Local search; Reconfigurable devices; Reconfigurable hardware
Low-cost sensing with ring oscillator arrays for healthier reconfigurable systems,2012,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867570616&doi=10.1145%2f2133352.2133353&partnerID=40&md5=73776c4c43ddbfa8d0c2a28839de7bfd,"Electronic systems on a chip increasingly suffer from component variation, voltage noise, thermal hotspots, and other subtle physical phenomena. Systems with reconfigurability have unique opportunities for adapting to such effects. Required, however, are low-cost, fine-grained methods for sensing physical parameters. This article presents powerful, novel approaches to online sensing, including methods for designing compact reconfigurable sensors, low-cost threshold detection, and several enhanced measurement procedures. Together, the approaches help enable systems to autonomously uncover a wealth of physical information. A highly efficient counter and improved ring oscillator are introduced, enabling an entire sensor node in just 8 Virtex-5 LUTs. We describe how variations can be measured in delay, temperature, switching-induced IR drop, and leakage-induced IR drop. We demonstrate the proposed approach with an experimental system based on a Virtex-5, instrumented with over 100 sensors at an overhead of only 1.3%. Results from thermally controlled experiments provide some surprising insights and illustrate the utility of the approach. Online sensing can help open the door to physically adaptive computing, including fine-grained power, reliability, and health management schemes for systems on a chip. © 2012 ACM.",FPGAs; Health management; Leakage; Physically-adaptive computing; Power; Process variation; Ring oscillator; Sensing; Temperature; Voltage; Wearout,Costs; Drops; Electric potential; Field programmable gate arrays (FPGA); Leakage (fluid); Sensor nodes; System-on-chip; Temperature; Health management; Physically-adaptive computing; Power; Process Variation; Ring oscillator; Sensing; Wearout; Online systems
Design and analysis of adaptive processor,2012,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867424660&doi=10.1145%2f2133352.2133357&partnerID=40&md5=0b53fb9bc30385aaecc7a9ce4b6f8390,"A new computationmodel called CACHE (Cache Architecture for ConfigurableHardware Engine) is proposed in this paper. This model does not require a dedicated host processor and its software to harness the reconfiguration. Autonomous reconfiguration is performed within a working-set of application datapaths. The CACHE model has lots of side effects; caching, resource allocation and assignment, placement and routing, and defragmentation, with a processing array itself and a special register called a working-set register file. The model aims to reduce three major workloads: (1) the processor and application design workload, (2) runtime resource management and scheduling workload, and (3) reconfiguration workload. In order to reduce these workloads, processor architecture is definitely different from traditional computing model and its microprocessor architecture. There are three major ideas to construct the computing system: (1) an on-chip working-set modelmainly in order to control load and store of streams, namely to control traffics introducing overheads, (2) an on-chip deadlock properties mode mainly in order to manage resources and to continuously configure datapaths corresponding to a working-set window, (3) a cache memory technique to work for these models, the mechanism is equivalent to the working-set window, and the cache memory's procedure is equivalent to resource request, acquirement, and release of deadlock properties. The first model focuses onto streaming applications, for example vector and matrix operations, filters, and so on, which takes coarser grained operations such as integer operations of C-language. Regarding performance compared with DSPs, that comes from constant throughput across different scale of the applications. In addition, extended model, we call Instantmodel that automatically generates instance of a datapath, outperforms the DSPs. This paper shows its computation model, architecture, low-level design, and analyses about basic characteristics of the execution. © 2012 ACM.",Deadlock properties model on chip; Design and analysis; Reconfigurable architecture; Runtime management; Runtime reconfiguration; Stack structure; Stream Processing; Working set model on chip,C (programming language); Integrated circuit design; Memory architecture; Reconfigurable architectures; Resource allocation; Scheduling; Deadlock properties model on chips; Design and analysis; Run time reconfiguration; Runtime management; Stack structure; Stream processing; Working set model on chips; Cache memory
A novel framework for exploring 3-D FPGAs with heterogeneous interconnect fabric,2012,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867537717&doi=10.1145%2f2133352.2133356&partnerID=40&md5=b42c041a481a7b932ed45ae941f55a2e,"A heterogeneous interconnect architecture can be a useful approach for the design of 3-D FPGAs. A methodology to investigate heterogeneous interconnection schemes for 3-D FPGAs under different 3-D fabrication technologies is proposed. Application of the proposed methodology on benchmark circuits demonstrates an improvement in delay, power consumption, and total wire-length of approximately 41%, 32%, and 36%, respectively, as compared to 2-D FPGAs. These improvements are additional to reducing the number of interlayer connections. The fewer interlayer connections are traded off for a higher yield. An area model to evaluate this trade-off is presented. Results indicate that a heterogeneous 3-D FPGA requires 37% less area as compared to a homogeneous 3-D FPGA. Consequently, the heterogeneous FPGAs can exhibit a higher manufacturing yield. A design toolset is also developed to support the design and exploration of various performance metrics for the proposed 3-D FPGAs. © 2012 ACM.",3-D integration; 3-D reconfigurable architectures; Design framework; FPGAs; Interconnection fabric,Benchmarking; Delay circuits; Economic and social effects; Integrated circuit design; Reconfigurable architectures; 3-D integration; 3-D reconfigurable architectures; Design frameworks; Fabrication Technologies; Interconnect architectures; Interconnect fabrics; Interconnection fabrics; Manufacturing yield; Field programmable gate arrays (FPGA)
Portable and scalable FPGA-based acceleration of a direct linear system solver,2012,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867537292&doi=10.1145%2f2133352.2133358&partnerID=40&md5=ad4bdec97061dc6f577dfcb93e7f193e,"FPGAs have the potential to serve as a platform for accelerating many computations including scientific applications. However, the large development cost and short life span for FPGA designs have limited their adoption by the scientific computing community. FPGA-based scientific computing and many kinds of embedded computing could become more practical if there were hardware libraries that were portable to any FPGA-based system with performance that scaled with the size of the FPGA. To illustrate this idea we have implemented one common super-computing library function: the LU factorizationmethod for solving systems of linear equations. This paper describes a method for making the design both portable and scalable that should be illustrative if such libraries are to be built in the future. The design is a software-based generator that leverages both the flexibility of a software programming language and the parameters inherent in an hardware description language. The generator accepts parameters that describe the FPGA capacity and external memory capabilities. We compare the performance of our engine executing on the largest FPGA available at the time of this work (an Altera Stratix III 3S340) to a single processor core fabricated in the same 65nm IC process running a highly optimized software implementation from the processor vendor. For single precision matrices on the order of 10,000 × 10,000 elements, the FPGA implementation is 2.2 times faster and the energy dissipated per useful GFLOP operation is a factor of 5 times less. For double precision, the FPGA implementation is 1.7 times faster and 3.5 times more energy efficient. © 2012 ACM.",Acceleration; FPGA; Linear system solver; LU decomposition; Portable; Scalable,Acceleration; Computer hardware description languages; Electric generators; Energy efficiency; Field programmable gate arrays (FPGA); Libraries; Linear systems; FPGA-based accelerations; Linear system solver; Lu decomposition; Portable; Scalable; Scientific applications; Software implementation; Systems of linear equations; Integrated circuit design
Reconfigurable architecture for VBSME with variable pixel precision,2012,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867502604&doi=10.1145%2f2133352.2133355&partnerID=40&md5=df1edf7b372bc45f68586cc3591e4434,"Current video coding standards, e.g. MPEG-4 H.264/AVC, include Variable Block Size Motion Estimation, in this paper, this process is implemented by a reconfigurable architecture based on Signed Digit arithmetic. Bit serial computation is applied to reconfigure pixel precision. The reconfigurable architectural model is extremely simple to reconfigure. Pixel truncation is used to speed up computation saving up 23.5% of clock cycles for 4-bit precision. This design allows to process all motion vectors of a block in just one iteration. This system has been implemented in FPGA, and HDTVp results are presented. Main characteristics, of this architecture are: very reduced cost, high performance, and reconfigurable pixel precision, these features could be useful in mobile devices. © 2012 ACM.",Bit serial computation; FPGA architectures; MPEG; Pixel truncation; Reconfigurable devices; Signed digit; Variable block size motion estimation; Video coding,Architecture; Field programmable gate arrays (FPGA); Image coding; Motion estimation; Motion Picture Experts Group standards; Pixels; Video signal processing; Bit serial computations; FPGA architectures; MPEG; Pixel truncations; Reconfigurable devices; Signed digits; Variable block-size motion estimation; Reconfigurable architectures
On the exploitation of a high-throughput SHA-256 FPGA design for HMAC,2012,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867547306&doi=10.1145%2f2133352.2133354&partnerID=40&md5=04bc1c66e9bdc1c9a665aee726c3c20a,"High-throughput and area-efficient designs of hash functions and corresponding mechanisms for Message Authentication Codes (MACs) are in high demand due to new security protocols that have arisen and call for security services in every transmitted data packet. For instance, IPv6 incorporates the IPSec protocol for secure data transmission. However, the IPSec's performance bottleneck is the HMAC mechanism which is responsible for authenticating the transmitted data. HMAC's performance bottleneck in its turn is the underlying hash function. In this article a high-throughput and small-size SHA-256 hash function FPGA design and the corresponding HMAC FPGA design is presented. Advanced optimization techniques have been deployed leading to a SHA-256 hashing core which performs more than 30% better, compared to the next better design. This improvement is achieved both in terms of throughput as well as in terms of throughput/area cost factor. It is the first reported SHA-256 hashing core that exceeds 11Gbps (after place and route in Xilinx Virtex 6 board). © 2012 ACM.",FPGA; Hash functions; Message authentication codes; Security,Authentication; Field programmable gate arrays (FPGA); Hash functions; Network security; High throughput; Message authentication codes; Optimization techniques; Performance bottlenecks; Place and route; Security; Security protocols; Security services; Integrated circuit design
Net-length-based routability-driven power-aware clustering,2011,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867548148&doi=10.1145%2f2068716.2068724&partnerID=40&md5=7a961b5cde0650fef430dd7dba5fade0,"The state-of-the-art power-aware clustering tool, P-T-VPack, achieves energy reduction by localizing nets with high switching activity at the expense of channel width and area. In this study, we employ predicted individual postplacement net length information during clustering and prioritize longer nets. This approach targets the capacitance factor for energy reduction, and prioritizes longer nets for channel width and area reduction. We first introduce a new clustering strategy, W-T-VPack, which replaces the switching activity in P-T-VPack with a net length factor. We obtain a 9.87% energy reduction over T-VPack (3.78% increase over P-T-VPack), while at the same time completely eliminating P-T-VPack's channel width and area overhead. We then introduce W-P-T-VPack, which combines switching activity and net length factors. W-P-T-VPack achieves 14.26% energy reduction (0.31% increase over P-T-VPack), while further improving channel width by up to 12.87% for different cluster sizes. We investigate the energy performance of routability (channel width)-driven clustering algorithms, and show that W-T-VPack consistently outperforms T-RPack and iRAC by at least 11.23% and 9.07%, respectively. We conclude that net-length-based clustering is an effective method to concurrently target energy and channel width. © 2011 ACM.",Channel width; Clustering; Field programmable gate array; Net length prediction; Power,Clustering algorithms; Field programmable gate arrays (FPGA); Area overhead; Area reduction; Channel widths; Cluster sizes; Clustering; Clustering strategy; Energy performance; Energy reduction; Length factors; Power; Power-aware; Routability; Switching activities; Reduction
FPGA technology mapping with encoded libraries and staged priority cuts,2011,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867487576&doi=10.1145%2f2068716.2068721&partnerID=40&md5=c702c39514f1cd7301c2e8a5e23ee558,"Technology mapping is an important step in the FPGA CAD flow in which a network of simple gates is converted into a network of logic blocks. This article considers enhancements to a traditional LUT-based mapping algorithm for an FPGA comprised of logic blocks which implement only a subset of functions of up to k variables; specifically, the logic block is a partial LUT, but it possesses more inputs than a typical LUT. An analysis of the logic block is presented, and techniques for postmapping area recovery and timing-driven buffer insertion are also described. Numerical results are put forth which substantiate the efficacy of the proposed methods using real circuits mapped to a commercial FPGA architecture. © 2011 ACM.",Electronic design automation; Mapping; Synthesis,Conformal mapping; Mapping; Synthesis (chemical); Area recovery; Buffer insertion; CAD flow; Electronic design automation; FPGA architectures; FPGA technology; Logic blocks; Mapping algorithms; Numerical results; Real circuits; Technology mapping; Timing-driven; Numerical methods
Choose-your-own-adventure routing: Lightweight load-time defect avoidance,2011,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867563276&doi=10.1145%2f2068716.2068719&partnerID=40&md5=93d5f50f6a727d33dc0d46ce54f8ca9b,"Aggressive scaling increases the number of devices we can integrate per square millimeter but makes it increasingly difficult to guarantee that each device fabricated has the intended operational characteristics. Without careful mitigation, component yield rates will fall, potentially negating the economic benefits of scaling. The fine-grained reconfigurability inherent in FPGAs is a powerful tool that can allow us to drop the stringent requirement that every device be fabricated perfectly in order for a component to be useful. To exploit inherent FPGA reconfigurabilitywhile avoiding full CADmapping,we propose lightweight techniques compatible with the current single bitstream model that can avoid defective devices, reducing yield loss at high defect rates. In particular, by embedding testing operations and alternative path configurations into the bitstream, each FPGA can avoid defects by making only simple, greedy decisions at bitstream load time. With 20% additional tracks above the minimum routable channel width, routes can tolerate 0.01% switch and wire defect rates, raising yield from essentially 0% to near 100%. © 2011 ACM.",Alternatives; Bitstream load; Defect tolerance; In-field repair; Programmable interconnect,Defects; Alternatives; Bit stream; Defect tolerance; In-field; Programmable interconnect; Load testing
Exploiting Data-Level Parallelism for energy-efficient implementation of LDPC decoders and DCT on an FPGA,2011,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867513256&doi=10.1145%2f2068716.2068723&partnerID=40&md5=c149870d422c1baa0e083f8f74b4e793,"We explore the use of Data-Level Parallelism (DLP) as a way of improving the energy efficiency and power consumption involved in running applications on an FPGA. We show that static power consumption is a significant fraction of the overall power consumption in an FPGA and that it does not change significantly even as the area required by an architecture increases, because of the dominance of interconnect in an FPGA. We show that the degree of DLP can be used in conjunction with frequency scaling to reduce the overall power consumption. © 2011 ACM.",DCT; FPGA; LDPC codes; Power,Field programmable gate arrays (FPGA); Data-level parallelism; DCT; Energy efficient; Frequency-scaling; LDPC codes; LDPC decoder; Power; Running applications; Static power consumption; Energy efficiency
"VPR 5.0: FPGA CAD and architecture exploration tools with single-driver routing, heterogeneity and process scaling",2011,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867488441&doi=10.1145%2f2068716.2068718&partnerID=40&md5=7a3bdb230da5d4fd0cdaa3a55ec7812c,"The VPR toolset has been widely used in FPGA architecture and CAD research, but has not evolved over the past decade. This article describes and illustrates the use of a new version of the toolset that includes four new features: first, it supports a broad range of single-driver routing architectures, which have superior architectural and electrical properties over the prior multidriver approach (and which is now employed in the majority of FPGAs sold). Second, it can now model, for placement and routing a heterogeneous selection of hard logic blocks. This is a key (but not final) step toward the incluion of blocks such as memory and multipliers. Third, we provide optimized electrical models for a wide range of architectures in different process technologies, including a range of area-delay trade-offs for each single architecture. Finally, to maintain robustness and support future development the release includes a set of regression tests for the software. To illustrate the use of the new features, we explore several architectural issues: the FPGA area efficiency versus logic block granularity, the effect of single-driver routing, and a simple use of the heterogeneity to explore the impact of hard multipliers on wiring track count. © 2011 ACM.",Field-programmable gate arrays; FPGA; Hard blocks; Heterogeneous; Placement; Routing; VPR,Field programmable gate arrays (FPGA); Software testing; Hard blocks; Heterogeneous; Placement; Routing; VPR; Electric properties
Compressor tree synthesis on commercial high-performance FPGAs,2011,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859033129&doi=10.1145%2f2068716.2068725&partnerID=40&md5=3b22e4c66061eb79d109f1ae9608f292,"Compressor trees are a class of circuits that generalizes multioperand addition and the partial product reduction trees of parallel multipliers using carry-save arithmetic. Compressor trees naturally occur in many DSP applications, such as FIR filters, and, in the more general case, their use can be maximized through the application of high-level transformations to arithmetically intensive data flow graphs. Due to the presence of carry-chains, it has long been thought that trees of 2- or 3-input carry-propagate adders are more efficient than compressor trees for FPGA synthesis; however, this is not the case. This article presents a heuristic for FPGA synthesis of compressor trees that outperforms adder trees and exploits carry-chains when possible. The experimental results show that, on average, the use of compressor trees can reduce critical path delay by 33% and 45% respectively, compared to adder trees synthesized on the Xilinx Virtex-5 and Altera Stratix III FPGAs. © 2011 ACM.",Carry chain; Compressor tree; Field Programamble Gate Array (FPGA); Look-up table (LUT),Compressors; Electric Circuits; Filters; Operations Research; Adders; Compressors; Data flow analysis; Data flow graphs; Field programmable gate arrays (FPGA); FIR filters; Table lookup; Adder tree; Carry-save; Critical path delays; DSP application; FPGA synthesis; Gate arrays; High-level transformations; Look up table; Parallel multipliers; Partial product reduction; Forestry
Introduction to special section FPGA 2009,2011,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867496868&doi=10.1145%2f2068716.2068717&partnerID=40&md5=ac0da2ff85f8ca3e592741831d5e2e3c,[No abstract available],,
Test compression for dynamically reconfigurable processors,2011,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867522257&doi=10.1145%2f2068716.2068726&partnerID=40&md5=6ea6facf212765d4738e4cda590a87df,"We present the world's first test compression technique that features automation of compression rules for test time reduction on dynamically reconfigurable processors. Evaluations on an actual 40-nm product show that our technique achieves a 2.7 times compression ratio for original configuration information (better than does GZIP), the peak decompression bandwidth of 1.6 GB/s, and 2.7 times shorter test times. © 2011 ACM.",Double pattern compression; Dynamically reconfigurable processor,Dynamically reconfigurable processors; Test compression; Test compression techniques; Test time reduction; Compression ratio (machinery)
Performance of Partial Reconfiguration in FPGA systems: A survey and a cost model,2011,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859467647&doi=10.1145%2f2068716.2068722&partnerID=40&md5=226ee7df75b38c33088468f292b5cb8a,"Fine-grain reconfigurable devices suffer from the time needed to load the configuration bitstream. Even for small bitstreams in partially reconfigurable FPGAs this time cannot be neglected. In this article we survey the performance of the factors that contribute to the reconfiguration speed. Then, we study an FPGA-based system architecture and with real experiments we produce a cost model of Partial Reconfiguration (PR). This model is introduced to calculate the expected reconfiguration time and throughput. In order to develop a realistic model we take into account all the physical components that participate in the reconfiguration process. We analyze the parameters that affect the generality of the model and the adjustments needed per system for error-free evaluation. We verify it with real measurements, and then we employ it to evaluate existing systems presented in previous publications. The percentage error of the cost model when comparing its results with the actual values of those publications varies from 36% to 63%, whereas existing works report differences up to two orders of magnitude. Present work enables a user to evaluate PR and decide whether it is suitable for a certain application prior entering the complex PR design flow. © 2011 ACM.",Field programmable gate arrays; Partial reconfiguration; Reconfigurable computing; Reconfiguration time,Reconfigurable architectures; Reconfigurable hardware; Surveys; Bitstreams; Configuration bitstream; Cost models; Design flows; Existing systems; Orders of magnitude; Partial reconfiguration; Percentage error; Physical components; Real measurements; Realistic model; Reconfigurable computing; Reconfigurable devices; Reconfiguration process; Reconfiguration time; System architectures; Field programmable gate arrays (FPGA)
Scalable don't-care-based logic optimization and resynthesis,2011,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867519427&doi=10.1145%2f2068716.2068720&partnerID=40&md5=87191472f175544af244e114be3029f0,"We describe an optimization method for combinational and sequential logic networks, with emphasis on scalability. The proposed resynthesis (a) is capable of substantial logic restructuring, (b) is customizable to solve a variety of optimization tasks, and (c) has reasonable runtime on industrial designs. The approach uses don't-cares computed for a window surrounding a node and can take into account external don't-cares (e.g., unreachable states). It uses a SAT solver for all aspects of Boolean manipulation: computing don't-cares for a node in the window, and deriving a new Boolean function of the node after resubstitution. Experimental results on 6-input LUT networks after a high effort synthesis show substantial reductions in area and delay. When applied to 20 large academic benchmarks, the LUT counts and logic levels are reduced by 45.0% and 12.2%, respectively. The longest runtime for synthesis and mapping is about two minutes. When applied to a set of 14 industrial benchmarks ranging up to 83K 6-LUTs, the LUT counts and logic levels are reduced by 11.8% and 16.5%, respectively. The longest runtime is about 30 minutes. © 2011 ACM.",Boolean satisfiability; Don't-cares; FPGA; Resynthesis,Boolean functions; Field programmable gate arrays (FPGA); Boolean manipulation; Boolean satisfiability; Customizable; Don't-cares; Logic levels; Logic optimization; Logic restructuring; Optimization method; Optimization task; Resubstitution; Resynthesis; Runtimes; SAT solvers; Sequential logic; Substantial reduction; Unreachable States; Optimization
Leveraging reconfigurability in the hardware/software codesign process,2011,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867547477&doi=10.1145%2f2000832.2000840&partnerID=40&md5=59b046b1ac9ab3ca52c21872733adf7f,"Current technology allows designers to implement complete embedded computing systems on a single FPGA. Using an FPGA as the implementation platform introduces greater flexibility into the design process and allows a new approach to embedded system design. Since there is no cost to reprogramming an FPGA, system performance can be measured on-chip in the runtime environment and the system's architecture can be altered based on an evaluation of the data to meet design requirements. In this article, we discuss a new hardware/software codesign methodology tailored to reconfigurable platforms and a design infrastructure created to incorporate on-chip design tools. This methodology utilizes the FPGA's reconfigurability during the design process to profile and verify system performance, thereby reducing system design time. Our current design infrastructure includes: a system specification tool, two on-chip profiling tools, and an on-chip system verification tool. © 2011 ACM.",Design methodologies; FPGAs; Hardware/software codesign; System-on-chips,Design; Embedded software; Field programmable gate arrays (FPGA); Hardware; Microprocessor chips; Specifications; Systems analysis; Current technology; Design Methodology; Design process; Design requirements; Design time; Embedded computing system; Hardware/software co-design; Hardware/software codesign methodologies; Implementation platforms; On chips; On-chip designs; On-chip profiling; On-chip systems; Reconfigurability; Reconfigurable plat-forms; Reducing systems; Runtime environments; System on chips; System specification; Reconfigurable hardware
SHMEM+: A multilevel-PGAS programming model for reconfigurable supercomputing,2011,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867556339&doi=10.1145%2f2000832.2000838&partnerID=40&md5=da1c35d1933feec43c50a0324c5aa135,"Reconfigurable Computing (RC) systems based on FPGAs are becoming an increasingly attractive solution to building parallel systems of the future. Applications targeting such systems have demonstrated superior performance and reduced energy consumption versus their traditional counterparts based on microprocessors. However, most of such work has been limited to small system sizes. Unlike traditional HPC systems, lack of integrated, system-wide, parallel-programming models and languages presents a significant design challenge for creating applications targeting scalable, reconfigurable HPC systems. In this article, we extend the traditional Partitioned Global Address Space (PGAS) model to provide a multilevel integration of memory, which simplifies development of parallel applications for such systems and improves developer productivity. The new multilevel-PGAS programming model captures the unique characteristics of reconfigurable HPC systems, such as the existence of multiple levels of memory hierarchy and heterogeneous computation resources. Based on this model, we extend and adapt the SHMEM communication library to become what we call SHMEM+, the first known SHMEM library enabling coordination between FPGAs and CPUs in a reconfigurable, heterogeneous HPC system. Applications designed with SHMEM+ yield improved developer productivity compared to current methods of multidevice RC design and exhibit a high degree of portability. In addition, our design of SHMEM+ library itself is portable and provides peak communication bandwidth comparable to vendor-proprietary versions of SHMEM. Application case studies are presented to illustrate the advantages of SHMEM+. © 2011 ACM.",Parallel programming; Portability; Productivity; Programming language; Programming model; Reconfigurable computing,Communication; Computer programming languages; Computer software portability; Energy utilization; Parallel programming; Productivity; Program processors; Reconfigurable architectures; Telecommunication systems; Attractive solutions; Communication bandwidth; Communication library; Design challenges; Heterogeneous computation; Memory hierarchy; Multidevice; Multilevel integration; Multiple levels; Parallel application; Parallel system; Parallel-programming models; Partitioned Global Address Space; Programming models; Reconfigurable computing; Small systems; Reconfigurable hardware
An analytical model for multilevel performance prediction of multi-FPGA systems,2011,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867202050&doi=10.1145%2f2000832.2000839&partnerID=40&md5=94824c95ae1a7d0ef37aef637f7c0efe,"Power limitations in semiconductors have made explicitly parallel device architectures such as Field-Programmable Gate Arrays (FPGAs) increasingly attractive for use in scalable systems. However,mitigating the significant cost of FPGA development requires efficient design-space exploration to plan and evaluate a range of potential algorithm and platform choices prior to implementation. The authors propose the RC Amenability Test for Scalable Systems (RATSS), an analytical model which enables straightforward, fast, and reasonably accurate performance prediction prior to implementation by extending current modeling concepts to multi-FPGA designs. RATSS provides a comprehensive strategic model to evaluate applications based on the computation and communication requirements of the algorithm and capabilities of the FPGA platform. The RATSS model targets data-parallel applications on current scalable FPGA systems. Three case studies with RATSS demonstrate nearly 90% prediction accuracy as compared to corresponding implementations. © 2011 ACM.",Formulation methodology; FPGA; Performance prediction; Reconfigurable computing; Strategic design space exploration,Algorithms; Analytical models; Forecasting; Models; Reconfigurable architectures; Space research; Accurate performance; Current modeling; Data parallel; Design space exploration; Formulation methodology; Multi-FPGA; Multi-FPGA system; On currents; Parallel devices; Performance prediction; Power limitations; Prediction accuracy; Reconfigurable computing; Scalable systems; Strategic design; Strategic models; Field programmable gate arrays (FPGA)
An FPGA-based accelerator for LambdaRank in web search engines,2011,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867541294&doi=10.1145%2f2000832.2000837&partnerID=40&md5=7814e0a39b29412bc9a73cfdd16b9b3d,"In modernWeb search engines, NeuralNetwork (NN)-based learning to rank algorithms is intensively used to increase the quality of search results. LambdaRank is one such algorithm. However, it is hard to be efficiently accelerated by computer clusters or GPUs, because: (i) the cost function for the ranking problem is much more complex than that of traditional Back-Propagation(BP) NNs, and (ii) no coarse-grained parallelism exists in the algorithm. This article presents an FPGA-based accelerator solution to provide high computing performance with low power consumption. A compact deep pipeline is proposed to handle the complex computing in the batch updating. The area scales linearly with the number of hidden nodes in the algorithm. We also carefully design a data format to enable streaming consumption of the training data from the host computer. The accelerator shows up to 15.3X (with PCIe x4) and 23.9X (with PCIe x8) speedup compared with the pure software implementation on datasets from a commercial search engine. © 2011 ACM.",Accelerator; FPGA; LambdaRank algorithm; Web search,Field programmable gate arrays (FPGA); Information retrieval; Linear accelerators; Particle accelerators; Program processors; Search engines; Websites; Batch updating; Coarse-grained; Complex computing; Computer clusters; Computing performance; Data format; Data sets; Deep pipelines; Hidden nodes; Host computers; Learning to rank; Low-power consumption; Ranking problems; Search results; Software implementation; Training data; Web searches; Backpropagation algorithms
Applying dynamic reconfiguration in the mobile robotics domain: A case study on computer vision algorithms,2011,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863658289&doi=10.1145%2f2000832.2000841&partnerID=40&md5=6ba3e32c132c3993f794bfd5ae36bdcb,"Mobile robots are widely used in industrial environments and are expected to be widely available in human environments in the near future, for example, in the area of care and service robots. This article proposes an implementation for a highly customizable color recognitionmodule based on Field Programmable Gate Array (FPGA) hardware to accomplish tasks like real-time frame processing for image streams. In comparison to a pure software solution on a CPU, an attached FPGA-based hardware accelerator enables real-time image processing and significantly reduces the required computing power of the CPU. Instead, the CPU can be used for tasks that cannot be efficiently implemented on FPGAs, for example, because of a large control overhead. We concentrate on a multirobot scenario where a group of robots follows a human team member by keeping a specific formation in order to support the human in exploration and object detection. Additionally, the robots provide a communication infrastructure to maintain a stable multihop communication network between the human and a base station recording all actions and evaluating the captured images and transmitted data. Depending on the current operating conditions, the robot system has to be able to execute a wide variety of different tasks. Since only a small number of tasks have to be executed concurrently, dynamic reconfiguration of the FPGA can be used to avoid the parallel implementation of all tasks on the FPGA. Within this context, this article discusses application fields where dynamic reconfiguration of FPGA-based coprocessors significantly reduces the CPU load and presents examples of how dynamic reconfiguration can be used in exploration. © 2011 ACM.",Color recognition; Computer vision; Dynamic reconfiguration; FPGA; Mobile robots,Computer hardware description languages; Dynamic models; Field programmable gate arrays (FPGA); Hardware; Industrial applications; Mobile robots; Application fields; Color recognition; Communication infrastructure; Computer vision algorithms; Computing power; Control overhead; Customizable; Dynamic re-configuration; FPGA-based coprocessors; Hardware accelerators; Human environment; Image streams; Industrial environments; Mobile robotic; Multi hop communication; Multirobots; Object Detection; Operating condition; Parallel implementations; Real-time frames; Real-time image processing; Robot system; Service robots; Software solution; Team members; Computer vision
TR-FSM: Transition-based Reconfigurable finite state machine,2011,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867499389&doi=10.1145%2f2000832.2000835&partnerID=40&md5=3c5ab6dc3a7f17a2406e8d08ab951dc6,"Finite State Machines (FSMs) are a key element of integrated circuits. Hard-coded FSMs do not allow changes after the ASIC production. While an embedded FPGA IP core provides flexibility, it is a complex circuit, requires difficult synthesis tools, and is expensive. This article presents and evaluates a novel architecture that is specifically optimized for implementing reconfigurable finite state machines: Transition-based Reconfigurable FSM (TR-FSM). The architecture shows a considerable reduction in area, delay, and power consumption compared to FPGA architectures with a (nearly) FPGA-like reconfigurability. © 2011 ACM.",Finite state machine; FPGA; Implementation; Low power; Reconfigurable logic; Wireless sensor network,Field programmable gate arrays (FPGA); Finite automata; Wireless sensor networks; Complex circuits; Embedded FPGA; FPGA architectures; Implementation; IP core; Key elements; Low Power; Novel architecture; Reconfigurability; Reconfigurable logic; Reduction in area; Synthesis tool; Reconfigurable architectures
Application-specific FPGA using heterogeneous logic blocks,2011,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864829702&doi=10.1145%2f2000832.2000836&partnerID=40&md5=add5f989a2f6a50ae08f78a1c395411b,"This work presents a new automatic mechanism to explore the solution space between Field Programmable Gate Arrays (FPGAs) and Application-Specific Integrated Circuits (ASICs). This new solution is termed as an Application-Specific Inflexible FPGA (ASIF) [Parvez et al. 2009]. An ASIF can be considered as an FPGA with reduced flexibility, or as a reconfigurable ASIC that can implement a set of application circuits which will operate at mutually exclusive times. Execution of different application circuits can be switched by loading their respective bitstream on an ASIF. An ASIF that is reduced from a heterogeneous FPGA is termed as a heterogeneous ASIF. It is shown that a standard-cell-based heterogeneous ASIF for a set of 10 opencore application circuits is 9.6 times smaller than a single-driver mesh-based heterogeneous FPGA. The area gap between ASIC and ASIF is not too significant; however, it can be reduced by designing repeatedly used components of ASIF in full-custom. Unlike an ASIC, an ASIF is a reprogrammable device that can be used to reprogram new or modified circuits at a limited scale. © 2011 ACM.",Application specific; Architecture; ASIF; CAD; FPGA,Architecture; Computer aided design; Field programmable gate arrays (FPGA); Reconfigurable hardware; Application circuits; Application specific; ASIF; Automatic mechanisms; Bit stream; Logic blocks; Reprogrammable; Solution space; Application specific integrated circuits
Optimizing memory bandwidth use and performance for matrix-vector multiplication in iterative methods,2011,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858198125&doi=10.1145%2f2000832.2000834&partnerID=40&md5=53ce81efcd2f8318458ba8667392df1e,"Computing the solution to a system of linear equations is a fundamental problem in scientific computing, and its acceleration has drawn wide interest in the FPGA community [Morris et al. 2006; Zhang et al. 2008; Zhuo and Prasanna 2006]. One class of algorithms to solve these systems, iterative methods, has drawn particular interest, with recent literature showing large performance improvements over General-Purpose Processors (GPPs) [Lopes and Constantinides 2008]. In several iterative methods, this performance gain is largely a result of parallelization of the matrix-vector multiplication, an operation that occurs in many applications and hence has also been widely studied on FPGAs [Zhuo and Prasanna 2005; El-Kurdi et al. 2006]. However, whilst the performance of matrix-vector multiplication on FPGAs is generally I/O bound [Zhuo and Prasanna 2005], the nature of iterative methods allows the use of on-chip memory buffers to increase the bandwidth, providing the potential for significantly more parallelism [deLorimier and DeHon 2005]. Unfortunately, existing approaches have generally only either been capable of solving large matrices with limited improvement over GPPs [Zhuo and Prasanna 2005; El-Kurdi et al. 2006; deLorimier and DeHon 2005], or achieve high performance for relatively smallmatrices [Lopes and Constantinides 2008; Boland and Constantinides 2008]. This article proposes hardware designs to take advantage of symmetrical and banded matrix structure, as well as methods to optimize the RAM use, in order to both increase the performance and retain this performance for larger-order matrices. © 2011 ACM.",Integer linear programming; Iterative methods,General purpose computers; Integer programming; Matrix algebra; Optimization; Banded matrices; General-purpose processors; Hardware design; Integer Linear Programming; Matrix vector multiplication; Memory bandwidths; On chip memory; Parallelizations; Performance Gain; Performance improvements; System of linear equations; Iterative methods
Platform-aware bottleneck detection for reconfigurable computing applications,2011,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864923010&doi=10.1145%2f2000832.2000842&partnerID=40&md5=f9d569ae16bd674a3fbe771f1034a6ec,"Reconfigurable Computing (RC) has the potential to provide substantial performance benefits and yet simultaneously consume less power than traditional microprocessors or GPUs. While experimental performance analysis of RC applications has previously been shown crucial for achieving this potential, existing methods still require application designers to manually locate bottlenecks and determine appropriate optimizations, typically requiring significant designer expertise and effort. Worse, the diversity of platforms employed by RC applications further complicates the process of detecting bottlenecks and formulating optimizations. To address these shortcomings, we first discuss our platform-template system, which enables a performance analysis tool to perform more accurate bottleneck detection and achieve a higher degree of portability across diverse FPGA systems. We then provide details for our implementation of these concepts and techniques in the Reconfigurable Computing Application Performance (ReCAP) tool. Next, we present a taxonomy of common RC bottlenecks, providing associated detection and optimization strategies for each bottleneck, which we use to populate ReCAP's knowledge base for bottleneck detection. Finally, we demonstrate the utility of our approach via two application case studies across a total of three platforms. © 2011 ACM.",FPGA; Optimization; Performance analysis,Inspection; Knowledge based systems; Optimization; Program processors; Reconfigurable architectures; Bottleneck detection; Experimental performance analysis; Knowledge base; Optimization strategy; Performance analysis; Performance benefits; Re-configurable computing applications; Reconfigurable computing; Field programmable gate arrays (FPGA)
Application-specific signatures for Transactional Memory in soft processors,2011,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867511780&doi=10.1145%2f2000832.2000833&partnerID=40&md5=243176986e92b1441950cbcda5338996,"As reconfigurable computing hardware and in particular FPGA-based systems-on-chip comprise an increasing number of processor and accelerator cores, supporting sharing and synchronization in away that is scalable and easy to program becomes a challenge. Transactional Memory (TM) is a potential solution to this problem, and an FPGA-based system provides the opportunity to support TM in hardware (HTM). Although there are many proposed approaches to HTM support for ASICs, these do not necessarily map well to FPGAs. In particular in this work we demonstrate that while signature-based conflict detection schemes (essentially bit-vectors) should intuitively be a good match to the bit parallelism of FPGAs, previous approaches result in unacceptable multicycle stalls, operating frequencies, or false-conflict rates. Capitalizing on the reconfigurable nature of FPGA-based systems, we propose an application-specific signature mechanism for HTM conflict detection. Our evaluation uses real and projected FPGA-based soft multiprocessor systems that support HTM and implement threaded, shared-memory network packet processing applications. We find that our application-specific approach: (i) maintains a reasonable operating frequency of 125 MHz, (ii) achieves a 9% to 71% increase in packet throughput relative to signatures with bit selection on a 2-thread architecture, and (iii) allows our HTM to achieve 6%, 54%, and 57% increases in packet throughput on an 8-thread architecture versus a baseline lock-based synchronization for three of four packet processing applications studied, due to reduced false synchronization. © 2011 ACM.",Signature; Soft processor; Transactional Memory,Application specific integrated circuits; Computer hardware; Hardware; Packet networks; Program processors; Reconfigurable architectures; Storage allocation (computer); Bit parallelism; Conflict detection; Lock-based synchronization; Multi-cycle; Network packets; Operating frequency; Packet processing; Packet throughput; Potential solutions; Reconfigurable computing; Shared memories; Signature; Soft multiprocessors; Soft processors; Systems on chips; Transactional memory; Reconfigurable hardware
A novel multicontext coarse-grained reconfigurable architecture (CGRA) for accelerating column-oriented databases,2011,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867537274&doi=10.1145%2f1968502.1968504&partnerID=40&md5=70d16b07772aaa6e4799b419d88ac444,"The storage model of column-oriented databases is similar in structure to densely packed matrices/vectors found in many high-performance computing applications. Hence, hardware-accelerated vectorized matrix operations using Reconfigurable Logic (RL) coprocessors may find parallels in hardware acceleration of databases. In this article, we explore this hypothesis by proposing a multicontext, coarse-grained Reconfigurable coprocessor Unit (RU) model that is used to accelerate some of the database operations in hardware for column-oriented databases. We then describe the implementation of hardware algorithms for the equi-join, nonequi-join, and inverse-lookup database operations. Finally, we evaluate these algorithms using a microbenchmark query. Our results indicate that the query execution on the proposed RU model is one to two orders of magnitude faster than the software-only query execution. © 2011 ACM.",CGRAs; Column-oriented databases; Databases; Hardware acceleration; Vectorized database algorithms,Algorithms; Computer hardware; Computer software selection and evaluation; Hardware; Reconfigurable hardware; CGRAs; Co-processors; Coarse grained reconfigurable architecture; Coarse-grained; Database algorithm; Hardware acceleration; Hardware algorithm; Hardware-accelerated; High-performance computing applications; Matrix operations; Micro-benchmark; Orders of magnitude; Query execution; Reconfigurable logic; Storage model; Database systems
Fast optical reconfiguration of a nine-context DORGA using a speed adjustment control,2011,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867529827&doi=10.1145%2f1968502.1968506&partnerID=40&md5=e381b7fd339acf84d57aae85f5da4892,"Demand for fast dynamic reconfiguration has increased since dynamic reconfiguration can accelerate the performance of implementation circuits. Such dynamic reconfiguration requires two important features: fast reconfiguration and numerous reconfiguration contexts. However, fast reconfiguration and numerous reconfiguration contexts share a trade-off relation on current VLSIs. Therefore, Optically Reconfigurable Gate Arrays (ORGAs) have been developed to resolve this dilemma. An ORGA architecture allows many configuration contexts by exploiting the large storage capacity of a holographic memory and fast reconfiguration using wide-bandwidth optical connections between a holographic memory and a programmable gate array VLSI. In addition, Dynamic Optically Reconfigurable Gate Arrays (DORGAs) using a photodiode memory architecture have already been developed to realize a high-gate-density VLSI. Therefore, this article presents the first demonstration of a nanosecond-order configuration of a nine-context DORGA architecture. Moreover, this article presents a proposal of a reconfiguration period adjustment technique to control each reconfiguration period to its best setting. © 2011 ACM.",Field programmable gate arrays; Holographic memory; Optically reconfigurable gate arrays,Bandwidth; Diodes; Dynamic models; Field programmable gate arrays (FPGA); Holography; Memory architecture; Dynamic re-configuration; Fast dynamics; Fast reconfiguration; Holographic memory; On currents; Optical connection; Optical reconfiguration; Optically reconfigurable gate arrays; Programmable gate array; Speed adjustment; Storage capacity; Logic gates
Domain-specific optimization of signal recognition targeting FPGAs,2011,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867560363&doi=10.1145%2f1968502.1968508&partnerID=40&md5=7c5c1beb04a7310d1bee7f652fc7818e,"Domain-specific optimizations on matrix computations exploiting specific arithmetic and matrix representation formats have achieved significant performance/area gains in Field-Programmable Gate Array (FPGA) hardware designs. In this article, we explore the application of data-driven optimizations to reduce both storage and computation requirements to the problem of signal recognition from a known dictionary. By starting with a high-level mathematical representation of a signal recognition problem, we perform optimizations across the layers of the system, exploiting mathematical structure to improve implementation efficiency. Specifically, we use Walsh wavelet packets in conjunction with a BestBasis algorithm to distinguish between spoken digits. The resulting transform matrices are quite sparse, and exhibit a rich algebraic structure that contains significant overlap across rows. As a consequence, dot-product computations of the transform matrix and signal vectors exhibit significant computation reuse, or repeated identical computations. We present an algorithm for identifying this computation reuse and scheduling of the row computations. We exploit this reuse to derive FPGA hardware implementations that reduce the amount of computation for an individual matrix by as much as 6.35× and an average of 2× for a single dot-product unit. The implementation that exploits reuse achieves a 2× computation reduction compared to three concurrently-executing simpler accumulator units with the same aggregate design area and outperforms software implementations on high-end desktop personal computers. © 2011 ACM.",Computation reuse; Configurable architectures; FPGA; Signal recognition,Algorithms; Computer software reusability; Field programmable gate arrays (FPGA); Hardware; Mathematical transformations; Optimization; Personal computers; Signal processing; Algebraic structures; Computation reduction; Configurable architectures; Data-driven optimization; Domain specific; FPGA-hardware implementation; Hardware design; Mathematical representations; Mathematical structure; Matrix computation; Matrix representation; Signal recognition; Signal vectors; Software implementation; Transform matrices; Wavelet Packet; Matrix algebra
Scientific application demands on a Reconfigurable Functional Unit interface,2011,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867488151&doi=10.1145%2f1968502.1968510&partnerID=40&md5=48cafdfb9c8b15e2a4877d3781844310,"Modern scientific applications are large, complex, and highly parallel they are commonly executed on supercomputers with tens of thousands of processors. Yet these applications still commonly require weeks or even months to execute. Thus, single-thread performance remains a concern for highly parallel scientific applications. Adding a reconfigurable accelerator to each CPU could improve system performance; however, scientific applications have design constraints that differ from most application domains commonly accelerated by reconfigurable logic. In this article, we discuss the constraints imposed by scientific applications on the computation model, the accelerator architecture, and the accelerator's communication interface with the CPU. Based on these constraints and application analysis, we have previously proposed adding a Reconfigurable Functional Unit (RFU) to accelerate integer graphs that calculate complex memory addresses. In this work, we now propose a flexible multi-instruction interface technique that allows dataflow graphs implemented on the RFU to access a large number of inputs and outputs with minor CPU datapath modifications. We present an in-depth examination of the performance effects of different communication interfaces that use this technique, and select one that best matches the needs of Sandia's scientific applications. Although RFU execution overall improves performance, we also isolate two key negative performance effects introduced by aggregating CPU instructions into dataflow graphs: delayed issue and graph serialization. Finally, to demonstrate the marketability of an RFU beyond scientific applications, we reanalyze the proposed interfaces using the SPEC-fp benchmark suite. We show that although choosing an interface based on SPEC-fp needs is detrimental to Sandia application performance, choosing an interface based on Sandia demands works well for more general-purpose applications. © 2011 ACM.",Communication interfaces; Reconfigurable functional units; Scientific applications,Communication; Data flow analysis; Supercomputers; Accelerator architectures; Application analysis; Application domains; Application performance; Benchmark suites; Best match; Communication interface; Computation model; Data paths; Data-flow graphs; Design constraints; Memory address; Parallel scientific applications; Performance effect; Reconfigurable functional unit; Reconfigurable logic; Scientific applications; Single-thread performance; Benchmarking
A performance-oriented algorithm with consideration on communication cost for dynamically reconfigurable FPGA partitioning,2011,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867508475&doi=10.1145%2f1968502.1968507&partnerID=40&md5=8a63d3ed3ac865e659147870ad341566,"Dynamically reconfigurable FPGAs (DRFPGAs) have high logic utilization because of time-multiplexed interconnects and logic. In this article, we propose a performance-oriented algorithm for the DRFPGA partitioning problem. This algorithm partitions a given circuit system into stages such that the upper bound of the execution times of subcircuits is minimized. The communication cost is taken into consideration in the process of searching for the optimal solution. A graph is first constructed to represent the precedence constraints and calculate the number of buffers needed in a partitioning. This algorithm includes three phases. The first phase reduces the problem size by clustering the gates into subsystems that have only one output. Such a subsystem has a large number of intraconnections because the fan-outs of all vertices except for the one output are fed to the vertices inside the subsystem. This phase significantly reduces the computational complexity of partitioning. The second phase finds a partition with optimal performance. Finally, the third phase minimizes the communication cost by using an iterative improvement approach. Experimental results based on the Xilinx architecture show that our algorithm yields better partitioning solutions than traditional approaches. © 2011 ACM.",Dynamically reconfigurable FPGA; Partitioning; Performance-oriented; Reconfigurable computing,Algorithms; Communication; Costs; Iterative methods; Reconfigurable architectures; Algorithm partition; Communication cost; Execution time; Iterative improvements; Optimal performance; Optimal solutions; Partitioning; Partitioning problem; Performance-oriented; Precedence constraints; Problem size; Reconfigurable computing; Reconfigurable FPGA; Second phase; Sub-circuits; Third phase; Three phasis; Time multiplexed; Upper Bound; Reconfigurable hardware
The instruction-set extension problem: A survey,2011,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867368085&doi=10.1145%2f1968502.1968509&partnerID=40&md5=144cb229bf4e37d5c880fbb08a21a07b,"The extension of a given instruction-set with specialized instructions has become a common technique used to speed up the execution of applications. By identifying computationally intensive portions of an application to be partitioned in segments of code to execute in software and segments of code to execute in hardware, the execution of an application can be considerably speeded up. Each segment of code implemented in hardware can then be seen as a specialized application-specific instruction extending a given instruction-set. Although a number of approaches exist in literature proposing different methodologies to customize an instructionset, the description of the problem consists only of sporadic comparisons limited to isolated problems. This survey presents a unique detailed description of the problem and provides an exhaustive overview of the research in the past years in instruction-set extension. This article presents a thorough analysis of the issues involved during the customization of an instruction-set by means of a set of specialized applicationspecific instructions. The investigation of the problem covers both instruction generation and instruction selection and different kinds of customizations are analyzed in a great detail. © 2011 ACM.",Customization; HW/SW codesign; Instruction generation; Instruction selection; Instruction-set; Instruction-set extension; Reconfigurable architecture,Hardware; Reconfigurable architectures; Customization; HW/SW Codesign; Instruction generation; Instruction selection; Instruction-set; Instruction-set extensions; Surveys
FPGA acceleration of MultiFactor CDO pricing,2011,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863403456&doi=10.1145%2f1968502.1968511&partnerID=40&md5=017bbe1875e2ab3f002ce161ec46881a,"The last decade has seen a significant growth in the financial industry. The recent widespread use of Internet technology has increased the accessibility of the general population to financial data, thereby increasing the average portfolio size. This increase, compounded by the need for accurate real-time results, has led to a rising demand for faster risk simulations. Often, accurately pricing widespread instruments, such as Collateralized Debt Obligations (CDOs), can take excessively long due to their multifactor assets dependency. We present a hardware implementation for a MultiFactor Gaussian Copula (MFGC) CDO pricing algorithm. Through a detailed benchmark exploration we demonstrate how reconfigurable hardware could be used to exploit fine-grain parallelism. Our results show that our implementation mapped onto a Xilinx Virtex 5 (XC5VSX50T) FPGA is over 71 times faster than corresponding software running on a single core 3.4 GHz Intel Xeon processor. © 2011 ACM.",Financial modeling; FPGA,Field programmable gate arrays (FPGA); Collateralized debt obligations; Financial data; Financial industry; Financial modeling; Fine grain parallelism; Gaussian copula; General population; Hardware implementations; Internet technology; Multi-factor; Pricing algorithm; Xeon processors; Hardware
Logarithmic-time FPGA bitstream analysis: A step towards JIT hardware compilation,2011,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867492512&doi=10.1145%2f1968502.1968503&partnerID=40&md5=3a95a72d257997f0ea4a78a98243e129,"Just-In-Time (JIT) compilation is frequently used in software engineering to accelerate program execution. Parts of the code are translated tomachine code at runtime to speedup their execution by exploiting local and dynamic information of the computation. Modern FPGAs manufactured by Xilinx allow partial and dynamic configuration. Such features make them eligible platforms for JIT hardware compilation. Nevertheless, this has not been achieved until now because the mapping between a bitstream and the programmable points inside these FPGAs is not documented. In this article, we propose a methodology to retrieve the relevant information in logarithmic time per bit by methodically using the tools distributed by Xilinx. We give a practical case study which details the analysis of a Virtex-II Pro FPGA bitstream. The mapping of CLBs, BRAMs, and multipliers has been fully determined. Thanks to this information, we have been able to prototype tools in the fields of reverse mapping FPGA bitstreams, low-level simulation, and custom placeand- route. Finally preliminary results demonstrate that a processor embedded in an FPGA can compile, place, and route arithmetic and logic expressions inside the FPGA within a few milliseconds. © 2011 ACM.",Bitstream; Dynamic configuration; FPGA; Hardware compilation; JIT; Reconfigurable computing,Hardware; Just in time production; Reconfigurable architectures; Routers; Software engineering; Bit stream; Dynamic configuration; Hardware compilation; JIT; Reconfigurable computing; Field programmable gate arrays (FPGA)
A scalable and programmable modular Traffic Manager architecture,2011,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861327928&doi=10.1145%2f1968502.1968505&partnerID=40&md5=4f040c21f792661e5f96e4673c906537,"A key issue in the design of next-generation Internet routers and switches will be provision of Traffic Manager (TM) functionality in the datapaths of their high-speed switching fabrics. A new architecture that allows dynamic deployment of different TM functions is presented. By considering the processing requirements of operations such as policing and congestion, queuing, shaping, and scheduling, a solution has been derived that is scalable with a consistent programmable interface. Programmability is achieved using a function computation unit which determines the action (e.g., drop, queue, remark, forward) based on the packet attribute information and a memory storage part. Results of a Xilinx Virtex-5 FPGA reference design are presented. © 2011 ACM.",Congestion; FPGA; Packet processing; Policing; Traffic management,Field programmable gate arrays (FPGA); Management; Attribute information; Congestion; Data-paths; Dynamic deployment; High-speed switching; Internet routers; Memory storage; Packet processing; Policing; Programmability; Reference designs; Traffic management; Traffic managers; Managers
An optimized hardware architecture of a multivariate gaussian random number generator,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867513793&doi=10.1145%2f1857927.1857929&partnerID=40&md5=de0f21541e2be7ec88929ec669d6d3b7,"Monte Carlo simulation is one of the most widely used techniques for computationally intensive simulations in mathematical analysis and modeling. A multivariate Gaussian random number generator is one of the main building blocks of such a system. Field Programmable Gate Arrays (FPGAs) are gaining increased popularity as an alternative means to the traditional general purpose processors targeting the acceleration of the computationally expensive random number generator block. This article presents a novel approach for mapping a multivariate Gaussian random number generator onto an FPGA by optimizing the computational path in terms of hardware resource usage subject to an acceptable error in the approximation of the distribution of interest. The proposed approach is based on the eigenvalue decomposition algorithm which leads to a design with different precision requirements in the computational paths. An analysis on the impact of the error due to truncation/rounding operation along the computational path is performed and an analytical expression of the error inserted into the system is presented. Based on the error analysis, three algorithms that optimize the resource utilization and at the same time minimize the error in the output of the system are presented and compared. Experimental results reveal that the hardware resource usage on an FPGA as well as the error in the approximation of the distribution of interest are significantly reduced by the use of the optimization techniques introduced in the proposed approach. © 2010 ACM.",FPGA; Hardware architecture optimization; Multivariate Gaussian distribution; Word-length optimization,Algorithms; Eigenvalues and eigenfunctions; Error analysis; Field programmable gate arrays (FPGA); Gaussian distribution; Hardware; Monte Carlo methods; Random number generation; Analytical expressions; Building blockes; Eigenvalue decomposition; Gaussians; General purpose processors; Hardware architecture; Hardware resources; Mathematical analysis; Monte Carlo Simulation; Multivariate Gaussian Distributions; Optimization techniques; Random number generators; Resource utilizations; Word length; Optimization
A variable-grain logic cell and routing architecture for a reconfigurable IP core,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867495479&doi=10.1145%2f1857927.1857932&partnerID=40&md5=12044be081dc6fbc52143c208900a9f5,"In the present study, we investigate the use of reconfigurable logic devices (RLDs) as intellectual properties (IPs) for system on a chip (SoC). Using RLDs, SoCs can achieve both high performance and high flexibility. However, conventional RLDs have problems related to performance, area, and power consumption. In order to resolve these problems, we investigated the features of RLD architecture. RLDs are classified into fine-grained and coarse-grained devices based on their architecture. Generally, the granularity of an RLD is limited to either type, which means that a device can only achieve high performance in applications that are suited to its architecture. Therefore, we propose a variable-grain logic cell (VGLC) architecture that can overcome the trade-off between fine-grained and coarse-grained architectures, which are required for the implementation of random and arithmetic logics, respectively. The VGLC is based on a 4-bit adder including configuration bits, which can perform arithmetic and random logic operations unlike the LUT. In the present paper, a local interconnection architecture for the VGLC is proposed. Several types of local interconnections composed of different crossbars are compared, and the trade-off between hardware resources and flexibility is discussed. Using local interconnection, the routing area is reduced by a maximum of 49%. © 2010 ACM.",Coarse-grain; Fine-grain; Reconfigurable IP; Routing architecture,Logic devices; Programmable logic controllers; 4-bit adders; Arithmetic logic; Coarse-grain; Coarse-grained; Fine-grain; Hardware resources; High flexibility; Interconnection architecture; IP core; ITS architecture; Logic cells; Random logic; Reconfigurable IP; Reconfigurable logic; Routing architecture; Routing area; System on a chip; Reconfigurable architectures
Guest editorial ARC 2009,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867547320&doi=10.1145%2f1857927.1857928&partnerID=40&md5=4609fc50fc2af04525e763a83551a39e,[No abstract available],,
An approach for solving large SAT problems on FPGA,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80455126704&doi=10.1145%2f1857927.1857937&partnerID=40&md5=a26964688c7a83b9fc6316a90d97010c,"WSAT and its variants are one of the best performing stochastic local search algorithms for the satisfiability (SAT) problem. In this article, we propose an approach for solving large 3-SAT problems on FPGA using a WSAT algorithm. In hardware solvers, it is important to solve large problems efficiently. In WSAT algorithms, an assignment of binary values to the variables that satisfy all clauses is searched by repeatedly choosing a variable in an unsatisfied clause using a heuristic, and flipping its value. In our solver, (1) only the clauses that may be unsatisfied by the flipping are evaluated in parallel to minimize the circuit size, and (2) several independent tries are executed at the same time on the pipelined circuit to achieve high performance. Our FPGA solver can solve larger problems than previous works with less hardware resources, and shows higher performance. © 2010 ACM.",FPGA; SAT; WSAT,Field programmable gate arrays (FPGA); Hardware; Heuristic algorithms; Binary values; Circuit size; Hardware resources; Pipelined circuits; SAT; SAT problems; Satisfiability problems; Stochastic local searches; WSAT; Problem solving
Efficient heterogeneous architecture floorplan optimization using analytical methods,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867500341&doi=10.1145%2f1857927.1857930&partnerID=40&md5=6bca7cd8452d062eb6d63509653831f7,"This paper argues the case for the use of analytical models in FPGA architecture exploration. We show that the problem, when simplified, is amenable to formal optimization techniques such as integer linear programming. However, the simplification process may lead to inaccurate models. To test the overall methodology, we feed the resulting architectures to VPR 5.0 and quantify their performance in comparison with traditional design methodologies. Our results show that the resulting architectures are better than those found using parameter sweep techniques. In addition, we show that these architectures can be further improved by combining the accuracy of VPR 5.0 with the efficiency of analytical techniques. This is achieved using a closed loop framework which iteratively refines the analytical model using the place and route outputs from VPR. © 2010 ACM.",Architecture exploration; ILP; Reconfigurable computing; VPR,Analytical models; Integer programming; Models; Optimization; Reconfigurable architectures; Analytical method; Analytical techniques; Architecture exploration; Closed loops; Design Methodology; Floorplans; FPGA architectures; Heterogeneous architectures; ILP; Integer Linear Programming; Optimization techniques; Place and route; Reconfigurable computing; Sweep technique; VPR; Iterative methods
Scheduling and placement of hardware/software real-time relocatable tasks in dynamically partially reconfigurable systems,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867497727&doi=10.1145%2f1857927.1857936&partnerID=40&md5=873b3caf69f5e41a65b3f8c790f1266e,"With the gradually fading distinction between hardware and software, it is now possible to relocate tasks from a microprocessor to reconfigurable logic and vice versa. However, existing hardware-software scheduling can rarely cope with such runtime task relocation. In this work, we propose a new Relocatable Hardware-Software Scheduling (RHSS) method that not only can be applied to dynamically relocatable hardware-software tasks, but also increases the reconfigurable hardware resource utilization, reduces the reconfigurable hardware resource fragmentation with realistic placement methods, and makes best efforts at meeting the real-time constraints of tasks. The feasibility of the proposed relocatable hardware-software scheduling algorithm was proved by applying it to some randomly generated examples and a real dynamically reconfigurable network security system example. Compared to the quadratic time complexity of the state-of-the-art Adaptive Hardware-Software Allocation (AHSA) method, RHSS is linear in time complexity, and improves the reconfigurable hardware utilization by as much as 117.8%. The scheduling and placement time and the memory usage are also drastically reduced by as much as 89.5% and 96.4%, respectively. © 2010 ACM.",Dynamically partially reconfigurable systems; Hardware resource utilization; Placement; Relocatable hardware-software tasks; Scheduling,Computer hardware; Reconfigurable hardware; Scheduling; Best effort; Dynamically reconfigurable network; Hardware and software; Hardware resource utilization; Hardware/software; Memory usage; Placement; Placement methods; Quadratic time; Real time constraints; Reconfigurable logic; Reconfigurable systems; Relocatable hardware-software tasks; Runtimes; Time complexity; Hardware
Design assurance strategy and toolset for Partially Reconfigurable FPGA systems,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867503322&doi=10.1145%2f1857927.1857931&partnerID=40&md5=b6e2c8c3db00f925b00fb30b505c636a,"The growth of the Reconfigurable Computing (RC) systems community exposes diverse requirements with regard to functionality of Electronic Design Automation (EDA) tools. Low-level design tools are increasingly required for RC bitstream debugging and IP core design assurance, particularly in multiparty Partially Reconfigurable (PR) designs. While tools for low-level analysis of design netlists do exist, there is increasing demand for automated and customisable bitstream analysis tools. This article discusses the need for low-level IP core verification within PR-enabled FPGA systems and reports FDAT (FPGA Design Analysis Tool), a versatile, modular and open tools framework for low-level analysis and verification of FPGA designs. FDAT provides a set of high-level Application Programming Interfaces (APIs) abstracting the Xilinx FPGA fabric, the implemented design (e.g., placed and routed netlist) and the related bitstream. A lightweight graphic frontend allows custom visualisation of the design within the FPGA fabric. The operation of FDAT is governed by ""recipe"" scripts which support rapid prototyping of the abstract algorithms for system-level design verification. FDAT recipes, being Python scripts, can be ported to embedded FPGA systems, for example, the previously reported Secure Reconfiguration Controller (SeReCon) which enforces an IP core spatial isolation policy in order to provide run-time protection to the PR system. The paper illustrates the application of FDAT for bit-pattern analysis of Virtex-II Pro and Virtex-5 inter-tile routing and verification of the spatial isolation between designs. © 2010 ACM.",Bitstream analysis; Design assurance; EDA tools; FPGA; Low-level design; Partial reconfiguration; Reconfigurable computing,Abstracting; Application programming interfaces (API); Embedded systems; Field programmable gate arrays (FPGA); Rapid prototyping; Reconfigurable architectures; Reconfigurable hardware; Abstract algorithm; Bit stream; Bitstream analysis; Design tool; EDA tools; Electronic design automation tools; Embedded fpga systems; FPGA design; FPGA fabric; High level applications; IP core; Low-level analysis; Netlist; Partial reconfiguration; Reconfigurable computing; Reconfigurable FPGA; Runtimes; Spatial isolation; System level design; Toolsets; Xilinx FPGA; Design
Optimized System-on-Chip integration of a programmable ECC coprocessor,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859449450&doi=10.1145%2f1857927.1857933&partnerID=40&md5=d41a2c22af59e56a703a792b3f6ef18c,"Most hardware/software (HW/SW) codesigns of Elliptic Curve Cryptography have focused on the computational aspect of the ECC hardware, and not on the system integration into a System-on-Chip (SoC) architecture. We study the impact of the communication link between CPU and coprocessor hardware for a typical ECC design, and demonstrate that the SoC may become performance-limited due to coprocessor data- and instruction-transfers. A dual strategy is proposed to remove the bottleneck: introduction of control hierarchy as well as local storage. The performance of the ECC coprocessor can be almost independent of the selection of bus protocols. Besides performance, the proposed ECC coprocessor is also optimized for scalability. Using design space exploration of a large number of system configurations of different architectures, our proposed ECC coprocessor architecture enables trade-offs between area, speed, and security. © 2010 ACM.",Cryptography; Elliptic curves; FPGA; System-on-chip,Cryptography; Digital storage; Field programmable gate arrays (FPGA); Hardware; Programmable logic controllers; Bus protocol; Co-designs; Co-processor architecture; Co-processors; Computational aspects; Control hierarchy; Design space exploration; Elliptic curve; Elliptic curve cryptography; Hardware/software; System configurations; System integration; System on chips; System-on-chip architecture; System-on-chip integration; Application specific integrated circuits
Evaluation of Random Delay Insertion against DPA on FPGAs,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867561560&doi=10.1145%2f1857927.1857938&partnerID=40&md5=f7cbcdac2b9efa5f74dccae47f8d6930,"Side-channel attacks (SCA) threaten electronic cryptographic devices and can be carried out by monitoring the physical characteristics of security circuits. Differential Power Analysis (DPA) is one the most widely studied side-channel attacks. Numerous countermeasure techniques, such as Random Delay Insertion (RDI), have been proposed to reduce the risk of DPA attacks against cryptographic devices. The RDI technique was first proposed for microprocessors but it was shown to be unsuccessful when implemented on smartcards as it was vulnerable to a variant of the DPA attack known as the Sliding-Window DPA attack. Previous research by the authors investigated the use of the RDI countermeasure for Field Programmable Gate Array (FPGA) based cryptographic devices. A split-RDI technique was proposed to improve the security of the RDI countermeasure. A set of critical parameters was also proposed that could be utilized in the design stage to optimize a security algorithm design with RDI in terms of area, speed and power. The authors also showed that RDI is an efficient countermeasure technique on FPGA in comparison to other countermeasures. In this article, a new RDI logic design is proposed that can be used to cost-efficiently implement RDI on FPGA devices. Sliding-Window DPA and realignment attacks, which were shown to be effective against RDI implemented on smartcard devices, are performed on the improved RDI FPGA implementation. We demonstrate that these attacks are unsuccessful and we also propose a realignment technique that can be used to demonstrate the weakness of RDI implementations. © 2010 ACM.",AES; Countermeasure; Cryptanalysis; Power analysis; Random delay insertion; Realignment; Side-channel attacks; Sliding window DPA,Field programmable gate arrays (FPGA); Logic design; Smart cards; AES; Countermeasure; Cryptanalysis; Power analysis; Random delay; Realignment; Side channel attack; Sliding Window; Cryptography
A new timing driven placement algorithm for dependable circuits on SRAM-based FPGAs,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867499828&doi=10.1145%2f1857927.1857934&partnerID=40&md5=887ba8d6b37ba86ee700cab9b500559c,"Electronic systems for safety critical applications such as space and avionics need the maximum level of dependability for guarantee the success of theirmissions. Simultaneously the computation capabilities required in these fields are constantly increasing for afford the implementation of different kind of applications ranging from signal processing to networking. SRAM-based FPGAs are the candidate devices to achieve this goal thanks to their high versatility of implementing complex circuits with a very short development time. However, in critical environments, the presence of Single Event Upsets (SEUs) affecting the FPGA's functionalities, requires the adoption of specific fault tolerant techniques, like Triple Modular Redundancy (TMR), able to increase the protection capability against radiation effects, but on the other side, introducing a dramatic penalty in terms of performances. In this paper, it is proposed a new timing-driven placement algorithm for implementing soft-errors resilient circuits on SRAM-based FPGAs with a negligible degradation of performance. The algorithmis based on a placement heuristic able to remove the crossing error domains while decreasing the routing congestions and delay inserted by the TMR routing and voting scheme. Experimental analysis performed by timing analysis and SEU static analysis point out a performance improvement of 29% on the average with respect to standard TMR approach and an increased robustness against SEU affecting the FPGA's configuration memory. Accurate analyses of SEUs sensitivity and performance optimization have been performed on a real microprocessor core demonstrating an improvement of performances of more than 62%. © 2010 ACM.",Fault tolerance; FPGA; Placement algorithm,Algorithms; Fault tolerance; Field programmable gate arrays (FPGA); Signal processing; Static random access storage; Timing circuits; Accurate analysis; Complex circuits; Configuration memory; Critical environment; Development time; Electronic systems; Experimental analysis; Fault tolerant technique; Microprocessor core; Performance improvements; Performance optimizations; Placement algorithm; Resilient circuits; Routing congestion; Safety critical applications; Single event upsets; Soft error; Timing Analysis; Timing-driven placement algorithm; Triple modular redundancy; Voting schemes; Computer control systems
Reconfiguration and communication-aware task scheduling for high-performance reconfigurable computing,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863025941&doi=10.1145%2f1862648.1862650&partnerID=40&md5=26ca76224759115d46240ab425c5cdb1,"High-performance reconfigurable computing involves acceleration of significant portions of an application using reconfigurable hardware. When the hardware tasks of an application cannot simultaneously fit in an FPGA, the task graph needs to be partitioned and scheduled into multiple FPGA configurations, in a way that minimizes the total execution time. This article proposes the Reduced Data Movement Scheduling (RDMS) algorithm that aims to improve the overall performance of hardware tasks by taking into account the reconfiguration time, data dependency between tasks, intertask communication as well as task resource utilization. The proposed algorithm uses the dynamic programming method. A mathematical analysis of the algorithm shows that the execution time would at most exceed the optimal solution by a factor of around 1.6, in the worst-case. Simulations on randomly generated task graphs indicate that RDMS algorithm can reduce interconfiguration communication time by 11% and 44% respectively, compared with two other approaches that consider data dependency and hardware resource utilization only. The practicality, as well as efficiency of the proposed algorithm over other approaches, is demonstrated by simulating a task graph from a real-life application - N-body simulation - along with constraints for bandwidth and FPGA parameters from existing high-performance reconfigurable computers. Experiments on SRC-6 are carried out to validate the approach. © 2010 ACM.",Hardware task scheduling; Reconfigurable computing,Communication; Computer hardware; Computer simulation; Hardware; Multitasking; Reconfigurable architectures; Reconfigurable hardware; Scheduling algorithms; Communication time; Data dependencies; Dynamic programming methods; Execution time; FPGA configuration; Hardware resource utilization; Interconfiguration; Intertask communications; Mathematical analysis; N-body simulation; Optimal solutions; Real-life applications; Reconfigurable computer; Reconfigurable computing; Reconfiguration time; Reduced data; Resource utilizations; Task graph; Task-scheduling; Field programmable gate arrays (FPGA)
Characterization of fixed and reconfigurable multi-core devices for application acceleration,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862668124&doi=10.1145%2f1862648.1862649&partnerID=40&md5=82a4b670939fd15a5c0293e746fd56ec,"As on-chip transistor counts increase, the computing landscape has shifted to multi- and manycore devices. Computational accelerators have adopted this trend by incorporating both fixed and reconfigurable many-core and multi-core devices. As more, disparate devices enter the market, there is an increasing need for concepts, terminology, and classification techniques to understand the device tradeoffs. Additionally, computational performance, memory performance, and power metrics are needed to objectively compare devices. These metrics will assist application scientists in selecting the appropriate device early in the development cycle. This article presents a hierarchical taxonomy of computing devices, concepts and terminology describing reconfigurability, and computational density and internal memory bandwidth metrics to compare devices. © 2010 ACM.",Computational density per watt; Internal memory bandwidth,Terminology; Transistors; Classification technique; Computational density; Computational performance; Computing devices; Development cycle; Hierarchical taxonomy; Internal memory; Many-core; Memory performance; Multi core; On-chip transistors; Power metrics; Reconfigurability; Commerce
A simulation framework for rapid analysis of reconfigurable computing systems,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867514346&doi=10.1145%2f1862648.1862655&partnerID=40&md5=0c912e07066cd5380e85edc769d54f35,"Reconfigurable computing (RC) is rapidly emerging as a promising technology for the future of high-performance and embedded computing, enabling systems with the computational density and power of custom-logic hardware and the versatility of software-driven hardware in an optimal mix. Novel methods for rapid virtual prototyping, performance prediction, and evaluation are of critical importance in the engineering of complex reconfigurable systems and applications. These techniques can yield insightful tradeoff analyses while saving valuable time and resources for researchers and engineers alike. The research described herein provides a methodology for mapping arbitrary applications to targeted reconfigurable platforms in a simulation environment called RCSE. By splitting the process into two domains, the application and simulation domains, characterization of each element can occur independently and in parallel, leading to fast and accurate performance prediction results for large and complex systems. This article presents the design of a novel framework for system-level simulative performance prediction of RC systems and applications. The article also presents a set of case studies analyzing two applications, Hyperspectral Imaging (HSI) and Molecular Dynamics (MD), across three disparate RC platforms within the simulation framework. The validation results using each of these applications and systems show that our framework can quickly obtain performance prediction results with reasonable accuracy on a variety of platforms. Finally, a set of simulative case studies are presented to illustrate the various capabilities of the framework to quickly obtain a wide range of performance prediction results and power consumption estimates. © 2010 ACM.",Discrete-event simulation; Performance prediction; Reconfigurable computing,Computer hardware; Hardware; Molecular dynamics; Reconfigurable architectures; Research; Spectroscopy; Structural design; Accurate performance; Computational density; Discrete events; Embedded computing; Hyperspectral Imaging; Novel methods; Optimal mixes; Performance prediction; Rapid analysis; RC system; Reasonable accuracy; Reconfigurable computing; Reconfigurable computing systems; Reconfigurable plat-forms; Reconfigurable systems; Simulation domain; Simulation environment; Simulation framework; System levels; Trade-off analysis; Two domains; Validation results; Virtual prototyping; Forecasting
Placement and floorplanning in dynamically reconfigurable FPGAs,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80455122876&doi=10.1145%2f1862648.1862654&partnerID=40&md5=9763fa7c0c8dd67069cd57b8be4a2495,"The aim of this article is to describe a complete partitioning and floorplanning algorithm tailored for reconfigurable architectures deployable on FPGAs and considering communication infrastructure feasibility. This article proposes a novel approach for resource- and reconfiguration- aware floorplanning. Different from existing approaches, our floorplanning algorithm takes specific physical constraints such as resource distribution and the granularity of reconfiguration possible for a given FPGA device into account. Due to the introduction of constraints typical of other problems like partitioning and placement, the proposed approach is named floorplacer in order to underline the great differences with respect to traditional floorplanners. These physical constraints are typically considered at the later placement stage. Different aspects of the problems have been described, focusing particularly on the FPGAs resource heterogeneity and the temporal dimension typical of reconfigurable systems. Once the problem is introduced a comparison among related works has been provided and their limits have been pointed out. Experimental results proved the validity of the proposed approach. © 2010 ACM.",Computeraided design; Floorplacement; FPGAs; Reconfigurable computing,Algorithms; Reconfigurable architectures; Reconfigurable hardware; Structural design; Communication infrastructure; Complete partitioning; Floor-planning; Floorplacement; FPGA devices; Physical constraints; Reconfigurable computing; Reconfigurable systems; Resource distribution; Resource heterogeneity; Temporal dimensions; Field programmable gate arrays (FPGA)
MPI as a programming model for high-performance reconfigurable computers,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80055089788&doi=10.1145%2f1862648.1862652&partnerID=40&md5=070b641285bd4f22b1ac594f042ca8f4,"High-Performance Reconfigurable Computers (HPRCs) consist of one or more standard microprocessors tightly-coupled with one or more reconfigurable FPGAs. HPRCs have been shown to provide good speedups and good cost/performance ratios, but not necessarily ease of use, leading to a slow acceptance of this technology. HPRCs introduce new design challenges, such as the lack of portability across platforms, incompatibilities with legacy code, users reluctant to change their code base, a prolonged learning curve, and the need for a system-level Hardware/Software co-design development flow. This article presents the evolution and current work on TMD-MPI, which started as an MPI-based programming model for Multiprocessor Systems-on-Chip implemented in FPGAs, and has now evolved to include multiple X86 processors. TMD-MPI is shown to address current design challenges in HPRC usage, suggesting that the MPI standard has enough syntax and semantics to program these new types of parallel architectures. Also presented is the TMD-MPI Ecosystem, which consists of research projects and tools that are developed around TMD-MPI to further improve HPRC usability. Finally, we present preliminary communication performance measurements. © 2010 ACM.",Co-design; FPGA; High-performance; MPI; Parallel programming; Programming model; Reconfigurable,Design; Field programmable gate arrays (FPGA); Parallel architectures; Parallel programming; Semantics; Co-designs; High-performance; MPI; Programming models; Reconfigurable; Reconfigurable hardware
FPGA-array with bandwidth-reduction mechanism for scalable and power-efficient numerical simulations based on finite difference methods,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79551552121&doi=10.1145%2f1862648.1862651&partnerID=40&md5=605e1353b437be7f1847d732116ef628,"For scientific numerical simulation that requires a relatively high ratio of data access to computation, the scalability of memory bandwidth is the key to performance improvement, and therefore custom-computing machines (CCMs) are one of the promising approaches to provide bandwidth-aware structures tailored for individual applications. In this article, we propose a scalable FPGA-array with bandwidth-reduction mechanism (BRM) to implement high-performance and power-efficient CCMs for scientific simulations based on finite difference methods. With the FPGA-array, we construct a systolic computational-memory array (SCMA), which is given a minimum of programmability to provide flexibility and high productivity for various computing kernels and boundary computations. Since the systolic computational-memory architecture of SCMA provides scalability of both memory bandwidth and arithmetic performance according to the array size, we introduce a homogeneously partitioning approach to the SCMA so that it is extensible over a 1D or 2D array of FPGAs connected with a mesh network. To satisfy the bandwidth requirement of inter-FPGA communication, we propose BRM based on time-division multiplexing. BRM decreases the required number of communication channels between the adjacent FPGAs at the cost of delay cycles. We formulate the trade-off between bandwidth and delay of inter-FPGA data-transfer with BRM. To demonstrate feasibility and evaluate performance quantitatively, we design and implement the SCMA of 192 processing elements over two ALTERA Stratix II FPGAs. The implemented SCMA running at 106MHz has the peak performance of 40.7 GFlops in single precision. We demonstrate that the SCMA achieves the sustained performances of 32.8 to 35.7 GFlops for three benchmark computations with high utilization of computing units. The SCMA has complete scalability to the increasing number of FPGAs due to the highly localized computation and communication. In addition, we also demonstrate that the FPGA-based SCMA is powerefficient: it consumes 69% to 87% power and requires only 2.8% to 7.0% energy of those for the same computations performed by a 3.4-GHz Pentium4 processor. With software simulation, we show that BRM works effectively for benchmark computations, and therefore commercially available low-end FPGAs with relatively narrow I/O bandwidth can be utilized to construct a scalable FPGA-array. © 2010 ACM.",Finite difference methods; FPGAs; Reconfigurable computing; Scalable array,Bandwidth; Communication; Computer simulation; Computer software; Field programmable gate arrays (FPGA); Finite difference method; Memory architecture; Reconfigurable architectures; Scalability; 2D arrays; Arithmetic performance; Array sizes; Bandwidth requirement; Bandwidth-aware; Computing units; Data access; Delay cycle; High productivity; I/O bandwidth; Memory bandwidths; Mesh network; Peak performance; Performance improvements; Power efficient; Processing elements; Programmability; Reconfigurable computing; Scalable array; Scientific simulations; Single precision; Software simulation; Sustained performance; Telecommunication systems
Molecular dynamics simulations on high-performance reconfigurable computing systems,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956007831&doi=10.1145%2f1862648.1862653&partnerID=40&md5=5fcb49a282e48f867342218f8f228a8a,"The acceleration of molecular dynamics (MD) simulations using high-performance reconfigurable computing (HPRC) has been much studied. Given the intense competition from multicore and GPUs, there is now a question whether MD on HPRC can be competitive. We concentrate here on the MD kernel computation: determining the short-range force between particle pairs. In one part of the study, we systematically explore the design space of the force pipeline with respect to arithmetic algorithm, arithmetic mode, precision, and various other optimizations. We examine simplifications and find that some have little effect on simulation quality. In the other part, we present the first FPGA study of the filtering of particle pairs with nearly zero mutual force, a standard optimization in MD codes. There are several innovations, including a novel partitioning of the particle space, and new methods for filtering and mapping work onto the pipelines. As a consequence, highly efficient filtering can be implemented with only a small fraction of the FPGA's resources. Overall, we find that, for an Altera Stratix-III EP3ES260, 8 force pipelines running at nearly 200 MHz can fit on the FPGA, and that they can perform at 95% efficiency. This results in an 80-fold per core speed-up for the short-range force, which is likely to make FPGAs highly competitive for MD. © 2010 ACM.",Application acceleration; Bioinformatics; Biological sequence alignment; FPGA-based coprocessors; High performance reconfigurable computing,Bioinformatics; Optimization; Pipelines; Program processors; Reconfigurable architectures; Arithmetic algorithms; Biological sequence alignment; Design spaces; FPGA-based coprocessors; High performance reconfigurable computing; Molecular dynamics simulations; Multi core; Particle pair; Reconfigurable computing; Reconfigurable computing systems; Short-range forces; Simulation quality; Standard optimization; Molecular dynamics
High-performance Quasi-Monte Carlo financial simulation: FPGA vs. GPP vs. GPU,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79551549439&doi=10.1145%2f1862648.1862656&partnerID=40&md5=9a1277b8d703ebe6e8afc68c430e6ee8,"Quasi-Monte Carlo simulation is a special Monte Carlo simulation method that uses quasi-random or low-discrepancy numbers as random sample sets. In many applications, this method has proved advantageous compared to the traditional Monte Carlo simulation method, which uses pseudorandom numbers, thanks to its faster convergence and higher level of accuracy. This article presents the design and implementation of a massively parallelized Quasi-Monte Carlo simulation engine on an FPGA-based supercomputer, called Maxwell. It also compares this implementation with equivalent graphics processing units (GPUs) and general purpose processors (GPP)-based implementations. The detailed comparison between these three implementations (FPGA vs. GPP vs. GPU) is done in the context of financial derivatives pricing based on our Quasi-Monte Carlo simulation engine. Real hardware implementations on the Maxwell machine show that FPGAs outperform equivalent GPP-based software implementations by 2 orders of magnitude, with the speed-up figure scaling linearly with the number of processing nodes used (FPGAs/GPPs). The same implementations show that FPGAs achieve a ∼ 3x speedup compared to equivalent GPUbased implementations. Power consumption measurements also show FPGAs to be 336x more energy efficient than CPUs, and 16x more energy efficient than GPUs. © 2010 ACM.",CPU; FPGA; GPU; Maxwell; Option pricing; Quasi-Monte Carlo simulations,Computer graphics; Economics; Energy efficiency; Field programmable gate arrays (FPGA); Hardware; Maxwell equations; Program processors; Supercomputers; CPU; GPU; Maxwell; Option pricing; Quasi-Monte Carlo simulation; Monte Carlo methods
Runtime reconfiguration of multiprocessors based on compile-time analysis,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867422426&doi=10.1145%2f1839480.1839487&partnerID=40&md5=d3f74aed7fb48eae395b9262af900dd5,"In multiprocessors, performance improvement is typically achieved by exploring parallelism with fixed granularities, such as instruction-level, task-level, or data-level parallelism. We introduce a new reconfiguration mechanism that facilitates variations in these granularities in order to optimize resource utilization in addition to performance improvements. Our reconfigurable multiprocessor QuadroCore combines the advantages of reconfigurability and parallel processing. In this article, a unified hardware-software approach for the design of our QuadroCore is presented. This design flow is enabled via compiler-driven reconfiguration which matches application-specific characteristics to a fixed set of architectural variations. A special reconfiguration mechanism has been developed that alters the architecture within a single clock cycle. The QuadroCore has been implemented on Xilinx XC2V6000 for functional validation and on UMC's 90nm standard cell technology for performance estimation. A diverse set of applications have been mapped onto the reconfigurable multiprocessor to meet orthogonal performance characteristics in terms of time and power. Speedup measurements show a 2-11 times performance increase in comparison to a single processor. Additionally, the reconfiguration scheme has been applied to save power in data-parallel applications. Gate-level simulations have been performed to measure the power-performance trade-offs for two computationally complex applications. The power reports confirm that introducing this scheme of reconfiguration results in power savings in the range of 15-24%. © 2010 ACM.",Design; Performance,Design; Digital storage; Electric batteries; Embedded systems; Compile time; Complex applications; Data parallel; Data-level parallelism; Design flows; Functional validation; Instruction-level; Parallel processing; Performance; Performance characteristics; Performance estimation; Performance improvements; Power savings; Power-performance trade-offs; Reconfigurability; Reconfiguration mechanisms; Reconfiguration schemes; Resource utilizations; Run time reconfiguration; Single processors; Single-clock-cycle; Standard cell; Multiprocessing systems
Bandwidth management in application mapping for dynamically reconfigurable architectures,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867409291&doi=10.1145%2f1839480.1839488&partnerID=40&md5=06520b243c935981fbcdc81be7780ce6,"Partial dynamic reconfiguration (often referred to as partial RTR) enables true on-demand computing. In an on-demand computing environment, a dynamically invoked application is assigned resources such as data bandwidth, configurable logic. The limited logic resources are customized during application execution by exploiting partial RTR. In this article, we propose an approach that maximizes application performance when available bandwidth and logic resources are limited. Our proposed approach is based on theoretical principles of minimizing application schedule length under bandwidth and logic resource constraints. It includes detailed microarchitectural considerations on a commercially popular reconfigurable device, and it exploits partial RTR very effectively by utilizing data-parallelism property of common image-processing applications. We present extensive application case studies on a cycle-accurate simulation platform that includes detailed resource considerations of the Xilinx Virtex XC2V3000. Our experimental results demonstrate that applying our proposed approach to common image-filtering applications leads to 15-20% performance gain in scenarios with limited bandwidth, when compared to prior work that also exploits data-parallelism with RTR but includes simpler bandwidth considerations. Last but not the least, we also demonstrate how our proposed theoretical principles can be directly applied to solve related problems such as minimizing schedule length under logic resource and power constraints. © 2010 ACM.",Algorithms; Design; Performance,Algorithms; Design; Dynamical systems; Embedded systems; Application execution; Application mapping; Application performance; Available bandwidth; Bandwidth management; Configurable logic; Cycle-accurate simulation; Data bandwidth; Data parallelism; Dynamically reconfigurable architecture; Limited bandwidth; Logic resources; On-demand computing; Partial dynamic reconfiguration; Performance; Performance Gain; Power constraints; Reconfigurable devices; Schedule length; Bandwidth
Implementation approaches trade-offs for WiMax OFDM functions on reconfigurable platforms,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867407202&doi=10.1145%2f1839480.1839482&partnerID=40&md5=0b60f3330b5a5deeef56c3c17126ebb6,"This work investigates several approaches for implementing the OFDM functions of the fixed- WiMax standard on reconfigurable platforms. In the first phase, a custom RTL approach, using VHDL, is investigated. The approach shows the capability of a medium-size FPGA to accommodate the OFDM functions of a fixed-WiMax transceiver with only 50% occupation rate. In the second phase, a high-level approach based on the AccelDSP tool is used and compared to the custom RTL approach. The approach presents an easy flow to transfer MATLAB floating-point code into synthesizable cores. The AccelDSP approach shows an area overhead of 10%, while allowing early architectural exploration and accelerating the design time by a factor of two. However, the performance figure obtained is almost 1/4 of that obtained in the custom RTL approach. In the third phase, the Tensilica Xtensa configurable processor is targeted, which presents remarkable figures in terms of power, area, and design time. Comparing the three approaches indicates that the custom RTL approach has the lead in terms of performance. However, both the AccelDSP and the Tensilica Xtensa approaches show fast design time and early architectural exploration capability. In terms of power, the obtained estimation results show that the configurable Xtensa processor approach has the lead, where approximately the total power consumed is about 12-15 times less than those results obtained by the other two approaches. © 2010 ACM.",Algorithms; Design; Performance,Algorithms; Design; Field programmable gate arrays (FPGA); Wimax; AccelDSP; Area overhead; Configurable processors; Design time; Estimation results; High-level approach; Implementation approach; Performance; Reconfigurable plat-forms; Second phase; Tensilica; Third phase; Total power; WiMAX standards; Fixed platforms
"Fast, efficient floating-point adders and multipliers for FPGAs",2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866919831&doi=10.1145%2f1839480.1839481&partnerID=40&md5=9504030314f65a3931d12caf0238cf44,"Floating-point applications are a growing trend in the FPGA community. As such, it has become critical to create floating-point units optimized for standard FPGA technology. Unfortunately, the FPGA design space is very different from the VLSI design space; thus, optimizations for FPGAs can differ significantly from optimizations for VLSI. In particular, the FPGA environment constrains the design space such that only limited parallelism can be effectively exploited to reduce latency. Obtaining the right balances between clock speed, latency, and area in FPGAs can be particularly challenging. This article presents implementation details for an IEEE-754 standard floating-point adder and multiplier for FPGAs. The designs presented here enable a Xilinx Virtex4 FPGA (-11 speed grade) to achieve 270 MHz IEEE compliant double precision floatingpoint performance with a 9-stage adder pipeline and 14-stage multiplier pipeline. The area requirement is approximately 500 slices for the adder and under 750 slices for the multiplier. © 2010 ACM.",Design; Performance,Design; Digital arithmetic; Optimization; Area requirement; Clock speed; Design spaces; Double precision; Floating point units; Floating-Point Adder; Floatingpoint; FPGA design; FPGA technology; IEEE-754 standard; Limited parallelism; Performance; Speed grades; VLSI design; Adders
An automated flow for arithmetic component generation in field-programmable gate arrays,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867431695&doi=10.1145%2f1839480.1839483&partnerID=40&md5=371715688d0969eb36057d9c6f3dd29e,"State-of-the-art configurable logic platforms, such as Field-Programmable Gate Arrays (FPGAs), consist of a heterogeneous mixture of different component types. Compared to traditional homogeneous configurable platforms, heterogeneity provides speed and density advantages. This is due to the replacement of inefficient programmable logic and routing with specialized logic and fixed interconnect in components such as memories, embedded processor units, and fused arithmetic units. Given the increasing complexity of these components, this article introduces a method to automatically propose and explore the benefits of different types of fused arithmetic units. The methods are based on common subgraph extraction techniques, meaning that it is possible to explore different subcircuits that occur frequently across a set of benchmarks. A quantitative analysis is performed of the various fused arithmetic circuits identified by our tool, which are then automatically synthesized to an ASIC process, providing a study of the speed and area benefits of the components. The results of this study provide bounds on the performance of heterogeneous FPGAs: by incorporating coarse-grain components which match the specific needs of a set of benchmarks we show that significant improvements in circuit speed and area can be made. © 2010 ACM.",Algorithms; Design; Experimentation; Performance,Algorithms; Benchmarking; Design; Program processors; Arithmetic circuit; Arithmetic unit; Automated flow; Coarse-grain component; Common subgraph; Component generation; Configurable logic; Configurable platform; Embedded processors; Experimentation; Heterogeneous mixtures; Performance; Programmable logic; Sub-circuits; Field programmable gate arrays (FPGA)
Hardware-accelerated RNA secondary-structure alignment,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865326009&doi=10.1145%2f1839480.1839484&partnerID=40&md5=bd3a80f3b149b23c139ff0019d219dd1,"The search for homologous RNA molecules-sequences of RNA that might behave simiarly due to similarity in their physical (secondary) structure-is currently a computationally intensive task. Moreover, RNA sequences are populating genome databases at a pace unmatched by gains in standard processor performance. While software tools such as INFERNAL can efficiently find homologies among RNA families and genome databases of modest size, the continuous advent of new RNA families and the explosive growth in volume of RNA sequences necessitate a faster approach. This work introduces two different architectures for accelerating the task of finding homologous RNA molecules in a genome database. The first architecture takes advantage of the tree-like configuration of the covariance models used to represent the consensus secondary structure of an RNA family and converts it directly into a highly-pipelined processing engine. Results for this architecture show a 24× speedup over INFERNAL when processing a small RNA model. It is estimated that the architecture could potentially offer several thousands of times speedup over INFERNAL on larger models, provided that there are sufficient hardware resources available. The second architecture is introduced to address the steep resource requirements of the first architecture. It utilizes a uniform array of processing elements and schedules all of the computations required to scan for an RNA homolog onto those processing elements. The estimated speedup for this architecture over the INFERNAL software package ranges from just under 20× to over 2, 350×. © 2010 ACM.",Algorithms; Design; Performance,Algorithms; Database systems; Design; Genes; Hardware; Molecules; Covariance models; Explosive growth; Genome database; Hardware resources; Hardware-accelerated; Performance; Processing elements; Processing engine; Processor performance; Resource requirements; RNA molecules; RNA sequences; Secondary structures; Small RNA; Uniform array; RNA
Reducing memory constraints in modulo scheduling synthesis for FPGAs,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867409534&doi=10.1145%2f1839480.1839485&partnerID=40&md5=0fdc106c951985bf20af8f494c14b51c,"In High-Level Synthesis (HLS), extracting parallelism in order to create small and fast circuits is the main advantage of HLS over software execution. Modulo Scheduling (MS) is a technique in which a loop is parallelized by overlapping different parts of successive iterations. This ability to extract parallelism makes MS an attractive synthesis technique for loop acceleration. In this work we consider two problems involved in the use of MS which are central when targeting FPGAs. Current MS scheduling techniques sacrifice execution times in order to meet resource and delay constraints. Let ""ideal"" execution times be the ones that could have been obtained by MS had we ignored resource and delay constraints. Here we pose the opposite problem, which is more suitable for HLS, namely, how to reduce resource constraints without sacrificing the ideal execution time. We focus on reducing the number of memory ports used by the MS synthesis, which we believe is a crucial resource for HLS. In addition to reducing the number of memory ports we consider the need to develop MS techniques that are fast enough to allow interactive synthesis times and repeated applications of the MS to explore different possibilities of synthesizing the circuits. Current solutions for MS synthesis that can handle memory constraints are too slow to support interactive synthesis. We formalize the problem of reducing the number of parallel memory references in every row of the kernel by a novel combinatorial setting. The proposed technique is based on inserting dummy operations in the kernel and by doing so, performing modulo-shift operations such that the maximal number of parallel memory references in a row is reduced. Experimental results suggest improved execution times for the synthesized circuit. The synthesis takes only a few seconds even for large-size loops. © 2010 ACM.",Algorithms; Design,Algorithms; Design; Delay constraints; Execution time; High-level synthesis; Loop acceleration; Memory constraints; Memory ports; Modulo scheduling; Parallel memory; Repeated application; Resource Constraint; Scheduling techniques; Software execution; Successive iteration; Synthesis techniques; Scheduling
A comparison study on implementing optical flow and digital communications on FPGAs and GPUs,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860871558&doi=10.1145%2f1754386.1754387&partnerID=40&md5=df80725e466d24c1f4651a28f6e33d18,"FPGA devices have often found use as higher-performance alternatives to programmable processors for implementing computations. Applications successfully implemented on FPGAs typically contain high levels of parallelism and often use simple statically scheduled control and modest arithmetic. Recently introduced computing devices such as coarse-grain reconfigurable arrays, multi-core processors, and graphical processing units promise to significantly change the computational landscape and take advantage of many of the same application characteristics that fit well on FPGAs. One real-time computing task, optical flow, is difficult to apply in robotic vision applications because of its high computational and data rate requirements, and so is a good candidate for implementation on FPGAs and other custom computing architectures. This article reports on a series of experiments mapping a collection of different algorithms onto both an FPGA and a GPU. For two different optical flow algorithms the GPU had better performance, while for a set of digital comm MIMOcomputations, they had similar performance. In all cases the FPGA implementations required 10x the development time. Finally, a discussion of the two technology's characteristics is given to show they achieve high performance in different ways. © 2010 ACM.",Design; Experimentation; Measurement; Performance,Algorithms; Design; Digital communication systems; Experiments; Measurements; Program processors; Coarse-grain reconfigurable; Comparison study; Computing devices; Custom computing; Data rates; Development time; Digital communications; Experimentation; FPGA devices; FPGA implementations; Graphical processing unit (GPUs); Multi-core processor; Optical flow algorithm; Performance; Programmable processors; Real time computing; Robotic vision; Optical flows
Titan-R: A multigigabit reconfigurable combined compression/decompression unit,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867433245&doi=10.1145%2f1754386.1754388&partnerID=40&md5=961265fc3befbebcfd867348bab5b89d,"Data compression techniques can alleviate bandwidth problems in even multigigabit networks and are especially useful when combined with encryption. This article demonstrates a reconfigurable hardware compressor/decompressor core, the Titan-R, which can compress/decompress data streams at 8.5 Gb/sec, making it the fastest reconfigurable such device ever proposed; the presented full-duplex implementation allows for fully symmetric compression and decompression rates at 8.5 Gbps each. Its compression algorithm is a variation of the most widely used and efficient such scheme, the Lempel-Ziv (LZ) algorithm that uses part of the previous input stream as the dictionary. In order to support this high network throughput, the Titan-R utilizes a very finegrained pipeline and takes advantage of the high bandwidth provided by the distributed on-chip RAMs of state-of-the-art FPGAs. © 2010 ACM.",Algorithms; Design; Performance,Bandwidth; Data compression; Design; Compression algorithms; Data compression techniques; Data stream; Decompression rate; Full-duplex; High bandwidth; High network throughput; Input streams; Multi-gigabit networks; On chips; Performance; Algorithms
Improving the robustness of ring oscillator TRNGs,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862068533&doi=10.1145%2f1754386.1754390&partnerID=40&md5=633aae44cbe9c677fe6722cbd15da2ed,"A ring oscillator-based true-random number generator design (Rings design) was introduced in Sunar et al. [2007]. The design was rigorously analyzed under a simple mathematical model and its performance characteristics were established. In this article we focus on the practical aspects of the Rings design on a reconfigurable logic platform and determine their implications on the earlier analysis framework. We make recommendations for avoiding pitfalls in real-life implementations by considering ring interaction, transistor-level effects, narrow signal rejection, transmission line attenuation, and sampler bias. Furthermore, we present experimental results showing that changing operating conditions such as the power supply voltage or the operating temperature may affect the output quality when the signal is subsampled. Hence, an attacker may shift the operating point via a simple noninvasive influence and easily bias the TRNG output. Finally, we propose modifications to the design which significantly improve its robustness against attacks, alleviate implementation-related problems, and simultaneously improve its area, throughput, and power performance. © 2010 ACM.",Design; Reliability; Security,Mathematical models; Random number generation; Reliability; Changing operating conditions; Narrow signals; Number generator; Operating points; Operating temperature; Output quality; Performance characteristics; Power performance; Power supply voltage; Reconfigurable logic; Ring interactions; Ring oscillator; Security; Design
SARFUM: Security architecture for remote FPGA update and monitoring,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-83455261502&doi=10.1145%2f1754386.1754389&partnerID=40&md5=0e9cc890762bf55e0760a3f7b990d673,"Remote update of hardware platforms or embedded systems is a convenient service enabled by Field Programmable Gate Array (FPGA)-based systems. This service is often essential in applications like space-based FPGA systems or set-top boxes. However, having the source of the update be remote from the FPGA system opens the door to a set of attacks that may challenge the confidentiality and integrity of the FPGA configuration, the bitstream. Existing schemes propose to encrypt and authenticate the bitstream to thwart these attacks. However, we show that they do not prevent the replay of old bitstream versions, and thus give adversaries an opportunity for downgrading the system. In this article, we propose a new architecture called SARFUM that, in addition to ensuring bitstream confidentiality and integrity, precludes the replay of old bitstreams. SARFUM also includes a protocol for the system designer to remotely monitor the running configuration of the FPGA. Following our presentation and analysis of the security protocols, we propose an example of implementation with the CCM (Counter with CBC-MAC) authenticated encryption standard. We also evaluate the impact of our architecture on the configuration time for different FPGA devices. © 2010 ACM.",,Authenticated encryption; Bit stream; Bitstreams; Cbc-mac; FPGA configuration; FPGA devices; Hardware platform; Security Architecture; Security protocols; Space-based; System designers; Network security
Security primitives for reconfigurable hardware-based systems,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862878377&doi=10.1145%2f1754386.1754391&partnerID=40&md5=d123c1e8ed5385e320028de486df6584,"Computing systems designed using reconfigurable hardware are increasingly composed using a number of different Intellectual Property (IP) cores, which are often provided by third-party vendors that may have different levels of trust. Unlike traditional software where hardware resources are mediated using an operating system, IP cores have fine-grain control over the underlying reconfigurable hardware. To address this problem, the embedded systems community requires novel security primitives that address the realities of modern reconfigurable hardware. In this work, we propose security primitives using ideas centered around the notion of ""moats and drawbridges."" The primitives encompass four design properties: logical isolation, interconnect traceability, secure reconfigurable broadcast, and configuration scrubbing. Each of these is a fundamental operation with easily understood formal properties, yet they map cleanly and efficiently to a wide variety of reconfigurable devices. We carefully quantify the required overheads of the security techniques on modern FPGA architectures across a number of different applications. © 2010 ACM.",Design; Security,Bridges; Design; Reconfigurable hardware; Computing system; Formal properties; FPGA architectures; Fundamental operations; Hardware resources; Hardware-based systems; Intellectual property cores; IP core; Reconfigurable devices; Security; Security primitives; Hardware
VFloat: A variable precision fixedand floating-point library for reconfigurable hardware,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856968087&doi=10.1145%2f1839480.1839486&partnerID=40&md5=5ff6c203bfa92e243f7dedba97a993ce,"Optimal reconfigurable hardware implementations may require the use of arbitrary floating-point formats that do not necessarily conform to IEEE specified sizes. We present a variable precision floating-point library (VFloat) that supports general floating-point formats including IEEE standard formats. Most previously published floating-point formats for use with reconfigurable hardware are subsets of our format. Custom datapaths with optimal bitwidths for each operation can be built using the variable precision hardware modules in the VFloat library, enabling a higher level of parallelism. The VFloat library includes three types of hardware modules for format control, arithmetic operations, and conversions between fixed-point and floating-point formats. The format conversions allow for hybrid fixed- and floating-point operations in a single design. This gives the designer control over a large number of design possibilities including format as well as number range within the same application. In this article, we give an overview of the components in the VFloat library and demonstrate their use in an implementation of the K-means clustering algorithm applied to multispectral satellite images. © 2010 ACM.",Algorithms; Design; Experimentation; Performance,Algorithms; Computer hardware; Design; Fixed point arithmetic; IEEE Standards; K-means clustering; Arithmetic operations; Experimentation; Floating point operations; Format conversion; Hardware implementations; Multispectral satellite image; Performance; Variable precision; Reconfigurable hardware
Performance analysis framework for high-level language applications in reconfigurable computing,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952760654&doi=10.1145%2f1661438.1661443&partnerID=40&md5=b2571c101cc23cd3cad0e5cc5dfaf976,"High-Level Languages (HLLs) for Field-Programmable Gate Arrays (FPGAs) facilitate the use of reconfigurable computing resources for application developers by using familiar, higher-level syntax, semantics, and abstractions, typically enabling faster development times than with traditional Hardware Description Languages (HDLs). However, programming at a higher level of abstraction is typically accompanied by some loss of performance as well as reduced transparency of application behavior, making it difficult to understand and improve application performance. While runtime tools for performance analysis are often featured in development with traditional HLLs for sequential and parallel programming, HLL-based development for FPGAs has an equal or greater need yet lacks these tools. This article presents a novel and portable framework for runtime performance analysis of HLL applications for FPGAs, including an automated tool for performance analysis of designs created with Impulse C, a commercial HLL for FPGAs. As a case study, this tool is used to successfully locate performance bottlenecks in a molecular dynamics kernel in order to gain speedup. © 2010 ACM.",Measurement; Performance,Abstracting; Computer hardware description languages; High level languages; Measurements; Molecular dynamics; Parallel programming; Semantics; Application developers; Application performance; Automated tools; Level of abstraction; Loss of performance; Performance; Performance analysis; Performance bottlenecks; Reconfigurable computing; Runtime performance; Runtimes; Reconfigurable architectures
Exploiting self-reconfiguration capability to improve SRAM-based FPGA robustness in space and avionics applications,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988344466&doi=10.1145%2f1857927.1857935&partnerID=40&md5=f039e00df1af5d0846dcb82445a51e65,"This article presents a novel configuration scrubbing core, used for internal detection and correction of radiation-induced configuration single and multiple bit errors, without requiring external scrubbing. The proposed technique combines the benefits of fast radiation-induced fault detection with fast restoration of the device functionality and small area and power overheads. Experimental results demonstrate that the novel approach significantly improves the availability in hostile radiation environments of FPGA-based designs. When implemented using a Xilinx XC2V1000 Virtex-II device, the presented technique detects and corrects single bit upsets and double, triple and quadruple multi bit upsets, occupying just 1488 slices and dissipating less than 30 mW at a 50MHz running frequency. © 2010 ACM.",Reconfigurable computing; Space and avionics applications; Xilinx FPGAs,Avionics; Fault detection; Field programmable gate arrays (FPGA); Reconfigurable architectures; Device functionality; Fast restoration; Hostile radiation; Radiation-induced; Reconfigurable computing; Self reconfiguration; Space and avionics applications; Xilinx fpgas; Reconfigurable hardware
A high throughput FPGA-based floating point conjugate gradient implementation for dense matrices,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953230605&doi=10.1145%2f1661438.1661439&partnerID=40&md5=bb9acafb115a153b11349cf08fd474e9,"Recent developments in the capacity of modern Field Programmable Gate Arrays (FPGAs) have significantly expanded their applications. One such field is the acceleration of scientific computation and one type of calculation that is commonplace in scientific computation is the solution of systems of linear equations. A method that has proven in software to be very efficient and robust for finding such solutions is the Conjugate Gradient (CG) algorithm. In this article we present a widely parallel and deeply pipelined hardware CG implementation, targeted at modern FPGA architectures. This implementation is particularly suited for accelerating multiple smallto- medium-sized dense systems of linear equations and can be used as a stand-alone solver or as building block to solve higher-order systems. In this article it is shown that through parallelization it is possible to convert the computation time per iteration for an order n matrix from θ(n 2) clock cycles on a microprocessor to θ(n) on a FPGA. Through deep pipelining it is also possible to solve several problems in parallel and maximize both performance and efficiency. I/O requirements are shown to be scalable and convergent to a constant value with the increase of matrix order. Post place-and-route results on a readily available VirtexII-6000 demonstrate sustained performance of 5 GFlops, and results on a Virtex5-330 indicate sustained performance of 35 GFlops. A comparison with an optimized software implementation running on a high-end CPU demonstrate that this FPGA implementation represents a significant speedup of at least an order of magnitude. © 2010 ACM.",Algorithms; Design; Performance,Algorithms; Conjugate gradient method; Design; Field programmable gate arrays (FPGA); Iterative methods; Linear equations; Building blockes; Clock cycles; Computation time; Conjugate gradient; Conjugate gradient algorithms; Dense matrices; Dense systems; Floating points; FPGA architectures; FPGA implementations; High throughput; Higher-order systems; N-matrix; Parallelizations; Performance; Pipelined hardware; Scientific computation; Software implementation; Sustained performance; Systems of linear equations; Matrix algebra
Configuration merging in point-to-point networks for module-based FPGA reconfiguration,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867407092&doi=10.1145%2f1661438.1661442&partnerID=40&md5=c779dc60454741e21febf7f091783dfa,"Partial runtime reconfiguration allows some circuit components to be reconfigured while the remaining circuitry continues to operate. Applications partitioned into modules have the potential to exploit this capability to virtualize hardware by swapping modules as required. One of the challenges in doing so is to provide a communication infrastructure that supports the interfaces and communication needs of a sequence of dynamic module swaps. In contrast to previous approaches which have examined the use of buses and networks-on-chip for this purpose, we examine the use of customized point-to-point wiring harnesses to provide the dynamic connections required for dynamic modular reconfiguration in an efficient manner. The COMMA methodology implements applications on tile-reconfigurable FPGAs, such as the Virtex-4, and its design flow is integrated with the early access partial reconfiguration tools from Xilinx. This article outlines the methodology and describes greedy and dynamic programming approaches to merging the communication graphs of successive configurations in order to generate effective wiring harnesses within the methodology. Our evaluation indicates merging can markedly reduce total reconfiguration delays at the cost of increased critical path delays. Application of the technique is likely to be limited to scenarios in which the execution time between reconfigurations is short. © 2010 ACM.",Algorithms; Design,Algorithms; Communication; Design; Merging; Reconfigurable hardware; Circuit components; Communication graphs; Communication infrastructure; Critical path delays; Design flows; Dynamic modules; Execution time; Module-based; Networks on chips; Partial reconfiguration; Partial run-time reconfiguration; Point-to-point network; Total reconfiguration; Wiring harness; Field programmable gate arrays (FPGA)
Sparse matrix-vector multiplication on a reconfigurable supercomputer with application,2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864921475&doi=10.1145%2f1661438.1661440&partnerID=40&md5=ed73a498cce336afb9b2e96640d71013,"Double precision floating point Sparse Matrix-Vector Multiplication (SMVM) is a critical computational kernel used in iterative solvers for systems of sparse linear equations. The poor data locality exhibited by sparse matrices along with the high memory bandwidth requirements of SMVM result in poor performance on general purpose processors. Field Programmable Gate Arrays (FPGAs) offer a possible alternative with their customizable and application-targeted memory sub-system and processing elements. In this work we investigate two separate implementations of the SMVM on an SRC-6 MAPStation workstation. The first implementation investigates the peak performance capability, while the second implementation balances the amount of instantiated logic with the available sustained bandwidth of the FPGA subsystem. Both implementations yield the same sustained performance with the second producing a much more efficient solution. The metrics of processor and application balance are introduced to help provide some insight into the efficiencies of the FPGA and CPU based solutions explicitly showing the tight coupling of the available bandwidth to peak floating point performance. Due to the FPGAs ability to balance the amount of implemented logic to the available memory bandwidth it can provide a much more efficient solution. Finally, making use of the lessons learned implementing the SMVM, we present a fully implemented non-preconditioned Conjugate Gradient Algorithm utilizing the second SMVM design. © 2010 ACM.",Accelerator; Architecture; Conjugate gradient; FPGA; Hybrid computing; Iterative methods; Reconfigurable; Reconfigurable hardware; Sparse matrix-vector multiplication,Architecture; Bandwidth; Conjugate gradient method; Digital arithmetic; Field programmable gate arrays (FPGA); Iterative methods; Matrix algebra; Multiprocessing systems; Particle accelerators; Reconfigurable architectures; Supercomputers; Available bandwidth; Computational kernels; Conjugate gradient; Conjugate gradient algorithms; Customizable; Data locality; Double precision; Floating points; General purpose processors; High memory bandwidth; Hybrid computing; Iterative solvers; Memory bandwidths; Peak performance; Poor performance; Processing elements; Reconfigurable; Reconfigurable supercomputers; Sparse matrices; Sparse matrix-vector multiplication; Sub-systems; Sustained performance; Tight coupling; Reconfigurable hardware
"DSPs, BRAMs, and a pinch of logic: Extended recipes for AES on FPGAs",2010,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951746759&doi=10.1145%2f1661438.1661441&partnerID=40&md5=379e99e94c6f04150f03b5e755a170c5,"We present three lookup-table-based AES implementations that efficiently use the BlockRAM and DSP units embedded within Xilinx Virtex-5 FPGAs. An iterative module outputs a 32-bit AES round column every clock cycle, with a throughput of 1.67 Gbit/s when processing two 128-bit inputs. This construct is then replicated four times to provide a complete AES round per cycle with 6.7 Gbit/s throughput when processing eight input streams. This, in turn, is replicated ten times for a fully unrolled design providing over 52 Gbit/s of throughput. We also present implementations of a BRAM-based AES key-expansion, CMAC, and CTR modes of operation. Results for designs where DSPs are replaced by regular logic are also presented. The combination and arrangement of the specialized embedded functions available in the FPGA allows us to implement our designs using very few traditional user logic elements such as flip-flops and lookup tables, yet still achieve these high throughputs. HDL source code, simulation testbenches, and software tool commands to reproduce reported results for the three AES variants and CMAC mode are made publicly available. Our contribution concludes with a discussion on comparing cipher implementations in the literature, and why these comparisons can be meaningless without a common reporting methodology, or within the context of a constrained target application. © 2010 ACM.",Algorithms; Performance; Security,Algorithms; Design; Iterative methods; Table lookup; Throughput; Clock cycles; Embedded function; High throughput; Input streams; Logic elements; Modes of operation; Performance; Security; Source codes; Target application; Digital signal processing
FPGA-based hardware acceleration of lithographic aerial image simulation,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862931159&doi=10.1145%2f1575774.1575776&partnerID=40&md5=c68d416bcfe5ae04756799811bf6c990,"Lithography simulation, an essential step in design for manufacturability (DFM), is still far from computationally efficient. Most leading companies use large clusters of server computers to achieve acceptable turn-around time. Thus coprocessor acceleration is very attractive for obtaining increased computational performance with a reduced power consumption. This article describes the implementation of a customized accelerator on FPGA using a polygon-based simulation model. An application-specific memory partitioning scheme is designed to meet the bandwidth requirements for a large number of processing elements. Deep loop pipelining and ping-pong buffer based function block pipelining are also implemented in our design. Initial results show a 15X speedup versus the software implementation running on a microprocessor, and more speedup is expected via further performance tuning. The implementation also leverages state-of-art C-to-RTL synthesis tools. At the same time, we also identify the need for manual architecture-level exploration for parallel implementations. Moreover, we implement the algorithm on NVIDIA GPUs using the CUDA programming environment, and provide some useful comparisons for different kinds of accelerators. © 2009 ACM.",Algorithms; Design; Performance,Closed loop control systems; Computer simulation; Design; Machine design; Program processors; Aerial image simulation; Bandwidth requirement; Co-processors; Computational performance; Computationally efficient; Design-for-manufacturability; Function Block; Hardware acceleration; Large clusters; Lithography Simulation; Loop pipelining; Parallel implementations; Performance; Performance tuning; Processing elements; Programming environment; Reduced power consumption; Simulation model; Software implementation; Synthesis tool; Algorithms
A-port networks: Preserving the timed behavior of synchronous systems for modeling on FPGAs,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867422032&doi=10.1145%2f1575774.1575775&partnerID=40&md5=18d57761ee5efce7d6de2dae9be13c93,"Computer architects need to run cycle-accurate performance models of processors orders of magnitude faster. We discuss why the speedup on traditional multicores is limited, and why FPGAs represent a good vehicle to achieve a dramatic performance improvement over software models. This article introduces A-Port Networks, a simulation scheme designed to expose the fine-grained parallelism inherent in performance models and efficiently exploit them using FPGAs.© 2009 ACM.",Design; Measurement; Performance,Design; Measurements; Parallel architectures; Computer architects; Cycle accurate; Fine-grained parallelism; Multi-cores; Orders of magnitude; Performance; Performance improvements; Performance Model; Software model; Synchronous system; Computer simulation
Exploring reconfigurable architectures for tree-based option pricing models,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859798733&doi=10.1145%2f1575779.1575781&partnerID=40&md5=32b54291b47776c76577beb23201d972,"This article explores the application of reconfigurable hardware to the acceleration of financial computation using tree-based pricing models. Two parallel pipelined architectures have been developed for option valuation using binomial trees and trinomial trees, with support for concurrent evaluation of independent options to achieve high pricing throughput. Our results show that the tree-based models executing on a Virtex 4 field programmable gate array (FPGA) at 82.7 MHz with fixed-point arithmetic can run over 160 times faster than a Core2 Duo processor at 2.2 GHz. The FPGA implementation is two times faster than the nVidia Geforce 7900GTX processor with 24 pipelines at 650 MHz, and 27%-35% slower than the nVidia Geforce 8600GTS processor with 32 Pipelines at 1450 MHz. Our preliminary experiments also indicate that while an FPGA implementation can be slower than a GPU, it could be more efficient when power consumption is taken into account. © 2009 ACM 1936-7406/2009/09-ART21.",Binomial; FPGA; Gpu; Hyperstreams; Lattice; Trinomial,Economics; Field programmable gate arrays (FPGA); Forestry; Reconfigurable architectures; Binomial; Gpu; Hyperstreams; Lattice; Trinomial; Pipeline processing systems
An FPGA logic cell and carry chain configurable as a 6:2 or 7:2 compressor,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858272465&doi=10.1145%2f1575774.1575778&partnerID=40&md5=ae467bf9a37eb7e6ac522706d925f23f,"To improve FPGA performance for arithmetic circuits that are dominated by multi-input addition operations, an FPGA logic block is proposed that can be configured as a 6:2 or 7:2 compressor. Compressors have been used successfully in the past to realize parallel multipliers in VLSI technology; however, the peculiar structure of FPGA logic blocks, coupled with the high cost of the routing network relative to ASIC technology, renders compressors ineffective when mapped onto the general logic of an FPGA. On the other hand, current FPGA logic cells have already been enhanced with carry chains to improve arithmetic functionality, for example, to realize fast ternary carry-propagate addition. The contribution of this article is a new FPGA logic cell that is specialized to help realize efficient compressor trees on FPGAs. The new FPGA logic cell has two variants that can respectively be configured as a 6:2 or a 7:2 compressor using additional carry chains that, coupled with lookup tables, provide the necessary functionality. Experiments show that the use of these modified logic cells significantly reduces the delay of compressor trees synthesized on FPGAs compared to state-of-the-art synthesis techniques, with a moderate increase in area and power consumption. © 2009 ACM.",Algorithms; Performance,Algorithms; Field programmable gate arrays (FPGA); Forestry; Logic devices; Arithmetic circuit; ASIC technologies; High costs; Logic blocks; Logic cells; Multiinput; Parallel multipliers; Performance; Routing networks; Synthesis techniques; VLSI technology; Compressors
Space optimization on counters for FPGA-based perl compatible regular expressions,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864742688&doi=10.1145%2f1575779.1575783&partnerID=40&md5=d4d1895d2524cd487bbc8a07cd8aacab,"With their expressiveness and simplicity, Perl compatible regular expressions (PCREs) have been adopted in mainstream signature based network intrusion detection systems (NIDSs) to describe known attack signatures, especially for polymorphic worms. NIDSs rely on an underlying string matching engine that simulates PCREs to inspect each network packet. PCRE is a superset of traditional regular expressions, and provides advanced features. However, this pattern matching becomes a performance bottleneck of software-based NIDSs, causing a big portion of their execution time to be dedicated to payload inspection, which results in an unacceptable packet drop rate. The penetration of these unexamined packets creates a security hole in such systems. Over the past decade, hardware acceleration for the pattern matching has been studied extensively and a marginal performance has been achieved. Among hardware approaches, FPGA-based acceleration engines provide great flexibility because new signatures can be compiled and programmed into their reconfigurable architecture. As more and more malicious signatures are discovered, it becomes harder to map a complete set of malicious signatures specified in PCREs to an FPGA chip. One of the space consuming components is the counter used in the constrained repetitions for PCREs. Therefore, we propose a space efficient SelectRAM counter for PCREs that use counting. The design takes advantage of the basic components contained in a configurable logic block, and thus optimizes space usage. A set of basic PCRE blocks has been built in hardware to implement PCREs. Experimental results show that the proposed scheme outperforms existing designs by at least fivefold. © 2009 ACM 1936-7406/2009/09-ART23.",FPGA; Network intrusion detection; Regular expression; String matching,Field programmable gate arrays (FPGA); Hardware; Intrusion detection; Network security; Reconfigurable architectures; Attack signature; Built-in-hardware; Configurable Logic Blocks; Execution time; FPGA chips; Hardware acceleration; Network intrusion detection; Network packets; Packet drops; Performance bottlenecks; Polymorphic worms; Regular expressions; Security holes; Signature-based network intrusion detection systems; Software-based; Space efficient; Space usage; String matching; Pattern matching
Packing techniques for virtex-5 FPGAs,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867413107&doi=10.1145%2f1575774.1575777&partnerID=40&md5=aa470202b4d9f28d42cc4d4b07146edc,"Packing is a key step in the FPGA tool flow that straddles the boundaries between synthesis, technology mapping and placement. Packing strongly influences circuit speed, density, and power, and in this article, we consider packing in the commercial FPGA context and examine the area and performance trade-offs associated with packing in a state-of-the-art FPGA-the Xilinx ® Virtex™-5 FPGA. In addition to look-up-table (LUT)-based logic blocks, modern FPGAs also contain large IP blocks. We discuss packing techniques for both types of blocks. Virtex-5 logic blocks contain dual-output 6-input LUTs. Such LUTs can implement any single logic function of up to 6 inputs, or any two logic functions requiring nomore than 5 distinct inputs. The second LUT output has reduced speed, and therefore, must be used judiciously. We present techniques for dual-output LUT packing that lead to improved area-efficiency, with minimal performance degradation. We then describe packing techniques for large IP blocks, namely, block RAMs and DSPs. We pack circuits into the large blocks in a way that leverages the unique block RAM and DSP layout/architecture in Virtex-5, achieving significantly improved design performance. © 2009 ACM.",Algorithms; Design,Algorithms; Design; Block rams; Design performance; IP block; Logic blocks; Logic functions; Look-up-table; Packing techniques; Performance degradation; Performance trade-off; Technology mapping; Tool flow; Digital signal processing
Introduction to the special issue ARC'08,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867608103&doi=10.1145%2f1575779.1575780&partnerID=40&md5=94877e200c9ef24e67664d10a7839a98,[No abstract available],,
Robust real-time super-resolution on FPGA and an application to video enhancement,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951957832&doi=10.1145%2f1575779.1575782&partnerID=40&md5=2bdd1de2785c76487a62b86c9210a8f9,"The high density image sensors of state-of-the-art imaging systems provide outputs with high spatial resolution, but require long exposure times. This limits their applicability, due to the motion blur effect. Recent technological advances have lead to adaptive image sensors that can combine several pixels together in real time to form a larger pixel. Larger pixels require shorter exposure times and produce high-frame-rate samples with reduced motion blur. This work proposes combining an FPGA with an adaptive image sensor to produce an output of high resolution both in space and time. The FPGA is responsible for the spatial resolution enhancement of the highframe- rate samples using super-resolution (SR) techniques in real time. To achieve it, this article proposes utilizing the Iterative Back Projection (IBP) SR algorithm. The original IBP method is modified to account for the presence of noise, leading to an algorithm more robust to noise. An FPGA implementation of this algorithm is presented. The proposed architecture can serve as a general purpose real-time resolution enhancement system, and its performance is evaluated under various noise levels. © 2009 ACM 1936-7406/2009/09-ART22.",FPGA; Motion deblurring; Real time; Reconfigurable computing; Smart camera; Super-resolution,Algorithms; Field programmable gate arrays (FPGA); Image enhancement; Image sensors; Iterative methods; Pixels; Reconfigurable architectures; Motion deblurring; Real time; Reconfigurable computing; Smart cameras; Super resolution; Image resolution
Optimal loop unrolling and shifting for reconfigurable architectures,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949366711&doi=10.1145%2f1575779.1575785&partnerID=40&md5=1fd58b2f614dc875003c308a161d1f51,"In this article, we present a new technique for optimizing loops that contain kernels mapped on a reconfigurable fabric. We assume the Molen machine organization as our framework. We propose combining loop unrolling with loop shifting, which is used to relocate the function calls contained in the loop body such that in every iteration of the transformed loop, software functions (running on GPP) execute in parallel with multiple instances of the kernel (running on FPGA). The algorithm computes the optimal unroll factor and determines the most appropriate transformation (which can be the combination of unrolling plus shifting or either of the two). This method is based on profiling information about the kernel's execution times on GPP and FPGA, memory transfers and area utilization. In the experimental part, we apply this method to several kernels from loop nests extracted from real-life applications (DCT and SAD from MPEG2 encoder, Quantizer from JPEG, and Sobel's Convolution) and perform an analysis of the results, comparing them with the theoretical maximum speedup by Amdahl's Law and showing when and how our transformations are beneficial. © 2009 ACM 1936-7406/2009/09-ART25.",Loop optimizations; Reconfigurable computing,Optimization; Reconfigurable architectures; Area utilization; Execution time; Function calls; Loop nests; Loop optimizations; Loop shifting; Loop unrolling; Multiple instances; Quantizers; Real-life applications; Reconfigurable computing; Reconfigurable fabrics; Software functions; Iterative methods
An application development framework for ARISE reconfigurable processors,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051614119&doi=10.1145%2f1575779.1575784&partnerID=40&md5=2677e1d2856160db05a930633906fdfe,"Coupling reconfigurable hardware accelerators with processors is an effective way to meet the performance and flexibility required to cope with modern embedded applications. The ARISE framework provides a systematic approach to extend a processor once. It will thereafter support the coupling of arbitrary hardware accelerators. The accelerators can be coupled as coprocessors or functional units of the processor's datapath, and therefore exploited as a hybrid, which includes both loose and tight computational models. This article presents a complete framework for developing applications on such hybrid reconfigurable ARISE machines. The framework integrates the automatic identification of custom instructions and the semiautomatic/profiling-driven identification of coprocessors supporting the hybrid computational model. Moreover, it supports a modular design approach where the software and the hardware modules are developed independently and later ported into any ARISE machine with reconfigurable technology. To evaluate efficiency, a set of benchmarks is implemented on an ARISE evaluation machine utilizing the proposed framework. In addition, the ARISE machine is compared against a well-established processor paradigm that utilizes reconfigurable accelerators following only the typical coprocessor approach. Experimental results prove that the framework can be used to exploit the hybrid computational model and achieve significant performance improvements over the typical coprocessor acceleration approach. Moreover, results demonstrate how the framework can be used to trade off performance, silicon area, and application development time. © 2009 ACM 1936-7406/2009/09-ART24.",Custom computing units; Development framework; Hybrid computational model; Instruction set extensions; Reconfigurable processors,Automation; Computational methods; Computer hardware; Couplings; Digital signal processing; Hardware; Reconfigurable hardware; Custom computing units; Development framework; Hybrid computational model; Instruction set extension; Reconfigurable processors; Program processors
From silicon to science: The long road to production reconfigurable supercomputing,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80055074289&doi=10.1145%2f1575779.1575786&partnerID=40&md5=fd9502e6bedec473fd6e035e510c7950,"The field of high performance computing (HPC) currently abounds with excitement about the potential of a broad class of things called accelerators. And, yet, few accelerator based systems are being deployed in general purpose HPC environments. Why is that? This article explores the challenges that accelerators face in the HPC world, with a specific focus on FPGA based systems. We begin with an overview of the characteristics and challenges of typical HPC systems and applications and discuss why FPGAs have the potential to have a significant impact. The bulk of the article is focused on twelve specific areas where FPGA researchers can make contributions to hasten the adoption of FPGAs in HPC environments. © 2009 ACM 1936-7406/2009/09-ART26.",FPGA; HPC; Reconfigurable computing,Field programmable gate arrays (FPGA); General purpose; High performance computing (HPC); HPC; Reconfigurable computing; Significant impacts; Specific areas; Reconfigurable architectures
TAS-MRAM-based low-power high-speed runtime reconfiguration (RTR) FPGA,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449353237&doi=10.1145%2f1534916.1534918&partnerID=40&md5=2c9ccc14f8dc47a33313a4f5816b30e2,"As one of the most promising Spintronics applications, MRAM combines the advantages of high writing and reading speed, limitless endurance, and nonvolatility. The integration of MRAM in FPGAs allows the logic circuit to rapidly configure the algorithm, the routing and logic functions, and easily realize the Runtime Reconfiguration (RTR) and multicontext configuration. However, the conventional MRAM technology based on the Field Induced Magnetic Switching (FIMS) writing approach consumes very high power, large circuit surfaces, and produces high disturbance between memory cells. These drawbacks prevent FIMS-MRAM's further development in memory and logic circuit. Thermally Assisted Switching (TAS)-based MRAM is then evaluated to address these issues. In this article, some design techniques, novel computing architecture, and logic components for FPGA logic circuits based on TAS-MRAM technology are presented. By using STMicroelectronics CMOS 90nm technology and a complete TAS-MTJ spice model, some chip characteristic results such as the programming latency (~25ns) and power dissipation (~124pJ) have been calculated or simulated to demonstrate the expected performance of TAS-MRAM-based FPGA logic circuits. © 2009 ACM.",Design; Experimentation,Computer simulation; Design; Logic design; Magnetic devices; MRAM devices; Nanotechnology; SPICE; Computing architecture; Design technique; Experimentation; Field induced; High-power; High-speed; Large circuits; Logic components; Logic functions; Low Power; Magnetic switching; Memory cell; MRAM technology; Nonvolatility; Reading speed; Run-time reconfiguration; Spice model; Spintronics application; STMicroelectronics; Field programmable gate arrays (FPGA)
Automation schemes for FPGA implementation of wave-pipelined circuits,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954397415&doi=10.1145%2f1534916.1534921&partnerID=40&md5=0c14619d89bb1fb31d93bceac3e2380c,"Operating frequencies of combinational logic circuits can be increased using Wave-Pipelining (WP), by adjusting the clock periods and clock skews. In this article, Built-In Self-Test (BIST) and System-on-Chip (SOC) approaches are proposed for automating this adjustment and they are evaluated by implementation of filters using a Distributed Arithmetic Algorithm (DAA) and sinewave generator using the COordinate Rotation DIgital Computer (CORDIC). Both the circuits are studied by adopting three schemes: wave-pipelining, pipelining, and nonpipelining. Xilinx Spartan II and Altera Cyclone II FPGAs with Nios II soft-core processor are used for implementation of the circuits with the BIST and SOC approaches, respectively. The proposed schemes increase the speed of the WP circuits by a factor of 1.19-2.6 compared to nonpipelined circuits. The pipelined circuits achieve higher speed than the WP circuits by a factor of 1.13-3.27 at the cost of increase in area and power. When both pipelined and WP circuits are operated at the same frequency, the former dissipates more power for circuits with higher word sizes and for moderate logic depths. The observation regarding the dependence of the superiority of the WP circuits with regard to power dissipation on the logic depth is one of the major contributions of this article. © 2009 ACM.",Design; Terms,Application specific integrated circuits; Built-in self test; Clocks; Design; Electric network topology; Altera cyclones; Automation scheme; Clock period; Clock skews; Co-ordinate rotation digital computers; Combinational logic circuits; Distributed arithmetic algorithm; FPGA implementations; Logic depth; NIOS II; Operating frequency; Pipelined circuits; Sine-wave; Soft-core processors; System-On-Chip; Terms; Wave-pipelining; Field programmable gate arrays (FPGA)
Guest editors' introduction ICFPT 2007,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867430130&doi=10.1145%2f1534916.1534917&partnerID=40&md5=6bed594a594a336bacaa2be9423c18e3,[No abstract available],,
Self-measurement of combinatorial circuit delays in FPGAs,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78549237663&doi=10.1145%2f1534916.1534920&partnerID=40&md5=6066c995277492c90284dcbc770d5fc2,"This article proposes a Built-In Self-Test (BIST) method to accurately measure the combinatorial circuit delays on an FPGA. The flexibility of the on-chip clock generation capability found in modern FPGAs is employed to step through a range of frequencies until timing failure in the combinatorial circuit is detected. In this way, the delay of any combinatorial circuit can be determined with a timing resolution of the order of picoseconds. Parallel and optimized implementations of the method for self-characterization of the delay of all the LUTs on an FPGA are also proposed. The method was applied to Altera Cyclone II and III FPGAs . A complete self-characterization of LUTs on a Cyclone II was achieved in 2.5 seconds, utilizing only 13kbit of block RAM to store the results. More extensive tests were carried out on the Cyclone III and the delays of adder circuits and embedded multiplier blocks were successfully measured. This self-measurement method paves the way for matching timing requirements in designs to FPGAs as a means of combating the problem of process variations. © 2009 ACM.",Performance,Built-in self test; Electric network analysis; Adder circuit; Altera cyclones; Block rams; Multiplier blocks; On-Chip Clock Generation; Optimized implementation; Performance; Picoseconds; Process Variation; Timing requirements; Timing resolutions; Combinatorial circuits
Hardware decompression techniques for FPGA-based embedded systems,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955723772&doi=10.1145%2f1534916.1534919&partnerID=40&md5=eaf1124d3bd8fdb3afda50f0cda2b84f,"In this work, we present hardware decompression accelerators for widening the bottleneck between slow nonvolatile memories on the one side and high-speed FPGA configuration interfaces and fast softcore CPUs on the other side. We discuss different compression algorithms suitable for a hardware accelerated decompression on FPGAs as well as on CPLDs. The algorithms will be investigated with respect to the achievable compression ratio, throughput, and hardware overhead. This leads to various decompressor implementations with one capable to decompress at high data rates of up to 400 megabytes per second under optimal conditions while only requiring slightly more than a hundred lookup tables. We will evaluate how these decompressors perform on configuration bitstreams for different FPGAs as well as for softcore CPU binaries © 2009 ACM.",Algorithms; Experimentation,Algorithms; Compression ratio (machinery); Program processors; Bitstreams; Compression algorithms; Decompressors; Experimentation; FPGA configuration; Hardware overheads; Hardware-accelerated; High data rate; High-speed; Non-volatile memories; Optimal conditions; Hardware
"PROTOFLEX: Towards scalable, full-system multiprocessor simulations using FPGAs",2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952560029&doi=10.1145%2f1534916.1534925&partnerID=40&md5=13072061e81967649fc9682e5e721ad7,"15 PROTOFLEX: Towards Scalable, Full-System Multiprocessor Simulations Using FPGAs ERIC S. CHUNG, MICHAEL K. PAPAMICHAEL, ERIKO NURVITADHI, JAMES C. HOE, and KEN MAI Computer Architecture Laboratory at Carnegie Mellon and BABAK FALSAFI Parallel Systems Architecture Laboratory École Polytechnique Fédérale de Lausanne Functional full-system simulators are powerful and versatile research tools for accelerating architectural exploration and advanced software development. Their main shortcoming is limited throughput when simulating large multiprocessor systems with hundreds or thousands of processors or when instrumentation is introduced. We propose the PROTOFLEX simulation architecture, which uses FPGAs to accelerate full-system multiprocessor simulation and to facilitate high-performance instrumentation. Prior FPGA approaches that prototype a complete system in hardware are either too complex when scaling to large-scale configurations or require significant effort to provide full-system support. In contrast, PROTOFLEX virtualizes the execution of many logical processors onto a consolidated number of multiple-context execution engines on the FPGA. Through virtualization, the number of engines can be judiciously scaled, as needed, to deliver on necessary simulation performance at a large savings in complexity. Further, to achieve low-complexity full-system support, a hybrid simulation technique called transplanting allows implementing in the FPGA only the frequently encountered behaviors, while a software simulator preserves the abstraction of a complete system. We have created a first instance of the PROTOFLEX simulation architecture, which is an FPGAbased, full-system functional simulator for a 16-way UltraSPARC III symmetric multiprocessor server, hosted on a single Xilinx Virtex-II XCV2P70 FPGA. On average, the simulator achieves a 38x speedup (and as high as 49×) over comparable software simulation across a suite of applications, including OLTP on a commercial database server. We also demonstrate the advantages of minimal-overhead FPGA-accelerated instrumentation through a CMP cache simulation technique that runs orders-of-magnitude faster than software. © 2009 ACM.",Design; Experimentation; Performance,Computer architecture; Computer simulation languages; Design; Instruments; Multiprocessing systems; Simulators; Cache simulation; Complete system; Data-base servers; Execution engine; Experimentation; Full system simulators; Functional simulators; Hybrid simulation; Low-complexity; Multi processor systems; Multiprocessor simulation; Orders-of-magnitude; Parallel system; Performance; Polytechnique; Research tools; Simulation architecture; Simulation performance; Software simulation; Software simulator; Symmetric multi-processors; UltraSPARC; Virtualizations; Computer software
Vector processing as a soft processor accelerator,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79551526575&doi=10.1145%2f1534916.1534922&partnerID=40&md5=3d7b6d440dd20c0aa1b84187fea8e14f,"Current FPGA soft processor systems use dedicated hardware modules or accelerators to speed up data-parallel applications. This work explores an alternative approach of using a soft vector processor as a general-purpose accelerator. The approach has the benefits of a purely softwareoriented development model, a fixed ISA allowing parallel software and hardware development, a single accelerator that can accelerate multiple applications, and scalable performance from the same source code. With no hardware design experience needed, a software programmer can make area-versus-performance trade-offs by scaling the number of functional units and register file bandwidth with a single parameter. A soft vector processor can be further customized by a number of secondary parameters to add or remove features for a specific application to optimize resource utilization. This article introduces VIPERS, a soft vector processor architecture that maps efficiently into an FPGA and provides a scalable amount of performance for a reasonable amount of area. Compared to a Nios II/s processor, instances of VIPERS with 32 processing lanes achieve up to 44?speedup using up to 26?the area. © 2009 ACM.",Design; Measurement; Performance,Computer architecture; Computer programming; Design; Measurements; Alternative approach; Data parallel; Dedicated hardware; Development model; Functional units; Hardware design; Hardware development; Multiple applications; NIOS II; Parallel software; Performance; Register files; Resource utilizations; Scalable performance; Single parameter; Soft processors; Source codes; Vector processing; Vector processors; Hardware
WireMap: FPGA technology mapping for improved routability and enhanced LUT merging,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951586144&doi=10.1145%2f1534916.1534924&partnerID=40&md5=4d2e031cda4df353dd3cc5d0ec42ba1e,"This article presents a new technology mapper, WireMap. The mapper uses an edge flow heuristic to improve the routability of a mapped design. The heuristic is applied during the iterative mapping optimization to reduce the total number of pin-to-pin connections (or edges). On academic benchmark (ISCAS,MCNC, and ITC designs), the average edge reduction of 9.3% is achieved while maintaining depth and LUT count compared to state-of-the-art technology mapping. Placing and routing the resulting netlists leads to an 8.5% reduction in the total wirelength, a 6.0% reduction in minimum channel width, and a 2.3% reduction in critical path delay. This technique is applied in the Xilinx ISE Design tool to evaluate its effect on industrial Virtex5 circuits. In a set of 20 large designs, we find the edge reduction is 6.8% while total wirelength measured in the placer is reduced by 3.6%. Applying WireMap has an additional advantage of reducing an average number of inputs of LUTs without increasing the total LUT count and depth. The percentages of 5- and 6-LUTs in a typical design are reduced, while the percentages of 2-, 3-, and 4-LUTs are increased. These smaller LUTs can be merged into pairs and implemented using the dual-output LUT structure found in commercial FPGAs. For academic benchmarks, WireMap leads to 9.4% fewer dual-output LUTs after merging. For the industrial designs, WireMap leads to 6.3% fewer dual-output Virtex5 LUTs. © 2009 ACM.",Algorithms; Design; Experimentation; Performance,Algorithms; Iterative methods; Mapping; Merging; Average numbers; Channel widths; Critical path delays; Design tool; Edge flow; Experimentation; FPGA technology; Iterative mapping; Large designs; Performance; Routability; State-of-the-art technology; Typical design; Wire length; Design
Field programmable compressor trees: Acceleration of multi-input addition on FPGAs,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954282426&doi=10.1145%2f1534916.1534923&partnerID=40&md5=5a5835de72c63eefba59b59e7a532b81,"Multi-input addition occurs in a variety of arithmetically intensive signal processing applications. The DSP blocks embedded in high-performance FPGAs perform fixed bitwidth parallel multiplication and Multiply-ACcumulate (MAC) operations. In theory, the compressor trees contained within the multipliers could implement multi-input addition; however, they are not exposed to the programmer. To improve FPGA performance for these applications, this article introduces the Field Programmable Compressor Tree (FPCT) as an alternative to the DSP blocks. By providing just a compressor tree, the FPCT can perform multi-input addition along with parallel multiplication and MAC in conjunction with a small amount of FPGA general logic. Furthermore, the user can configure the FPCT to precisely match the bitwidths of the operands being summed. Although an FPCT cannot beat the performance of a well-designed ASIC compressor tree of fixed bitwidth, for example, 9×9 and 18×18-bit multipliers/MACs in DSP blocks, its configurable bitwidth and ability to perform multi-input addition is ideal for reconfigurable devices that are used across a variety of applications. © 2009 ACM.",Algorithms; Performance,Algorithms; Compressors; Digital signal processing; Forestry; Bit-Width; Multiinput; Multiply-accumulate operation; Performance; Reconfigurable devices; Signal processing applications; Field programmable gate arrays (FPGA)
Electromagnetic radiations of FPGAs: High spatial resolution cartography and attack on a cryptographic module,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349408011&doi=10.1145%2f1502781.1502785&partnerID=40&md5=29d717d033dde3a3087e898b2a6115ec,"Since the first announcement of a Side Channel Analysis (SCA) about ten years ago, considerable research has been devoted to studying these attacks on Application Specific Integrated Circuits (ASICs), such as smart cards or TPMs. In this article, we compare power-line attacks with ElectroMagnetic (EM) attacks, specifically targeting Field Programmable Gate Array devices (FPGAs), as they are becoming widely used for sensitive applications involving cryptography. We show experimentally that ElectroMagnetic Analysis (EMA) is always faster than the historical Differential Power Analysis (DPA) in retrieving keys of symmetric ciphers. In addition, these analyses prove to be very convenient to conduct, as they are totally non-invasive. Research reports indicate that EMA can be conducted globally, typically withmacroscopic homemade coils circling the device under attack, with fair results. However, as accurate professional EM antennas are now becoming more accessible, it has become commonplace to carry out EM analyses locally. Cartography has been carried out by optical means on circuits realized with technology greater than 250 nanometers. Nonetheless, for deep submicron technologies, the feature size of devices that are spied upon is too small to be visible with photographic techniques. In addition, the presence of the 6+metallization layers obviously prevents a direct observation of the layout. Therefore, EM imaging is emerging as a relevant means to discover the underlying device structure. In this article, we present the first images of deep-submicron FPGAs. The resolution is not as accurate as photographic pictures: we notably compare the layout of toy design examples placed at the four corners of the FPGAs with the EM images we collected. We observe that EM imaging has the advantage of revealing active regions, which can be useful in locating a particular processor (visible while active-invisible when inactive). In the context of EM attacks, we stress that the exact localization of the cryptographic target is not necessary: the coarse resolution we obtain is sufficient. We note that the EM imaging does not reveal the exact layout of the FPGA, but instead directly guides the attacker towards the areas which are leaking the most. We achieve attacks with an accurate sensor, both far from (namely on a SMC capacitor on the board) and close to (namely directly over the FPGA) the encryption co-processor. As compared to the previously published attacks, we report a successful attack on a DES module in fewer than 6,300 measurements, which is currently the best cracking performance against this encryption algorithm implemented in FPGAs. © 2009 ACM 1936-7406/2009/03-ART4.",Cartography; DPA; EMA; FPGA; SCA; Security,Application specific integrated circuits; Electromagnetism; Field programmable gate arrays (FPGA); Mapping; Maps; Smart cards; Active regions; Co-processors; Cracking performance; Cryptographic Module; Deep sub-micron; Deep sub-micron technology; Device structures; Differential power Analysis; DPA; Electromagnetic analysis; EM analysis; EMA; Encryption algorithms; Feature sizes; High spatial resolution; Metallization layers; Photographic techniques; Power lines; Research reports; SCA; Security; Sensitive application; Side-channel analysis; Symmetric cipher; Cryptography
Guest editors' introduction to security in reconfigurable systems design,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867408984&doi=10.1145%2f1502781.1502782&partnerID=40&md5=98d34b24e73a861ae81d495f23787065,"This special issue on Security in Reconfigurable Systems Design reports on recent research results in the design and implementation of trustworthy reconfigurable systems. Five articles cover topics including power-efficient implementation of public-key cryptography, side-channel analysis of electromagnetic radiation, side-channel resistant design, design of robust unclonable functions on an FPGA, and Trojan detection in an FPGA bitstream. © 2009 ACM.",Physically unclonable function; Side-channel resistant design; Trojan; Trustworthy design,Electromagnetic waves; Public key cryptography; Systems analysis; Bit stream; Power efficient; Reconfigurable systems; Research results; Side-channel; Side-channel analysis; Trojans; Structural design
Techniques for design and implementation of secure reconfigurable PUFs,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-76949093125&doi=10.1145%2f1502781.1502786&partnerID=40&md5=e5bb90176f46a25e7f33c03d300556c2,"Physically unclonable functions (PUFs) provide a basis for many security and digital rights management protocols. PUF-based security approaches have numerous comparative strengths with respect to traditional cryptography-based techniques, including resilience against physical and side channel attacks and suitability for lightweight protocols. However, classical delay-based PUF structures have a number of drawbacks including susceptibility to guessing, reverse engineering, and emulation attacks, as well as sensitivity to operational and environmental variations. To address these limitations, we have developed a new set of techniques for FPGA-based PUF design and implementation. We demonstrate how reconfigurability can be exploited to eliminate the stated PUF limitations. We also show how FPGA-based PUFs can be used for privacy protection. Furthermore, reconfigurability enables the introduction of new techniques for PUF testing. The effectiveness of all the proposed techniques is validated using extensive implementations, simulations, and statistical analysis. © 2009 ACM.",Hardware security; Physically unclonable functions; Process variation; Reconfigurable systems,Copyrights; Reverse engineering; Sensitivity analysis; Structural design; Digital Rights Management; Environmental variations; Hardware security; Lightweight protocols; Privacy protection; Process Variation; Reconfigurability; Reconfigurable systems; Security approach; Side channel attack; Cryptography
Elliptic curve cryptography on FPGA for low-power applications,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862643550&doi=10.1145%2f1502781.1502783&partnerID=40&md5=3bd580738bb32955bc6faceb9bf804b6,"Elliptic curve cryptography has generated a lot of research interest due to its ability to provide greater security per bit compared to public key systems such as RSA. The designer of an elliptic curve hardware accelerator is faced withmany choices at design time, each of which can impact the performance of the accelerator in different ways. There are many examples in the literature of how these design choices can effect the area and/or speed of an elliptic curve hardware accelerator. The effect of design choices on power and energy consumption in elliptic curve hardware has been less well studied. This article studies the effect of design choices on the power and energy consumption of an FPGA-based reconfigurable elliptic curve hardware accelerator. A reconfigurable processor has been used for different system parameters and the power and energy consumption measured. The power and energy results are presented and compared. © 2009 ACM.",Cryptography; Elliptic curves; FPGA; Low-power,Cryptography; Design; Energy utilization; Field programmable gate arrays (FPGA); Hardware; Design time; Elliptic curve; Elliptic curve cryptography; Hardware accelerators; Low Power; Low power application; Public key systems; Reconfigurable processors; Reconfigurable hardware
Trust-based design and check of FPGA circuits using two-level randomized ECC structures,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864751110&doi=10.1145%2f1502781.1508209&partnerID=40&md5=4b50d557bbd6424fa088ccc72cbacfe2,"A novel trust-based design method for FPGA circuits that uses error-correcting code (ECC) structures for detecting design tampers (changes, deletion of existing logic, and addition of extradesign logic-like Trojans) is proposed in this article. We determine ECC-based CLB (configuration logic block) ""parity groups"" and embed the check CLBs for each parity group in the FPGA circuit. During a trust-checking phase, a Test-Pattern Generator (TPG) and an Output Response Analyzer (ORA), configured in the FPGA, are used to check that each parity group of CLB outputs produce the expected parities. We use two levels of randomization to thwart attempts by an adversary to discover the parity groups and inject tampers that mask each other, or to tamper with the TPG and ORA so that design tampers remain undetected: (a) randomization of the mapping of the ECC parity groups to the CLB array; (b) randomization within each parity group of odd and even parities for different input combinations (classically, all ECC parity groups have even parities across all inputs). These randomizations along with the error-detecting property of the underlying ECC lead to design tampers being uncovered with very high probabilities, as we show both analytically and empirically. We also classify different CLB function structures and impose a parity group selection in which only similarly structured functions are randomly selected to be in the same parity group in order to minimize check function complexity. Using the 2D code as our underlying ECC and its 2-level randomization, our experiments with inserting 1-10 circuit CLB tampers and 1-5 extraneous logic CLBs in two medium-size circuits and a RISC processor circuit implemented on a Xilinx Spartan-3 FPGA show promising results of 100% tamper detection and 0% false alarms, obtained at a hardware overhead of only 7-10%. © 2009 ACM.",Error-correcting codes; FPGAs; Masking probability; Parity groups; Parity randomization; Trust checking; Trust-based design,Design; Field programmable gate arrays (FPGA); Information theory; Random processes; Error correcting code; Masking probability; Parity groups; Parity randomization; Trust checking; Logic circuits
Isolated WDDL: A hiding countermeasure for differential power analysis on FPGAs,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864132329&doi=10.1145%2f1502781.1502784&partnerID=40&md5=5edc6d4a9db14e2162344b678ca4256b,"Security protocols are frequently accelerated by implementing the underlying cryptographic functions in reconfigurable hardware. However, unprotected hardware implementations are susceptible to side-channel attacks, and Differential Power Analysis (DPA) has been shown to be especially powerful. In this work, we evaluate and compare the effectiveness of common hiding countermeasures against DPA in FPGA-based designs, using the Whirlpool hash function as a case study. In particular, we develop a new design flow called Isolated WDDL (IWDDL). In contrast with previous works, IWDDL isolates the direct and complementary circuit paths, and also provides DPA resistance in the Hamming distance power model. The analysis is supported using actual implementation results. Security protocols are frequently accelerated by implementing the underlying cryptographic functions in reconfigurable hardware. However, unprotected hardware implementations are susceptible to side-channel attacks, and Differential Power Analysis (DPA) has been shown to be especially powerful. In this work, we evaluate and compare the effectiveness of common hiding countermeasures against DPA in FPGA-based designs, using the Whirlpool hash function as a case study. In particular, we develop a new design flow called Isolated WDDL (IWDDL). In contrast with previous works, IWDDL isolates the direct and complementary circuit paths, and also provides DPA resistance in the Hamming distance power model. The analysis is supported using actual implementation results. © 2009 ACM.",DPA; FPGA; Secure logic; Side-channel attacks; Whirlpool,Design; Field programmable gate arrays (FPGA); Function evaluation; Hamming distance; Hash functions; Reconfigurable hardware; Complementary circuits; Cryptographic functions; Differential power Analysis; DPA; Hardware implementations; New design; Power model; Secure logic; Security protocols; Side channel attack; Whirlpool; Hardware
Searching for transient pulses with the ETA radio telescope,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950591739&doi=10.1145%2f1462586.1462589&partnerID=40&md5=689217be6344b19fe448b3675802cacb,"Array-based, direct-sampling radio telescopes have computational and communication requirements unsuited to conventional computer and cluster architectures. Synchronization must be strictly maintained across a large number of parallel data streams, from A/D conversion, through operations such as beamforming, to dataset recording. FPGAs supporting multigigabit serial I/O are ideally suited to this application. We describe a recently-constructed radio telescope called ETA having all-sky observing capability for detecting low frequency pulses from transient events such as gamma ray bursts and primordial black hole explosions. Signals from 24 dipole antennas are processed by a tiered arrangement of 28 commercial FPGA boards and 4 PCs with FPGAbased data acquisition cards, connected with custom I/O adapter boards supporting InfiniBand and LVDS physical links. ETA is designed for unattended operation, allowing configuration and recording to be controlled remotely. © 2009 ACM 1936-7406/2009/01-ART20.",Direct sampling radio telescope array; FPGA cluster computing; RFI mitigation; Signal dedispersion,Analog to digital conversion; Cluster computing; Computer architecture; Gamma rays; Signal sampling; A/D conversion; All-sky; Cluster architecture; Conventional computers; Data acquisition cards; Data sets; Direct sampling; FPGA boards; Gamma ray bursts; Infiniband; Low frequency; Parallel data; Primordial black holes; RFI mitigation; Serial I/O; Signal dedispersion; Tiered arrangement; Transient events; Transient pulse; Radio telescopes
Static and dynamic memory footprint reduction for FPGA routing algorithms,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867425309&doi=10.1145%2f1462586.1462587&partnerID=40&md5=51cb28910c6272373cc3f81d05d9c86c,"This article presents techniques to reduce the static and dynamic memory requirements of routing algorithms that target field-programmable gate arrays. During routing, memory is required to store both architectural data and temporary routing data. The architectural data is static, and provides a representation of the physical routing resources and programmable connections on the device. We show that by taking advantage of the regularity in FPGAs, we can reduce the amount of information that must be explicitly represented, leading to significant memory savings. The temporary routing data is dynamic, and contains scoring parameters and traceback information for each routing resource in the FPGA. By studying the lifespan of the temporary routing data objects, we develop several memory management schemes to reduce this component. To make our proposals concrete, we applied them to the routing algorithm in VPR and empirically quantified the impact on runtime memory footprint, and place and route time. © 2009 ACM.",CAD; FPGA; Memory; Routing; Scalability,Computer aided design; Data storage equipment; Field programmable gate arrays (FPGA); Scalability; Amount of information; FPGA routing algorithms; Life span; Memory footprint; Memory management; Memory savings; Physical routing; Place and route; Programmable connections; Routing; Routing data; Routing resources; Runtimes; Static and dynamic; Traceback; Routing algorithms
RAT: RC amenability test for rapid performance prediction,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651395583&doi=10.1145%2f1462586.1462591&partnerID=40&md5=49e3e1da15a670fe98a39a8d75fc4090,"While the promise of achieving speedup and additional benefits such as high performance per watt with FPGAs continues to expand, chief among the challenges with the emerging paradigm of reconfigurable computing is the complexity in application design and implementation. Before a lengthy development effort is undertaken to map a given application to hardware, it is important that a high-level parallel algorithm crafted for that application first be analyzed relative to the target platform, so as to ascertain the likelihood of success in terms of potential speedup. This article presents the RC Amenability Test, or RAT, a methodology and model developed for this purpose, supporting rapid exploration and prediction of strategic design tradeoffs during the formulation stage of application development. © 2009 ACM.",Formulation methodology; FPGA; Performance prediction; Reconfigurable computing; Strategic design methodology,Design; Field programmable gate arrays (FPGA); Rats; Reconfigurable architectures; Application design; Application development; Formulation methodology; Performance prediction; Reconfigurable computing; Strategic design; Forecasting
Synthesis and optimization of 2D filter designs for heterogeneous FPGAs,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865041366&doi=10.1145%2f1462586.1462593&partnerID=40&md5=ec9330a838951527a9dedff7d3147dd3,"Many image processing applications require fast convolution of an image with one or more 2D filters. Field-Programmable Gate Arrays (FPGAs) are often used to achieve this goal due to their fine grain parallelism and reconfigurability. However, the heterogeneous nature of modern reconfigurable devices is not usually considered during design optimization. This article proposes an algorithm that explores the space of possible implementation architectures of 2D filters, targeting the minimization of the required area, by optimizing the usage of the different components in a heterogeneous device. This is achieved by exploring the heterogeneous nature of modern reconfigurable devices using a Singular Value Decomposition based algorithm, which provides an efficient mapping of filter's implementation requirements to the heterogeneous components of modern FPGAs. In the case of multiple 2D filters, the proposed algorithm also exploits any redundancy that exists within each filter and between different filters in the set, leading to designs with minimized area. Experiments with real filter sets from computer vision applications demonstrate an average of up to 38% reduction in the required area. © 2009 ACM.",2D filter design; FPGA; Reconfigurable logic; Singular Value Decomposition,Algorithms; Computer vision; Field programmable gate arrays (FPGA); Optimization; Singular value decomposition; 2D filters; Computer vision applications; Design optimization; Fast convolution; Filter sets; Fine grain parallelism; Heterogeneous component; Heterogeneous devices; Image processing applications; Implementation architecture; Reconfigurability; Reconfigurable devices; Reconfigurable logic; Singular-value decomposition-based algorithms; Reconfigurable hardware
FPGA acceleration of rankboost in web search engines,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951611018&doi=10.1145%2f1462586.1462588&partnerID=40&md5=bb55bb9738c504a245afdb1099dc18c5,"Search relevance is a key measurement for the usefulness of search engines. Shift of search relevance among search engines can easily change a search company's market cap by tens of billions of dollars. With the ever-increasing scale of the Web, machine learning technologies have become important tools to improve search relevance ranking. RankBoost is a promising algorithm in this area, but it is not widely used due to its long training time. To reduce the computation time for RankBoost, we designed a FPGA-based accelerator system and its upgraded version. The accelerator, plugged into a commodity PC, increased the training speed on MSN search engine data up to 1800x compared to the original software implementation on a server. The proposed accelerator has been successfully used by researchers in the search relevance ranking. © 2009 ACM.",FPGA; Hardware acceleration,Field programmable gate arrays (FPGA); Personal computers; Accelerator system; Computation time; Hardware acceleration; Machine learning technology; Rankboost; Relevance ranking; Software implementation; Training speed; Training time; Search engines
Compute bound and I/O bound cellular automata simulations on FPGA logic,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958729316&doi=10.1145%2f1462586.1462592&partnerID=40&md5=f2e600f80391bb10d09a23f69e8cd8d3,"FPGA-based computation engines have been used as Cellular Automata accelerators in the scientific community for some time now. With the recent availability of more advanced FPGA logic it becomes necessary to better understand the mapping of Cellular Automata to these systems. There are many trade-offs to consider when mapping a Cellular Automata algorithm from an abstract system to the physical implementation using FPGA logic. The trade-offs include both the available FPGA resources and the Cellular Automata algorithm's execution time. The most important aspect is to fully understand the behavior of the specified CA algorithm in terms of its execution times which are either compute bound or I/O bound. In this article, we present a methodology to categorize a specified CA algorithm as a compute bound or an I/O bound. We take the methodology further by presenting rigorous analysis for each of the two cases identifying the various parameters that control the mapping process and are defined both by the Cellular Automata algorithm and the given FPGA hardware specifications. This methodology helps to predict the performance of running Cellular Automata algorithms on specific FPGA hardware and to determine optimal values for the various parameters that control the mapping process. The model is validated for both compute and I/O bound two-dimensional Cellular Automata algorithms. We find that our model predictions are accurate within 7%. © 2009 ACM.",Cellular automata; FPGA-based hardware accelerator; High-performance computing; Lattice Boltzman simulations,Algorithms; Cellular automata; Commerce; Computer software selection and evaluation; Field programmable gate arrays (FPGA); Hardware; Mapping; Process control; Abstract systems; Cellular automata algorithms; Cellular automata simulations; Execution time; FPGA-based computations; Hardware accelerators; Hardware specifications; High-performance computing; Lattice Boltzman; Mapping process; Model prediction; Optimal values; Rigorous analysis; Scientific community; Two-dimensional cellular automata; Automata theory
Exploiting partial runtime reconfiguration for high-performance reconfigurable computing,2009,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951259591&doi=10.1145%2f1462586.1462590&partnerID=40&md5=66ebe7ee1937acf46a39d3fb7a2a8aef,"Runtime Reconfiguration (RTR) has been traditionally utilized as a means for exploiting the flexibility of High-Performance Reconfigurable Computers (HPRCs). However, the RTR feature comes with the cost of high configuration overhead which might negatively impact the overall performance. Currently, modern FPGAs have more advanced mechanisms for reducing the configuration overheads, particularly Partial Runtime Reconfiguration (PRTR). It has been perceived that PRTR on HPRC systems can be the trend for improving the performance. In this work, we will investigate the potential of PRTR on HPRC by formally analyzing the execution model and experimentally verifying our analytical findings by enabling PRTR for the first time, to the best of our knowledge, on one of the current HPRC systems, Cray XD1. Our approach is general and can be applied to any of the available HPRC systems. The paper will conclude with recommendations and conditions, based on our conceptual and experimental work, for the optimal utilization of PRTR as well as possible future usage in HPRC. © 2009 ACM.",Dynamic partial reconfiguration; Field programmable gate arrays (FPGA); High performance computing; Reconfigurable computing,Reconfigurable architectures; Dynamic partial reconfiguration; Execution model; High performance computing; Optimal utilization; Partial run-time reconfiguration; Possible futures; Reconfigurable computer; Reconfigurable computing; Run-time reconfiguration; Field programmable gate arrays (FPGA)
An Open-Source HyperTransport Core,2008,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009377317&doi=10.1145%2f1391732.1391734&partnerID=40&md5=5ec25d403b15428ffe5ed42709843cba,"This article presents the design of a generic HyperTransport (HT) core. HyperTransport is a packetbased interconnect technology for low-latency, high-bandwidth point-to-point connections. It is specially optimized to achieve a very low latency. The core has been verified in system using an FPGA. This exhaustive verification and the generic design allow the mapping to both ASICs and FPGAs. The implementation described in this work supports a 16-bit link width, as used by Opteron processors. On a Xilinx Virtex-4 FX60, the core supports a link frequency of 400 MHz DDR and offers a maximum bidirectional bandwidth of 3.2GB/s. The in-system verification has been performed using a custom FPGA board that has been plugged into a HyperTransport extension connector (HTX) of a standard Opteron-based motherboard. HTX slots in Opteron-based motherboards allow very high-bandwidth, low-latency communication, since the HTX device is directly connected to one of the HyperTransport links of the processor. Performance analysis shows a unidirectional payload bandwidth of 1.4GB/s and a read latency of 180 ns. The HT core in combination with the HTX board is an ideal base for prototyping systems and implementing FPGA coprocessors. The HT core is available as open source. © 2008, ACM. All rights reserved.",Design; FPGA; HTX; HyperTransport; prototyping; RTL; Verification,
Perfecto: A SystemC-Based Design-Space Exploration Framework for Dynamically Reconfigurable Architectures,2008,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011074950&doi=10.1145%2f1391732.1391737&partnerID=40&md5=fb6e3f9a796120b6a199d004b30e6f7c,"To cope with increasing demands for higher computational power and greater system flexibility, dynamically and partially reconfigurable logic has started to play an important role in embedded systems and systems-on-chip (SoC). However, when using traditional design methods and tools, it is difficult to estimate or analyze the performance impact of including such reconfigurable logic devices into a system design. In this work, we present a system-level framework, called Perfecto, which is able to perform rapid exploration of different reconfigurable design alternatives and to detect system performance bottlenecks. This framework is based on the popular IEEE standard system-level design language SystemC, which is supported by most EDA and ESL tools. Given an architecture model and an application model, Perfecto uses SystemC transaction-level models (TLMs) to simulate the system design alternatives automatically. Different hardware-software copartitioning, coscheduling, and placement algorithms can be embedded into the framework for analysis; thus, Perfecto can also be used to design the algorithms to be used in an operating system for reconfigurable systems. Applications to a simple illustration example and a network security system have shown how Perfecto helps a designer make intelligent partition decisions, optimize system performance, and evaluate task placements. © 2008, ACM. All rights reserved.",Design; design-space exploration; Experimentation; partitioning; Performance; performance evaluation; placement; Reconfigurable systems; scheduling; Verification,
Particle Graphics on Reconfigurable Hardware,2008,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951896536&doi=10.1145%2f1391732.1391735&partnerID=40&md5=79b7d7daab8688c3c89f224ae02853a1,"Particle graphics simulations are well suited for modeling complex phenomena such as water, cloth, explosions, fire, smoke, and clouds. They are normally realized in software as part of an interactive graphics application. The computational complexity of particle graphics simulations restricts the number of particles that can be updated in software at interactive frame rates. This article presents the design and implementation of a hardware particle graphics engine for accelerating real-time particle graphics simulations. We explore the design process, implementation issues, and limitations of using field-programmable gate arrays (FPGAs) for the acceleration of particle graphics. The FPGA particle engine processes million-particle systems at a rate from 47 to 112 million particles per second, which represents one to two orders of magnitude speedup over a 2.8 GHz CPU. Using three FPGAs, a maximum sustained performance of 112 million particles per second was achieved. © 2008, ACM. All rights reserved.",Algorithms; Design; FPGAs; particle systems; Performance; Reconfigurable computing; specialpurpose architectures,
Perturb+Mutate: Semisynthetic Circuit Generation for Incremental Placement and Routing,2008,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867115510&doi=10.1145%2f1391732.1391736&partnerID=40&md5=26fe84daa2348caeae93c805f5c7c75d,"CAD tool designers are always searching for more benchmark circuits to stress their software. In this article we present a heuristic method to generate benchmark circuits specially suited for incremental place-and-route tools. The method removes part of a real circuit and replaces it with an altered version of the same circuit to mimic an incremental design change. The alteration consists of two steps: mutate followed by perturb. The perturb step exactly preserves as many circuit characteristics as possible.While perturbing, reproduction of interconnect locality, a characteristic that is difficult to measure reliably or reproduce exactly, is controlled using a new technique, ancestor depth control (ADC). Perturbing with ADC produces circuits with postrouting properties that match the best techniques known to-date. The mutate step produces targetted mutations resulting in controlled changes to specific circuit properties (while keeping other properties constant). We demonstrate one targetted mutation heuristic, scale, to significantly change circuit size with little change to other circuit characteristics. The method is simple enough for inclusion in a CAD tool directly, and fast enough for use in on-the-fly benchmark generation. © 2008, ACM. All rights reserved.",Algorithms; Automated development tools; Design; design automation; Experimentation; graph algorithms; hardware-supporting software; Measurement; place and route; Reliability; testing; Verification,
On the Trade-Off between Power and Flexibility of FPGA Clock Networks,2008,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949399871&doi=10.1145%2f1391732.1391733&partnerID=40&md5=a6da62bee8ec8056f5bd309ed6e53640,"FPGA clock networks consume a significant amount of power, since they toggle every clock cycle and must be flexible enough to implement the clocks for a wide range of different applications. The efficiency of FPGA clock networks can be improved by reducing this flexibility; however, reducing the flexibility introduces stricter constraints during the clustering and placement stages of the FPGA CAD flow. These constraints can reduce the overall efficiency of the final implementation. This article examines the trade-off between the power consumption and flexibility of FPGA clock networks. Specifically, this article makes three contributions. First, it presents a new parameterized clocknetwork framework for describing and comparing FPGA clock networks. Second, it describes new clock-aware placement techniques that are needed to find a legal placement satisfying the constraints imposed by the clock network. Finally, it performs an empirical study to examine the trade-off between the power consumption of the clock network and the impact of the CAD constraints for a number of different clock networks with varying amounts of flexibility. The results show that the techniques used to produce a legal placement can have a significant influence on power and the ability of the placer to find a legal solution. On average, circuits placed using the most effective techniques dissipate 5% less overall energy and are significantly more likely to be legal than circuits placed using other techniques. Moreover, the results show that the architecture of the clock network is also important. On average, FPGAs with an efficient clock network are up to 14.6% more energy efficient compared to other FPGAs. © 2008, ACM. All rights reserved.",Algorithms; clock distribution networks; clock-aware placement; Design; Experimentation; FPGA; low-power design,
Merged Dictionary Code Compression for FPGA Implementation of Custom Microcoded PEs,2008,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-68549098848&doi=10.1145%2f1371579.1371583&partnerID=40&md5=3fe62c7497800234017eb5e02b39bd8c,"Horizontal Microcoded Architecture (HMA) is a paradigm for designing programmable highperformance processing elements (PEs). However, it suffers from large code size, which can be addressed by compression. In this article, we study the code size of one of the new HMA-based technologies called No-Instruction-Set Computer (NISC). We show that NISC code size can be several times larger than a typical RISC processor, and we propose several low-overhead dictionarybased code compression techniques to reduce its code size. Our compression algorithm leverages the knowledge of “don't care” values in the control words and can reduce the code size by 3.3 times, on average. Despite such good results, as shown in this article, these compression techniques lead to poor FPGA implementations because they require many on-chip RAMs. To address this issue, we introduce an FPGA-aware dictionary-based technique that uses the dual-port feature of on-chip RAMs to reduce the number of utilized block RAMs by half. Additionally, we propose cascading two-levels of dictionaries for code size and block RAM reduction of large programs. For an MP3 application, a merged, cascaded, three-dictionary implementation reduces the number of utilized block RAMs by 4.3 times (76%) compared to a NISC without compression. This corresponds to 20% additional savings over the best single level dictionary-based compression. © 2008, ACM. All rights reserved.",Algorithms; Design; dictionary based compression; Experimentation; FPGA; memory optimization; Microcoded architectures; no-instruction-set computer; Performance,
Special-Purpose Hardware for Solving the Elliptic Curve Discrete Logarithm Problem,2008,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859465967&doi=10.1145%2f1371579.1371580&partnerID=40&md5=efd71a4257dd04d4db58abb71bf80370,"The resistance against powerful index-calculus attacksmakes Elliptic Curve Cryptosystems (ECC) an interesting alternative to conventional asymmetric cryptosystems, like RSA. Operands in ECC require significantly less bits at the same level of security, resulting in a higher computational efficiency compared to RSA.With growing computational capabilities and continuous technological improvements over the years, however, the question of the security of ECC against attacks based on special-purpose hardware arises. In this context, recently emerged low-cost FPGAs demand for attention in the domain of hardware-based cryptanalysis: the extraordinary efficiency of modern programmable hardware devices allow for a low-budget implementation of hardware-based ECC attacks—without the requirement of the expensive development of ASICs. With focus on the aspect of cost-efficiency, this contribution presents and analyzes an FPGAbased architecture of an attack against ECC over prime fields. A multi-processing hardware architecture for Pollard's Rho method is described. We provide results on actually used key lengths of ECC (128 bits and above) and estimate the expected runtime for a successful attack. As a first result, currently used elliptic curve cryptosystems with a security of 160 bit and above turn out to be infeasible to break with available computational and financial resources. However, some of the security standards proposed by the Standards for Efficient Cryptography Group (SECG) become subject to attacks based on low-cost FPGAs. © 2008, ACM. All rights reserved.",Algorithms; cryptanalysis; Discrete logarithm; elliptic curve cryptosystem; Performance; Pollard's rho; Security,
Parametric Yield Modeling and Simulations of FPGA Circuits Considering Within-Die Delay Variations,2008,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016058885&doi=10.1145%2f1371579.1371582&partnerID=40&md5=5c3ad5feca0080734aee0b57f9e06a25,"Variations in the semiconductor fabrication process results in differences in parameters between transistors on the same die, a problem exacerbated by lithographic scaling. Field-Programmable Gate Arrays may be able to compensate for within-die delay variability, by judicious use of reconfigurability. This article presents two strategies for compensating within-die stochastic delay variability by using reconfiguration: reconfiguring the entire FPGA, and relocating subcircuits within an FPGA. Analytical models for the theoretical bounds on the achievable gains are derived for both strategies and compared to models for worst-case design as well as statistical static timing analysis (SSTA). All models are validated by comparison to circuit-level Monte Carlo simulations. It is demonstrated that significant improvements in circuit yield and timing are possible using SSTA alone, and these improvements can be enhanced by employing reconfigurationbased techniques. © 2008, ACM. All rights reserved.",Delay; FPGA; modeling; process variation; reconfiguration; statistical theory; Theory; within-die variability; yield,
Multivariate Gaussian Random Number Generation Targeting Reconfigurable Hardware,2008,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949360042&doi=10.1145%2f1371579.1371584&partnerID=40&md5=0d1f4a77c6ff6c6637f914576d4af96f,"The multivariate Gaussian distribution is often used to model correlations between stochastic time-series, and can be used to explore the effect of these correlations across N time-series in Monte-Carlo simulations. However, generating random correlated vectors is an O(N2) process, and quickly becomes a computational bottleneck in software simulations. This article presents an efficient method for generating vectors in parallel hardware, using N parallel pipelined components to generate a new vector every N cycles. This method maps well to the embedded block RAMs and multipliers in contemporary FPGAs, particularly as extensive testing shows that the limited bit-width arithmetic does not reduce the statistical quality of the generated vectors. An implementation of the architecture in the Virtex-4 architecture achieves a 500MHz clock-rate, and can support vector lengths up to 512 in the largest devices. The combination of a high clockrate and parallelism provides a significant performance advantage over conventional processors, with an xc4vsx55 device at 500MHz providing a 200 times speedup over an Opteron 2.6GHz using an AMD optimised BLAS package. In a case study in Delta-Gamma Value-at Risk, an RC2000 accelerator card using an xc4vsx55 at 400MHz is 26 times faster than a quad Opteron 2.6GHz SMP. © 2008, ACM. All rights reserved.",Algorithms; Design; Economics; FPGA; multivariate Gaussian distribution; Random numbers,
Mercury BLASTP: Accelerating Protein Sequence Alignment,2008,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874601312&doi=10.1145%2f1371579.1371581&partnerID=40&md5=19ec36d06b8c6380793fe68434c95b2f,"Large-scale protein sequence comparison is an important but compute-intensive task in molecular biology. BLASTP is the most popular tool for comparative analysis of protein sequences. In recent years, an exponential increase in the size of protein sequence databases has required either exponentially more running time or a cluster of machines to keep pace. To address this problem, we have designed and built a high-performance FPGA-accelerated version of BLASTP, Mercury BLASTP. In this article, we describe the architecture of the portions of the application that are accelerated in the FPGA, and we also describe the integration of these FPGA-accelerated portions with the existing BLASTP software. We have implemented Mercury BLASTP on a commodity workstation with two Xilinx Virtex-II 6000 FPGAs. We show that the new design runs 11-15 times faster than software BLASTP on a modern CPU while delivering close to 99% identical results. © 2008, ACM. All rights reserved.",Algorithms; Bioinformatics; biological sequence alignment; Design; Performance,
Guest Editorial: TRETS Special Edition on the 15th International Symposium on FPGAs,2008,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024256203&doi=10.1145%2f1331897.1341292&partnerID=40&md5=524a05b0e5a1266dba62438f91da4a0a,[No abstract available],,
Designing Efficient Input Interconnect Blocks for LUT Clusters Using Counting and Entropy,2008,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024265847&doi=10.1145%2f1331897.1331902&partnerID=40&md5=ed95da2bfb6fb1bb13596d914a7de821,"In a cluster-based FPGA, the interconnect from external routing tracks and cluster feedbacks to the LUT inputs consumes significant area, and no consensus has emerged among different implementations (e.g., 1-level or 2-level). In this paper, we model this interconnect as a unified input interconnect block (IIB). We identify three types of IIBs and develop general combinatorial techniques to count the number of distinct functional configurations for them. We use entropy, defined as the logarithm of this count, to estimate an IIB's routing flexibility. This enables us to analytically evaluate different IIBs without the customary time-consuming place and route experiments. We show that both depopulated 1-level IIBs and VPR-style 2-level IIBs achieve high routing flexibility but lack area efficiency. We propose a novel class of highly efficient, yet still simple, IIBs that use substantially fewer switches with only a small degradation in routing flexibility. Experimental results verify the routability of these IIBs, and confirm that entropy is a good predictor of routability. © 2008, ACM. All rights reserved.",cluster; counting; Design; entropy; FPGAs; interconnect; LUT; PLDs; Theory,
Suppression of Intrinsic Delay Variation in FPGAs using Multiple Configurations,2008,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896862219&doi=10.1145%2f1331897.1331899&partnerID=40&md5=1bd971617af26c5c3699cc8b32564911,"A new method for improving the timing yield of field-programmable gate array (FPGA) devices affected by intrinsic within-die variation is proposed. The timing variation is reduced by selecting an appropriate configuration for each chip from a set of independent configurations, the critical paths of which do not share the same circuit resources on the FPGA. In this article, the actual method used to generate independent multiple configurations by simply repeating the routing phase is shown, along with the results of Monte Carlo simulation with 10,000 samples. One simulation result showed that the standard deviations of maximum critical path delays are reduced by 28% and 49% for 10% and 30% Vth variations (σ/μ), respectively, with 10 independent configurations. Therefore, the proposed method is especially effective for larger Vth variation and is expected to be useful for suppressing the performance variation of FPGAs due to the future increase of parameter variation. Another simulation result showed that the effectiveness of the proposed technique was saturated at the use of 10 or more configurations because of the degradation of the quality of the configurations. Therefore, the use of 10 or fewer configurations is reasonable. © 2008, ACM. All rights reserved.",configuration; FPGA; Reliability; timing yield; within-die variation,
A Desktop Computer with a Reconfigurable Pentium,2008,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955680177&doi=10.1145%2f1331897.1331901&partnerID=40&md5=667c5ce4965daee826379c86a257b9ed,"Advancements in reconfigurable technologies, specifically FPGAs, have yielded faster, more powerefficient reconfigurable devices with enormous capacities. In our work, we provide testament to the impressive capacity of recent FPGAs by hosting a complete Pentium R in a single FPGA chip. In addition we demonstrate how FPGAs can be used for microprocessor design space exploration while overcoming the tension between simulation speed, model accuracy, and model completeness found in traditional software simulator environments. Specifically, we perform preliminary experimentation/prototyping with an original Socket 7 based desktop processor system with typical hardware peripherals running modern operating systems such as Fedora Core 4 and Windows XP; however we have inserted a Xilinx Virtex-4 in place of the processor that should sit in the motherboard and have used the Virtex-4 to host a complete version of the Pentium R microprocessor (which consumes less than half its resources). We can therefore apply architectural changes to the processor and evaluate their effects on the complete desktop system. We use this FPGA-based emulation system to conduct preliminary architectural experiments including growing the branch target buffer and the level 1 caches. In addition, we experimented with interfacing hardware accelerators such as DES and AES engines which resulted in a 27x speedup. © 2008, ACM. All rights reserved.",accelerator; architecture; Design; emulator; exploration; FPGA; Measurement; model; operating system; Pentium R; Performance; processor; reconfigurable; simulator,
Introduction,2008,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024249355&doi=10.1145%2f1331897.1331898&partnerID=40&md5=87f950790f0dee495b3127d00a123e49,[No abstract available],,
Statistical Analysis and Process Variation-Aware Routing and Skew Assignment for FPGAs,2008,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013813444&doi=10.1145%2f1331897.1331900&partnerID=40&md5=ec6204a859ecb84cd3426896b77ea6c0,"With constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. The FPGA community has only recently started focusing on the effects of variations. In this work we present a statistical analysis to compare the effects of variations on designs mapped to FPGAs and ASICs. We also present CAD and architecture techniques to mitigate the impact of variations. First we present a variation-aware router that optimizes statistical criticality. We then propose a modification to the clock network to deliver programmable skews to different flip-flops. Finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12% improvement in timing yield. When the desired timing yield is set to 99%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10% over a purely deterministic approach. © 2008, ACM. All rights reserved.",Algorithms; Design; Performance; routing; skew assignment; Statistical timing analysis,
A Synthesizable Datapath-Oriented Embedded FPGA Fabric for Silicon Debug Applications,2008,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886144313&doi=10.1145%2f1331897.1331903&partnerID=40&md5=e4c05fd8d9537342f82fca708107cda8,"We present an architecture for a synthesizable datapath-oriented FPGA core that can be used to provide post-fabrication flexibility to an SoC. Our architecture is optimized for bus-based operations and employs a directional routing architecture, which allows it to be synthesized using standard ASIC design tools and flows. The primary motivation for this architecture is to provide an efficient mechanism to support on-chip debugging. The fabric can also be used to implement other datapath-oriented circuits such as those needed in signal processing and computation-intensive applications. We evaluate our architecture using a set of benchmark circuits and compare it to previous fabrics in terms of area, speed, and power. © 2008, ACM. All rights reserved.",Design; Field programmable gate array; integrated circuit; silicon debug; system-on-chip; Verification,
Streaming Overlay Architecture for Lightweight LSTM Computation on FPGA SoCs,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141857990&doi=10.1145%2f3543069&partnerID=40&md5=d316d78226ff2c32a27f1051fd03a946,"Long-Short Term Memory (LSTM) networks, and Recurrent Neural Networks (RNNs) in general, have demonstrated their suitability in many time series data applications, especially in Natural Language Processing (NLP). Computationally, LSTMs introduce dependencies on previous outputs in each layer that complicate their computation and the design of custom computing architectures, compared to traditional feed-forward networks. Most neural network acceleration work has focused on optimising the core matrix-vector operations on highly capable FPGAs in server environments. Research that considers the embedded domain has often been unsuitable for streaming inference, relying heavily on batch processing to achieve high throughput. Moreover, many existing accelerator architectures have not focused on fully exploiting the underlying FPGA architecture, resulting in designs that achieve lower operating frequencies than the theoretical maximum. This paper presents a flexible overlay architecture for LSTMs on FPGA SoCs that is built around a streaming dataflow arrangement, uses DSP block capabilities directly, and is tailored to keep parameters within the architecture while moving input data serially to mitigate external memory access overheads. The architecture is designed as an overlay that can be configured to implement alternative models or update model parameters at runtime. It achieves higher operating frequency and demonstrates higher performance than other lightweight LSTM accelerators, as demonstrated in an FPGA SoC implementation.  © 2022 Copyright held by the owner/author(s).",LSTM; machine learning; neural networks; overlay,Batch data processing; Learning algorithms; Long short-term memory; Memory architecture; Multilayer neural networks; Network architecture; System-on-chip; Data application; Language processing; Machine-learning; Memory computations; Memory network; Natural languages; Neural-networks; Overlay; Overlay architecture; Time-series data; Field programmable gate arrays (FPGA)
Advantages of a Statistical Estimation Approach for Clock Frequency Estimation of Heterogeneous and Irregular CGRAs,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149177602&doi=10.1145%2f3531062&partnerID=40&md5=d9d509f14a646b5a6d0f0f0b42a72940,"Estimating the maximum clock frequency of homogeneous Coarse Grained Reconfigurable Arrays/Architectures (CGRAs) with an arbitrary number of Processing Elements (PE) is difficult. Clock frequency estimation of highly heterogeneous CGRAs takes additional factors into account, thus is even more difficult. Main challenges are the heterogeneous set of operators for each Processing Element (PE) and the irregular interconnect (connecting a CGRA's PEs). Multiple estimation approaches could be reasonable. We propose an optimized statistical estimator, which is based on our prior work. We demonstrate its superiority to state-of-the-art neural networks in terms of accuracy and robustness, especially in situations with a sparse set of training data.  © 2022 Association for Computing Machinery.",automation; Coarse grained reconfigurable architecture; design space exploration; heterogeneity; machine learning,Automation; Clocks; Reconfigurable architectures; Array architecture; Clock frequency; Coarse grained reconfigurable architecture; Coarse-grained reconfigurable arrays; Design space exploration; Estimation approaches; Heterogeneity; Machine-learning; Processing elements; Statistical estimation; Frequency estimation
Remarn: A Reconfigurable Multi-threaded Multi-core Accelerator for Recurrent Neural Networks,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149167755&doi=10.1145%2f3534969&partnerID=40&md5=e855d344ef17d74f878f5a9d49b80a6b,"This work introduces Remarn, a reconfigurable multi-threaded multi-core accelerator supporting both spatial and temporal co-execution of Recurrent Neural Network (RNN) inferences. It increases processing capabilities and quality of service of cloud-based neural processing units (NPUs) by improving their hardware utilization and by reducing design latency, with two innovations. First, a custom coarse-grained multi-threaded RNN/Long Short-Term Memory (LSTM) hardware architecture, switching tasks among threads when RNN computational engines meet data hazards. Second, the partitioning of this hardware architecture into multiple full-fledged sub-accelerator cores, enabling spatially co-execution of multiple RNN/LSTM inferences. These innovations improve the exploitation of the available parallelism to increase runtime hardware utilization and boost design throughput. Evaluation results show that a dual-threaded quad-core Remarn NPU achieves 2.91 times higher performance while only occupying 5.0% more area than a single-threaded one on a Stratix 10 FPGA. When compared with a Tesla V100 GPU implementation, our design achieves 6.5 times better performance and 15.6 times higher power efficiency, showing that our approach contributes to high performance and energy-efficient FPGA-based multi-RNN inference designs for datacenters.  © 2022 Association for Computing Machinery.",Accelerator architecture; multi-tenant execution; recurrent neural networks,Energy efficiency; Integrated circuit design; Long short-term memory; Network architecture; Quality of service; Reconfigurable architectures; Reconfigurable hardware; Accelerator architectures; Multi tenants; Multi-cores; Multi-tenant execution; Multithreaded; Network inference; Neural-processing; Performance; Processing units; Reconfigurable; Field programmable gate arrays (FPGA)
"A High-Throughput, Resource-Efficient Implementation of the RoCEv2 Remote DMA Protocol and its Application",2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149185887&doi=10.1145%2f3543176&partnerID=40&md5=d4f995ce3b04a08ea9fcb5396423a7b4,"The use of application-specific accelerators in data centers has been the state of the art for at least a decade, starting with the availability of General Purpose GPUs achieving higher performance either overall or per watt. In most cases, these accelerators are coupled via PCIe interfaces to the corresponding hosts, which leads to disadvantages in interoperability, scalability and power consumption. As a viable alternative to PCIe-attached FPGA accelerators this paper proposes standalone FPGAs as Network-attached Accelerators (NAAs). To enable reliable communication for decoupled FPGAs we present an RDMA over Converged Ethernet v2 (RoCEv2) communication stack for high-speed and low-latency data transfer integrated into a hardware framework.For NAAs to be used instead of PCIe coupled FPGAs the framework must provide similar throughput and latency with low resource usage. We show that our RoCEv2 stack is capable of achieving 100 Gb/s throughput with latencies of less than 4μs while using about 10% of the available resources on a mid-range FPGA. To evaluate the energy efficiency of our NAA architecture, we built a demonstrator with 8 NAAs for machine learning based image classification. Based on our measurements, network-attached FPGAs are a great alternative to the more energy-demanding PCIe-attached FPGA accelerators.  © 2022 Copyright held by the owner/author(s).",data center; FPGA; high-performance computing; machine learning; Network-attached Accelerator; RDMA; RoCEv2,Data transfer; Energy efficiency; Machine learning; Program processors; Datacenter; Efficient implementation; High-performance computing; High-throughput; Machine-learning; Network-attached accelerator; Performance computing; RDMA; RDMA over converged ethernet v2; Resource-efficient; Field programmable gate arrays (FPGA)
A Scalable Systolic Accelerator for Estimation of the Spectral Correlation Density Function and Its FPGA Implementation,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149187274&doi=10.1145%2f3546181&partnerID=40&md5=0be16dffb3287950196db2a0ffe57f9e,"The spectral correlation density (SCD) function is the time-averaged correlation of two spectral components used for analyzing periodic signals with time-varying spectral content. Although the analysis is extremely powerful, it has not been widely adopted in real-time applications due to its high computational complexity. In this article, we present an efficient FPGA implementation of the FFT accumulation method (FAM) for estimating the SCD function and its alpha profile. The implementation uses a linear systolic array with a bi-directional datapath consisting of DSP-based processing elements (PEs) with a dedicated instruction schedule, achieving a PE utilization of 88.2%.The 128-PE implementation achieves a clock frequency in excess of 530 MHz and consumes 151K LUTs, 151K FFs, 264 BRAMs, 4 URAMs, and 1,054 DSPs, which is less than 36% of the logic fabric on a Zynq UltraScale+ XCZU28DR-2FFVG1517E RFSoC device. It has a modest 12.5W power consumption and an energy efficiency of 4,832 MOPS/W, which is 20.6× better than the published state-of-the-art GPU implementation. In terms of throughput, it achieves 15,340 windows/s (15,340 windows/s × 2,048 samples/window = 31.4 MS/s), which is a 4.65× improvement compared to the above-mentioned GPU implementation and 807× compared to an existing hybrid FPGA-GPU implementation.  © 2022 Association for Computing Machinery.",FFT accumulation method; FPGA; spectral correlation density; systolic array,Energy efficiency; Fast Fourier transforms; Graphics processing unit; Systolic arrays; Correlation density function; FFT accumulation method; FPGA implementations; FPGAs implementation; GPU implementation; Periodic signal; Processing elements; Spectral components; Spectral correlation density; Time-averaged; Field programmable gate arrays (FPGA)
An Optimized GIB Routing Architecture with Bent Wires for FPGA,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149174393&doi=10.1145%2f3519599&partnerID=40&md5=a160d869626e9c9b7b42099ab63eae82,"Field-programmable gate arrays (FGPAs) are widely used because of the superiority in flexibility and lower non-recurring engineering cost. How to optimize the routing architecture is a key problem for FPGA architects because it has a large impact on FPGA area, delay, and routability. In academia, the routing architecture is mainly based on the connection blocks (CBs) and switch blocks (SBs), whereas most research has focused on SB architectures, such as Wilton, Universal, and Disjoint SB patterns. In this article, we propose a novel unidirectional routing architecture - general interconnection block (GIB) - to improve FPGA performance. With the GIB architecture, logic block (LB) pins can directly connect with the adjacent GIBs without programmable switches. Inside a GIB, LB pins can connect to the routing channel tracks on the four sides of a GIB. In particular, the logic pins from different neighboring LBs that connect to the same GIB can connect with each other with only one programmable switch. In addition, we enhance VTR to support the GIB with bent wires and develop a searching framework based on the simulated annealing algorithm to search for a near-optimal distribution of wire types. We evaluate the GIB architecture on VTR 8 with the provided benchmark circuits. The experimental results show that the GIB architecture with length-4 wires can achieve 9.5% improvement on the critical path delay and 11.1% improvement on the area-delay product compared to the VTR CB-SB architecture with length-4 wires. After exploring mixed wire types, the optimized GIB architecture can further improve the delay by 16.4% and area-delay product by 17.1% compared to the CB-SB architecture with length-4 wires.  © 2022 Association for Computing Machinery.",connection block; Routing architecture; switch block,Computer aided design; Computer circuits; Cost engineering; Field programmable gate arrays (FPGA); Simulated annealing; Connection block; Delay product; Engineering costs; Field programmables; Logic blocks; Non recurring engineering; Programmable gate array; Programmable switches; Routing architecture; Switch blocks; Wire
LW-GCN: A Lightweight FPGA-based Graph Convolutional Network Accelerator,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149000115&doi=10.1145%2f3550075&partnerID=40&md5=1d5cd0c0618dffb0beb3bd6cbca5cce0,"Graph convolutional networks (GCNs) have been introduced to effectively process non-Euclidean graph data. However, GCNs incur large amounts of irregularity in computation and memory access, which prevents efficient use of traditional neural network accelerators. Moreover, existing dedicated GCN accelerators demand high memory volumes and are difficult to implement onto resource limited edge devices.In this work, we propose LW-GCN, a lightweight FPGA-based accelerator with a software-hardware co-designed process to tackle irregularity in computation and memory access in GCN inference. LW-GCN decomposes the main GCN operations into Sparse Matrix-Matrix Multiplication (SpMM) and Matrix-Matrix Multiplication (MM). We propose a novel compression format to balance workload across PEs and prevent data hazards. Moreover, we apply data quantization and workload tiling, and map both SpMM and MM of GCN inference onto a uniform architecture on resource limited hardware. Evaluation on GCN and GraphSAGE are performed on Xilinx Kintex-7 FPGA with three popular datasets. Compared to existing CPU, GPU, and state-of-the-art FPGA-based accelerator, LW-GCN reduces latency by up to 60×, 12×, and 1.7× and increases power efficiency by up to 912×, 511×, and 3.87×, respectively. Furthermore, compared with NVIDIA's latest edge GPU Jetson Xavier NX, LW-GCN achieves speedup and energy savings of 32× and 84×, respectively.  © 2022 Association for Computing Machinery.",FPGA-based accelerator; Graph convolutional network (GCN); sparse-dense matrix multiplication,Convolution; Energy efficiency; Matrix algebra; Memory architecture; Convolutional networks; Dense matrix; FPGA-based accelerator; Graph convolutional network; MAtrix multiplication; Memory access; Network inference; Non-Euclidean; Sparse matrix-matrix multiplications; Sparse-dense matrix multiplication; Field programmable gate arrays (FPGA)
Cross-VM Covert- and Side-Channel Attacks in Cloud FPGAs,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143251921&doi=10.1145%2f3534972&partnerID=40&md5=d86b73f1734d8e73adc5f336cbf102e4,"The availability of FPGAs in cloud data centers offers rapid, on-demand access to reconfigurable hardware compute resources that users can adapt to their own needs. However, the low-level access to the FPGA hardware and associated resources such as the PCIe bus, SSD drives, or DRAM modules also opens up threats of malicious attackers uploading designs that are able to infer information about other users or about the cloud infrastructure itself. In particular, this work presents a new, fast PCIe-contention-based channel that is able to transmit data between FPGA-accelerated virtual machines (VMs) by modulating the PCIe bus usage. This channel further works with different operating systems and achieves bandwidths reaching 20 kbps with 99% accuracy. This is the first cross-FPGA covert channel demonstrated on commercial clouds and has a bandwidth which is over 2000× larger than prior voltage- or temperature-based cross-board attacks. This article further demonstrates that the PCIe receivers are able to not just receive covert transmissions, but can also perform fine-grained monitoring of the PCIe bus, including detecting when co-located VMs are initialized, even prior to their associated FPGAs being used. Moreover, the proposed mechanism can be used to infer the activities of other users, or even slow down the programming of the co-located FPGAs as well as other data transfers between the host and the FPGA. Beyond leaking information across different virtual machines, the ability to monitor the PCIe bandwidth over hours or days can be used to estimate the data center utilization and map the behavior of the other users. The article also introduces further novel threats in FPGA-accelerated instances, including contention due to network traffic, contention due to shared NVMe SSDs, as well as thermal monitoring to identify FPGA co-location using the DRAM modules attached to the FPGA boards. This is the first work to demonstrate that it is possible to break the separation of privilege in FPGA-accelerated cloud environments, and highlights that defenses for public clouds using FPGAs need to consider PCIe, SSD, and DRAM resources as part of the attack surface that should be protected.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Cloud FPGAs; covert channels; FPGA security; information leakage; interference attacks; PCIe contention; side channels,Bandwidth; Data transfer; Digital storage; Network security; Reconfigurable hardware; Side channel attack; Virtual machine; Cloud data centers; Cloud FPGA; Co-located; Covert channels; FPGA security; Information leakage; Interference attack; PCIe contention; Side-channel; Side-channel attacks; Field programmable gate arrays (FPGA)
Data and Computation Reuse in CNNs Using Memristor TCAMs,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149185820&doi=10.1145%2f3549536&partnerID=40&md5=c9bb705fa6126c130c96be01383fc978,"Exploiting computational and data reuse in CNNs is crucial for the successful design of resource-constrained platforms. In image recognition applications, high levels of input locality and redundancy present in CNNs have become the golden goose for skipping costly arithmetic operations. One promising technique for this consists in storing function responses of some input patterns into offline lookup tables and replacing online computation with search operations, which are highly efficient when implemented by emerging non-volatile memory technologies. In this work, we rethink both algorithm and architecture for exploiting locality and reuse opportunities by replacing entire convolutions with searches on Content-addressable Memories. By previously calculating convolution results and building compact lookup tables with our novel clustering algorithm, one can evaluate activations at constant time complexity, also requiring a single read operation of the current input tensor. Then, we devise a reconfigurable array of processing elements based on memristive Ternary Content-addressable Memories to efficiently implement the algorithmic solution and meet the flexibility requirements of several CNN architectures. Results show that our design reduces the number of multiplications and memory accesses proportionally to the number of convolutional layer channels. The average performance is 1,172 and 82 FPS for AlexNet and VGG-16 models, thus outperforming state-of-the-art works by 13×.  © 2022 Association for Computing Machinery.",in-memory processing; Neural network accelerators; reconfigurable computing; Ternary Content-addressable Memories (TCAM),Associative processing; Associative storage; Clustering algorithms; Image recognition; Logic gates; Memory architecture; Network architecture; Reconfigurable architectures; Table lookup; Ternary content adressable memory; Computation reuse; Data reuse; In-memory processing; Memristor; Neural network accelerator; Neural-networks; Reconfigurable computing; Reconfigurable- computing; Ternary content addressable memory; Ternary content-addressable memory; Convolution
Voltage Sensor Implementations for Remote Power Attacks on FPGAs,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147130308&doi=10.1145%2f3555048&partnerID=40&md5=57bf6969adb0f5ca010140a3b239485e,"This article presents a study of two types of on-chip FPGA voltage sensors based on ring oscillators (ROs) and time-to-digital converter (TDCs), respectively. It has previously been shown that these sensors are often used to extract side-channel information from FPGAs without physical access. The performance of the sensors is evaluated in the presence of circuits that deliberately waste power, resulting in localized voltage drops. The effects of FPGA power supply features and sensor sensitivity in detecting voltage drops in an FPGA power distribution network (PDN) are evaluated for Xilinx Artix-7, Zynq 7000, and Zynq UltraScale+ FPGAs. We show that both sensor types are able to detect supply voltage drops, and that their measurements are consistent with each other. Our findings show that TDC-based sensors are more sensitive and can detect voltage drops that are shorter in duration, while RO sensors are easier to implement because calibration is not required. Furthermore, we present a new time-interleaved TDC design that sweeps the sensor phase. The new sensor generates data that can reconstruct voltage transients on the order of tens of picoseconds.  © 2022 Association for Computing Machinery.",Multi-tenant FPGA; on-chip voltage sensor; ring oscillator; side-channel attacks; time-to-digital converter,Electric power distribution; Frequency converters; Side channel attack; Signal processing; Digital converters; Multi tenants; Multi-tenant FPGA; On chips; On-chip voltage sensor; Ring oscillator; Side-channel attacks; Time-to-digital; Time-to-digital converter; Voltage sensor; Field programmable gate arrays (FPGA)
FPGA-based Acceleration of Time Series Similarity Prediction: From Cloud to Edge,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149040110&doi=10.1145%2f3555810&partnerID=40&md5=aee299d0d59e46c3e2e16d6f2c72a26b,"With the proliferation of low-cost sensors and the Internet of Things, the rate of producing data far exceeds the compute and storage capabilities of today's infrastructure. Much of this data takes the form of time series, and in response, there has been increasing interest in the creation of time series archives in the past decade, along with the development and deployment of novel analysis methods to process the data. The general strategy has been to apply a plurality of similarity search mechanisms to various subsets and subsequences of time series data to identify repeated patterns and anomalies; however, the computational demands of these approaches renders them incompatible with today's power-constrained embedded CPUs.To address this challenge, we present FA-LAMP, an FPGA-Accelerated implementation of the Learned Approximate Matrix Profile (LAMP) algorithm, which predicts the correlation between streaming data sampled in real-Time and a representative time series dataset used for training. FA-LAMP lends itself as a real-Time solution for time series analysis problems such as classification. We present the implementation of FA-LAMP on both edge-and cloud-based prototypes. On the edge devices, FA-LAMP integrates accelerated computation as close as possible to IoT sensors, thereby eliminating the need to transmit and store data in the cloud for posterior analysis. On the cloud-based accelerators, FA-LAMP can execute multiple LAMP models on the same board, allowing simultaneous processing of incoming data from multiple data sources across a network.LAMP employs a Convolutional Neural Network (CNN) for prediction. This work investigates the challenges and limitations of deploying CNNs on FPGAs using the Xilinx Deep Learning Processor Unit (DPU) and the Vitis AI development environment. We expose several technical limitations of the DPU, while providing a mechanism to overcome them by attaching custom IP block accelerators to the architecture. We evaluate FA-LAMP using a low-cost Xilinx Ultra96-V2 FPGA as well as a cloud-based Xilinx Alveo U280 accelerator card and measure their performance against a prototypical LAMP deployment running on a Raspberry Pi 3, an Edge TPU, a GPU, a desktop CPU, and a server-class CPU. In the edge scenario, the Ultra96-V2 FPGA improved performance and energy consumption compared to the Raspberry Pi; in the cloud scenario, the server CPU and GPU outperformed the Alveo U280 accelerator card, while the desktop CPU achieved comparable performance; however, the Alveo card offered an order of magnitude lower energy consumption compared to the other four platforms. Our implementation is publicly available at https://github.com/aminiok1/lamp-Alveo.  © 2022 Association for Computing Machinery.",Field-programmable gate array (FPGA); Matrix Profile; Time series,Computing power; Convolutional neural networks; Costs; Data handling; Deep learning; Digital storage; Energy utilization; Field programmable gate arrays (FPGA); Graphics processing unit; Green computing; Internet of things; Program processors; Time series analysis; Cloud-based; Field programmables; Field-programmable gate array; Low-cost sensors; matrix; Matrix profile; Performance; Programmable gate array; Storage capability; Times series; Matrix algebra
"Near-memory Computing on FPGAs with 3D-stacked Memories: Applications, Architectures, and Optimizations",2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148485904&doi=10.1145%2f3547658&partnerID=40&md5=eda1223e7018d9133fdf4363fa7783ce,"The near-memory computing (NMC) paradigm has transpired as a promising method for overcoming the memory wall challenges of future computing architectures. Modern systems integrating 3D-stacked DRAM memory can be leveraged to prevent unnecessary data movement between the main memory and the CPU. FPGA vendors have started introducing 3D memories to their products in an effort to remain competitive on bandwidth requirements of modern memory-intensive applications. Recent NMC proposals target various types of data processing workloads such as graph processing, MapReduce, sorting, machine learning, and database analytics.In this article, we conduct a literature survey on previous proposals of NMC systems on FPGAs integrated with 3D memories. By leveraging the high bandwidth offered from such memories together with specifically designed hardware, FPGA architectures have become a competitor to GPU solutions in terms of speed and energy efficiency. Various FPGA-based NMC designs have been proposed with software and hardware optimization methods to achieve high performance and energy efficiency. Our review investigates various aspects of NMC designs such as platforms, architectures, workloads, and tools. We identify the key challenges and open issues with future research directions.  © 2022 Association for Computing Machinery.",3D stacking; FPGA architectures; high-bandwidth memory; Near-memory computing,Bandwidth; Data handling; Dynamic random access storage; Energy efficiency; Memory architecture; Three dimensional integrated circuits; 3D memory; 3D stacking; 3D-stacked memory; Bandwidth memory; FPGA architectures; High bandwidth; High-bandwidth memory; Memory applications; Memory optimization; Near-memory computing; Field programmable gate arrays (FPGA)
Efficient Design of Low Bitwidth Convolutional Neural Networks on FPGA with Optimized Dot Product Units,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149177613&doi=10.1145%2f3546182&partnerID=40&md5=9aaa3d3fec56cae4187f1cf01a2f7d31,"Designing hardware accelerators to run the inference of convolutional neural networks (CNN) is under intensive research. Several different architectures have been proposed along with hardware-oriented optimizations of the neural network models. One of the most used optimizations is quantization since it reduces the memory requirements to store weights and layer maps, the memory bandwidth requirements and the hardware complexity. As a consequence, the inference throughput has improved and the computing cost has been reduced, allowing inference to be executed on embedded devices. In this work, we propose highly efficient dot-product arithmetic units for ternary and non-ternary convolutional neural networks on FPGA. The non-ternary dot-product unit uses a fused multiply-add that avoids expensive adder trees, while the ternary dot-product unit uses a dual product unit followed by an optimized conditional adder tree structure. In both cases, designs with and without embedded DSP are considered. The solution is configurable and can be adapted to the available number of resources of the FPGA to achieve the best efficiency. A CNN architecture was developed and characterized using the proposed dot product units. The results show a performance improvement of 1.8 × with a 2× more area efficiency for low bit-width quantizations when compared to previous works running large CNNs in FPGA.  © 2022 Association for Computing Machinery.",convolutional neural network; dot-product; FPGA; Fused multiply-add; hardware acceleration,Adders; Convolution; Convolutional neural networks; Efficiency; Integrated circuit design; Memory architecture; Network architecture; Trees (mathematics); Adder tree; Bit-Width; Convolutional neural network; Dot-product; Efficient designs; Fused multiply-add; Hardware acceleration; Optimisations; Product-unit; Quantisation; Field programmable gate arrays (FPGA)
Introduction to Special Section on FPGA 2021,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146475047&doi=10.1145%2f3536335&partnerID=40&md5=4cd85e9d4574619a7c915095695daada,[No abstract available],,
Introduction to the Special Section on FPL 2020,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146482896&doi=10.1145%2f3536336&partnerID=40&md5=c0b5805c1840802d1ac254593ff33590,[No abstract available],,
SyncNN: Evaluating and Accelerating Spiking Neural Networks on FPGAs,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146377851&doi=10.1145%2f3514253&partnerID=40&md5=534155d7cae68167498ba5343599cd28,"Compared to conventional artificial neural networks, spiking neural networks (SNNs) are more biologically plausible and require less computation due to their event-driven nature of spiking neurons. However, the default asynchronous execution of SNNs also poses great challenges to accelerate their performance on FPGAs.In this work, we present a novel synchronous approach for rate-encoding-based SNNs, which is more hardware friendly than conventional asynchronous approaches. We first quantitatively evaluate and mathematically prove that the proposed synchronous approach and asynchronous implementation alternatives of rate-encoding-based SNNs are similar in terms of inference accuracy, and we highlight the computational performance advantage of using SyncNN over an asynchronous approach. We also design and implement the SyncNN framework to accelerate SNNs on Xilinx ARM-FPGA SoCs in a synchronous fashion. To improve the computation and memory access efficiency, we first quantize the network weights to 16-bit, 8-bit, and 4-bit fixed-point values with the SNN-friendly quantization techniques. Moreover, we encode only the activated neurons by recording their positions and corresponding number of spikes to fully utilize the event-driven characteristics of SNNs, instead of using the common binary encoding (i.e., 1 for a spike and 0 for no spike).For the encoded neurons that have dynamic and irregular access patterns, we design parameterized compute engines to accelerate their performance on the FPGA, where we explore various parallelization strategies and memory access optimizations. Our experimental results on multiple Xilinx ARM-FPGA SoC boards demonstrate that our SyncNN is scalable to run multiple networks, such as LeNet, Network in Network, and VGG, on various datasets such as MNIST, SVHN, and CIFAR-10. SyncNN not only achieves competitive accuracy (99.6%) but also achieves state-of-the-art performance (13,086 frames per second) for the MNIST dataset. Finally, we compare the performance of SyncNN with conventional CNNs using the Vitis AI and find that SyncNN can achieve similar accuracy and better performance compared to Vitis AI for image classification using small networks.  © 2022 Association for Computing Machinery.",deep learning; FPGA; hardware acceleration; Spiking neural network; synchronous execution,ARM processors; Classification (of information); Deep learning; Encoding (symbols); Field programmable gate arrays (FPGA); Integrated circuit design; Memory architecture; Neurons; Signal encoding; System-on-chip; Asynchronous executions; Deep learning; Encodings; Event-driven; Hardware acceleration; Neural-networks; Performance; Spiking neural network; Spiking Neurones; Synchronoi execution; Neural networks
Exploiting HBM on FPGAs for Data Processing,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146431272&doi=10.1145%2f3491238&partnerID=40&md5=a359a2533b1656ac81741d8114ab6dbe,"Field Programmable Gate Arrays (FPGAs) are increasingly being used in data centers and the cloud due to their potential to accelerate certain workloads as well as for their architectural flexibility, since they can be used as accelerators, smart-NICs, or stand-alone processors. To meet the challenges posed by these new use cases, FPGAs are quickly evolving in terms of their capabilities and organization. The utilization of High Bandwidth Memory (HBM) in FPGA devices is one recent example of such a trend. In this article, we study the potential of FPGAs equipped with HBM from a data analytics perspective. We consider three workloads common in analytics-oriented databases and implement them on an FPGA showing in which cases they benefit from HBM: range selection, hash join, and stochastic gradient descent for linear model training. We integrate our designs into a columnar database (MonetDB) and show the trade-offs arising from the integration related to data movement and partitioning. We consider two possible configurations of the HBM, using a single and a dual clock version design. With the right design, FPGA+HBM-based solutions are able to surpass the highest performance provided by either a two-socket POWER91 system or a 14-core Xeon2 E5 by up to 5.9× (range selection), 18.3× (hash join), and 6.1× (SGD).  © 2022 Copyright held by the owner/author(s).",advanced analytics; database; FPGA; High bandwidth memory (HBM),Bandwidth; Data handling; Database systems; Economic and social effects; Field programmable gate arrays (FPGA); Gradient methods; Integrated circuit design; Stochastic models; Stochastic systems; Bandwidth memory; Datacenter; Field programmable gate array; Field programmables; Hash join; High bandwidth; High bandwidth memory; In-field; Programmable gate array; Stand-alone processors; Data Analytics
ThunderGP: Resource-Efficient Graph Processing Framework on FPGAs with HLS,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143818494&doi=10.1145%2f3517141&partnerID=40&md5=db9d3f669796431efbad7b2a580028c2,"FPGA has been an emerging computing infrastructure in datacenters benefiting from fine-grained parallelism, energy efficiency, and reconfigurability. Meanwhile, graph processing has attracted tremendous interest in data analytics, and its performance is in increasing demand with the rapid growth of data. Many works have been proposed to tackle the challenges of designing efficient FPGA-based accelerators for graph processing. However, the largely overlooked programmability still requires hardware design expertise and sizable development efforts from developers. ThunderGP, a high-level synthesis based graph processing framework on FPGAs, is hence proposed to close the gap, with which developers could enjoy high performance of FPGA-accelerated graph processing by writing only a few high-level functions with no knowledge of the hardware. ThunderGP adopts the gather-apply-scatter model as the abstraction of various graph algorithms and realizes the model by a built-in highly parallel and memory-efficient accelerator template. With high-level functions as inputs, ThunderGP automatically explores massive resources of multiple super-logic regions of modern FPGA platforms to generate and deploy accelerators, as well as schedule tasks for them. Although ThunderGP on DRAM-based platforms is memory bandwidth bounded, recent high bandwidth memory (HBM) brings large potentials to performance. However, the system bottleneck shifts from memory bandwidth to resource consumption on HBM-enabled platforms. Therefore, we further propose to improve resource efficiency of ThunderGP to utilize more memory bandwidth from HBM. We conduct evaluation with seven common graph applications and 19 graphs. ThunderGP on DRAM-based hardware platforms provides 1.9× ∼5.2× improvement on bandwidth efficiency over the state of the art, whereas ThunderGP on HBM-based hardware platforms delivers up to 5.2× speedup over the state-of-the-art RTL-based approach.  © 2022 Copyright held by the owner/author(s).",FPGA; framework; graph processing; HBM; HLS,Bandwidth; Computation theory; Data Analytics; Dynamic random access storage; Energy efficiency; High level synthesis; Integrated circuit design; Bandwidth memory; Framework; Graph processing; Hardware platform; High bandwidth; High bandwidth memory; High-level functions; HLS; Memory bandwidths; Performance; Field programmable gate arrays (FPGA)
Detailed Placement for Dedicated LUT-Level FPGA Interconnect,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137319238&doi=10.1145%2f3501802&partnerID=40&md5=e779455dda023d3f87d9c4e8580553f8,"In this work, we develop timing-driven CAD support for FPGA architectures with direct connections between LUTs. We do so by proposing an efficient ILP-based detailed placer, which moves a carefully selected subset of LUTs from their original positions, so that connections of the user circuit can be appropriately aligned with the direct connections of the FPGA, reducing the circuit's critical path delay. We discuss various aspects of making such an approach practicable, from efficient formulation of the integer programs themselves, to appropriate selection of the movable nodes. These careful considerations enable simultaneous movement of tens of LUTs with tens of candidate positions each, in a matter of minutes. In this manner, the impact of additional connections on the critical path delay more than doubles, compared to the previously reported results that relied solely on architecture-oblivious placement.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",algorithm; direct connection; FPGA; ILP; LP; LUT; placement; timing-driven,Computer aided design; Delay circuits; Integer programming; Timing circuits; Critical path delays; Direct connection; Efficient formulation; FPGA architectures; FPGA interconnects; ILP; LP; LUT; Placement; Timing-driven; Field programmable gate arrays (FPGA)
Adaptive Clock Management of HLS-generated Circuits on FPGAs,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146437792&doi=10.1145%2f3520140&partnerID=40&md5=8adae4cd55edf354fffb5268b395afce,"In this article, we present Syncopation, a performance-boosting fine-grained timing analysis and adaptive clock management technique for High-Level Synthesis-generated circuits implemented on Field-Programmable Gate Arrays. The key idea is to use the HLS scheduling information along with the placement and routing results to determine the worst-case timing path for individual clock cycles. By adjusting the clock period on a cycle-by-cycle basis, we can increase performance of an HLS-generated circuit. Our experiments show that Syncopation improves performance by 3.2% (geomean) across all benchmarks (up to 47%). In addition, by employing targeted synthesis techniques along with Syncopation, we can achieve 10.3% performance improvement (geomean) across all benchmarks (up to 50%). Syncopation instrumentation is implemented entirely in soft logic without requiring alterations to the HLS-synthesis toolchain or changes to the FPGA, and has been validated on real hardware.  © 2022 Association for Computing Machinery.",Adaptive Clock Management; CAD; FPGAs; HLS,Benchmarking; Clocks; High level synthesis; Logic Synthesis; Reconfigurable hardware; Timing circuits; Adaptive clock management; Adaptive clocks; Field programmables; Fine grained; High-level synthesis; HLS; Management techniques; Performance; Programmable gate array; Timing Analysis; Field programmable gate arrays (FPGA)
"Median Filters on FPGAs for Infinite Data and Large, Rectangular Windows",2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146428088&doi=10.1145%2f3530273&partnerID=40&md5=367f6b177818ecd632cc01a48e0cd3e2,"Efficient architectures and implementations of median filters have been well investigated in the past. In this article, we focus on median filters for very big scientific applications with very large windows and an infinite stream of data, inspired by big data needs in the Square Kilometre Array (SKA) pulsar search engine, but transferable to other big data domains. We propose a novel approach for very large rectangular windows on an FPGA accelerator device able to support the processing of infinite streams of data. OpenCL is used for rapid parameter sweeping and design space exploration based on a pipelined model of the system. Evaluation on a host/accelerator system with an Arria 10 device surpassed 64 million values processed per second considered for the SKA real time requirement, achieving 83.4M value/s while reading from/writing to disk. These results are compared with a state-of-the-art software implementation only achieving 41M value/s for over twice the total system energy cost.  © 2022 Association for Computing Machinery.",infinite input data; large window filtering; Median filtering; OpenCL; radio astronomy,Big data; Field programmable gate arrays (FPGA); Radio astronomy; Search engines; Efficient architecture; Efficient implementation; Infinite input data; Input datas; Large window filtering; Median filtering; Median-Filter; Opencl; Rectangular windows; Square Kilometer Array; Median filters
A Unified FPGA Virtualization Framework for General-Purpose Deep Neural Networks in the Cloud,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130801625&doi=10.1145%2f3480170&partnerID=40&md5=47640f20297607be21cf196766ebcdb7,"INFerence-As-A-Service (INFaaS) has become a primary workload in the cloud. However, existing FPGA-based Deep Neural Network (DNN) accelerators are mainly optimized for the fastest speed of a single task, while the multi-Tenancy of INFaaS has not been explored yet. As the demand for INFaaS keeps growing, simply increasing the number of FPGA-based DNN accelerators is not cost-effective, while merely sharing these single-Task optimized DNN accelerators in a time-division multiplexing way could lead to poor isolation and high-performance loss for INFaaS. On the other hand, current cloud-based DNN accelerators have excessive compilation overhead, especially when scaling out to multi-FPGA systems for multi-Tenant sharing, leading to unacceptable compilation costs for both offline deployment and online reconfiguration. Therefore, it is far from providing efficient and flexible FPGA virtualization for public and private cloud scenarios.Aiming to solve these problems, we propose a unified virtualization framework for general-purpose deep neural networks in the cloud, enabling multi-Tenant sharing for both the Convolution Neural Network (CNN), and the Recurrent Neural Network (RNN) accelerators on a single FPGA. The isolation is enabled by introducing a two-level instruction dispatch module and a multi-core based hardware resources pool. Such designs provide isolated and runtime-programmable hardware resources, which further leads to performance isolation for multi-Tenant sharing. On the other hand, to overcome the heavy re-compilation overheads, a tiling-based instruction frame package design and a two-stage static-dynamic compilation, are proposed. Only the lightweight runtime information is re-compiled with g1/41 ms overhead, thus guaranteeing the private cloud's performance. Finally, the extensive experimental results show that the proposed virtualized solutions achieve up to 3.12× and 6.18× higher throughput in the private cloud compared with the static CNN and RNN baseline designs, respectively.  © 2021 Association for Computing Machinery.",cloud computing; FPGA; neural networks; Virtualization,Acceleration; Computer hardware; Cost effectiveness; Deep neural networks; Online systems; Recurrent neural networks; Virtual reality; Virtualization; Cloud-computing; Convolution neural network; Cost effective; Hardware resources; Multi tenancies; Multi tenants; Neural-networks; Private clouds; Time-division multiplexing; Virtualizations; Field programmable gate arrays (FPGA)
Design of Distributed Reconfigurable Robotics Systems with ReconROS,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123467317&doi=10.1145%2f3494571&partnerID=40&md5=2983fc7f3773d0fe822fe2599c7810bd,"Robotics applications process large amounts of data in real time and require compute platforms that provide high performance and energy efficiency. FPGAs are well suited for many of these applications, but there is a reluctance in the robotics community to use hardware acceleration due to increased design complexity and a lack of consistent programming models across the software/hardware boundary. In this article, we present ReconROS, a framework that integrates the widely used robot operating system (ROS) with ReconOS, which features multithreaded programming of hardware and software threads for reconfigurable computers. This unique combination gives ROS 2 developers the flexibility to transparently accelerate parts of their robotics applications in hardware. We elaborate on the architecture and the design flow for ReconROS and report on a set of experiments that underline the feasibility and flexibility of our approach.  © 2021 Association for Computing Machinery.",FPGA acceleration; Robot operating sysstem (ROS); robotics,Application programs; Computer operating systems; Energy efficiency; Integrated circuit design; Machine design; Reconfigurable architectures; Reconfigurable hardware; Robot programming; Robotics; Application process; FPGA acceleration; Large amounts of data; Performance; Real- time; Reconfigurable; Robot operating sysstem; Robotic community; Robotic systems; Robotics applications; Field programmable gate arrays (FPGA)
FPGA Architecture Exploration for DNN Acceleration,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130772919&doi=10.1145%2f3503465&partnerID=40&md5=ec7b1e23372025ac472ec4b8863f9c37,"Recent years have seen an explosion of machine learning applications implemented on Field-Programmable Gate Arrays (FPGAs). FPGA vendors and researchers have responded by updating their fabrics to more efficiently implement machine learning accelerators, including innovations such as enhanced Digital Signal Processing (DSP) blocks and hardened systolic arrays. Evaluating architectural proposals is difficult, however, due to the lack of publicly available benchmark circuits.This paper addresses this problem by presenting an open-source benchmark circuit generator that creates realistic DNN-oriented circuits for use in FPGA architecture studies. Unlike previous generators, which create circuits that are agnostic of the underlying FPGA, our circuits explicitly instantiate embedded blocks, allowing for meaningful comparison of recent architectural proposals without the need for a complete inference computer-Aided design (CAD) flow. Our circuits are compatible with the VTR CAD suite, allowing for architecture studies that investigate routing congestion and other low-level architectural implications.In addition to addressing the lack of machine learning benchmark circuits, the architecture exploration flow that we propose allows for a more comprehensive evaluation of FPGA architectures than traditional static benchmark suites. We demonstrate this through three case studies which illustrate how realistic benchmark circuits can be generated to target different heterogeneous FPGAs.  © 2022 Association for Computing Machinery.",benchmarking; FPGA architecture; hardware acceleration; neural networks,Computer aided design; Computer architecture; Computer circuits; Digital signal processing; Electric network analysis; Machine learning; Network architecture; Systolic arrays; Architecture exploration; Array architecture; Benchmark circuit; Computer-aided design; Field-programmable gate array architecture; Hardware acceleration; In-field; Machine learning applications; Neural-networks; Open-source; Field programmable gate arrays (FPGA)
Improving Loop Parallelization by a Combination of Static and Dynamic Analyses in HLS,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130822052&doi=10.1145%2f3501801&partnerID=40&md5=a7572accd8b217edc21c87c2e417a0de,"High-level synthesis (HLS) can be used to create hardware accelerators for compute-intense software parts such as loop structures. Usually, this process requires significant amount of user interaction to steer kernel selection and optimizations. This can be tedious and time-consuming. In this article, we present an approach that fully autonomously finds independent loop iterations and reductions to create parallelized accelerators. We combine static analysis with information available only at runtime to maximize the parallelism exploited by the created accelerators. For loops where we see potential for parallelism, we create fully parallelized kernel implementations. If static information does not suffice to deduce independence, then we assume independence at compile time. We verify this assumption by statically created checks that are dynamically evaluated at runtime, before using the optimized kernel. Evaluating our approach, we can generate speedups for five out of seven benchmarks. With four loop iterations running in parallel, we achieve ideal speedups of up to 4× and on average speedups of 2.27×, both in comparison to an unoptimized accelerator.  © 2022 Copyright held by the owner/author(s).",FPGA; high-level synthesis; loop parallelization; scalar evolution analysis; system-on-chip,High level synthesis; Static analysis; System-on-chip; Evolution analysis; Hardware accelerators; High-level synthesis; Loop iteration; Loop parallelization; Loop structure; Runtimes; Scalar evolution analyse; Software parts; Static and dynamic analysis; Field programmable gate arrays (FPGA)
A BNN Accelerator Based on Edge-skip-calculation Strategy and Consolidation Compressed Tree,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130770198&doi=10.1145%2f3494569&partnerID=40&md5=fb67c72a721224bb17fd15474fa1cc2c,"Binarized neural networks (BNNs) and batch normalization (BN) have already become typical techniques in artificial intelligence today. Unfortunately, the massive accumulation and multiplication in BNN models bring challenges to field-programmable gate array (FPGA) implementations, because complex arithmetics in BN consume too much computing resources. To relax FPGA resource limitations and speed up the computing process, we propose a BNN accelerator architecture based on consolidation compressed tree scheme by combining both XNOR and accumulation operation of the low bit into a systematic one. During the compression process, we adopt 0-padding (not ±1) to achieve no-Accuracy-loss from software modeling to hardware implementation. Moreover, we introduce shift-Addition-BN free binarization technique to shorten the delay path and optimize on-chip storage. To sum up, we drastically cut down the hardware consumption while maintaining great speed performance with the same model complexity as the previous design. We evaluate our accelerator on MNIST and CIFAR-10 dataset and implement the whole system on the ARTIX-7 100T FPGA with speed performance of 2052.65 GOP/s and area efficiency of 70.15 GOPS/KLUT.  © 2022 Association for Computing Machinery.",batch normalization; Binarized neural network; FPGA; hardware accelerator,Complex networks; Forestry; Batch normalization; Binarized neural network; Complex arithmetic; Computing resource; Field-programmable gate array implementations; Hardware accelerators; Neural network model; Neural-networks; Normalisation; Speed performance; Field programmable gate arrays (FPGA)
A Real-Time Deep Learning OFDM Receiver,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130804361&doi=10.1145%2f3494049&partnerID=40&md5=cb61fab56aecd3b05bb20ec29a85ad80,"Machine learning in the physical layer of communication systems holds the potential to improve performance and simplify design methodology. Many algorithms have been proposed; however, the model complexity is often unfeasible for real-Time deployment. The real-Time processing capability of these systems has not been proven yet. In this work, we propose a novel, less complex, fully connected neural network to perform channel estimation and signal detection in an orthogonal frequency division multiplexing system. The memory requirement, which is often the bottleneck for fully connected neural networks, is reduced by ≈ 27 times by applying known compression techniques in a three-step training process. Extensive experiments were performed for pruning and quantizing the weights of the neural network detector. Additionally, Huffman encoding was used on the weights to further reduce memory requirements. Based on this approach, we propose the first field-programmable gate array based, real-Time capable neural network accelerator, specifically designed to accelerate the orthogonal frequency division multiplexing detector workload. The accelerator is synthesized for a Xilinx RFSoC field-programmable gate array, uses small-batch processing to increase throughput, efficiently supports branching neural networks, and implements superscalar Huffman decoders.  © 2021 Association for Computing Machinery.",FPGA; machine learning acceleration; Neural networks; OFDM; physical layer processing; real time,Batch data processing; Complex networks; Convolution; Deep learning; Field programmable gate arrays (FPGA); Frequency estimation; Multilayer neural networks; Multiplexing equipment; Network layers; Real time systems; Signal receivers; Silica; Communications systems; Fully connected neural network; Improve performance; Machine learning acceleration; Memory requirements; Neural-networks; OFDM receiver; Physical layer processing; Physical layers; Real- time; Orthogonal frequency division multiplexing
Quick-Div: Rethinking Integer Divider Design for FPGA-based Soft-processors,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130854358&doi=10.1145%2f3502492&partnerID=40&md5=8b5ffc389b5f0dcb5f21f664e2b19e7f,"In today's FPGA-based soft-processors, one of the slowest instructions is integer division. Compared to the low single-digit latency of other arithmetic operations, the fixed 32-cycle latency of radix-2 division is substantially longer. Given that today's soft-processors typically only implement radix-2 division-if they support hardware division at all-there is significant potential to improve the performance of integer dividers.In this work, we present a set of high-performance, data-dependent, variable-latency integer dividers for FPGA-based soft-processors that we call Quick-Div. We compare them to various radix-N dividers and provide a thorough analysis in terms of latency and resource usage. In addition, we analyze the frequency scaling for such divider designs when (1) treated as a stand-Alone unit and (2) integrated as part of a high-performance soft-processor. Moreover, we provide additional theoretical analysis of different dividers' behaviour and develop a new better-performing Quick-Div variant, called Quick-radix-4. Experimental results show that our Quick-radix-4 design can achieve up to 6.8× better performance and 6.1× better performance-per-LUT over the radix-2 divider for applications such as random number generation. Even in cases where division operations constitute as little as 1% of all executed instructions, Quick-radix-4 provides a performance uplift of 16% compared to the radix-2 divider.  © 2022 Association for Computing Machinery.",Arithmetic operator; integer divider; soft-processor; variable-latency pipeline,Dynamic frequency scaling; Integrated circuit design; Pipeline processing systems; Random number generation; Arithmetic operations; Arithmetic operator; Integer divider; Integer division; Performance; Radix 2; Radix-4; Soft processors; Variable latencies; Variable-latency pipeline; Field programmable gate arrays (FPGA)
"Introduction to Special Issue on FPGAs in Data Centers, Part II",2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130772600&doi=10.1145%2f3495231&partnerID=40&md5=f21fb7c307401d9fd620406c28435fcc,[No abstract available],,
AIgean: An Open Framework for Deploying Machine Learning on Heterogeneous Clusters,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130810768&doi=10.1145%2f3482854&partnerID=40&md5=524cbaae3ba71f1d3f294f91148284f7,"AIgean, pronounced like the sea, is an open framework to build and deploy machine learning (ML) algorithms on a heterogeneous cluster of devices (CPUs and FPGAs). We leverage two open source projects: Galapagos, for multi-FPGA deployment, and hls4ml, for generating ML kernels synthesizable using Vivado HLS. AIgean provides a full end-To-end multi-FPGA/CPU implementation of a neural network. The user supplies a high-level neural network description, and our tool flow is responsible for the synthesizing of the individual layers, partitioning layers across different nodes, as well as the bridging and routing required for these layers to communicate. If the user is an expert in a particular domain and would like to tinker with the implementation details of the neural network, we define a flexible implementation stack for ML that includes the layers of Algorithms, Cluster Deployment & Communication, and Hardware. This allows the user to modify specific layers of abstraction without having to worry about components outside of their area of expertise, highlighting the modularity of AIgean. We demonstrate the effectiveness of AIgean with two use cases: An autoencoder, and ResNet-50 running across 10 and 12 FPGAs. AIgean leverages the FPGA's strength in low-latency computing, as our implementations target batch-1 implementations.  © 2021 Association for Computing Machinery.",data center; FPGAs; hardware/software co-design,Integrated circuit design; Machine learning; Multilayer neural networks; Open source software; Program processors; Datacenter; End to end; Hardware/software codesign; Heterogeneous clusters; Machine learning algorithms; Machine-learning; Multi-FPGA; Neural-networks; Open frameworks; Open source projects; Field programmable gate arrays (FPGA)
The Future of FPGA Acceleration in Datacenters and the Cloud,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130782359&doi=10.1145%2f3506713&partnerID=40&md5=d2754116e4bde093f595f13543ced4cd,"In this article, we survey existing academic and commercial efforts to provide Field-Programmable Gate Array (FPGA) acceleration in datacenters and the cloud. The goal is a critical review of existing systems and a discussion of their evolution from single workstations with PCI-Attached FPGAs in the early days of reconfigurable computing to the integration of FPGA farms in large-scale computing infrastructures. From the lessons learned, we discuss the future of FPGAs in datacenters and the cloud and assess the challenges likely to be encountered along the way. The article explores current architectures and discusses scalability and abstractions supported by operating systems, middleware, and virtualization. Hardware and software security becomes critical when infrastructure is shared among tenants with disparate backgrounds. We review the vulnerabilities of current systems and possible attack scenarios and discuss mitigation strategies, some of which impact FPGA architecture and technology. The viability of these architectures for popular applications is reviewed, with a particular focus on deep learning and scientific computing. This work draws from workshop discussions, panel sessions including the participation of experts in the reconfigurable computing field, and private discussions among these experts. These interactions have harmonized the terminology, taxonomy, and the important topics covered in this manuscript.  © 2022 Association for Computing Machinery.",Cloud; datacenter; FPGA; security; virtualization,Deep learning; Middleware; Reconfigurable architectures; Virtual reality; Virtualization; 'current; Computing infrastructures; Critical review; Datacenter; Existing systems; Large-scale computing; Reconfigurable computing; Reconfigurable- computing; Security; Virtualizations; Field programmable gate arrays (FPGA)
Approximate Constant-Coefficient Multiplication Using Hybrid Binary-Unary Computing for FPGAs,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130857164&doi=10.1145%2f3494570&partnerID=40&md5=0acd2a9bc2f98b0eae0d4f8b6dfbc9bc,"Multipliers are used in virtually all Digital Signal Processing (DSP) applications such as image and video processing. Multiplier efficiency has a direct impact on the overall performance of such applications, especially when real-Time processing is needed, as in 4K video processing, or where hardware resources are limited, as in mobile and IoT devices. We propose a novel, low-cost, low energy, and high-speed approximate constant coefficient multiplier (CCM) using a hybrid binary-unary encoding method. The proposed method implements a CCM using simple routing networks with no logic gates in the unary domain, which results in more efficient multipliers compared to Xilinx LogiCORE IP CCMs and table-based KCM CCMs (Flopoco) on average. We evaluate the proposed multipliers on 2-D discrete cosine transform algorithm as a common DSP module. Post-routing FPGA results show that the proposed multipliers can improve the {area, area × delay, power consumption, and energy-delay product} of a 2-D discrete cosine transform on average by {30%, 33%, 30%, 31%}. Moreover, the throughput of the proposed 2-D discrete cosine transform is on average 5% more than that of the binary architecture implemented using table-based KCM CCMs. We will show that our method has fewer routability issues compared to binary implementations when implementing a DCT core.  © 2021 Association for Computing Machinery.",Constant coefficient multiplication; FIR; JPEG; stochastic computing; unary computing,Computation theory; Computer circuits; Digital signal processing; Field programmable gate arrays (FPGA); Logic gates; Stochastic systems; Video signal processing; All digital; Constant coefficient multiplication; Constant coefficient multipliers; Constant coefficients; FIR; Image and video processing; JPEG; Signal processing applications; Stochastic computing; Unary computing; Discrete cosine transforms
Scalable Phylogeny Reconstruction with Disaggregated Near-memory Processing,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130833308&doi=10.1145%2f3484983&partnerID=40&md5=54f4159a901da46a5766d1bbae89fd39,"Disaggregated computer architectures eliminate resource fragmentation in next-generation datacenters by enabling virtual machines to employ resources such as CPUs, memory, and accelerators that are physically located on different servers. While this paves the way for highly compute-and/or memory-intensive applications to potentially deploy all CPUs and/or memory resources in a datacenter, it poses a major challenge to the efficient deployment of hardware accelerators: input/output data can reside on different servers than the ones hosting accelerator resources, thereby requiring time-and energy-consuming remote data transfers that diminish the gains of hardware acceleration. Targeting a disaggregated datacenter architecture similar to the IBM dReDBox disaggregated datacenter prototype, the present work explores the potential of deploying custom acceleration units adjacently to the disaggregated-memory controller on memory bricks (in dReDBox terminology), which is implemented on FPGA technology, to reduce data movement and improve performance and energy efficiency when reconstructing large phylogenies (evolutionary relationships among organisms). A fundamental computational kernel is the Phylogenetic Likelihood Function (PLF), which dominates the total execution time (up to 95%) of widely used maximum-likelihood methods. Numerous efforts to boost PLF performance over the years focused on accelerating computation; since the PLF is a data-intensive, memory-bound operation, performance remains limited by data movement, and memory disaggregation only exacerbates the problem. We describe two near-memory processing models, one that addresses the problem of workload distribution to memory bricks, which is particularly tailored toward larger genomes (e.g., plants and mammals), and one that reduces overall memory requirements through memory-side data interpolation transparently to the application, thereby allowing the phylogeny size to scale to a larger number of organisms without requiring additional memory.  © 2021 Copyright held by the owner/author(s).",Disaggregated datacenter; dReDBox; near-memory processing; phylogenetics; RAxML,Acceleration; Brick; Computer architecture; Computer hardware; Data transfer; Energy efficiency; Mammals; Maximum likelihood; Program processors; CPU memory; Data movements; Datacenter; Disaggregated datacenter; Dredbox; Near-memory processing; Phylogenetic likelihood functions; Phylogenetics; Phylogeny reconstruction; RAxML; Phylogenetic
Approaches for FPGA Design Assurance,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130831641&doi=10.1145%2f3491233&partnerID=40&md5=69106cc4df6c638569d96e4ad00132ff,"Field-Programmable Gate Arrays (FPGAs) are widely used for custom hardware implementations, including in many security-sensitive industries, such as defense, communications, transportation, medical, and more. Compiling source hardware descriptions to FPGA bitstreams requires the use of complex computer-Aided design (CAD) tools. These tools are typically proprietary and closed-source, and it is not possible to easily determine that the produced bitstream is equivalent to the source design.In this work, we present various FPGA design flows that leverage pre-synthesizing or pre-implementing parts of the design, combined with open-source synthesis tools, bitstream-To-netlist tools, and commercial equivalence-checking tools, to verify that a produced hardware design is equivalent to the designer's source design.We evaluate these different design flows on several benchmark circuits and demonstrate that they are effective at detecting malicious modifications made to the design during compilation. We compare our proposed design flows with baseline commercial design flows and measure the overheads to area and runtime.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",design assurance; equivalence checking; FPGAs,Binary sequences; Computer aided design; Computer hardware; Integrated circuit design; Bitstreams; Closed source; Computer aided design tools; Custom hardwares; Design assurances; Design flows; Equivalence checking; Field programmable gate arrays designs; Hardware descriptions; Hardware implementations; Field programmable gate arrays (FPGA)
Highly Parallel Multi-FPGA System Compilation from Sequential C/C++ Code in the AWS Cloud,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146420455&doi=10.1145%2f3507698&partnerID=40&md5=4e4718a97a0227052517126ac802e36e,"We present a High Level Synthesis compiler that automatically obtains a multi-chip accelerator system from a single-threaded sequential C/C++ application. Invoking the multi-chip accelerator is functionally identical to invoking the single-threaded sequential code the multi-chip accelerator is compiled from. Therefore, software development for using the multi-chip accelerator hardware is simplified, but the multi-chip accelerator can exhibit extremely high parallelism. We have implemented, tested, and verified our push-button system design model on multiple field-programmable gate arrays (FPGAs) of the Amazon Web Services EC2 F1 instances platform, using, as an example, a sequential-natured DES key search application that does not have any DOALL loops and that tries each candidate key in order and stops as soon as a correct key is found. An 8- FPGA accelerator produced by our compiler achieves 44,600 times better performance than an x86 Xeon CPU executing the sequential single-threaded C program the accelerator was compiled from. New features of our compiler system include: an ability to parallelize outer loops with loop-carried control dependences, an ability to pipeline an outer loop without fully unrolling its inner loops, and fully automated deployment, execution and termination of multi-FPGA application-specific accelerators in the AWS cloud, without requiring any manual steps.  © 2022 Association for Computing Machinery.",AWS cloud; compilers; DES cracker; hierarchical software pipelining; high-level synthesis; Multi-FPGA,C++ (programming language); Codes (symbols); Field programmable gate arrays (FPGA); Microprocessor chips; Program compilers; Software design; Web services; AWS cloud; Compiler; DES cracker; Field programmables; Hierarchical software pipelining; High-level synthesis; Multi-field; Multi-field-programmable gate array; Programmable gate array; Software pipelining; High level synthesis
Tensor Slices: FPGA Building Blocks For The Deep Learning Era,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146434889&doi=10.1145%2f3529650&partnerID=40&md5=5c93bcafdf8dd421a51ab0dfeb444617,"FPGAs are well-suited for accelerating deep learning (DL) applications owing to the rapidly changing algorithms, network architectures and computation requirements in this field. However, the generic building blocks available on traditional FPGAs limit the acceleration that can be achieved. Many modifications to FPGA architecture have been proposed and deployed including adding specialized artificial intelligence (AI) processing engines, adding support for smaller precision math like 8-bit fixed point and IEEE half-precision (fp16) in DSP slices, adding shadow multipliers in logic blocks, etc. In this paper, we describe replacing a portion of the FPGA's programmable logic area with Tensor Slices. These slices have a systolic array of processing elements at their heart that support multiple tensor operations, multiple dynamically-selectable precisions and can be dynamically fractured into individual multipliers and MACs (multiply-and-accumulate). These slices have a local crossbar at the inputs that helps with easing the routing pressure caused by a large block on the FPGA. Adding these DL-specific coarse-grained hard blocks to FPGAs increases their compute density and makes them even better hardware accelerators for DL applications, while still keeping the vast majority of the real estate on the FPGA programmable at fine-grain.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",computer architecture; deep learning; FPGA; hardware acceleration; machine learning; neural networks; tensor slice,Computation theory; Computer architecture; Computer hardware; Deep learning; Field programmable gate arrays (FPGA); Learning systems; Network architecture; Neural networks; Systolic arrays; Building blockes; Deep learning; Fixed points; FPGA architectures; Hardware acceleration; Intelligence processing; Machine-learning; Neural-networks; Processing engine; Tensor slice; Tensors
"FPGA HLS Today: Successes, Challenges, and Opportunities",2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143186626&doi=10.1145%2f3530775&partnerID=40&md5=c9720e180c47dda468df32bbcc5db8f4,"The year 2011 marked an important transition for FPGA high-level synthesis (HLS), as it went from prototyping to deployment. A decade later, in this article, we assess the progress of the deployment of HLS technology and highlight the successes in several application domains, including deep learning, video transcoding, graph processing, and genome sequencing. We also discuss the challenges faced by today's HLS technology and the opportunities for further research and development, especially in the areas of achieving high clock frequency, coping with complex pragmas and system integration, legacy code transformation, building on open source HLS infrastructures, supporting domain-specific languages, and standardization. It is our hope that this article will inspire more research on FPGA HLS and bring it to a new height.  © 2022 Copyright held by the owner/author(s).",customizable computing; design automation; design tools; electronic design automation; FPGA acceleration; FPGA architecture; hardware compiler; hardware description; High-level synthesis; reconfigurable applications,Computer hardware description languages; Deep learning; High level synthesis; Integrated circuit design; Open source software; Open systems; Problem oriented languages; Program compilers; Reconfigurable architectures; Reconfigurable hardware; Video signal processing; Customizable; Customizable computing; Design automations; Design tool; Electronics design automation; FPGA acceleration; FPGA architectures; Hardware compilers; Hardware descriptions; High-level synthesis; Reconfigurable; Reconfigurable application; Field programmable gate arrays (FPGA)
HopliteML: Evolving Application Customized FPGA NoCs with Adaptable Routers and Regulators,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146421205&doi=10.1145%2f3507699&partnerID=40&md5=27176010f376836126bc7692b3c1d784,"We can overcome the pessimism in worst-case routing latency analysis of timing-predictable Network-on-Chip (NoC) workloads by single-digit factors through the use of a hybrid field-programmable gate array (FPGA)-optimized NoC and workload-adapted regulation. Timing-predictable FPGA-optimized NoCs such as HopliteBuf integrate stall-free FIFOs that are sized using offline static analysis of a user-supplied flow pattern and rates. For certain bursty traffic and flow configurations, static analysis delivers very large, sometimes infeasible, FIFO size bounds and large worst-case latency bounds. Alternatively, backpressure-based NoCs such as HopliteBP can operate with lower latencies for certain bursty flows. However, they suffer from severe pessimism in the analysis due to the effect of pipelining of packets and interleaving of flows at switch ports. As we show in this article, a hybrid FPGA NoC that seamlessly composes both design styles on a per-switch basis delivers the best of both worlds, with improved feasibility (bounded operation) and tighter latency bounds. We select the NoC switch configuration through a novel evolutionary algorithm based on Maximum Likelihood Estimation (MLE). For synthetic (RANDOM, LOCAL) and real-world (SpMV, Graph) workloads, we demonstrate ≈2-3× improvements in feasibility and ≈1-6.8× in worst-case latency while requiring an LUT cost only ≈1-1.5× larger than the cheapest HopliteBuf solution. We also deploy and verify our NoC (PL) and MLE framework (PS) on a Pynq-Z1 to adapt and reconfigure NoC switches dynamically. We can further improve a workload's routability by learning to surgically tune regulation rates for each traffic trace to maximize available routing bandwidth. We capture critical dependency between traces by modelling the regulation space as a multivariate Gaussian distribution and learn the distribution's parameters using Covariance Matrix Adaptation Evolution Strategy (CMA-ES). We also propose nested learning, which learns switch configurations and regulation rates in tandem. Compared with stand-alone switch learning, this symbiotic nested learning helps achieve ≈ 1.5× lower cost constrained latency, ≈ 3.1× faster individual rates, and ≈ 1.4× faster mean rates. We also evaluate improvements to vanilla NoCs' routing using only stand-alone rate learning (no switch learning), with ≈ 1.6× lower latency across synthetic and real-world benchmarks.  © 2022 Association for Computing Machinery.",FPGA overlays; machine learning; unidirectional torus,Covariance matrix; Evolutionary algorithms; Field programmable gate arrays (FPGA); Machine learning; Network-on-chip; Routers; Field programmables; Field-programmable gate array overlay; Hybrid fields; Latency bounds; Low latency; Machine-learning; Networks on chips; Programmable gate array; Unidirectional torus; Worst-case latencies; Optimization
Stratix 10 NX Architecture,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145344099&doi=10.1145%2f3520197&partnerID=40&md5=ae0014705d5280c10b29321fff1e76ba,"The advent of AI has driven the exploration of high-density low-precision arithmetic on FPGAs. This has resulted in new methods in mapping both arithmetic functions as well as dataflows onto the fabric, as well as some changes to the embedded DSP Blocks. Technologies outside of the FPGA realm have also evolved, such as the addition of tensor structures for GPUs, as well as the introduction of numerous AI ASSPs, all of which have a higher claimed performance and efficiency than current FPGAs. In this article, we will introduce the Stratix 10 NX device, which is a variant of FPGA specifically optimized for the AI application space. In addition to the computational capabilities of the standard programmable soft-logic fabric, a new type of DSP Block provides the dense arrays of low-precision multipliers typically used in AI implementations. The architecture of the block is tuned for the common matrix-matrix or vector-matrix multiplications in AI, with capabilities designed to work efficiently for both small and large matrix sizes. The base precisions are INT8 and INT4, along with shared exponent support to support block FP16 and block FP12 numerics. All additions/accumulations can be done in INT32 or IEEE-754 single precision floating point (FP32), and multiple blocks can be cascaded together to support larger matrices. We will also describe methods by which the smaller precision multipliers can be aggregated to create larger multipliers that are more applicable to standard signal processing requirements.In the AI market, the FPGA must compete directly with other types of devices, rather than occupy a unique niche. Deterministic system performance is as important as the performance of individual FPGA elements, such as logic, memory, and DSP. We will show that the feed forward datapath structures that are needed to support the typical AI matrix-vector and matrix-matrix multiplication operations can consistently close timing at over 500 MHz on a mid-speed grade device, even if all of the Tensor Blocks on the device are used. We will also show a full-chip NPU processor implementation that out performs GPUs at the same process node for a variety of AI inferencing workloads, even though it has a lower operating frequency of 365 MHz.In terms of overall compute throughput, Stratix 10 NX is specified at 143 INT8/FP16 TOPs/FLOPs or 286 INT4/FP12 TOPS/FLOPs. Depending on the configuration, power efficiency is in the range of 1-4 TOPs or TFLOPs/W.  © 2022 Association for Computing Machinery.",AI tensor block; FPGA accelerator; FPGA architecture; place and route,Computation theory; Computer circuits; Digital arithmetic; Efficiency; Field programmable gate arrays (FPGA); Frequency multiplying circuits; Matrix algebra; Number theory; Program processors; Signal processing; AI tensor block; FPGA accelerator; FPGA architectures; Lower precision; Matrix-matrix multiplications; Matrix-vector; Performance; Place and route; Precision arithmetic; Vector-matrix multiplications; Tensors
RapidLayout: Fast Hard Block Placement of FPGA-optimized Systolic Arrays Using Evolutionary Algorithm,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146427929&doi=10.1145%2f3501803&partnerID=40&md5=cf17a245c9ffb15ece3c99da46c046e2,"Evolutionary algorithms can outperform conventional placement algorithms such as simulated annealing, analytical placement, and manual placement on runtime, wirelength, pipelining cost, and clock frequency when mapping hard block intensive designs such as systolic arrays on Xilinx UltraScale+ FPGAs. For certain hard-block intensive designs, the commercial-grade Xilinx Vivado CAD tool cannot provide legal routing solutions without tedious manual placement constraints. Instead, we formulate hard block placement as a multi-objective optimization problem that targets wirelength squared and bounding box size. We build an end-to-end placement-and-routing flow called RapidLayout using the Xilinx RapidWright framework. RapidLayout runs 5-6 faster than Vivado with manual constraints and eliminates the weeks-long effort to manually generate placement constraints. RapidLayout enables transfer learning from similar devices and bootstrapping from much smaller devices. Transfer learning in the UltraScale+ family achieves 11-14 shorter runtime and bootstrapping from a 97% smaller device delivers 2.1-3.2 faster optimizations. RapidLayout outperforms (1) a tuned simulated annealer by 2.7-30.8 in runtime while achieving similar quality of results, (2) VPR by 1.5 in runtime, 1.9-2.4 in wirelength, and 3-4 in bounding box size, while also (3) beating the analytical placer UTPlaceF by 9.3 in runtime, 1.8-2.2 in wirelength, and 2-2.7 in bounding box size.  © 2022 Association for Computing Machinery.",evolutionary algorithm; FPGA placement; systolic array,Computer aided logic design; Evolutionary algorithms; Multiobjective optimization; Simulated annealing; Systolic arrays; Block placement; Bounding-box; Box sizes; FPGA placement; Hard blocks; Placement algorithm; Runtimes; Small devices; Transfer learning; Wire length; Field programmable gate arrays (FPGA)
Inducing Non-uniform FPGA Aging Using Configuration-based Short Circuits,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145560665&doi=10.1145%2f3517042&partnerID=40&md5=021bc05bece87b897281dac13a53f8ef,"This work demonstrates a novel method of accelerating FPGA aging by configuring FPGAs to implement thousands of short circuits, resulting in high on-chip currents and temperatures. Patterns of ring oscillators are placed across the chip and are used to characterize the operating frequency of the FPGA fabric.Over the course of several months of running the short circuits on two-thirds of the reconfigurable fabric, with daily characterization of the FPGA 6 performance, we demonstrate a decrease in FPGA frequency of 8.5%. We demonstrate that this aging is induced in a non-uniform manner. The maximum slowdown outside of the shorted regions is 2.1%, or about a fourth of the maximum slowdown that is experienced inside the shorted region. In addition, we demonstrate that the slowdown is linear after the first two weeks of the experiment and is unaffected by a recovery period.Additional experiments involving short circuits are also performed to demonstrate the results of our initial experiments are repeatable. These experiments also use a more fine-grained characterization method that provides further insight into the non-uniformed nature of the aging caused by short circuits.  © 2022 Association for Computing Machinery.",Aging; electromigration; FPGA; hardware security; NBTI; PBTI; short circuits,Field programmable gate arrays (FPGA); Negative bias temperature instability; 'current; FPGA fabric; Non-uniform; Novel methods; On-chip temperature; Operating frequency; PBTI; Performance; Reconfigurable fabrics; Ring oscillator; Timing circuits
Demystifying the Soft and Hardened Memory Systems of Modern FPGAs for Software Programmers through Microbenchmarking,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146427212&doi=10.1145%2f3517131&partnerID=40&md5=b736847569772c12f7cc057eaa1a164f,"Both modern datacenter and embedded Field Programmable Gate Arrays (FPGAs) provide great opportunities for high-performance and high-energy-efficiency computing. With the growing public availability of FPGAs from major cloud service providers such as AWS, Alibaba, and Nimbix, as well as uniform hardware accelerator development tools (such as Xilinx Vitis and Intel oneAPI) for software programmers, hardware and software developers can now easily access FPGA platforms. However, it is nontrivial to develop efficient FPGA accelerators, especially for software programmers who use high-level synthesis (HLS).The major goal of this article is to figure out how to efficiently access the memory system of modern datacenter and embedded FPGAs in HLS-based accelerator designs. This is especially important for memory-bound applications; for example, a naive accelerator design only utilizes less than 5% of the available off-chip memory bandwidth. To achieve our goal, we first identify a comprehensive set of factors that affect the memory bandwidth, including (1) the clock frequency of the accelerator design, (2) the number of concurrent memory access ports, (3) the data width of each port, (4) the maximum burst access length for each port, and (5) the size of consecutive data accesses. Then, we carefully design a set of HLS-based microbenchmarks to quantitatively evaluate the performance of the memory systems of datacenter FPGAs (Xilinx Alveo U200 and U280) and embedded FPGA (Xilinx ZCU104) when changing those affecting factors, and we provide insights into efficient memory access in HLS-based accelerator designs. Comparing between the typically used soft and hardened memory systems, respectively, found on datacenter and embedded FPGAs, we further summarize their unique features and discuss the effective approaches to leverage these systems. To demonstrate the usefulness of our insights, we also conduct two case studies to accelerate the widely used K-nearest neighbors (KNN) and sparse matrix-vector multiplication (SpMV) algorithms on datacenter FPGAs with a soft (and thus more flexible) memory system. Compared to the baseline designs, optimized designs leveraging our insights achieve about and speedups for the KNN and SpMV accelerators. Our final optimized KNN and SpMV designs on a Xilinx Alveo U200 FPGA fully utilize its off-chip memory bandwidth, and achieve about and speedups over the 24-core CPU implementations.  © 2022 Association for Computing Machinery.",benchmarking; Datacenter FPGAs; embedded FPGAs; HLS; memory system,Bandwidth; Computer software; Embedded systems; Energy efficiency; Field programmable gate arrays (FPGA); Hardening; Integrated circuit design; Memory architecture; Nearest neighbor search; Datacenter; Datacenter field programmable gate array; Embedded field programmable gate array; Embedded fields; Field programmables; High-level synthesis; Memory systems; Programmable gate array; High level synthesis
Accelerating Weather Prediction Using Near-Memory Reconfigurable Fabric,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141759382&doi=10.1145%2f3501804&partnerID=40&md5=ae4adbe677ce77763e3cc68d527de120,"Ongoing climate change calls for fast and accurate weather and climate modeling. However, when solving large-scale weather prediction simulations, state-of-the-art CPU and GPU implementations suffer from limited performance and high energy consumption. These implementations are dominated by complex irregular memory access patterns and low arithmetic intensity that pose fundamental challenges to acceleration. To overcome these challenges, we propose and evaluate the use of near-memory acceleration using a reconfigurable fabric with high-bandwidth memory (HBM). We focus on compound stencils that are fundamental kernels in weather prediction models. By using high-level synthesis techniques, we develop NERO, an field-programmable gate array+HBM-based accelerator connected through Open Coherent Accelerator Processor Interface to an IBM POWER9 host system. Our experimental results show that NERO outperforms a 16-core POWER9 system by and when running two different compound stencil kernels. NERO reduces the energy consumption by and for the same two kernels over the POWER9 system with an energy efficiency of 1.61 GFLOPS/W and 21.01 GFLOPS/W. We conclude that employing near-memory acceleration solutions for weather prediction modeling is promising as a means to achieve both high performance and high energy efficiency.  © 2022 Association for Computing Machinery.",FPGA; high-performance computing; near-memory computing; processing in memory; weather modeling,Climate change; Energy efficiency; Energy utilization; Green computing; High level synthesis; Weather forecasting; Bandwidth memory; High bandwidth; High-performance computing; Near-memory computing; Performance computing; Processing-in-memory; Reconfigurable fabrics; Weather modeling; Weather prediction; Weather prediction model; Field programmable gate arrays (FPGA)
XDNN: Inference for Deep Convolutional Neural Networks,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125847363&doi=10.1145%2f3473334&partnerID=40&md5=d7c8e6fb3477c7e434eceff365e80358,"We present xDNN, an end-to-end system for deep-learning inference based on a family of specialized hardware processors synthesized on Field-Programmable Gate Array (FPGAs) and Convolution Neural Networks (CNN). We present a design optimized for low latency, high throughput, and high compute efficiency with no batching. The design is scalable and a parametric function of the number of multiply-accumulate units, on-chip memory hierarchy, and numerical precision. The design can produce a scale-down processor for embedded devices, replicated to produce more cores for larger devices, or resized to optimize efficiency. On Xilinx Virtex Ultrascale+ VU13P FPGA, we achieve 800 MHz that is close to the Digital Signal Processing maximum frequency and above 80% efficiency of on-chip compute resources.On top of our processor family, we present a runtime system enabling the execution of different networks for different input sizes (i.e., from 224× 224 to 2048× 1024). We present a compiler that reads CNNs from native frameworks (i.e., MXNet, Caffe, Keras, and Tensorflow), optimizes them, generates codes, and provides performance estimates. The compiler combines quantization information from the native environment and optimizations to feed the runtime with code as efficient as any hardware expert could write. We present tools partitioning a CNN into subgraphs for the division of work to CPU cores and FPGAs. Notice that the software will not change when or if the FPGA design becomes an ASIC, making our work vertical and not just a proof-of-concept FPGA project.We show experimental results for accuracy, latency, and power for several networks: In summary, we can achieve up to 4 times higher throughput, 3 times better power efficiency than the GPUs, and up to 20 times higher throughput than the latest CPUs. To our knowledge, we provide solutions faster than any previous FPGA-based solutions and comparable to any other top-of-the-shelves solutions.  © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",AI inference; and optimizations; custom architectures; high efficiency; low latency,Convolution; Deep neural networks; Digital signal processing; Efficiency; Integrated circuit design; Memory architecture; Program compilers; AI inference; And optimization; Convolution neural network; Custom architecture; End-to-end systems; High-throughput; Higher efficiency; Low latency; On chips; Optimisations; Field programmable gate arrays (FPGA)
BurstZ+: Eliminating The Communication Bottleneck of Scientific Computing Accelerators via Accelerated Compression,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125846488&doi=10.1145%2f3476831&partnerID=40&md5=1410c86603102a00972774c231bd1102,"We present BurstZ+, an accelerator platform that eliminates the communication bottleneck between PCIe-attached scientific computing accelerators and their host servers, via hardware-optimized compression. While accelerators such as GPUs and FPGAs provide enormous computing capabilities, their effectiveness quickly deteriorates once data is larger than its on-board memory capacity, and performance becomes limited by the communication bandwidth of moving data between the host memory and accelerator. Compression has not been very useful in solving this issue due to performance and efficiency issues of compressing floating point numbers, which scientific data often consists of. BurstZ+ is an FPGA-based prototype accelerator platform which addresses the bandwidth issue via a class of novel hardware-optimized floating point compression algorithm called ZFP-V. We demonstrate that BurstZ+ can completely remove the host-side communication bottleneck for accelerators, using multiple stencil kernels with a wide range of operational intensities. Evaluated against hand-optimized implementations of kernel accelerators of the same architecture, our single-pipeline BurstZ+ prototype outperforms an accelerator without compression by almost 4×, and even an accelerator with enough memory for the entire dataset by over 2×. Furthermore, the projected performance of BurstZ+ on a future, faster FPGA scales to almost 7× that of the same accelerator without compression, whose performance is still limited by the PCIe bandwidth.  © 2022 Association for Computing Machinery.",bandwidth; compression; FPGA accelerators; stencil codes,Acceleration; Bandwidth; Bandwidth compression; Computer hardware; Digital arithmetic; Program processors; Communication bandwidth; Compression; Computing capability; FPGA accelerator; Host servers; Memory capacity; Memory performance; On-board memory; Performance; Stencil codes; Field programmable gate arrays (FPGA)
Hardware Acceleration of High-Performance Computational Flow Dynamics Using High-Bandwidth Memory-Enabled Field-Programmable Gate Arrays,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125839675&doi=10.1145%2f3476229&partnerID=40&md5=3281b212474b565acd36d5b62ebfc714,"Scientific computing is at the core of many High-Performance Computing applications, including computational flow dynamics. Because of the utmost importance to simulate increasingly larger computational models, hardware acceleration is receiving increased attention due to its potential to maximize the performance of scientific computing. Field-Programmable Gate Arrays could accelerate scientific computing because of the possibility to fully customize the memory hierarchy important in irregular applications such as iterative linear solvers. In this article, we study the potential of using Field-Programmable Gate Arrays in High-Performance Computing because of the rapid advances in reconfigurable hardware, such as the increase in on-chip memory size, increasing number of logic cells, and the integration of High-Bandwidth Memories on board. To perform this study, we propose a novel Sparse Matrix-Vector multiplication unit and an ILU0 preconditioner tightly integrated with a BiCGStab solver kernel. We integrate the developed preconditioned iterative solver in Flow from the Open Porous Media project, a state-of-the-art open source reservoir simulator. Finally, we perform a thorough evaluation of the FPGA solver kernel in both stand-alone mode and integrated in the reservoir simulator, using the NORNE field, a real-world case reservoir model using a grid with more than 105 cells and using three unknowns per cell.  © 2021 Copyright held by the owner/author(s).",BiCGStab; CFD; FPGA; GPU; HPC; ILU0; Iterative solvers,Bandwidth; Computation theory; Computational fluid dynamics; Graphics processing unit; Logic gates; Porous materials; Reconfigurable hardware; Bandwidth memory; Bi-CGSTAB; Computational flows; Flow dynamics; Hardware acceleration; High bandwidth; HPC; ILU0; Iterative solvers; Performance; Field programmable gate arrays (FPGA)
NASCENT2: Generic Near-Storage Sort Accelerator for Data Analytics on SmartSSD,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125875971&doi=10.1145%2f3472769&partnerID=40&md5=680e7c8f362dac5f960dfa3c497b70b9,"As the size of data generated every day grows dramatically, the computational bottleneck of computer systems has shifted toward storage devices. The interface between the storage and the computational platforms has become the main limitation due to its limited bandwidth, which does not scale when the number of storage devices increases. Interconnect networks do not provide simultaneous access to all storage devices and thus limit the performance of the system when executing independent operations on different storage devices. Offloading the computations to the storage devices eliminates the burden of data transfer from the interconnects. Near-storage computing offloads a portion of computations to the storage devices to accelerate big data applications. In this article, we propose a generic near-storage sort accelerator for data analytics, NASCENT2, which utilizes Samsung SmartSSD, an NVMe flash drive with an on-board FPGA chip that processes data in situ.NASCENT2 consists of dictionary decoder, sort, and shuffle FPGA-based accelerators to support sorting database tables based on a key column with any arbitrary data type. It exploits data partitioning applied by data processing management systems, such as SparkSQL, to breakdown the sort operations on colossal tables to multiple sort operations on smaller tables. NASCENT2 generic sort provides 2 × speedup and 15.2 × energy efficiency improvement as compared to the CPU baseline. It moreover considers the specifications of the SmartSSD (e.g., the FPGA resources, interconnect network, and solid-state drive bandwidth) to increase the scalability of computer systems as the number of storage devices increases. With 12 SmartSSDs, NASCENT2 is 9.9× (137.2 ×) faster and 7.3 × (119.2 ×) more energy efficient in sorting the largest tables of TPCC and TPCH benchmarks than the FPGA (CPU) baseline.  © 2022 Association for Computing Machinery.",,Bandwidth; Data Analytics; Data transfer; Field programmable gate arrays (FPGA); Information management; Integrated circuit interconnects; Sorting; Virtual storage; Big data applications; Computational bottlenecks; Computational platforms; Data analytics; Flash drives; Interconnect networks; Limited bandwidth; Performance; Samsung; Simultaneous access; Energy efficiency
Cloud Building Block Chip for Creating FPGA and ASIC Clouds,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125833699&doi=10.1145%2f3466822&partnerID=40&md5=7f73263dc2d914c77c522e131fe1f902,"Hardware-accelerated cloud computing systems based on FPGA chips (FPGA cloud) or ASIC chips (ASIC cloud) have emerged as a new technology trend for power-efficient acceleration of various software applications. However, the operating systems and hypervisors currently used in cloud computing will lead to power, performance, and scalability problems in an exascale cloud computing environment. Consequently, the present study proposes a parallel hardware hypervisor system that is implemented entirely in special-purpose hardware, and that virtualizes application-specific multi-chip supercomputers, to enable virtual supercomputers to share available FPGA and ASIC resources in a cloud system. In addition to the virtualization of multi-chip supercomputers, the system's other unique features include simultaneous migration of multiple communicating hardware tasks, and on-demand increase or decrease of hardware resources allocated to a virtual supercomputer. Partitioning the flat hardware design of the proposed hypervisor system into multiple partitions and applying the chip unioning technique to its partitions, the present study introduces a cloud building block chip that can be used to create FPGA or ASIC clouds as well. Single-chip and multi-chip verification studies have been done to verify the functional correctness of the hypervisor system, which consumes only a fraction of (10%) hardware resources.  © 2021 Association for Computing Machinery.",Application-specific multi-chip supercomputers; parallel preemptive scheduling; virtual supercomputers,Application programs; Application specific integrated circuits; Cloud computing; Field programmable gate arrays (FPGA); Application specific; Application-specific multi-chip supercomputer; Block chips; Building blockes; FPGAs and ASICs; Hypervisors; Multichips; Parallel preemptive scheduling; Pre-emptive scheduling; Virtual supercomputer; Supercomputers
"Request, Coalesce, Serve, and Forget: Miss-Optimized Memory Systems for Bandwidth-Bound Cache-Unfriendly Applications on FPGAs",2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125830678&doi=10.1145%2f3466823&partnerID=40&md5=d5b195f2467526d8e257030eca4dd171,"Applications such as large-scale sparse linear algebra and graph analytics are challenging to accelerate on FPGAs due to the short irregular memory accesses, resulting in low cache hit rates. Nonblocking caches reduce the bandwidth required by misses by requesting each cache line only once, even when there are multiple misses corresponding to it. However, such reuse mechanism is traditionally implemented using an associative lookup. This limits the number of misses that are considered for reuse to a few tens, at most. In this article, we present an efficient pipeline that can process and store thousands of outstanding misses in cuckoo hash tables in on-chip SRAM with minimal stalls. This brings the same bandwidth advantage as a larger cache for a fraction of the area budget, because outstanding misses do not need a data array, which can significantly speed up irregular memory-bound latency-insensitive applications. In addition, we extend nonblocking caches to generate variable-length bursts to memory, which increases the bandwidth delivered by DRAMs and their controllers. The resulting miss-optimized memory system provides up to 25% speedup with 24× area reduction on 15 large sparse matrix-vector multiplication benchmarks evaluated on an embedded and a datacenter FPGA system.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",cuckoo hashing; DRAM; High performance computing; irregular memory accesses; nonblocking caches; reconfigurable computing,Bandwidth; Budget control; Cache memory; Dynamic random access storage; Memory architecture; Reconfigurable architectures; Reusability; Static random access storage; Cuckoo hashing; High performance computing; Irregular memory access; Large-scales; Memory access; Memory systems; Non-blocking caches; Performance computing; Reconfigurable computing; Reconfigurable- computing; Field programmable gate arrays (FPGA)
Introduction to Special Issue on FPGAs in Data Centers,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125868093&doi=10.1145%2f3493607&partnerID=40&md5=4371387d9ef740b80acc02704867d236,[No abstract available],,
Elastic-DF: Scaling Performance of DNN Inference in FPGA Clouds through Automatic Partitioning,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125872766&doi=10.1145%2f3470567&partnerID=40&md5=41e9a89bc288fc118fc733a1d0c166f0,"Customized compute acceleration in the datacenter is key to the wider roll-out of applications based on deep neural network (DNN) inference. In this article, we investigate how to maximize the performance and scalability of field-programmable gate array (FPGA)-based pipeline dataflow DNN inference accelerators (DFAs) automatically on computing infrastructures consisting of multi-die, network-connected FPGAs. We present Elastic-DF, a novel resource partitioning tool and associated FPGA runtime infrastructure that integrates with the DNN compiler FINN. Elastic-DF allocates FPGA resources to DNN layers and layers to individual FPGA dies to maximize the total performance of the multi-FPGA system. In the resulting Elastic-DF mapping, the accelerator may be instantiated multiple times, and each instance may be segmented across multiple FPGAs transparently, whereby the segments communicate peer-to-peer through 100 Gbps Ethernet FPGA infrastructure, without host involvement. When applied to ResNet-50, Elastic-DF provides a 44% latency decrease on Alveo U280. For MobileNetV1 on Alveo U200 and U280, Elastic-DF enables a 78% throughput increase, eliminating the performance difference between these cards and the larger Alveo U250. Elastic-DF also increases operating frequency in all our experiments, on average by over 20%. Elastic-DF therefore increases performance portability between different sizes of FPGA and increases the critical throughput per cost metric of datacenter inference.  © 2021 Association for Computing Machinery.",Deep neural networks; distributed inference; partitioning,Deep neural networks; Distributed computer systems; Automatic partitioning; Datacenter; Dataflow; Distributed inference; In-field; Network inference; Partitioning; Performance; Performance and scalabilities; Scalings; Field programmable gate arrays (FPGA)
The Impact of Terrestrial Radiation on FPGAs in Data Centers,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125860229&doi=10.1145%2f3457198&partnerID=40&md5=fc27c129f942f9570e6e02b1589a7940,"Field programmable gate arrays (FPGAs) are used in large numbers in data centers around the world. They are used for cloud computing and computer networking. The most common type of FPGA used in data centers are re-programmable SRAM-based FPGAs. These devices offer potential performance and power consumption savings. A single device also carries a small susceptibility to radiation-induced soft errors, which can lead to unexpected behavior. This article examines the impact of terrestrial radiation on FPGAs in data centers. Results from artificial fault injection and accelerated radiation testing on several data-center-like FPGA applications are compared. A new fault injection scheme provides results that are more similar to radiation testing. Silent data corruption (SDC) is the most commonly observed failure mode followed by FPGA unavailable and host unresponsive. A hypothetical deployment of 100,000 FPGAs in Denver, Colorado, will experience upsets in configuration memory every half-hour on average and SDC failures every 0.5-11 days on average.  © 2021 Association for Computing Machinery.",configuration scrubbing; fault injection; neutron radiation testing; SEU,Atmospheric radiation; Computer control systems; Field programmable gate arrays (FPGA); Neutron irradiation; Radiation effects; Radiation hardening; Static random access storage; Cloud-computing; Configuration scrubbing; Datacenter; Fault injection; Neutron radiation testing; Neutron radiations; Radiation testing; SEU; Silent data corruptions; Terrestrial radiation; Software testing
BlastFunction: A Full-stack Framework Bringing FPGA Hardware Acceleration to Cloud-native Applications,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125849160&doi=10.1145%2f3472958&partnerID=40&md5=a4120ac80ee8bbe391a3b38dbcaeb627,"""Cloud-native""is the umbrella adjective describing the standard approach for developing applications that exploit cloud infrastructures' scalability and elasticity at their best. As the application complexity and user-bases grow, designing for performance becomes a first-class engineering concern. As an answer to these needs, heterogeneous computing platforms gained widespread attention as powerful tools to continue meeting SLAs for compute-intensive cloud-native workloads. We propose BlastFunction, an FPGA-as-a-Service full-stack framework to ease FPGAs' adoption for cloud-native workloads, integrating with the vast spectrum of fundamental cloud models. At the IaaS level, BlastFunction time-shares FPGA-based accelerators to provide multi-tenant access to accelerated resources without any code rewriting. At the PaaS level, BlastFunction accelerates functionalities leveraging the serverless model and scales functions proactively, depending on the workload's performance. Further lowering the FPGAs' adoption barrier, an accelerators' registry hosts accelerated functions ready to be used within cloud-native applications, bringing the simplicity of a SaaS-like approach to the developers. After an extensive experimental campaign against state-of-the-art cloud scenarios, we show how BlastFunction leads to higher performance metrics (utilization and throughput) against native execution, with minimal latency and overhead differences. Moreover, the scaling scheme we propose outperforms the main serverless autoscaling algorithms in workload performance and scaling operation amount.  © 2022 Association for Computing Machinery.",autoscaling; cloud-native; Field Programmable Gate Arrays (FPGAs); hardware acceleration; time-sharing,Computer hardware; Application complexity; Autoscaling; Cloud infrastructures; Cloud-native; Computing platform; Field programmable gate array; Hardware acceleration; Heterogeneous computing; Performance; Time-sharing; Field programmable gate arrays (FPGA)
Deploying Multi-tenant FPGAs within Linux-based Cloud Infrastructure,2022,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125863724&doi=10.1145%2f3474058&partnerID=40&md5=41df14fb9d58ebab4dec8658f7bc5509,"Cloud deployments now increasingly exploit Field-Programmable Gate Array (FPGA) accelerators as part of virtual instances. While cloud FPGAs are still essentially single-tenant, the growing demand for efficient hardware acceleration paves the way to FPGA multi-tenancy. It then becomes necessary to explore architectures, design flows, and resource management features that aim at exposing multi-tenant FPGAs to the cloud users. In this article, we discuss a hardware/software architecture that supports provisioning space-shared FPGAs in Kernel-based Virtual Machine (KVM) clouds. The proposed hardware/software architecture introduces an FPGA organization that improves hardware consolidation and support hardware elasticity with minimal data movement overhead. It also relies on VirtIO to decrease communication latency between hardware and software domains. Prototyping the proposed architecture with a Virtex UltraScale+ FPGA demonstrated near specification maximum frequency for on-chip data movement and high throughput in virtual instance access to hardware accelerators. We demonstrate similar performance compared to single-tenant deployment while increasing FPGA utilization, which is one of the goals of virtualization. Overall, our FPGA design achieved about 2× higher maximum frequency than the state of the art and a bandwidth reaching up to 28 Gbps on 32-bit data width.  © 2021 Association for Computing Machinery.",Cloud; FPGA; KVM; multi-tenancy; network-on-chip; virtualization,Computer architecture; Integrated circuit design; Linux; Network architecture; Network-on-chip; Servers; Virtual reality; Virtualization; Cloud deployments; Cloud infrastructures; Data movements; Growing demand; Hardware/software; Kernel-based virtual machine; Maximum frequency; Multi tenancies; Multi tenants; Virtualizations; Field programmable gate arrays (FPGA)
"Process variability analysis in interconnect, logic, and arithmetic blocks of 16-nm FinFET FPGAs",2021,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122621750&doi=10.1145%2f3458843&partnerID=40&md5=c5073c150262eeab3f13e39e7dd988aa,"In the current work, we study the process variability of logic, interconnect, and arithmetic/DSP resources in commercial 16-nm FPGAs. We create multiple, soft-macro sensors for each distinct resource under evaluation, and we deploy them across the FPGA fabric to measure intra-die variation, as well as across multiple FPGAs to measure inter-die variation. The derived results are used to create device-signature variability maps characterizing the distribution of variability across the die. Our study includes decoupling of variability to systematic and stochastic parts, exploration of variability under various voltage and temperature conditions and correlation analysis between the variability maps of the different resources. Furthermore, we scrutinize the impact of variability on the performance of actual test circuits and correlate the retrieved results with the sensor-based maps. Our experimental results on four Zynq XCZU7EV FPGAs showed significant intra- and inter-die variability, up to 7.8% and 8.9%, respectively, with a small increase under certain operating conditions. The correlation analysis demonstrated a strong correlation between the logic and arithmetic resources, whereas the interconnects showed a slightly weaker correlation in specific devices. Finally, a relatively moderate correlation was calculated between the variability maps and performance of test circuits due their dissimilar operating behavior versus our sensors. © 2021 Association for Computing Machinery.",FPGA; Process variability; Ring oscillator,Computer circuits; Correlation methods; Dies; FinFET; Petroleum reservoir evaluation; Stochastic systems; 'current; Arithmetic blocks; Correlation analysis; Interconnect blocks; Logic blocks; Performance; Process Variability; Ring oscillator; Test circuit; Variability analysis; Field programmable gate arrays (FPGA)
CGRA-EAM - Rapid Energy and Area Estimation for Coarse-grained Reconfigurable Architectures,2021,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152185825&doi=10.1145%2f3468874&partnerID=40&md5=138dd6e590d250a8de458c55c01e6402,"Reconfigurable architectures are quickly gaining in popularity due to their flexibility and ability to provide high energy efficiency. However, reconfigurable systems allow for a huge design space. Iterative design space exploration (DSE) is often required to achieve good Pareto points with respect to some combination of performance, area, and/or energy. DSE tools depend on information about hardware characteristics in these aspects. These characteristics can be obtained from hardware synthesis and net-list simulation, but this is very time-consuming. Therefore, architecture models are common. This work introduces CGRA-EAM (Coarse-Grained Reconfigurable Architecture - Energy & Area Model), a model for energy and area estimation framework for coarse-grained reconfigurable architectures. The model is evaluated for the Blocks CGRA. The results demonstrate that the mean absolute percentage error is 15.5% and 2.1% for energy and area, respectively, while the model achieves a speedup of close to three orders of magnitude compared to synthesis. © 2021 Association for Computing Machinery.",CGRA; energy efficiency; reconfigurable architecture,Computer aided design; Computer hardware; Reconfigurable architectures; Area estimation; CGRA; Coarse grained reconfigurable architecture; Design space exploration; Design spaces; Energy; Energy estimation; High energy efficiency; Iterative design; Reconfigurable-systems; Energy efficiency
Enhancing the scalability of multi-FPGA stencil computations via highly optimized HDL components,2021,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122621360&doi=10.1145%2f3461478&partnerID=40&md5=55161448f8f33499a96452efba364aab,"Stencil-based algorithms are a relevant class of computational kernels in high-performance systems, as they appear in a plethora of fields, from image processing to seismic simulations, from numerical methods to physical modeling. Among the various incarnations of stencil-based computations, Iterative Stencil Loops (ISLs) and Convolutional Neural Networks (CNNs) represent two well-known examples of kernels belonging to the stencil class. Indeed, ISLs apply the same stencil several times until convergence, while CNN layers leverage stencils to extract features from an image. The computationally intensive essence of ISLs, CNNs, and in general stencil-based workloads, requires solutions able to produce efficient implementations in terms of throughput and power efficiency. In this context, FPGAs are ideal candidates for such workloads, as they allow design architectures tailored to the stencil regular computational pattern. Moreover, the ever-growing need for performance enhancement leads FPGA-based architectures to scale to multiple devices to benefit from a distributed acceleration. For this reason, we propose a library of HDL components to effectively compute ISLs and CNNs inference on FPGA, along with a scalable multi-FPGA architecture, based on custom PCB interconnects. Our solution eases the design flow and guarantees both scalability and performance competitive with state-of-the-art works. © 2021 Association for Computing Machinery.",CNN; HDL; Multi-FPGA; Stencil computation,Convolutional neural networks; Field programmable gate arrays (FPGA); Image processing; Integrated circuit design; Network architecture; Numerical methods; Polychlorinated biphenyls; Scalability; Computational kernels; Convolutional neural network; HDL; High performance systems; Images processing; Iterative stencil loops; Multi-FPGA; Physical modelling; Seismic simulation; Stencil computations; Iterative methods
ACE-GCN: A Fast Data-driven FPGA Accelerator for GCN Embedding,2021,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144611602&doi=10.1145%2f3470536&partnerID=40&md5=c6d48f4312faa0c4c5c526ff2ced68a9,"ACE-GCN is a fast and resource/energy-efficient FPGA accelerator for graph convolutional embedding under data-driven and in-place processing conditions. Our accelerator exploits the inherent power law distribution and high sparsity commonly exhibited by real-world graphs datasets. Contrary to other hardware implementations of GCN, on which traditional optimization techniques are employed to bypass the problem of dataset sparsity, our architecture is designed to take advantage of this very same situation. We propose and implement an innovative acceleration approach supported by our ""implicit-processing-by-association""concept, in conjunction with a dataset-customized convolutional operator. The computational relief and consequential acceleration effect arise from the possibility of replacing rather complex convolutional operations for a faster embedding result estimation. Based on a computationally inexpensive and super-expedited similarity calculation, our accelerator is able to decide from the automatic embedding estimation or the unavoidable direct convolution operation. Evaluations demonstrate that our approach presents excellent applicability and competitive acceleration value. Depending on the dataset and efficiency level at the target, between 23× and 4,930× PyG baseline, coming close to AWB-GCN by 46% to 81% on smaller datasets and noticeable surpassing AWB-GCN for larger datasets and with controllable accuracy loss levels. We further demonstrate the unique hardware optimization characteristics of our approach and discuss its multi-processing potentiality. © 2021 Association for Computing Machinery.",data-driven; embedded systems; FPGA; GCN; Graph convolutional neural networks; graph embedding; graph processing; node identification; power law distribution; sparse datasets,Convolution; Convolutional neural networks; Embedded systems; Embeddings; Graph neural networks; Convolutional neural network; Data driven; Embedded-system; GCN; Graph convolutional neural network; Graph embeddings; Graph processing; Node identifications; Power law distribution; Sparse dataset; Field programmable gate arrays (FPGA)
Dependency Graph-based High-level Synthesis for Maximum Instruction Parallelism,2021,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174005672&doi=10.1145%2f3468875&partnerID=40&md5=a7cd4b5c5d4a6812122ab6e22b48971c,"Performance optimization is an important goal for High-level Synthesis (HLS). Existing HLS scheduling algorithms are all based on Control and Data Flow Graph (CDFG) and will schedule basic blocks in sequential order. Our study shows that the sequential scheduling order of basic blocks is a big limiting factor for achievable circuit performance. In this article, we propose a Dependency Graph (DG) with two important properties for scheduling. First, DG is a directed acyclic graph. Thus, no loop breaking heuristic is needed for scheduling. Second, DG can be used to identify the exact instruction parallelism. Our experiment shows that DG can lead to 76% instruction parallelism increase over CDFG. Based on DG, we propose a bottom-up scheduling algorithm to achieve much higher instruction parallelism than existing algorithms. Hierarchical state transition graph with guard conditions is proposed for efficient implementation of such high parallelism scheduling. Our experimental results show that our DG-based HLS algorithm can outperform the CDFG-based LegUp and the state-of-the-art industrial tool Vivado HLS by 2.88× and 1.29× on circuit latency, respectively. © 2021 Association for Computing Machinery.",Dependency graph; high-level synthesis; instruction parallelism; scheduling,Data flow analysis; Data flow graphs; Graphic methods; Scheduling algorithms; Basic blocks; Circuit performance; Control and data flow graphs; Dependency graphs; Graph-based; High-level synthesis; Instruction parallelism; Performance optimizations; Property; Sequential ordering; High level synthesis
Programming and Synthesis for Software-defined FPGA Acceleration: Status and Future Prospects,2021,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125633273&doi=10.1145%2f3469660&partnerID=40&md5=7056b99b3237361b014199d70d0f7d4b,"FPGA-based accelerators are increasingly popular across a broad range of applications, because they offer massive parallelism, high energy efficiency, and great flexibility for customizations. However, difficulties in programming and integrating FPGAs have hindered their widespread adoption. Since the mid 2000s, there has been extensive research and development toward making FPGAs accessible to software-inclined developers, besides hardware specialists. Many programming models and automated synthesis tools, such as high-level synthesis, have been proposed to tackle this grand challenge. In this survey, we describe the progression and future prospects of the ongoing journey in significantly improving the software programmability of FPGAs. We first provide a taxonomy of the essential techniques for building a high-performance FPGA accelerator, which requires customizations of the compute engines, memory hierarchy, and data representations. We then summarize a rich spectrum of work on programming abstractions and optimizing compilers that provide different trade-offs between performance and productivity. Finally, we highlight several additional challenges and opportunities that deserve extra attention by the community to bring FPGA-based computing to the masses. © 2021 Association for Computing Machinery.",domain-specific language; Field-programmable gate array; hardware acceleration; high-level synthesis,Economic and social effects; Energy efficiency; High level synthesis; Problem oriented languages; Program compilers; Customisation; Domains specific languages; Field programmables; Field-programmable gate array; Future prospects; Hardware acceleration; High-level synthesis; Massive parallelism; Performance; Programmable gate array; Field programmable gate arrays (FPGA)
BISWSRBS: A Winograd-based CNN Accelerator with a Fine-grained Regular Sparsity Pattern and Mixed Precision Quantization,2021,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147395861&doi=10.1145%2f3467476&partnerID=40&md5=506c4159f775c8da796646b1706559d1,"Field-programmable Gate Array (FPGA) is a high-performance computing platform for Convolution Neural Networks (CNNs) inference. Winograd algorithm, weight pruning, and quantization are widely adopted to reduce the storage and arithmetic overhead of CNNs on FPGAs. Recent studies strive to prune the weights in the Winograd domain, however, resulting in irregular sparse patterns and leading to low parallelism and reduced utilization of resources. Besides, there are few works to discuss a suitable quantization scheme for Winograd. In this article, we propose a regular sparse pruning pattern in the Winograd-based CNN, namely, Sub-row-balanced Sparsity (SRBS) pattern, to overcome the challenge of the irregular sparse pattern. Then, we develop a two-step hardware co-optimization approach to improve the model accuracy using the SRBS pattern. Based on the pruned model, we implement a mixed precision quantization to further reduce the computational complexity of bit operations. Finally, we design an FPGA accelerator that takes both the advantage of the SRBS pattern to eliminate low-parallelism computation and the irregular memory accesses, as well as the mixed precision quantization to get a layer-wise bit width. Experimental results on VGG16/VGG-nagadomi with CIFAR-10 and ResNet-18/34/50 with ImageNet show up to 11.8×/8.67× and 8.17×/8.31×/10.6× speedup, 12.74×/9.19× and 8.75×/8.81×/11.1× energy efficiency improvement, respectively, compared with the state-of-the-art dense Winograd accelerator [20] with negligible loss of model accuracy. We also show that our design has 4.11× speedup compared with the state-of-the-art sparse Winograd accelerator [19] on VGG16. © 2021 Association for Computing Machinery.",convolutional neural networks; FPGA; hardware-friendly sparsity; mixed precision quantization; Winograd fast algorithm,Convolution; Convolutional neural networks; Energy efficiency; Integrated circuit design; Convolutional neural network; Fast algorithms; Field programmables; Field-programmable gate array; Hardware-friendly sparsity; Mixed precision; Mixed precision quantization; Programmable gate array; Quantisation; Winograd; Winograd fast algorithm; Field programmable gate arrays (FPGA)
Analytical performance estimation for large-scale reconfigurable dataflow platforms,2021,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122625544&doi=10.1145%2f3452742&partnerID=40&md5=c7a16cb4455c923538e0a76fe51d3f34,"Next-generation high-performance computing platforms will handle extreme data- and compute-intensive problems that are intractable with today's technology. A promising path in achieving the next leap in high-performance computing is to embrace heterogeneity and specialised computing in the form of reconfigurable accelerators such as FPGAs, which have been shown to speed up compute-intensive tasks with reduced power consumption. However, assessing the feasibility of large-scale heterogeneous systems requires fast and accurate performance prediction. This article proposes Performance Estimation for Reconfigurable Kernels and Systems (PERKS), a novel performance estimation framework for reconfigurable dataflow platforms. PERKS makes use of an analytical model with machine and application parameters for predicting the performance of multi-accelerator systems and detecting their bottlenecks. Model calibration is automatic, making the model flexible and usable for different machine configurations and applications, including hypothetical ones. Our experimental results show that PERKS can predict the performance of current workloads on reconfigurable dataflow platforms with an accuracy above 91%. The results also illustrate how the modelling scales to large workloads, and how performance impact of architectural features can be estimated in seconds. © 2021 Association for Computing Machinery.",FPGAs; Heterogeneous systems; Performance modelling; Reconfigurable dataflow platforms,Data flow analysis; Forecasting; Reconfigurable architectures; Reconfigurable hardware; Analytical performance; Dataflow; Heterogeneous systems; Large-scales; Performance; Performance computing; Performance estimation; Performance Modeling; Reconfigurable; Reconfigurable dataflow platform; Field programmable gate arrays (FPGA)
Reconfigurable Framework for Resilient Semantic Segmentation for Space Applications,2021,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160542217&doi=10.1145%2f3472770&partnerID=40&md5=ff7557030b5471e596cd6f6e2c0cf1e3,"Deep learning (DL) presents new opportunities for enabling spacecraft autonomy, onboard analysis, and intelligent applications for space missions. However, DL applications are computationally intensive and often infeasible to deploy on radiation-hardened (rad-hard) processors, which traditionally harness a fraction of the computational capability of their commercial-off-the-shelf counterparts. Commercial FPGAs and system-on-chips present numerous architectural advantages and provide the computation capabilities to enable onboard DL applications; however, these devices are highly susceptible to radiation-induced single-event effects (SEEs) that can degrade the dependability of DL applications. In this article, we propose Reconfigurable ConvNet (RECON), a reconfigurable acceleration framework for dependable, high-performance semantic segmentation for space applications. In RECON, we propose both selective and adaptive approaches to enable efficient SEE mitigation. In our selective approach, control-flow parts are selectively protected by triple-modular redundancy to minimize SEE-induced hangs, and in our adaptive approach, partial reconfiguration is used to adapt the mitigation of dataflow parts in response to a dynamic radiation environment. Combined, both approaches enable RECON to maximize system performability subject to mission availability constraints. We perform fault injection and neutron irradiation to observe the susceptibility of RECON and use dependability modeling to evaluate RECON in various orbital case studies to demonstrate a 1.5-3.0× performability improvement in both performance and energy efficiency compared to static approaches. © 2021 Association for Computing Machinery.",dependability modeling; Environmentally adaptive computing; single-event effects; space computing,Deep learning; Energy efficiency; Neutron irradiation; Orbits; Radiation effects; Radiation hardening; Space applications; System-on-chip; Adaptive approach; Adaptive computing; Convnet; Dependability modeling; Environmentally adaptive computing; Performance; Reconfigurable; Semantic segmentation; Single event effects; Space computing; Semantics
A deep learning framework to predict routability for FPGA circuit placement,2021,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122611264&doi=10.1145%2f3465373&partnerID=40&md5=4fd427f0b3ea58989c675fb168ca01e4,"The ability to accurately and efficiently estimate the routability of a circuit based on its placement is one of the most challenging and difficult tasks in the Field Programmable Gate Array (FPGA) flow. In this article, we present a novel, deep learning framework based on a Convolutional Neural Network (CNN) model for predicting the routability of a placement. Since the performance of the CNN model is strongly dependent on the hyper-parameters selected for the model, we perform an exhaustive parameter tuning that significantly improves the model's performance and we also avoid overfitting the model. We also incorporate the deep learning model into a state-of-the-art placement tool and show how the model can be used to (1) avoid costly, but futile, place-and-route iterations, and (2) improve the placer's ability to produce routable placements for hard-to-route circuits using feedback based on routability estimates generated by the proposed model. The model is trained and evaluated using over 26K placement images derived from 372 benchmarks supplied by Xilinx Inc. We also explore several opportunities to further improve the reliability of the predictions made by the proposed DLRoute technique by splitting the model into two separate deep learning models for (a) global and (b) detailed placement during the optimization process. Experimental results show that the proposed framework achieves a routability prediction accuracy of 97% while exhibiting runtimes of only a few milliseconds. © 2021 Association for Computing Machinery.",Deep learning; FPGA placement; Routability prediction; Xilinx UltraScale,Convolutional neural networks; Deep learning; Field programmable gate arrays (FPGA); Timing circuits; Array circuits; Convolutional neural network; Deep learning; Field programmable gate array placements; Learning frameworks; Learning models; Neural network model; Routability; Routability prediction; Xilinx ultrascale; Forecasting
Hardware context switch-based cryptographic accelerator for handling multiple streams,2021,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121196996&doi=10.1145%2f3460941&partnerID=40&md5=48e67c20079eb6cd8c2b9fe77c6a0d68,"The confidentiality and integrity of a stream has become one of the biggest issues in telecommunication. The best available algorithm handling the confidentiality of a data stream is the symmetric key block cipher combined with a chaining mode of operation such as cipher block chaining (CBC) or counter mode (CTR). This scheme is difficult to accelerate using hardware when multiple streams coexist. This is caused by the computation time requirement and mainly by management of the streams. In most accelerators, computation is treated at the block-level rather than as a stream, making the management of multiple streams complex. This article presents a solution combining CBC and CTR modes of operation with a hardware context switching. The hardware context switching allows the accelerator to treat the data as a stream. Each stream can have different parameters: key, initialization value, state of counter. Stream switching was managed by the hardware context switching mechanism. A high-level synthesis tool was used to generate the context switching circuit. The scheme was tested on three cryptographic algorithms: AES, DES, and BC3. The hardware context switching allowed the software to manage multiple streams easily, efficiently, and rapidly. The software was freed of the task of managing the stream state. Compared to the original algorithm, about 18%-38% additional logic elements were required to implement the CBC or CTR mode and the additional circuits to support context switching. Using this method, the performance overhead when treating multiple streams was low, and the performance was comparable to that of existing hardware accelerators not supporting multiple streams. © 2021 Association for Computing Machinery.",FPGA; Hardware context switch; HLS; Multiple stream,Computation theory; Computer hardware description languages; Cryptography; High level synthesis; Switching; Block chaining modes; Cipher-block-chaining; Context switch; Context switching; Counter modes; Hardware context switch; HLS; Mode of operations; Multiple streams; Performance; Field programmable gate arrays (FPGA)
Mitigating Voltage Attacks in Multi-Tenant FPGAs,2021,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111570780&doi=10.1145%2f3451236&partnerID=40&md5=b4ef8d4a3cee146f7313fb597e95324c,"Recent research has exposed a number of security issues related to the use of FPGAs in embedded system and cloud computing environments. Circuits that deliberately waste power can be carefully crafted by a malicious cloud FPGA user and deployed to cause denial-of-service and fault injection attacks. The main defense strategy used by FPGA cloud services involves checking user-submitted designs for circuit structures that are known to aggressively consume power. Unfortunately, this approach is limited by an attacker's ability to conceive new designs that defeat existing checkers. In this work, our contributions are twofold. We evaluate a variety of circuit power wasting techniques that typically are not flagged by design rule checks imposed by FPGA cloud computing vendors. The efficiencies of five power wasting circuits, including our new design, are evaluated in terms of power consumed per logic resource. We then show that the source of voltage attacks based on power wasters can be identified. Our monitoring approach localizes the attack and suppresses the clock signal for the target region within 21 μs, which is fast enough to stop an attack before it causes a board reset. All experiments are performed using a state-of-the-art Intel Stratix 10 FPGA.  © 2021 Association for Computing Machinery.",Cloud FPGAs; Embedded FPGAs; Voltage attacks,Cloud computing; Computation theory; Computer circuits; Denial-of-service attack; Integrated circuit design; Network security; Circuit structures; Cloud computing environments; Denial of Service; Design rule checks; Fault injection attacks; Monitoring approach; Recent researches; State of the art; Field programmable gate arrays (FPGA)
Introduction to the Special Section on FPL 2019,2021,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111616345&doi=10.1145%2f3459587&partnerID=40&md5=b563b8f2fc31fbc1a8593681b1981491,[No abstract available],,
Practical Model Checking on FPGAs,2021,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110729298&doi=10.1145%2f3448272&partnerID=40&md5=990f030c09cf7e50e0f8e1635f22b2e8,"Software verification is an important stage of the software development process, particularly for mission-critical systems. As the traditional methodology of using unit tests falls short of verifying complex software, developers are increasingly relying on formal verification methods, such as explicit state model checking, to automatically verify that the software functions properly. However, due to the ever-increasing complexity of software designs, model checking cannot be performed in a reasonable amount of time when running on general-purpose cores, leading to the exploration of hardware-accelerated model checking. FPGAs have been demonstrated to be promising verification accelerators, exhibiting nearly three orders of magnitude speedup over software. Unfortunately, the ""FPGA programmability wall,""particularly the long synthesis and place-and-route times, block the general adoption of FPGAs for model checking.To address this problem, we designed a runtime-programmable pipeline specifically for model checkers on FPGAs to minimize the ""preparation time""before a model can be checked. Our design of the successor state generator and the state validator modules enables FPGA-acceleration of model checking without incurring the time-consuming FPGA implementation stages, reducing the preparation time before checking a model from hours to less than a minute, while incurring only a 26% execution time overhead compared to model-specific implementations.  © 2021 Association for Computing Machinery.",Accelerators; overlay architecture,Field programmable gate arrays (FPGA); Formal verification; Software design; Software testing; Explicit-state model checking; Formal verification methods; FPGA implementations; Hardware-accelerated; Mission critical systems; Software development process; Software verification; Three orders of magnitude; Model checking
Design and Analysis of Configurable Ring Oscillators for True Random Number Generation Based on Coherent Sampling,2021,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110704487&doi=10.1145%2f3433166&partnerID=40&md5=dc2637a8a4793096d999ad2676146d8b,"True Random Number Generators (TRNGs) are indispensable in modern cryptosystems. Unfortunately, to guarantee high entropy of the generated numbers, many TRNG designs require a complex implementation procedure, often involving manual placement and routing. In this work, we introduce, analyse, and compare three dynamic calibration mechanisms for the COherent Sampling ring Oscillator based TRNG: GateVar, WireVar, and LUTVar, enabling easy integration of the entropy source into complex systems. The TRNG setup procedure automatically selects a configuration that guarantees the security requirements. In the experiments, we show that two out of the three proposed mechanisms are capable of assuring correct TRNG operation even when an automatic placement is carried out and when the design is ported to another Field-Programmable Gate Array (FPGA) family. We generated random bits on both a Xilinx Spartan 7 and a Microsemi SmartFusion2 implementation that, without post processing, passed the AIS-31 statistical tests at a throughput of 4.65 Mbit/s and 1.47 Mbit/s, respectively.  © 2021 Association for Computing Machinery.",AIS-31; entropy; NIST SP 800-90B; Security; TRNG,Automatic identification; Cryptography; Entropy; Field programmable gate arrays (FPGA); Integrated circuit design; Number theory; Automatic placement; Coherent sampling; Design and analysis; Dynamic calibration; Entropy sources; Placement and routing; Ring oscillator; Security requirements; Random number generation
A Software/Hardware Co-Design of Crystals-Dilithium Signature Scheme,2021,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110757428&doi=10.1145%2f3447812&partnerID=40&md5=3e7a5843f58e5e4b95b692021ef7729e,"As quantum computers become more affordable and commonplace, existing security systems that are based on classical cryptographic primitives, such as RSA and Elliptic Curve Cryptography (ECC), will no longer be secure. Hence, there has been interest in designing post-quantum cryptographic (PQC) schemes, such as those based on lattice-based cryptography (LBC). The potential of LBC schemes is evidenced by the number of such schemes passing the selection of NIST PQC Standardization Process Round-3. One such scheme is the Crystals-Dilithium signature scheme, which is based on the hard module-lattice problem. However, there is no efficient implementation of the Crystals-Dilithium signature scheme. Hence, in this article, we present a compact hardware architecture containing elaborate modular multiplication units using the Karatsuba algorithm along with smart generators of address sequence and twiddle factors for NTT, which can complete polynomial addition/multiplication with the parameter setting of Dilithium in a short clock period. Also, we propose a fast software/hardware co-design implementation on Field Programmable Gate Array (FPGA) for the Dilithium scheme with a tradeoff between speed and resource utilization. Our co-design implementation outperforms a pure C implementation on a Nios-II processor of the platform Altera DE2-115, in the sense that our implementation is 11.2 and 7.4 times faster for signature and verification, respectively. In addition, we also achieve approximately 51% and 31% speed improvement for signature and verification, in comparison to the pure C implementation on processor ARM Cortex-A9 of ZYNQ-7020 platform.  © 2021 Association for Computing Machinery.",Crystals-Dilithium; hardware implementation; Lattice-based cryptography(LBC); polynomial multiplication,Authentication; Crystals; Field programmable gate arrays (FPGA); Integrated circuit design; Polynomials; Public key cryptography; Quantum computers; Quantum cryptography; Cryptographic primitives; Efficient implementation; Elliptic Curve Cryptography(ECC); Lattice-based cryptography; Modular Multiplication; Resource utilizations; Software/hardware co designs; Standardization process; Hardware-software codesign
Specializing FGPU for Persistent Deep Learning,2021,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110584952&doi=10.1145%2f3457886&partnerID=40&md5=fe44bbc29c33d33482ae514d25ccc18d,"Overlay architectures are a good way to enable fast development and debug on FPGAs at the expense of potentially limited performance compared to fully customized FPGA designs. When used in concert with hand-tuned FPGA solutions, performant overlay architectures can improve time-to-solution and thus overall productivity of FPGA solutions. This work tunes and specializes FGPU, an open source OpenCL-programmable GPU overlay for FPGAs. We demonstrate that our persistent deep learning (PDL)-FGPU architecture maintains the ease-of-programming and generality of GPU programming while achieving high performance from specialization for the persistent deep learning domain. We also propose an easy method to specialize for other domains. PDL-FGPU includes new instructions, along with micro-architecture and compiler enhancements. We evaluate both the FGPU baseline and the proposed PDL-FGPU on a modern high-end Intel Stratix 10 2800 FPGA in simulation running persistent DL applications (RNN, GRU, LSTM), and non-DL applications to demonstrate generality. PDL-FGPU requires 1.4-3× more ALMs, 4.4-6.4× more M20ks, and 1-9.5× more DSPs than baseline, but improves performance by 56-693× for PDL applications with an average 23.1% degradation on non-PDL applications. We integrated the PDL-FGPU overlay into Intel OPAE to measure real-world performance/power and demonstrate that PDL-FGPU is only 4.0-10.4× slower than the Nvidia V100.  © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.",FPGA; GPU; Overlay; persistent deep learning; RNN; soft GPU; specialization,Architecture; Computer architecture; Field programmable gate arrays (FPGA); Long short-term memory; Open source software; FPGA design; GPU programming; Micro architectures; Open sources; Overlay architecture; Programmable gpu; Real-world performance; Time-to-solution; Deep learning
CoNFV: A heterogeneous platform for scalable network function virtualization,2020,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097987019&doi=10.1145%2f3409113&partnerID=40&md5=4ba7b24257b640e06dbcc3e797834851,"Network function virtualization (NFV) is a powerful networking approach that leverages computing resources to perform a time-varying set of network processing functions. Although microprocessors can be used for this purpose, their performance limitations and lack of specialization present implementation challenges. In this article, we describe a new heterogeneous hardware-software NFV platform called CoNFV that provides scalability and programmability while supporting significant hardware-level parallelism and reconfiguration. Our computing platform takes advantage of both field-programmable gate arrays (FPGAs) and microprocessors to implement numerous virtual network functions (VNF) that can be dynamically customized to specific network flow needs. The most distinctive feature of our system is the use of global network state to coordinate NFV operations. Traffic management and hardware reconfiguration functions are performed by a global coordinator that allows for the rapid sharing of network function states and continuous evaluation of network function needs. With the help of state sharing mechanism offered by the coordinator, customer-defined VNF instances can be easily migrated between heterogeneous middleboxes as the network environment changes. A resource allocation and scheduling algorithm dynamically assesses resource deployments as network flows and conditions are updated. We show that our deployment algorithm can successfully reallocate FPGA and microprocessor resources in a fraction of a second in response to changes in network flow capacity and network security threats including intrusion.  © 2020 ACM.",coordinator; FPGA; heterogeneous middleboxes; Network function virtualization; performance-aware VNF deployment,Field programmable gate arrays (FPGA); Network security; Reconfigurable hardware; Transfer functions; Allocation and scheduling; Deployment algorithms; Heterogeneous hardware; Heterogeneous platforms; Network environments; Network processing; Performance limitations; Resource deployments; Network function virtualization
Large-scale Cellular Automata on FPGAs,2020,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098001372&doi=10.1145%2f3423185&partnerID=40&md5=064723cc44e8348c947826e6509a078d,"Cellular automata (CA) are discrete mathematical models discovered in the 1940s by John von Neumann and Stanislaw Ulam and have been used extensively in many scientific disciplines ever since. The present work evolved from a Field Programmable Gate Array- (FPGA) based design to simulate urban growth into a generic architecture that is automatically generated by a framework to efficiently compute complex cellular automata with large 29 × 29 neighborhoods in Cartesian or toroidal grids, with 16 or 256 states per cell. The new architecture and the framework are presented in detail, including results in terms of modeling capabilities and performance. Large neighborhoods greatly enhance CA modeling capabilities, such as the implementation of anisotropic rules. Performance-wise, the proposed architecture runs on a medium-size FPGA up to 51 times faster vs. a CPU running highly optimized C code. Compared to GPUs the speedup is harder to quantify, because CA results have been reported on GPU implementations with neighborhoods up to 11 × 11, in which case FPGA performance is roughly on par with GPU; however, based on published GPU trends, for 29 × 29 neighborhoods the proposed architecture is expected to have better performance vs. a GPU, at one-10th the energy requirements. The architecture and sample designs are open source available under the creative commons license.  © 2020 ACM.",Cellular automata; FPGA accelerator; framework; generic architecture,Architecture; Field programmable gate arrays (FPGA); Program processors; Robots; Urban growth; Automatically generated; Discrete mathematical models; Energy requirements; Generic architecture; GPU implementation; Large neighborhood; Proposed architectures; Scientific discipline; Cellular automata
An OpenGL compliant hardware implementation of a graphic processing unit using field programmable gate array-system on chip technology,2020,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097981389&doi=10.1145%2f3410357&partnerID=40&md5=f1cad0c2ebf488f987d7cdb949d84544,"FPGA-SoC technology provides a heterogeneous platform for advanced, high-performance systems. The System on Chip (SoC) architecture combines traditional single and multiple core processor topologies with flexible FPGA fabric. Dynamic reconfiguration allows the hardware accelerators to be changed at run-time. This article presents a novel OpenGL compliant GPU design implemented on an FPGA. The design uses an FPGA-SoC environment allowing the embedded processor to offload graphics operation onto a more suitable architecture. To the authors' knowledge, this is a first. The graphics processor consists of GLSL compliant shaders, an efficient Barycentric Rasterizer, and a draw mode manager. Performance analysis shows the throughput of the shaders to be hundreds of millions of vertices per second. The design uses both pipelining and resource reuse to optimise throughput and resource use, allowing implementation on a low-cost, FPGA device. Pixel processing rates from this implementation are almost 80% higher than other FPGA implementations. Power consumption compared with comparative embedded devices shows the FPGA consuming as little as 2% of the power of a Mali device, and an up to 11.9-fold increase in efficiency compared to an Nvidia RTX 2060 - Turing architecture device.  © 2020 ACM.",FPGA-SoC; graphics rendering; partial dynamic reconfiguration,Application programming interfaces (API); Application specific integrated circuits; Computer architecture; Dynamic models; Energy efficiency; Field programmable gate arrays (FPGA); Graphics processing unit; Programmable logic controllers; System-on-chip; Dynamic re-configuration; Graphic processing units; Hardware accelerators; Hardware implementations; Heterogeneous platforms; High performance systems; Performance analysis; System-on-chip architecture; Integrated circuit design
Parallel Unary Computing Based on Function Derivatives,2020,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097983952&doi=10.1145%2f3418464&partnerID=40&md5=a129f6e15e90929313eb5fd5c257bfdc,"The binary number representation has dominated digital logic for decades due to its compact storage requirements. An alternative representation is the unary number system: We use N bits, from which the first M are 1 and the rest are 0 to represent the value M/N. One-hot representation is a variation of the unary number system where it has one 1 in the N bits, where the 1's position represents its value. We present a novel method that first converts binary numbers to unary using thermometer (one-hot) encoders and then uses a ""scaling network""followed by voting gates that we call ""alternator logic,""followed by a decoder to convert the numbers back to the binary format. For monotonically increasing functions, the scaling network is all we need, which essentially uses only the routing resources and flip-flops on a typical FPGA architecture. Our method is clearly superior to the conventional binary implementation: Our area×delay cost is on average only 0.4%, 4%, and 39% of the binary method for 8-, 10-, and 12-bit resolutions, respectively, in thermometer encoding scheme, and 0.5%, 15%, and 147% in the one-hot encoding scheme. In terms of power efficiency, our one-hot method is between about 69× and 114× better compared to conventional binary.  © 2020 ACM.",alternator logic; scaling network; stochastic computing; thermometer code; Unary computing,Computation theory; Encoding (symbols); Flip flop circuits; Numbering systems; Signal encoding; State assignment; Thermometers; Compact storages; Digital logic; Encoding schemes; FPGA architectures; Increasing functions; Power efficiency; Routing resources; Thermometer encoding; Computer circuits
PipeArch: Generic and context-switch capable data processing on fpgas,2020,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098001948&doi=10.1145%2f3418465&partnerID=40&md5=2aebb42c11d2953c76129088a71bdc46,"Data processing systems based on FPGAs offer high performance and energy efficiency for a variety of applications. However, these advantages are achieved through highly specialized designs. The high degree of specialization leads to accelerators with narrow functionality and designs adhering to a rigid execution flow. For multi-tenant systems this limits the scope of applicability of FPGA-based accelerators, because, first, supporting a single operation is unlikely to have any significant impact on the overall performance of the system, and, second, serving multiple users satisfactorily is difficult due to simplistic scheduling policies enforced when using the accelerator. Standard operating system and database management system features that would help address these limitations, such as context-switching, preemptive scheduling, and thread migration are practically non-existent in current FPGA accelerator efforts. In this work, we propose PipeArch, an open-source project1 for developing FPGA-based accelerators that combine the high efficiency of specialized hardware designs with the generality and functionality known from conventional CPU threads. PipeArch provides programmability and extensibility in the accelerator without losing the advantages of SIMD-parallelism and deep pipelining. PipeArch supports context-switching and thread migration, thereby enabling for the first time new capabilities such as preemptive scheduling in FPGA accelerators within a high-performance data processing setting. We have used PipeArch to implement a variety of machine learning methods for generalized linear model training and recommender systems showing empirically their advantages over a high-end CPU and even over fully specialized FPGA designs.  © 2020 ACM.",context-switch; data processing; FPGA; generalized linear models; generic architecture; high-performance; machine learning; matrix factorization; programmable; training,Acceleration; Data handling; Energy efficiency; Field programmable gate arrays (FPGA); Linear accelerators; Open systems; Scheduling; Data processing systems; Database management; Fpga accelerators; Generalized linear model; Machine learning methods; Pre-emptive scheduling; Scheduling policies; Specialized hardware; Learning systems
Introduction to Special Section on FCCM 2019,2020,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093852991&doi=10.1145%2f3410373&partnerID=40&md5=c95ab668207585b8c3ea4aa0d39583ab,[No abstract available],accelerators; HBM; memory bandwidth; parallelism; routing; source code; Tools,
MEG: A RISCV-based system emulation infrastructure for near-data processing using fpgas and high-bandwidth memory,2020,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093855596&doi=10.1145%2f3409114&partnerID=40&md5=c33209da8e652b05083b21b932bd64c8,"Emerging three-dimensional (3D) memory technologies, such as the Hybrid Memory Cube (HMC) and High Bandwidth Memory (HBM), provide high-bandwidth and massive memory-level parallelism. With the growing heterogeneity and complexity of computer systems (CPU cores and accelerators, etc.), efficiently integrating emerging memories into existing systems poses new challenges and requires detailed evaluation in a realistic computing environment. In this article, we propose MEG, an open source, configurable, cycle-exact, and RISC-V-based full-system emulation infrastructure using FPGA and HBM. MEG provides a highly modular hardware design and includes a bootable Linux image for a realistic software flow, so that users can perform cross-layer software-hardware co-optimization in a full-system environment. To improve the observability and debuggability of the system, MEG also provides a flexible performance monitoring scheme to guide the performance optimization. The proposed MEG infrastructure can potentially benefit broad communities across computer architecture, system software, and application software. Leveraging MEG, we present two cross-layer system optimizations as illustrative cases to demonstrate the usability of MEG. In the first case study, we present a reconfigurable memory controller to improve the address mapping of standard memory controller. This reconfigurable memory controller along with its OS support allows us to optimize the address mapping scheme to fully exploit the massive parallelism provided by the emerging three-dimensional (3D) memories. In the second case study, we present a lightweight IOMMU design to tackle the unique challenges brought by 3D memory in providing virtual memory support for near-memory accelerators. We provide a prototype implementation of MEG on a Xilinx VU37P FPGA and demonstrate its capability, fidelity, and flexibility on real-world benchmark applications. We hope MEG fills a gap in the space of publicly available FPGA-based full-system emulation infrastructures, specifically targeting memory systems, and inspires further collaborative software/hardware innovations. © 2020 ACM.",3d-stacking memory; address mapping scheme; FPGAs; Full-system emulation; high bandwidth memory; near-memory acceleration; RISC-V core,Application programs; Bandwidth; Benchmarking; Computer hardware; Computer operating systems; Controllers; Data handling; Field programmable gate arrays (FPGA); Groupware; Integrated circuit design; Mapping; Open source software; Physical addresses; Benchmark applications; Collaborative softwares; Computing environments; Memory level parallelisms; Performance monitoring; Performance optimizations; Prototype implementations; Three-dimensional (3D) memory; Open systems
UNILOGIC: A novel architecture for highly parallel reconfigurable systems,2020,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093826030&doi=10.1145%2f3409115&partnerID=40&md5=130b39bc7b3ddf788c247970a6a833ce,"One of the main characteristics of High-performance Computing (HPC) applications is that they become increasingly performance and power demanding, pushing HPC systems to their limits. Existing HPC systems have not yet reached exascale performance mainly due to power limitations. Extrapolating from today's top HPC systems, about 100-200 MWatts would be required to sustain an exaflop-level of performance. A promising solution for tackling power limitations is the deployment of energy-efficient reconfigurable resources (in the form of Field-programmable Gate Arrays (FPGAs)) tightly integrated with conventional CPUs. However, current FPGA tools and programming environments are optimized for accelerating a single application or even task on a single FPGA device. In this work, we present UNILOGIC (Unified Logic), a novel HPC-tailored parallel architecture that efficiently incorporates FPGAs. UNILOGIC adopts the Partitioned Global Address Space (PGAS) model and extends it to include hardware accelerators, i.e., tasks implemented on the reconfigurable resources. The main advantages of UNILOGIC are that (i) the hardware accelerators can be accessed directly by any processor in the system, and (ii) the hardware accelerators can access any memory location in the system. In this way, the proposed architecture offers a unified environment where all the reconfigurable resources can be seamlessly used by any processor/operating system. The UNILOGIC architecture also provides hardware virtualization of the reconfigurable logic so that the hardware accelerators can be shared among multiple applications or tasks. The FPGA layer of the architecture is implemented by splitting its reconfigurable resources into (i) a static partition, which provides the PGAS-related communication infrastructure, and (ii) fixed-size and dynamically reconfigurable slots that can be programmed and accessed independently or combined together to support both fine and coarse grain reconfiguration.1 Finally, the UNILOGIC architecture has been evaluated on a custom prototype that consists of two 1U chassis, each of which includes eight interconnected daughter boards, called Quad-FPGA Daughter Boards (QFDBs); each QFDB supports four tightly coupled Xilinx Zynq Ultrascale+ MPSoCs as well as 64 Gigabytes of DDR4 memory, and thus, the prototype features a total of 64 Zynq MPSoCs and 1 Terabyte of memory. We tuned and evaluated the UNILOGIC prototype using both low-level (baremetal) performance tests, as well as two popular real-world HPC applications, one compute-intensive and one data-intensive. Our evaluation shows that UNILOGIC offers impressive performance that ranges from being 2.5 to 400 times faster and 46 to 300 times more energy efficient compared to conventional parallel systems utilizing only high-end CPUs, while it also outperforms GPUs by a factor ranging from 3 to 6 times in terms of time to solution, and from 10 to 20 times in terms of energy to solution. © 2020 ACM.",Accelerators; FPGA unification; partial reconfiguration; prototyping,Computation theory; Computer hardware; Energy efficiency; Field programmable gate arrays (FPGA); Memory architecture; Parallel architectures; Program processors; Reconfigurable architectures; Signal receivers; System-on-chip; Communication infrastructure; Hardware virtualization; High performance computing (HPC); Partitioned Global Address Space; Programming environment; Proposed architectures; Reconfigurable resources; Reconfigurable systems; Reconfigurable hardware
Accelerating FPGA Routing through Algorithmic Enhancements and Connection-aware Parallelization,2020,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093818859&doi=10.1145%2f3406959&partnerID=40&md5=c63393fbb3db0605208e01bf71e7d332,"Routing is a crucial step in Field Programmable Gate Array (FPGA) physical design, as it determines the routes of signals in the circuit, which impacts the design implementation quality significantly. It can be very time-consuming to successfully route all the signals of large circuits that utilize many FPGA resources. Attempts have been made to shorten the routing runtime for efficient design exploration while expecting high-quality implementations. In this work, we elaborate on the connection-based routing strategy and algorithmic enhancements to improve the serial FPGA routing. We also explore a recursive partitioning-based parallelization technique to further accelerate the routing process. To exploit more parallelism by a finer granularity in both spatial partitioning and routing, a connection-aware routing bounding box model is proposed for the source-sink connections of the nets. It is built upon the location information of each connection's source, sink, and the geometric center of the net that the connection belongs to, different from the existing net-based routing bounding box that covers all the pins of the entire net. We present that the proposed connection-aware routing bounding box is more beneficial for parallel routing than the existing net-based routing bounding box. The quality and runtime of the serial and multi-threaded routers are compared to the router in VPR 7.0.7. The large heterogeneous Titan23 designs that are targeted to a detailed representation of the Stratix IV FPGA are used for benchmarking. With eight threads, the parallel router using the connection-aware routing bounding box model reaches a speedup of 6.1× over the serial router in VPR 7.0.7, which is 1.24× faster than the one using the existing net-based routing bounding box model, while reducing the total wire-length by 10% and the critical path delay by 7%. © 2020 ACM.",algorithmic enhancements; connection-aware parallelization; connection-based routing; FPGA routing; partitioning-based; routing bounding box model; timing-driven,Integrated circuit design; Regression analysis; Routing algorithms; Critical path delays; Design implementation; High-quality implementation; Location information; Parallelization techniques; Recursive Partitioning; Routing strategies; Spatial partitioning; Field programmable gate arrays (FPGA)
FPGA Logic Block Architectures for Efficient Deep Learning Inference,2020,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091027276&doi=10.1145%2f3393668&partnerID=40&md5=86414b60f10a5708e97250aa20f0f007,"Reducing the precision of deep neural network (DNN) inference accelerators can yield large efficiency gains with little or no accuracy degradation compared to half or single precision floating-point by enabling more multiplication operations per unit area. A wide range of precisions fall on the pareto-optimal curve of hardware efficiency vs. accuracy with no single precision dominating, making the variable precision capabilities of FPGAs very valuable. We propose three types of logic block architectural enhancements and fully evaluate a total of six architectures that improve the area efficiency of multiplications and additions implemented in the soft fabric. Increasing the LUT fracturability and adding two adders to the ALM (4-bit Adder Double Chain architecture) leads to a 1.5× area reduction for arithmetic heavy machine learning (ML) kernels, while increasing their speed. In addition, this architecture also reduces the logic area of general applications by 6%, while increasing the critical path delay by only 1%. However, our highest impact option, which adds a 9-bit shadow multiplier to the logic clusters, reduces the area and critical path delay of ML kernels by 2.4× and 1.2×, respectively. These large gains come at a cost of 15% logic area increase for general applications.  © 2020 ACM.",CAD tools; Deep neural networks; FPGA,Adders; Computer circuits; Deep neural networks; Digital arithmetic; Efficiency; Field programmable gate arrays (FPGA); Network architecture; Pareto principle; Architectural enhancement; Critical path delays; FPGA logic blocks; General applications; Hardware efficiency; Multiplication operations; Pareto-optimal curves; Variable precision; Deep learning
Optimizing OpenCL-Based CNN Design on FPGA with Comprehensive Design Space Exploration and Collaborative Performance Modeling,2020,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091031122&doi=10.1145%2f3397514&partnerID=40&md5=137341deb1fd069d697693a7d6835105,"Recent success in applying convolutional neural networks (CNNs) to object detection and classification has sparked great interest in accelerating CNNs using hardware-like field-programmable gate arrays (FPGAs). However, finding an efficient FPGA design for a given CNN model and FPGA board is not trivial since a strong background in hardware design and detailed knowledge of the target board are required. In this work, we try to solve this problem by design space exploration with a collaborative framework. Our framework consists of three main parts: FPGA design generation, coarse-grained modeling, and fine-grained modeling. In the FPGA design generation, we propose a novel data structure, LoopTree, to capture the details of the FPGA design for CNN applications without writing down the source code. Different LoopTrees, which indicate different FPGA designs, are automatically generated in this process. A coarse-grained model will evaluate LoopTrees at the operation level, e.g., add, mult, and so on, so that the most efficient LoopTrees can be selected. A fine-grained model, which is based on the source code, will then refine the selected design in a cycle-accurate manner. A set of comprehensive OpenCL-based designs have been implemented on board to verify our framework. An average estimation error of 8.87% and 4.8% has been observed for our coarse-grained model and fine-grained model, respectively. This is much lower than the prevalent operation-statistics-based estimation, which is obtained according to a predefined formula for specific loop schedules.  © 2020 ACM.",CNN; design space exploration; hardware design; modeling,Convolutional neural networks; Field programmable gate arrays (FPGA); Object detection; Automatically generated; Coarse grained models; Collaborative framework; Collaborative performance; Comprehensive designs; Design space exploration; Estimation errors; Operation levels; Integrated circuit design
Partitioning and Scheduling with Module Merging on Dynamic Partial Reconfigurable FPGAs,2020,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090268760&doi=10.1145%2f3403702&partnerID=40&md5=5cff7a84a203b3d33a08bac7f272cabf,"Field programmable gate array (FPGA) is ubiquitous nowadays and is applied to many areas. Dynamic partial reconfiguration (DPR) is introduced to most modern FPGAs, enabling changing the function of a part of the FPGA by dynamically loading new bitstreams to the logic regions without affecting the function of other parts of the FPGA. However, delivering the powerful capacity of the DPR FPGA to the user depends on the efficient partitioning and scheduling technology. This article proposes the module merging technique for the partitioning and scheduling problem to reduce the reconfiguration overhead and improve the schedule performance. An exact approach based on the integer linear programming (ILP) for the partitioning and scheduling problem with module merging is proposed. The ILP-based approach is capable of solving the problem optimally, and can be used to further improve the performance of schedules produced by other non-optimal algorithms; however, it is time-consuming to solve large-scale problems. Therefore, a K-sliced-ILP algorithm based on the methodology of divide-and-conquer is proposed, which is able to reduce the time complexity significantly with the solution quality being degraded marginally. Experiments are carried out with a set of real-life applications, and the result demonstrates the effectiveness of the proposed methods.  © 2020 ACM.",Dynamic partial reconfiguration; integer linear programming; partitioning; scheduling,Binary sequences; Dynamic loads; Field programmable gate arrays (FPGA); Inductive logic programming (ILP); Integer programming; Merging; Scheduling; Divide and conquer; Dynamic partial reconfiguration; Integer Linear Programming; Large-scale problem; Merging techniques; Real-life applications; Reconfiguration overhead; Schedule performance; Reconfigurable hardware
Processing Grid-format Real-world Graphs on DRAM-based FPGA Accelerators with Application-specific Caching Mechanisms,2020,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091019198&doi=10.1145%2f3391920&partnerID=40&md5=974b75ac61c0e453655a4a40d0c08deb,"Graph processing is one of the important research topics in the big-data era. To build a general framework for graph processing by using a DRAM-based FPGA board with deep memory hierarchy, one of the reasonable methods is to partition a given big graph into multiple small subgraphs, represent the graph with a two-dimensional grid, and then process the subgraphs one after another to divide and conquer the whole problem. Such a method (grid-graph processing) stores the graph data in the off-chip memory devices (e.g., on-board or host DRAM) that have large storage capacities but relatively small bandwidths, and processes individual small subgraphs one after another by using the on-chip memory devices (e.g., FFs, BRAM, and URAM) that have small storage capacities but superior random access performances. However, directly exchanging graph (vertex and edge) data between the processing units in FPGA chip with slow off-chip DRAMs during grid-graph processing leads to limited performances and excessive data transmission amounts between the FPGA chip and off-chip memory devices. In this article, we show that it is effective in improving the performance of grid-graph processing on DRAM-based FPGA hardware accelerators by leveraging the flexibility and programmability of FPGAs to build application-specific caching mechanisms, which bridge the performance gaps between on-chip and off-chip memory devices, and reduce the data transmission amounts by exploiting the localities on data accessing. We design two application-specific caching mechanisms (i.e., vertex caching and edge caching) to exploit two types of localities (i.e., vertex locality and subgraph locality) that exist in grid-graph processing, respectively. Experimental results show that with the vertex caching mechanism, our system (named as FabGraph) achieves up to 3.1× and 2.5× speedups for BFS and PageRank, respectively, over ForeGraph when processing medium graphs stored in the on-board DRAM. With the edge caching mechanism, the extension of FabGraph (named as FabGraph+) achieves up to 9.96× speedups for BFS over FPGP when processing large graphs stored in the host DRAM.  © 2020 ACM.",graph analytics; Hardware accelerators; large graph processing,Data handling; Data transfer; Dynamic random access storage; Field programmable gate arrays (FPGA); Graph algorithms; Graph structures; Image coding; Transmissions; Application specific; Caching mechanism; Divide and conquer; Fpga accelerators; Processing units; Real-world graphs; Storage capacity; Two-dimensional grids; Graph theory
Reconfigurable Framework for Environmentally Adaptive Resilience in Hybrid Space Systems,2020,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091033229&doi=10.1145%2f3398380&partnerID=40&md5=490a38c026675bfdf3a30873f01d9ee6,"Due to ongoing innovations in both sensor technology and spacecraft autonomy, onboard space processing continues to be outpaced by the escalating computational demands required for next-generation missions. Commercial-off-the-shelf, hybrid system-on-chips, combining fixed-logic CPUs with reconfigurable-logic FPGAs, present numerous architectural advantages that address onboard computing challenges. However, commercial devices are highly susceptible to space radiation and require dependable computing strategies to mitigate radiation-induced single-event effects. Depending upon the mission, the dynamics of the near-Earth space-radiation environment expose spacecraft to radiation fluxes that can vary by several orders of magnitude. By adopting an adaptive approach to dependable computing, spacecraft computers can reconfigure system resources to efficiently accommodate changing environmental conditions to maximize system performance while satisfying availability constraints throughout the mission. In this article, we propose Hybrid, Adaptive, Reconfigurable Fault Tolerance (HARFT), a reconfigurable framework for environmentally adaptive resilience in hybrid space systems. Furthermore, we describe a methodology to model adaptive systems, represented as phased-mission systems using Markov chains, subject to the near-Earth space-radiation environment, using a combination of orbital perturbation, geomagnetic field, and single-event effect rate prediction tools. We apply this methodology to evaluate the HARFT architecture using various static and adaptive strategies for several orbital case studies and demonstrate the achievable performability gains.  © 2020 ACM.",dependability modeling; Environmentally adaptive computing; graceful degradation; single-event effects; space computing,Computation theory; Earth (planet); Fault tolerance; Geomagnetism; Hybrid systems; Markov chains; Orbits; Program processors; Radiation effects; Reconfigurable hardware; System-on-chip; Availability constraints; Computational demands; Dependable computing; Environmental conditions; Orbital perturbation; Phased mission systems; Reconfigurable logic; Single event effects; Adaptive systems
FPGADefender: Malicious Self-oscillator Scanning for Xilinx UltraScale + FPGAs,2020,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091045808&doi=10.1145%2f3402937&partnerID=40&md5=b22b499b64675c5fabe83a2c498d8b68,"Sharing configuration bitstreams rather than netlists is a very desirable feature to protect IP or to share IP without longer CAD tool processing times. Furthermore, an increasing number of systems could hugely benefit from serving multiple users on the same FPGA, for example, for resource pooling in cloud infrastructures. This article researches the threat that a malicious application can impose on an FPGA-based system in a multi-tenancy scenario from a hardware security point of view. In particular, this article evaluates the risk systematically for FPGA power-hammering through short-circuits and self-oscillating circuits, which potentially may cause harm to a system. This risk includes implementing, tuning, and evaluating all FPGA self-oscillators known from the literature but also developing a large number of new power-hammering designs that have not been considered before. Our experiments demonstrate that malicious circuits can be tuned to the point that just 3% of the logic available on an Ultra96 FPGA board can draw the power budget of the entire FPGA board. This fact suggests a waste power potential for datacenter FPGAs in the range of kilowatts. In addition to carefully analyzing FPGA hardware security threats, we present the FPGA virus scanner FPGADefender, which can detect (possibly) any self-oscillating FPGA circuit, as well as detecting short-circuits, high fanout nets, and a tapping onto signals outside the scope of a module for protecting data center FPGAs, such as Xilinx UltraScale+ devices at the bitstream level.  © 2020 ACM.",bitstream; Cloud computing; countermeasure; denial-of-service; FPGA; hardware security; mitigation; power-hammering; side-channel,Binary sequences; Budget control; Computer aided design; Computer viruses; Convolutional codes; Data privacy; Field programmable gate arrays (FPGA); Oscillators (electronic); Viruses; Cloud infrastructures; Desirable features; Malicious circuits; Resource pooling; Security threats; Self-oscillating; Self-oscillating circuits; Self-oscillators; Hardware security
Model-based Design of Hardware SC Polar Decoders for FPGAs,2020,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089501867&doi=10.1145%2f3391431&partnerID=40&md5=e69f7726880ae0b4044cc1c5e3516b86,"Polar codes are a new error correction code family that should be benchmarked and evaluated in comparison to LDPC and turbo-codes. Indeed, recent advances in the 5G digital communication standard recommended the use of polar codes in EMBB control channels. However, in many cases, the implementation of efficient FEC hardware decoders is challenging. Specialised knowledge is required to enable and facilitate testing, rapid design iterations, and fast prototyping. In this article, a model-based design methodology to generate efficient hardware SC polar code decoders is presented. With HLS design process and tools, we demonstrate how FPGA system designers can quickly develop complex hardware systems with good performances. The favourable impact of design space exploration is underlined on achievable performances when a relevant computation model is used. The flexibility of the abstraction layers is evaluated. Hardware decoder generation efficiency is assessed and compared to competing approaches. It is shown that the fine-tuning of computation parallelism, bit length, pruning level, and working frequency help to design high-throughput decoders with moderate hardware complexities. Decoding throughputs higher than 300 Mbps are achieved on an Xilinx Virtex-7 device and on an Altera Stratix IV device.  © 2020 ACM.",ECC; FPGA; High level synthesis; LDPC; model-based design; polar code; RTL,5G mobile communication systems; Abstracting; Decoding; Digital communication systems; Error correction; Field programmable gate arrays (FPGA); Abstraction layer; Achievable performance; Computation model; Design space exploration; Digital communications; Error correction codes; Hardware complexity; Model- based designs; Integrated circuit design
VTR 8: High-performance CAD and Customizable FPGA Architecture Modelling,2020,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088052686&doi=10.1145%2f3388617&partnerID=40&md5=ba0a2b3a4002462c710b9faaebae812b,"Developing Field-programmable Gate Array (FPGA) architectures is challenging due to the competing requirements of various application domains and changing manufacturing process technology. This is compounded by the difficulty of fairly evaluating FPGA architectural choices, which requires sophisticated high-quality Computer Aided Design (CAD) tools to target each potential architecture. This article describes version 8.0 of the open source Verilog to Routing (VTR) project, which provides such a design flow. VTR 8 expands the scope of FPGA architectures that can be modelled, allowing VTR to target and model many details of both commercial and proposed FPGA architectures. The VTR design flow also serves as a baseline for evaluating new CAD algorithms. It is therefore important, for both CAD algorithm comparisons and the validity of architectural conclusions, that VTR produce high-quality circuit implementations. VTR 8 significantly improves optimization quality (reductions of 15% minimum routable channel width, 41% wirelength, and 12% critical path delay), run-time (5.3× faster) and memory footprint (3.3× lower). Finally, we demonstrate VTR is run-time and memory footprint efficient, while producing circuit implementations of reasonable quality compared to highly-tuned architecture-specific industrial tools - showing that architecture generality, good implementation quality, and run-time efficiency are not mutually exclusive goals.  © 2020 ACM.",Computer aided design (CAD); electronic design automation (EDA); field programmable gate array (FPGA); packing; placement; routing; verilog to routing (VTR); versatile place and route (VPR),Field programmable gate arrays (FPGA); Integrated circuit design; Memory architecture; Algorithm comparison; Circuit implementation; Competing requirements; Computer aided design tools; Critical path delays; Manufacturing process; Optimization quality; Run-time efficiency; Computer aided logic design
Substream-Centric Maximum Matchings on FPGA,2020,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089553276&doi=10.1145%2f3377871&partnerID=40&md5=7770655a6ab6ea420534376b13803138,"Developing high-performance and energy-efficient algorithms for maximum matchings is becoming increasingly important in social network analysis, computational sciences, scheduling, and others. In this work, we propose the first maximum matching algorithm designed for FPGAs; it is energy-efficient and has provable guarantees on accuracy, performance, and storage utilization. To achieve this, we forego popular graph processing paradigms, such as vertex-centric programming, that often entail large communication costs. Instead, we propose a substream-centric approach, in which the input stream of data is divided into substreams processed independently to enable more parallelism while lowering communication costs. We base our work on the theory of streaming graph algorithms and analyze 14 models and 28 algorithms. We use this analysis to provide theoretical underpinning that matches the physical constraints of FPGA platforms. Our algorithm delivers high performance (more than 4× speedup over tuned parallel CPU variants), low memory, high accuracy, and effective usage of FPGA resources. The substream-centric approach could easily be extended to other algorithms to offer low-power and high-performance graph processing on FPGAs.  © 2020 ACM.",energy-efficient graph processing; Graph computations; streaming graph processing,Computation theory; Data streams; Digital storage; Energy efficiency; Field programmable gate arrays (FPGA); Graph algorithms; Graph theory; Communication cost; Computational science; Energy efficient algorithms; Graph processing; Maximum matching algorithms; Maximum matchings; Physical constraints; Storage utilization; Computational efficiency
HopliteBuf: Network calculus-based design of FPGA NOCs with provably stall-free FIFOs,2020,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079896122&doi=10.1145%2f3375899&partnerID=40&md5=17c580df04cf86a97838220240513ad4,"HopliteBuf is a deflection-free, low-cost, and high-speed FPGA overlay Network-on-chip (NoC) with stall-free buffers. It is an FPGA-friendly 2D unidirectional torus topology built on top of HopliteRT overlay NoC. The stall-free buffers in HopliteBuf are supported by static analysis tools based on network calculus that help determine worst-case FIFO occupancy bounds for a prescribed workload. We implement these FIFOs using cheap LUT SRAMs (Xilinx SRL32s and Intel MLABs) to reduce cost. HopliteBuf is a hybrid microarchitecture that combines the performance benefits of conventional buffered NoCs by using stall-free buffers with the cost advantages of deflection-routed NoCs by retaining the lightweight unidirectional torus topology structure. We present two design variants of the HopliteBuf NoC: (1) single corner-turn FIFO (W → S) and (2) dual corner-turn FIFO (W → S + N). The single corner-turn (W → S) design is simpler and only introduces a buffering requirement for packets changing dimension from the X ring to the downhill Y ring (or West to South). The dual corner-turn variant requires two FIFOs for turning packets going downhill (W → S) as well as uphill (W → N). The dual corner-turn design overcomes the mathematical analysis challenges associated with single corner-turn designs for communication workloads with cyclic dependencies between flow traversal paths at the expense of a small increase in resource cost. Our static analysis delivers bounds that are not only better (in latency) than HopliteRT but also tighter by 2 − 3×. Across 100 randomly generated flowsets mapped to a 5×5 system size, HopliteBuf is able to route a larger fraction of these flowsets with <128-deep FIFOs, boost worst-case routing latency by ≈ 2× for mutually feasible flowsets, and support a 10% higher injection rate than HopliteRT. At 20% injection rates, HopliteRT is only able to route 1-2% of the flowsets, while HopliteBuf can deliver 40-50% sustainability. When compared to the W → Sbkp backpressure-based router, we observe that our HopliteBuf solution offers 25-30% better feasibility at 30-40% lower LUT cost. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",FPGA overlay NoC; Network calculus; Stall-free buffers,Calculations; Cost benefit analysis; Field programmable gate arrays (FPGA); Network-on-chip; Routers; Static analysis; Topology; Buffering requirements; Cyclic dependencies; Injection rates; Mathematical analysis; Micro architectures; Network calculus; Performance benefits; Topology structure; Integrated circuit design
Kernel normalised least mean squares with delayed model adaptation,2020,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079835127&doi=10.1145%2f3376924&partnerID=40&md5=08dc4829e6c9f1e7a48a8bb27c04cc2c,"Kernel adaptive filters (KAFs) are non-linear filters which can adapt temporally and have the additional benefit of being computationally efficient through use of the “kernel trick”. In a number of real-world applications, such as channel equalisation, the non-linear mapping provides significant improvements over conventional linear techniques such as the least mean squares (LMS) and recursive least squares (RLS) algorithms. Prior works have focused mainly on the theory and accuracy of KAFs, with little research on their implementations. This article proposes several variants of algorithms based on the kernel normalised least mean squares (KNLMS) algorithm which utilise a delayed model update to minimise dependencies. Subsequently, this work proposes corresponding hardware architectures which utilise this delayed model update to achieve high sample rates and low latency while also providing high modelling accuracy. The resultant delayed KNLMS (DKNLMS) algorithms can achieve clock rates up to 12× higher than the standard KNLMS algorithm, with minimal impact on accuracy and stability. A system implementation achieves 250 GOps/s and a throughput of 187.4 MHz on an Ultra96 board with 1.8× higher throughput than previous state of the art. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Field programmable gate array; Kernel adaptive filters; Kernel normalised least mean squares,Adaptive filtering; Field programmable gate arrays (FPGA); Computationally efficient; Hardware architecture; Kernel adaptive filters; Least mean square (LMS); Nonlinear mappings; Normalised least mean square; Recursive least squares algorithms; System implementation; Adaptive filters
Fast turnaround HLS debugging using dependency analysis and debug overlays,2020,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079420719&doi=10.1145%2f3372490&partnerID=40&md5=1e9c5b99b428c32ba35393508ccf6837,"High-level synthesis (HLS) has gained considerable traction over recent years, as it allows for faster development and verification of hardware accelerators than traditional RTL design. While HLS allows for most bugs to be caught during software verification, certain non-deterministic or data-dependent bugs still require debugging the actual hardware system during execution. Recent work has focused on techniques to allow designers to perform in-system debug of HLS circuits in the context of the original software code; however, like RTL debug, the user must still determine the root cause of a bug using small execution traces, with lengthy debug turns. In this work, we demonstrate techniques aimed at reducing the time HLS designers spend performing in-system debug. Our approaches consist of performing data dependency analysis to guide the user in selecting which variables are observed by the debug instrumentation, as well as an associated debug overlay that allows for rapid reconfiguration of the debug logic, enabling rapid switching of variable observation between debug iterations. In addition, our overlay provides additional debug capability, such as selective function tracing and conditional buffer freeze points. We explore the area overhead of these different overlay features, showing a basic overlay with only a 1.7% increase in area overhead from the baseline debug instrumentation, while a deluxe variant offers 2×-7× improvement in trace buffer memory utilization with conditional buffer freeze support. © 2020 Association for Computing Machinery.",Debug; Dependency analysis; FPGA; High-level synthesis,Computer debugging; Field programmable gate arrays (FPGA); High level synthesis; Verification; Data-dependency analysis; Debug; Dependency analysis; Execution trace; Hardware accelerators; Hardware system; In-system debug; Software verification; Program debugging
Feel free to interrupt: Safe task stopping to enable FPGA checkpointing and context switching,2020,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079418254&doi=10.1145%2f3372491&partnerID=40&md5=a1dcb4abbc5919db7662d5d56224c440,"Saving and restoring an FPGA task state in an orderly manner is essential to enable hardware checkpointing, which is highly desirable to improve the ability to debug cloud-scale hardware services, and context switching, which allows multiple users to share FPGA resources. However, these features require task interruption, and stopping a task at an arbitrary time can cause several hazards including deadlock and data loss. In this article, we build a context saving and restoring simulator to simulate and identify these hazards. In addition, we derive design rules that should be followed to achieve safe task interruption. Finally, we propose task wrappers that can be placed around an FPGA task to implement these rules. The timing and area overheads added by these wrappers are very small; they add 1.8% area and no timing overhead to a full Memcached system. Taken together, these design rules and wrappers enable safe checkpointing and context switching in a wide variety of FPGA tasks, including those with multiple clocks, multi-cycle I/O transactions, and interface dependencies. © 2020 Association for Computing Machinery.",Checkpointing; Context switching; Debugging; FPGA; Interrupt,Computer debugging; Hazards; Integrated circuit design; Switching; Arbitrary time; Area overhead; Check pointing; Context switching; Design rules; Interrupt; Multi cycle; Multiple user; Field programmable gate arrays (FPGA)
In-circuit debugging with dynamic reconfiguration of FPGA interconnects,2020,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079423842&doi=10.1145%2f3375459&partnerID=40&md5=d24f1edb694299430845a135e8077031,"In this work, a novel method for in-circuit debugging on FPGAs is introduced that allows the insertion of low-overhead debugging infrastructure by exploiting the technique of parameterized configurations. This allows the parameterization of the LUTs and the routing infrastructure to create a virtual network of debugging multiplexers. It aims to facilitate debugging, to increase the internal signal observability, and to reduce the debugging (area and reconfiguration) overhead. Signal ranking techniques are also introduced that classify signals that can be traced during debug. Finally, the results of the method are presented and compared with a commercial tool. The area and time results and the tradeoffs between internal signal observability and area and reconfiguration overhead are also explored. © 2020 Association for Computing Machinery.",Debugging; FPGA; Parameterized configuration; Post-silicon debug; Reconfiguration; Verification,Computer debugging; Dynamic models; Field programmable gate arrays (FPGA); Observability; Parameterization; Timing circuits; Verification; Dynamic re-configuration; FPGA interconnects; In-circuit debugging; Parameterized; Post-silicon debug; Reconfiguration; Reconfiguration overhead; Routing infrastructure; Reconfigurable hardware
FOS: A Modular FPGA Operating System for DynamicWorkloads,2020,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093862568&doi=10.1145%2f3405794&partnerID=40&md5=7f9b7f49f8ff7e71bb2d95fa2c9f8576,"With FPGAs now being deployed in the cloud and at the edge, there is a need for scalable design methods that can incorporate the heterogeneity present in the hardware and software components of FPGA systems. Moreover, these FPGA systems need to be maintainable and adaptable to changing workloads while improving accessibility for the application developers. However, current FPGA systems fail to achieve modularity and support for multi-tenancy due to dependencies between system components and the lack of standardised abstraction layers. To solve this, we introduce a modular FPGA operating system - FOS, which adopts a modular FPGA development flow to allow each system component to be changed and be agnostic to the heterogeneity of EDA tool versions, hardware and software layers. Further, to dynamically maximise the utilisation transparently from the users, FOS employs resource-elastic scheduling to arbitrate the FPGA resources in both time and spatial domain for any type of accelerators. Our evaluation on different FPGA boards shows that FOS can provide performance improvements in both single-tenant and multi-tenant environments while substantially reducing the development time and, at the same time, improving flexibility. © 2020 ACM.",dynamic workloads; FPGA; FPGA shell; high-level synthesis; modular development; operating system; resource-elasticity; runtime systems,Computer networks; Computer science; Abstraction layer; Application developers; Changing workload; Development flow; Development time; Hardware and software; Hardware and software components; System components; Field programmable gate arrays (FPGA)
RAiSD-X: A fast and accurate FPGA system for the detection of positive selection in thousands of genomes,2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077356717&doi=10.1145%2f3364225&partnerID=40&md5=b1e5828ba64a8b16fc59a08ebeee2734,"Detecting traces of positive selection in genomes carries theoretical significance and has practical applications from shedding light on the forces that drive adaptive evolution to the design of more effective drug treatments. The size of genomic datasets currently grows at an unprecedented pace, fueled by continuous advances in DNA sequencing technologies, leading to ever-increasing compute and memory requirements for meaningful genomic analyses. Themajority of existing methods for positive selection detection either are not designed to handle whole genomes or scale poorly with the sample size; they inevitably resort to a runtime versus accuracy tradeoff, raising an alarming concern for the feasibility of future large-scale scans. To this end, we present RAiSD-X, a high-performance system that relies on a decoupled access-execute processing paradigm for efficient FPGA acceleration and couples a novel, to our knowledge, sliding-window algorithm for the recently introduced μ statistic with a mutation-driven hashing technique to rapidly detect patterns in the data. RAiSD-X achieves up to three orders of magnitude faster processing than widely used software implementations, and more importantly, it can exhaustively scan thousands of human chromosomes in minutes, yielding a scalable full-system solution for future studies of positive selection in species of flora and fauna. © 2019 Copyright held by the owner/author(s).",Decoupled access-execute architecture; Hardware accelerator; Positive selection; Selective sweep,Digital storage; DNA sequences; Gene encoding; Hardware accelerators; High performance systems; Memory requirements; Positive selection; Selective sweep; Sliding window algorithms; Software implementation; Three orders of magnitude; Field programmable gate arrays (FPGA)
DSL-based hardware generation with scala: Example fast fourier transforms and sorting networks,2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077391875&doi=10.1145%2f3359754&partnerID=40&md5=4cdbb6bac7217169669f55d45a445e14,"We present a hardware generator for computationswith regular structure including the fast Fourier transform (FFT), sorting networks, and others. The input of the generator is a high-level description of the algorithm; the output is a token-based, synchronized design in the form of RTL-Verilog. Building on prior work, the generator uses several layers of domain-specific languages (DSLs) to represent and optimize at different levels of abstraction to produce a RAM- and area-efficient hardware implementation. Two of these layers and DSLs are novel. The first one allows the use and domain-specific optimization of state-of-the-art streaming permutations. The second DSL enables the automatic pipelining of a streaming hardware dataflow and the synchronization of its data-independent control signals. The generator including the DSLs are implemented in Scala, leveraging its type system, and uses concepts from lightweight modular staging (LMS) to handle the constraints of streaming hardware. Particularly, these concepts offer genericity over hardware number representation, including seamless switching between fixed-point arithmetic and FloPoCo generated IEEE floating-point operators, while ensuring type-safety. We show benchmarks of generated FFTs, sorting networks, and Walsh-Hadamard transforms that outperform prior generators. © 2019 Association for Computing Machinery.",Fast Fourier transform; Hardware generation; IP core; Scala; Sorting network; Streaming datapaths; Walsh-Hadamard transform,Computer hardware description languages; Digital subscriber lines; Fixed point arithmetic; Hadamard transforms; Intellectual property core; Problem oriented languages; Signal receivers; Sorting; Walsh transforms; Data-paths; Domain specific languages; Hardware implementations; High level description; Number representation; Scala; Sorting network; Walsh Hadamard Transforms; Fast Fourier transforms
GrAVF-M: Graph processing system generation for multi-FPGA platforms,2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077790628&doi=10.1145%2f3357596&partnerID=40&md5=14c6c0ae6a139fedc89eb2c9e89234e0,"Due to the irregular nature of connections in most graph datasets, partitioning graph analysis algorithms across multiple computational nodes that do not share a common memory inevitably leads to large amounts of interconnect traffic. Previous research has shown that FPGAs can outcompete software-based graph processing in shared memory contexts, but it remains an open question if this advantage can be maintained in distributed systems. In this work, we present GraVF-M, a framework designed to ease the implementation of FPGA-based graph processing accelerators for multi-FPGA platforms with distributed memory. Based on a lightweight description of the algorithm kernel, the framework automatically generates optimized RTL code for the whole multi-FPGA design. We exploit an aspect of the programming model to present a familiar message-passing paradigm to the user, while under the hood implementing a more efficient architecture that can reduce the necessary inter-FPGA network traffic by a factor equal to the average degree of the input graph. A performance model based on a theoretical analysis of the factors influencing performance serves to evaluate the efficiency of our implementation. With a throughput of up to 5.8GTEPS (billions of traversed edges per second) on a 4-FPGA system, the designs generated by GraVF-M compare favorably to state-of-the-art frameworks from the literature and reach 94% of the projected performance limit of the system. © 2019 Copyright held by the owner/author(s).",FPGA; Graph processing; GraVF-M; Multi-FPGA architecture; Performance modelling; Vertex centric,Graph theory; Large dataset; Memory architecture; Message passing; Network architecture; Graph processing; GraVF-M; Multi-FPGA; Performance modelling; Vertex centric; Field programmable gate arrays (FPGA)
"The FPOA, a medium-grained reconfigurable architecture for high-level synthesis",2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075623883&doi=10.1145%2f3340556&partnerID=40&md5=df1f5093562ec3623a618d96c4043cbb,"In this article, we present a novel type of medium-grained reconfigurable architecture that we term the Field Programmable Operation Array (FPOA). This device has been designed specifically for the implementation of HLS-generated circuitry. At the core of the FPOA is the OP-block. Unlike a standard LUT, an OP-block performs multi-bit operations through gate-based logic structures, translating into greater speed and efficiency in digital circuit implementation. Our device is not optimized for a specific application domain. Rather, we have created a device that is optimized for a specific circuit structure, namely those generated by HLS. This gives the FPOA a significant advantage as it can be used across all application domains. In this work, we add support for both distributed and block memory to the FPOA architecture. Experimental results show up to a 13.5× reduction in logic area and a 9.5× reduction in critical path delay for circuit implementation using the FPOA compared to a standard FPGA. © 2019 Copyright held by the owner/author(s).",FPGA; HLS; Reconfigurable architectures,Computer circuits; Delay circuits; Field programmable gate arrays (FPGA); High level synthesis; Block memory; Circuit implementation; Circuit structures; Critical path delays; Field programmables; Logic structures; Multi-bits; Reconfigurable architectures
A design flow engine for the support of customized dynamic high level synthesis flows,2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075609342&doi=10.1145%2f3356475&partnerID=40&md5=5714cfd9d1577120643e7091783da7b2,"High Level Synthesis is a set of methodologies aimed at generating hardware descriptions starting from specifications written in high-level languages.While thesemethodologies share different elements with traditional compilation flows, there are characteristics of the addressed problem which require ad hoc management. In particular, differently from most of the traditional compilation flows, the complexity and the execution time of the High Level Synthesis techniques are much less relevant than the quality of the produced results. For this reason, fixed-point analyses, as well as successive refinement optimizations, can be accepted, provided that they can improve the quality of the generated designs. This article presents a design flow engine for the description and the execution of complex and customized synthesis flows. It supports dynamic addition of passes and dependencies, cyclic dependencies, and selective pass invalidation. Experimental results show the benefits of such type of design flows with respect to static linear design flows when applied to High Level Synthesis. © 2019 Association for Computing Machinery.",Compilation steps; High level synthesis,Engines; Compilation steps; Cyclic dependencies; Design flows; Fixed point analysis; Hardware descriptions; It supports; Linear design; Successive refinement; High level synthesis
Unrolling ternary neural networks,2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075605494&doi=10.1145%2f3359983&partnerID=40&md5=cf7d206058fb1cb12b25752fbed5b99e,"The computational complexity of neural networks for large-scale or real-time applications necessitates hardware acceleration. Most approaches assume that the network architecture and parameters are unknown at design time, permitting usage in a large number of applications. This article demonstrates, for the case where the neural network architecture and ternary weight values are known a priori, that extremely high throughput implementations of neural network inference can be made by customising the datapath and routing to remove unnecessary computations and data movement. This approach is ideally suited to FPGA implementations as a specialized implementation of a trained network improves efficiency while still retaining generality with the reconfigurability of an FPGA. A VGG-style network with ternary weights and fixed point activations is implemented for the CIFAR10 dataset on Amazon's AWS F1 instance. This article demonstrates how to remove 90% of the operations in convolutional layers by exploiting sparsity and compile-time optimizations. The implementation in hardware achieves 90.9 ± 0.1% accuracy and 122k frames per second, with a latency of only 29μs, which is the fastest CNN inference implementation reported so far on an FPGA. © 2019 Copyright held by the owner/author(s).",Low-precision machine learning; Sparse matrix operations; Ternary neural networks,Field programmable gate arrays (FPGA); FPGA implementations; Frames per seconds; Hardware acceleration; High throughput implementation; Network inference; Precision machines; Real-time application; Sparse matrices; Network architecture
Introduction to the Special Section on Security in FPGA-Accelerated Cloud and Datacenters,2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075566304&doi=10.1145%2f3352060&partnerID=40&md5=d8d09b66a76a6216036ce5fc49b3d380,[No abstract available],,
Leakier Wires: Exploiting FPGA Long Wires for Covert-and Side-channel Attacks,2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075553799&doi=10.1145%2f3322483&partnerID=40&md5=a485f84540cd21ab49e126c1181e217e,"In complex FPGA designs, implementations of algorithms and protocols from third-party sources are common. However, the monolithic nature of FPGAs means that all sub-circuits share common on-chip infrastructure, such as routing resources. This presents an attack vector for all FPGAs that contain designs from multiple vendors, especially for FPGAs used in multi-Tenant cloud environments, or integrated intomulti-core processors. In this article, we show that ""long"" routing wires present a new source of information leakage on FPGAs, by influencing the delay of adjacent long wires.We show that the effect is measurable for both static and dynamic signals and that it can be detected using small on-board circuits. We characterize the channel in detail and show that it is measurable even when multiple competing circuits (including multiple long-wire transmitters) are present and can be replicated on different generations and families of Xilinx devices (Virtex 5, Virtex 6, Artix 7, and Spartan 7). We exploit the leakage to create a covert channel with 6kbps of bandwidth and 99.9% accuracy, and a side channel, which can recover signals kept constant for only 1.3us, with an accuracy of more than 98.4%. Finally, we propose countermeasures to reduce the impact of this leakage. © 2019 Association for Computing Machinery. All rights reserved.",crosstalk; FPGA covert channel; information leakage; long-wire delay,Crosstalk; Delay circuits; Field programmable gate arrays (FPGA); Wire; Algorithms and protocols; Cloud environments; Core processors; Covert channels; Information leakage; Multiple vendors; Routing resources; Wire delays; Side channel attack
FRoC 2.0: Automatic BRAM and logic testing to enable dynamic voltage scaling for FPGA applications,2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072165112&doi=10.1145%2f3354188&partnerID=40&md5=72b38df441d39e1abac4a5498996e6b0,"In earlier technology nodes, FPGAs had low power consumption compared to other compute chips such as CPUs and GPUs. However, in the 14nm technology node, FPGAs are consuming unprecedented power in the 100+W range, making power consumption a pressing concern. To reduce FPGA power consumption, several researchers have proposed deploying dynamic voltage scaling. While the previously proposed solutions show promising results, they have dificulty guaranteeing safe operation at reduced voltages for applications that use the FPGA hard blocks. In this work, we present the first DVS solution that is able to fully handle FPGA applications that use BRAMs. Our solution not only robustly tests the soft logic component of the application but also tests all components connected to the BRAMs. We extend a previously proposed CAD tool, FRoC, to automatically generate calibration bitstreams that are used to measure the application's critical path delays on silicon. The calibration bitstreams also include testers that ensure all used SRAM cells operate safely while scaling Vdd. We experimentally show that using our DVS solution we can save 32% of the total power consumed by a discrete Fourier transform application running with the fixed nominal supply voltage and clocked at the Fmax reported by static timing analysis. © 2019 Association for Computing Machinery.",BRAM interface logic delay testing; BRAM testing; FPGA DVS,Binary sequences; Calibration; Computer aided design; Computer circuits; Discrete Fourier transforms; Electric power utilization; Program processors; Static random access storage; Voltage scaling; Critical path delays; FPGA applications; Interface logic; Low-power consumption; Safe operation; Static timing analysis; Supply voltages; Technology nodes; Field programmable gate arrays (FPGA)
Distributed inference over decision tree ensembles on clusters of FPGAs,2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072173907&doi=10.1145%2f3340263&partnerID=40&md5=30def88d9261e7aeb9b06ab0a8f86716,"Given the growth in data inputs and application complexity, it is often the case that a single hardware accelerator is not enough to solve a given problem. In particular, the computational demands and I/O of many tasks in machine learning often require a cluster of accelerators to make a relevant difference in performance. In this article, we explore the eficient construction of FPGA clusters using inference over Decision Tree Ensembles as the target application. The article explores several levels of the problem: (1) a lightweight inter-FPGA communication protocol and routing layer to facilitate the communication between the different FPGAs, (2) the data partitioning and distribution strategies maximizing performance, (3) and an in depth analysis on how applications can be eficiently distributed over such a cluster. The experimental analysis shows that the resulting system can support inference over decision tree ensembles at a significantly higher throughput than that achieved by existing systems. © 2019 Association for Computing Machinery.",Decision trees; Distributed systems; FPGA cluster; Inference; Intel HARP; Machine learning; Microsoft catapult,Decision trees; Learning systems; Machine learning; Application complexity; Computational demands; Distributed systems; Distribution strategies; Experimental analysis; Inference; Intel HARP; MicroSoft; Field programmable gate arrays (FPGA)
Recent Attacks and Defenses on FPGA-based Systems,2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074241759&doi=10.1145%2f3340557&partnerID=40&md5=fdf9a4602c902ac74697b2011280c398,"Field-programmable gate array (FPGA) is a kind of programmable chip that is widely used in many areas, including automotive electronics, medical devices, military and consumer electronics, and is gaining more popularity. Unlike the application specific integrated circuits (ASIC) design, an FPGA-based system has its own supply-chain model and design flow, which brings interesting security and trust challenges. In this survey, we review the security and trust issues related to FPGA-based systems from the market perspective, where we model the market with the following parties: FPGA vendors, foundries, IP vendors, EDA tool vendors, FPGA-based system developers, and end-users. For each party, we show the security and trust problems they need to be aware of and the associated solutions that are available.We also discuss some challenges and opportunities in the security and trust of FPGA-based systems used in large-scale cloud and datacenters. © 2019 Association for Computing Machinery. All rights reserved.",FPGA security; FPGA trust; hardware security,Commerce; Hardware security; Integrated circuit design; Supply chains; Design flows; FPGA security; FPGA vendors; Medical Devices; Programmable chips; Security and trusts; Supply chain modeling; System developers; Field programmable gate arrays (FPGA)
Optimizing Bit-Serial Matrix Multiplication for Reconfigurable Computing,2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075545551&doi=10.1145%2f3337929&partnerID=40&md5=8358128121468f688387603aa85d6ac9,"Matrix-matrix multiplication is a key computational kernel for numerous applications in science and engineering, with ample parallelism and data locality that lends itself well to high-performance implementations. Many matrix multiplication-dependent applications can use reduced-precision integer or fixed-point representations to increase their performance and energy efficiency while still offering adequate quality of results. However, precision requirements may vary between different application phases or depend on input data, rendering constant-precision solutions ineffective. BISMO, a vectorized bit-serial matrix multiplication overlay for reconfigurable computing, previously utilized the excellent binary-operation performance of FPGAs to offer a matrix multiplication performance that scales with required precision and parallelism. We show how BISMO can be scaled up on Xilinx FPGAs using an arithmetic architecture that better utilizes six-input LUTs. The improved BISMO achieves a peak performance of 15.4 binary TOPS on the Ultra96 board with a Xilinx UltraScale+ MPSoC. © 2019 Association for Computing Machinery. All rights reserved.",Bit serial; FPGA; matrix multiplication; overlay,Energy efficiency; Field programmable gate arrays (FPGA); Reconfigurable architectures; System-on-chip; Bit-serial; Computational kernels; High performance implementations; Matrix matrix multiplications; MAtrix multiplication; overlay; Reconfigurable computing; Science and engineering; Matrix algebra
Mitigating Electrical-level Attacks towards Secure Multi-Tenant FPGAs in the Cloud,2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070723982&doi=10.1145%2f3328222&partnerID=40&md5=395d69fb413da6ff7a66cb68dfe5d903,"A rising trend is the use of multi-Tenant FPGAs, particularly in cloud environments, where partial access to the hardware is given to multiple third parties. This leads to new types of attacks in FPGAs, which operate not only on the logic level, but also on the electrical level through the common power delivery network. Since FPGAs are configured from the software-side, attackers are enabled to launch hardware attacks from software, impacting the security of an entire system. In this article, we show the first attempt of a countermeasure against attacks on the electrical level, which is based on a bitstream checking methodology. Bitstreams are translated back into flat technology mapped netlists, which are then checked for properties that indicate potential malicious runtime behavior of FPGA logic. Our approach can provide a metric of potential risk of the FPGA bitstream being used in active fault or passive side-channel attacks against other users of the FPGA fabric or the entire SoC platform. © 2019 Copyright held by the owner/author(s).",Bitstream; Countermeasure; Fault attack; FPGA; Mitigation; Security; Side-channel attack,Binary sequences; Computer circuits; Electric power transmission; Field programmable gate arrays (FPGA); System-on-chip; Bit stream; Cloud environments; Countermeasure; Electrical levels; Mitigation; Power delivery network; Runtime behaviors; Security; Side channel attack
A Protection and Pay-per-use Licensing Scheme for On-cloud FPGA Circuit IPs,2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070694380&doi=10.1145%2f3329861&partnerID=40&md5=860ef862e502a7708b456c6e1640f1bd,"Using security primitives, a novel scheme for licensing hardware intellectual properties (HWIPs) on Field Programmable Gate Arrays (FPGAs) in public clouds is proposed. The proposed scheme enforces a pay-peruse model, allows HWIP's installation only on specific on-cloud FPGAs, and efficiently protects the HWIPs from being cloned, reverse engineered, or used without the owner's authorization by any party, including a cloud insider. It also provides protection for the users' designs integrated with the HWIP on the same FPGA. This enables cloud tenants to license HWIPs in the cloud from the HWIP vendors at a relatively low price based on usage instead of paying the expensive unlimited HWIP license fee. The scheme includes a protocol for FPGA authentication, HWIP secure decryption, and usage by the clients without the need for the HWIP vendor to be involved or divulge their secret keys. A complete prototype test-bed implementation showed that the proposed scheme is very feasible with relatively low resource utilization. Experiments also showed that a HWIP could be licensed and set up in the on-cloud FPGA in 0.9s. This is 15 times faster than setting up the same HWIP from outside the cloud, which takes about 14s based on the average global Internet speed. © 2019 Association for Computing Machinery.",Cryptographic protocols and algorithms; FPGAs; Hardware cloning and reverse engineering; Hardware IPs; Hardware protection; Hardware security; Key management,Clone cells; Cloning; Cryptography; Hardware security; Reverse engineering; Timing circuits; Cryptographic protocols; Global Internet; Hardware protection; Key management; Prototype tests; Public clouds; Resource utilizations; Security primitives; Field programmable gate arrays (FPGA)
Novel Congestion-estimation and Routability-prediction Methods based on Machine Learning for Modern FPGAs,2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070674961&doi=10.1145%2f3337930&partnerID=40&md5=1370d81d8697d5c1b0b9f9b1d53dbe8e,"Effectively estimating andmanaging congestion during placement can save substantial placement and routing runtime. In this article, we present a machine-learning model for accurately and efficiently estimating congestion during FPGA placement. Comparedwith the state-of-The-Art machine-learning congestion-estimation model, our results show a 25% improvement in prediction accuracy. This makes our model competitive with congestion estimates produced using a global router. However, our model runs, on average, 291 faster than the global router. Overall, we are able to reduce placement runtimes by 17% and router runtimes by 19%. An additional machine-learning model is also presented that uses the output of the first congestion-estimation model to determine whether or not a placement is routable. This second model has an accuracy in the range of 93% to 98%, depending on the classification algorithm used to implement the learning model, and runtimes of a few milliseconds, thus making it suitable for inclusion in any placer with no worry of additional computational overhead. © 2019 Association for Computing Machinery.",Congestion estimation; Machine learning; Routability prediction; Xilinx UltraScale FPGA,Field programmable gate arrays (FPGA); Forecasting; Learning systems; Classification algorithm; Computational overheads; Congestion estimation; Machine learning models; Placement and routing; Prediction accuracy; Prediction methods; Routability; Machine learning
"Automata processing in reconfigurable architectures: In-the-cloud deployment, cross-platform evaluation, and fast symbol-only reconfiguration",2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074859065&doi=10.1145%2f3314576&partnerID=40&md5=40fe19c1aefcc1c8072d975bef1b133b,"We present a general automata processing framework on FPGAs,which generates an RTL kernel for automata processing together with an AXI and PCIe based I/O circuitry. We implement the framework on both local nodes and cloud platforms (Amazon AWS and Nimbix) with novel features. A full performance comparison of the proposed framework is conducted against state-of-the-art automata processing engines on CPUs, GPUs, and Micron's Automata Processor using the ANMLZoo benchmark suite and some real-world datasets. Results show that FPGAs enable extremely high-throughput automata processing compared to von Neumann architectures. We also collect the resource utilization and power consumption on the two cloud platforms, and find that the I/O circuitry consumes most of the hardware resources and power. Furthermore, we propose a fast, symbol-only reconfiguration mechanism based on the framework for large pattern sets that cannot fit on a single device and need to be partitioned. The proposed method supports multiple passes of the input stream and reduces the re-compilation cost from hours to seconds. © 2019 Association for Computing Machinery.",Acceleration; Automata processing; Cloud-computing; FPGA,Acceleration; Benchmarking; Cloud computing; Field programmable gate arrays (FPGA); Green computing; Program processors; Reconfigurable architectures; Cloud deployments; Hardware resources; Neumann architecture; Performance comparison; Processing engine; Real-world datasets; Reconfiguration mechanisms; Resource utilizations; Automata theory
A novel FPGA implementation of a time-to-digital converter supporting run-time estimation and compensation,2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074871339&doi=10.1145%2f3322482&partnerID=40&md5=1b53f03afba28e136de18ddd16066b95,"Time-to-digital converters (TDCs) are widely used in applications that require the measurement of the time interval between events. In previous designs using a feedback loop and an extended delay line, processvoltage- temperature (PVT) variation often decreases the accuracy of measurements. To overcome the loss of accuracy caused by PVT variation, this study proposes a novel design of a synthesizable TDC that employs run-time estimation and compensation of PVT variation. A delay line consisting of a series of buffers is used to detect the period of a ring oscillator designed to measure the time interval between two events. By comparing the detected period and the system clock, the variation of the oscillation period is compensated at run-time. The proposed TDC is successfully implemented by using a low-cost Xilinx Spartan-6 LX9 FPGA with a 50-MHz oscillator. Experimental results show that the proposed TDC is robust to PVT variation with a resolution of 19.1 ps. In comparison with previous design, the proposed TDC achieves about five times better tradeoff in the area, resolution, and frequency of the reference clock. © 2019 Association for Computing Machinery.",Delay line; Field programmable gate array (FPGA); Ring oscillator; Time-to-digital converter (TDC),Clocks; Electric delay lines; Field programmable gate arrays (FPGA); Integrated circuit design; Signal processing; Accuracy of measurements; FPGA implementations; Loss of accuracy; Oscillation periods; Reference clock; Ring oscillator; Time to digital converters; Time to digital converters (TDCs); Frequency converters
Editorial: A message from the new editor-in-chief,2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074864048&doi=10.1145%2f3326451&partnerID=40&md5=bc13ca42aea991aecccdc6ac73bf5da2,[No abstract available],,
[DL] A survey of FPGA-based neural network inference accelerators,2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065803944&doi=10.1145%2f3289185&partnerID=40&md5=a90831469f6f6526ef886dc9f87f3e76,"Recent research on neural networks has shown a significant advantage in machine learning over traditional algorithms based on handcrafted features and models. Neural networks are now widely adopted in regions like image, speech, and video recognition. But the high computation and storage complexity of neural network inference poses great difficulty on its application. It is difficult for CPU platforms to offer enough computation capacity. GPU platforms are the first choice for neural network processes because of its high computation capacity and easy-to-use development frameworks. However, FPGA-based neural network inference accelerator is becoming a research topic. With specifically designed hardware, FPGA is the next possible solution to surpass GPU in speed and energy efficiency. Various FPGA-based accelerator designs have been proposed with software and hardware optimization techniques to achieve high speed and energy efficiency. In this article, we give an overview of previous work on neural network inference accelerators based on FPGA and summarize the main techniques used. An investigation from software to hardware, from circuit level to system level is carried out to complete analysis of FPGA-based neural network inference accelerator design and serves as a guide to future work. © 2019 Association for Computing Machinery.",FPGA architecture; Neural network; Parallel processing,Acceleration; Energy efficiency; Graphics processing unit; Machine learning; Neural networks; Speech recognition; Accelerator design; Computation capacity; Development frameworks; FPGA architectures; Parallel processing; Recent researches; Software and hardwares; Storage complexity; Field programmable gate arrays (FPGA)
In-depth analysis on microarchitectures of modern heterogeneous CPU-FPGA platforms,2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062362675&doi=10.1145%2f3294054&partnerID=40&md5=4c574e263b748f3d4744b13c954ced8b,"Conventional homogeneous multicore processors are not able to provide the continued performance and energy improvement that we have expected from past endeavors. Heterogeneous architectures that feature specialized hardware accelerators are widely considered a promising paradigm for resolving this issue. Among different heterogeneous devices, FPGAs that can be reconfigured to accelerate a broad class of applications with orders-of-magnitude performance/watt gains, are attracting increased attention from both academia and industry. As a consequence, a variety of CPU-FPGA acceleration platforms with diversified microarchitectural features have been supplied by industry vendors. Such diversity, however, poses a serious challenge to application developers in selecting the appropriate platform for a specific application or application domain. This article aims to address this challenge by determining which microarchitectural characteristics affect performance, and in what ways. Specifically, we conduct a quantitative comparison and an in-depth analysis on five state-of-the-art CPU-FPGA acceleration platforms: (1) the Alpha Data board and (2) the Amazon F1 instance that represent the traditional PCIe-based platform with private device memory; (3) the IBM CAPI that represents the PCIe-based system with coherent shared memory; (4) the first generation of the Intel Xeon+FPGA Accelerator Platform that represents the QPI-based system with coherent shared memory; and (5) the second generation of the Intel Xeon+FPGA Accelerator Platform that represents a hybrid PCIe-based (non-coherent) and QPI-based (coherent) system with shared memory. Based on the analysis of their CPU-FPGA communication latency and bandwidth characteristics, we provide a series of insights for both application developers and platform designers. Furthermore, we conduct two case studies to demonstrate how these insights can be leveraged to optimize accelerator designs. The microbenchmarks used for evaluation have been released for public use. © 2019 Association for Computing Machinery.",AWS F1; CAPI; CPU-FPGA platform; Heterogeneous computing; Xeon+FPGA,Acceleration; Memory architecture; Application developers; AWS F1; Bandwidth characteristics; CAPI; Fpga platforms; Heterogeneous architectures; Heterogeneous computing; Quantitative comparison; Field programmable gate arrays (FPGA)
FlexSaaS: A reconfigurable accelerator for web search selection,2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062366972&doi=10.1145%2f3301409&partnerID=40&md5=9230ebf44b311ba8a2fca8f040e7d9df,"Web search engines deploy large-scale selection services on CPUs to identify a set of web pages that match user queries. An FPGA-based accelerator can exploit various levels of parallelism and provide a lower latency, higher throughput, more energy-efficient solution than commodity CPUs. However, maintaining such a customized accelerator in a commercial search engine is challenging because selection services are changed often. This article presents our design for FlexSaaS (Flexible Selection as a Service), an FPGA-based accelerator for web search selection. To address efficiency and flexibility challenges, FlexSaaS abstracts computing models and separates memory access from computation. Specifically, FlexSaaS (i) contains a reconfigurable number of matching processors that can handle various possible query plans, (ii) decouples index stream reading from matching computation to fetch and decode index files, and (iii) includes a universal memory accessor that hides the complex memory hierarchy and reduces host data access latency. Evaluated on FPGAs in the selection service of a commercial web search-the Bing web search engine-FlexSaaS can be evolved quickly to adapt to new updates. Compared to the software baseline, FlexSaaS on Arria 10 reduces average latency by 30% and increases throughput by 1.5×. © 2019 Association for Computing Machinery.",FPGA; Inverted index; Selection service; Web search engine,Energy efficiency; Field programmable gate arrays (FPGA); Information retrieval; Linear accelerators; Memory architecture; Program processors; Search engines; Computing model; Energy efficient; Inverted indices; Memory hierarchy; Reconfigurable; Scale selection; Selection service; Universal memory; Websites
An efficient memory partitioning approach for multi-pattern data access via data reuse,2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062367707&doi=10.1145%2f3301296&partnerID=40&md5=7541d0cf260d09e07447ab794a267898,"Memory bandwidth has become a bottleneck that impedes performance improvement during the parallelism optimization of the datapath. Memory partitioning is a practical approach to reduce bank-level conflicts and increase the bandwidth on a field-programmable gate array. In this work, we propose a memory partitioning approach for multi-pattern data access. First, we propose to combine multiple patterns into a single pattern to reduce the complexity of multi-pattern. Then, we propose to perform data reuse analysis on the combined pattern to find data reuse opportunities and the non-reusable data pattern. Finally, an efficient bank mapping algorithm with low complexity and low overhead is proposed to find the optimal memory partitioning solution. Experimental results demonstrated that compared to the state-of-the-art method, our proposed approach can reduce the number of block RAMS by 58.9% on average, with 79.6% reduction in SLICEs, 85.3% reduction in LUTs, 67.9% in reduction Flip-Flops, 54.6% reduction in DSP48Es, 83.9% reduction in SRLs, 50.0% reduction in storage overhead, 95.0% reduction in execution time, and 77.3% reduction in dynamic power consumption on average. Meanwhile, the performance can be improved by 14.0% on average. © 2019 Association for Computing Machinery.",Data reuse; High-level synthesis; Memory partitioning; Multi-pattern,Bandwidth; Computational complexity; Conformal mapping; Digital storage; Field programmable gate arrays (FPGA); Flip flop circuits; High level synthesis; Data reuse; Data reuse analysis; Dynamic power consumption; Mapping algorithms; Memory bandwidths; Memory Partitioning; Multi patterns; State-of-the-art methods; Data reduction
Feathernet: An accelerated convolutional neural network design for resource-constrained FPGAs,2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065797459&doi=10.1145%2f3306202&partnerID=40&md5=6e5a68f03e2476d1a82ceea161670b8e,"Convolutional Neural Network (ConvNet or CNN) algorithms are characterized by a large number of model parameters and high computational complexity. These two requirements have made it challenging for implementations on resource-limited FPGAs. The challenges are magnified when considering designs for low-end FPGAs. While previous work has demonstrated successful ConvNet implementations with high-end FPGAs, this article presents a ConvNet accelerator design that enables the implementation of complex deep ConvNet architectures on resource-constrained FPGA platforms aimed at the IoT market. We call the design “FeatherNet” for its light resource utilization. The implementations are VHDL-based providing flexibility in design optimizations. As part of the design process, new methods are introduced to address several design challenges. The first method is a novel stride-aware graph-based method targeted at ConvNets that aims at achieving efficient signal processing with reduced resource utilization. The second method addresses the challenge of determining the minimal precision arithmetic needed while preserving high accuracy. For this challenge, we propose variable-width dynamic fixed-point representations combined with a layer-by-layer design-space pruning heuristic across the different layers of the deep ConvNet model. The third method aims at achieving a modular design that can support different types of ConvNet layers while ensuring low resource utilization. For this challenge, we propose the modules to be relatively small and composed of computational filters that can be interconnected to build an entire accelerator design. These model elements can be easily configured through HDL parameters (e.g., layer type, mask size, stride, etc.) to meet the needs of specific ConvNet implementations and thus they can be reused to implement a wide variety of ConvNet architectures. The fourth method addresses the challenge of design portability between two different FPGA vendor platforms, namely, Intel/Altera and Xilinx. For this challenge, we propose to instantiate the device-specific hardware blocks needed in each computational filter, rather than relying on the synthesis tools to infer these blocks, while keeping track of the similarities and differences between the two platforms. We believe that the solutions to these design challenges further advance knowledge as they can benefit designers and other researchers using similar devices or facing similar challenges. Our results demonstrated the success of addressing the design challenges and achieving low (30%) resource utilization for the low-end FPGA platforms: Zedboard and Cyclone V. The design overcame the limitation of designs targeted for high-end platforms and that cannot fit on low-end IoT platforms. Furthermore, our design showed superior performance results (measured in terms of [Frame/s/W] per Dollar) compared to high-end optimized designs. © 2019 Association for Computing Machinery.",Convolutional neural networks; Embedded-vision; IoT applications; Resource-constrained FPGAs,Complex networks; Convolution; Field programmable gate arrays (FPGA); Graphic methods; Internet of things; Network architecture; Neural networks; Optimization; Signal processing; Computational filter; Convolutional neural network; Design optimization; Embedded visions; Graph-based methods; IOT applications; Precision arithmetic; Resource utilizations; Integrated circuit design
Fast adjustable NPN classification using generalized symmetries,2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065755341&doi=10.1145%2f3313917&partnerID=40&md5=18e177bb24da81f8fe9b367a70437946,"NPN classification of Boolean functions is a powerful technique used in many logic synthesis and technology mapping tools in both standard cell and FPGA design flows. Computing the canonical form is the most common approach of Boolean function classification. This article proposes two different hybrid NPN canonical forms and a new algorithm to compute them. By exploiting symmetries under different phase assignment as well as higher-order symmetries, the search space of NPN canonical form computation is pruned and the runtime is dramatically reduced. Nevertheless, the runtime for some difficult functions remains high. Fast heuristic method can be used for such functions to compute semi-canonical forms in a reasonable time. The proposed algorithm can be adjusted to be a slow exact algorithm or a fast heuristic algorithm with lower quality. For exact NPN classification, the proposed algorithm is 40× faster than state-of-the-art. For heuristic classification, the proposed algorithm has similar performance as state-of-the-art with a possibility to trade runtime for quality. © 2019 Association for Computing Machinery.",Boolean matching; Boolean signatures; Canonical form; NPN classification; Symmetry,Boolean functions; Crystal symmetry; Heuristic algorithms; Heuristic methods; Logic design; Logic Synthesis; Boolean matching; Boolean signatures; Canonical form; Fast heuristic algorithms; Heuristic classification; NPN-classification; State of the art; Technology mapping; Data mining
Exact and practical modulo scheduling for high-level synthesis,2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065776767&doi=10.1145%2f3317670&partnerID=40&md5=3d684f774b53c762fcf6188f1daa37c0,"Loop pipelining is an essential technique in high-level synthesis to increase the throughput and resource utilisation of field-programmable gate array-based accelerators. It relies on modulo schedulers to compute an operator schedule that allows subsequent loop iterations to overlap partially when executed while still honouring all precedence and resource constraints. Modulo schedulers face a bi-criteria problem: minimise the initiation interval (II; i.e., the number of timesteps after which new iterations are started) and minimise the schedule length. We present Moovac, a novel exact formulation that models all aspects (including the II minimisation) of the modulo scheduling problem as a single integer linear program, and discuss simple measures to prevent excessive runtimes, to challenge the old preconception that exact modulo scheduling is impractical. We substantiate this claim by conducting an experimental study covering 188 loops from two established high-level synthesis benchmark suites, four different time limits, and three bounds for the schedule length, to compare our approach against a highly tuned exact formulation and a state-of-the-art heuristic algorithm. In the fastest configuration, an accumulated runtime of under 16 minutes is spent on scheduling all loops, and proven optimal IIs are found for 179 test instances. © 2019 Association for Computing Machinery.",Exact; High-level synthesis; II minimisation; Modulo scheduling; Optimal,Field programmable gate arrays (FPGA); Heuristic algorithms; Integer programming; Scheduling; Benchmark suites; Exact; Integer linear programs; Minimisation; Modulo scheduling; Optimal; Resource Constraint; Resource utilisation; High level synthesis
COFFE 2: Automatic modelling and optimization of complex and heterogeneous FPGA architectures,2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061232899&doi=10.1145%2f3301298&partnerID=40&md5=1c8651e702edaaf3a2254a60738c4202,"FPGAs are becoming more heteregeneous to better adapt to different markets, motivating rapid exploration of different blocks/tiles for FPGAs. To evaluate a new FPGA architectural idea, one should be able to accurately obtain the area, delay, and energy consumption of the block of interest. However, current FPGA circuit design tools can only model simple, homogeneous FPGA architectures with basic logic blocks and also lack DSP and other heterogeneous block support. Modern FPGAs are instead composed of many different tiles, some of which are designed in a full custom style and some of which mix standard cell and full custom styles. To fll this modelling gap, we introduce COFFE 2, an open-source FPGA design toolset for automatic FPGA circuit design. COFFE 2 uses a mix of full custom and standard cell flows and supports not only complex logic blocks with fracturable lookup tables and hard arithmetic but also arbitrary heterogeneous blocks. To validate COFFE 2 and demonstrate its features, we design and evaluate a multi-mode Stratix III-like DSP block and several logic tiles with fracturable LUTs and hard arithmetic. We also demonstrate how COFFE 2s interface to VTR allows full evaluation of block-routing interfaces and various fracturable 6-LUT architectures. © 2019 Association for Computing Machinery.",Architecture exploration; Automatic circuit design; FPGA,Computer circuits; Energy utilization; Field programmable gate arrays (FPGA); Fracture; Integrated circuit manufacture; Table lookup; Architecture exploration; Automatic circuits; Complex logic blocks; FPGA architectures; FPGA circuits; FPGA design; Open sources; Standard cell; Integrated circuit design
PIMap: A flexible framework for improving LUT-based technology mapping via parallelized iterative optimization,2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060007179&doi=10.1145%2f3268344&partnerID=40&md5=5ba923b6f64dfecd4b03b839b24740f1,"Modern FPGA synthesis tools typically apply a predetermined sequence of logic optimizations on the input logic network before carrying out technology mapping. While the “known recipes” of logic transformations often lead to improved mapping results, there remains a nontrivial gap between the quality metrics driving the pre-mapping logic optimizations and those targeted by the actual technology mapping. Needless to mention, such miscorrelations would eventually result in suboptimal quality of results. In this article, we propose PIMap, which couples logic transformations and technology mapping under an iterative improvement framework for LUT-based FPGAs. In each iteration, PIMap randomly proposes a transformation on the given logic network from an ensemble of candidate optimizations; it then invokes technology mapping and makes use of the mapping result to determine the likelihood of accepting the proposed transformation. By adjusting the optimization objective and incorporating required time constraints during the iterative process, PIMap can flexibly optimize for different objectives including area minimization, delay optimization, and delay-constrained area reduction. To mitigate the runtime overhead, we further introduce parallelization techniques to decompose a large design into multiple smaller sub-netlists that can be optimized simultaneously. Experimental results show that PIMap achieves promising quality improvement over a set of commonly used benchmarks, including improving the majority of the best-known area and delay records for the EPFL benchmark suite. © 2019 Association for Computing Machinery.",Lookup table; Technology mapping,Benchmarking; Constrained optimization; Field programmable gate arrays (FPGA); Iterative methods; Mapping; Table lookup; Delay optimization; Iterative improvements; Iterative Optimization; Logic optimization; Parallelization techniques; Quality improvement; Quality of results; Technology mapping; Computer circuits
Efficient fine-grained processor-logic interactions on the cache-coherent zynq platform,2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060026975&doi=10.1145%2f3277506&partnerID=40&md5=ca7ebbec844e1b2872915ea208cadb67,"The introduction of cache-coherent processor-logic interconnects in CPU-FPGA platforms promises low-latency communication between CPU and FPGA fabrics. This reduced latency improves the performance of heterogeneous systems implemented on such devices and gives rise to new software architectures that can better use the available hardware. Via an extended study accelerating the software task scheduler of a microkernel operating system, this article reports on the potential for accelerating applications that exhibit fine-grained interactions. In doing so, we evaluate the performance of direct and cache-coherent communication methods for applications that involve frequent, low-bandwidth transactions between CPU and programmable logic. In the specific case we studied, we found that replacing a highly optimised software implementation of the task scheduler with an FPGA-based scheduler reduces the cost of communication between two software threads by 5.5%. We also found that, while hardware acceleration reduces cache footprint, we still observe execution time variability because of other non-deterministic features of the CPU. © 2019 Association for Computing Machinery.",Cache-coherent interconnect; Reconfigurable Computing,Application programs; Computation theory; Field programmable gate arrays (FPGA); Reconfigurable architectures; Cache-coherent interconnect; Coherent communication; Hardware acceleration; Heterogeneous systems; Low-latency communication; Programmable logic; Reconfigurable computing; Software implementation; Computer circuits
FPGA-based acceleration of FT convolution for pulsar search using OpenCL,2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060050464&doi=10.1145%2f3268933&partnerID=40&md5=cee8932af7a4dafa25cbd571f675eac9,"The Square Kilometre Array (SKA) project will be the world's largest radio telescope array. With its large number of antennas, the number of signals that need to be processed is dramatic. One important element of the SKA's Central Signal Processor package is pulsar search. This article focuses on the FPGA-based acceleration of the Frequency-Domain Acceleration Search module, which is a part of SKA pulsar search engine. In this module, the frequency-domain input signals have to be processed by 85 Finite Impulse response (FIR) filters within a short period of limitation and for thousands of input arrays. Because of the large scale of the input length and FIR filter size, even high-end FPGA devices cannot parallelise the task completely. We start by investigating both time-domain FIR filter (TDFIR) and frequency-domain FIR filter (FDFIR) to tackle this task. We applied the overlap-add algorithm to split the coefficient array of TDFIR and the overlap-save algorithm to split the input signals of FDFIR. To achieve fast prototyping design, we employed OpenCL, which is a high-level FPGA development technique. The performance and power consumption are evaluated using multiple FPGA devices simultaneously and compared with GPU results, which is achieved by porting FPGA-based OpenCL kernels. The experimental evaluation shows that the FDFIR solution is very competitive in terms of performance, with a clear energy consumption advantage over the GPU solution. © 2019 Association for Computing Machinery.",FIR filter; High-level design; OpenCL; SKA,Energy utilization; Field programmable gate arrays (FPGA); Frequency domain analysis; Graphics processing unit; Impulse response; Pulsars; Search engines; Signal processing; Development technique; Experimental evaluation; FPGA-based accelerations; Frequency domains; High-level design; OpenCL; Overlap-add algorithm; Signal processor; FIR filters
Loop unrolling for energy efficiency in low-cost field-programmable gate arrays,2019,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060888176&doi=10.1145%2f3289186&partnerID=40&md5=0e5de46af21362f684ca6fac8f11313a,"Field-programmable gate arrays (FPGAs) are used for a wide variety of computations in low-cost embedded systems. Although these systems often have modest performance constraints, their energy consumption must typically be limited. Many FPGA applications employ repetitive loops that cannot be straightforwardly split into parallel computations. Performing a loop sequentially generally requires high-speed clocks that consume considerable clock power and sometimes require clock generation using a phase-locked loop (PLL). Loop unrolling addresses the high-speed clock issue, but its use often leads to significant combinational glitch power. In this work, a computer-aided design (CAD) approach that unrolls loops for designs targeted to low-cost FPGAs is described. Our approach considers latency constraints in an effort to minimize energy consumption for loop-based computation. To reduce glitch power, a glitch-filtering approach is introduced that provides a balance between glitch reduction and design performance. Glitch-filter enable signals are generated and routed to the filters using resources best suited to the target FPGA. Our approach automatically inserts glitch filters and associated control logic into a design prior to processing with FPGA synthesis, place, and route tools. Our energy-saving loop-unrolling approach has been evaluated using five benchmarks often used in low-cost FPGAs. The energy-saving capabilities of the approach have been evaluated for an Intel Cyclone IV and a Xilinx Artix-7 FPGA using board-level power measurement. The use of unrolling and glitch filtering is shown to reduce energy by at least 65% for an Artix-7 device and 50% for a Cyclone IV device while meeting design latency constraints. © 2019 Association for Computing Machinery.",Energy; Field-programmable gate array; Loop unrolling,Computation theory; Computer aided design; Computer aided logic design; Costs; Electric clocks; Embedded systems; Energy conservation; Energy efficiency; Energy utilization; Integrated circuit design; Logic gates; Logic Synthesis; Phase locked loops; Signal processing; Signal receivers; Design performance; Energy; FPGA applications; Latency constraints; Loop unrolling; Parallel Computation; Performance constraints; Phase Locked Loop (PLL); Field programmable gate arrays (FPGA)
FinN-R: An end-to-end deep-learning framework for fast exploration of quantized neural networks,2018,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058812986&doi=10.1145%2f3242897&partnerID=40&md5=9ec2b521fef33131982f5998f3fa840e,"Convolutional Neural Networks have rapidly become the most successful machine-learning algorithm, enabling ubiquitous machine vision and intelligent decisions on even embedded computing systems. While the underlying arithmetic is structurally simple, compute and memory requirements are challenging. One of the promising opportunities is leveraging reduced-precision representations for inputs, activations, and model parameters. The resulting scalability in performance, power efficiency, and storage footprint provides interesting design compromises in exchange for a small reduction in accuracy. FPGAs are ideal for exploiting low-precision inference engines leveraging custom precisions to achieve the required numerical accuracy for a given application. In this article, we describe the second generation of the FINN framework, an end-to-end tool that enables design-space exploration and automates the creation of fully customized inference engines on FPGAs. Given a neural network description, the tool optimizes for given platforms, design targets, and a specific precision. We introduce formalizations of resource cost functions and performance predictions and elaborate on the optimization algorithms. Finally, we evaluate a selection of reduced precision neural networks ranging from CIFAR-10 classifiers to YOLO-based object detection on a range of platforms including PYNQ and AWS F1, demonstrating new unprecedented measured throughput at 50 TOp/s on AWS F1 and 5 TOp/s on embedded devices. © 2018 Association for Computing Machinery.",Artificial intelligence; Convolutional neural networks; FINN; FPGA; Hardware accellerator; Inference; Neural network; Quantized neural networks,Artificial intelligence; Convolution; Cost functions; Embedded systems; Engines; Field programmable gate arrays (FPGA); Integrated circuit design; Learning algorithms; Neural networks; Object detection; Ubiquitous computing; Convolutional neural network; Design space exploration; Embedded computing system; FINN; Inference; Intelligent decisions; Optimization algorithms; Performance prediction; Deep learning
Instruction driven cross-layer CNN accelerator for fast detection on FPGA,2018,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058798999&doi=10.1145%2f3283452&partnerID=40&md5=7e51177fe9e14167125c0f743ce3dd8d,"In recent years, Convolutional Neural Networks (CNNs) have been widely applied in computer vision and have achieved significant improvements in object detection tasks. Although there are many optimizing methods to speed up CNN-based detection algorithms, it is still difficult to deploy detection algorithms on real-time low-power systems. Field-Programmable Gate Array (FPGA) has been widely explored as a platform for accelerating CNN due to its promising performance, high energy efficiency, and flexibility. Previous works show that the energy consumption of CNN accelerators is dominated by the memory access. By fusing multiple layers in CNN, the intermediate data transfer can be reduced. However, previous accelerators with the cross-layer scheduling are designed for a particular CNN model. In addition to the memory access optimization, the Winograd algorithm can greatly improve the computational performance of convolution. In this article, to improve the flexibility of hardware, we design an instruction-driven CNN accelerator, supporting the Winograd algorithm and the cross-layer scheduling, for object detection. We modify the loop unrolling order of CNN, so that we can schedule a CNN across different layers with instructions and eliminate the intermediate data transfer. We propose a hardware architecture to support the instructions with Winograd computation units and reach the state-of-the-art energy efficiency. To deploy image detection algorithms onto the proposed accelerator with fixed-point computation units, we adopt the fixed-point fine-tune method, which can guarantee the accuracy of the detection algorithms. We evaluate our accelerator and scheduling policy on the Xilinx KU115 FPGA platform. The intermediate data transfer can be reduced by more than 90% on the VGG-D CNN model with the cross-layer strategy. Thus, the performance of our hardware accelerator reaches 1700GOP/s on the classification model VGG-D. We also implement a framework for object detection algorithms, which achieves 2.3× and 50× in energy efficiency compared with GPU and CPU, respectively. Compared with floating-point algorithms, the accuracy of the fixed-point detection algorithms only drops by less than 1%. © 2018 Association for Computing Machinery.",CNN; FPGA; Image object detection,Acceleration; Computer hardware; Convolution; Data transfer; Digital arithmetic; Energy efficiency; Energy utilization; Field programmable gate arrays (FPGA); Hardware; Memory architecture; Neural networks; Object recognition; Real time systems; Scheduling; Signal detection; Computational performance; Convolutional neural network; Fixed-point computation; Floating-point algorithms; High energy efficiency; Image object detection; Memory access optimization; Object detection algorithms; Object detection
Optimizing CNN-based Segmentation with Deeply customized convolutional and deconvolutional architectures on FPGA,2018,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060874677&doi=10.1145%2f3242900&partnerID=40&md5=1edd91918c4ea3601f3202f4b198415f,"Convolutional Neural Networks– (CNNs) based algorithms have been successful in solving image recognition problems, showing very large accuracy improvement. In recent years, deconvolution layers are widely used as key components in the state-of-the-art CNNs for end-to-end training and models to support tasks such as image segmentation and super resolution. However, the deconvolution algorithms are computationally intensive, which limits their applicability to real-time applications. Particularly, there has been little research on the efficient implementations of deconvolution algorithms on FPGA platforms that have been widely used to accelerate CNN algorithms by practitioners and researchers due to their high performance and power efficiency. In this work, we propose and develop deconvolution architecture for efficient FPGA implementation. FPGA-based accelerators are proposed for both deconvolution and CNN algorithms. Besides, memory sharing between the computation modules is proposed for the FPGA-based CNN accelerator as well as for other optimization techniques. A non-linear optimization model based on the performance model is introduced to efficiently explore the design space to achieve optimal processing speed of the system and improve power efficiency. Furthermore, a hardware mapping framework is developed to automatically generate the low-latency hardware design for any given CNN model on the target device. Finally, we implement our designs on Xilinx Zynq ZC706 board and the deconvolution accelerator achieves a performance of 90.1 giga operations per second (GOPS) under 200MHz working frequency and a performance density of 0.10 GOPS/DSP using 32-bit quantization, which significantly outperforms previous designs on FPGAs. A real-time application of scene segmentation on Cityscapes Dataset is used to evaluate our CNN accelerator on Zynq ZC706 board, and the system achieves a performance of 107 GOPS and 0.12 GOPS/DSP using 16-bit quantization and supports up to 17 frames per second for 512 × 512 image inputs with a power consumption of only 9.6W. © 2018 Association for Computing Machinery.",Convolutional neural networks (CNNs); Deconvolution; FPGA; Hardware acceleration; Segmentation,Acceleration; Convolution; Efficiency; Field programmable gate arrays (FPGA); Image enhancement; Image recognition; Image segmentation; Integrated circuit design; Network architecture; Neural networks; Nonlinear programming; Convolutional neural network; Deconvolution algorithm; Efficient implementation; Giga-operations per seconds; Hardware acceleration; Non-linear optimization; Optimization techniques; Real-time application; Deconvolution
Neuraghe: Exploiting CPU-FPGA synergies for efficient and flexible CNN inference acceleration on zynQ SoCs,2018,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058815973&doi=10.1145%2f3284357&partnerID=40&md5=0107c76a32bcd72b37e312a5baa8ce8d,"Deep convolutional neural networks (CNNs) obtain outstanding results in tasks that require human-level understanding of data, like image or speech recognition. However, their computational load is significant, motivating the development of CNN-specialized accelerators. This work presents NEURAghe, a flexible and efficient hardware/software solution for the acceleration of CNNs on Zynq SoCs. NEURAghe leverages the synergistic usage of Zynq ARM cores and of a powerful and flexible Convolution-Specific Processor deployed on the reconfigurable logic. The Convolution-Specific Processor embeds both a convolution engine and a programmable soft core, releasing the ARM processors from most of the supervision duties and allowing the accelerator to be controlled by software at an ultra-fine granularity. This methodology opens the way for cooperative heterogeneous computing: While the accelerator takes care of the bulk of the CNN workload, the ARM cores can seamlessly execute hard-to-accelerate parts of the computational graph, taking advantage of the NEON vector engines to further speed up computation. Through the companion NeuDNN SW stack, NEURAghe supports end-to-end CNN-based classification with a peak performance of 169GOps/s, and an energy efficiency of 17GOps/W. Thanks to our heterogeneous computing model, our platform improves upon the state-of-the-art, achieving a frame rate of 5.5 frames per second (fps) on the end-to-end execution of VGG-16 and 6.6fps on ResNet-18. © 2018 Association for Computing Machinery.",Convolutional neural networks; FPGAS; HW accelerator; Image classification,Acceleration; ARM processors; Computation theory; Convolution; Deep neural networks; Energy efficiency; Engines; Field programmable gate arrays (FPGA); Image classification; Neural networks; Speech recognition; Computational graph; Computational loads; Convolutional neural network; Deep convolutional neural networks; Frames per seconds; Heterogeneous computing; Reconfigurable logic; Specific processor; Reconfigurable hardware
High-efficiency convolutional ternary neural networks with custom adder trees and weight compression,2018,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058804005&doi=10.1145%2f3270764&partnerID=40&md5=2b38071617257966be9e9122c65847cd,"Although performing inference with artificial neural networks (ANN) was until quite recently considered as essentially compute intensive, the emergence of deep neural networks coupled with the evolution of the integration technology transformed inference into a memory bound problem. This ascertainment being established, many works have lately focused on minimizing memory accesses, either by enforcing and exploiting sparsity on weights or by using few bits for representing activations and weights, to be able to use ANNs inference in embedded devices. In this work, we detail an architecture dedicated to inference using ternary {−1, 0, 1} weights and activations. This architecture is configurable at design time to provide throughput vs. power trade-offs to choose from. It is also generic in the sense that it uses information drawn for the target technologies (memory geometries and cost, number of available cuts, etc.) to adapt at best to the FPGA resources. This allows to achieve up to 5.2k frames per second per Watt for classification on a VC709 board using approximately half of the resources of the FPGA. © 2018 Association for Computing Machinery.",FPGA; Hardware acceleration; Low power inference; Ternary CNN,Adders; Chemical activation; Economic and social effects; Field programmable gate arrays (FPGA); Memory architecture; Network architecture; Neural networks; Embedded device; Frames per seconds; Hardware acceleration; High-efficiency; Integration technologies; Low Power; Target technology; Ternary CNN; Deep neural networks
Lightening the load with highly accurate storage- and energy-efficient lightnns,2018,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058801493&doi=10.1145%2f3270689&partnerID=40&md5=36ec809e3cb7a091593869b961babc35,"Hardware implementations of deep neural networks (DNNs) have been adopted in many systems because of their higher classification speed. However, while they may be characterized by better accuracy, larger DNNs require significant energy and area, thereby limiting their wide adoption. The energy consumption of DNNs is driven by both memory accesses and computation. Binarized neural networks (BNNs), as a tradeoff between accuracy and energy consumption, can achieve great energy reduction and have good accuracy for large DNNs due to their regularization effect. However, BNNs show poor accuracy when a smaller DNN configuration is adopted. In this article, we propose a new DNN architecture, LightNN, which replaces the multiplications to one shift or a constrained number of shifts and adds. Our theoretical analysis for LightNNs shows that their accuracy is maintained while dramatically reducing storage and energy requirements. For a fixed DNN configuration, LightNNs have better accuracy at a slight energy increase than BNNs, yet are more energy efficient with only slightly less accuracy than conventional DNNs. Therefore, LightNNs provide more options for hardware designers to trade off accuracy and energy. Moreover, for large DNN configurations, LightNNs have a regularization effect, making them better in accuracy than conventional DNNs. These conclusions are verified by experiment using the MNIST and CIFAR-10 datasets for different DNN configurations. Our FPGA implementation for conventional DNNs and LightNNs confirms all theoretical and simulation results and shows that LightNNs reduce latency and use fewer FPGA resources compared to conventional DNN architectures. © 2018 Association for Computing Machinery.",Deep neural networks; FPGA; Low-power design,Economic and social effects; Electric power supplies to apparatus; Energy efficiency; Energy utilization; Field programmable gate arrays (FPGA); Hardware; Low power electronics; Network architecture; Energy efficient; Energy reduction; Energy requirements; FPGA implementations; Hardware designers; Hardware implementations; Highly accurate; Low-power design; Deep neural networks
Introduction to the special section on deep learning in FPGAS,2018,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060867705&doi=10.1145%2f3294768&partnerID=40&md5=5ab29cd1b3405ed23a8f2bdc0b955505,[No abstract available],,
You cannot improve what you do not measure: FPGA vs. ASIC efficiency gaps for convolutional neural network inference,2018,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058803178&doi=10.1145%2f3242898&partnerID=40&md5=a1b9224251b9dd59b36933d640a32653,"Recently, deep learning (DL) has become best-in-class for numerous applications but at a high computational cost that necessitates high-performance energy-efficient acceleration. The reconfigurability of FPGAs is appealing due to the rapid change in DL models but also causes lower performance and area-efficiency compared to ASICs. In this article, we implement three state-of-the-art computing architectures (CAs) for convolutional neural network (CNN) inference on FPGAs and ASICs. By comparing the FPGA and ASIC implementations, we highlight the area and performance costs of programmability to pinpoint the inefficiencies in current FPGA architectures. We perform our experiments using three variations of these CAs for AlexNet, VGG-16 and ResNet-50 to allow extensive comparisons. We find that the performance gap varies significantly from 2.8× to 6.3×, while the area gap is consistent across CAs with an 8.7 average FPGA-to-ASIC area ratio. Among different blocks of the CAs, the convolution engine, constituting up to 60% of the total area, has a high area ratio ranging from 13 to 31. Motivated by our FPGA vs. ASIC comparisons, we suggest FPGA architectural changes such as increasing DSP block count, enhancing low-precision support in DSP blocks and rethinking the on-chip memories to reduce the programmability gap for DL applications. © 2018 Association for Computing Machinery.",ASIC; Convolutional neural networks; Deep learning; FPGA,Application specific integrated circuits; Computer architecture; Convolution; Deep learning; Energy efficiency; Network architecture; Neural networks; Architectural changes; Computational costs; Computing architecture; Convolutional neural network; Convolutional Neural Networks (CNN); FPGA architectures; Performance costs; Re-configurability; Field programmable gate arrays (FPGA)
Redcrypt: Real-time privacy-preserving deep learning inference in clouds using FPGAS,2018,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058802900&doi=10.1145%2f3242899&partnerID=40&md5=9e9a840dceb9e610461343f4bc6abc90,"Artificial Intelligence (AI) is increasingly incorporated into the cloud business in order to improve the functionality (e.g., accuracy) of the service. The adoption of AI as a cloud service raises serious privacy concerns in applications where the risk of data leakage is not acceptable. Examples of such applications include scenarios where clients hold potentially sensitive private information such as medical records, financial data, and/or location. This article proposes ReDCrypt, the first reconfigurable hardware-accelerated framework that empowers privacy-preserving inference of deep learning models in cloud servers. ReDCrypt is well-suited for streaming (a.k.a., real-time AI) settings where clients need to dynamically analyze their data as it is collected over time without having to queue the samples to meet a certain batch size. Unlike prior work, ReDCrypt neither requires to change how AI models are trained nor relies on two non-colluding servers to perform. The privacy-preserving computation in ReDCrypt is executed using Yao's Garbled Circuit (GC) protocol. We break down the deep learning inference task into two phases: (i) privacy-insensitive (local) computation, and (ii) privacy-sensitive (interactive) computation. We devise a high-throughput and power-efficient implementation of GC protocol on FPGA for the privacy-sensitive phase. ReDCrypt's accompanying API provides support for seamless integration of ReDCrypt into any deep learning framework. Proof-of-concept evaluations for different DL applications demonstrate up to 57-fold higher throughput per core compared to the best prior solution with no drop in the accuracy. © 2018 Association for Computing Machinery.",Data mining; Deep learning; Garbled circuit; Privacy-preserving computation; Secure machine learning,Artificial intelligence; Data mining; Data privacy; Reconfigurable hardware; Garbled circuits; Learning frameworks; Privacy concerns; Privacy preserving; Privacy preserving computation; Private information; Proof of concept; Seamless integration; Deep learning
Efficient reconfigurable architecture for pricing exotic options,2018,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040306479&doi=10.1145%2f3158228&partnerID=40&md5=15acf875be9ac96e3fe1b9a2e85de594,"This article presents a new method for Monte Carlo (MC) option pricing using field-programmable gate arrays (FPGAs), which use a discrete-space random walk over a binomial lattice, rather than the continuous space-walks used by existing approaches. The underlying hypothesis is that the discrete-space walk will significantly reduce the area needed for eachMC engine, and the resulting increase in parallelisation and raw performance outweighs any accuracy losses introduced by the discretisation. Experimental results support this hypothesis, showing that for a given MC simulation size, there is no significant loss in accuracy by using a discrete space model for the path-dependent exotic financial options. Analysis of the binomial simulation model shows that only limited-precision fixed-point arithmetic is needed, and also shows that pairs of MC kernels are able to share RAM resources. When using realistic constraints on pricing problems, it was found that the size of a discrete-space MC engine can be kept to 370 Flip-Flops and 233 Lookup Tables, allowing up to 3,000 variance-reduced MC cores in one FPGA. The combination of a highly parallelisable architecture and model-specific optimisations means that the binomial pricing technique allows for a 50× improvement in throughput compared to existing FPGA approaches, without any reduction in accuracy. © 2017 ACM.",Control variate; Exotic financial options; Monte Carlo optimizations; Reconfigurable hardware,Bins; Costs; Economics; Engines; Field programmable gate arrays (FPGA); Fixed point arithmetic; Flip flop circuits; Monte Carlo methods; Reconfigurable architectures; Table lookup; Binomial lattices; Continuous spaces; Control variates; Discrete space model; Exotic financial options; Monte Carlo optimization; Pricing problems; Simulation model; Reconfigurable hardware
Reconfigurable hardware architecture for authenticated key agreement protocol over binary edwards curve,2018,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060872019&doi=10.1145%2f3231743&partnerID=40&md5=f408970ee6a7250999aff9e81f13a9ce,"                             In this article, we present a high-performance hardware architecture for Elliptic curve based (authenticated) key agreement protocol “Elliptic Curve Menezes, Qu and Vanstone” (ECMQV) over Binary Edwards Curve (BEC). We begin by analyzing inversion module on a 251-bit binary field. Subsequently, we present Field Programmable Gate Array (FPGA) implementations of the unified formula for computing elliptic curve point addition on BEC in affine and projective coordinates and investigate the relative performance of these two coordinates. Then, we implement the w-coordinate based differential addition formulae suitable for usage in Montgomery ladder. Next, we present a novel hardware architecture of BEC point multiplication using mixed w-coordinates of the Montgomery laddering algorithm and analyze it in terms of resistance to Simple Power Analysis (SPA) attack. In order to improve the performance, the architecture utilizes registers efficiently and uses efficient scheduling mechanisms for the BEC arithmetic implementations. Our implementation results show that the proposed architecture is resistant against SPA attack and yields a better performance when compared to the existing state-of-the-art BEC designs for computing point multiplication (PM). Finally, we present an FPGA design of ECMQV key agreement protocol using BEC defined over GF(2                             251                             ). The execution of ECMQV protocol takes 66.47μs using 32,479 slices on Virtex-4 FPGA and 52.34μs using 15,988 slices on Virtex-5 FPGA. To the best of our knowledge, this is the first FPGA design of the ECMQV protocol using BEC.                          © 2018 ACM.",And Vanstone (ECMQV); Applied cryptography; Binary Edwards curve (BEC); Elliptic curve cryptography (ECC); Elliptic curve Menezes; Field programmable gate array (FPGA); Key exchange; Point multiplication (PM); Qu; Simple power analysis (SPA),Computer hardware; Field programmable gate arrays (FPGA); Geometry; Integrated circuit design; Internet protocols; Logic gates; Network architecture; Public key cryptography; Reconfigurable architectures; Side channel attack; And Vanstone (ECMQV); Applied cryptography; Binary Edwards Curve(BEC); Elliptic curve; Elliptic Curve Cryptography(ECC); Key exchange; Point multiplication; Simple power analysis; Reconfigurable hardware
Automated synthesis of streaming transfer level hardware designs,2018,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060878244&doi=10.1145%2f3243930&partnerID=40&md5=295c8a6d2cccf63e8c126089a647fae5,"As modern field-programmable gate arrays (FPGA) enable high computing performance and efficiency, their programming with low-level hardware description languages is time-consuming and remains a major obstacle to their adoption. High-level synthesis compilers are able to produce register-transfer-level (RTL) designs from C/C++ algorithmic descriptions, but despite allowing significant design-time improvements, these tools are not always able to generate hardware designs that compare to handmade RTL designs. In this article, we consider synthesis from an intermediate-level (IL) language that allows the description of algorithmic state machines handling connections between streaming sources and sinks. However, the interconnection of streaming sources and sinks can lead to cyclic combinational relations, resulting in undesirable behaviors or un-synthesizable designs. We propose a functional-level methodology to automate the resolution of such cyclic relations into acyclic combinational functions. The proposed IL synthesis methodology has been applied to the design of pipelined floating-point cores. The results obtained show how the proposed IL methodology can simplify the description of pipelined architectures while enabling performances that are close to those achievable through an RTL design methodology. © 2018 Association for Computing Machinery.",Design automation; Hardware compiler; Hardware description language; High-level synthesis; Pipelined architectures,Algorithmic languages; C++ (programming language); Computer aided design; Digital arithmetic; Field programmable gate arrays (FPGA); High level synthesis; Integrated circuit design; Pipeline processing systems; Program compilers; Algorithmic state machines; Combinational functions; Computing performance; Design automations; Hardware compilers; Pipelined architecture; Register transfer level; Synthesis methodology; Computer hardware description languages
Preemption of the partial reconfiguration process to enable Real-Time ComputingWith FPGAs,2018,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051864440&doi=10.1145%2f3182183&partnerID=40&md5=65a87dbe59857a230d40750e504cc0c8,"To improve computing performance in real-time applications, modern embedded platforms comprise hardware accelerators that speed up the task's most compute-intensive parts. A recent trend in the design of real-time embedded systems is to integrate field-programmable gate arrays (FPGA) that are reconfigured with different accelerators at runtime, to cope with dynamic workloads that are subject to timing constraints. One of the major limitations when dealing with partial FPGA reconfiguration in real-time systems is that the reconfiguration port can only perform one reconfiguration at a time: If a high-priority task issues a reconfiguration request while the reconfiguration port is already occupied by a lower-priority task, the high-priority task has to wait until the current reconfiguration is completed (a phenomenon known as priority inversion), unless the current reconfiguration is aborted (introducing unbounded delays in low-priority tasks, a phenomenon known as starvation). This article shows how priority inversion and starvation can be solved by making the reconfiguration process preemptive-that is, allowing it to be interrupted at any time and resumed at a later time without restarting it from scratch. Such a feature is crucial for the design of runtime reconfigurable real-time systems but not yet available in today's platforms. Furthermore, the trade-off of achieving a guaranteed bound on the reconfiguration delay for low-priority tasks and the maximum delay induced for high-priority tasks when preempting an ongoing reconfiguration has been identified and analyzed. Experimental results on the Xilinx Zynq-7000 platform show that the proposed implementation of preemptive reconfiguration introduces a low runtime overhead, thus effectively solving priority inversion and starvation. © 2018 Association for Computing Machinery. All rights reserved.",Dynamic Partial Reconfiguration; Field Programmable Gate Array (FPGA); Preemptive Reconfiguration; Real-Time Reconfiguration,Economic and social effects; Embedded systems; Field programmable gate arrays (FPGA); Integrated circuit design; Interactive computer systems; Logic gates; Real time systems; Signal receivers; Dynamic partial reconfiguration; Partial reconfiguration; Preemptive Reconfiguration; Real time; Real-time application; Real-time embedded systems; Reconfiguration process; Run-time reconfigurable; Reconfigurable hardware
Wotan: Evaluating FPGA architecture routability without benchmarks,2018,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051854808&doi=10.1145%2f3195800&partnerID=40&md5=36971fb1e7971d0922d2da9d07936a1c,"FPGA routing architectures consist of routing wires and programmable switches that together account for the majority of the fabric delay and area, making evaluation and optimization of an FPGA's routing architecture very important. Routing architectures have traditionally been evaluated using a full synthesize, pack, place and route CAD flow over a suite of benchmark circuits. While the results are accurate, a full CAD flow has a long runtime and is often tuned to a specific FPGA architecture type, which limits exploration of different architecture options early in the design process. In this article, we present Wotan, a tool to quickly estimate routability for a wide range of architectures without the use of benchmark circuits. At its core, our routability predictor efficiently counts paths through the FPGA routing graph to (1) estimate the probability of node congestion and (2) estimate the probabilities to successfully route a randomized subset of (source, sink) pairs, which are then combined into an overall routability metric.We describe our predictor and present routability estimates for a range of 6-LUT and 4-LUT architectures using mixes of wire types connected in complex ways, showing a rank correlation of 0.91 with routability results from the full VPR CAD flow while requiring 18× less CPU effort. © 2018 Association for Computing Machinery. All rights reserved.",CAD; FPGA; Routing Architecture; Wotan,Computer aided design; Field programmable gate arrays (FPGA); Flow graphs; Benchmark circuit; Different architectures; FPGA architectures; Fpga routing architectures; Programmable switches; Rank correlation; Routing architecture; Wotan; Computer aided logic design
Framework for rapid performance estimation of embedded soft core processors,2018,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051844889&doi=10.1145%2f3195801&partnerID=40&md5=4edf7dc041ee97120ae8ed2c284a3a47,"The large number of embedded soft core processors available today make it tedious and time consuming to select the best processor for a given application. This task is even more challenging due to the numerous configuration options available for a single soft core processor while optimizing for contradicting design requirements such as performance and area. In this article, we propose a generic framework for rapid performance estimation of applications on soft core processors. The proposed technique is scalable to the large number of configuration options available in modern soft core processors by relying on rapid and accurate estimation models instead of time-consuming FPGA synthesis and execution-based techniques. Experimental results on two leading commercial soft core processors executing applications from the widely used CHStone benchmark suite show an average error of less than 6% while running in the order of minutes when compared to hours taken by synthesis-based techniques. © 2018 Association for Computing Machinery. All rights reserved.",Design Methodologies; Performancemodeling And Analysis; Soft Processor,Computer networks; Computer science; Accurate estimation; Configuration options; Design Methodology; Embedded soft-core processors; Performance estimation; Performancemodeling And Analysis; Soft processors; Soft-core processors; Benchmarking
Introduction to the Special Section on FCCM'16,2018,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097881203&doi=10.1145%2f3183572&partnerID=40&md5=c4d4091b052166fcf696ed8a6ea89c2d,[No abstract available],,
An Evaluation on the Accuracy of the Minimum-Width Transistor Area Models in Ranking the Layout Area of FPGA Architectures,2018,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083191876&doi=10.1145%2f3182394&partnerID=40&md5=f40121a0549cfaab4ddd20b21fceff29,"This work provides an evaluation on the accuracy of the minimum-width transistor area models in ranking the actual layout area of FPGA architectures. Both the original VPR area model and the new COFFE area model are compared against the actual layouts with up to three metal layers for the various FPGA building blocks. We found that both models have significant variations with respect to the accuracy of their predictions across the building blocks. In particular, the original VPR model overestimates the layout area of larger buffers, full adders, and multiplexers by as much as 38%, while they underestimate the layout area of smaller buffers and multiplexers by as much as 58%, for an overall prediction error variation of 96%. The newer COFFE model also significantly overestimates the layout area of full adders by 13% and underestimates the layout area of multiplexers by a maximum of 60% for a prediction error variation of 73%. Such variations are particularly significant considering sensitivity analyses are not routinely performed in FPGA architectural studies. Our results suggest that such analyses are extremely important in studies that employ the minimum-width area models so the tolerance of the architectural conclusions against the prediction error variations can be quantified. Furthermore, an open-source version of the layouts of the actual FPGA building blocks should be created so their actual layout area can be used to achieve a highly accurate ranking of the implementation area of FPGA architectures built upon these layouts. © 2018 ACM.",area estimation; area modeling; Field-Programmable Gate Array (FPGA); layout; reconfigurable fabrics; Silicon-On-Chip (SOC),Adders; Architecture; Errors; Field programmable gate arrays (FPGA); Forecasting; Learning to rank; Multiplexing equipment; Sensitivity analysis; Architectural studies; Building blockes; FPGA architectures; Full adders; Highly accurate; Metal layer; Open sources; Prediction errors; Integrated circuit layout
RIPL: A Parallel Image Processing Language for FPGAs,2018,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065668297&doi=10.1145%2f3180481&partnerID=40&md5=152b107c89595b37ef11c2334f57164b,"Specialized FPGA implementations can deliver higher performance and greater power efficiency than embedded CPU or GPU implementations for real-time image processing. Programming challenges limit their wider use, because the implementation of FPGA architectures at the register transfer level is time consuming and error prone. Existing software languages supported by high-level synthesis (HLS), although providing a productivity improvement, are too general purpose to generate efficient hardware without the use of hardware-specific code optimizations. Such optimizations leak hardware details into the abstractions that software languages are there to provide, and they require knowledge of FPGAs to generate efficient hardware, such as by using language pragmas to partition data structures across memory blocks. This article presents a thorough account of the Rathlin image processing language (RIPL), a high-level image processing domain-specific language for FPGAs. We motivate its design, based on higher-order algorithmic skeletons, with requirements from the image processing domain. RIPL's skeletons suffice to elegantly describe image processing stencils, as well as recursive algorithms with nonlocal random access patterns. At its core, RIPL employs a dataflow intermediate representation. We give a formal account of the compilation scheme from RIPL skeletons to static and cyclostatic dataflow models to describe their data rates and static scheduling on FPGAs. RIPL compares favorably to the Vivado HLS OpenCV library and C++ compiled with Vivado HLS. RIPL achieves between 54 and 191 frames per second (FPS) at 100MHz for four synthetic benchmarks, faster than HLS OpenCV in three cases. Two real-world algorithms are implemented in RIPL: visual saliency and mean shift segmentation. For the visual saliency algorithm, RIPL achieves 71 FPS compared to optimized C++ at 28 FPS. RIPL is also concise, being 5x shorter than C++ and 111x shorter than an equivalent direct dataflow implementation. For mean shift segmentation, RIPL achieves 7 FPS compared to optimized C++ on 64 CPU cores at 1.1, and RIPL is 10x shorter than the direct dataflow FPGA implementation. © 2018 Owner/Author.",Cyclo static dataflow; Dataflow; Domain specific languages; FPGA; Hardware accelerators; High level synthesis; Image processing; OpenCV; Parallel processing; RIPL; Semantics,C++ (programming language); Data flow analysis; Field programmable gate arrays (FPGA); High level synthesis; Musculoskeletal system; Problem oriented languages; Productivity; Visualization; Cyclo-static dataflow; Domain specific languages; Intermediate representations; Mean-shift segmentation; Parallel image processing; Productivity improvements; Real-time image processing; Register transfer level; Image processing
Continuous online self-monitoring introspection circuitry for timing repair by incremental partial-reconfiguration (COSMIC TRIP),2018,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042463876&doi=10.1145%2f3158229&partnerID=40&md5=4c724ff417529b8603661dd492bb1d8c,"We show that continuously monitoring on-chip delays at the LUT-to-LUT link level during operation allows a field-programmable gate array to detect and self-adapt to aging and environmental timing effects. Using a lightweight (<4% added area) mechanism for monitoring transition timing, a Difference Detector with First- Fail Latch, we can estimate the timing margin on circuits and identify the individual links that have degraded and whose delay is determining the worst-case circuit delay. Combined with Choose-Your-own-Adventure precomputed, fine-grained repair alternatives, we introduce a strategy for rapid, in-system incremental repair of links with degraded timing. We show that these techniques allow us to respond to a single aging event in less than 190ms for the toronto20 benchmarks. The result is a step toward systems where adaptive reconfiguration on the time-scale of seconds is viable and beneficial. © 2018 Association for Computing Machinery. All rights reserved.",Aging; Algorithms; Component-specific mapping; Measurement; Performance; Reliability; Self-measure,Aging of materials; Algorithms; Cosmology; Delay circuits; Field programmable gate arrays (FPGA); Measurement; Reconfigurable hardware; Reliability; Repair; Circuit delays; Fine grained; Partial reconfiguration; Performance; Self-measure; Self-monitoring; Time-scales; Timing margin; Timing circuits
"KAPow: High-accuracy, low-overhead online per-module power estimation for FPGA designs",2018,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040548637&doi=10.1145%2f3129789&partnerID=40&md5=773d3cbd13fd7c7a99b9476e5fe6ce74,"In an FPGA system-on-chip design, it is often insufficient to merely assess the power consumption of the entire circuit by compile-time estimation or runtime power measurement. Instead, to make better decisions, one must understand the power consumed by each module in the system. In thiswork,we combine measurements of register-level switching activity and system-level power to build an adaptive online model that produces live breakdowns of power consumption within the design. Online model refinement avoids time-consuming characterization while also allowing the model to track long-term operating condition changes. Central to our method is an automated flow that selects signals predicted to be indicative of high power consumption, instrumenting them for monitoring. We named this technique KAPow, for 'K'ounting Activity for Power estimation, which we show to be accurate and to have low overheads across a range of representative benchmarks. We also propose a strategy allowing for the identification and subsequent elimination of counters found to be of low significance at runtime, reducing algorithmic complexity without sacrificing significant accuracy. Finally, we demonstrate an application example in which a module-level power breakdown can be used to determine an efficient mapping of tasks to modules and reduce system-wide power consumption by up to 7%. © 2017 ACM.",Fine-grained power estimation; Online modeling; Power-aware scheduling,Computational complexity; Electric power utilization; Energy efficiency; Field programmable gate arrays (FPGA); Integrated circuit design; Parallel processing systems; Power management; System-on-chip; Algorithmic complexity; Application examples; Fine-grained power; High power consumption; Online modeling; Power-aware scheduling; Switching activities; System on chip design; Electric power system measurement
Enhancing FPGAs with magnetic tunnel junction-based block RAMs,2018,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042466343&doi=10.1145%2f3154425&partnerID=40&md5=7272c60871f4de40ee13c6e9a4cd6b89,"While plentiful on-chip memory is necessary for many designs to fully utilize an FPGA's computational capacity, SRAM scaling is becoming more difficult because of increasing device variation. An alternative is to build FPGA block RAM (BRAM) from magnetic tunnel junctions (MTJ), as this emerging embedded memory has a small cell size, low energy usage, and good scalability. We conduct a detailed comparison study of SRAM and MTJ BRAMs that includes cell designs that are robust with device variation, transistor-level design and optimization of all the required BRAM-specific circuits, and variation-aware simulation at the 22nm node. At a 256Kb block size, MTJ-BRAM is 3.06× denser and 55% more energy efficient and its Fmax is 274MHz, which is adequate for most FPGA system clock domains. We also detail further enhancements that allow these 256 Kb MTJ BRAMs to operate at a higher speed of 353MHz for the streaming FIFOs, which are very common in FPGA designs and describe how the non-volatility of MTJ BRAM enables novel on-chip configuration and power-down modes. For a RAM architecture similar to the latest commercial FPGAs, MTJ-BRAMs could expand FPGA memory capacity by 2.95× with no die size increase. © 2018 ACM.",FPGA; Magnetic tunnel junction; On-chip memory; SRAM,Energy efficiency; Field programmable gate arrays (FPGA); Magnetism; Memory architecture; Random access storage; Static random access storage; Tunnel junctions; Comparison study; Computational capacity; Device variations; Energy efficient; Magnetic tunnel junction; Memory capacity; On chip memory; Transistor level design; Magnetic devices
General-purpose computing with soft GPUs on FPGAs,2018,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042518925&doi=10.1145%2f3173548&partnerID=40&md5=83d7060454ca879aad63d75f15e025d9,"Using field-programmable gate arrays (FPGAs) as a substrate to deploy soft graphics processing units (GPUs) would enable offering the FPGA compute power in a very flexible GPU-like tool flow. Application-specific adaptations like selective hardening of floating-point operations and instruction set subsetting would mitigate the high area and power demands of soft GPUs. This work explores the capabilities and limitations of soft General Purpose Computing on GPUs (GPGPU) for both fixed- and floating point arithmetic. For this purpose, we have developed FGPU: a configurable, scalable, and portable GPU architecture designed especially for FPGAs. FGPU is open-source and implemented entirely in RTL. It can be programmed in OpenCL and controlled through a Python API. This article introduces its hardware architecture as well as its tool flow.We evaluated the proposed GPGPU approach against multiple other solutions. In comparison to homogeneous Multi-Processor System-On-Chips (MPSoCs), we found that using a soft GPU is a Pareto-optimal solution regarding throughput per area and energy consumption. On average, FGPU has a 2.9× better compute density and 11.2× less energy consumption than a single MicroBlaze processor when computing in IEEE-754 floating-point format. An average speedup of about 4×over the ARM Cortex-A9 supported with the NEON vector co-processor has been measured for fixed- or floating-point benchmarks. In addition, the biggest FGPU cores we could implement on a Xilinx Zynq-7000 System-On-Chip (SoC) can deliver similar performance to equivalent implementations with High-Level Synthesis (HLS). © 2018 ACM.",GPGPU; OpenCL; PYNQ; Soft GPUs,Computer graphics; Computer hardware; Digital arithmetic; Distributed computer systems; Energy utilization; Field programmable gate arrays (FPGA); Fixed point arithmetic; Graphics processing unit; Green computing; High level synthesis; Pareto principle; Program processors; Programmable logic controllers; Semiconductor device manufacture; Field programmable gate array (FPGAs); Floating point operations; General-purpose computing; GPGPU; Homogeneous multi processor system on chips; OpenCL; PYNQ; Soft GPUs; System-on-chip
High-performance instruction scheduling circuits for superscalar out-of-order soft processors,2018,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040598928&doi=10.1145%2f3093741&partnerID=40&md5=07a9b2ab87c8ed7788016c0c86f38af0,"Soft processors have a role to play in simplifying field-programmable gate array (FPGA) application design as they can be deployed only when needed, and it is easier to write and debug single-threaded software code than create hardware. The breadth of this second role increases when the performance of the soft processor increases, yet the sophisticated out-of-order superscalar approaches that arrived in the mid-1990s are not employed, despite their area cost now being easily tolerable. In this article, we take an important step toward out-of-order execution in soft processors by exploring instruction scheduling in an FPGA substrate. This differs from the hard-processor design problem because the logic substrate is restricted to LUTs, whereas hard processor scheduling circuits employ CAM and wired-OR structures to great benefit. We discuss both circuit and microarchitectural trade-offs and compare three circuit structures for the scheduler, including a new structure called a fused-logic matrix scheduler. Using our optimized circuits, we show that four-issue distributed schedulers with up to 54 entries can be built with the same cycle time as the commercial Nios II/f soft processor (240MHz). This careful design has the potential to significantly increase both the IPC and raw compute performance of a soft processor, compared to current commercial soft processors. © 2017 ACM.",Out-of-order instruction scheduling; Soft processor,Application programs; Computer circuits; Economic and social effects; Field programmable gate arrays (FPGA); Program debugging; Scheduling; Timing circuits; Application design; Circuit structures; Distributed schedulers; Instruction scheduling; Out-of-order execution; Processor design; Processor scheduling; Soft processors; Integrated circuit design
Fine-grained module-based error recovery in FPGA-based TMR systems,2018,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042516329&doi=10.1145%2f3173549&partnerID=40&md5=0c607cd3d97a3bd085ce1c5c2b8e89a4,"Space processing applications deployed on SRAM-based Field Programmable Gate Arrays (FPGAs) are vulnerable to radiation-induced Single Event Upsets (SEUs). Compared with the well-known SEU mitigation solution - Triple Modular Redundancy (TMR) with configuration memory scrubbing - TMR with modulebased error recovery (MER) is notably more energy efficient and responsive in repairing soft-errors in the system. Unfortunately, TMR-MER systems also need to resort to scrubbing when errors occur between subcomponents, such as in interconnection nets, which are not recovered by MER. This article addresses this problem by proposing a fine-grained module-based error recovery technique, which can localize and correct errors that classic MER fails to do without additional system hardware. We evaluate our proposal via fault-injection campaigns on three types of circuits implemented in Xilinx 7-Series devices. With respect to scrubbing, we observed reductions in the mean time to repair configuration memory errors of between 48.5% and 89.4%, while reductions in energy used recovering from configuration memory errors were estimated at between 77.4% and 96.1%. These improvements result in higher reliability for systems employing TMR with fine-grained reconfiguration than equivalent systems relying on scrubbing for configuration error recovery. © 2018 ACM.",Configuration memory errors; Dynamic reconfiguration; Mean time to recover; Partial reconfiguration; Radiation-induced errors; Recovery energy; Reliability; SRAM FPGA,Computer control systems; Dynamic models; Energy efficiency; Fault tolerant computer systems; Field programmable gate arrays (FPGA); Radiation hardening; Recovery; Reliability; Repair; Static random access storage; Configuration memory; Dynamic re-configuration; Mean time to recover; Partial reconfiguration; Radiation-induced; Errors
Pipelined parallel join and its FPGA-based acceleration,2017,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040310289&doi=10.1145%2f3079759&partnerID=40&md5=8bb3f6d4e5ca617f9d926a21f90b7b9b,"A huge amount of data is being generated and accumulated in data centers, which leads to an important increase in the required energy consumption to analyze these data. Thus, we must consider the redesign of current computer systems architectures to be more friendly to applications based on distributed algorithms that require a high data transfer rate. Novel computer architectures that introduce dedicated accelerators to enable near-data processing have been discussed and developed for high-speed big-data analysis. In this work, we propose a computer system with an FPGA-based accelerator, namely, interconnected-FPGAs, which offers two advantages: (1) direct data transmission and (2) offloading computation into data-flow in the FPGA. In this article, we demonstrate the capability of the proposed interconnected-FPGAs system to accelerate join operations in a relational database. We developed a newparallel join algorithm, PPJoin, targeted to big-data analysis in a shared-nothing architecture. PPJoin is an extended version of the NUMA-based parallel join algorithm, created by overlapping computation by multicore processors and data communication. The data communication between computational nodes can be accelerated by direct data transmission without passing through the main memory of the hosts. To confirm the performance of the PPJoin algorithm and its acceleration process using an interconnected-FPGA platform, we evaluated a simple query for large tables. Additionally, to support availability, we also evaluated the actual benchmark query. Our evaluation results confirm that the PPJoin algorithm is faster than a software-based query engine by 1.5-5 times. Moreover, we experimentally confirmed that the direct data transmission by interconnected FPGAs reduces computational time around 20% for PPJoin. 2017 Copyright is held by the owner/author(s).",Databases; Field programmable gate arrays; In-datapath computing; Join operation; Parallel computing,Computer architecture; Convolutional codes; Data communication systems; Data handling; Data transfer; Data transfer rates; Database systems; Distributed computer systems; Energy utilization; Field programmable gate arrays (FPGA); Green computing; Information analysis; Parallel processing systems; Query processing; Acceleration process; Data paths; FPGA-based accelerations; High data transfer rates; Join operation; Multi-core processor; Offloading computations; Systems architecture; Big data
Fast and cycle-accurate emulation of large-scale networks-on-chip using a single FPGA,2017,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040306461&doi=10.1145%2f3151758&partnerID=40&md5=53f56d9124ee7929e7ae213080355a2c,"Modeling and simulation/emulation play a major role in research and development of novel Networks-on-Chip (NoCs). However, conventional software simulators are so slow that studying NoCs for emerging manycore systems with hundreds to thousands of cores is challenging. State-of-the-art FPGA-based NoC emulators have shown great potential in speeding up the NoC simulation, but they cannot emulate large-scale NoCs due to the FPGA capacity constraints. Moreover, emulating large-scaleNoCs under syntheticworkloads on FPGAs typically requires a large amount of memory and thus involves the use of off-chip memory, which makes the overall design much more complicated and may substantially degrade the emulation speed. This article presents methods for fast and cycle-accurate emulation of NoCs with up to thousands of nodes using a single FPGA.We first describe how to emulate a NoC under a synthetic workload using only FPGA on-chip memory (BRAMs). We next present a novel use of time-division multiplexing where BRAMs are effectively used for emulating a network using a small number of nodes, thereby overcoming the FPGA capacity constraints.We propose methods for emulating both direct and indirect networks, focusing on the commonly used meshes and fat-trees (k-ary n-trees). This is different from prior work that considers only direct networks. Using the proposed methods, we build a NoC emulator, called FNoC, and demonstrate the emulation of some meshbased and fat-tree-based NoCs with canonical router architectures. Our evaluation results show that (1) the size of the largest NoC that can be emulated depends on only the FPGA on-chip memory capacity; (2) a meshbased NoC with 16,384 nodes (128 × 128 NoC) and a fat-tree-based NoC with 6,144 switch nodes and 4,096. terminal nodes (4-ary 6-tree NoC) can be emulated using a single Virtex-7 FPGA; and (3) when emulating these two NoCs, we achieve, respectively, 5,047× and 232× speedups over BookSim, one of the most widely used software-based NoC simulators, while maintaining the same level of accuracy. © 2017 ACM.",Emulation; FPGA; Many-core; Network-on-chip,Binary alloys; Field programmable gate arrays (FPGA); MESH networking; Routers; Capacity constraints; Emulation; Large-scale network; Many core; Model and simulation; Research and development; Router architecture; Synthetic workloads; Network-on-chip
FPGA implementations of kernel normalised least mean squares processors,2017,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040310186&doi=10.1145%2f3106744&partnerID=40&md5=1b19fcc9cd6fc22f5718559e276bc24c,"Kernel adaptive filters (KAFs) are online machine learning algorithms which are amenable to highly efficient streaming implementations. They require only a single pass through the data and can act as universal approximators, i.e. approximate any continuous function with arbitrary accuracy. KAFs are members of a family of kernel methods which apply an implicit non-linear mapping of input data to a high dimensional feature space, permitting learning algorithms to be expressed entirely as inner products. Such an approach avoids explicit projection into the feature space, enabling computational efficiency. In this paper, we propose the first fully pipelined implementation of the kernel normalised least mean squares algorithm for regression. Independent training tasks necessary for hyperparameter optimisation fill pipeline stages, so no stall cycles to resolve dependencies are required. Together with other optimisations to reduce resource utilisation and latency, our core achieves 161 GFLOPS on a Virtex 7 XC7VX485T FPGA for a floating point implementation and 211 GOPS for fixed point. Our PCI Express based floating-point system implementation achieves 80% of the core's speed, this being a speedup of 10× over an optimised implementation on a desktop processor and 2.66× over a GPU. © 2017 ACM.",FPGAs; Hyperparameter search; Machine learning; Pipeline,Adaptive filtering; Adaptive filters; Artificial intelligence; Computational efficiency; Digital arithmetic; Field programmable gate arrays (FPGA); Learning systems; Optimization; Pipelines; Floating point implementation; FPGA implementations; High-dimensional feature space; Hyper-parameter; Kernel adaptive filters; Normalised least mean square; System implementation; Universal approximators; Learning algorithms
Network on chip architecture for multi-agent systems in FPGA,2017,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037675519&doi=10.1145%2f3121112&partnerID=40&md5=1a72fcd5144d6ce48bf93c1fe70c69bc,"A system of interacting agents is, by definition, very demanding in terms of computational resources. Although multi-agent systems have been used to solve complex problems in many areas, it is usually very difficult to perform large-scale simulations in their targeted serial computing platforms. Reconfigurable hardware, in particular Field Programmable Gate Arrays devices, have been successfully used in High Performance Computing applications due to their inherent flexibility, data parallelism, and algorithm acceleration capabilities. Indeed, reconfigurable hardware seems to be the next logical step in the agency paradigm, but only a few attempts have been successful in implementing multi-agent systems in these platforms. This article discusses the problem of inter-agent communications in Field Programmable Gate Arrays. It proposes a Network-on-Chip in a hierarchical star topology to enable agents’ transactions through message broadcasting using the Open Core Protocol as an interface between hardware modules. A customizable router microarchitecture is described and a multi-agent system is created to simulate and analyse message exchanges in a generic heavy traffic load agent-based application. Experiments have shown a throughput of 1.6Gbps per port at 100MHz without packet loss and seamless scalability characteristics. © 2017 ACM.",Agent-based simulation; Multi-agent systems; Network-on-chip; Open core protocol,Computer architecture; Computer hardware; Field programmable gate arrays (FPGA); Hardware; Internet protocols; Logic gates; Network-on-chip; Reconfigurable hardware; Routers; Servers; Acceleration capabilities; Agent based simulation; Agent-based applications; High-performance computing applications; Inter-agent communications; Network-on-chip architectures; Open core; Router microarchitecture; Multi agent systems
"Optimizing FPGA performance, power, and dependability with linear programming",2017,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027018065&doi=10.1145%2f3079756&partnerID=40&md5=edf2dbc890b5aa1aa52d1c2677bf7c7c,"Field-programmable gate arrays (FPGA) are an increasingly attractive alternative to traditional microprocessor-based computing architectures in extreme-computing domains, such as aerospace and supercomputing. FPGAs offer several resource types that offer different tradeoffs between speed, power, and area, which make FPGAs highly flexible for varying application computational requirements. However, since an application's computational operations can map to different resource types, a major challenge in leveraging resource-diverse FPGAs is determining the optimal distribution of these operations across the device's available resources for varying FPGA devices, resulting in an extremely large design space. In order to facilitate fast design-space exploration, this article presents a method based on linear programming (LP) that determines the optimal operation distribution for a particular device and application with respect to performance, power, or dependability metrics. Our LP method is an effective tool for exploring early designs by quickly analyzing thousands of FPGAs to determine the best FPGA devices and operation distributions, which significantly reduces design time. We demonstrate our LP method's effectiveness with two case studies involving dot-product and distance-calculation kernels on a range of Virtex-5 FPGAs. Results show that our LP method selects optimal distributions of operations to within an average of 4% of actual values. © 2017 ACM.",Dependability; Design methodologies; FPGA; Linear programming; Optimization; Power,Computer architecture; Integrated circuit design; Linear programming; Optimization; Computational operations; Computational requirements; Computing architecture; Dependability; Design Methodology; Distance calculation; Optimal distributions; Power; Field programmable gate arrays (FPGA)
Reducing the performance gap between soft scalar CPUs and custom hardware with TILT,2017,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022324474&doi=10.1145%2f3079757&partnerID=40&md5=64c8c36f11fdab13dae104ff5d756642,"By using resource sharing field-programmable gate array (FPGA) compute engines, we can reduce the performance gap between soft scalar CPUs and resource-intensive custom datapath designs. This article demonstrates that Thread- and Instruction-Level parallel Template architecture (TILT), a programmable FPGA-based horizontally microcoded compute engine designed to highly utilize floating point (FP) functional units (FUs), can improve significantly the average throughput of eight FP-intensive applications compared to a soft scalar CPU (similar to a FP-extended Nios). For eight benchmark applications, we show that: (i) a base TILT configuration having a single instance for each FU type can improve the performance over a soft scalar CPU by 15.8×, while requiring on average 26% of the custom datapaths' area; (ii) selectively increasing the number of FUs canmore than double TILT's average throughput, reducing the custom-datapath-throughputgap from 576× to 14×; and (iii) replicated instances of the most computationally dense TILT configuration that fit within the area of each custom datapath design can reduce the gap to 8.27×, while replicated instances of application-tuned configurations of TILT can reduce the custom-datapath-throughput-gap to an average of 5.22×, and up to 3.41× for the Matrix Multiply benchmark. Last, we present methods for design space reduction, and we correctly predict the computationally densest design for seven out of eight benchmarks. © 2017 ACM.",Compiling; Computational density; Computer architecture; Design space; FPGA; Scheduling; Soft processors; Throughput,Computer architecture; Engines; Field programmable gate arrays (FPGA); Integrated circuit design; Program processors; Scheduling; Throughput; Average throughput; Benchmark applications; Compiling; Computational density; Design spaces; Instruction level parallel; Resource sharing; Soft processors; Benchmarking
Efficient branch and bound on FPGAS using work stealing and instance-specific designs,2017,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023165109&doi=10.1145%2f3053687&partnerID=40&md5=e8693450896c2a8faabb82f9187afc99,"Branch and bound (B&B) algorithms structure the search space as a tree and eliminate infeasible solutions early by pruning subtrees that cannot lead to a valid or optimal solution. Custom hardware designs significantly accelerate the execution of these algorithms. In this article, we demonstrate a high-performance B&B implementation on FPGAS. First, we identify general elements of B&B algorithms and describe their implementation as a finite state machine. Then, we introduce workers that autonomously cooperate using work stealing to allow parallel execution and full utilization of the target FPGA. Finally, we explore advantages of instance-specific designs that target a specific problem instance to improve performance. We evaluate our concepts by applying them to a branch and bound problem, the reconstruction of corrupted AES keys obtained from cold-boot attacks. The evaluation shows that our work stealing approach is scalable with the available resources and provides speedups proportional to the number of workers. Instance-specific designs allow us to achieve an overall speedup of 47× compared to the fastest implementation of AES key reconstruction so far. Finally, we demonstrate how instance-specific designs can be generated just-in-time such that the provided speedups outweigh the additional time required for design synthesis. © 2017 ACM.",AES; Cold-boot attacks; FPGA; Instance-specific computing; Just-intime synthesis; Key reconstruction; Key schedule; On-demand synthesis; Work stealing in hardware,Branch and bound method; Field programmable gate arrays (FPGA); Hardware; Cold-boot attacks; Improve performance; Infeasible solutions; Instance-specific computing; On demands; Optimal solutions; Parallel executions; Specific problems; Cryptography
Improved reliability of FPGA-based PUF identification generator design,2017,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020177173&doi=10.1145%2f3053681&partnerID=40&md5=f5c0552e62f0726b36deb6e5390c2dd6,"Physical unclonable functions (PUFs), a form of physical security primitive, enable digital identifiers to be extracted from devices, such as field programmable gate arrays (FPGAS). Many PUF implementations have been proposed to generate these unique n-bit binary strings. However, they often offer insufficient uniqueness and reliability when implemented on FPGAS and can consume excessive resources. To address these problems, in this article we present an efficient, lightweight, and scalable PUF identification (ID) generator circuit that offers a compact design with good uniqueness and reliability properties and is specifically designed for FPGAS. A novel post-characterisation methodology is also proposed that improves the reliability of a PUF without the need for any additional hardware resources. Moreover, the proposed post-characterisation method can be generally used for any FPGA-based PUF designs. The PUF ID generator consumes 8.95% of the hardware resources of a low-cost Xilinx Spartan-6 LX9 FPGA and 0.81% of a Xilinx Artix-7 FPGA. Experimental results show good uniqueness, reliability, and uniformity with no occurrence of bit-aliasing. In particular, the reliability of the PUF is close to 100% over an environmental temperature range of 25°C to 70°C with ±10% variation in the supply voltage. ©2017 ACM.",Authentication; Field programmable gate arrays (FPGAS); Identification generation; Physical unclonable functions (PUFs); Reliability,Authentication; Cryptography; Digital devices; Hardware; Hardware security; Integrated circuit design; Logic gates; Reliability; Compact designs; Environmental temperature; Generator circuits; Generator design; Hardware resources; Physical security; Reliability properties; Supply voltages; Field programmable gate arrays (FPGA)
Microarchitectural comparison of the MXP and octavo soft-processor FPGA overlays,2017,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020179702&doi=10.1145%2f3053679&partnerID=40&md5=a26190de0dd1de906cb19a602d250b97,"Field-Programmable Gate Arrays (FPGAS) can yield higher performance and lower power than software solutions on CPUs or GPUs. However, designing with FPGAS requires specialized hardware design skills and hours-long CAD processing times. To reduce and accelerate the design effort, we can implement an overlay architecture on the FPGA, on which we then more easily construct the desired system but at a large cost in performance and area relative to a direct FPGA implementation. In this work, we compare the micro-architecture, performance, and area of two soft-processor overlays: the Octavo multi-threaded soft-processor and the MXP soft vector processor. To measure the area and performance penalties of these overlays relative to the underlying FPGA hardware, we compare direct FPGA implementations of the microbenchmarks written in C synthesized with the LegUp HLS tool and also written in the Verilog HDL. Overall, Octavo's higher operating frequency and MXP's more efficient code execution results in similar performance from both, within an order of magnitude of direct FPGA implementations, but with a penalty of an order of magnitude greater area. © 2017 ACM.",Benchmarking; FPGA; Multi-threading; Overlay; Soft-processor; Vector,Benchmarking; Computer aided design; Computer architecture; Computer hardware description languages; Field programmable gate arrays (FPGA); Hardware; Logic Synthesis; Program processors; Vectors; FPGA implementations; Multi-threading; Operating frequency; Overlay; Overlay architecture; Performance penalties; Soft processors; Specialized hardware; Integrated circuit design
Bandwidth compression of floating-point numerical data streams for FPGA-based high-performance computing,2017,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020217765&doi=10.1145%2f3053688&partnerID=40&md5=697f4f5971fede76f92a8e747625cdc0,"Although computational performance is often limited by insufficient bandwidth to/from an external memory, it is not easy to physically increase off-chip memory bandwidth. In this study, we propose a hardware-based bandwidth compression technique that can be applied to field-programmable gate array- (FPGA) based high-performance computation with a logically wider effective memory bandwidth. Our proposed hardware approach can boost the performance of FPGA-based stream computations by applying a data compression technique to effectively transfer more data streams. To apply this data compression technique to bandwidth compression via hardware, several requirements must first be satisfied, including an acceptable level of compression performance and a sufficiently small hardware footprint. Our proposed hardware-based bandwidth compressor utilizes an efficient prediction-based data compression algorithm. Moreover, we propose a multichannel serializer and deserializer that enable applications to use multiple channels of computational data with the bandwidth compression. The serializer encodes compressed data blocks of multiple channels into a data stream, which is efficiently written to an external memory. Based on preliminary evaluation, we define an encoding format considering both high compression ratio and small hardware area. As a result, we demonstrate that our area saving bandwidth compressor increases performance of an FPGA-based fluid dynamics simulation by deployingmore processing elements to exploit spatial parallelism with the enhanced memory bandwidth. ©2017 ACM.",Data compression; Dedicated circuit; FPGA-based accelerator; Memory bandwidth; Numerical data stream; Stream computing,Bandwidth; Computer hardware description languages; Data communication systems; Data compression; Digital arithmetic; Encoding (symbols); Field programmable gate arrays (FPGA); Hardware; Data compression algorithms; Data compression techniques; High performance computation; High performance computing; Memory bandwidths; Numerical data; Serializer and de-serializer; Stream computing; Bandwidth compression
Throughput-optimized FPGA accelerator for deep convolutional neural networks,2017,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026485084&doi=10.1145%2f3079758&partnerID=40&md5=3f015c4d0395aadc68e93276c01f4b5c,"Deep convolutional neural networks (CNNs) have gained great success in various computer vision applications. State-of-the-art CNN models for large-scale applications are computation intensive and memory expensive and, hence, are mainly processed on high-performance processors like server CPUs and GPUs. However, there is an increasing demand of high-accuracy or real-time object detection tasks in large-scale clusters or embedded systems, which requires energy-efficient accelerators because of the green computation requirement or the limited battery restriction. Due to the advantages of energy efficiency and reconfigurability, Field-Programmable Gate Arrays (FPGAS) have been widely explored as CNN accelerators. In this article, we present an in-depth analysis of computation complexity and the memory footprint of each CNN layer type. Then a scalable parallel framework is proposed that exploits four levels of parallelism in hardware acceleration. We further put forward a systematic design space exploration methodology to search for the optimal solution that maximizes accelerator throughput under the FPGA constraints such as on-chip memory, computational resources, external memory bandwidth, and clock frequency. Finally, we demonstrate the methodology by optimizing three representative CNNs (LeNet, AlexNet, and VGG-S) on a Xilinx VC709 board. The average performance of the three accelerators is 424.7, 445.6, and 473.4GOP/s under 100MHz working frequency, which outperforms the CPU and previous work significantly. ©2017 ACM.",Application mapping; Convolutional neural networks; FPGA architecture; High performance computing; Optimisation,Acceleration; Convolution; Embedded systems; Energy efficiency; Field programmable gate arrays (FPGA); Neural networks; Object detection; Program processors; Real time systems; Application mapping; Convolutional neural network; FPGA architectures; High performance computing; Optimisations; Deep neural networks
Efficient and versatile FPGA acceleration of support counting for stream mining of sequences and frequent itemsets,2017,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020174082&doi=10.1145%2f3027485&partnerID=40&md5=568b956ebac3cb0c156775921a92733d,"Stream processing has become extremely popular for analyzing huge volumes of data for a variety of applications, including IoT, social networks, retail, and software logs analysis. Streams of data are produced continuously and are mined to extract patterns characterizing the data. A class of data mining algorithm, called generate-and-test, produces a set of candidate patterns that are then evaluated over data. The main challenges of these algorithms are to achieve high throughput, low latency, and reduced power consumption. In this article, we present a novel power-efficient, fast, and versatile hardware architecture whose objective is to monitor a set of target patterns to maintain their frequency over a stream of data. This accelerator can be used to accelerate data-mining algorithms, including itemsets and sequences mining. The massive fine-grain reconfiguration capability of field-programmable gate array (FPGA) technologies is ideal to implement the high number of pattern-detection units needed for these intensive data-mining applications. We have thus designed and implemented an IP that features high-density FPGA occupation and high working frequency. We provide detailed description of the IP internal micro-architecture and its actual implementation and optimization for the targeted FPGA resources. We validate our architecture by developing a co-designed implementation of the Apriori Frequent Itemset Mining (FIM) algorithm, and perform numerous experiments against existing hardware and software solutions.We demonstrate that FIM hardware acceleration is particularly efficient for large and low-density datasets (i.e., long-tailed datasets). Our IP reaches a data throughput of 250 million items/s and monitors up to 11.6k patterns simultaneously, on a prototyping board that overall consumes 24W in the worst case. Furthermore, our hardware accelerator remains generic and can be integrated to other generate and test algorithms. © 2017 ACM.",Apriori algorithm; Datamining; FPGA architecture; Frequent itemsetmining; Hardware accelerator; Sequence mining; Streammining,Acceleration; Application programs; Computer architecture; Computer hardware description languages; Field programmable gate arrays (FPGA); Hardware; Hardware-software codesign; Internet protocols; Network architecture; Optimization; Pattern recognition; Social sciences computing; Apriori algorithms; FPGA architectures; Frequent itemsetmining; Hardware accelerators; Sequence mining; Streammining; Data mining
Synthesizable standard cell FPGA fabrics targetable by the Verilog-to-Routing CAD flow,2017,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017516823&doi=10.1145%2f3024063&partnerID=40&md5=ce0ab6b0f5d2aa66875d12468a8d95e8,"In this article, we consider implementing field-programmable gate arrays (FPGAs) using a standard cell design methodology and present a framework for the automated generation of synthesizable FPGA fabrics. The open-source Verilog-to-Routing (VTR) FPGA architecture evaluation framework [Rose et al. 2012] is extended to generate synthesizable Verilog for its in-memory FPGA architectural device model. The Verilog can subsequently be synthesized into standard cells, placed and routed using an ASIC design flow. A second extension to VTR generates a configuration bitstream for the FPGA, where the bitstream configures the FPGA to realize a user-provided placed and routed design. The proposed framework and methodology makes possible the silicon implementation of a wide range of VTR-modeled FPGA fabrics. In an experimental study, area and timing-optimized FPGA implementations in 65nm TSMC standard cells are compared to a 65nm Altera commercial FPGA. In addition, we consider augmenting the generic standard-cell library from TSMC with a manually designed and laid-out FPGA-specific cell. We demonstrate the utility of the custom cell in reducing the area of the synthesized FPGA fabric. © 2017 ACM.",Field-programmable gate array,Binary sequences; Cells; Computer aided design; Computer aided logic design; Cytology; Field programmable gate arrays (FPGA); Integrated circuit design; Logic gates; Signal receivers; ASIC design flow; Automated generation; Configuration bitstream; Device modeling; FPGA architectures; FPGA implementations; Silicon implementation; Standard cell design; Logic Synthesis
Performance scalability of adaptive processor architecture,2017,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017524678&doi=10.1145%2f3007902&partnerID=40&md5=97a698cce3aff86f0c0cdf0d4bf2e645,"In this article, we evaluate the performance scalability of architectures called adaptive processors, which dynamically configure an application-specific pipelined datapath and perform a data-flow streaming execution. Previous works have examined the basics of the following: (1) a computational model that supports the swap-in/out of a partial datapath - namely, a virtual hardware is realized by hardware, without a host processor and its software; (2) an architecture that has shown a minimum pipeline requirement and a minimum component requirement; and (3) the characteristics of the execution phase and a stack shift that realizes the swap-in/out. However, these works did not explore the design space, particularly with respect to the following: (1) the clock cycle time on the adaptive processor, which must depend on a wire delay that is primarily used for the global communication of requests, acknowledgments, acquirements, releases, and so forth, and (2) a revised control system that can handle the out-of-order acknowledgment and in-order acquirement that guarantee the correct datapath configuration with a conditional branch for the configurations. This article explores the scaling of the ALU resources versus pipelining of the wires. © 2017 ACM.",Adaptive processors; Performance-scaling,Adaptive control systems; Computer architecture; Hardware; Scalability; Adaptive processors; Application specific; Computational model; Conditional branch; Global communication; Performance scalability; Performance-scaling; Streaming execution; Pipeline processing systems
Introduction to the special section on FPL 2015,2017,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017519667&doi=10.1145%2f3041224&partnerID=40&md5=5a2d38ecf2997a2f5ba06a0dbc54e84e,[No abstract available],,
Efficient assembly for high-order unstructured FEM meshes (FPL 2015),2017,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017606301&doi=10.1145%2f3024064&partnerID=40&md5=348332d946a697d1b079bb74476c05a4,"The Finite Element Method (FEM) is a common numerical technique used for solving Partial Differential Equations on large and unstructured domain geometries. Numerical methods for FEM typically use algorithms and data structures which exhibit an unstructured memory access pattern. This makes acceleration of FEM on Field-Programmable Gate Arrays using an efficient, deeply pipelined architecture particularly challenging. In this work, we focus on implementing and optimising a vector assembly operation which, in the context of FEM, induces the unstructured memory access. We propose a dataflow architecture, graphbased theoretical model, and design flow for optimising the assembly operation for spectral/hp finite element method on reconfigurable accelerators. We evaluate the proposed approach on two benchmark meshes and show that the graph-theoretic method of generating a static data access schedule results in a significant improvement in resource utilisation compared to prior work. This enables supporting larger FEM meshes on FPGA than previously possible. © 2017 ACM.",Application mapping; FPGA architecture; Graph algorithms; High performance computing; Reconfigurable computing,Data flow analysis; Field programmable gate arrays (FPGA); Graph theory; Memory architecture; Numerical methods; Reconfigurable architectures; Application mapping; FPGA architectures; Graph algorithms; High performance computing; Reconfigurable computing; Finite element method
(FPL 2015) Scavenger: Automating the construction of application-optimized memory hierarchies,2017,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016226321&doi=10.1145%2f3009971&partnerID=40&md5=f11ca8e6da4a1a3b3abd69202f9b1918,"High-level abstractions separate algorithm design from platform implementation, allowing programmers to focus on algorithms while building complex systems. This separation also provides system programmers and compilers an opportunity to optimize platform services on an application-by-application basis. In field-programmable gate arrays (FPGAs), plat form-level malleability extends to the memory system: Unlike general-purpose processors, in which memory hardware is fixed at design time, the capacity, associativity, and topology of FPGA memory systems may all be tuned to improve application performance. Since application kernels may only explicitly use few memory resources, substantial memory capacity may be available to the platform for use on behalf of the user program. In this work, we present Scavenger, which utilizes spare resources to construct program-optimized memories, and we also perform an initial exploration of methods for automating the construction of these application-specific memory hierarchies. Although exploiting spare resources can be beneficial, naïvely consuming all memory resources may cause frequency degradation. To relieve timing pressure in large block RAM (BRAM) structures, we provide microarchitectural techniques to trade memory latency for design frequency. We demonstrate, by examining a set of benchmarks, that our scalable cache microarchitecture achieves performance gains of 7% to 74% (with a 26% geometric mean on average) over the baseline cache microarchitecture when scaling the size of first-level caches to the maximum. © 2017 ACM.",FPGA; Memory hierarchy; Resource-aware optimization; Scalable cache,Application programs; Benchmarking; Computer architecture; Field programmable gate arrays (FPGA); General purpose computers; Integrated circuit design; Optimization; Program compilers; Random access storage; Application performance; Application specific; General purpose processors; High-level abstraction; Memory hierarchy; Platform implementations; Resource aware; Scalable cache; Cache memory
The first 25 years of the FPL conference: Significant papers,2017,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016217677&doi=10.1145%2f2996468&partnerID=40&md5=a4783063bc9c9b6bcc050a9f08a1ebcf,"A summary of contributions made by significant papers from the first 25 years of the Field-Programmable Logic and Applications conference (FPL) is presented. The 27 papers chosen represent those which have most strongly influenced theory and practice in the field.Grant: Authors Leong and Lee acknowledge partial support from the Australian Research Council's Linkage Projects funding scheme (project number LP130101034) Zomojo Pty Ltd, and the Faculty of Engineering and Information Technologies, the University of Sydney, by the Faculty Research Cluster Program.",25 years; Field-programmable logic and applications; FPL; Significant papers,Computer circuits; Engineering education; 25 years; Field programmable logic; Project number; Theory and practice; University of Sydney; Paper
Hoplite: A deflection-routed directional torus NoC for FPGAs,2017,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016172682&doi=10.1145%2f3027486&partnerID=40&md5=0a2956c3fa886135549a4c0312dc3666,"We can design an FPGA-optimized lightweight network-on-chip (NoC) router for flit-oriented packet-switched communication that is an order ofmagnitude smaller (in terms of LUTs and FFs) than state-of-the-art FPGA overlay routers available today. We present Hoplite, an efficient, lightweight, and fast FPGA overlay NoC that is designed to be small and compact by (1) using deflection routing instead of buffered switching to eliminate expensive FIFO buffers and (2) using a torus topology to reduce the cost of switch crossbar. Buffering and crossbar implementation complexities have traditionally limited speeds and imposed heavy resource costs in conventional FPGA overlay NoCs. We take care to exploit the fracturable lookup tables (LUT) organization of the FPGA to further improve the resource efficiency ofmapping the expensive crossbar multiplexers. Hoplite can outperform classic, bidirectional, buffered mesh networks for single-flit-oriented FPGA applications by as much as 1.5× (best achievable throughputs for a 10×10 system) or 2.5× (allocating same amount of FPGA resources to both NoCs) for uniform random traffic. When compared to buffered mesh switches, FPGA-based deflection routers are ≈3.5× smaller (HLS-generated switch) and 2.5× faster (clock period) for 32b payloads. In a separate experiment, we hand-crafted an RTL version of our switch with location constraints that requires only 60 LUTs and 100 FFs per router and runs at 2.9ns. We conduct additional layout experiments on modern Xilinx and Altera FPGAs and demonstrate wide-channel chipspanning layouts that run in excess of 300MHz while consuming 10-15% of overall chip resources. We also demonstrate a clustered RISC-V multiprocessor organization that uses Hoplite to help deliver the high processing throughputs of the FPGA architecture to user applications. © 2017 ACM.",Deflection routing; FPGA overlays; Unidirectional torus,Deflection (structures); Field programmable gate arrays (FPGA); Mesh generation; MESH networking; Routers; Table lookup; Topology; Achievable throughputs; Deflection routings; Implementation complexity; Location constraints; Network-on-chip(NoC); Packet switched communications; Resource efficiencies; Unidirectional torus; Network-on-chip
"Analysis of fixed, reconfigurable, and hybrid devices with computational, memory, I/O, & realizable-utilization metrics",2016,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008499545&doi=10.1145%2f2888401&partnerID=40&md5=4689cc54df3358e166b3fddb782c2c8e,"The modern processor landscape isavaried and diverse community. Assuch, developers need a waytoquickly and fairly compare various devices for use with particular applications. This article expands the authors' previously published computational-density metrics and presents an analysis of a new generation of various device architectures, including CPU, DSP, FPGA, GPU, and hybrid architectures. Also, new memory metrics are added to expand the existing suite of metrics to characterize the memory resources on various processing devices. Finally, a new relational metric, realizable utilization (RU), is introduced, which quantifies the fraction of the computational density metric that an application achieves within an individual implementation. The RU metric can be used to provide valuable feedback to application developers and architecture designers by highlighting the upper bound on specific application optimization and providing a quantifiable measure of theoretical and realizable performance. Overall, the analysis in this article quantifies the performance tradeoffs among the architectures studied, the memory characteristics of different device types, and the efficiency of device architectures. © 2016 ACM 1936-7406/2016/09-ART2 $15.00.",Benchmarking; Comparative analysis; Device characterization; Device studies; Performance,Benchmarking; Application developers; Architecture designers; Comparative analysis; Device characterization; Device studies; Performance; Performance trade-off; Quantifiable measures; Memory architecture
Microarchitecture and circuits for a 200 MHz out-of-order soft processor memory system,2016,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008430549&doi=10.1145%2f2974022&partnerID=40&md5=b2e7c9574856d770381863ef6935fb65,"Although FPGAs have grown in capacity, FPGA-based soft processors have grown very little because of the difficulty of achieving higher performance in exchange for area. Superscalar out-of-order processors promise large performance gains, and the memory subsystem is a key part of such a processor that must help supply increased performance. In this article, we describe and explore microarchitectural and circuit-level tradeoffs in the design of such a memory system. We show the significant instructions-per-cycle wins for providing various levels of out-of-order memory access and memory dependence speculation (1.32× SPECint2000) and for the addition of a second-level cache (another 1.60×). With careful microarchitecture and circuit design, we also achieve a L1 translation lookaside buffers and cache lookup with 29% less logic delay than the simpler Nios II/f memory system. © 2016 ACM 1936-7406/2016/12-ART7 $15.00.",Caches; Out-of-order execution; Soft processor,Delay circuits; Field programmable gate arrays (FPGA); Integrated circuit design; Integrated circuit manufacture; Memory architecture; Timing circuits; Caches; Instructions per cycles; Memory dependence speculation; Micro architectures; Out-of-order execution; Out-of-order processors; Soft processors; Translation lookaside buffer; Cache memory
A microcoded Kernel recursive least squares processor using FPGA technology,2016,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008455053&doi=10.1145%2f2950061&partnerID=40&md5=3a04492d249cac39f41a7beb25d98650,"Kernel methods utilize linear methods in a nonlinear feature space and combine the advantages of both. Online kernel methods, such as kernel recursive least squares (KRLS) and kernel normalized least mean squares (KNLMS), perform nonlinear regression in a recursive manner, with similar computational requirements to linear techniques. In this article, an architecture for a microcoded kernel method accelerator is described, and high-performance implementationsofsliding-window KRLS,fixed-budget KRLS, and KNLMS are presented. The architecture utilizes pipelining and vectorization for performance, and microcoding for reusability. The design can be scaled to allow tradeoffs between capacity, performance, and area. The design is compared with a central processing unit (CPU), digital signal processor (DSP), and Altera OpenCL implementations. In different configurations on an Altera Arria 10 device, our SW-KRLS implementation delivers floating-point throughput of approximately 16 GFLOPs, latency of 5.5μS, and energy consumption of 10-4 J, these being improvements over a CPU by factors of 12, 17, and 24, respectively. © 2016 ACM 1936-7406/2016/09-ART5 $15.00.",Field-programmable gate array (FPGA); Kernel methods; Pipeline; Vector processors,Budget control; Digital arithmetic; Digital signal processors; Energy utilization; Field programmable gate arrays (FPGA); Integrated circuit design; Least squares approximations; Pipelines; Program processors; Reusability; Signal processing; Computational requirements; Digital Signal Processor (DSP); Kernel methods; Kernel recursive least squares; Non-linear regression; Nonlinear features; Normalized least mean square; Vector processors; Pipeline processing systems
A framework for evaluating and optimizing FPGA-based socs for aerospace computing,2016,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008473018&doi=10.1145%2f2888400&partnerID=40&md5=87b6e6e879983b4d19c54131f3e1512e,"On-board processing systems are often deployed in harsh aerospace environments and must therefore adhere to stringent constraints such as low power, small size, and high dependability in the presence of faults. Field-programmable gate arrays (FPGAs) are often an attractive option for designers seeking low-power, high-performance devices. However, unlike nonreconfigurable devices, radiation effects can alter an FPGA's functionality instead of just the device's data, requiring designers to consider fault-tolerant strategies to mitigate these effects. In this article, we present a framework to ease these system design challenges and aid designers in considering a broad range of devices and fault-tolerant strategies for on-board processing, highlighting the most promising options and tradeoffs early in the design process. This article focuses on the power, dependability, and lifetime evaluation metrics, which our framework calculates and leverages to evaluate the effectiveness of varying system-on-chip (SoC) designs. Finally, we use our framework to evaluate SoC designs for a case study on a hyperspectral-imaging (HSI) mission to demonstrate our framework's ability to identify efficient and effective SoC designs. © 2016 ACM 1936-7406/2016/09-ART1 $15.00.",Aerospace; Fault-tolerant; FPGA; Pareto optimal; Single-event upset; System design,Computer control systems; Fault tolerance; Field programmable gate arrays (FPGA); Hyperspectral imaging; Pareto principle; Programmable logic controllers; Radiation effects; Radiation hardening; Spectroscopy; System-on-chip; Systems analysis; Aerospace; Fault-tolerant; Fault-tolerant strategy; Field programmable gate array (FPGAs); High performance devices; Hyperspectral imaging (HSI); Pareto-optimal; Single event upsets; Integrated circuit design
Dynamic task mapping with congestion speculation for reconfigurable network-on-chip,2016,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008466324&doi=10.1145%2f2892633&partnerID=40&md5=535a7ac6a37adf9618665c8c004f8dc6,"Network-on-Chip (NoC) has been proposed as a promising communication architecture to replace the dedicated interconnections and shared buses for future embedded system platforms. In such a parallel platform, mapping application tasks to the NoC is a key issue because it affects throughput significantly due to the problem of communication congestion. Increased communication latency, low system performance, and low resource utilization are some side-effects of a bad mapping. Current mapping algorithms either do not consider link utilizations or consider only the current utilizations. Besides, to design an efficient NoC platform, mapping task to computation nodes and scheduling communication should be taken into consideration. In this work, we propose an efficient algorithm for dynamic task mapping with congestion speculation (DTMCS) that not only includes the conventional application mapping, but also further considers future traffic patterns based on the link utilization. The proposed algorithm can reduce overall congestion, instead of only improving the current packet blocking situation. Our experiment results have demonstrated that compared to the state-of-the-art congestion-aware Path Load algorithm, the proposed DTMCS algorithm can reduce up to 40.5% of average communication latency, while the maximal communication latency can be reduced by up to 67.7%. © 2016 ACM 1936-7406/2016/09-ART3 $15.00.",Application mapping; Congestion speculation; Network-on-chip; Task mapping,Conformal mapping; Embedded systems; Mapping; Multiprocessing systems; Servers; Application mapping; Communication architectures; Congestion speculation; Dynamic task mappings; Reconfigurable network; Resource utilizations; Scheduling communication; Task mapping; Network-on-chip
A flexible soc and its methodology for parser-based applications,2016,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008455090&doi=10.1145%2f2939379&partnerID=40&md5=68ecc75750d4c29f225872b4dbe1a53c,Embedded systems are being increasingly network interconnected. They are required to interact with their environment through text-based protocol messages. Parsing such messages is control dominated. The work presented in this article attempts to accelerate message parsers using a codesign-based approach. We propose a generic architecture associated with an automated design methodology that enables SoC/SoPC system generation from high-level specifications of message protocols. Experimental results obtained on a Xilinx ML605 board show acceleration factors ranging from four to 11. Both static and dynamic reconfigurations of coprocessors are discussed and then evaluated so as to reduce the system hardware complexity. © 2016 ACM 1936-7406/2016/09-ART4 $15.00.,Automated design methodology; Dynamic reconfiguration; FPGA; FSM; Leon-3; System on chip,Dynamic models; Embedded systems; Field programmable gate arrays (FPGA); Network architecture; Programmable logic controllers; Reconfigurable hardware; Acceleration factors; Automated design; Co-design based approach; Dynamic re-configuration; Generic architecture; High level specification; Leon-3; Text-based protocols; System-on-chip
Automated real-time analysis of streaming big and dense data on reconfigurable platforms,2016,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008441744&doi=10.1145%2f2974023&partnerID=40&md5=c3021a2b5b1a7d41e9ca696fbe36805f,"We propose SSketch, a novel automated framework for efficient analysis of dynamic big data with dense (nonsparse) correlation matrices on reconfigurable platforms. SSketch targets streaming applications where each data sample can be processed only once and storage is severely limited. Our framework adaptively learns from the stream of input data and updates a corresponding ensemble of lower-dimensional data structures, a.k.a., a sketch matrix. A new sketching methodology is introduced that tailors the problem of transforming the big data with dense correlations to an ensemble of lower-dimensional subspaces such that it is suitable for hardware-based acceleration performed by reconfigurable hardware. The new method is scalable, while it significantly reduces costly memory interactions and enhances matrix computation performance by leveraging coarse-grained parallelism existing in the dataset. SSketch provides an automated optimization methodology for creating the most accurate data sketch for a given set of user-defined constraints, including runtime and power as well as platform constraints such as memory. To facilitate automation, SSketch takes advantage of a Hardware/Software (HW/SW) co-design approach: It provides an Application Programming Interface that can be customized for rapid prototyping of an arbitrary matrix-based data analysis algorithm. Proof-of-concept evaluations on a variety of visual datasets with more than 11 million non-zeros demonstrate up to a 200-fold speedup on our hardware-accelerated realization of SSketch compared to a software-based deployment on a general-purpose processor. © 2016 ACM 1936-7406/2016/12-ART8 $15.00.",Big data; Dense matrix; FPGA; HW/SW co-design; Lower dimensional embedding; Matrix sketching; Matrix-based analysis; Streaming model,Application programming interfaces (API); Automation; Computer hardware; Digital storage; Field programmable gate arrays (FPGA); General purpose computers; Hardware; Hardware-software codesign; Matrix algebra; Metadata; Reconfigurable hardware; Automated optimization; Data analysis algorithms; Dense matrices; General purpose processors; HW/SW Codesign; Lower dimensional embedding; Reconfigurable plat-forms; Streaming model; Big data
Generating efficient context-switch capable circuits through autonomous design flow,2016,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008425439&doi=10.1145%2f2996199&partnerID=40&md5=4cd6b40b2ba16bb5e9c919fcec0d2510,"Commercial off-the-shelf (COTS) Field-Programmable Gate Arrays (FPGAs) are becoming increasingly powerful. In addition to their huge hardware resources, they are also integrated into complete systems on chips (SOCs), e.g., in the latest Xilinx Zynq or Altera Stratix platforms. However, cooperation between FPGAs and their surroundings, and the flexibility of hardware task management could still be improved. For instance, mechanisms have yet to be automated to allow multi-user approaches. A reconfigurable resource can be shared between applications or users only if it has a context-switch ability allowing applications to be paused and resumed in response to system demands. Here, we present a high-level synthesis (HLS) design flow producing a context-switch-capable circuit. The design flow manipulates the intermediate representation of an HLS tool to build the context extraction mechanism and to optimize performance for the circuit produced. The method is based on efficient checkpoint selection and insertion of a powerful scanchain into the initial circuit. This scan-chain can extract flip-flops or memory content. Experiments with the system produced show that it has a low hardware overhead for many benchmark applications, and that the hardware added has a negligible impact on application performance. Comparisons with current standard methods highlight the efficiency of our contributions. © 2016 ACM 1936-7406/2016/12-ART9 $15.00.",Checkpointing; Context-switch on FPGA; Partial scan-chain,Benchmarking; Chains; Computer hardware; Field programmable gate arrays (FPGA); Flip flop circuits; Hardware; High level synthesis; System-on-chip; Timing circuits; Application performance; Check pointing; Commercial off-the shelves; Context switch; Field programmable gate array (FPGAs); Intermediate representations; Partial scan; Reconfigurable resources; Integrated circuit design
Acceleration of K-means algorithm using Altera SDK for OpenCL,2016,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008506368&doi=10.1145%2f2964910&partnerID=40&md5=a656c829da5053e2e6639275f5173dfe,"A K-means clustering algorithm involves partitioning of data iteratively into k clusters. It is one of the most popular data-mining algorithms [Wu et al. 2007], and is widely used in other applications, such as image processing and machine learning. However, k-means is highly time-consuming when data or cluster size is large. Traditionally, FPGAs have shown great promise for accelerating computationally intensive algorithms, but they are harder to use for acceleration if we rely on traditional HD-based design methods. The recent introduction of Altera SDK for the OpenCL high-level synthesis tool allows developers to utilize FPGA's potential without long development periods and extensive hardware knowledge. This article presents an optimized implementation of a k-means clustering algorithm on an FPGA using Altera SDK for OpenCL. Performance and power consumption is measured with various data, cluster, and dimension sizes. When compared to state-of-the-art solutions, this implementation supports larger cluster sizes, offers up to 21x speed over a CPU and is more power efficient than a GPU. Unlike previous implementations, it can deliver consistently high throughput across large or small feature dimensions given reasonable cluster sizes and large enough data size. © 2016 ACM 1936-7406/2016/09-ART6 $15.00.",FPGA; Hardware acceleration; High-level synthesis; K-means clustering; OpenCL,Cluster analysis; Data mining; Field programmable gate arrays (FPGA); Hardware; High level synthesis; Image processing; Iterative methods; Learning systems; Computationally intensive algorithms; Data mining algorithm; Hardware acceleration; Implementation support; K-means clustering; K-Means clustering algorithm; OpenCL; Optimized implementation; Clustering algorithms
A retargetable compilation framework for heterogeneous reconfigurable computing,2016,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055433016&doi=10.1145%2f2843946&partnerID=40&md5=27e686a7d41348ff8bc939b47b88f988,"The future trend in microprocessors for the more advanced embedded systems is focusing on massively parallel reconfigurable architectures, consisting of heterogeneous ensembles of hundreds of processing elements communicating over a reconfigurable interconnection network. However, the mastering of low-level microarchitectural details involved in the programming of such massively parallel platforms becomes too cumbersome, which limits their adoption in many applications. Thus, there is a dire need for an approach to produce high-performance scalable implementations that harness the computational resources of the emerging reconfigurable platforms. This article addresses the grand challenge of accessibility of these diverse reconfigurable platforms by suggesting the use of a high-level language, occam-pi, and developing a complete design flow for building, compiling, and generating machine code for heterogeneous coarse-grained hardware. We have evaluated the approach by implementing complex industrial case studies and three common signal processing algorithms. The results of the implemented case studies suggest that the occam-pi language-based approach, because of its well-defined semantics for expressing concurrency and reconfigurability, simplifies the development of applications employing runtime reconfigurable devices. The associated compiler framework ensures portability as well as the performance benefits across heterogeneous platforms. © 2016 ACM.",Compiler frameworks; Occam-pi; Reconfigurable processor arrays; Runtime reconfiguration,Embedded systems; High level languages; Interconnection networks (circuit switching); Parallel processing systems; Program compilers; Semantics; Signal processing; Common signal processing; Compiler frameworks; Occam-pi; Reconfigurable computing; Reconfigurable interconnection networks; Reconfigurable plat-forms; Reconfigurable processors; Run time reconfiguration; Reconfigurable architectures
Introduction to special section of international symposium on field-programmable gate arrays (FPGA) 2015,2016,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988360801&doi=10.1145%2f2955103&partnerID=40&md5=fb73ee33dc45d9f00095e39684aacc5d,[No abstract available],,
Fine-grained interconnect synthesis,2016,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060282423&doi=10.1145%2f2892641&partnerID=40&md5=27ad44c7215cd45b8d41f625b85f41e7,"One of the key challenges for the FPGA industry going forward is to make the task of designing hardware easier. A significant portion of that design task is the creation of the interconnect pathways between functional structures. We present a synthesis tool that automates this process and focuses on the interconnect needs in the fine-grained (sub-IP-block) design space. Here there are several issues that prior research and tools do not address well: The need to have fixed, deterministic latency between communicating units (to enable high-performance local communication without the area overheads of latency insensitivity), and the ability to avoid generating unnecessary arbitration hardware when the application design can avoid it. Using a design example, our tool generates interconnect that requires 69% fewer lines of specification code than a handwritten Verilog implementation, which is a 32% overall reduction for the entire application. The resulting system, while requiring 6% more total functional and interconnect area, achieves the same performance. We also show a quantitative and qualitative advantages against an existing commercial interconnect synthesis tool, over which we achieve a 25% performance advantage and 15%/57% logic/memory area savings. © 2016 ACM.",Automated synthesis; FPGA; Interconnect,Computer networks; Computer science; Application design; Automated synthesis; Functional structure; Interconnect; Interconnect synthesis; Latency insensitivities; Local communications; Qualitative advantages; Field programmable gate arrays (FPGA)
Hardware accelerated alignment algorithm for optical labeled genomes,2016,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988409362&doi=10.1145%2f2840811&partnerID=40&md5=bb241028924bfebad27b1d2975ae9819,"De novo assembly is a widely used methodology in bioinformatics. However, the conventional short-readbased de novo assembly is incapable of reliably reconstructing the large-scale structures of human genomes. Recently, a novel optical label-based technology has enabled reliable large-scale de novo assembly. Despite its advantage in large-scale genome analysis, this new technology requires a more computationally intensive alignment algorithm than its conventional counterpart. For example, the runtime of reconstructing a human genome is on the order of 10,000 hours on a sequential CPU. Therefore, in order to practically apply this new technology in genome research, accelerated approaches are desirable. In this article, we present three different accelerated approaches, multicore CPU, GPU, and FPGA. Against the sequential software baseline, our multicore CPU design achieved an 8.4× speedup, while the GPU and FPGA designs achieved 13.6× and 115× speedups, respectively. We also discuss the details of the design space exploration of this new assembly algorithm on these three different devices. Finally, we compare these devices in performance, optimization techniques, prices, and design efforts. © 2016 ACM.",Acceleration; De novo assembly; FPGA; Genome; GPU,Acceleration; Bioinformatics; Field programmable gate arrays (FPGA); Integrated circuit design; Optimization; Reconfigurable hardware; Alignment algorithms; Assembly algorithm; De novo assemblies; Design space exploration; Hardware-accelerated; Large scale structures; Optimization techniques; Sequential softwares; Genes
Impact of parallelism and memory architecture on FPGA communication energy,2016,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984653700&doi=10.1145%2f2857057&partnerID=40&md5=4ff3820e28f9e34d36e3fe88b0623fe2,"The energy in FPGA computations is dominated by data communication energy, either in the form of memory references or data movement on interconnect. In this article, we explore how to use data placement and parallelism to reduce communication energy. We show that parallelism can reduce energy and that the optimal level of parallelism increases with the problem size. We further explore how FPGA memory architecture (memory block size(s), memory banking, and spacing between memory banks) can impact communication energy, and determine how to organize the memory architecture to guarantee that the energy overhead compared to the optimally matched architecture for the design is never more than 60%. We specifically show that an architecture with 32 bit wide, 16Kb internally banked memories placed every 8 columns of 10 4-LUT logic blocks is within 61% of the optimally matched architecture across the VTR 7 benchmark set and a set of parallelism-tunable benchmarks. Without internal banking, the worst-case overhead is 98%, achieved with an architecture with 32 bit wide, 8Kb memories placed every 9 columns, roughly comparable to the memory organization on the Cyclone V (where memories are placed about every 10 columns). Monolithic 32 bit wide, 16Kb memories placed every 10 columns (comparable to 18Kb and 20Kb memories used in Virtex 4 and Stratix V FPGAs) have a 180% worst-case energy overhead. Furthermore, we show practical cases where designs mapped for optimal parallelism use 4.7× less energy than designs using a single processing element.",Architecture; Banking; Communication; Energy; FPGA; Memory; Power,Architecture; Communication; Data storage equipment; Field programmable gate arrays (FPGA); Storms; Banking; Communication energy; Data-communication; Energy; Memory organizations; Memory references; Power; Processing elements; Memory architecture
Shared memory multicore microblaze system with SMP Linux support,2016,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981308314&doi=10.1145%2f2870638&partnerID=40&md5=e8147da3173751491eb450b74ad1210b,"In this work, we present PolyBlaze, a scalable and configurable multicore platform for FPGA-based embedded systems and systems research. PolyBlaze is an extension of the MicroBlaze soft processor, leveraging the configurability of the MicroBlaze and bringing it into the multicore era with Linux Symmetric Multi- Processor (SMP) support. This work details the hardware modifications required for the MicroBlaze processor and its software stack to enable fully validated SMP operations, including atomic operation support, shared interrupts and timers, and exception handling. New in this work, we present a scalable and flexible memory hierarchy optimized for Field Programmable Gate Arrays (FPGAs), which manages atomic operations and provides support for future flexible memory hierarchies and heterogeneous systems. Also new is an in-depth analysis of key performance characteristics, including memory bandwidth, latency, and resource usage. For all system configurations, bandwidth is found to scale linearly with the addition of processor cores until the memory interface is saturated. Additionally, average memory latency remains constant until the memory interface is saturated; after which, it scales linearly with each additional processor core. © 2016 ACM.",Computer architecture; FPGA-based; Linux; Multiprocessing systems; Performance evaluation,Bandwidth; Computer operating systems; Embedded systems; Field programmable gate arrays (FPGA); Linux; Memory architecture; Multiprocessing systems; Reconfigurable hardware; Exception handling; Hardware modifications; Heterogeneous systems; Multi-core platforms; Performance characteristics; Performance evaluation; Symmetric multi-processors; System configurations; Computer architecture
FPGA-based dynamically reconfigurable SQL query processing,2016,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984660700&doi=10.1145%2f2845087&partnerID=40&md5=74aa67c0e6323ec6bee3758345717e42,"In this article, we propose an FPGA-based SQL query processing approach exploiting the capabilities of partial dynamic reconfiguration of modern FPGAs. After the analysis of an incoming query, a query-specific hardware processing unit is generated on the fly and loaded on the FPGA for immediate query execution. For each query, a specialized hardware accelerator pipeline is composed and configured on the FPGA from a set of presynthesized hardware modules. These partially reconfigurable hardware modules are gathered in a library covering all major SQL operations like restrictions and aggregations, as well as more complex operations such as joins and sorts. Moreover, this holistic query processing approach in hardware supports different data processing strategies including row-as column-wise data processing in order to optimize data communication and processing. This article gives an overview of the proposed query processing methodology and the corresponding library of modules. Additionally, a performance analysis is introduced that is able to estimate the processing time of a query for different processing strategies and different communication and processing architecture configurations. With the help of this performance analysis, architectural bottlenecks may be exposed and future optimized architectures, besides the two prototypes presented here, may be determined. © 2016 ACM.",Dynamic partial reconfiguration; FPGA; Reconfigurable computing; SQL processing,Architecture; Computer hardware; Data handling; Dynamic models; Field programmable gate arrays (FPGA); Hardware; Memory architecture; Query languages; Query processing; Reconfigurable architectures; Reconfigurable hardware; Dynamic partial reconfiguration; Optimized architectures; Partial dynamic reconfiguration; Performance analysis; Processing architectures; Processing strategies; Reconfigurable computing; SQL query processing; Pipeline processing systems
ODoST: Automatic hardware acceleration for biomedical model integration,2016,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046296721&doi=10.1145%2f2870639&partnerID=40&md5=5daccdd1260f2aa9dfbea0fd69ad6ab9,"Dynamic biomedical systems are mathematically described by Ordinary Differential Equations (ODEs) and their solution is often one of the most computationally intensive parts in biomedical simulations. With high inherent parallelism, hardware acceleration based on Field-Programmable Gate Arrays (FPGAS) has great potential to increase the computational performance of the model simulations, while being very powerefficient. However, the manual hardware implementation is complex and time consuming. The advantages of FPGA designs can only be realised if there is a general solution to automate the process. In this article, we propose a domain-specific high-level synthesis tool called ODoST that automatically generates an FPGA-based Hardware Accelerator Module (HAM) from a high-level description. In this direct approach, ODE equations are directly mapped to processing pipelines without any intermediate architecture layer of processing elements. We evaluate the generated HAMs on real hardware based on their resource usage, processing speed, and power consumption, and compare them with CPUs and a GPU. The results show that FPGA implementations can achieve 15.3 times more speedup compared to a single core CPU solution and perform similarly to an auto-generated GPU solution, while the FPGA implementations can achieve 14.5 times more power efficiency than the CPU and 3.1 times compared to the optimised GPU solution. Improved speedups are foreseeable based on further optimisations. © ACM 2016.",biomedical modeling; high-level synthesis; High-performance reconfigurable computing,Bioinformatics; Computer hardware; Field programmable gate arrays (FPGA); Graphics processing unit; High level synthesis; Ordinary differential equations; Program processors; Reconfigurable architectures; Bio-medical models; Biomedical simulation; Computational performance; FPGA-based hardware accelerators; Hardware acceleration; Hardware implementations; High level description; High performance reconfigurable computing; Pipeline processing systems
Application of specific delay window routing for timing optimization in FPGA designs,2016,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981333812&doi=10.1145%2f2892640&partnerID=40&md5=a6fb112fb0a2f2855ddd1ce4eea405d6,"In addition to optimizing for long-path timing and routability, commercial FPGA routing engines must also optimize for various timing constraints, enabling users to fine tune their designs. These timing constraints involve both long- and short-path timing requirements. The intricacies of commercial FPGA architectures add difficulty to the problem of supporting such constraints. In this work, we introduce specific delay window routing as a general method for optimization during the routing stage of the FPGA design flow, which can be applied to various timing constraints constituting both long- and short-path requirements. Furthermore, we propose a key adjustment to standard FPGA routing technology for the purposes of specific delay window routing. By using dual-wave expansion instead of traditional single-wave expansion, we solve the critical issue of inaccurate delay estimation in our wave search, which would otherwise make routing according to a specific delay window difficult. Our results show that this dual-wave method can support stricter timing constraints than the standard single-wave method. For a suite of designs with constraints requiring connections to meet a target delay within 250ps, our dual-wave method could satisfy the requirement for all designs, whereas the single-wave method failed for more than two thirds of the designs. © 2016 ACM.",Graph traversal; Optimization; Routing,Design; Optimization; Reconfigurable hardware; Critical issues; Delay estimation; FPGA architectures; Graph traversals; Routing; Timing constraints; Timing optimization; Timing requirements; Field programmable gate arrays (FPGA)
Modular switched multiported SRAM-Based memories,2016,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045990502&doi=10.1145%2f2851506&partnerID=40&md5=ab1975a1408e54ecb6d1d8f2104db6bf,"Multiported RAMs are essential for high-performance parallel computation systems. VLIW and vector processors, CGRAs, DSPs, CMPs, and other processing systems often rely upon multiported memories for parallel access. Although memories with a large number of read and write ports are important, their high implementation cost means that they are used sparingly. As a result, FPGA vendors only provide dualported block RAMs (BRAMs) to handle the majority of usage patterns. Furthermore, recent attempts to create FPGA-based multiported memories suffer from low storage utilization. Whereas most approaches provide simple unidirectional ports with a fixed read or write, others propose true bidirectional ports where each port dynamically switches read and write. True RAM ports are useful for systems with transceivers and provide high RAM flexibility; however, this flexibility incurs high BRAM consumption. In this article, a novel, modular, and BRAM-based switched multiported RAM architecture is proposed. In addition to unidirectional ports with fixed read/write, this switched architecture allows a group of write ports to switch with another group of read ports dynamically, hence altering the number of active ports. The proposed switched-ports architecture is less flexible than a true-multiported RAM where each port is switched individually. Nevertheless, switched memories can dramatically reduce BRAM consumption compared to true ports for systems with alternating port requirements. Previous live-value-table (LVT) and XOR approaches are merged and optimized into a generalized and modular structure that we call an invalidation-based live-value-table (ILVT). Like a regular LVT, the I-LVT determines the correct bank to read from, but it differs in how updates to the table are made; the LVT approach requires multiple write ports, often leading to an area-intensive register-based implementation, whereas the XOR approach suffers from excessive storage overhead since wider memories are required to accommodate the XOR-ed data. Two specific I-LVT implementations are proposed and evaluated: binary and thermometer coding. The I-LVT approach is especially suitable for deep memories because the table is implemented only in SRAM cells. The I-LVT method gives higher performance while occupying fewer BRAMs than earlier approaches: for several configurations, BRAM usage is reduced by greater than 44% and clock speed is improved by greater than 76%. The I-LVT can be used with fixed ports, true ports, or the proposed switched ports architectures. Formal proofs for the suggested methods, resources consumption analysis, usage guidelines, and analytic comparison to other methods are provided. A fully parameterized Verilog implementation is released as an open source library. The library has been extensively tested using Altera's EDA tools. © 2016 ACM.",Block RAM; Cache memory; Embedded memory; Multiported memory; Parallel memory access; Programmable memory; Register file; Shared memory,Buffer storage; Field programmable gate arrays (FPGA); Memory architecture; Parallel processing systems; Static random access storage; Very long instruction word architecture; Block rams; Embedded memory; Parallel memory; Programmable memory; Register files; Shared memory; Cache memory
Open-source variable-precision floating-point library for major commercial FPGAs,2016,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009415121&doi=10.1145%2f2851507&partnerID=40&md5=983fb4d16ae166b050530fd75b11b043,"There is increased interest in implementing floating-point designs for different precisions that take advantage of the flexibility offered by Field-Programmable Gate Arrays (FPGAs). In this article, we present updates to the Variable-precision FLOATing Point Library (VFLOAT) developed at Northeastern University and highlight recent improvements in implementations for implementing reciprocal, division, and square root components that scale to double precision for FPGAs from the two major vendors: Altera and Xilinx. Our library is open source and flexible and provides the user with many options. A designer has many tradeoffs to consider including clock frequency, total latency, and resource usage as well as target architecture. We compare the generated cores to those produced by each vendor and to another popular open-source tool: FloPoCo. VFLOAT has the advantage of not tying the user's design to a specific target architecture and of providing the maximum flexibility for all options including clock frequency and latency compared to other alternatives. Our results show that variable-precision as well as double-precision designs can easily be accommodated and the resulting components are competitive and in many cases superior to the alternatives.",Cross-platform; Floating point; FPGA; Variable precision,Clocks; Digital arithmetic; Cross-platform; Double precision; Floating points; Floating-point designs; Northeastern University; Open source tools; Target architectures; Variable precision; Field programmable gate arrays (FPGA)
Optimizing soft vector processing in FPGA-based embedded systems,2016,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973400109&doi=10.1145%2f2912884&partnerID=40&md5=5e7b0c5780e888d2d4ee06c91ae86df1,"Soft vector processors can augment and extend the capability of FPGA-based embedded systems-on-chip such as the Xilinx Zynq. However, configuring and optimizing the soft processor for best performance is hard. We must consider architectural parameters such as precision, vector lane count, vector length, chunk size, and DMA scheduling to ensure efficient execution of code on the soft vector processing platform. To simplify the design process, we develop a compiler framework and an autotuning runtime that splits the optimization into a combination of static and dynamic passes that map data-parallel computations to the soft processor. We compare and contrast implementations running on the scalar ARM processor, the embedded NEON hard vector engine, and low-level streaming Verilog designs with the VectorBlox MXP soft vector processor. Across a range of data-parallel benchmarks, we show that the MXP soft vector processor can outperform other organizations by up to 4× while saving ≈10% dynamic power. Our compilation and runtime framework is also able to outperform the GCC NEON vectorizer under certain conditions by explicit generation of NEON intrinsics and performance tuning of the autogenerated data-parallel code. When constrained by IO bandwidth, soft vector processors are even competitive with spatial Verilog implementations of computation. © 2016 ACM.",Soft processors; Streaming computations; Vector processors,Array processing; Embedded systems; Field programmable gate arrays (FPGA); Parallel processing systems; Reconfigurable hardware; System-on-chip; Vectors; Architectural parameters; Design process; Performance tuning; Runtime frameworks; Soft processors; Streaming computations; Vector processing; Vector processors; Program compilers
"The unified accumulator architecture: A configurable, portable, and extensible floating-point accumulator",2016,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973334465&doi=10.1145%2f2809432&partnerID=40&md5=da039bc115c60b3c3980670450c6fc4e,"Applications accelerated by field-programmable gate arrays (FPGAs) often require pipelined floating-point accumulators with a variety of different trade-offs. Although previous work has introduced numerous floating-point accumulation architectures, few cores are available for public use, which forces designers to use fixed-point implementations or vendor-provided cores that are not portable and are often not optimized for the desired set of trade-offs. In this article, we combine and extend previous floating-point accumulator architectures into a configurable, open-source core, referred to as the unified accumulator architecture (UAA), which enables designers to choose between different trade-offs for different applications. UAA is portable across FPGAs and allows designers to specialize the underlying adder core to take advantage of devicespecific optimizations. By providing an extensible, open-source implementation, we hope for the research community to extend the provided core with new architectures and optimizations. © 2016 ACM.",Floating-point accumulation; FPGA; Reduction circuits,Commerce; Digital arithmetic; Economic and social effects; Field programmable gate arrays (FPGA); Reconfigurable hardware; Fixed-point implementation; Floating points; Open source implementation; Open sources; Research communities; Trade off; Computer architecture
Introduction to special issue on reconfigurable components with source code,2016,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973390660&doi=10.1145%2f2907949&partnerID=40&md5=cce77ba2319d41b9183c9a7bd7c5d4d2,[No abstract available],Design reuse; Source code,
A parallel sliding-window generator for high-performance digital-signal processing on FPGAs,2016,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973320549&doi=10.1145%2f2800789&partnerID=40&md5=c4cab7ae2eb2840bedb8d56c6b0c0d90,"Sliding-window applications, an important class of the digital-signal processing domain, are highly amenable to pipeline parallelism on field-programmable gate arrays (FPGAs). Although memory bandwidth often restricts parallelism for many applications, sliding-window applications can leverage custom buffers, referred to as sliding-window generators, that provide massive input bandwidth that far exceeds the capabilities of external memory. Previous work has introduced a variety of sliding-window generators, but those approaches typically generate at most one window per cycle, which significantly restricts parallelism. In this article, we address this limitation with a parallel sliding-window generator that can generate a configurable number of windows every cycle. Although in practice the number of parallel windows is limited by memory bandwidth, we show that even with common bandwidth limitations, the presented generator enables near-linear speedups up to 16x faster than previous FPGA studies that generate a single window per cycle, which were already in some cases faster than graphics-processing units and microprocessors. © 2016 ACM.",FPGA; Parallelism; Pipelining; Sliding-window applications,Bandwidth; Computer graphics; Digital signal processing; Graphics processing unit; Pipe linings; Pipeline processing systems; Program processors; Bandwidth limitation; External memory; Field programmable gate array (FPGAs); Linear speed-up; Memory bandwidths; Parallelism; Pipeline parallelisms; Sliding-window applications; Field programmable gate arrays (FPGA)
Guest editorial RAW 2014,2016,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969662828&doi=10.1145%2f2841314&partnerID=40&md5=f7ee3b3275f479d549f864c1b9c7ee71,[No abstract available],,
Compact and on-the-fly secure dynamic reconfiguration for volatile FPGAs,2016,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964324449&doi=10.1145%2f2816822&partnerID=40&md5=24b4c05b1a6e264426d2c922339f46a9,"The dynamic partial reconfiguration functionality of FPGAs can be attacked, particularly when the FPGA is remotely located or the configuration bitstreams are sent through insecure networks. The existing FPGA technologies provide some built-in security mechanisms; however, these are often inadequate. The existing solutions still impose a significant impact on the reconfiguration process and on the available resources. This article proposes a solution to improve the security of dynamic partial reconfiguration of FPGAs, without significantly affecting the reconfiguration performance. The proposed solution changes the encryption key of the remotely received bitstream by a randomly generated key, unique for each configuration, when storing them in the external unsecured memory. The native frame-wise error detection mechanism combined with an additional CBC-MAC authentication mechanism, allows for an improved countermeasure against replay attack and wrongful bitstream usage. The proposed solution introduces an overhead of 1% of the available resources on the target FPGA and provides the lowest impact on the reconfiguration process when compared to the state of the art, achieving a reconfiguration throughput of 2.5Gbps. Regarding the built-in security mechanism provided by the Xilinx FPGAs, the solution herein proposed provides better security and improves the reconfiguration performance by more than 3 times. © 2016 ACM.",And system downgrade prevention; Bitstream encryption; Bitstream security; Reconfigurable architectures; Secure dynamic partial reconfiguration,Binary sequences; Bit error rate; Cryptography; Dynamic models; Dynamics; Field programmable gate arrays (FPGA); Reconfigurable architectures; And system downgrade prevention; Authentication mechanisms; Bit stream; Bitstream encryption; Dynamic partial reconfiguration; Dynamic re-configuration; Error-detection mechanism; Reconfiguration process; Reconfigurable hardware
Across Time and Space: Senju's Approach for Scaling Iterative Stencil Loop Accelerators on Single and Multiple FPGAs,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184519166&doi=10.1145%2f3634920&partnerID=40&md5=413585b3c33b7c36a420b7c7ddaf2113,"Stencil-based applications play an essential role in high-performance systems as they occur in numerous computational areas, such as partial differential equation solving. In this context, Iterative Stencil Loops (ISLs) represent a prominent and well-known algorithmic class within the stencil domain. Specifically, ISL-based calculations iteratively apply the same stencil to a multi-dimensional point grid multiple times or until convergence. However, due to their iterative and intensive nature, ISLs are highly performance-hungry, demanding specialized solutions. Here, Field Programmable Gate Arrays (FPGAs) represent a valid architectural choice as they enable the design of custom, parallel, and scalable ISL accelerators. Besides, the regular structure of ISLs makes them an ideal candidate for automatic optimization and generation flows. For these reasons, this article introduces Senju, an automation framework for the design of highly parallel ISL accelerators targeting single-/multi-FPGA systems. Given an input description, Senju automates the entire design process and provides accurate performance estimations. The experimental evaluation shows remarkable and scalable results, outperforming single- and multi-FPGA literature approaches under different metrics. Finally, we present a new analysis of temporal and spatial parallelism trade-offs in a real-case scenario and discuss our performance through a single- and novel specialized multi-FPGA formulation of the Roofline Model.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",automation framework; FPGA; Iterative Stencil Loops; multi-FPGA system; spatial parallelism; temporal parallelism,Economic and social effects; Integrated circuit design; Iterative methods; Array systems; Automation framework; Field programmable gate array; Field programmables; Iterative stencil loops; Multi-field; Multi-field programmable gate array system; Programmable gate array; Spatial parallelism; Temporal parallelism; Field programmable gate arrays (FPGA)
ScalaBFS2: A High-performance BFS Accelerator on an HBM-enhanced FPGA Chip,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195032298&doi=10.1145%2f3650037&partnerID=40&md5=d4bb2d4a9179c7c742acba03a1066f35,"The introduction of High Bandwidth Memory (HBM) to the FPGA chip makes it possible for an FPGA-based accelerator to leverage the huge memory bandwidth of HBM to improve its performance when implementing a specific algorithm, which is especially true for the Breadth-First Search (BFS) algorithm that demands a high bandwidth for accessing the graph data stored in memory. Different from traditional FPGA-DRAM platforms where memory bandwidth is the precious resource due to the limited DRAM channels, FPGA chips equipped with HBM have much higher memory bandwidths provided by the large quantities of HBM channels, but still a limited amount of logic (LUT, FF, and BRAM/URAM) resources. Therefore, the key to design a high-performance BFS accelerator on an HBM-enhanced FPGA chip is to efficiently use the logic resources to build as many as possible Processing Elements (PEs) and configure them flexibly to obtain as high as possible effective memory bandwidth that is useful to the algorithm from the HBM, rather than partially emphasizing the absolute memory bandwidth. To exploit as high as possible effective bandwidth from the HBM, ScalaBFS2 conducts BFS in graphs in a vertex-centric manner and proposes designs, including the independent module (HBM Reader) for memory accessing, multi-layer crossbar, and PEs that implement hybrid mode (i.e., capable of working in both push and pull modes) algorithm processing, to utilize the FPGA logic resources efficiently. Consequently, ScalaBFS2 is able to build up to 128 PEs on the XCU280 FPGA chip (produced with the 16 nm process and configured with two HBM2 stacks) of a Xilinx Alveo U280 board and achieves performance of 56.92 Giga Traversed Edges Per Second (GTEPS) by fully using its 32 HBM memory channels. Compared with the state-of-the-art graph processing system (i.e., ReGraph) built on top of the same board, ScalaBFS2 achieves 2.52x∼4.40x performance speedups. Moreover, when compared with Gunrock running on an Nvidia A100 GPU that is produced with the 7 nm process and configured with five HBM2e stacks, ScalaBFS2 achieves 1.34x∼2.40x speedups on absolute performance, and 7.35x∼13.18x speedups on power efficiency.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",breadth-first search; graph analytics; Hardware accelerators,Bandwidth; Computer circuits; Dynamic random access storage; Graphic methods; Bandwidth memory; Breadth-first-search; FPGA chips; Graph-analytic; Hardware accelerators; High bandwidth; Memory bandwidths; Memory channels; Performance; Processing elements; Field programmable gate arrays (FPGA)
High-efficiency Compressor Trees for Latest AMD FPGAs,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195064098&doi=10.1145%2f3645097&partnerID=40&md5=15f1ed7222e7b7427499819cc6bd7497,"High-fan-in dot product computations are ubiquitous in highly relevant application domains, such as signal processing and machine learning. Particularly, the diverse set of data formats used in machine learning poses a challenge for flexible efficient design solutions. Ideally, a dot product summation is composed from a carry-free compressor tree followed by a terminal carry-propagate addition. On FPGA, these compressor trees are constructed from generalized parallel counters whose architecture is closely tied to the underlying reconfigurable fabric. This work reviews known counter designs and proposes new ones in the context of the new AMD Versal™ fabric. On this basis, we develop a compressor generator featuring variable-sized counters, novel counter composition heuristics, explicit clustering strategies, and case-specific optimizations like logic gate absorption. In comparison to the Vivado™ default implementation, the combination of such a compressor with a novel, highly efficient quaternary adder reduces the LUT footprint across different bit matrix input shapes by 45% for a plain summation and by 46% for a terminal accumulation at a slight cost in critical path delay still allowing an operation well above 500 MHz. We demonstrate the aptness of our solution at examples of low-precision integer dot product accumulation units.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Compressor tree; matrix compression; parallel counters,Compressors; Machine learning; Matrix algebra; Reconfigurable architectures; Reconfigurable hardware; Signal processing; Applications domains; Compressor trees; Dot product computations; High fan-in; Higher efficiency; Machine-learning; Matrix compression; Parallel counters; Signal machines; Signal-processing; Field programmable gate arrays (FPGA)
Designing an IEEE-Compliant FPU that Supports Configurable Precision for Soft Processors,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195072379&doi=10.1145%2f3650036&partnerID=40&md5=fc92b8d5d55b2030095eaf7c329f9085,"Field Programmable Gate Arrays (FPGAs) are commonly used to accelerate floating-point (FP) applications. Although researchers have extensively studied FPGA FP implementations, existing work has largely focused on standalone operators and frequency-optimized designs. These works are not suitable for FPGA soft processors which are more sensitive to latency, impose a lower frequency ceiling, and require IEEE FP standard compliance. We present an open-source floating-point unit (FPU) for FPGA RISC-V soft processors that is fully IEEE compliant with configurable levels of FP precision. Our design emphasizes runtime performance with 25% lower latency in the most common instructions compared to previous works while maintaining efficient resource utilization.Our FPU also allows users to explore various mantissa widths without having to rewrite or recompile their algorithms. We use this to investigate the scalability of our reduced-precision FPU across numerous microbenchmark functions as well as more complex case studies. Our experiments show that applications like the discrete cosine transformation and the Black-Scholes model can realize a speedup of more than 1.35x in conjunction with a 43% and 35% reduction in lookup table and flip-flop resources while experiencing less than a 0.025% average loss in numerical accuracy with a 16-bit mantissa width.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",floating point; FPGA; reduced precision; RISC-V; soft processor,Digital arithmetic; Discrete cosine transforms; Field programmable gate arrays (FPGA); Flip flop circuits; Image coding; Integrated circuit design; Investments; Regulatory compliance; Signal encoding; Field programmable gate array; Field programmables; Floating point implementation; Floating point units; Floating points; Optimized designs; Programmable gate array; Reduced precision; RISC-V; Soft processors; Table lookup
"R-Blocks: an Energy-Efficient, Flexible, and Programmable CGRA",2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195035668&doi=10.1145%2f3656642&partnerID=40&md5=efbc307fae08324e3580480eb5ba7405,"Emerging data-driven applications in the embedded, e-Health, and internet of things (IoT) domain require complex on-device signal analysis and data reduction to maximize energy efficiency on these energy-constrained devices. Coarse-grained reconfigurable architectures (CGRAs) have been proposed as a good compromise between flexibility and energy efficiency for ultra-low power (ULP) signal processing. Existing CGRAs are often specialized and domain-specific or can only accelerate simple kernels, which makes accelerating complete applications on a CGRA while maintaining high energy efficiency an open issue. Moreover, the lack of instruction set architecture (ISA) standardization across CGRAs makes code generation using current compiler technology a major challenge. This work introduces R-Blocks; a ULP CGRA with HW/SW co-design tool-flow based on the OpenASIP toolset. This CGRA is extremely flexible due to its well-established VLIW-SIMD execution model and support for flexible SIMD-processing, while maintaining an extremely high energy efficiency using software bypassing, optimized instruction delivery, and local scratchpad memories. R-Blocks is synthesized in a commercial 22-nm FD-SOI technology and achieves a full-system energy efficiency of 115 MOPS/mW on a common FFT benchmark, 1.45× higher than a highly tuned embedded RISC-V processor. Comparable energy efficiency is obtained on multiple complex workloads, making R-Blocks a promising acceleration target for general-purpose computing.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Coarse-grained reconfigurable architecture; code generation; Energy efficiency; HW/SW co-design,Codes (symbols); Embedded systems; Hardware-software codesign; Internet of things; Program compilers; Reconfigurable architectures; Signal processing; Very long instruction word architecture; Co-designs; Coarse grained reconfigurable architecture; Codegeneration; Data-driven applications; E health; Ehealth; Energy efficient; High energy efficiency; HW/SW co-design; Ultra-low power; Energy efficiency
HyBNN: Quantifying and Optimizing Hardware Efficiency of Binary Neural Networks,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195094299&doi=10.1145%2f3631610&partnerID=40&md5=c0d0059f5e5cd5025ce64f645dda7c46,"Binary neural network (BNN), where both the weight and the activation values are represented with one bit, provides an attractive alternative to deploy highly efficient deep learning inference on resource-constrained edge devices. However, our investigation reveals that, to achieve satisfactory accuracy gains, state-of-the-art (SOTA) BNNs, such as FracBNN and ReActNet, usually have to incorporate various auxiliary floating-point components and increase the model size, which in turn degrades the hardware performance efficiency. In this article, we aim to quantify such hardware inefficiency in SOTA BNNs and further mitigate it with negligible accuracy loss. First, we observe that the auxiliary floating-point (AFP) components consume an average of 93% DSPs, 46% LUTs, and 62% FFs, among the entire BNN accelerator resource utilization. To mitigate such overhead, we propose a novel algorithm-hardware co-design, called FuseBNN, to fuse those AFP operators without hurting the accuracy. On average, FuseBNN reduces AFP resource utilization to 59% DSPs, 13% LUTs, and 16% FFs. Second, SOTA BNNs often use the compact MobileNetV1 as the backbone network but have to replace the lightweight 3 × 3 depth-wise convolution (DWC) with the 3 × 3 standard convolution (SC, e.g., in ReActNet and our ReActNet-adapted BaseBNN) or even more complex fractional 3 × 3 SC (e.g., in FracBNN) to bridge the accuracy gap. As a result, the model parameter size is significantly increased and becomes 2.25× larger than that of the 4-bit direct quantization with the original DWC (4-Bit-Net); the number of multiply-accumulate operations is also significantly increased so that the overall LUT resource usage of BaseBNN is almost the same as that of 4-Bit-Net. To address this issue, we propose HyBNN, where we binarize depth-wise separation convolution (DSC) blocks for the first time to decrease the model size and incorporate 4-bit DSC blocks to compensate for the accuracy loss. For the ship detection task in synthetic aperture radar imagery on the AMD-Xilinx ZCU102 FPGA, HyBNN achieves a detection accuracy of 94.8% and a detection speed of 615 frames per second (FPS), which is 6.8× faster than FuseBNN+ (94.9% accuracy) and 2.7× faster than 4-Bit-Net (95.9% accuracy). For image classification on the CIFAR-10 dataset on the AMD-Xilinx Ultra96-V2 FPGA, HyBNN achieves 1.5× speedup and 0.7% better accuracy over SOTA FracBNN.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Binary neural network; FPGA accelerator; hardware efficiency; hybrid design; operator fusion,Convolution; Deep learning; Digital arithmetic; Efficiency; Integrated circuit design; Radar imaging; Synthetic aperture radar; Tracking radar; Accuracy loss; Binary neural networks; Floating points; FPGA accelerator; Hardware efficiency; Hybrid design; Model size; Operator fusion; Resources utilizations; State of the art; Field programmable gate arrays (FPGA)
AEKA: FPGA Implementation of Area-Efficient Karatsuba Accelerator for Ring-Binary-LWE-Based Lightweight PQC,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195070129&doi=10.1145%2f3637215&partnerID=40&md5=0801dc91ff6fc9377e3a926723901fea,"Lightweight PQC-related research and development have gradually gained attention from the research community recently. Ring-Binary-Learning-with-Errors (RBLWE)-based encryption scheme (RBLWE-ENC), a promising lightweight PQC based on small parameter sets to fit related applications (but not in favor of deploying popular fast algorithms like number theoretic transform). To solve this problem, in this article, we present a novel implementation of hardware acceleration for RBLWE-ENC based on Karatsuba algorithm, particularly on the field-programmable gate array (FPGA) platform. In detail, we have proposed an area-efficient Karatsuba Accelerator (AEKA) for RBLWE-ENC, based on three layers of innovative efforts. First of all, we reformulate the signal processing sequence within the major arithmetic component of the KA-based polynomial multiplication for RBLWE-ENC to obtain a new algorithm. Then, we have designed the proposed algorithm into a new hardware accelerator with several novel algorithm-to-architecture mapping techniques. Finally, we have conducted thorough complexity analysis and comparison to demonstrate the efficiency of the proposed accelerator, e.g., it involves 62.5% higher throughput and 60.2% less area-delay product (ADP) than the state-of-the-art design for n=512 (Virtex-7 device, similar setup). The proposed AEKA design strategy is highly efficient on the FPGA devices, i.e., small resource usage with superior timing, which can be integrated with other necessary systems for lightweight-oriented high-performance applications (e.g., servers). The outcome of this work is also expected to generate impacts for lightweight PQC advancement.  © 2024 Copyright held by the owner/author(s).",Area-efficient Karatsuba accelerator; field-programmable gate array (FPGA); hardware design; lightweight post-quantum cryptography; polynomial multiplication; Ring-Binary-Learning-with-Errors (RBLWE),Computer hardware; Integrated circuit design; Logic gates; Signal processing; Area-Efficient; Area-efficient karatsuba accelerator; Field programmables; Field-programmable gate array; Hardware design; Learning with Errors; Lightweight post-quantum cryptography; Polynomial multiplication; Post quantum cryptography; Programmable gate array; Ring-binary-learning-with-error; Field programmable gate arrays (FPGA)
Covert-channels in FPGA-enabled SmartSSDs,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195038401&doi=10.1145%2f3635312&partnerID=40&md5=3617f89bae7c8de51163a0696f7ddc57,"Cloud computing providers today offer access to a variety of devices, which users can rent and access remotely in a shared setting. Among these devices are SmartSSDs, which are solid-state disks (SSD) augmented with an FPGA, enabling users to instantiate custom circuits within the FPGA, including potentially malicious circuits for power and temperature measurement. Normally, cloud users have no remote access to power and temperature data, but with SmartSSDs they could abuse the FPGA component to instantiate circuits to learn this information. Additionally, custom power waster circuits can be instantiated within the FPGA. This paper shows for the first time that by leveraging ring oscillator sensors and power wasters, numerous covert-channels in FPGA-enabled SmartSSDs could be used to transmit information. This work presents two channels in single-tenant setting (SmartSSD is used by one user at a time) and two channels in multi-tenant setting (FPGA and SSD inside SmartSSD is shared by different users). The presented covert channels can reach close to 100% accuracy. Meanwhile, bandwidth of the channels can be easily scaled by cloud users renting more SmartSSDs as the bandwidth of the covert channels is proportional to number of SmartSSD used.  © 2024 Copyright held by the owner/author(s).",covert channels; information leakage; Ring oscillators; SmartSSDs,Bandwidth; Temperature measurement; Cloud-computing; Covert channels; Custom circuits; Information leakage; Malicious circuits; Power; Ring oscillator; Smartssd; Solid state disks; Two channel; Field programmable gate arrays (FPGA)
On the Malicious Potential of Xilinx's Internal Configuration Access Port (ICAP),2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195052345&doi=10.1145%2f3633204&partnerID=40&md5=02c4ade5a3176c7868700cac80b07908,"Field Programmable Gate Arrays (FPGAs) have become increasingly popular in computing platforms. With recent advances in bitstream format reverse engineering, the scientific community has widely explored static FPGA security threats. For example, it is now possible to convert a bitstream to a netlist, revealing design information, and apply modifications to the static bitstream based on this knowledge. However, a systematic study of the influence of the bitstream format understanding in regards to the security aspects of the dynamic configuration process, particularly for Xilinx's Internal Configuration Access Port (ICAP), is lacking. This article fills this gap by comprehensively analyzing the security implications of ICAP interfaces, which primarily support dynamic partial reconfiguration. We delve into the Xilinx bitstream file format, identify misconceptions in official documentation, and propose novel configuration (attack) primitives based on dynamic reconfiguration, i.e., create/read/update/delete circuits in the FPGA, without requiring pre-definition during the design phase. Our primitives are consolidated in a novel Stealthy Reconfigurable Adaptive Trojan framework to conceal Trojans and evade state-of-the-art netlist reverse engineering methods. As FPGAs become integral to modern cloud computing, this research presents crucial insights on potential security risks, including the possibility of a malicious tenant or provider altering or spying on another tenant's configuration undetected.  © 2024 Copyright held by the owner/author(s).",Bitstream Reverse Engineering; FPGA Security; Hardware Security; Hardware Trojans; ICAP,Binary sequences; Dynamic models; Field programmable gate arrays (FPGA); Integrated circuit design; Malware; Reconfigurable hardware; Reverse engineering; Bitstream reverse engineering; Bitstreams; Computing platform; Field programmable gate array security; Field programmables; Internal configuration access ports; Netlist; Programmable gate array; Scientific community; Trojans; Hardware security
AxOMaP: Designing FPGA-based Approximate Arithmetic Operators using Mathematical Programming,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195079884&doi=10.1145%2f3648694&partnerID=40&md5=e62817fe372247d1ad6686b2501653c8,"With the increasing application of machine learning (ML) algorithms in embedded systems, there is a rising necessity to design low-cost computer arithmetic for these resource-constrained systems. As a result, emerging models of computation, such as approximate and stochastic computing, that leverage the inherent error-resilience of such algorithms are being actively explored for implementing ML inference on resource-constrained systems. Approximate computing (AxC) aims to provide disproportionate gains in the power, performance, and area (PPA) of an application by allowing some level of reduction in its behavioral accuracy (BEHAV). Using approximate operators (AxOs) for computer arithmetic forms one of the more prevalent methods of implementing AxC. AxOs provide the additional scope for finer granularity of optimization, compared to only precision scaling of computer arithmetic. To this end, the design of platform-specific and cost-efficient approximate operators forms an important research goal. Recently, multiple works have reported the use of AI/ML-based approaches for synthesizing novel FPGA-based AxOs. However, most of such works limit the use of AI/ML to designing ML-based surrogate functions that are used during iterative optimization processes. To this end, we propose a novel data analysis-driven mathematical programming-based approach to synthesizing approximate operators for FPGAs. Specifically, we formulate mixed integer quadratically constrained programs based on the results of correlation analysis of the characterization data and use the solutions to enable a more directed search approach for evolutionary optimization algorithms. Compared to traditional evolutionary algorithms-based optimization, we report up to 21% improvement in the hypervolume, for joint optimization of PPA and BEHAV, in the design of signed 8-bit multipliers. Further, we report up to 27% better hypervolume than other state-of-the-art approaches to DSE for FPGA-based application-specific AxOs.  Copyright © 2024 held by the owner/author(s).",AI-based exploration; Approximate computing; arithmetic operator design; circuit synthesis,Computational efficiency; Constrained optimization; Embedded systems; Evolutionary algorithms; Inference engines; Integer programming; Integrated circuit design; Iterative methods; Machine learning; Mathematical operators; Stochastic models; Stochastic systems; Timing circuits; AI-based exploration; Approximate computing; Arithmetic operator design; Circuit synthesis; Computer arithmetic; Constrained systems; Hypervolume; Machine-learning; Optimisations; Power performance; Field programmable gate arrays (FPGA)
HierCGRA: A Novel Framework for Large-scale CGRA with Hierarchical Modeling and Automated Design Space Exploration,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195104568&doi=10.1145%2f3656176&partnerID=40&md5=966961b310eb553dddc68ae678f4b1d5,"Coarse-grained reconfigurable arrays (CGRAs) are promising design choices in computation-intensive domains, since they can strike a balance between energy efficiency and flexibility. A typical CGRA comprises processing elements (PEs) that can execute operations in applications and interconnections between them. Nevertheless, most CGRAs suffer from the ineffectiveness of supporting flexible architecture design and solving large-scale mapping problems. To address these challenges, we introduce HierCGRA, a novel framework that integrates hierarchical CGRA modeling, Chisel-based Verilog generation, LLVM-based data flow graph (DFG) generation, DFG mapping, and design space exploration (DSE). With the graph homomorphism (GH) mapping algorithm, HierCGRA achieves a faster mapping speed and higher PE utilization rate compared with the existing state-of-the-art CGRA frameworks. The proposed hierarchical mapping strategy achieves 41× speedup on average compared with the ILP mapping algorithm in CGRA-ME. Furthermore, the automated DSE based on Bayesian optimization achieves a significant performance improvement by the heterogeneity of PEs and interconnections. With these features, HierCGRA enables the agile development for large-scale CGRA and accelerates the process of finding a better CGRA architecture.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",automated design space exploration; CGRA modeling; hierarchical mapping,Automation; Computer aided design; Data flow analysis; Data flow graphs; Energy efficiency; Array model; Automated design; Automated design space exploration; Coarse-grained reconfigurable array modeling; Coarse-grained reconfigurable arrays; Dataflow graphs; Design space exploration; Hierarchical mapping; Large-scales; Processing elements; Conformal mapping
"Design, Calibration, and Evaluation of Real-time Waveform Matching on an FPGA-based Digitizer at 10 GS/s",2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195023971&doi=10.1145%2f3635719&partnerID=40&md5=df18332544f382e0e8199103e3483149,"Digitizing side-channel signals at high sampling rates produces huge amounts of data, while side-channel analysis techniques only need those specific trace segments containing Cryptographic Operations (COs). For detecting these segments, waveform-matching techniques have been established comparing the signal with a template of the CO's characteristic pattern. Real-time waveform matching requires highly parallel implementations as achieved by hardware design but also reconfigurability as provided by Field-Programmable Gate Arrays (FPGAs) to adapt the matching hardware to a specific CO pattern. However, currently proposed designs process the samples from analog-to-digital converters sequentially and can only process low sampling rates due to the limited clock speed of FPGAs. In this article, we present a parallel waveform-matching architecture capable of performing high-speed waveform matching on a high-end FPGA-based digitizer. We also present a workflow for calibrating the waveform-matching system to the specific pattern of the CO in the presence of hardware restrictions provided by the FPGA hardware. Our implementation enables waveform matching at 10 GS/s, offering a speedup of 50× compared to the fastest state-of-the-art implementation known to us. We demonstrate how to apply the technique for attacking the widespread XTS-AES algorithm using waveform matching to recover the encrypted tweak even in the presence of so-called systemic noise.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",feature selection; interval matching; Side-channel analysis; waveform matching,Analog to digital conversion; Integrated circuit design; Side channel attack; Cryptographic operations; Features selection; Field programmables; Interval matching; Matchings; Programmable gate array; Side-channel analysis; Waveform matching; Waveforms; Field programmable gate arrays (FPGA)
ExHiPR: Extended High-Level Partial Reconfiguration for Fast Incremental FPGA Compilation,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195035055&doi=10.1145%2f3617837&partnerID=40&md5=202659a8011d0a48f9cf71c06f932f4a,"Partial Reconfiguration (PR) is a key technique in the application design on modern FPGAs. However, current PR tools heavily rely on the developer to manually conduct PR module definition, floorplanning, and flow control at a low level. The existing PR tools do not consider High-Level-Synthesis languages either, which are of great interest to software developers. We propose HiPR, an open-source framework, to bridge the gap between HLS and PR. HiPR allows the developer to define partially reconfigurable C/C++ functions, instead of Verilog modules, to accelerate the FPGA incremental compilation and automate the flow from C/C++ to bitstreams. We use a lightweight Simulated Annealing floorplanner and show that it can produce high-quality PR floorplans an order of magnitude faster than analytic methods. By mapping Rosetta HLS benchmarks, we demonstrate that the incremental compilation can be accelerated by 3-10× compared with state-of-the-art Xilinx Vitis flow without performance loss, at the cost of 15-67% one-time overlay set-up time.  © 2024 Copyright held by the owner/author(s).",Dataflow; Floorplanning; Incremental Compilation; Latency Insensitive; Partial Reconfiguration; Streams,Benchmarking; C++ (programming language); Computer hardware description languages; High level synthesis; Integrated circuit design; Open source software; Open systems; Reconfigurable hardware; Simulated annealing; 'current; Application design; Dataflow; Floor-planning; High-level synthesis; Incremental compilation; Latency insensitive; Partial reconfiguration; Reconfiguration tools; Stream; Field programmable gate arrays (FPGA)
XVDPU: A High-Performance CNN Accelerator on the Versal Platform Powered by the AI Engine,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195033253&doi=10.1145%2f3617836&partnerID=40&md5=de0c082db5b6f67e364b21e51c3206c9,"Today, convolutional neural networks (CNNs) are widely used in computer vision applications. However, the trends of higher accuracy and higher resolution generate larger networks. The requirements of computation or I/O are the key bottlenecks. In this article, we propose XVDPU: the AI Engine (AIE)-based CNN accelerator on Versal chips to meet heavy computation requirements. To resolve the IO bottleneck, we adopt several techniques to improve data reuse and reduce I/O requirements. An arithmetic logic unit is further proposed that can better balance resource utilization, new feature support, and efficiency of the whole system. We have successfully deployed more than 100 CNN models with our accelerator. Our experimental results show that the 96-AIE-core implementation can achieve 1,653 frames per second (FPS) for ResNet50 on VCK190, which is 9.8× faster than the design on ZCU102 running at 168.5 FPS. The 256-AIE-core implementation can further achieve 4,050 FPS. We propose a tilling strategy to achieve feature-map-stationary for high-definition CNN with the accelerator, achieving 3.8× FPS improvement on the residual channel attention network and 3.1× on super-efficient super-resolution. This accelerator can also solve the 3D convolution task in disparity estimation, achieving end-to-end performance of 10.1 FPS with all the optimizations.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",ACAP; acceleration; AI Engine; ALU engine; CNN; FPGA; hardware heterogeneous architecture; Versal,Computation theory; Convolution; Convolutional neural networks; Engines; Network architecture; ACAP; AI engine; ALU engine; Computer vision applications; Convolutional neural network; Frames per seconds; Hardware heterogeneous architecture; Heterogeneous architectures; Performance; Versal; Field programmable gate arrays (FPGA)
GraphScale: Scalable Processing on FPGAs for HBM and Large Graphs,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195070655&doi=10.1145%2f3616497&partnerID=40&md5=4a47e8289c3dfcd5d3b54c77a9dd34d5,"Recent advances in graph processing on FPGAs promise to alleviate performance bottlenecks with irregular memory access patterns. Such bottlenecks challenge performance for a growing number of important application areas like machine learning and data analytics. While FPGAs denote a promising solution through flexible memory hierarchies and massive parallelism, we argue that current graph processing accelerators either use the off-chip memory bandwidth inefficiently or do not scale well across memory channels.In this work, we propose GraphScale, a scalable graph processing framework for FPGAs. GraphScale combines multi-channel memory with asynchronous graph processing (i.e., for fast convergence on results) and a compressed graph representation (i.e., for efficient usage of memory bandwidth and reduced memory footprint). GraphScale solves common graph problems like breadth-first search, PageRank, and weakly connected components through modular user-defined functions, a novel two-dimensional partitioning scheme, and a high-performance two-level crossbar design. Additionally, we extend GraphScale to scale to modern high-bandwidth memory (HBM) and reduce partitioning overhead of large graphs with binary packing.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",FPGA; Graph processing; HBM,Bandwidth; Data Analytics; Application area; Bandwidth memory; Graph processing; High bandwidth; High-bandwidth memory; Large graphs; Memory access patterns; Memory bandwidths; Performance; Performance bottlenecks; Field programmable gate arrays (FPGA)
The Open-source DeLiBA2 Hardware/Software Framework for Distributed Storage Accelerators,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195089161&doi=10.1145%2f3624482&partnerID=40&md5=8c07a6321d80e81041625fddb2ee058d,"With the trend towards ever larger ""big data""applications, many of the gains achievable by using specialized compute accelerators become diminished due to the growing I/O overheads. While there have been several research efforts into computational storage and FPGA implementations of the NVMe interface, to our knowledge, there have been only very limited efforts to move larger parts of the Linux block I/O stack into FPGA-based hardware accelerators. Our hardware/software framework DeLiBA initially addressed this deficiency by allowing high-productivity development of software components of the I/O stack in user instead of kernel space and leverages a proven FPGA SoC framework to quickly compose and deploy the actual FPGA-based I/O accelerators. In its initial form, it achieves 10% higher throughput and up to 2.3× the I/Os per second (IOPS) for a proof-of-concept Ceph accelerator running in a real multi-node Ceph cluster. In DeLiBA2, we have extended the framework further to better support distributed storage systems, specifically by directly integrating the block I/O accelerators with a hardware-accelerated network stack, as well as by accelerating more storage functions. With these improvements, performance grows significantly: The cluster-level speedups now reach up to 2.8× for both throughput and IOPS relative to Ceph in software in synthetic benchmarks and achieve end-to-end wall-clock speedups of 20% for the real workload of building a large software package.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",application and architecture; FPGA acceleration; FPGA architecture; High-level synthesis; Linux; open-source; programming tools,Benchmarking; Computer architecture; Digital storage; Field programmable gate arrays (FPGA); Linux; Multiprocessing systems; Network architecture; Open source software; Open systems; System-on-chip; Application and architecture; Big data applications; Distributed storage; FPGA acceleration; FPGA architectures; Hardware/software; High-level synthesis; Open-source; Programming tools; Software frameworks; High level synthesis
An Efficient FPGA-based Depthwise Separable Convolutional Neural Network Accelerator with Hardware Pruning,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197777088&doi=10.1145%2f3615661&partnerID=40&md5=35e2b08cf814215a31294c8f3f4c1258,"Convolutional neural networks (CNNs) have been widely deployed in computer vision tasks. However, the computation and resource intensive characteristics of CNN bring obstacles to its application on embedded systems. This article proposes an efficient inference accelerator on Field Programmable Gate Array (FPGA) for CNNs with depthwise separable convolutions. To improve the accelerator efficiency, we make four contributions: (1) an efficient convolution engine with multiple strategies for exploiting parallelism and a configurable adder tree are designed to support three types of convolution operations; (2) a dedicated architecture combined with input buffers is designed for the bottleneck network structure to reduce data transmission time; (3) a hardware padding scheme to eliminate invalid padding operations is proposed; and (4) a hardware-assisted pruning method is developed to support online tradeoff between model accuracy and power consumption. Experimental results show that for MobileNetV2 the accelerator achieves 10× and 6× energy efficiency improvement over the CPU and GPU implementation, and 302.3 frames per second and 181.8 GOPS performance that is the best among several existing single-engine accelerators on FPGAs. The proposed hardware-assisted pruning method can effectively reduce 59.7% power consumption at the accuracy loss within 5%.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",bottleneck; CNN accelerator; depthwise-seperable convolution; model compression,Computer hardware; Convolutional neural networks; Electric power utilization; Embedded systems; Energy efficiency; Engines; Field programmable gate arrays (FPGA); Network architecture; Trees (mathematics); Bottleneck; Convolutional neural network; Convolutional neural network accelerator; Depthwise-seperable convolution; Field programmables; Hardware-assisted; ITS applications; Model compression; Programmable gate array; Pruning methods; Convolution
Eciton: Very Low-power Recurrent Neural Network Accelerator for Real-time Inference at the Edge,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198559898&doi=10.1145%2f3629979&partnerID=40&md5=36d0b1cea09482dbca57a5fc9773da67,"This article presents Eciton, a very low-power recurrent neural network accelerator for time series data within low-power edge sensor nodes, achieving real-time inference with a power consumption of 17 mW under load. Eciton reduces memory and chip resource requirements via 8-bit quantization and hard sigmoid activation, allowing the accelerator as well as the recurrent neural network model parameters to fit in a low-cost, low-power Lattice iCE40 UP5K FPGA. We evaluate Eciton on multiple, established time-series classification applications including predictive maintenance of mechanical systems, sound classification, and intrusion detection for IoT nodes. Binary and multi-class classification edge models are explored, demonstrating that Eciton can adapt to a variety of deployable environments and remote use cases. Eciton demonstrates real-time processing at a very low power consumption with minimal loss of accuracy on multiple inference scenarios with differing characteristics, while achieving competitive power efficiency against the state-of-the-art of similar scale. We show that the addition of this accelerator actually reduces the power budget of the sensor node by reducing power-hungry wireless transmission. The resulting power budget of the sensor node is small enough to be powered by a power harvester, potentially allowing it to run indefinitely without a battery or periodic maintenance. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",binary classification; CPS; edge IoT; FPGA; iCE40; low power; LSTM; multi-class classification; neural networks; predictive maintenance; quantization; RNN,Budget control; Classification (of information); Classifiers; Costs; Electric power utilization; Internet of things; Intrusion detection; Learning systems; Long short-term memory; Low power electronics; Maintenance; Sensor nodes; Time series; Binary classification; CPS; Edge IoT; Ice40; Low Power; LSTM; Multi-class classification; Neural-networks; Predictive maintenance; Quantisation; RNN; Field programmable gate arrays (FPGA)
An All-digital Compute-in-memory FPGA Architecture for Deep Learning Acceleration,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198729023&doi=10.1145%2f3640469&partnerID=40&md5=ec223288eef5014575a9584929276a82,"Field Programmable Gate Array (FPGA) is a versatile and programmable hardware platform, which makes it a promising candidate for accelerating Deep Neural Networks (DNNs). However, FPGA's computing energy efficiency is low due to the domination of energy consumption by interconnect data movement. In this article, we propose an all-digital Compute-in-memory FPGA architecture for deep learning acceleration. Furthermore, we present a bit-serial computing circuit of the Digital CIM core for accelerating vector-matrix multiplication (VMM) operations. A Network-CIM-deployer (NCIMD) is also developed to support automatic deployment and mapping of DNN networks. NCIMD provides a user-friendly API of DNN models in Caffe format. Meanwhile, we introduce a Weight-stationary dataflow and describe the method of mapping a single layer of the network to the CIM array in the architecture. We conduct experimental tests on the proposed FPGA architecture in the field of Deep Learning (DL), as well as in non-DL fields, using different architectural layouts and mapping strategies. We also compare the results with the conventional FPGA architecture. The experimental results show that compared to the conventional FPGA architecture, the energy efficiency can achieve a maximum speedup of 16.1×, while the latency can decrease up to 40% in our proposed CIM FPGA architecture. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesDeep neural networks; Compute-In-Memory; FPGA architecture; Toolchain; Verilog-to-Routing,E-learning; Energy efficiency; Energy utilization; Field programmable gate arrays (FPGA); Green computing; Mapping; Memory architecture; Network architecture; Network layers; Additional key word and phrasesdeep neural network; Array architecture; Compute-in-memory; Field programmable gate array architecture; Field programmables; Key words; Neural-networks; Programmable gate array; Routings; Toolchain; Verilog-to-routing; Deep neural networks
Exploring FPGA Switch-Blocks without Explicitly Listing Connectivity Patterns,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198544954&doi=10.1145%2f3597417&partnerID=40&md5=3db26d55d6922d072728f8af75a0534b,"Increased lower metal resistance makes physical aspects of Field-Programmable Gate Array (FPGA) switch-blocks more relevant than before. The need to navigate a design space where each individual switch can have significant impact on the FPGA's performance in turn makes automated switch-pattern exploration techniques increasingly appealing. However, most existing exploration techniques have a fundamental limitation - they use the CAD tools as a black box to evaluate the performance of explicitly listed switch-patterns. Given the time needed to route a modern circuit on a single architecture, the number of switch-patterns that can be explicitly tested quickly becomes negligible compared to the size of the design space. This article presents a technique that removes this fundamental limitation by making the entire design space visible to the router and letting it choose the switches to be added to the pattern, based on the requirements of the circuits being routed. The key to preventing the router from selecting arbitrary switches that would render the final pattern excessively large is to apply the same negotiation principle used by the router to remove congestion, just in the opposite direction, to make the signals reach a consensus on which switches are worthy of being included in the final switch-pattern.  © 2024 Copyright held by the owner/author(s).",algorithm; automated exploration; avalanche; design automation; FPGA; interconnect; multiplexer; optimization; PathFinder; router; switch; switch-block; switch-pattern,Automation; Computer aided logic design; Integrated circuit design; Automated exploration; Avalanche; Design automations; Field programmables; Field-programmable gate array; Interconnect; Multiplexer; Optimisations; Pathfinder; Programmable gate array; Switch blocks; Switch-pattern; Field programmable gate arrays (FPGA)
Introduction to the FPL 2021 Special Section,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196193610&doi=10.1145%2f3635115&partnerID=40&md5=e075158db7742d64ce11dfa55b3b2ccc,[No abstract available],,
Evaluating the Impact of Using Multiple-Metal Layers on the Layout Area of Switch Blocks for Tile-Based FPGAs in FinFET 7nm,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198563587&doi=10.1145%2f3639055&partnerID=40&md5=398a7c7d035ee08918bfb813d5bc0d9f,"A new area model for estimating the layout area of switch blocks is introduced in this work. The model is based on a realistic layout strategy. As a result, it not only takes into consideration the active area that is needed to construct a switch block but also the number of metal layers available and the actual dimensions of these metals. The model assigns metal layers to the routing tracks in a way that reduces the number of vias that are needed to connect different routing tracks together while maintaining the tile-based structure of FPGAs. It also takes into account the wiring area required for buffer insertion for long wire segments. The model is evaluated based on the layouts constructed in the ASAP7 FinFET 7nm Predictive Design Kit. We found that the new model, while specific to the layout strategy that it employs, improves upon the traditional active-based area estimation models by considering the growth of the metal area independently from the growth of the active area. As a result, the new model is able to more accurately estimate the layout area by predicting when the metal area will overtake the active area as the number of routing tracks is increased. This ability allows the more accurate estimation of the true layout cost of FPGA fabrics at the early floor planning and architectural exploration stage; and this increase in accuracy can encourage a wider use of custom FPGA fabrics that target specific sets of benchmarks in future SOC designs. Furthermore, our data indicate that the conclusions drawn from several significant prior architectural studies remain to be correct under FinFET geometries and wiring area considerations despite their exclusive use of active-only area models. This correctness is due to the small channel widths, around 30–60 tracks per channel, of the architectures that these studies investigate. For architectures that approach the channel width of modern commercial FPGAs with more than 100–200 tracks per channel, our data show that wiring area models justified by detailed layout considerations are an essential addition to active area models in the correct prediction of the implementation area of FPGAs. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",area estimation models; buffer insertion; FinFET layouts; routing tracks; switch blocks; Tile-based FPGAs; wire segment length,Architecture; FinFET; Integrated circuit layout; System-on-chip; Area estimation; Area estimation model; Buffer insertion; Estimation models; FinFET layout; Routing track; Routings; Segment lengths; Switch blocks; Tile-based FPGA; Wire segment length; Wire segments; Field programmable gate arrays (FPGA)
Programmable Analog System Benchmarks Leading to Efficient Analog Computation Synthesis,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198749016&doi=10.1145%2f3625298&partnerID=40&md5=5a7abb20c73fa361b70a8774a26f0889,"This effort develops the first rich suite of analog and mixed-signal benchmark of various sizes and domains, intended for use with contemporary analog and mixed-signal designs and synthesis tools. Benchmarking enables analog-digital co-design exploration as well as extensive evaluation of analog synthesis tools and the generated analog/mixed-signal circuit or device. The goals of this effort are defining analog computation system benchmarks, developing the required concepts for higher-level analog and mixed-signal tools to utilize these benchmarks, and enabling future automated architectural design space exploration (DSE) to determine the best configurable architecture (e.g., a new FPAA) for a certain family of applications. The benchmarks comprise multiple levels of an acoustic, a vision, a communications, and an analog filter system that must be simultaneously satisfied for a complete system. © 2024 Copyright held by the owner/author(s).",Analog benchmarks; analog computing; analog system synthesis; mixed-signal HLS,Computer aided design; Petroleum reservoir evaluation; Analog and mixed signals; Analog benchmark; Analog computation; Analog computing; Analog system synthesis; Analog systems; Mixed signal; Mixed-signal HLS; Synthesis tool; System synthesis; Benchmarking
Tailor: Altering Skip Connections for Resource-Efficient Inference,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192904359&doi=10.1145%2f3624990&partnerID=40&md5=8736a66a8c851b9f372e0557f3e89bd6,"Deep neural networks use skip connections to improve training convergence. However, these skip connections are costly in hardware, requiring extra buffers and increasing on- and off-chip memory utilization and bandwidth requirements. In this article, we show that skip connections can be optimized for hardware when tackled with a hardware-software codesign approach. We argue that while a network's skip connections are needed for the network to learn, they can later be removed or shortened to provide a more hardware-efficient implementation with minimal to no accuracy loss. We introduce Tailor, a codesign tool whose hardware-aware training algorithm gradually removes or shortens a fully trained network's skip connections to lower the hardware cost. Tailor improves resource utilization by up to 34% for block random access memories (BRAMs), 13% for flip-flops (FFs), and 16% for look-up tables (LUTs) for on-chip, dataflow-style architectures. Tailor increases performance by 30% and reduces memory bandwidth by 45% for a two-dimensional processing element array architecture.  © 2024 Copyright held by the owner/author(s).",Hardware-software co-design; neural networks,Bandwidth; Deep neural networks; Flip flop circuits; Memory architecture; Network architecture; Table lookup; Bandwidth requirement; Co-design approach; Hardware/software codesign; Learn+; Memory bandwidths; Memory utilization; Neural-networks; Off-chip memory; On-chip-memory; Resource-efficient; Hardware-software codesign
Designing Deep Learning Models on FPGA with Multiple Heterogeneous Engines,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198731236&doi=10.1145%2f3615870&partnerID=40&md5=79913acf1e460f63c0cb13888385266a,"Deep learning models are becoming more complex and heterogeneous with new layer types to improve their accuracy. This brings a considerable challenge to the designers of accelerators of deep neural networks. There have been several architectures and design flows to map deep learning models on hardware, but they are limited to a particular model and/or layer types. Also, the architectures generated by these tools target, in general, high-performance devices, not appropriate for embedded computing. This article proposes a multi-engine architecture and a design flow to implement deep learning models on FPGA. The hardware design uses high-level synthesis to allow design space exploration. The architecture is scalable and therefore applicable to any density FPGAs. The architecture and design flow were applied to the development of a hardware/software system for image classification with ResNet50, object detection with YOLOv3-Tiny, and image segmentation with DeepLabV3+. The system was tested in a low-density Zynq UltraScale+ ZU3EG FPGA to show its scalability. The results show that the proposed multi-engine architecture generates efficient accelerators. An accelerator of ResNet50 with a 4-bit quantization achieves 67 FPS, and the object detector with YOLOv3-Tiny with a throughput of 36 FPS and the image segmentation application achieves 1.4 FPS.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Deep learning models; DNN to FPGA mapping; FPGA; hardware acceleration,Deep neural networks; Engines; High level synthesis; Image segmentation; Integrated circuit design; Learning systems; Network architecture; Object detection; Object recognition; Deep learning model; Design flows; Design use; DNN to FPGA mapping; Embedded computing; Hardware acceleration; Hardware design; Higher performance devices; Images segmentations; Learning models; Field programmable gate arrays (FPGA)
A Partitioned CAM Architecture with FPGA Acceleration for Binary Descriptor Matching,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198521417&doi=10.1145%2f3624749&partnerID=40&md5=3681f073dfe68db225f87c34d6c8a8ce,"An efficient architecture for image descriptor matching that uses a partitioned content-addressable memory (CAM)-based approach is proposed. CAM is frequently used in high-speed content-matching applications. However, due to its lack of functionality to support approximate matching, conventional CAM is not directly useful for image descriptor matching. Our modifications improve the CAM architecture to support approximate content matching for selecting image matches with local binary descriptors. Matches are based on Hamming distances computed for all possible pairs of binary descriptors extracted from two images. We demonstrate an FPGA-based implementation of our CAM-based descriptor-matching unit to illustrate the high matching speed of our design. The time complexity of our modified CAM method for binary descriptor matching is O(n). Our method performs binary descriptor matching at a rate of one descriptor per clock cycle at a frequency of 102 MHz. The resource utilization and timing metrics of several experiments are reported to demonstrate the efficacy and scalability of our design.  © 2024 Copyright held by the owner/author(s).",binary descriptor matching; CAM; Content-addressable memory; FPGA; image matching; partitioned CAM,Associative processing; Associative storage; Hamming distance; Image enhancement; Image matching; Integrated circuit design; Memory architecture; Binary descriptor matching; Content matching; Content-addressable memory; Descriptor matching; Descriptors; Efficient architecture; Image Descriptor; Partitioned content-addressable memory; Field programmable gate arrays (FPGA)
FDRA: A Framework for a Dynamically Reconfigurable Accelerator Supporting Multi-Level Parallelism,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198735838&doi=10.1145%2f3614224&partnerID=40&md5=48918b5b3673f69e28a7d74a8ce6c7c9,"Coarse-grained reconfigurable architectures (CGRAs) have emerged as promising accelerators due to their high flexibility and energy efficiency. However, existing open source works often lack integration of CGRAs with CPU systems and corresponding toolchains. Moreover, there is rare support for the accelerator instruction pipelining to overlap data communication, computation, and configuration across multiple tasks. In this article, we propose FDRA, an open source exploration framework for a heterogeneous system-on-chip (SoC) with a RISC-V processor and a dynamically reconfigurable accelerator (DRA) supporting loop, instruction, and task levels of parallelism. FDRA encompasses parameterized SoC modeling, Verilog generation, source-to-source application code transformation using frontend and DRA compilers, SoC simulation, and FPGA prototyping. FDRA incorporates the extraction of periodic accumulative operators and multi-dimensional linear load/store operators from nested loops. The DRA enables accessing the shared L2 cache with virtual addresses and supports direct memory access with arbitrary start addresses and data lengths. Integrated into the RISC-V Rocket SoC, our DRA achieves a remarkable 55× acceleration for loop kernels and improves energy efficiency by 29×. Compared to state-of-the-art RISC-V vector units, our DRA demonstrates a 2.9× speed improvement and 3.5× greater energy efficiency. In contrast to previous CGRA+RISC-V SoCs, our SoC achieves a minimum speedup of 5.2×.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",CGRA; dynamically reconfigurable accelerator; instruction-level parallelism,Acceleration; Cache memory; Closed loop control systems; Cosine transforms; Energy efficiency; Memory architecture; Open source software; Open systems; Programmable logic controllers; Reconfigurable architectures; Coarse grained reconfigurable architecture; CPU systems; Data configuration; Dynamically reconfigurable accelerator; High flexibility; Instruction level parallelism; Multi-level parallelism; Open-source; Reconfigurable; Systems-on-Chip; System-on-chip
A Hardware Accelerator for the Semi-Global Matching Stereo Algorithm: An Efficient Implementation for the Stratix v and Zynq UltraScale+ FPGA Technology,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196145654&doi=10.1145%2f3615869&partnerID=40&md5=529d91d9416342ffbfa93f44f88cea04,"The semi-global matching stereo algorithm is a top performing algorithm in stereo vision. The recursive nature of the computations involved in this algorithm introduces an inherent data dependency problem, hindering the progressive computations of disparities at pixel clock. In this work, a novel hardware implementation of the semi-global matching algorithm is presented. A hardware structure of parallel comparators is proposed for the fast computation of the minima among large cost arrays in one clock cycle. Also, a hardware-friendly algorithm is proposed for the computation of the minima among far-indexed disparity costs, shortening the length of computations in the datapath. As a result, the recursive path cost computation is accelerated considerably. The system is implemented in a Stratix V device and in a Zynq UltraScale+ device. A throughput of 55,1 million disparities per second is achieved with maximum disparity 128 pixels and frame resolution 1280 × 720. The proposed architecture is less elaborate and more resource efficient than other systems in the literature and its performance compares favorably to them. An implementation on an actual FPGA board is also presented and serves as a real-world verification of the proposed system.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesFPGAs; stereo algorithms; Stratix V; Zynq,Clocks; Field programmable gate arrays (FPGA); Stereo image processing; Stereo vision; Additional key word and phrasesfpgas; Data dependencies; Efficient implementation; FPGA technology; Hardware accelerators; Key words; Semi-global matching; Stereo algorithms; Stratix V; Zynq; Pixels
Automated Buffer Sizing of Dataflow Applications in a High-level Synthesis Workflow,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198543612&doi=10.1145%2f3626103&partnerID=40&md5=14dfbd45746ebbc06919393d4dc486b6,"High-Level Synthesis (HLS) tools are mature enough to provide efficient code generation for computation kernels on FPGA hardware. For more complex applications, multiple kernels may be connected by a dataflow graph. Although some tools, such as Xilinx Vitis HLS, support dataflow directives, they lack efficient analysis methods to compute the buffer sizes between kernels in a dataflow graph. This article proposes an original method to safely approximate such buffer sizes. The first contribution computes an initial overestimation of buffer sizes without knowing the memory access patterns of kernels. The second contribution iteratively refines those buffer sizes, thanks to cosimulation. Moreover, the article introduces an open source framework using these methods to facilitate dataflow programming on FPGA using HLS. The proposed methods and framework have been tested on seven dataflow applications and outperform Vitis HLS cosimulation in five benchmarks, either in terms of BRAM and LUT usage, or in terms of exploration time. In the two other benchmarks, our best method gets results similar to Vitis HLS. Last but not least, our method admits directed cycles in the application graphs.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",buffer sizing; dataflow; FPGA; high-level synthesis,Benchmarking; Data flow analysis; High level synthesis; Iterative methods; Open source software; Buffer sizes; Buffer sizing; Codegeneration; Computation kernel; Cosimulation; Dataflow; Dataflow graphs; High-level synthesis; Synthesis tool; Work-flows; Field programmable gate arrays (FPGA)
Reprogrammable Non-Linear Circuits Using ReRAM for NN Accelerators,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197149383&doi=10.1145%2f3617894&partnerID=40&md5=5822c42eb96e04104b92f1ef3b5f79e5,"As the massive usage of artificial intelligence techniques spreads in the economy, researchers are exploring new techniques to reduce the energy consumption of Neural Network (NN) applications, especially as the complexity of NNs continues to increase. Using analog Resistive RAM devices to compute matrix-vector multiplication in O(1) time complexity is a promising approach, but it is true that these implementations often fail to cover the diversity of non-linearities required for modern NN applications. In this work, we propose a novel approach where Resistive RAMs themselves can be reprogrammed to compute not only the required matrix multiplications but also the activation functions, Softmax, and pooling layers, reducing energy in complex NNs. This approach offers more versatility for researching novel NN layouts compared to custom logic. Results show that our device outperforms analog and digital field-programmable approaches by up to 8.5× in experiments on real-world human activity recognition and language modeling datasets with convolutional neural network, generative pre-trained Transformer, and long short-term memory models.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",in-memory processing; Neural networks; reconfigurable computing; Resistive RAM (ReRAM),Complex networks; Matrix algebra; Modeling languages; Reconfigurable architectures; Recurrent neural networks; RRAM; Timing circuits; Artificial intelligence techniques; Energy-consumption; In-memory processing; Neural network application; Neural-networks; Nonlinear circuit; Reconfigurable computing; Reconfigurable- computing; Reprogrammable; Resistive  ReRAM; Energy utilization
Strega: An HTTP Server for FPGAs,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198742207&doi=10.1145%2f3611312&partnerID=40&md5=cc3e30841f30d4c8a334df1d133dce99,"The computer architecture landscape is being reshaped by the new opportunities, challenges, and constraints brought by the cloud. On the one hand, high-level applications profit from specialised hardware to boost their performance and reduce deployment costs. On the other hand, cloud providers maximise the CPU time allocated to client applications by offloading infrastructure tasks to hardware accelerators. While it is well understood how to do this for, e.g., network function virtualisation and protocols such as TCP/IP, support for higher networking layers is still largely missing, limiting the potential of accelerators. In this article, we present Strega, an open source1 light-weight Hypertext Transfer Protocol (HTTP) server that enables crucial functionality such as FPGA-accelerated functions being called through a RESTful protocol (FPGA-as-a-Function). Our experimental analysis shows that a single Strega node sustains a throughput of 1.7 M HTTP requests per second with an end-to-end latency as low as 16, μs, outperforming nginx running on 32 vCPUs in both metrics, and can even be an alternative to the traditional OpenCL flow over the PCIe bus. Through this work, we pave the way for running microservices directly on FPGAs, bypassing CPU overhead and realising the full potential of FPGA acceleration in distributed cloud applications.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",disaggregated accelerator; distributed systems; FPGA; HTTP; Network on chip; RESTful API; Webserver,Computer architecture; Field programmable gate arrays (FPGA); Hypertext systems; Network architecture; Network layers; Network-on-chip; Open systems; Servers; Cloud providers; Deployment costs; Disaggregated accelerator; Distributed systems; High level applications; Networks on chips; Performance; RESTful API; Specialized hardware; Web servers; HTTP
Montgomery Multiplication Scalable Systolic Designs Optimized for DSP48E2,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198707122&doi=10.1145%2f3624571&partnerID=40&md5=d39435b3e1338c1d3270a11b6c28359d,"This article describes an extensive study of the use of DSP48E2 Slices in Ultrascale FPGAs to design hardware versions of the Montgomery Multiplication algorithm for the hardware acceleration of modular multiplications. Our fully scalable systolic architectures result in parallelized, DSP48E2-optimized scheduling of operations analogous to the FIOS block variant of the Montgomery Multiplication. We explore the impacts of different pipelining strategies within DSP blocks, scheduling of operations, processing element configurations, global design structures and their tradeoffs in terms of performance and resource costs. We discuss the application of our methodology to multiple types of DSP primitives. We provide ready-to-use fast, efficient, and fully parametrizable designs, which can adapt to a wide range of requirements and applications. Implementations are scalable to any operand width. Our most efficient designs can perform 128, 256, 512, 1024, 2048, and 4096 bits Montgomery modular multiplications in 0.0992 μs, 0.2032 μs, 0.3952 μs, 0.7792μs, 1.550 μs, and 3.099 μs using 4, 6, 11, 21, 41, and 82 DSP blocks, respectively.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",DSP; FPGA; hardware acceleration; Montgomery Multiplication; systolic architecture,Digital signal processing; Integrated circuit design; Block scheduling; DSP; Fully scalable; Hardware acceleration; Modular Multiplication; Montgomery multiplication; Montgomery's multiplication algorithm; Optimized scheduling; Processing elements; Systolic architecture; Field programmable gate arrays (FPGA)
A Hardware Design Framework for Computer Vision Models Based on Reconfigurable Devices,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198521843&doi=10.1145%2f3635157&partnerID=40&md5=6a760aae1c177dfb9041e6874ab18db6,"In computer vision, the joint development of the algorithm and computing dimensions cannot be separated. Models and algorithms are constantly evolving, while hardware designs must adapt to new or updated algorithms. Reconfigurable devices are recognized as important platforms for computer vision applications because of their reconfigurability. There are two typical design approaches: customized and overlay design. However, existing work is unable to achieve both efficient performance and scalability to adapt to a wide range of models. To address both considerations, we propose a design framework based on reconfigurable devices to provide unified support for computer vision models. It provides software-programmable modules while leaving unit design space for problem-specific algorithms. Based on the proposed framework, we design a model mapping method and a hardware architecture with two processor arrays to enable dynamic and static reconfiguration, thereby relieving redesign pressure. In addition, resource consumption and efficiency can be balanced by adjusting the hyperparameter. In experiments on CNN, vision Transformer, and vision MLP models, our work’s throughput is improved by 18.8x–33.6x and 1.4x–2.0x compared to CPU and GPU. Compared to others on the same platform, accelerators based on our framework can better balance resource consumption and efficiency. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",FPGA; hardware acceleration; hardware friendly,Computer vision; Efficiency; Integrated circuit design; Reconfigurable hardware; Design frameworks; Hardware acceleration; Hardware design; Hardware friendly; Joint development; Model and algorithms; Reconfigurable devices; Resource efficiencies; Resources consumption; Vision model-based; Field programmable gate arrays (FPGA)
High Throughput FPGA-Based Object Detection via Algorithm-Hardware Co-Design,2024,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189909182&doi=10.1145%2f3634919&partnerID=40&md5=6d22c1dc58f9d3a2082ba896f8a47ab0,"Object detection and classification is a key task in many computer vision applications such as smart surveillance and autonomous vehicles. Recent advances in deep learning have significantly improved the quality of results achieved by these systems, making them more accurate and reliable in complex environments. Modern object detection systems make use of lightweight convolutional neural networks (CNNs) for feature extraction, coupled with single-shot multi-box detectors (SSDs) that generate bounding boxes around the identified objects along with their classification confidence scores. Subsequently, a non-maximum suppression (NMS) module removes any redundant detection boxes from the final output. Typical NMS algorithms must wait for all box predictions to be generated by the SSD-based feature extractor before processing them. This sequential dependency between box predictions and NMS results in a significant latency overhead and degrades the overall system throughput, even if a high-performance CNN accelerator is used for the SSD feature extraction component. In this paper, we present a novel pipelined NMS algorithm that eliminates this sequential dependency and associated NMS latency overhead. We then use our novel NMS algorithm to implement an end-to-end fully pipelined FPGA system for low-latency SSD-MobileNet-V1 object detection. Our system, implemented on an Intel Stratix 10 FPGA, runs at 400 MHz and achieves a throughput of 2,167 frames per second with an end-to-end batch-1 latency of 2.13 ms. Our system achieves 5.3× higher throughput and 5× lower latency compared to the best prior FPGA-based solution with comparable accuracy. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",algorithm-hardware co-design; FPGA accelerator; neural networks; object detection,Computer hardware; Convolutional neural networks; Deep learning; Extraction; Feature extraction; Hardware-software codesign; Integrated circuit design; Object detection; Object recognition; Pipeline processing systems; Pipelines; Algorithm-hardware co-design; Co-designs; Convolutional neural network; FPGA accelerator; High-throughput; Neural-networks; Non-maximum suppression; Objects detection; Single-shot; Suppression algorithm; Field programmable gate arrays (FPGA)
Introduction to the Special Section on FPGA 2022,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181450785&doi=10.1145%2f3618114&partnerID=40&md5=054134d2ab615e432d60b0800838130a,[No abstract available],,
TAPA: A Scalable Task-parallel Dataflow Programming Framework for Modern FPGAs with Co-optimization of HLS and Physical Design,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181479404&doi=10.1145%2f3609335&partnerID=40&md5=b28ad3d119bf271007788d0b9c378c30,"In this article, we propose TAPA, an end-to-end framework that compiles a C++ task-parallel dataflow program into a high-frequency FPGA accelerator. Compared to existing solutions, TAPA has two major advantages. First, TAPA provides a set of convenient APIs that allows users to easily express flexible and complex inter-task communication structures. Second, TAPA adopts a coarse-grained floorplanning step during HLS compilation for accurate pipelining of potential critical paths. In addition, TAPA implements several optimization techniques specifically tailored for modern HBM-based FPGAs. In our experiments with a total of 43 designs, we improve the average frequency from 147 MHz to 297 MHz (a 102% improvement) with no loss of throughput and a negligible change in resource utilization. Notably, in 16 experiments, we make the originally unroutable designs achieve 274 MHz, on average. The framework is available at https://github.com/UCLA-VAST/tapa and the core floorplan module is available at https://github.com/UCLA-VAST/AutoBridge  © 2023 Copyright held by the owner/author(s).",floorplanning; frequency optimization; hardware acceleration; HBM optimization; high-level synthesis; Multi-die FPGA,C++ (programming language); Computer software; High level synthesis; Integrated circuit design; Dataflow programming; Floor-planning; Frequency optimization; Hardware acceleration; HBM optimization; High-level synthesis; Multi-die FPGA; Optimisations; Parallel dataflow; Task parallel; Field programmable gate arrays (FPGA)
CHIP-KNNv2: A Configurable and High-Performance K-Nearest Neighbors Accelerator on HBM-based FPGAs,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180000828&doi=10.1145%2f3616873&partnerID=40&md5=7572d4c141058116c9f6962f296b0514,"The k-nearest neighbors (KNN) algorithm is an essential algorithm in many applications, such as similarity search, image classification, and database query. With the rapid growth in the dataset size and the feature dimension of each data point, processing KNN becomes more compute and memory hungry. Most prior studies focus on accelerating the computation of KNN using the abundant parallel resource on FPGAs. However, they often overlook the memory access optimizations on FPGA platforms and only achieve a marginal speedup over a multi-thread CPU implementation for large datasets. In this article, we design and implement CHIP-KNN: an HLS-based, configurable, and high-performance KNN accelerator. CHIP-KNN optimizes the off-chip memory access on modern HBM-based FPGAs such as the AMD/Xilinx Alveo U280 FPGA board. CHIP-KNN is configurable for all essential parameters used in the algorithm, including the size of the search dataset, the feature dimension and data type representation of each data point, the distance metric, and the number of nearest neighbors - K. In terms of design architecture, we explore and discuss the tradeoffs between two design versions: CHIP-KNNv1 (Ping-Pong buffer based) and CHIP-KNNv2 (streaming-based). Moreover, we investigate the routing congestion issue in our accelerator design, implement hierarchical structures to shorten critical paths, and integrate an open-source floorplanning optimization tool called TAPA/AutoBridge to eliminate the place-and-route issues. To explore the design space and balance the computation and memory access performance, we also build an analytical performance model. Given a user configuration of the KNN parameters, our tool can automatically generate TAPA HLS C code for the optimal accelerator design and the corresponding host code, on the HBM-based FPGA platform. Our experimental results on the Alveo U280 show that, compared to a 48-thread CPU implementation, CHIP-KNNv2 achieves a geomean performance speedup of 15×, with a maximum speedup of 45×. Additionally, we show that CHIP-KNNv2 achieves up to 2.1× performance speedup over CHIP-KNNv1 while increasing configurability. Compared with the state-of-the-art Facebook AI Similarity Search (FAISS) [23] GPU implementation running on a Nvidia Tesla V100 GPU, CHIP-KNNv2 achieves an average latency reduction of 30.6× while requiring 34.3% of GPU power consumption.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",automation framework; HBM-based FPGA; high-level synthesis; K-Nearest neighbors,C (programming language); Classification (of information); Data handling; High level synthesis; Integrated circuit design; Large dataset; Learning algorithms; Memory architecture; Motion compensation; Nearest neighbor search; Optimal systems; Query processing; Automation framework; Datapoints; Feature dimensions; HBM-based FPGA; High-level synthesis; K-near neighbor; Memory access; Nearest-neighbour; Performance; Similarity search; Field programmable gate arrays (FPGA)
Introduction to the Special Section on FCCM 2022,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181528254&doi=10.1145%2f3632092&partnerID=40&md5=8e23ee97776099eac5a412b62b797589,[No abstract available],,
High-efficiency TRNG Design Based on Multi-bit Dual-ring Oscillator,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181513031&doi=10.1145%2f3624991&partnerID=40&md5=c36de9d31a587ece6fe0f3dbb5f86a75,"Unpredictable true random numbers are required in security technology fields such as information encryption, key generation, mask generation for anti-side-channel analysis, algorithm initialization, and so on. At present, the true random number generator (TRNG) is not enough to provide fast random bits by low-speed bits generation. Therefore, it is necessary to design a faster TRNG. This work presents an ultra-compact TRNG with high throughput based on a novel extendable dual-ring oscillator (DRO). Owing to multiple bits output per cycle in DRO can be used to obtain the original random sequence, the proposed DRO achieves a maximum resource utilization to build a more efficient TRNG, compared with the conventional TRNG system based on ring oscillator (RO), which only has a single output and needs to build multiple groups of ring oscillators. TRNG based on the 2-bit DRO and its 8-bit derivative structure has been verified on Xilinx Artix-7 and Kintex-7 FPGA under the automatic layout and routing and has achieved a throughput of 550 Mbps and 1,100 Mbps, respectively. Moreover, in terms of throughput performance over operating frequency, hardware consumption, and entropy, the proposed scheme has obvious advantages. Finally, the generated sequences show good randomness in the test of NIST SP800-22 and Dieharder test suite and pass the entropy estimation test kit NIST SP800-90B and AIS-31. © 2023 Copyright held by the owner/author(s)",Dual-ring oscillator; Field-Programmable Gate Array; True random number generator,Cryptography; Entropy; Hardware security; Integrated circuit design; Number theory; Random number generation; Dual-ring; Dual-ring oscillator; Field programmables; Field-programmable gate array; Higher efficiency; Programmable gate array; Random number generators; Ring oscillator; True random number generator; True randoms; Field programmable gate arrays (FPGA)
Constraint-Aware Multi-Technique Approximate High-Level Synthesis for FPGAs,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181437799&doi=10.1145%2f3624481&partnerID=40&md5=3b63aa558cff65eb979b7c1f86364c5b,"Numerous approximate computing (AC) techniques have been developed to reduce the design costs in error-resilient application domains, such as signal and multimedia processing, data mining, machine learning, and computer vision, to trade-off computation accuracy with area and power savings or performance improvements. Selecting adequate techniques for each application and optimization target is complex but crucial for high-quality results. In this context, Approximate High-Level Synthesis (AHLS) tools have been proposed to alleviate the burden of hand-crafting approximate circuits by automating the exploitation of AC techniques. However, such tools are typically tied to a specific approximation technique or a difficult-to-extend set of techniques whose exploitation is not fully automated or steered by optimization targets. Therefore, available AHLS tools overlook the benefits of expanding the design space by mixing diverse approximation techniques toward meeting specific design objectives with minimum error. In this work, we propose an AHLS design methodology for FPGAs that automatically identifies efficient combinations of multiple approximation techniques for different applications and design constraints. Compared to single-technique approaches, decreases of up to 30% in mean squared error and absolute increases of up to 6.5% in percentage accuracy were obtained for a set of image, video, signal processing and machine learning benchmarks. © 2023 Copyright held by the owner/author(s)",approximate computing; design space exploration; field-programmable gate array; High-level synthesis,Data handling; Data mining; Economic and social effects; Errors; High level synthesis; Integrated circuit design; Learning algorithms; Machine learning; Mean square error; Signal processing; Approximate computing; Approximation techniques; Computing techniques; Design space exploration; Field programmables; Field-programmable gate array; High-level synthesis; Machine-learning; Optimisations; Programmable gate array; Field programmable gate arrays (FPGA)
FPGA-based Deep Learning Inference Accelerators: Where Are We Standing?,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181502284&doi=10.1145%2f3613963&partnerID=40&md5=3865dffab7fa8a68e80be86347309681,"Recently, artificial intelligence applications have become part of almost all emerging technologies around us. Neural networks, in particular, have shown significant advantages and have been widely adopted over other approaches in machine learning. In this context, high processing power is deemed a fundamental challenge and a persistent requirement. Recent solutions facing such a challenge deploy hardware platforms to provide high computing performance for neural networks and deep learning algorithms. This direction is also rapidly taking over the market. Here, FPGAs occupy the middle ground regarding flexibility, reconfigurability, and efficiency compared to general-purpose CPUs, GPUs, on one side, and manufactured ASICs on the other. FPGA-based accelerators exploit the features of FPGAs to increase the computing performance for specific algorithms and algorithm features. Filling a gap, we provide holistic benchmarking criteria and optimization techniques that work across several classes of deep learning implementations. This article summarizes the current state of deep learning hardware acceleration: More than 120 FPGA-based neural network accelerator designs are presented and evaluated based on a matrix of performance and acceleration criteria, and corresponding optimization techniques are presented and discussed. In addition, the evaluation criteria and optimization techniques are demonstrated by benchmarking ResNet-2 and LSTM-based accelerators.  © 2023 Copyright held by the owner/author(s).",accelerators; Deep learning; FPGAs; inference,Benchmarking; Graphics processing unit; Learning algorithms; Long short-term memory; Program processors; Computing performance; Deep learning; Emerging technologies; Hardware platform; High processing power; Inference; Machine-learning; Neural-networks; Optimization techniques; Reconfigurability; Field programmable gate arrays (FPGA)
A Reconfigurable Architecture for Real-time Event-based Multi-Object Tracking,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181673552&doi=10.1145%2f3593587&partnerID=40&md5=57a382858ba3e7d1d36a968b1fffd41b,"Although advances in event-based machine vision algorithms have demonstrated unparalleled capabilities in performing some of the most demanding tasks, their implementations under stringent real-time and power constraints in edge systems remain a major challenge. In this work, a reconfigurable hardware-software architecture called REMOT, which performs real-time event-based multi-object tracking on FPGAs, is presented. REMOT performs vision tasks by defining a set of actions over attention units (AUs). These actions allow AUs to track an object candidate autonomously by adjusting its region of attention and allow information gathered by each AU to be used for making algorithmic-level decisions. Taking advantage of this modular structure, algorithm-architecture codesign can be performed by implementing different parts of the algorithm in either hardware or software for different tradeoffs. Results show that REMOT can process 0.43-2.91 million events per second at 1.75-5.45 W. Compared with the software baseline, our implementation achieves up to 44 times higher throughput and 35.4 times higher power efficiency. Migrating the Merge operation to hardware further reduces the worst-case latency to be 95 times shorter than the software baseline. By varying the AU configuration and operation, a reduction of 0.59-0.77 mW per AU on the programmable logic has also been demonstrated.  © 2023 Copyright held by the owner/author(s).",attention unit; Dynamic Vision Sensors; event camera; event sensors; FPGA; hardware/software co-design; HOTA; multi-object tracking; REMOT,Hardware-software codesign; Integrated circuit design; Real time systems; Reconfigurable architectures; Reconfigurable hardware; Tracking (position); Attention unit; Dynamic vision sensors; Event camera; Event sensor; Event-based; Hardware/software codesign; HOTA; Multi-object tracking; Real- time; REMOT; Field programmable gate arrays (FPGA)
Resource Sharing in Dataflow Circuits,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181526378&doi=10.1145%2f3597614&partnerID=40&md5=ca1327289640183e47f3644adccef601,"To achieve resource-efficient hardware designs, high-level synthesis (HLS) tools share (i.e., time-multiplex) functional units among operations of the same type. This optimization is typically performed in conjunction with operation scheduling to ensure the best possible unit usage at each point in time. Dataflow circuits have emerged as an alternative HLS approach to efficiently handle irregular and control-dominated code. However, these circuits do not have a predetermined schedule - in its absence, it is challenging to determine which operations can share a functional unit without a performance penalty. More critically, although sharing seems to imply only some trivial circuitry, time-multiplexing units in dataflow circuits may cause deadlock by blocking certain data transfers and preventing operations from executing. In this paper, we present a technique to automatically identify performance-acceptable resource sharing opportunities in dataflow circuits. More importantly, we describe a sharing mechanism which achieves functionally correct and deadlock-free dataflow designs. On a set of benchmarks obtained from C code, we show that our approach effectively implements resource sharing. It results in significant area savings at a minor performance penalty compared to dataflow circuits which do not support this feature (i.e., it achieves a 64%, 2%, and 18% average reduction in DSPs, LUTs, and FFs, respectively, with an average increase in total execution time of only 2%) and matches the sharing capabilities of a state-of-the-art HLS tool.  © 2023 Copyright held by the owner/author(s).",Dataflow circuits; high-level synthesis; resource sharing,C (programming language); Data transfer; Time division multiplexing; Timing circuits; Dataflow; Dataflow circuit; Functional units; Hardware design; High-level synthesis; Optimisations; Performance penalties; Resource-efficient; Resources sharing; Synthesis tool; High level synthesis
An FPGA Accelerator for Genome Variant Calling,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178160821&doi=10.1145%2f3595297&partnerID=40&md5=3bb2fcba0463e0337f618ab6dc7af451,"In genome analysis, it is often important to identify variants from a reference genome. However, identifying variants that occur with low frequency can be challenging, as it is computationally intensive to do so accurately. LoFreq is a widely used program that is adept at identifying low-frequency variants. This article presents a design framework for an FPGA-based accelerator for LoFreq. In particular, this accelerator is targeted at virus analysis, which is particularly challenging, compared to human genome analysis, as the characteristics of the data to be analyzed are fundamentally different. Across the design space, this accelerator can achieve up to 120× speedups on the core computation of LoFreq and speedups of up to 51.7× across the entire program.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",FPGA; HLS; Variant Calling,Genes; Integrated circuit design; Viruses; Core computation; Design frameworks; Design spaces; Genome analysis; HLS; Human genomes; Lower frequencies; Variant calling; Field programmable gate arrays (FPGA)
Parallelising Control Flow in Dynamic-scheduling High-level Synthesis,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181529015&doi=10.1145%2f3599973&partnerID=40&md5=cd3656b34ff4dab7c4e820661c376a24,"Recently, there is a trend to use high-level synthesis (HLS) tools to generate dynamically scheduled hardware. The generated hardware is made up of components connected using handshake signals. These handshake signals schedule the components at runtime when inputs become available. Such approaches promise superior performance on ""irregular""source programs, such as those whose control flow depends on input data. This is at the cost of additional area. Current dynamic scheduling techniques are well able to exploit parallelism among instructions within each basic block (BB) of the source program, but parallelism between BBs is under-explored, due to the complexity in runtime control flows and memory dependencies. Existing tools allow some of the operations of different BBs to overlap, but to simplify the analysis required at compile time they require the BBs to start in strict program order, thus limiting the achievable parallelism and overall performance.We formulate a general dependency model suitable for comparing the ability of different dynamic scheduling approaches to extract maximal parallelism at runtime. Using this model, we explore a variety of mechanisms for runtime scheduling, incorporating and generalising existing approaches. In particular, we precisely identify the restrictions in existing scheduling implementation and define possible optimisation solutions. We identify two particularly promising examples where the compile-time overhead is small and the area overhead is minimal and yet we are able to significantly speed up execution time: (1) parallelising consecutive independent loops; and (2) parallelising independent inner-loop instances in a nested loop as individual threads. Using benchmark sets from related works, we compare our proposed toolflow against a state-of-the-art dynamic-scheduling HLS tool called Dynamatic. Our results show that, on average, our toolflow yields a 4× speedup from (1) and a 2.9× speedup from (2), with a negligible area overhead. This increases to a 14.3× average speedup when combining (1) and (2).  © 2023 Copyright held by the owner/author(s).",dynamic scheduling; FPGA; high-level synthesis; static analysis,High level synthesis; Static analysis; Area overhead; Compile time; Control-flow; Dynamic scheduling; High-level synthesis; Input datas; Performance; Runtimes; Synthesis tool; Tool flow; Field programmable gate arrays (FPGA)
RapidStream 2.0: Automated Parallel Implementation of Latency-Insensitive FPGA Designs Through Partial Reconfiguration,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181398964&doi=10.1145%2f3593025&partnerID=40&md5=ce151d5d6cb1276177055cc6ffce0c2e,"Field-programmable gate arrays (FPGAs) require a much longer compilation cycle than conventional computing platforms such as CPUs. In this article, we shorten the overall compilation time by co-optimizing the HLS compilation (C-to-RTL) and the back-end physical implementation (RTL-to-bitstream). We propose a split compilation approach based on the pipelining flexibility at the HLS level, which allows us to partition designs for parallel placement and routing. We outline a number of technical challenges and address them by breaking the conventional boundaries between different stages of the traditional FPGA tool flow and reorganizing them to achieve a fast end-to-end compilation.Our research produces RapidStream, a parallelized and physical-integrated compilation framework that takes in a latency-insensitive program in C/C++ and generates a fully placed and routed implementation. We present two approaches. The first approach (RapidStream 1.0) resolves inter-partition routing conflicts at the end when separate partitions are stitched together. When tested on the Xilinx U250 FPGA with a set of realistic HLS designs, RapidStream achieves a 5 to 7× reduction in compile time and up to 1.3× increase in frequency when compared with a commercial off-the-shelf toolchain. In addition, we provide preliminary results using a customized open-source router to reduce the compile time up to an order of magnitude in cases with lower performance requirements. The second approach (RapidStream 2.0) prevents routing conflicts using virtual pins. Testing on Xilinx U280 FPGA, we observed 5 to 7× compile time reduction and 1.3× frequency increase.  © 2023 Copyright held by the owner/author(s).",floorplanning; frequency optimization; hardware acceleration; HBM optimization; high-level synthesis; Multi-die FPGA,C++ (programming language); Field programmable gate arrays (FPGA); Integrated circuit design; Program processors; Reconfigurable hardware; Compile time; Field programmables; Floor-planning; Frequency optimization; Hardware acceleration; HBM optimization; High-level synthesis; Multi-die field-programmable gate array; Optimisations; Programmable gate array; High level synthesis
Topgun: An ECC Accelerator for Private Set Intersection,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168903117&doi=10.1145%2f3603114&partnerID=40&md5=1401538c17740dfa3325adad8f66dee4,"Elliptic Curve Cryptography (ECC), one of the most widely used asymmetric cryptographic algorithms, has been deployed in Transport Layer Security (TLS) protocol, blockchain, secure multiparty computation, and so on. As one of the most secure ECC curves, Curve25519 is employed by some secure protocols, such as TLS 1.3 and Diffie-Hellman Private Set Intersection (DH-PSI) protocol. High-performance implementation of ECC is required, especially for the DH-PSI protocol used in privacy-preserving platform.Point multiplication, the chief cryptographic primitive in ECC, is computationally expensive. To improve the performance of DH-PSI protocol, we propose Topgun, a novel and high-performance hardware architecture for point multiplication over Curve25519. The proposed architecture features a pipelined Finite-field Arithmetic Unit and a simple and highly efficient instruction set architecture. Compared to the best existing work on Xilinx Zynq 7000 series FPGA, our implementation with one Processing Element can achieve 3.14× speedup on the same device. To the best of our knowledge, our implementation appears to be the fastest among the state-of-the-art works. We also have implemented our architecture consisting of 4 Compute Groups, each with 16 PEs, on an Intel Agilex AGF027 FPGA. The measured performance of 4.48 Mops/s is achieved at the cost of 86 Watts power, which is the record-setting performance for point multiplication over Curve25519 on FPGAs.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",curve25519; elliptic curve cryptography; FPGA; hardware acceleration; private set intersection,Acceleration; Computer architecture; Geometry; Network architecture; Privacy-preserving techniques; Public key cryptography; Curve cryptography; Curve25519; Diffie Hellman; Elliptic curve; Elliptic curve cryptography; Hardware acceleration; Intersection protocols; Point multiplication; Private set intersection; Set intersection; Field programmable gate arrays (FPGA)
Logic Shrinkage: Learned Connectivity Sparsification for LUT-Based Neural Networks,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181450686&doi=10.1145%2f3583075&partnerID=40&md5=08504b7d17ee32618a3ca0c816ac0292,"Field-programmable gate array (FPGA)-specific deep neural network (DNN) architectures using native lookup tables (LUTs) as independently trainable inference operators have been shown to achieve favorable area-accuracy and energy-accuracy trade-offs. The first work in this area, LUTNet, exhibited state-of-the-art performance for standard DNN benchmarks. In this article, we propose the learned optimization of such LUT-based topologies, resulting in higher-efficiency designs than via the direct use of off-the-shelf, hand-designed networks. Existing implementations of this class of architecture require the manual specification of the number of inputs per LUT, K. Choosing appropriate K a priori is challenging. Doing so at even high granularity, for example, per layer, is a time-consuming and error-prone process that leaves FPGAs' spatial flexibility underexploited. Furthermore, prior works see LUT inputs connected randomly, which does not guarantee a good choice of network topology. To address these issues, we propose logic shrinkage, a fine-grained netlist pruning methodology enabling K to be automatically learned for every LUT in a neural network targeted for FPGA inference. By removing LUT inputs determined to be of low importance, our method increases the efficiency of the resultant accelerators. Our GPU-friendly solution to LUT input removal is capable of processing large topologies during their training with negligible slowdown. With logic shrinkage, we improve the area and energy efficiency of the best-performing LUTNet implementation of the CNV network classifying CIFAR-10 by 1.54× and 1.31×, respectively, while matching its accuracy. This implementation also reaches 2.71× the area efficiency of an equally accurate, heavily pruned binary neural network (BNN). On ImageNet, with the Bi-Real Net architecture, employment of logic shrinkage results in a post-synthesis area reduction of 2.67× vs. LUTNet, allowing for implementation that was previously impossible on today's largest FPGAs. We validate the benefits of logic shrinkage in the context of real application deployment by implementing a face mask detection DNN using a BNN, LUTNet, and logic-shrunk layers. Our results show that logic shrinkage results in area gains versus LUTNet (up to 1.20×) and equally pruned BNNs (up to 1.08×), along with accuracy improvements.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",binary neural networks; LUT-based neural networks; neural architecture search; pruning,Benchmarking; Computer circuits; Deep neural networks; Economic and social effects; Energy efficiency; Field programmable gate arrays (FPGA); Network architecture; Shrinkage; Topology; Binary neural networks; Field programmables; Lookup table-based neural network; Neural architecture search; Neural architectures; Neural network architecture; Neural-networks; Programmable gate array; Pruning; Sparsification; Table lookup
CoMeFa: Deploying Compute-in-Memory on FPGAs for Deep Learning Acceleration,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168807698&doi=10.1145%2f3603504&partnerID=40&md5=ea9762d093db404896c43bcda3a10364,"Block random access memories (BRAMs) are the storage houses of FPGAs, providing extensive on-chip memory bandwidth to the compute units implemented using logic blocks and digital signal processing slices. We propose modifying BRAMs to convert them to CoMeFa (Compute-in-Memory Blocks for FPGAs) random access memories (RAMs). These RAMs provide highly parallel compute-in-memory by combining computation and storage capabilities in one block. CoMeFa RAMs utilize the true dual-port nature of FPGA BRAMs and contain multiple configurable single-bit bit-serial processing elements. CoMeFa RAMs can be used to compute with any precision, which is extremely important for applications like deep learning (DL). Adding CoMeFa RAMs to FPGAs significantly increases their compute density while also reducing data movement. We explore and propose two architectures of these RAMs: CoMeFa-D (optimized for delay) and CoMeFa-A (optimized for area). Compared to existing proposals, CoMeFa RAMs do not require changing the underlying static RAM technology like simultaneously activating multiple wordlines on the same port, and are practical to implement. CoMeFa RAMs are especially suitable for parallel and compute-intensive applications like DL, but these versatile blocks find applications in diverse applications like signal processing and databases, among others. By augmenting an Intel Arria 10-like FPGA with CoMeFa-D (CoMeFa-A) RAMs at the cost of 3.8% (1.2%) area, and with algorithmic improvements and efficient mapping, we observe a geomean speedup of 2.55× (1.85×) across microbenchmarks from various applications and a geomean speedup of up to 2.5× across multiple deep neural networks. Replacing all or some BRAMs with CoMeFa RAMs in FPGAs can make them better accelerators of DL workloads.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesFPGA; Block RAM; Compute-In-Memory; Deep Learning; Machine Learning; Processing-In-Memory,Computation theory; Deep neural networks; Digital signal processing; Learning systems; Memory architecture; Random access storage; Additional key word and phrasesfpgum; Block random access memory; Compute-in-memory; Deep learning; Key words; Machine-learning; Memory blocks; On-chip-memory; Processing-in-memory; Random access memory; Field programmable gate arrays (FPGA)
BLOOP: Boolean Satisfiability-based Optimized Loop Pipelining,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168802656&doi=10.1145%2f3599972&partnerID=40&md5=dc0198568978a3563551cfe3e1cb3a8a,"Modulo scheduling is the premier technique for throughput maximization of loops in high-level synthesis by interleaving consecutive loop iterations. The number of clock cycles between data insertions is called the initiation interval (II). For throughput maximization, this value should be as low as possible; therefore, its minimization is the main optimization goal.Despite its long historical existence, modulo scheduling always remained a relevant research topic over the years with many exact and heuristic algorithms available in the literature.Nevertheless, we are able to leverage the scalability of modern Boolean Satisfiability (SAT) solvers to outperform state-of-the-art ILP-based algorithms for latency-optimal modulo scheduling for both integer and rational IIs. Our algorithm is able to compute valid modulo schedules for the whole CHStone and MachSuite benchmark suites, with 99% of the solutions being proven to be throughput optimal for a timeout of only 10 minutes per candidate II. For various time limits, not a single tested scheduler from the state of the art is able to compute more verified optimal solutions or even a single schedule with a higher throughput than our proposed approach. Using an HLS toolflow, we show that our algorithm can be effectively used to generate Pareto-optimal FPGA implementations regarding throughput and resource usage.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesModulo scheduling; Boolean satisfiability; loop pipelining,Formal logic; Heuristic algorithms; Pareto principle; Additional key word and phrasesmodulo scheduling; Boolean satisfiability; High-level synthesis; Interleavings; Key words; Loop iteration; Loop pipelining; Modulo scheduling; State of the art; Throughput maximization; High level synthesis
Increasing the Robustness of TERO-TRNGs Against Process Variation,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168795957&doi=10.1145%2f3597418&partnerID=40&md5=7dd9e342174b19e0abb172a4e65ea89d,"The transition effect ring oscillator is a popular design for building entropy sources because it is compact, built from digital elements only, and is very well suited for FPGAs. However, it is known to be quite sensitive to process variation. Although the latter is useful for building physical unclonable functions, it is interfering with the application as an entropy source.In this article, we investigate an approach to increase reliability. We show that adding a third stage eliminates much of the susceptibility to process variation and how a resulting gigahertz oscillation can be evaluated on an FPGA. The design is supported by physical and stochastic modeling. The physical model is validated using an experiment with dynamically reconfigurable look-up tables.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesHardware random number generators; entropy; FPGA; ring oscillators,Field programmable gate arrays (FPGA); Integrated circuit design; Number theory; Random number generation; Stochastic systems; Table lookup; Additional key word and phraseshardware random number generator; Digital elements; Entropy sources; Key words; Physical modelling; Process Variation; Random number generators; Reconfigurable; Ring oscillator; Stochastic-modeling; Entropy
ADAS: A High Computational Utilization Dynamic Reconfigurable Hardware Accelerator for Super Resolution,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153910693&doi=10.1145%2f3570927&partnerID=40&md5=ac0605a125f2c07af6802b5e76458bb7,"Super-resolution (SR) based on deep learning has obtained superior performance in image reconstruction. Recently, various algorithm efforts have been committed to improving image reconstruction quality and speed. However, the inference of SR contains huge amounts of computation and data access, leading to low hardware implementation efficiency. For instance, the up-sampling with the deconvolution process requires considerable computation resources. In addition, the sizes of output feature maps of several middle layers are extraordinarily large, which is challenging to optimize, causing serious data access issues. In this work, we present an all-on-chip hardware architecture based on the deconvolution scheme and feature map segmentation strategy, namely ADAS, where all the generated data by the middle layers are buffered on-chip to avoid large data movements between on- and off-chip. In ADAS, we develop a hardware-friendly and efficient deconvolution scheme to accelerate the computation. Also, the dynamic reconfigurable process element (PE) combined with efficient mapping is proposed to enhance PE utilization up to nearly 100% and support multiple scaling factors. Based on our experimental results, ADAS demonstrates real-time image SR and better image reconstruction quality with PSNR (37.15 dB) and SSIM (0.9587). Compared to baseline and validated with the FPGA platform, ADAS can support scaling factors of 2, 3, and 4, achieving 2.68 ×, 5.02 ×, and 8.28 × speedup.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesSuper resolution; computation efficiency; data access; deconvolution; reconfigurable architecture,Deep learning; Efficiency; Image enhancement; Memory architecture; Optical resolving power; Reconfigurable architectures; Reconfigurable hardware; Additional key word and phrasessupe resolution; Computation efficiency; Data access; Deconvolutions; Feature map; Images reconstruction; Key words; On chips; Reconstruction quality; Superresolution; Image reconstruction
An Empirical Approach to Enhance Performance for Scalable CORDIC-Based Deep Neural Networks,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168803248&doi=10.1145%2f3596220&partnerID=40&md5=13531336402eeae4fe0d1e57bcb82784,"Practical implementation of deep neural networks (DNNs) demands significant hardware resources, necessitating high computational power and memory bandwidth. While existing field-programmable gate array (FPGA)-based DNN accelerators are primarily optimized for fast single-task performance, cost, energy efficiency, and overall throughput are crucial considerations for their practical use in various applications. This article proposes a performance-centric pipeline Coordinate Rotation Digital Computer (CORDIC)-based MAC unit and implements a scalable CORDIC-based DNN architecture that is area- and power-efficient and has high throughput. The CORDIC-based neuron engine uses bit-rounding to maintain input-output precision and minimal hardware resource overhead. The results demonstrate the versatility of the proposed pipelined MAC, which operates at 460 MHz and allows for higher network throughput. A software-based implementation platform evaluates the proposed MAC operation's accuracy for more extensive neural networks and complex datasets. The DNN accelerator with parameterized and modular layer-multiplexed architecture is designed. Empirical evaluation through Pareto analysis is used to improve the efficiency of DNN implementations by fixing the arithmetic precision and optimal pipeline stages. The proposed architecture utilizes layer-multiplexing, a technique that effectively reuses a single DNN layer to enhance efficiency while maintaining modularity and adaptability for integrating various network configurations. The proposed CORDIC MAC-based DNN architecture is scalable for any bit-precision network size, and the DNN accelerator is prototyped using the Xilinx Virtex-7 VC707 FPGA board, operating at 66 MHz. The proposed design does not use any Xilinx macros, making it easily adaptable for ASIC implementation. Compared with state-of-the-art designs, the proposed design reduces resource use by 45% and power consumption by 4× without sacrificing performance. The accelerator is validated using the MNIST dataset, achieving 95.06% accuracy, only 0.35% less than other cutting-edge implementations.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesASIC; CORDIC; DNN accelerators; enhance throughput; modular architecture; Pareto analysis; pipeline MAC,Computer hardware; Digital computers; Energy efficiency; Field programmable gate arrays (FPGA); Integrated circuit design; Network architecture; Pipelines; Additional key word and phrasesasic; Co-ordinate rotation digital computers; Deep neural network accelerator; Enhance throughput; Hardware resources; Key words; Modular architectures; Pareto analysis; Performance; Pipeline MAC; Deep neural networks
Design Space Exploration of Galois and Fibonacci Configuration Based on Espresso Stream Cipher,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170577386&doi=10.1145%2f3567428&partnerID=40&md5=218351d705c26ff49d006e81074223ee,"Fibonacci and Galois are two different kinds of configurations in stream ciphers. Although many transformations between two configurations have been proposed, there is no sufficient analysis of their FPGA performance. Espresso stream cipher provides an ideal sample to explore such a problem. The 128-bit secret key Espresso is designed in Galois configuration, and there is a Fibonacci-configured Espresso variant proved with the equivalent security level. To fully leverage the efficiency of two configurations, we explore the hardware optimization approaches toward area and throughput, respectively. In short, the FPGA-implemented Fibonacci cipher is more suitable for extremely resource-constrained or high-throughput applications, while the Galois cipher compromises both area and speed. To the best of our knowledge, this is the first work to systematically compare the FPGA performance of cipher configurations under relatively fair cryptographic security. We hope this work can serve as a reference for the cryptography hardware architecture research community. © 2023 Association for Computing Machinery.",Espresso; Fibonacci NFSR; FPGA optimization; Galois NFSR; Lightweight cryptography; stream cipher,Computer hardware description languages; Cryptography; Hardware security; Design space exploration; Espresso; Fibonacci NFSR; FPGA optimization; Galoi NFSR; Ideal sample; Light-weight cryptography; Optimisations; Performance; Stream Ciphers; Field programmable gate arrays (FPGA)
FPGA Implementation of Compact Hardware Accelerators for Ring-Binary-LWE-based Post-quantum Cryptography,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168796590&doi=10.1145%2f3569457&partnerID=40&md5=d769b37531f1b6ccb143b0367d06c296,"Post-quantum cryptography (PQC) has recently drawn substantial attention from various communities owing to the proven vulnerability of existing public-key cryptosystems against the attacks launched from well-established quantum computers. The Ring-Binary-Learning-with-Errors (RBLWE), a variant of Ring-LWE, has been proposed to build PQC for lightweight applications. As more Field-Programmable Gate Array (FPGA) devices are being deployed in lightweight applications like Internet-of-Things (IoT) devices, it would be interesting if the RBLWE-based PQC can be implemented on the FPGA with ultra-low complexity and flexible processing. However, thus far, limited information is available for such implementations. In this article, we propose novel RBLWE-based PQC accelerators on the FPGA with ultra-low implementation complexity and flexible timing. We first present the process of deriving the key operation of the RBLWE-based scheme into the proposed algorithmic operation. The corresponding hardware accelerator is then efficiently mapped from the proposed algorithm with the help of algorithm-to-architecture implementation techniques and extended to obtain higher-throughput designs. The final complexity analysis and implementation results (on a variety of FPGAs) show that the proposed accelerators have significantly smaller area-time complexities than the state-of-the-art designs. Overall, the proposed accelerators feature low implementation complexity and flexible processing, making them desirable for emerging FPGA-based lightweight applications.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesCompact hardware accelerator; Field-Programmable Gate Array (FPGA); flexible processing throughput; low implementation complexity; post-quantum cryptography (PQC); Ring-Binary-Learning-with-Errors (RBLWE)-based scheme,Computer hardware; Internet of things; Logic gates; Public key cryptography; Quantum cryptography; Qubits; Additional key word and phrasescompact hardware accelerator; Field programmables; Field-programmable gate array; Flexible processing; Flexible processing throughput; Hardware accelerators; Implementation complexity; Key words; Learning with Errors; Low implementation complexity; Post quantum cryptography; Post-quantum cryptography; Programmable gate array; Ring-binary-learning-with-error -based scheme; Field programmable gate arrays (FPGA)
NeuroHSMD: Neuromorphic Hybrid Spiking Motion Detector,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168806348&doi=10.1145%2f3588318&partnerID=40&md5=e23a4091d7f5541ccef7e2b63ce158d7,"Vertebrate retinas are highly-efficient in processing trivial visual tasks such as detecting moving objects, which still represent complex challenges for modern computers. In vertebrates, the detection of object motion is performed by specialised retinal cells named Object Motion Sensitive Ganglion Cells (OMS-GC). OMS-GC process continuous visual signals and generate spike patterns that are post-processed by the Visual Cortex. Our previous Hybrid Sensitive Motion Detector (HSMD) algorithm was the first hybrid algorithm to enhance Background subtraction (BS) algorithms with a customised 3-layer Spiking Neural Network (SNN) that generates OMS-GC spiking-like responses. In this work, we present a Neuromorphic Hybrid Sensitive Motion Detector (NeuroHSMD) algorithm that accelerates our HSMD algorithm using Field-Programmable Gate Arrays (FPGAs). The NeuroHSMD was compared against the HSMD algorithm, using the same 2012 Change Detection (CDnet2012) and 2014 Change Detection (CDnet2014) benchmark datasets. When tested against the CDnet2012 and CDnet2014 datasets, NeuroHSMD performs object motion detection at 720 × 480 at 28.06 Frames Per Second (fps) and 720 × 480 at 28.71 fps, respectively, with no degradation of quality. Moreover, the NeuroHSMD proposed in this article was completely implemented in Open Computer Language (OpenCL) and therefore is easily replicated in other devices such as Graphical Processing Units (GPUs) and clusters of Central Processing Units (CPUs).  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesSNN; background subtraction; FPGA; HMSD; NeuroHSMD; object motion detection; OMS-GC; retinal cells,Aldehydes; Deep learning; Field programmable gate arrays (FPGA); Graphics processing unit; Motion analysis; Object detection; Ophthalmology; Pixels; Program processors; Vision; Additional key word and phrasessnn; Background subtraction; Field programmables; Field-programmable gate array; Ganglia cells; HMSD; Key words; Motion detection; Motion detectors; Neuromorphic; Neuromorphic hybrid sensitive motion detector; Object motion; Object motion detection; Object motion sensitive ganglion cell; Programmable gate array; Retinal cells; Neural networks
FSEAD: A Composable FPGA-based Streaming Ensemble Anomaly Detection Library,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168803115&doi=10.1145%2f3568992&partnerID=40&md5=666102585a48adddb32ab66d04c1bada,"Machine learning ensembles combine multiple base models to produce a more accurate output. They can be applied to a range of machine learning problems, including anomaly detection. In this article, we investigate how to maximize the composability and scalability of an FPGA-based streaming ensemble anomaly detector (fSEAD). To achieve this, we propose a flexible computing architecture consisting of multiple partially reconfigurable regions, pblocks, which each implement anomaly detectors. Our proof-of-concept design supports three state-of-the-art anomaly detection algorithms: Loda, RS-Hash, and xStream. Each algorithm is scalable, meaning multiple instances can be placed within a pblock to improve performance. Moreover, fSEAD is implemented using High-level synthesis (HLS), meaning further custom anomaly detectors can be supported. Pblocks are interconnected via an AXI-switch, enabling them to be composed in an arbitrary fashion before combining and merging results at runtime to create an ensemble that maximizes the use of FPGA resources and accuracy. Through utilizing reconfigurable Dynamic Function eXchange (DFX), the detector can be modified at runtime to adapt to changing environmental conditions. We compare fSEAD to an equivalent central processing unit (CPU) implementation using four standard datasets, with speedups ranging from 3× to 8×.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesFPGA; anomaly detection; composability; partial reconfiguration,Anomaly detection; High level synthesis; Machine learning; Program processors; Reconfigurable architectures; Reconfigurable hardware; Additional key word and phrasesfpgum; Anomaly detection; Anomaly detector; Composability; Composable; Key words; Machine-learning; Partial reconfiguration; Reconfigurable; Runtimes; Field programmable gate arrays (FPGA)
Artifact Evaluation for ACM TRETS Papers Submitted from the FPT Journal Track,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170673512&doi=10.1145%2f3596513&partnerID=40&md5=b5f1603ed4df41c50c834b6f65fb114e,[No abstract available],,
NAPOLY: A Non-deterministic Automata Processor OverLaY,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168794744&doi=10.1145%2f3593586&partnerID=40&md5=85057b836761875a7a03d721b45781cd,"Deterministic and Non-deterministic Finite Automata (DFA and NFA) comprise the core of many big data applications. Recent efforts to develop Domain-Specific Architectures (DSAs) for DFA/NFA have taken divergent approaches, but achieving consistent throughput for arbitrarily-large pattern sets, state activation rates, and pattern match rates remains a challenge. In this article, we present NAPOLY (Non-Deterministic Automata Processor OverLaY), an FPGA overlay and associated compiler. A common limitation of prior efforts is a limit on NFA size for achieving the advertised throughput. NAPOLY is optimized for fast re-programming to permit practical time-division multiplexing of the hardware and permit high asymptotic throughput for NFAs of unlimited size, unlimited state activation rate, and high pattern reporting rate. NAPOLY also allows for offline generation of configurations having tradeoffs between state capacity and transition capacity. In this article, we (1) evaluate NAPOLY using benchmarks packaged in the ANMLZoo benchmark suite, (2) evaluate the use of an SAT solver for allocating physical resources, and (3) compare NAPOLY's performance against existing solutions. NAPOLY performs most favorably on larger benchmarks, benchmarks with higher state activation frequency, and benchmarks with higher reporting frequency. NAPOLY outperforms the fastest of the CPU and GPU implementations in 10 out of 12 benchmarks.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesFPGA; automata processing; FPGA overlay; pattern matching,Benchmarking; Chemical activation; Finite automata; Pattern matching; Pipeline processing systems; Activation rates; Additional key word and phrasesfpgum; Automaton processing; Big data applications; Deterministics; FPGA overlay; Key words; Nondeterministic automata; Nondeterministic finite automaton; Pattern-matching; Field programmable gate arrays (FPGA)
Fixed-point FPGA Implementation of the FFT Accumulation Method for Real-time Cyclostationary Analysis,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168795506&doi=10.1145%2f3567429&partnerID=40&md5=fe4db55c7fbfc625c0f114f150000513,"The spectral correlation density (SCD) is an important tool in cyclostationary signal detection and classification. Even using efficient techniques based on the fast Fourier transform (FFT), real-time implementations are challenging because of the high computational complexity. A key dimension for computational optimization lies in minimizing the wordlength employed. In this article, we analyze the relationship between wordlength and signal-to-quantization noise in fixed-point implementations of the SCD function. A canonical SCD estimation algorithm, the FFT accumulation method (FAM) using fixed-point arithmetic, is studied. We derive closed-form expressions for SQNR and compare them at wordlengths ranging from 14 to 26 bits. The differences between the calculated SQNR and bit-exact simulations are less than 1 dB. Furthermore, an HLS-based FPGA design is implemented on a Xilinx Zynq UltraScale+ XCZU28DR-2FFVG1517E RFSoC. Using less than 25% of the logic fabric on the device, it consumes 7.7 W total on-chip power and has a power efficiency of 12.4 GOPS/W, which is an order of magnitude improvement over an Nvidia Tesla K40 graphics processing unit (GPU) implementation. In terms of throughput, it achieves 50 MS/sec, which is a speedup of 1.6 over a recent optimized FPGA implementation.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesSCD; FAM; FPGAs; HLS; quantization error,Computer graphics; Fast Fourier transforms; Fixed point arithmetic; Graphics processing unit; Program processors; Quantization (signal); Real time control; Additional key word and phrasesscd; FFT accumulation method; Fixed points; FPGA implementations; FPGAs implementation; HLS; Key words; Quantization errors; Spectral correlation density; Word length; Field programmable gate arrays (FPGA)
High-performance and Configurable SW/HW Co-design of Post-quantum Signature CRYSTALS-Dilithium,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165843883&doi=10.1145%2f3569456&partnerID=40&md5=14e3e8fc695f86bda8445d88dc1ab8ec,"CRYSTALS-Dilithium is a lattice-based post-quantum digital signature scheme that is resistant to attacks by quantum computers and has been selected to be standardized in the NIST post-quantum cryptography (PQC) standardization process. However, the speed performance and design flexibility of the Dilithium still need to be evaluated. This article presents a high-performance software/hardware co-design of CRYSTALS-Dilithium based on the NIST PQC round-3 parameters. High-speed pipelined hardware modules for NTT/INTT, point-wise multiplication/addition, and for SHAKE are included in the design to accelerate the time-consuming operations in Dilithium. All hardware modules are parameterized, thus allowing full support of runtime configuration to increase versatility. Moreover, the proposed software/hardware architecture and tight operating workflows reduce the data transmission overhead between the processor and other hardware modules. The hardware accelerator is implemented with a reconfigurable logic on FPGA and is integrated with the high-performance ARM Cortex-A9 processor in the Xilinx Zynq Architecture. We measure the performance of the software/hardware system for Dilithium in NIST security levels 2, 3, and 5. Compared to pure software implementations, we achieve 8.7-12.5 times speedup in Key generation, 6.3-7.3 times speedup in Sign, and 9.1-12.2 times speedup in Verify operations.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesPost-quantum cryptography; CRYSTALS-Dilithium; digital signature; lattice-based cryptography; software-hardware co-design,Authentication; Chromium compounds; Field programmable gate arrays (FPGA); Hardware-software codesign; Integrated circuit design; Quantum computers; Reconfigurable architectures; Reconfigurable hardware; Additional key word and phrasespost-quantum cryptography; CRYSTALS-dilithium; Dilithium; Hardware modules; Key words; Lattice-based cryptography; Performance; Post quantum; Post quantum cryptography; Software/hardware co designs; Computer software
A Survey of Processing Systems for Phylogenetics and Population Genetics,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168794430&doi=10.1145%2f3588033&partnerID=40&md5=f3fb4fc8df6987e7f6860145e19eb4b0,"The COVID-19 pandemic brought Bioinformatics into the spotlight, revealing that several existing methods, algorithms, and tools were not well prepared to handle large amounts of genomic data efficiently. This led to prohibitively long execution times and the need to reduce the extent of analyses to obtain results in a reasonable amount of time. In this survey, we review available high-performance computing and hardware-accelerated systems based on FPGA and GPU technology. Optimized and hardware-accelerated systems can conduct more thorough analyses considerably faster than pure software implementations, allowing to reach important conclusions in a timely manner to drive scientific discoveries. We discuss the reasons that are currently hindering high-performance solutions from being widely deployed in real-world biological analyses and describe a research direction that can pave the way to enable this.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesPhylogenetics; population genetics,Chromosomes; Digital storage; Additional key word and phrasesphylogenetic; Genomic data; Hardware-accelerated systems; High-performance hardware; Key words; Large amounts; Performance computing; Phylogenetics; Population genetics; Processing systems; Phylogenetic
Stream Aggregation with Compressed Sliding Windows,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168808056&doi=10.1145%2f3590774&partnerID=40&md5=c625bbbe3d45a512a7ece10c54dd493e,"High performance stream aggregation is critical for many emerging applications that analyze massive volumes of data. Incoming data needs to be stored in a sliding window during processing, in case the aggregation functions cannot be computed incrementally. Updating the window with new incoming values and reading it to feed the aggregation functions are the two primary steps in stream aggregation. Although window updates can be supported efficiently using multi-level queues, frequent window aggregations remain a performance bottleneck as they put tremendous pressure on the memory bandwidth and capacity. This article addresses this problem by enhancing StreamZip, a dataflow stream aggregation engine that is able to compress the sliding windows. StreamZip deals with a number of data and control dependency challenges to integrate a compressor in the stream aggregation pipeline and alleviate the memory pressure posed by frequent aggregations. In addition, StreamZip incorporates a caching mechanism for dealing with skewed-key distributions in the incoming data stream. In doing so, StreamZip offers higher throughput as well as larger effective window capacity to support larger problems. StreamZip supports diverse compression algorithms offering both lossless and lossy compression to integers as well as floating-point numbers. Compared to designs without compression, StreamZip lossless and lossy designs achieve up to 7.5× and 22× higher throughput, while improving the effective memory capacity by up to 5× and 23×, respectively.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesCompression; aggregation; dataflow; sliding windows; stream processing,Digital arithmetic; Additional key word and phrasescompression; Aggregation functions; Dataflow; Emerging applications; High-throughput; Key words; Memory capacity; Performance; Sliding Window; Stream processing; Decoding
ZyPR: End-to-end Build Tool and Runtime Manager for Partial Reconfiguration of FPGA SoCs at the Edge,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168798190&doi=10.1145%2f3585521&partnerID=40&md5=d4b505f19a5696a917cd79f1dbd4be69,"Partial reconfiguration (PR) is a key enabler to the design and development of adaptive systems on modern Field Programmable Gate Array (FPGA) Systems-on-Chip (SoCs), allowing hardware to be adapted dynamically at runtime. Vendor-supported PR infrastructure is performance-limited and blocking, drivers entail complex memory management, and software/hardware design requires bespoke knowledge of the underlying hardware. This article presents ZyPR: a complete end-to-end framework that provides high-performance reconfiguration of hardware from within a software abstraction in the Linux userspace, automating the process of building PR applications with support for the Xilinx Zynq and Zynq UltraScale+ architectures, aimed at enabling non-expert application designers to leverage PR for edge applications. We compare ZyPR against traditional vendor tooling for PR management as well as recent open source tools that support PR under Linux. The framework provides a high-performance runtime along with low overhead for its provided abstractions. We introduce improvements to our previous work, increasing the provisioning throughput for PR bitstreams on the Zynq Ultrascale+ by 2× and 5.4× compared to Xilinx's FPGA Manager.  © 2023 Copyright held by the owner/author(s).",adaptive systems; Additional Key Words and PhrasesField programmable gate arrays; partial reconfiguration,Abstracting; Adaptive systems; Application programs; Integrated circuit design; Linux; Logic gates; Open source software; Reconfigurable hardware; System-on-chip; Additional key word and phrasesfield programmable gate array; Array systems; End to end; Field programmables; Key words; Partial reconfiguration; Performance; Programmable gate array; Runtimes; Systems-on-Chip; Field programmable gate arrays (FPGA)
AutoScaleDSE: A Scalable Design Space Exploration Engine for High-Level Synthesis,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168807516&doi=10.1145%2f3572959&partnerID=40&md5=130d85e7c40e1bfc355ec24d65c62826,"High-Level Synthesis (HLS) has enabled users to rapidly develop designs targeted for FPGAs from the behavioral description of the design. However, to synthesize an optimal design capable of taking better advantage of the target FPGA, a considerable amount of effort is needed to transform the initial behavioral description into a form that can capture the desired level of parallelism. Thus, a design space exploration (DSE) engine capable of optimizing large complex designs is needed to achieve this goal. We present a new DSE engine capable of considering code transformation, compiler directives (pragmas), and the compatibility of these optimizations. To accomplish this, we initially express the structure of the input code as a graph to guide the exploration process. To appropriately transform the code, we take advantage of ScaleHLS based on the multi-level compiler infrastructure (MLIR). Finally, we identify problems that limit the scalability of existing DSEs, which we name the ""design space merging problem.""We address this issue by employing a Random Forest classifier that can successfully decrease the number of invalid design points without invoking the HLS compiler as a validation tool. We evaluated our DSE engine against the ScaleHLS DSE, outperforming it by a maximum of 59×. We additionally demonstrate the scalability of our design by applying our DSE to large-scale HLS designs, achieving a maximum speedup of 12× for the benchmarks in the MachSuite and Rodinia set.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesHigh-Level Synthesis; design space exploration; static analysis,Field programmable gate arrays (FPGA); High level synthesis; Integrated circuit design; Optimal systems; Program compilers; Scalability; Static analysis; Additional key word and phraseshigh-level synthesis; Behavioral descriptions; Code transformation; Compiler directives; Complex designs; Design space exploration; High-level synthesis; Key words; Optimal design; Scalable design; Engines
Introduction to the Special Issue on FPT 2021,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164338203&doi=10.1145%2f3603701&partnerID=40&md5=2c7455bb57473a18625a07b8db7c037f,[No abstract available],,
QiCells: A Modular RFSoC-based Approach to Interface Superconducting Quantum Bits,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164244630&doi=10.1145%2f3571820&partnerID=40&md5=09deae62c2d8c82ad0ab9d03ab203aa5,"Quantum computers will be a revolutionary extension of the heterogeneous computing world. They consist of many quantum bits (qubits) and require a careful design of the interface between the classical computer architecture and the quantum processor. For example, even single nanosecond variations of the interaction may have an influence on the quantum state. Designing a tailored interface electronics is therefore a major challenge, both in terms of signal integrity with respect to single channels, as well as the scaling of the signal count. We developed such an interface electronics, an RFSoC-based qubit control system called QiController. In this article, we present the modular FPGA firmware design of our system. It features so-called digital unit cells, or QiCells. Each cell contains all the logic necessary to interact with a single superconducting qubit, including a custom-built RISC-V-based sequencer. Synchronization and data exchange between the cells is facilitated using a special star-point structure. Versatile routing and frequency-division multiplexing of generated signals between QiCells and converters are also supported. High-level programmability is provided using a custom Python-based description language and an associated compiler. We furthermore provide the resource utilization of our design and demonstrate its correct operation using an actual superconducting five-qubit chip.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesData acquisition; FPGA; pulse generation; quantum bits; quantum computing; quantum-classical interface; RFSoC; RISC-V,Computation theory; Computer architecture; Firmware; High level languages; Integrated circuit design; Program compilers; Quantum computers; Additional key word and phrasesdata acquisition; Key words; Modulars; Pulse generation; Quantum bit; Quantum Computing; Quantum-classical; Quantum-classical interface; RFSoC; RISC-V; Field programmable gate arrays (FPGA)
Toward Software-like Debugging for FPGAs via Checkpointing and Transaction-based Co-Simulation,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159310342&doi=10.1145%2f3552521&partnerID=40&md5=ae2c5f9364bdbfb073ced024b1ba0285,"Checkpoint-based debugging flows have recently been developed that allow the user to move the design state back and forth between an FPGA and a simulator. They provide a softwarelike debugging experience by combining the speed of hardware execution and the full visibility of simulation. However, they assume the entire system state can be moved to a simulator, limiting them to self-contained systems. In this article, we present StateLink, a transaction-based co-simulation framework that allows part of the system (the task) to run in a simulator and still interact with other system components that reside in hardware. StateLink allows tasks to remain connected to and active in the overall hardware system after their state is moved to a simulator. This extends the functionality of checkpoint-based debugging frameworks to designs with external I/Os and significantly speeds up the simulation of tasks that are part of a large system. StateLink typically adds no timing overhead and a modest hardware area overhead. The total area overhead of using the proposed flow on a Memcached system is only 13%. This flow allows the user to benefit from both the hardware speedup of ĝ1/41M× and the StateLink speedup of up to 44× versus full system simulation.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesFPGA; checkpointing; co-simulation; debugging,Computer software; Program debugging; Additional key word and phrasesfpgum; Area overhead; Check pointing; Cosimulation; Debugging; Design state; Entire system; Key words; Self-contained systems; System state; Field programmable gate arrays (FPGA)
Algorithm-hardware Co-optimization for Energy-efficient Drone Detection on Resource-constrained FPGA,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164236262&doi=10.1145%2f3583074&partnerID=40&md5=3b7796b9ea1023e3fa5fc93c84d3fc89,"Convolutional neural network (CNN)-based object detection has achieved very high accuracy; e.g., single-shot multi-box detectors (SSDs) can efficiently detect and localize various objects in an input image. However, they require a high amount of computation and memory storage, which makes it difficult to perform efficient inference on resource-constrained hardware devices such as drones or unmanned aerial vehicles (UAVs). Drone/UAV detection is an important task for applications including surveillance, defense, and multi-drone self-localization and formation control. In this article, we designed and co-optimized an algorithm and hardware for energy-efficient drone detection on resource-constrained FPGA devices. We trained an SSD object detection algorithm with a custom drone dataset. For inference, we employed low-precision quantization and adapted the width of the SSD CNN model. To improve throughput, we use dual-data rate operations for DSPs to effectively double the throughput with limited DSP counts. For different SSD algorithm models, we analyze accuracy or mean average precision (mAP) and evaluate the corresponding FPGA hardware utilization, DRAM communication, and throughput optimization. We evaluated the FPGA hardware for a custom drone dataset, Pascal VOC, and COCO2017. Our proposed design achieves a high mAP of 88.42% on the multi-drone dataset, with a high energy efficiency of 79 GOPS/W and throughput of 158 GOPS using the Xilinx Zynq ZU3EG FPGA device on the Open Vision Computer version 3 (OVC3) platform. Our design achieves 1.1 to 8.7× higher energy efficiency than prior works that used the same Pascal VOC dataset, using the same FPGA device, but at a low-power consumption of 2.54 W. For the COCO dataset, our MobileNet-V1 implementation achieved an mAP of 16.8, and 4.9 FPS/W for energy-efficiency, which is ∼1.9× higher than prior FPGA works or other commercial hardware platforms.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesFPGA accelerator; algorithm-hardware co-design; neural networks; object detection,Aircraft detection; Antennas; Constrained optimization; Drones; Dynamic random access storage; Energy efficiency; Field programmable gate arrays (FPGA); Integrated circuit design; Neural networks; Object recognition; Signal detection; Additional key word and phrasesfpgum accelerator; Algorithm-hardware co-design; Co-designs; Convolutional neural network; Energy efficient; FPGA devices; Key words; Neural-networks; Objects detection; Single-shot; Object detection
Deterministic Approach for Range-enhanced Reconfigurable Packet Classification Engine,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164239563&doi=10.1145%2f3586577&partnerID=40&md5=acac1ae813c95913581d60429a823dec,"Reconfigurable hardware is a promising technology for implementing firewalls, routing mechanisms, and new protocols for evolving high-performance network systems. This work presents a novel deterministic approach for a Range-enhanced Reconfigurable Packet Classification Engine based on the number of rules on FPGAs. The proposed framework uses a RAM-established Ternary Match to represent the prefix and the range prefix and efficient rule-reordering for priority selection to get both best-match and multi-match in the same architecture. The recommended framework exhibits 3.2 Mbits of LUT-RAM-based ternary content addressable memory (TCAM) to hold a maximum of 31.3 K of 104-bit rules with 520 MPPS. LUT-RAM, along with BRAM, shows 4 Mbits of TCAM space to implement 38.5 K of 104-bit rules to sustain a throughput of 400 MPPS on Virtex-7 FPGA. The complete architecture offers scalability, better resource utilization (minimum of 50%), representation of inverse prefix with single entry, range expansion with a single rule, getting best- and multi-match, and determination of the required number of FPGA resources for a particular dataset.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesReconfigurable computing; application specific integrated circuit (ASIC); deterministic packet classifier; field programmable gate array (FPGA)-based TCAM; firewall; packet classification engine (PCE); priority selection; range prefix; rule-based inspection; scalable ternary content addressable memory (TCAM); ternary match circuit,Associative storage; Computer system firewalls; Field programmable gate arrays (FPGA); Logic gates; Memory architecture; Network architecture; Random access storage; Reconfigurable architectures; Reconfigurable hardware; Ternary content adressable memory; Additional key word and phrasesreconfigurable computing; Application specific integrated circuit; Application-specific integrated circuits; Deterministic packet classifier; Deterministics; Field programmable gate array -based ternary content addressable memory; Field programmables; Firewall; Key words; Packet classification; Packet classification engine; Priority selection; Programmable gate array; Range prefix; Rule based; Rule-based inspection; Scalable ternary content addressable memory; Ternary content addressable memory; Ternary match circuit; Engines
SASA: A Scalable and Automatic Stencil Acceleration Framework for Optimized Hybrid Spatial and Temporal Parallelism on HBM-based FPGAs,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164243460&doi=10.1145%2f3572547&partnerID=40&md5=333ca90609240f5fc8ac3b360fd90cff,"Stencil computation is one of the fundamental computing patterns in many application domains such as scientific computing and image processing. While there are promising studies that accelerate stencils on FPGAs, there lacks an automated acceleration framework to systematically explore both spatial and temporal parallelisms for iterative stencils that could be either computation-bound or memory-bound. In this article, we present SASA, a scalable and automatic stencil acceleration framework on modern HBM-based FPGAs. SASA takes the high-level stencil DSL and FPGA platform as inputs, automatically exploits the best spatial and temporal parallelism configuration based on our accurate analytical model, and generates the optimized FPGA design with the best parallelism configuration in TAPA high-level synthesis C++ as well as its corresponding host code. Compared to state-of-the-art automatic stencil acceleration framework SODA that only exploits temporal parallelism, SASA achieves an average speedup of 3.41× and up to 15.73× speedup on the HBM-based Xilinx Alveo U280 FPGA board for a wide range of stencil kernels.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesStencil acceleration; automation framework; HBM-based FPGA; high-level synthesis; hybrid parallelism,C++ (programming language); High level synthesis; Image processing; Additional key word and phrasesstencil acceleration; Applications domains; Automation framework; HBM-based FPGA; High-level synthesis; Hybrid parallelisms; Key words; Spatial parallelism; Stencil computations; Temporal parallelism; Field programmable gate arrays (FPGA)
Improving Energy Efficiency of CGRAs with Low-Overhead Fine-Grained Power Domains,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164246853&doi=10.1145%2f3558394&partnerID=40&md5=a6c5d098209eb7510deccb4accb57f31,"To effectively minimize static power for a wide range of applications, power domains for coarse-grained reconfigurable array (CGRA) architectures need to be more fine-grained than those found in a typical application-specific integrated circuit. However, the special isolation logic needed to ensure electrical protection between off and on domains makes fine-grained power domains area- and timing-inefficient. We propose a novel design of the CGRA routing fabric that reduces the area overhead of power domain boundary protection from around 9% to less than 1% without incurring any extra timing delay from the isolation cells. Conventional Unified Power Format based flow for power domain boundary protection does not support this design choice. Therefore, we create our own compiler-like passes that iteratively introduce the needed design changes, and formally verify the transformations using methods based on satisfiability modulo theories. These passes also let us optimize how we handle test and debug signals through the off tiles in the CGRA. Using our framework, we add power domains to a CGRA that we designed and taped out. The CGRA has 32 × 16 processing element and memory tiles and 4-MB secondary memory. We address the implementation challenges encountered due to the introduction of fine-grained power domains, including the addressing of the CGRA tiles, the power grid design, well substrate connections, and distribution of global signals. Our CGRA achieves up to 83% reduction in leakage power and 26% reduction in total power versus an identical CGRA without multiple power domains, for a range of image processing and machine learning applications.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesReconfigurable computing; coarse-grained reconfigurable arrays; hardware generators; power domains,Computation theory; Computing power; Electric equipment protection; Energy efficiency; Field programmable gate arrays (FPGA); Image processing; Reconfigurable architectures; Reconfigurable hardware; % reductions; Additional key word and phrasesreconfigurable computing; Boundary protection; Coarse-grained reconfigurable arrays; Domain boundary; Fine-grained power; Hardware generator; Key words; Low overhead; Powerdomains; Iterative methods
Adaptive Selection and Clustering of Partial Reconfiguration Modules for Modern FPGA Design Flow,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164246317&doi=10.1145%2f3567427&partnerID=40&md5=c31e9b0dc1167b1b9a7e483bed4942ab,"Dynamic Partially Reconfiguration (DPR) on FPGA has attracted significant research interest in recent years since it provides benefits such as reduced area and flexible functionality. However, due to the lack of supporting synthesis tools in the current DPR design flow, leveraging benefits from DPR requires specific design expertise with laborious manual design effort. Considering the complicated concurrency relations among various functions, it is challenging to select appropriate Partial Reconfiguration Modules (PR Modules) and cluster them into proper groups with a proper reconfiguration schedule so that the hardware modules can be swapped in and out correctly during the run time. Furthermore, the design of PR Modules also impacts reconfiguration latency and resource utilization greatly. In this paper, we propose a Maximum-Weight Independent Set model to formulate the PR Module selection and clustering problem so that the original manual exploration can be solved efficiently and automatically. We also propose a step-wise adjustment configuration prefetching strategy incorporated in our model to generate optimized reconfiguration schedules. Our proposed approach not only supports various design constraints but also can consider multiple objectives such as area and reconfiguration delay. Experimental results show that our approach can optimize resource utilization and reduce reconfiguration delay with good scalability. Especially, the implementation of the real design case shows that our approach can be embedded in Xilinx's DPR design flow successfully.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesDynamic partially reconfiguration; independent set-based model; partial reconfiguration module; prefetching,Integrated circuit design; Reconfigurable hardware; Adaptive clustering; Additional key word and phrasesdynamic partially reconfiguration; Design flows; Independent set; Independent set-based model; Key words; Partial reconfiguration; Partial reconfiguration module; Prefetching; Resources utilizations; Field programmable gate arrays (FPGA)
FPGA Acceleration of Probabilistic Sentential Decision Diagrams with High-level Synthesis,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152523530&doi=10.1145%2f3561514&partnerID=40&md5=e9ac4490c6576ce67997c7549158d526,"Probabilistic Sentential Decision Diagrams (PSDDs) provide efficient methods for modeling and reasoning with probability distributions in the presence of massive logical constraints. PSDDs can also be synthesized from graphical models such as Bayesian networks (BNs) therefore offering a new set of tools for performing inference on these models (in time linear in the PSDD size). Despite these favorable characteristics of PSDDs, we have found multiple challenges in PSDD's FPGA acceleration. Problems include limited parallelism, data dependency, and small pipeline iterations. In this article, we propose several optimization techniques to solve these issues with novel pipeline scheduling and parallelization schemes. We designed the PSDD kernel with a high-level synthesis (HLS) tool for ease of implementation and verified it on the Xilinx Alveo U250 board. Experimental results show that our methods improve the baseline FPGA HLS implementation performance by 2,200X and the multicore CPU implementation by 20X. The proposed design also outperforms state-of-the-art BN and Sum Product Network (SPN) accelerators that store the graph information in memory.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesPSDD; FPGA; HLS,Acceleration; Bayesian networks; High level synthesis; Pipelines; Probability distributions; Product design; Additional key word and phrasespsdd; Bayesia n networks; Decision diagram; GraphicaL model; High-level synthesis; Key words; Logical constraints; Probabilistics; Probability: distributions; Synthesised; Field programmable gate arrays (FPGA)
Hardware Optimizations of Fruit-80 Stream Cipher: Smaller than Grain,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164236162&doi=10.1145%2f3569455&partnerID=40&md5=e9eea6debf9340a1f780af9c9d1950a4,"Fruit-80, which emerged as an ultra-lightweight stream cipher with 80-bit secret key, is oriented toward resource-constrained devices in the Internet of Things. In this article, we propose area and speed optimization architectures of Fruit-80 on FPGAs. Our implementations include both serial and parallel structure and optimize area, power, speed, and throughput, respectively. The area optimization architecture aims to achieve the most suitable ratio of look-up-tables and flip-flops to fully utilize the reconfigurable unit. It also reuses NFSR and LFSR feedback functions to save resources for high throughput. The speed optimization architecture adopts a hybrid approach for parallelization and reduces the latency of long data paths by pre-generating primary feedback and inserting flip-flops. Besides, we recommend using the round key function to optimize serial or parallel implementations for Fruit-80 and using indexing and shifting methods for different throughput. In conclusion, our results show that the area optimization architecture occupies up to 35 slices on Xilinx Spartan-3 FPGA and 18 slices on Xilinx 7 series FPGA, smaller than that of Grain and other common stream ciphers. The optimal throughput/area ratio of the speed optimization architecture is 7.74 Mbps/slice, better than that of Grain v1, which is 5.98 Mbps/slice. The serial implementation of Fruit-80 with round key function occupies only 75 slices on Spartan-3 FPGA. To the best of our knowledge, the result sets a new record of the minimum area in lightweight cipher implementation on FPGA.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesHardware optimization; Fruit-80; lightweight cryptography; parallelism,Computer hardware description languages; Field programmable gate arrays (FPGA); Flip flop circuits; Reconfigurable architectures; Table lookup; Additional key word and phraseshardware optimization; Area optimization; Fruit-80; Key words; Light-weight cryptography; Optimisations; Optimization architecture; Parallelism; Speed optimization; Stream Ciphers; Fruits
Efficient Compilation and Mapping of Fixed Function Combinational Logic onto Digital Signal Processors Targeting Neural Network Inference and Utilizing High-level Synthesis,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164241985&doi=10.1145%2f3559543&partnerID=40&md5=826346031f4ad575b7147a69ca40491e,"Recent efforts for improving the performance of neural network (NN) accelerators that meet today's application requirements have given rise to a new trend of logic-based NN inference relying on fixed function combinational logic. Mapping such large Boolean functions with many input variables and product terms to digital signal processors (DSPs) on Field-programmable gate arrays (FPGAs) needs a novel framework considering the structure and reconfigurability of DSP blocks during this process. The proposed methodology in this article maps the fixed function combinational logic blocks to a set of Boolean functions where Boolean operations corresponding to each function are mapped to DSP devices rather than look-up tables on the FPGAs to take advantage of the high performance, low latency, and parallelism of DSP blocks. This article also presents an innovative design and optimization methodology for compilation and mapping of NNs, utilizing fixed function combinational logic to DSPs on FPGAs employing high-level synthesis flow. Our experimental evaluations across several datasets and selected NNs demonstrate the comparable performance of our framework in terms of the inference latency and output accuracy compared to prior art FPGA-based NN accelerators employing DSPs.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesDigital signal processors; Boolean function; FPGA devices; high-level synthesis,Boolean functions; Computer circuits; Digital signal processing; Digital signal processors; Field programmable gate arrays (FPGA); Logic Synthesis; Mapping; Table lookup; Additional key word and phrasesdigital signal processor; Array devices; Combinational logic; Field programmables; Field-programmable gate array device; High-level synthesis; Key words; Neural-networks; Programmable gate array; Signal processor; High level synthesis
Hardware-accelerated Real-time Drift-awareness for Robust Deep Learning on Wireless RF Data,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164235381&doi=10.1145%2f3563394&partnerID=40&md5=a980c1da8e9449c7221c65b9be350e9c,"Proactive and intelligent management of network resource utilization (RU) using deep learning (DL) can significantly improve the efficiency and performance of the next generation of wireless networks. However, variations in wireless RU are often affected by uncertain events and change points due to the deviations of real data distribution from that of the original training data. Such deviations, which are known as dataset drifts, can subsequently lead to a shift in the corresponding decision boundary degrading the DL model prediction performance. To address these challenges, we present hardware-accelerated real-time radio frequency (RF) analytics and drift-awareness modules for robust DL predictions. We have prototyped the proposed design on a Zynq-7000 System-on-Chip that contains an FPGA and an embedded ARM processor. We have used Xilinx Vivado design suite for synthesis and analysis of the HDL design for the proposed solution. To detect dataset drifts, the proposed solution adopts a distance-based technique on FPGA to quantify in real-time the change between the prediction distribution obtained from DL predictions and data distribution of input streaming samples. Using various performance metrics, we have extensively evaluated the performance of the proposed solution and shown that it can significantly improve the DL model robustness in the presence of dataset drifts.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesProactive resource allocation; dataset drift detection; deep learning; FPGA; HDL design; HLS; resource utilization; robustness; Xilinx,Computer hardware description languages; Deep learning; Forecasting; Integrated circuit design; System-on-chip; Uncertainty analysis; Additional key word and phrasesproactive resource allocation; Dataset drift detection; Deep learning; HDL design; HLS; Key words; Resources allocation; Resources utilizations; Robustness; Xilinx; Field programmable gate arrays (FPGA)
A Survey on FPGA Cybersecurity Design Strategies,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164237950&doi=10.1145%2f3561515&partnerID=40&md5=5ba5eb701b408691f1bca5eeed5efefb,"This article presents a critical literature review on the security aspects of field-programmable gate array (FPGA) devices. FPGA devices present unique challenges to cybersecurity through their reconfigurable nature. The article also pays special attention to emerging system-on-chip (SoC) FPGA devices that incorporate a hard processing system (HPS) on the same die as the FPGA logic. While this incorporation reduces the need for vulnerable external signals, the HPS in SoC FPGA devices adds a level of complexity that is not present for stand-alone FPGA devices. This added complexity necessarily hands over the task of securing the device to developers. Even with standard security features in place, the HPS might still have unhindered access to the FPGA logic. A single software flaw could open up a breach that might allow an attacker to extract the FPGA's configuration data. A robust cybersecurity strategy is thus required for developers. As such, this work aims to provide the groundwork to build a solid threat-based cybersecurity design strategy that is specially adapted to SoC FPGA devices.  © 2023 Association for Computing Machinery.",Additional Key Words and PhrasesCybersecurity; FPGA; SoC FPGA; threat model,Computer circuits; Cybersecurity; Field programmable gate arrays (FPGA); Integrated circuit design; Programmable logic controllers; Additional key word and phrasescybersecurity; Field programmables; Field-programmable gate array; Key words; Programmable gate array; System-on-chip field-programmable gate array; Systems-on-Chip; Threat modeling; System-on-chip
Automatic Creation of High-bandwidth Memory Architectures from Domain-specific Languages: The Case of Computational Fluid Dynamics,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164246626&doi=10.1145%2f3563553&partnerID=40&md5=aee831d935552316fd7061d0603f855f,"Numerical simulations can help solve complex problems. Most of these algorithms are massively parallel and thus good candidates for FPGA acceleration thanks to spatial parallelism. Modern FPGA devices can leverage high-bandwidth memory technologies, but when applications are memory-bound designers must craft advanced communication and memory architectures for efficient data movement and on-chip storage. This development process requires hardware design skills that are uncommon in domain-specific experts. In this article, we propose an automated tool flow from a domain-specific language for tensor expressions to generate massively parallel accelerators on high-bandwidth-memory-equipped FPGAs. Designers can use this flow to integrate and evaluate various compiler or hardware optimizations. We use computational fluid dynamics (CFD) as a paradigmatic example. Our flow starts from the high-level specification of tensor operations and combines a multi-level intermediate representation-based compiler with an in-house hardware generation flow to generate systems with parallel accelerators and a specialized memory architecture that moves data efficiently, aiming at fully exploiting the available CPU-FPGA bandwidth. We simulated applications with millions of elements, achieving up to 103 GFLOPS with one compute unit and custom precision when targeting a Xilinx Alveo U280. Our FPGA implementation is up to 25× more energy efficient than expert-crafted Intel CPU implementations.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesHigh-level synthesis; automatic memory generation; computational-fluid dynamics; domain-specific languages; HBM; MLIR,Bandwidth; Computational fluid dynamics; Digital storage; Energy efficiency; Memory architecture; Problem oriented languages; Program compilers; Tensors; Additional key word and phraseshigh-level synthesis; Automatic memory generation; Bandwidth memory; Domains specific languages; HBM; High bandwidth; Key words; Massively parallels; MLIR; Parallel accelerators; Field programmable gate arrays (FPGA)
FlexCNN: An End-to-end Framework for Composing CNN Accelerators on FPGA,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164242825&doi=10.1145%2f3570928&partnerID=40&md5=2e1af4fb42512a355082a74da9dc170f,"With reduced data reuse and parallelism, recent convolutional neural networks (CNNs) create new challenges for FPGA acceleration. Systolic arrays (SAs) are efficient, scalable architectures for convolutional layers, but without proper optimizations, their efficiency drops dramatically for reasons: (1) the different dimensions within same-type layers, (2) the different convolution layers especially transposed and dilated convolutions, and (3) CNN's complex dataflow graph. Furthermore, significant overheads arise when integrating FPGAs into machine learning frameworks. Therefore, we present a flexible, composable architecture called FlexCNN, which delivers high computation efficiency by employing dynamic tiling, layer fusion, and data layout optimizations. Additionally, we implement a novel versatile SA to process normal, transposed, and dilated convolutions efficiently. FlexCNN also uses a fully pipelined software-hardware integration that alleviates the software overheads. Moreover, with an automated compilation flow, FlexCNN takes a CNN in the ONNX1 representation, performs a design space exploration, and generates an FPGA accelerator. The framework is tested using three complex CNNs: OpenPose, U-Net, and E-Net. The architecture optimizations achieve 2.3× performance improvement. Compared to a standard SA, the versatile SA achieves close-to-ideal speedups, with up to 5.98× and 13.42× for transposed and dilated convolutions, with a 6% average area overhead. The pipelined integration leads to a 5× speedup for OpenPose.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesFPGA; CNN; dilated convolution; E-Net; ONNX; OpenPose; systolic array; transposed convolution; U-Net,Complex networks; Convolutional neural networks; Data flow analysis; Efficiency; Field programmable gate arrays (FPGA); Network architecture; Pipelines; Systolic arrays; Additional key word and phrasesfpgum; Convolutional neural network; Dilated convolution; E-net; End to end; Key words; ONNX; Openpose; Transposed convolution; U-net; Convolution
VCSN: Virtual Circuit-Switching Network for Flexible and Simple-to-Operate Communication in HPC FPGA Cluster,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164245232&doi=10.1145%2f3579848&partnerID=40&md5=0b544c53137cdc85d65742c6044cd986,"FPGA clusters promise to play a critical role in high-performance computing (HPC) systems in the near future due to their flexibility and high power efficiency. The operation of large-scale general-purpose FPGA clusters on which multiple users run diverse applications requires flexible network topology to be divided and reconfigured. This paper proposes Virtual Circuit-Switching Network (VCSN) that provides an arbitrarily reconfigurable network topology and simple-to-operate network system among FPGA nodes. With virtualization, user logic on FPGAs can communicate with each other as if a circuit-switching network was available. This paper demonstrates that VCSN with 100 Gbps Ethernet achieves highly-efficient point-to-point communication among FPGAs due to its unique and efficient communication protocol. We compare VCSN with a direct connection network (DCN) that connects FPGAs directly. We also show a concrete procedure to realize collective communication on an FPGA cluster with VCSN. We demonstrate that the flexible virtual topology provided by VCSN can accelerate collective communication with simple operations. Furthermore, based on experimental results, we model and estimate communication performance by DCN and VCSN in a large FPGA cluster. The result shows that VCSN has the potential to accelerate gather communication up to about 1.97 times more than DCN.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Additional Key Words and PhrasesFPGA cluster; collective communication; Ethernet; network virtualization,Computation theory; Ethernet; Switching networks; Timing circuits; Topology; Virtualization; Additional key word and phrasesfpgum cluster; Circuit-switching networks; Collective communications; High performance computing systems; Key words; Network topology; Network virtualization; Performance computing; Simple++; Virtual Circuits Switching; Field programmable gate arrays (FPGA)
Multi-FPGA Designs and Scaling of HPC Challenge Benchmarks via MPI and Circuit-switched Inter-FPGA Networks,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85157966574&doi=10.1145%2f3576200&partnerID=40&md5=a5f7c54347a818189d83f259aecf9798,"While FPGA accelerator boards and their respective high-level design tools are maturing, there is still a lack of multi-FPGA applications, libraries, and not least, benchmarks and reference implementations towards sustained HPC usage of these devices. As in the early days of GPUs in HPC, for workloads that can reasonably be decoupled into loosely coupled working sets, multi-accelerator support can be achieved by using standard communication interfaces like MPI on the host side. However, for performance and productivity, some applications can profit from a tighter coupling of the accelerators. FPGAs offer unique opportunities here when extending the dataflow characteristics to their communication interfaces.In this work, we extend the HPCC FPGA benchmark suite by multi-FPGA support and three missing benchmarks that particularly characterize or stress inter-device communication: b_eff, PTRANS, and LINPACK. With all benchmarks implemented for current boards with Intel and Xilinx FPGAs, we established a baseline for multi-FPGA performance. Additionally, for the communication-centric benchmarks, we explored the potential of direct FPGA-to-FPGA communication with a circuit-switched inter-FPGA network that is currently only available for one of the boards. The evaluation with parallel execution on up to 26 FPGA boards makes use of one of the largest academic FPGA installations.  © 2023 Copyright held by the owner/author(s).",Additional Key Words and PhrasesFPGA; high-level synthesis; HPC benchmarking; OpenCL,Benchmarking; High level synthesis; Program processors; Timing circuits; Additional key word and phrasesfpgum; Communication interface; FPGA design; High-level synthesis; HPC benchmarking; Key words; Multi-FPGA; Opencl; Performance; Scalings; Field programmable gate arrays (FPGA)
Introduction to Special Section on FPT'20,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149033502&doi=10.1145%2f3579850&partnerID=40&md5=5c4634dedf17284f006fc37aa921255c,[No abstract available],,
A Scalable Many-core Overlay Architecture on an HBM2-enabled Multi-Die FPGA,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147445600&doi=10.1145%2f3547657&partnerID=40&md5=83dc163d3fd0d46963aeddea7c6ffbda,"The overlay architecture enables to raise the abstraction level of hardware design and enhances hardware-accelerated applications' portability. In FPGAs, there is a growing awareness of the overlay structure as typified by many-core architecture. It works in theory; however, it is difficult in practice, because it is beset with serious design issues. For example, the size of FPGAs is bigger than before. It is exacerbating the issue of the place-and-route. Besides, a single FPGA is actually the sum of small-to-middle FPGAs by advancing packaging technology like silicon interposers. Thus, the tightly coupled many-core designs will face this covert issue that the wires among the regions are extremely restricted. This article proposes efficient essential processing elements, micro-architecture design, and the interconnect architecture toward a scalable many-core overlay design. In particular, our work proposes a novel compact buffering technique to reduce memory resource utilization in tightly connected overlays while preserving computational efficiency. This technique reduces the utilization of BlockRAM to nearly 50% while achieving a best-case computational efficiency of 91.93% in a three-dimensional Jacobi benchmark. Besides, the proposed enhancements led to around 2× and 3× improvement in performance and power efficiency, respectively. Moreover, the improved scalability allowed increasing compute resources and delivering around 4× better performance and power efficiency, as compared to the baseline Dynamically Re-programmable Architecture of Gather-scatter Overlay Nodes overlay.  © 2023 Copyright held by the owner/author(s).",compact buffering; EPR; HBM2; multi-die FPGA; network interconnect; overlay architecture; stencil computation,Closed loop control systems; Computation theory; Computational efficiency; Computer architecture; Integrated circuit design; Network architecture; Abstraction level; Compact buffering; HBM2; Many-core; Multi-die FPGA; Network interconnects; Overlay architecture; Performance efficiency; Power-efficiency; Stencil computations; Field programmable gate arrays (FPGA)
Jitter-based Adaptive True Random Number Generation Circuits for FPGAs in the Cloud,2023,ACM Transactions on Reconfigurable Technology and Systems,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149033697&doi=10.1145%2f3487554&partnerID=40&md5=ab5926411eade53bdddb0f46b35f0ab0,"In this article, we present and evaluate a true random number generator (TRNG) design that is compatible with the restrictions imposed by cloud-based Field Programmable Gate Array (FPGA) providers such as Amazon Web Services (AWS) EC2 F1. Because cloud FPGA providers disallow the ring oscillator circuits that conventionally generate TRNG entropy, our design is oscillator-free and uses clock jitter as its entropy source. The clock jitter is harvested with a time-to-digital converter (TDC) and a controllable delay line that is continuously tuned to compensate for process, voltage, and temperature variations. After describing the design, we present and validate a stochastic model that conservatively quantifies its worst-case entropy. We deploy and model the design in the cloud on 60 EC2 F1 FPGA instances to ensure sufficient randomness is captured. TRNG entropy is further validated using NIST test suites, and experiments are performed to understand how the TRNG responds to on-die power attacks that disturb the FPGA supply voltage in the vicinity of the TRNG. After introducing and validating our basic TRNG design, we introduce and validate a new variant that uses four instances of a linkable sampling module to increase the entropy per sample and improve throughput. The new variant improves throughput by 250% at a modest 17% increase in CLB count.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",AWS EC2; Cloud FPGAs; entropy; NIST; stochastic model; true random number generator,Clocks; Entropy; Field programmable gate arrays (FPGA); Frequency converters; Integrated circuit design; Jitter; Number theory; Oscillators (electronic); Random number generation; Stochastic systems; Timing circuits; Web services; Amazon web service EC2; Amazon web services; Cloud FPGA; Field programmables; NIST; Programmable gate array; Random number generators; Stochastic-modeling; True random number generator; True randoms; Stochastic models
